- en: Evaluating Sentiment on Twitter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twitter is a highly popular social network with over 300 million monthly active
    users. The platform has been developed around short posts (limited to a number
    of characters; currently, the limit is 280 characters). The posts themselves are
    called tweets. On average, 6000 tweets are tweeted every second, which equates
    to around 200 billion tweets per year. This constitutes a huge amount of data
    that contains an equal amount of information. As is obvious, it is not possible
    to analyze this volume of data by hand. Thus, automated solutions have been employed,
    both by Twitter and third parties. One of the hottest topics involves a tweet's
    sentiment, or how the user feels about the topic that they tweets. Sentiment analysis
    comes in many flavors. The most common approach is a positive or negative classification
    of each tweet. Other approaches involve a more complex analysis of positive and
    negative emotions, such as anger, disgust, fear, happiness, sadness, and surprise.
    In this chapter, we will briefly present some sentiment analysis tools and practices.
    Following this, we will cover the basics of building a classifier that leverages
    ensemble learning techniques in order to classify tweets. Finally, we will see
    how we can classify tweets in real time by using Twitter's API.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting Twitter data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying tweets in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter11)'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/2XSLQ5U](http://bit.ly/2XSLQ5U).
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis can be implemented in a number of ways. The easiest to both
    implement and understand are lexicon-based approaches. These methods leverage
    the use of lists (lexicons) of polarized words and expressions. Given a sentence,
    these methods count the number of positive and negative words and expressions.
    If there are more positive words/expressions, the sentence is labeled as positive.
    If there are more negative than positive words/expressions, the sentence is labeled
    as negative. If the number of positive and negative words/expressions are equal,
    the sentence is labeled as neutral. Although this approach is relatively easy
    to code and does not require any training, it has two major disadvantages. First,
    it does not take into account interactions between words. For example, *not bad*,
    which is actually a positive expression, can be classified as negative, as it
    is composed of two negative words. Even if the expression is included in the lexicon
    under positive, the expression *not that bad* may not be included. The second
    disadvantage is that the whole process relies on good and complete lexicons. If
    the lexicon omits certain words, the results can be very poor.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to train a machine learning model in order to classify sentences.
    In order to do so, a training dataset has to be created, where a number of sentences
    are labeled as positive or negative by human experts. This process indirectly
    uncovers a hidden problem in (and also indicates the difficulty of) sentiment
    analysis. Human analysts agree on 80% to 85% of the cases. This is partly due
    to the subjective nature of many expressions. For example, the sentence *Today
    the weather is nice, yesterday it was bad*, can be either positive, negative,
    or neutral. This depends on intonation. Assuming that the bold word is intonated,
    *Today the weather is **nice**, yesterday it was bad* is positive. *Today the
    weather is nice, yesterday it was **bad** *is negative, while *Today the weather
    is nice, yesterday it was bad* is actually neutral (a simple observation of a
    change in the weather).
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about the problem of disagreement between human analysts in
    sentiment classification at: [https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview](https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create machine learning features from text data, usually, n-grams
    are created. N-grams are sequences of *n* words extracted from each sentence.
    For example, the sentence "Hello there, kids" contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '1-grams: "Hello", "there,", "kids"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2-grams: "Hello there,”, "there, kids"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3-grams: "Hello there, kids"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to create numeric features for a dataset, a single feature is created
    for each unique N-gram. For each instance, the feature''s value depends on the
    number of times it appears in the sentence. For example, consider the following
    toy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sentence** | **Polarity** |'
  prefs: []
  type: TYPE_TB
- en: '| My head hurts | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| The food was good food | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| The sting hurts | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| That was a good time | Negative |'
  prefs: []
  type: TYPE_TB
- en: A sentiment toy dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we will only use 1-grams (unigrams). The unique unigrams contained
    in the dataset are: "My", "head", "hurts", "The", "food", "was", "good", "sting",
    "That", "a", and "time". Thus, each instance has 11 features. Each feature corresponds
    to a single *n*-gram (in our case, a unigram). Each feature’s value equals the
    number of appearances of the corresponding *n*-gram in the instance. The final
    dataset is depicted in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **My** | **Head** | **Hurts** | **The** | **Food** | **Was** | **Good** |
    **Sting** | **That** | **A** | **Time** | **Polarity** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 1 | 2 | 1 | 1 | 0 | 0 | 0 | 0 | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 | 1 | Negative |'
  prefs: []
  type: TYPE_TB
- en: The extracted features dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, each instance is normalized, so each feature represents the relative
    frequency, rather than the absolute frequency (count), of each *n*-gram. This
    method is called **Term Frequency** (**TF**). The TF dataset is depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **My** | **Head** | **Hurts** | **The** | **Food** | **Was** | **Good** |
    **Sting** | **That** | **A** | **Time** | **Polarity** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.33 | 0.33 | 0.33 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0.2 | 0.4 | 0.2 | 0.2 | 0 | 0 | 0 | 0 | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0.33 | 0.33 | 0 | 0 | 0 | 0.33 | 0 | 0 | 0 | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 | 0 | 0.2 | 0.2 | 0 | 0.2 | 0.2 | 0.2 | Negative |'
  prefs: []
  type: TYPE_TB
- en: The TF dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In the English language, some terms exhibit a really high frequency, while
    contributing little towards the expression’s sentiment. In order to account for
    this fact, **Inverse Document Frequency** (**IDF**) is employed. IDF puts more
    emphasis on infrequent terms. For *N* instances with *K* unique unigrams, the
    IDF of unigram *u*, which is present in *M* instances, is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb7ea6d-9315-4036-8d83-4203240902dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following table depicts the IDF-transformed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **My** | **Head** | **Hurts** | **The** | **Food** | **Was** | **Good** |
    **Sting** | **That** | **A** | **Time** | **Polarity** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.6 | 0.6 | 0.3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0.3 | 0.6 | 0.3 | 0.3 | 0 | 0 | 0 | 0 | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0.3 | 0.3 | 0 | 0 | 0 | 0.6 | 0 | 0 | 0 | Positive |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 | 0 | 0.3 | 0.3 | 0 | 0.6 | 0.6 | 0.6 | Negative |'
  prefs: []
  type: TYPE_TB
- en: The IDF dataset
  prefs: []
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stemming is another practice usually utilized in sentiment analysis. It is the
    process of reducing words to their root. This lets us handle words that originate
    from the same root as a single unigram. For example, *love*, *loving*, and *loved*
    will be all handled as the same unigram, *love*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Twitter data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of ways to gather Twitter data. From web scraping to using
    custom libraries, each one has different advantages and disadvantages. For our
    implementation, as we also need sentiment labeling, we will utilize the `Sentiment140`
    dataset ([http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip)).
    The reason that we do not collect our own data is mostly due to the time we would
    need to label it. In the last section of this chapter, we will see how we can
    collect our own data and analyze it in real time. The dataset consists of 1.6
    million tweets, containing the following 6 fields:'
  prefs: []
  type: TYPE_NORMAL
- en: The tweet's polarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A numeric ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The date it was tweeted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The query used to record the tweet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user's name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tweet's text content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our models, we will only need the tweet''s text and polarity. As can be
    seen in the following graph, there are 800,000 positive (with a polarity 4) and
    800,000 negative (with a polarity 0) tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b782be9-bd85-4c47-a9ef-d13a8eaf4a44.png)'
  prefs: []
  type: TYPE_IMG
- en: Polarity distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can also verify the statement we made earlier about word frequencies.
    The following graph depicts the 30 most common words in the dataset. As is evident,
    none of them bears any sentiment. Thus, an IDF transform would be more beneficial
    to our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58641b42-29bc-42aa-b6f0-89246483f46d.png)'
  prefs: []
  type: TYPE_IMG
- en: The 30 most common words in the dataset and the number of occurrences of each
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most important step in sentiment analysis (as is the case with most machine
    learning problems) is the preprocessing of our data. The following table contains
    10 tweets, randomly sampled from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **id** | **text** |'
  prefs: []
  type: TYPE_TB
- en: '| 44 | @JonathanRKnight Awww I soo wish I was there to see... |'
  prefs: []
  type: TYPE_TB
- en: '| 143873 | Shaking stomach flipping........god i hate thi... |'
  prefs: []
  type: TYPE_TB
- en: '| 466449 | why do they refuse to put nice things in our v... |'
  prefs: []
  type: TYPE_TB
- en: '| 1035127 | @KrisAllenmusic visit here |'
  prefs: []
  type: TYPE_TB
- en: '| 680337 | Rafa out of Wimbledon Love Drunk by BLG out S... |'
  prefs: []
  type: TYPE_TB
- en: '| 31250 | It''s official, printers hate me Going to sul... |'
  prefs: []
  type: TYPE_TB
- en: '| 1078430 | @_Enigma__ Good to hear |'
  prefs: []
  type: TYPE_TB
- en: '| 1436972 | Dear Photoshop CS2\. i love you. and i miss you! |'
  prefs: []
  type: TYPE_TB
- en: '| 401990 | my boyfriend got in a car accident today ! |'
  prefs: []
  type: TYPE_TB
- en: '| 1053169 | Happy birthday, Wisconsin! 161 years ago, you ... |'
  prefs: []
  type: TYPE_TB
- en: An outline of 10 random samples from the dataset
  prefs: []
  type: TYPE_NORMAL
- en: We can immediately make the following observations. First, there are references
    to other users, for example, `@KrisAllenmusic`. These references do not provide
    any information about the tweet's sentiment. Thus, during preprocessing, we will
    remove them. Second, there are numbers and punctuation. These also do not contribute
    to the tweet’s sentiment, so they must also be removed. Third, some letters are
    capitalized while others are not. As capitalization does not alter the word’s
    sentiment, we can choose to either convert all letters to lowercase or to convert
    them to uppercase. This ensures that words such as *LOVE*, *love*, and *Love*
    will be handled as the same unigram. If we sample more tweets, we can identify
    more problems. There are hashtags (such as `#summer`), which also do not contribute
    to the tweet’s sentiment. Furthermore, there are URL links (for example [https://www.packtpub.com/eu/](https://www.packtpub.com/eu/))
    and HTML attributes (such as `&amp` which corresponds to `&`). These will also
    be removed during our preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to preprocess our data, first, we must import the required libraries.
    We will use pandas; Python''s built-in regular expressions library, `re`; `punctuation`
    from `string`; and the **Natural Language Toolkit** (**NLTK**). The `nltk` library
    can be easily installed either through `pip` or `conda` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the libraries, we load the data, change the polarity from *[0,
    4]* to *[0, 1]*, and discard all fields except for the text content and the polarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw earlier, many words do not contribute to a tweet''s sentiment, although
    they frequently appear in text. Search engines handle this by removing such words,
    which are called stop words. NLTK has a list of the most common stop words that
    we are going to utilize. Furthermore, as there are a number of stop words that
    are contractions (such as "you''re" and "don''t") and tweets frequently omit single
    quotes in contractions, we expand the list in order to include contractions without
    single quotes (such as "dont"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define two distinct functions. The first function, `clean_string`,
    cleans the tweet by removing all elements we discussed earlier (such as references,
    hashtags, and so on). The second function removes all punctuation or stop word
    and stems each word, by utilizing NLTK''s `PorterStemmer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we would like to compare the performance of the ensemble with the base learners
    themselves, we will define a function that will evaluate any given classifier.
    The two most important factors that will define our dataset are the n-grams we
    will use and the number of features. Scikit-learn has an implementation of an
    IDF feature extractor, the `TfidfVectorizer` class. This allows us to only utilize
    the top *M* most frequent features, as well as define the n-gram range we will
    use, through the `max_features` and `ngram_range` parameters. It creates sparse
    arrays of features, which saves a great deal of memory, but the results must be
    converted to normal arrays before they can be processed by scikit-learn''s classifiers.
    This is achieved by calling the `toarray()` function. Our `check_features_ngrams` function
    accepts the number of features, a tuple of minimum and maximum n-grams, and a
    list of named classifiers (a name, classifier tuple). It extracts the required
    features from the dataset and passes them to the nested `check_classifier`. This
    function trains and evaluates each classifier, as well as exports the results
    to the specified file, `outs.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are depicted in the following diagram. As is evident, as we increase
    the number of features, the accuracy increases for all classifiers. Furthermore,
    if the number of features is relatively small, unigrams outperform combinations
    of unigrams and bigrams/trigrams. This is due to the fact that the most frequent
    expressions are not sentimental. Finally, although voting exhibits a relatively
    satisfactory performance, it is not able to outperform logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0300f491-87fe-4535-8f10-53273d2ff456.png)'
  prefs: []
  type: TYPE_IMG
- en: Results of voting and base learners
  prefs: []
  type: TYPE_NORMAL
- en: Classifying tweets in real time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use our model in order to classify tweets in real time using Twitter’s
    API. In order to simplify things, we will make use of a very popular wrapper library
    for the API, `tweepy` ([https://github.com/tweepy/tweepy](https://github.com/tweepy/tweepy)).
    Installation is easily achieved with `pip install tweepy`. The first step to accessing
    Twitter programmatically is to generate relevant credentials. This is achieved
    by navigating to [https://apps.twitter.com/](https://apps.twitter.com/) and selecting
    Create an app. The application process is straightforward and should be accepted
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using tweepy''s `StreamListener`, we will define a class that listens for incoming
    tweets, and as soon as they arrive, it classifies them and prints the original
    text and predicted polarity. First, we will load the required libraries. As a
    classifier, we will utilize the voting ensemble we trained earlier. First, we
    load the required libraries. We need the `json` library, as tweets are received
    in the JSON format; parts of the `tweepy` library; as well as the scikit-learn
    components we utilized earlier. Furthermore, we store our API keys in variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed to create and train our `TfidfVectorizer` and `VotingClassifier`
    with 30,000 features and n-grams in the *[1, 3]* range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed with defining our `StreamClassifier` class, responsible for
    listening for incoming tweets and classifying them as they arrive. It inherits
    the `StreamListener` class from `tweepy`. By overriding the `on_data` function,
    we are able to process tweets as they arrive through the stream. The tweets arrive
    in JSON format, so we first parse them with `json.loads(data)`, which returns
    a dictionary, and then extract the text using the `"text"` key. We can then extract
    the features using the fitted `vectorizer` and utilize the features in order to
    predict its polarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we instantiate `StreamClassifier`, passing as arguments, the trained
    voting ensemble and `TfidfVectorizer` and authenticate using the `OAuthHandler`.
    In order to start the stream, we instantiate a `Stream` object with the `OAuthHandler`
    and `StreamClassifier` objects as parameters and define the keywords we want to
    track with `filter(track=[''Trump''])`. In this case, we track tweets that contain
    the keyword `''Trump''` as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it! The preceding code now tracks any tweet containing the keyword
    Trump and predicts its sentiment in real time. The following table depicts some
    simple tweets that were classified:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Text** | **Polarity** |'
  prefs: []
  type: TYPE_TB
- en: '| **RT @BillyBaldwin**: Only two things funnier than my brothers impersonation
    of Trump. Your daughters impersonation of being an honest, decent… | Negative
    |'
  prefs: []
  type: TYPE_TB
- en: '| **RT @danpfeiffer**: This is a really important article for Democrats to
    read. Media reports of Trump’s malfeasance is only the start. It''s the… | Positive
    |'
  prefs: []
  type: TYPE_TB
- en: '| **RT @BillKristol**: "In other words, Trump had backed himself, not Mexico,
    into a corner. They had him. He had to cave. And cave he did. He go… | Positive
    |'
  prefs: []
  type: TYPE_TB
- en: '| **RT @SenJeffMerkley**: That Ken Cuccinelli started today despite not being
    nominated is unacceptable. Trump is doing an end run around the Sen… | Negative
    |'
  prefs: []
  type: TYPE_TB
- en: Example of tweets being classified
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the possibility of using ensemble learning in
    order to classify tweets. Although a simple logistic regression can outperform
    ensemble learning techniques, it is an interesting introduction to the realm of
    natural language processing and the techniques that are used in order to preprocess
    the data and extract useful features. In summary, we introduced the concepts of
    n-grams, IDF feature extraction, stemming, and stop word removal. We discussed
    the process of cleaning the data, as well as training a voting classifier and
    using it to classify tweets in real time using Twitter's API.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how ensemble learning can be utilized in the
    design of recommender systems, with the aim of recommending movies to a specific
    user.
  prefs: []
  type: TYPE_NORMAL

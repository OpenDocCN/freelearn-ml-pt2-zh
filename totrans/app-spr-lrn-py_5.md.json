["```py\n    import pandas as pd\n    import numpy as np\n    %matplotlib inline\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import KFold\n    ```", "```py\n    data = pd.read_csv('titanic.csv')\n    data.head()\n    ```", "```py\n    def preprocess(data):\n        def fix_age(age):\n            if np.isnan(age):\n                return -1\n            elif age < 1:\n                return age*100\n            else:\n                return age\n\n        data.loc[:, 'Age'] = data.Age.apply(fix_age)\n        data.loc[:, 'Sex'] = data.Sex.apply(lambda s: int(s == 'female'))\n\n        embarked = pd.get_dummies(data.Embarked, prefix='Emb')[['Emb_C','Emb_Q','Emb_S']]\n        cols = ['Pclass','Sex','Age','SibSp','Parch','Fare']\n\n        return pd.concat([data[cols], embarked], axis=1).values\n    ```", "```py\n    train, val = train_test_split(data, test_size=0.2, random_state=11)\n    x_train = preprocess(train)\n    y_train = train['Survived'].values\n    x_val = preprocess(val)\n    y_val = val['Survived'].values\n    ```", "```py\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import BaggingClassifier\n    ```", "```py\n    dt_params = {\n        'criterion': 'entropy',\n        'random_state': 11\n    }\n    dt = DecisionTreeClassifier(**dt_params)\n    bc_params = {\n        'base_estimator': dt,\n        'n_estimators': 50,\n        'max_samples': 0.5,\n        'random_state': 11,\n        'n_jobs': -1\n    }\n    bc = BaggingClassifier(**bc_params)\n    ```", "```py\n    bc.fit(x_train, y_train)\n    bc_preds_train = bc.predict(x_train)\n    bc_preds_val = bc.predict(x_val)\n    print('Bagging Classifier:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(\n        accuracy_score(y_true=y_train, y_pred=bc_preds_train),\n        accuracy_score(y_true=y_val, y_pred=bc_preds_val)\n    ))\n    ```", "```py\n    dt.fit(x_train, y_train)\n    dt_preds_train = dt.predict(x_train)\n    dt_preds_val = dt.predict(x_val)\n    print('Decision Tree:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(\n        accuracy_score(y_true=y_train, y_pred=dt_preds_train),\n        accuracy_score(y_true=y_val, y_pred=dt_preds_val)\n    ))\n    ```", "```py\n    from sklearn.ensemble import RandomForestClassifier\n    ```", "```py\n    rf_params = {\n        'n_estimators': 100,\n        'criterion': 'entropy',\n        'max_features': 0.5,\n        'min_samples_leaf': 10,\n        'random_state': 11,\n        'n_jobs': -1\n    }\n    rf = RandomForestClassifier(**rf_params)\n    ```", "```py\n    rf.fit(x_train, y_train)\n    rf_preds_train = rf.predict(x_train)\n    rf_preds_val = rf.predict(x_val)\n    print('Random Forest:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(\n        accuracy_score(y_true=y_train, y_pred=rf_preds_train),\n        accuracy_score(y_true=y_val, y_pred=rf_preds_val)\n    ))\n    ```", "```py\n    from sklearn.ensemble import AdaBoostClassifier\n    ```", "```py\n    dt_params = {\n        'max_depth': 1,\n        'random_state': 11\n    }\n    dt = DecisionTreeClassifier(**dt_params)\n    ab_params = {\n        'n_estimators': 100,\n        'base_estimator': dt,\n        'random_state': 11\n    }\n    ab = AdaBoostClassifier(**ab_params)\n    ```", "```py\n    ab.fit(x_train, y_train)\n    ab_preds_train = ab.predict(x_train)\n    ab_preds_val = ab.predict(x_val)\n    print('Adaptive Boosting:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(\n        accuracy_score(y_true=y_train, y_pred=ab_preds_train),\n        accuracy_score(y_true=y_val, y_pred=ab_preds_val)\n    ))\n    ```", "```py\n    ab_params = {\n        'base_estimator': dt,\n        'random_state': 11\n    }\n    n_estimator_values = list(range(10, 210, 10))\n    train_accuracies, val_accuracies = [], []\n    for n_estimators in n_estimator_values:\n        ab = AdaBoostClassifier(n_estimators=n_estimators, **ab_params)\n        ab.fit(x_train, y_train)\n        ab_preds_train = ab.predict(x_train)\n        ab_preds_val = ab.predict(x_val)\n\n        train_accuracies.append(accuracy_score(y_true=y_train, y_pred=ab_preds_train))\n        val_accuracies.append(accuracy_score(y_true=y_val, y_pred=ab_preds_val))\n    ```", "```py\n    plt.figure(figsize=(10,7))\n    plt.plot(n_estimator_values, train_accuracies, label='Train')\n    plt.plot(n_estimator_values, val_accuracies, label='Validation')\n    plt.ylabel('Accuracy score')\n    plt.xlabel('n_estimators')\n    plt.legend()\n    plt.show() \n    ```", "```py\n    from sklearn.ensemble import GradientBoostingClassifier\n    ```", "```py\n    gbc_params = {\n        'n_estimators': 100,\n        'max_depth': 3,\n        'min_samples_leaf': 5,\n        'random_state': 11\n    }\n    gbc = GradientBoostingClassifier(**gbc_params)\n    ```", "```py\n    gbc.fit(x_train, y_train)\n    gbc_preds_train = gbc.predict(x_train)\n    gbc_preds_val = gbc.predict(x_val)\n    print('Gradient Boosting Classifier:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(\n        accuracy_score(y_true=y_train, y_pred=gbc_preds_train),\n        accuracy_score(y_true=y_val, y_pred=gbc_preds_val)\n    ))\n    ```", "```py\n    # Base models\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm import LinearSVC\n    # Stacking model\n    from sklearn.linear_model import LogisticRegression\n    ```", "```py\n    x_train_with_metapreds = np.zeros((x_train.shape[0], x_train.shape[1]+2))\n    x_train_with_metapreds[:, :-2] = x_train\n    x_train_with_metapreds[:, -2:] = -1\n    print(x_train_with_metapreds)\n    ```", "```py\n    kf = KFold(n_splits=5, random_state=11)\n    for train_indices, val_indices in kf.split(x_train):\n        kfold_x_train, kfold_x_val = x_train[train_indices], x_train[val_indices]\n        kfold_y_train, kfold_y_val = y_train[train_indices], y_train[val_indices]\n\n        svm = LinearSVC(random_state=11, max_iter=1000)\n        svm.fit(kfold_x_train, kfold_y_train)\n        svm_pred = svm.predict(kfold_x_val)\n\n        knn = KNeighborsClassifier(n_neighbors=4)\n        knn.fit(kfold_x_train, kfold_y_train)\n        knn_pred = knn.predict(kfold_x_val)\n\n        x_train_with_metapreds[val_indices, -2] = svm_pred\n        x_train_with_metapreds[val_indices, -1] = knn_pred\n    ```", "```py\n    x_val_with_metapreds = np.zeros((x_val.shape[0], x_val.shape[1]+2))\n    x_val_with_metapreds[:, :-2] = x_val\n    x_val_with_metapreds[:, -2:] = -1\n    print(x_val_with_metapreds)\n    ```", "```py\n    svm = LinearSVC(random_state=11, max_iter=1000)\n    svm.fit(x_train, y_train)\n    knn = KNeighborsClassifier(n_neighbors=4)\n    knn.fit(x_train, y_train)\n    svm_pred = svm.predict(x_val)\n    knn_pred = knn.predict(x_val)\n    x_val_with_metapreds[:, -2] = svm_pred\n    x_val_with_metapreds[:, -1] = knn_pred\n    ```", "```py\n    lr = LogisticRegression(random_state=11)\n    lr.fit(x_train_with_metapreds, y_train)\n    lr_preds_train = lr.predict(x_train_with_metapreds)\n    lr_preds_val = lr.predict(x_val_with_metapreds)\n    print('Stacked Classifier:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(\n        accuracy_score(y_true=y_train, y_pred=lr_preds_train),\n        accuracy_score(y_true=y_val, y_pred=lr_preds_val)\n    ))\n    ```", "```py\n    print('SVM:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(\n        accuracy_score(y_true=y_train, y_pred=svm.predict(x_train)),\n        accuracy_score(y_true=y_val, y_pred=svm_pred)\n    ))\n    print('kNN:\\n> Accuracy on training data = {:.4f}\\n> Accuracy on validation data = {:.4f}'.format(\n        accuracy_score(y_true=y_train, y_pred=knn.predict(x_train)),\n        accuracy_score(y_true=y_val, y_pred=knn_pred)\n    ))\n    ```", "```py\n    dt_params = {\n        'criterion': 'mae',\n        'min_samples_leaf': 10,\n        'random_state': 11\n    }\n    ```", "```py\n    knn_params = {\n        'n_neighbors': 5\n    }\n    ```", "```py\n    rf_params = {\n        'n_estimators': 50,\n        'criterion': 'mae',\n        'max_features': 'sqrt',\n        'min_samples_leaf': 10,\n        'random_state': 11,\n        'n_jobs': -1\n    }\n    ```", "```py\n    gbr_params = {\n        'n_estimators': 50,\n        'criterion': 'mae',\n        'max_features': 'sqrt',\n        'min_samples_leaf': 10,\n        'random_state': 11\n    }\n    ```"]
["```py\nimport pandas as pd\ndf = pd.read_csv('exoplanets.csv', nrows=400)\ndf.head()\n```", "```py\ndf['LABEL'].value_counts()\n```", "```py\n1    363 2     37 Name: LABEL, dtype: int64\n```", "```py\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import seaborn as sns\n    sns.set()\n    ```", "```py\n    X = df.iloc[:,1:]\n    y = df.iloc[:,0]\n    ```", "```py\n    def light_plot(index):\n        y_vals = X.iloc[index]\n        x_vals = np.arange(len(y_vals))\n        plt.figure(figsize=(15,8))\n        plt.xlabel('Number of Observations')\n        plt.ylabel('Light Flux')\n        plt.title('Light Plot ' + str(index), size=15)\n        plt.plot(x_vals, y_vals)\n        plt.show()\n    ```", "```py\n    light_plot(0)\n    ```", "```py\n    light_plot(37)\n    ```", "```py\n    light_plot(1)\n    ```", "```py\n    df.info()\n    ```", "```py\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 400 entries, 0 to 399\n    Columns: 3198 entries, LABEL to FLUX.3197\n    dtypes: float64(3197), int64(1)\n    memory usage: 9.8 MB\n    ```", "```py\n    df.isnull().sum().sum()\n    ```", "```py\n    0\n    ```", "```py\n    from xgboost import XGBClassifier from sklearn.metrics import accuracy_score\n    ```", "```py\n    from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n    ```", "```py\n    model = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2)model.fit(X_train, y_train)y_pred = model.predict(X_test)score = accuracy_score(y_pred, y_test)print('Score: ' + str(score))\n    ```", "```py\n    Score: 0.89\n    ```", "```py\narray([[88, 0],\n       [ 0,  12]])\n```", "```py\nfrom sklearn.metrics import confusion_matrix\n```", "```py\nconfusion_matrix(y_test, y_pred)\n```", "```py\narray([[86, 2],\n       [9,  3]])\n```", "```py\n    from sklearn.metrics import classification_report\n    ```", "```py\n    print(classification_report(y_test, y_pred))\n    ```", "```py\n                  precision    recall  f1-score   support\n               1       0.91      0.98      0.94        88\n               2       0.60      0.25      0.35        12\n        accuracy                           0.89       100\n       macro avg       0.75      0.61      0.65       100\n    weighted avg       0.87      0.89      0.87       100\n    ```", "```py\n    from sklearn.metrics import recall_score\n    ```", "```py\n    recall_score(y_test, y_pred, pos_label=2)\n    ```", "```py\n    0.25\n    ```", "```py\n    def xgb_clf(model, nrows):\n    ```", "```py\n        df = pd.read_csv('exoplanets.csv', nrows=nrows)\n        X = df.iloc[:,1:]\n        y = df.iloc[:,0]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n    ```", "```py\n        model.fit(X_train, y_train)\n        y_pred = xg_clf.predict(X_test)\n        score = recall_score(y_test, y_pred, pos_label=2)\n    ```", "```py\n        print(confusion_matrix(y_test, y_pred))\n        print(classification_report(y_test, y_pred))\n        return score\n    ```", "```py\nxgb_clf(XGBClassifier(random_state=2), nrows=800)\n```", "```py\n[[189   1]\n [  9   1]]\n              precision    recall  f1-score   support\n           1       0.95      0.99      0.97       190\n           2       0.50      0.10      0.17        10\n    accuracy                           0.95       200\n   macro avg       0.73      0.55      0.57       200\nweighted avg       0.93      0.95      0.93       200\n0.1\n```", "```py\nxgb_clf(XGBClassifier(random_state=2), nrows=200)\n```", "```py\n[[37  0]\n [ 8  5]]\n              precision    recall  f1-score   support\n           1       0.82      1.00      0.90        37\n           2       1.00      0.38      0.56        13\n    accuracy                           0.84        50\n   macro avg       0.91      0.69      0.73        50\nweighted avg       0.87      0.84      0.81        50\n```", "```py\nxgb_clf(XGBClassifier(random_state=2), nrows=74)\n```", "```py\n[[6 2]\n [5 6]]\n              precision    recall  f1-score   support\n           1       0.55      0.75      0.63         8\n           2       0.75      0.55      0.63        11\n    accuracy                           0.63        19\n   macro avg       0.65      0.65      0.63        19\nweighted avg       0.66      0.63      0.63        19\n0.5454545454545454\n```", "```py\n    df_train = pd.merge(y_train, X_train, left_index=True, right_index=True)\n    ```", "```py\n    new_df = pd.DataFrame(np.repeat(df_train[df_train['LABEL']==2].values,9,axis=0))\n    ```", "```py\n    new_df.columns = df_train.columns\n    ```", "```py\n    df_train_resample = pd.concat([df_train, new_df])\n    ```", "```py\n    df_train_resample['LABEL'].value_counts()\n    ```", "```py\n    1.0    275\n    2.0    250\n    Name: LABEL, dtype: int64\n    ```", "```py\n    X_train_resample = df_train_resample.iloc[:,1:]\n    y_train_resample = df_train_resample.iloc[:,0]\n    ```", "```py\n    model = XGBClassifier(random_state=2)\n    model.fit(X_train_resample, y_train_resample)\n    ```", "```py\n    y_pred = model.predict(X_test)\n    score = recall_score(y_test, y_pred, pos_label=2)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    print(score)\n    ```", "```py\n    [[86  2]\n     [ 8  4]]\n                  precision    recall  f1-score   support\n               1       0.91      0.98      0.95        88\n               2       0.67      0.33      0.44        12\n        accuracy                           0.90       100\n       macro avg       0.79      0.66      0.69       100\n    weighted avg       0.89      0.90      0.88       100\n    0.3333333333333333\n    ```", "```py\ndf['LABEL'] = df['LABEL'].replace(1, 0)\ndf['LABEL'] = df['LABEL'].replace(2, 1)\n```", "```py\ndf['LABEL'].value_counts()\n```", "```py\n0    363\n1     37\nName: LABEL, dtype: int64\n```", "```py\n    X = df.iloc[:,1:]\n    y = df.iloc[:,0]\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n    ```", "```py\n    model = XGBClassifier(scale_pos_weight=10, random_state=2)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    score = recall_score(y_test, y_pred)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    print(score)\n    ```", "```py\n    [[86  2]\n     [ 8  4]]\n                  precision    recall  f1-score   support\n               0       0.91      0.98      0.95        88\n               1       0.67      0.33      0.44        12\n        accuracy                           0.90       100\n       macro avg       0.79      0.66      0.69       100\n    weighted avg       0.89      0.90      0.88       100\n    0.3333333333333333\n    ```", "```py\n    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score\n    ```", "```py\n    kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n    ```", "```py\n    model = XGBClassifier(scale_pos_weight=10, random_state=2)\n    ```", "```py\n    scores = cross_val_score(model, X, y, cv=kfold, scoring='recall')\n    print('Recall: ', scores)\n    print('Recall mean: ', scores.mean())\n    ```", "```py\n    Recall:  [0.10526316 0.27777778]\n    Recall mean:  0.1915204678362573\n    ```", "```py\n    def grid_search(params, random=False, X=X, y=y, model=XGBClassifier(random_state=2)): \n        xgb = model\n        if random:\n            grid = RandomizedSearchCV(xgb, params, cv=kfold, n_jobs=-1, random_state=2, scoring='recall')\n        else:\n            grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1, scoring='recall')\n        grid.fit(X, y)\n        best_params = grid.best_params_\n        print(\"Best params:\", best_params)\n        best_score = grid.best_score_\n        print(\"Best score: {:.5f}\".format(best_score))\n    ```", "```py\n    grid_search(params={'n_estimators':[50, 200, 400, 800]})\n    ```", "```py\n    Best params: {'n_estimators': 50}Best score: 0.19152\n    ```", "```py\n    grid_search(params={'learning_rate':[0.01, 0.05, 0.2, 0.3]})\n    ```", "```py\n    Best params: {'learning_rate': 0.01}\n    Best score: 0.40351\n    ```", "```py\n    grid_search(params={'max_depth':[1, 2, 4, 8]})\n    ```", "```py\n    Best params: {'max_depth': 2}\n    Best score: 0.24415\n    ```", "```py\n    grid_search(params={'subsample':[0.3, 0.5, 0.7, 0.9]})\n    ```", "```py\n    Best params: {'subsample': 0.5}\n    Best score: 0.21637\n    ```", "```py\n    grid_search(params={'gamma':[0.05, 0.1, 0.5, 1]})\n    ```", "```py\n    Best params: {'gamma': 0.05}\n    Best score: 0.24415\n    ```", "```py\n    grid_search(params={'learning_rate':[0.001, 0.01, 0.03], 'max_depth':[1, 2], 'gamma':[0.025, 0.05, 0.5]})\n    ```", "```py\n    Best params: {'gamma': 0.025, 'learning_rate': 0.001, 'max_depth': 2}\n    Best score: 0.53509\n    ```", "```py\n    grid_search(params={'max_delta_step':[1, 3, 5, 7]})\n    ```", "```py\n    Best params: {'max_delta_step': 1}\n    Best score: 0.24415\n    ```", "```py\n    grid_search(params={'subsample':[0.3, 0.5, 0.7, 0.9, 1], \n    'colsample_bylevel':[0.3, 0.5, 0.7, 0.9, 1], \n    'colsample_bynode':[0.3, 0.5, 0.7, 0.9, 1], \n    'colsample_bytree':[0.3, 0.5, 0.7, 0.9, 1]}, random=True)\n    ```", "```py\n    Best params: {'subsample': 0.3, 'colsample_bytree': 0.7, 'colsample_bynode': 0.7, 'colsample_bylevel': 1}\n    Best score: 0.35380\n    ```", "```py\nX_short = X.iloc[:74, :]\ny_short = y.iloc[:74]\n```", "```py\ngrid_search(params={'max_depth':[1, 2, 3], 'colsample_bynode':[0.5, 0.75, 1]}, X=X_short, y=y_short, model=XGBClassifier(random_state=2)) \n```", "```py\nBest params: {'colsample_bynode': 0.5, 'max_depth': 2}\nBest score: 0.65058\n```", "```py\n    df_all = pd.read_csv('exoplanets.csv')\n    ```", "```py\n    df_all['LABEL'] = df_all['LABEL'].replace(1, 0)df_all['LABEL'] = df_all['LABEL'].replace(2, 1)\n    ```", "```py\n    X_all = df_all.iloc[:,1:]y_all = df_all.iloc[:,0]\n    ```", "```py\n    df_all['LABEL'].value_counts()\n    ```", "```py\n    0    5050 1      37 Name: LABEL, dtype: int64\n    ```", "```py\n    weight = int(5050/37)\n    ```", "```py\n    model = XGBClassifier(scale_pos_weight=weight, random_state=2)\n    scores = cross_val_score(model, X_all, y_all, cv=kfold, scoring='recall')\n    print('Recall:', scores)\n    print('Recall mean:', scores.mean())\n    ```", "```py\n    Recall: [0.10526316 0.        ]\n    Recall mean: 0.05263157894736842\n    ```", "```py\n    grid_search(params={'learning_rate':[0.001, 0.01]}, X=X_all, y=y_all, model=XGBClassifier(scale_pos_weight=weight, random_state=2)) \n    ```", "```py\n    Best params: {'learning_rate': 0.001}\n    Best score: 0.26316\n    ```", "```py\n    grid_search(params={'max_depth':[1, 2],'learning_rate':[0.001]}, X=X_all, y=y_all, model=XGBClassifier(scale_pos_weight=weight, random_state=2)) \n    ```", "```py\n    Best params: {'learning_rate': 0.001, 'max_depth': 2}\n    Best score: 0.53509\n    ```", "```py\ndef final_model(X, y, model):\n    model.fit(X, y)\n    y_pred = model.predict(X_all)\n    score = recall_score(y_all, y_pred,)\n    print(score)\n    print(confusion_matrix(y_all, y_pred,))\n    print(classification_report(y_all, y_pred))\n```", "```py\nfinal_model(X_short, y_short, XGBClassifier(max_depth=2, colsample_by_node=0.5, random_state=2))\n```", "```py\n1.0\n[[3588 1462]\n [   0   37]]\n              precision    recall  f1-score   support\n           0       1.00      0.71      0.83      5050\n           1       0.02      1.00      0.05        37\n    accuracy                           0.71      5087\n   macro avg       0.51      0.86      0.44      5087\nweighted avg       0.99      0.71      0.83      5087\n```", "```py\nfinal_model(X, y, XGBClassifier(max_depth=2, colsample_bynode=0.5, scale_pos_weight=10, random_state=2))\n```", "```py\n1.0\n[[4901  149]\n [   0   37]]\n              precision    recall  f1-score   support\n           0       1.00      0.97      0.99      5050\n           1       0.20      1.00      0.33        37\n    accuracy                           0.97      5087\n   macro avg       0.60      0.99      0.66      5087\nweighted avg       0.99      0.97      0.98      5087\n```", "```py\nfinal_model(X_all, y_all, XGBClassifier(max_depth=2, colsample_bynode=0.5, scale_pos_weight=weight, random_state=2))\n```", "```py\n1.0\n[[5050    0]\n [   0   37]]\n              precision    recall  f1-score   support\n           0       1.00      1.00      1.00      5050\n           1       1.00      1.00      1.00        37\n    accuracy                           1.00      5087\n   macro avg       1.00      1.00      1.00      5087\nweighted avg       1.00      1.00      1.00      5087\n```"]
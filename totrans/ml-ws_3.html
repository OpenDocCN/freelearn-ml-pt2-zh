<html><head></head><body>
		<div>
			<div id="_idContainer051" class="Content">
			</div>
		</div>
		<div id="_idContainer052" class="Content">
			<h1 id="_idParaDest-81"><a id="_idTextAnchor083"/>3. Supervised Learning – Key Steps</h1>
		</div>
		<div id="_idContainer066" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will learn about key concepts for solving a supervised learning data problem. Starting from splitting the dataset to effectively create unbiased models that perform well on unseen data, you will learn how to measure the performance of the model in order to analyze it and take the necessary actions to improve it. By the end of this chapter, you will have a firm understanding of how to split a dataset, measure a model's performance, and perform error analysis.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor084"/>Introduction</h1>
			<p>In the preceding chapter, we saw how to solve data problems using unsupervised learning algorithms and applied the concepts that we learned about to a real-life dataset. We also learned how to compare the performance of various algorithms and studied two different metrics for performance evaluation.</p>
			<p>In this chapter, we will explore the main steps for working on a supervised machine learning problem. First, this chapter explains the different sets in which data needs to be split for training, validating, and testing your model. Next, the most common evaluation metrics will be explained. It is important to highlight that, among all the metrics available, only one should be selected as the evaluation metric of the study, and its selection should be made by considering the purpose of the study. Finally, we will learn how to perform error analysis, with the purpose of understanding what measures to take to improve the results of a model.</p>
			<p>The previous concepts apply to both classification and regression tasks, where the former refers to problems where the output corresponds to a finite number of labels, while the latter deals with a continuous output number. For instance, a model that's created to determine whether a person will attend a meeting falls within the classification tasks group. On the other hand, a model that predicts the price of a product is solving a regression task.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor085"/>Supervised Learning Tasks</h1>
			<p>Differing from unsupervised learning algorithms, supervised learning algorithms are characterized by their ability to find relationships between a set of features and a target value (be it discrete or continuous). Supervised learning can solve two types of tasks:</p>
			<ul>
				<li><strong class="bold">Classification</strong>: The objective of these tasks is to approximate a function that maps a set of features to a discrete set of outcomes. These outcomes are commonly known as class labels or categories. Each observation in the dataset should have a class label associated with it to be able to train a model that is capable of predicting such an outcome for future data.<p>An example of a classification task is one that uses demographical data to determine someone's marital status.</p></li>
				<li><strong class="bold">Regression</strong>: Although in regression tasks a function is also created to map a relationship between some inputs and some targets, in regression tasks, the outcome is continuous. This means that the outcome is a real value that can be an integer or a float. <p>An example of a regression task is using the different characteristics of a product to predict its price.</p></li>
			</ul>
			<p>Although many algorithms can be adapted to solve both of these tasks, it is important to highlight that there are some algorithms that don't, which is why it is important to know the task that we want to perform in order to choose the algorithm accordingly. </p>
			<p>Next, we will explore several topics that are crucial for performing any supervised learning task.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor086"/>Model Validation and Testing</h1>
			<p>With all the information now available online, it is easy for almost anybody to start working on a machine learning project. However, choosing the right algorithm for your data is a challenge when there are many options available. Due to this, the decision to use one algorithm over another is achieved through trial and error, where different alternatives are tested. </p>
			<p>Moreover, the decision process to arrive at a good model covers not only the selection of the algorithm but also the tuning of its hyperparameters. To do this, a conventional approach is to divide the data into three parts (training, validation, and testing sets), which will be explained further in the next section.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor087"/>Data Partitioning</h2>
			<p><strong class="bold">Data partitioning</strong> is a process involving dividing a dataset into three subsets so that each set can be used for a different purpose. This way, the development of a model is not affected by the introduction of bias. The following is an explanation of each subset:</p>
			<ul>
				<li><strong class="bold">Training set</strong>: As the name suggests, this is the portion of the dataset that's used for training the model. It consists of the input data (the observations) paired with an outcome (the label class). This set can be used to train as many models as desired, using different algorithms. However, performance evaluation is not done on this set because, since this set was used to train the model, the measure would be biased.</li>
				<li><strong class="bold">Validation set</strong>: Also known as the dev set, this set is used to perform an unbiased evaluation of each model while fine-tuning the hyperparameters. Performance evaluation is frequently done on this set of data to test different configurations of the hyperparameters.<p>Although the model does not learn from this data (it learns from the training set data), it is indirectly affected by the data in this set due to its participation in the process of deciding the changes to the hyperparameters.</p><p>After running different configurations of hyperparameters based on the performance of the model on the validation set, a winning model is selected for each algorithm.</p></li>
				<li><strong class="bold">Testing set</strong>: This is used to perform the final evaluation of the model's performance (after training and validation) on unseen data. This helps measure the performance of the model with real-life data for future predictions.<p>The testing set is also used to compare competing models. Considering that the training set was used to train different models and the validation set was used to fine-tune the hyperparameters of each model to select a winning configuration, the purpose of the testing set is to perform an unbiased comparison of the final models.</p></li>
			</ul>
			<p>The following diagram shows the process of selecting the ideal model and using the sets mentioned previously:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B15781_03_01.jpg" alt="Figure 3.1: Dataset partition purposes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1: Dataset partition purposes</p>
			<p>The sections <strong class="source-inline">A</strong>–<strong class="source-inline">D</strong> shown in the preceding diagram are described as follows:</p>
			<ul>
				<li>Section <strong class="source-inline">A</strong> refers to the process of training the model for the desired algorithms using the data contained in the training set.</li>
				<li>Section <strong class="source-inline">B</strong> represents the fine-tuning process of the hyperparameters of each model. The selection of the best configuration of hyperparameters is based on the performance of the model on the validation set.</li>
				<li>Section <strong class="source-inline">C</strong> shows the process of selecting the final model by comparing the final configuration of each algorithm based on its performance on the testing set.</li>
				<li>Finally, section <strong class="source-inline">D</strong> represents the selected model that will be applied to real-life data for prediction.</li>
			</ul>
			<p>Initially, machine learning problems were solved by only partitioning data into two sets: a training and a testing set. This approach consisted of using the training set to train the model, which is the same as the approach with three sets. However, the testing set was used for fine-tuning the hyperparameters as well as for determining the ultimate performance of the algorithm.</p>
			<p>Although this approach can also work, models that are created using this approach do not always perform equally well on unseen real-life data. This is mainly because, as mentioned previously, the use of the sets to fine-tune the hyperparameters indirectly introduces bias into the model.</p>
			<p>Considering this, there is one way to achieve a less biased model while dividing the dataset into two sets, which is called a <strong class="bold">cross-validation split</strong>. We will explore this in the <em class="italic">Cross-Validation</em> section of this chapter.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor088"/>Split Ratio</h2>
			<p>Now that the purposes of the various sets are clear, it is important to clarify the split ratio in which data needs to be divided. Although there is no exact science for calculating the split ratio, there are a couple of things to consider when doing so:</p>
			<ul>
				<li><strong class="bold">Size of the dataset</strong>: Previously, when data was not easily available, datasets contained between 100 and 100,000 instances, and the conventionally accepted split ratio was 60/20/20% for the training, validation, and testing sets, respectively. <p>With software and hardware improving every day, researchers can put together datasets that contain over a million instances. This capacity to gather huge amounts of data allows the split ratio to be 98/1/1%, respectively. This is mainly because the larger the dataset, the more data can be used for training a model, without compromising the amount of data left for the validation and testing sets.</p></li>
				<li><strong class="bold">The algorithm</strong>: It is important to consider that some algorithms may require higher amounts of data to train a model, as is the case with neural networks. In this case, as with the preceding approaches, you should always opt for a larger training set. <p>On the other hand, some algorithms do not require the validation and testing sets to be split equally. For instance, a model with fewer hyperparameters can be easily tuned, which allows the validation set to be smaller than the testing set. However, if a model has many hyperparameters, you will need to have a larger validation set.</p></li>
			</ul>
			<p>Nevertheless, even though the preceding measures serve as a guide for splitting the dataset, it is always important to consider the distribution of your dataset and the purpose of the study. For instance, a model that is going to be used to predict an outcome on data with a different distribution than the one used to train the model, the real-life data, even if limited, must at least be a part of the testing set to make sure that the model will work for the desired purpose.</p>
			<p>The following diagram displays the proportional partition of the dataset into three subsets. It is important to highlight that the training set must be larger than the other two, as it is the one to be used for training the model. Additionally, it is possible to observe that both the training and validation sets have an effect on the model, while the testing set is mainly used to validate the actual performance of the model with unseen data. Considering this, the training and validation sets must come from the same distribution:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B15781_03_02.jpg" alt="Figure 3.2: Visualization of the split ratio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2: Visualization of the split ratio</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor089"/>Exercise 3.01: Performing a Data Partition on a Sample Dataset</h2>
			<p>In this exercise, we will be performing a data partition on the <strong class="source-inline">wine</strong> dataset using the split ratio method. The partition in this exercise will be done using the three-splits approach. Follow these steps to complete this exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For the exercises and activities within this chapter, you will need to have Python 3.7, NumPy, Jupyter, Pandas, and scikit-learn installed on your system.</p>
			<ol>
				<li>Open a Jupyter Notebook to implement this exercise. Import the required elements, as well as the <strong class="source-inline">load_wine</strong> function from scikit-learn's <strong class="source-inline">datasets</strong> package:<p class="source-code">from sklearn.datasets import load_wine</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p>The first line imports the function that will be used to load the dataset from scikit-learn. Next, <strong class="source-inline">pandas</strong> library is imported. Finally, the <strong class="source-inline">train_test_split</strong> function is imported, which will be in charge of partitioning the dataset. The function partitions the data into two subsets (a train and a test set). As the objective of this exercise is to partition data into three subsets, the function will be used twice to achieve the desired result.</p></li>
				<li>Load the <strong class="source-inline">wine</strong> toy dataset and store it in a variable named <strong class="source-inline">data</strong>. Use the following code snippet to do so:<p class="source-code">data = load_wine()</p><p>The <strong class="source-inline">load_wine</strong> function loads the toy dataset provided by scikit-learn.</p><p class="callout-heading">Note</p><p class="callout">To check the characteristics of the dataset, visit the following link: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html</a>.</p><p>The output from the preceding function is a dictionary-like object, which separates the features (callable as data) from the target (callable as target) into two attributes.</p></li>
				<li>Convert each attribute (data and target) into a Pandas DataFrame to facilitate data manipulation. Print the shape of both DataFrames:<p class="source-code">X = pd.DataFrame(data.data)</p><p class="source-code">Y = pd.DataFrame(data.target)</p><p class="source-code">print(X.shape,Y.shape)</p><p>The output from the <strong class="source-inline">print</strong> function should be as follows:</p><p class="source-code">(178, 13) (178, 1)</p><p>Here, the values in the first parenthesis represent the shape of DataFrame <strong class="source-inline">X</strong> (known as the features matrix), while the values in the second parenthesis refer to the shape of DataFrame <strong class="source-inline">Y</strong> (known as the target matrix).</p></li>
				<li>Perform your first split of the data using the <strong class="source-inline">train_test_split</strong> function. Use the following code snippet to do so:<p class="source-code">X, X_test, Y, Y_test = train_test_split(X, Y, test_size = 0.2)</p><p>The inputs of the <strong class="source-inline">train_test_split</strong> function are the two matrices <strong class="source-inline">(X,Y)</strong> and the size of the test set, as a value between 0 and 1, which represents the proportion.</p><p class="callout-heading">Note</p><p class="callout">Considering that we are dealing with a small dataset, as per the explanation in the <em class="italic">Split Ratio</em> section, we're using a split ratio of 60/20/20%. Remember that for larger datasets, the split ratio usually changes to 98/1/1%.</p><p>The outputs of the preceding function are four matrices: <strong class="source-inline">X</strong> divided into two subsets (train and test) and <strong class="source-inline">Y</strong> divided into two corresponding subsets:</p><p class="source-code">print(X.shape, X_test.shape, Y.shape, Y_test.shape)</p><p>By printing the shape of all four matrices, as per the preceding code snippet, it is possible to confirm that the size of the test subset (both <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>) is 20% of the total size of the original dataset (150 * 0.2 = 35.6) rounded to an integer, while the size of the train set is the remaining 80%:</p><p class="source-code">(142, 13) (36, 13) (142, 1) (36, 1)</p></li>
				<li>To create a validation set (dev set), we will use the <strong class="source-inline">train_test_split</strong> function to divide the train sets we obtained in the previous step. However, to obtain a dev set that's the same shape as the test set, it is necessary to calculate the proportion of the size of the test set over the size of the train set before creating a validation set. This value will be used as the <strong class="source-inline">test_size</strong> for the next step:<p class="source-code">dev_size = 36/142</p><p class="source-code">print(dev_size)</p><p>Here, <strong class="source-inline">36</strong> is the size of the test set we created in the previous step, while <strong class="source-inline">142</strong> is the size of the train set that will be further split. The result from this operation is around <strong class="source-inline">0.25</strong>, which can be verified using the <strong class="source-inline">print</strong> function.</p></li>
				<li>Use the <strong class="source-inline">train_test_split</strong> function to divide the train set into two subsets (train and dev sets). Use the result from the operation in the previous step as the <strong class="source-inline">test_size</strong>:<p class="source-code">X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, \</p><p class="source-code">                                 test_size = dev_size)</p><p class="source-code">print(X_train.shape, Y_train.shape, X_dev.shape, \</p><p class="source-code">      Y_dev.shape, X_test.shape, Y_test.shape)</p><p>The output of the <strong class="source-inline">print</strong> function is as follows:</p><p class="source-code">(106, 13) (106, 1) (36, 13) (36, 1) (36, 13) (36, 1)</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2AtXAWS">https://packt.live/2AtXAWS</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2YECtsG">https://packt.live/2YECtsG</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>You have successfully split the dataset into three subsets to develop efficient machine learning projects. Feel free to test different split ratios.</p>
			<p>In conclusion, the split ratio to partition data is not fixed and should be decided by taking into account the amount of data available, the type of algorithm to be used, and the distribution of the data.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor090"/>Cross-Validation</h2>
			<p><strong class="bold">Cross-validation</strong> is also a procedure that's used to partition data by resampling the data that's used to train and validate the model. It consists of a parameter, <em class="italic">K,</em> that represents the number of groups that the dataset will be divided into. </p>
			<p>Due to this, the procedure is also referred to as K-fold cross-validation, where <em class="italic">K</em> is usually replaced by a number of your choice. For instance, a model that's created using a 10-fold cross-validation procedure signifies a model where data is divided into 10 subgroups. The procedure of cross-validation is illustrated in the following diagram:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B15781_03_03.jpg" alt="Figure 3.3: Cross-validation procedure&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3: Cross-validation procedure</p>
			<p>The preceding diagram displays the general procedure that's followed during cross-validation:</p>
			<ol>
				<li value="1">Data is shuffled randomly, considering that the cross-validation process is repeated.</li>
				<li>Data is split into <em class="italic">K</em> subgroups.</li>
				<li>The validation/testing set is selected as one of the subgroups that were created. The rest of the subgroups become the training set. </li>
				<li>The model is trained on the training set, as usual. The model is evaluated using the validation/testing set.</li>
				<li>The result from that iteration is saved. The parameters are tuned based on the results, and the process starts again by reshuffling the data. This process is repeated K number of times.</li>
			</ol>
			<p>According to the preceding steps, the dataset is divided into <em class="italic">K</em> sets and the model is trained <em class="italic">K</em> times. Each time, one set is selected as the validation set and the remaining sets are used for the training process. </p>
			<p>Cross-validation can be done using a three-split approach or a two-split one. For the former, the dataset is initially divided into training and testing sets, after which the training set is divided using cross-validation to create different configurations of training and validation sets. The latter approach, on the other hand, uses cross-validation on the entire dataset.</p>
			<p>The popularity of cross-validation is due to its capacity to build "unbiased" models as it allows us to measure the performance of the algorithm on different segments of the dataset, which also provides us with an idea of its performance on unseen data. It is also popular because it allows you to build highly effective models out of a small dataset.</p>
			<p>There is no exact science to choosing the value for <em class="italic">K</em>, but it is important to consider that lower values for <em class="italic">K</em> tend to decrease variance and increase bias, while higher <em class="italic">K</em> values result in the opposite behavior. Also, the lower <em class="italic">K</em> is, the less expensive the processes, which results in faster running times.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The concepts of variance and bias will be explained in the <em class="italic">Bias, Variance, and Data Mismatch</em> section.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor091"/>Exercise 3.02: Using Cross-Validation to Partition the Train Set into a Training and a Validation Set</h2>
			<p>In this exercise, we will be performing a data partition on the <strong class="source-inline">wine</strong> dataset using the cross-validation method. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook to implement this exercise and import all the required elements:<p class="source-code">from sklearn.datasets import load_wine</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.model_selection import KFold</p><p>The last line in the preceding code imports the <strong class="source-inline">KFold</strong> class from scikit-learn, which will be used to partition the dataset.</p></li>
				<li>Load the <strong class="source-inline">wine</strong> dataset as per the previous exercise and create the Pandas DataFrames containing the features and target matrices:<p class="source-code">data = load_wine()</p><p class="source-code">X = pd.DataFrame(data.data)</p><p class="source-code">Y = pd.DataFrame(data.target)</p></li>
				<li>Split the data into training and testing sets using the <strong class="source-inline">train_test_split</strong> function, which you learned about in the previous exercise, using a <strong class="source-inline">test_size</strong> of 0.10:<p class="source-code">X, X_test, Y, Y_test = train_test_split(X, Y, \</p><p class="source-code">                                        test_size = 0.10)</p></li>
				<li>Instantiate the <strong class="source-inline">KFold</strong> class with a 10-fold configuration: <p class="source-code">kf = KFold(n_splits = 10)</p><p class="callout-heading">Note</p><p class="callout">Feel free to experiment with the values of <strong class="source-inline">K</strong> to see how the output shapes of this exercise vary.</p></li>
				<li>Apply the <strong class="source-inline">split</strong> method to the data in <strong class="source-inline">X</strong>. This method will output the index of the instances to be used as training and validation sets. This method creates 10 different split configurations. Save the output in a variable named <strong class="source-inline">splits</strong>:<p class="source-code">splits = kf.split(X)</p><p>Note that it is not necessary to run the <strong class="source-inline">split</strong> method on the data in <strong class="source-inline">Y</strong>, as the method only saves the index numbers, which will be the same for <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>. The actual splitting is handled next.</p></li>
				<li>Perform a <strong class="source-inline">for</strong> loop that will go through the different split configurations. In the loop body, create the variables that will hold the data for the training and validation sets. Use the following code snippet to do so:<p class="source-code">for train_index, test_index in splits:</p><p class="source-code">    X_train, X_dev = X.iloc[train_index,:], \</p><p class="source-code">                     X.iloc[test_index,:]</p><p class="source-code">    Y_train, Y_dev = Y.iloc[train_index,:], \</p><p class="source-code">                     Y.iloc[test_index,:]</p><p>The <strong class="source-inline">for</strong> loop goes through <strong class="source-inline">K</strong> number of configurations. In the body of the loop, the data is split using the index numbers:</p><p class="source-code">print(X_train.shape, Y_train.shape, X_dev.shape, \</p><p class="source-code">      Y_dev.shape, X_test.shape, Y_test.shape)</p><p>By printing the shape of all the subsets, as per the preceding snippet, the output is as follows:</p><p class="source-code">(144, 13) (144, 1) (16, 13) (16, 1) (18, 13) (18, 1)</p><p class="callout-heading">Note</p><p class="callout">The code to train and evaluate the model should be written inside the loop body, given that the objective of the cross-validation procedure is to train and validate the model using the different split configurations.</p></li>
			</ol>
			<p>You have successfully performed a cross-validation split on a sample dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2N0lPi0">https://packt.live/2N0lPi0</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Y290tK">https://packt.live/2Y290tK</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In conclusion, cross-validation is a procedure that's used to shuffle and split the data into training and validation sets so that the process of training and validating is done each time on a different set of data, thus achieving a model with low bias.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor092"/>Activity 3.01: Data Partitioning on a Handwritten Digit Dataset</h2>
			<p>Your company specializes in recognizing handwritten characters. It wants to improve the recognition of digits, which is why they have gathered a dataset of 1,797 handwritten digits from 0 to 9. The images have already been converted into their numeric representation, and so they have provided you with the dataset to split it into training/validation/testing sets. You can choose to either perform conventional splitting or cross-validation. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the required elements to split a dataset, as well as the <strong class="source-inline">load_digits</strong> function from scikit-learn to load the <strong class="source-inline">digits</strong> dataset.</li>
				<li>Load the <strong class="source-inline">digits</strong> dataset and create Pandas DataFrames containing the features and target matrices.</li>
				<li>Take the conventional split approach, using a split ratio of 60/20/20%.</li>
				<li>Using the same DataFrames, perform a 10-fold cross-validation split.<p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found on page 228. Feel free to try different parameters to arrive at different results. </p></li>
			</ol>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor093"/>Evaluation Metrics</h1>
			<p>Model evaluation is indispensable for creating effective models that not only perform well on the data that was used to train the model but also on unseen data. The task of evaluating the model is especially easy when dealing with supervised learning problems, where there is a ground truth that can be compared against the prediction of the model.</p>
			<p>Determining the accuracy percentage of the model is crucial for its application to unseen data that does not have a label class to compare to. For example, a model with an accuracy of 98% may allow the user to assume that the odds of having an accurate prediction are high, and hence the model should be trusted.</p>
			<p>The evaluation of performance, as mentioned previously, should be done on the validation set (dev set) to fine-tune the model, and on the test set to determine the expected performance of the selected model on unseen data.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor094"/>Evaluation Metrics for Classification Tasks</h2>
			<p>A classification task refers to a model where the class label is a discrete value, as mentioned previously. Considering this, the most common measure to evaluate the performance of such tasks is calculating the accuracy of the model, which involves comparing the actual prediction to the real value. Even though this may be an appropriate metric in many cases, there are several others to consider as well before choosing one.</p>
			<p>Now, we will take a look at the different performance metrics.</p>
			<h3 id="_idParaDest-93"><a id="_idTextAnchor095"/>Confusion Matrix</h3>
			<p>The <strong class="bold">confusion matrix</strong> is a table that contains the performance of the model, and is described as follows:</p>
			<ul>
				<li>The columns represent the instances that belong to a predicted class.</li>
				<li>The rows refer to the instances that actually belong to that class (ground truth).</li>
			</ul>
			<p>The configuration that confusion matrices present allows the user to quickly spot the areas in which the model is having greater difficulty. Consider the following table:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B15781_03_04.jpg" alt="Figure 3.4: A confusion matrix of a classifier that predicts whether a woman is pregnant&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4: A confusion matrix of a classifier that predicts whether a woman is pregnant</p>
			<p>The following can be observed from the preceding table:</p>
			<ul>
				<li>By summing up the values in the first row, it is possible to know that there are 600 observations of pregnant women. However, from those 600 observations, the model predicted 556 as pregnant, and 44 as non-pregnant. Hence, the model's ability to predict that a woman is pregnant has a correctness level of 92.6%.</li>
				<li>Regarding the second row, there are also 600 observations of non-pregnant women. Out of those 600, the model predicted that 123 of them were pregnant, and 477 were non-pregnant. The model successfully predicted non-pregnant women 79.5% of the time.</li>
			</ul>
			<p>Based on these statements, it is possible to conclude that the model performs at its worst when classifying observations that are not pregnant.</p>
			<p>Considering that the rows in a confusion matrix refer to the occurrence or non-occurrence of an event, and the columns refer to the model's predictions, the values in the confusion matrix can be explained as follows:</p>
			<ul>
				<li><strong class="bold">True positives</strong> (<strong class="bold">TP</strong>): Refers to the instances that the model correctly classified the event as positive—for example, the instances correctly classified as pregnant.</li>
				<li><strong class="bold">False positives</strong> (<strong class="bold">FP</strong>): Refers to the instances that the model incorrectly classified the event as positive—for example, the non-pregnant instances that were incorrectly classified as pregnant.</li>
				<li><strong class="bold">True negatives</strong> (<strong class="bold">TN</strong>): Represents the instances that the model correctly classified the event as negative—for example, the instances correctly classified as non-pregnant.</li>
				<li><strong class="bold">False negatives</strong> (<strong class="bold">FN</strong>): Refers to the instances that the model incorrectly classified the event as negative—for example, the pregnant instances that were incorrectly predicted as non-pregnant.</li>
			</ul>
			<p>The values in the confusion matrix can be demonstrated as follows:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B15781_03_05.jpg" alt="Figure 3.5: A table showing confusion matrix values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5: A table showing confusion matrix values</p>
			<h3 id="_idParaDest-94"><a id="_idTextAnchor096"/>Accuracy</h3>
			<p><strong class="bold">Accuracy</strong>, as explained previously, measures the model's ability to correctly classify all instances. Although this is considered to be one of the simplest ways of measuring performance, it may not always be a useful metric when the objective of the study is to minimize/maximize the occurrence of one class independently of its performance on other classes.</p>
			<p>The accuracy level of the confusion matrix from <em class="italic">Figure 3.4</em> is measured as follows:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B15781_03_06.jpg" alt="Figure 3.6: An equation showing the calculation for accuracy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6: An equation showing the calculation for accuracy</p>
			<p>Here, <em class="italic">m</em> is the total number of instances. </p>
			<p>The <strong class="source-inline">86%</strong> accuracy refers to the overall performance of the model in classifying both class labels.</p>
			<h3 id="_idParaDest-95"><a id="_idTextAnchor097"/>Precision</h3>
			<p>This metric measures the model's ability to correctly classify positive labels (the label that represents the occurrence of the event) by comparing it with the total number of instances predicted as positive. This is represented by the ratio between the <em class="italic">true positives</em> and the sum of the <em class="italic">true positives</em> and <em class="italic">false positives</em>, as shown in the following equation:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B15781_03_07.jpg" alt="Figure 3.7: An equation showing the calculation for precision&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7: An equation showing the calculation for precision</p>
			<p>The precision metric is only applicable to binary classification tasks, where there are only two class labels (for instance, true or false). It can also be applied to multiclass tasks considering that the classes are converted into two (for instance, predicting whether a handwritten number is a 6 or any other number), where one of the classes refers to the instances that have a condition while the other refers to those that do not.</p>
			<p>For the example in <em class="italic">Figure 3.4</em>, the precision of the model is equal to 81.8%. </p>
			<h3 id="_idParaDest-96"><a id="_idTextAnchor098"/>Recall</h3>
			<p>The recall metric measures the number of correctly predicted positive labels against all positive labels. This is represented by the ratio between <em class="italic">true positives</em> and the sum of <em class="italic">true positives</em> and <em class="italic">false negatives</em>:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B15781_03_08.jpg" alt="Figure 3.8: An equation showing the calculation for recall&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8: An equation showing the calculation for recall</p>
			<p>Again, this measure should be applied to two class labels. The value of recall for the example in <em class="italic">Figure 3.4</em> is 92.6%, which, when compared to the other two metrics, represents the highest performance of the model. The decision to choose one metric or the other will depend on the purpose of the study, which will be explained in more detail later.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor099"/>Exercise 3.03: Calculating Different Evaluation Metrics on a Classification Task</h2>
			<p>In this exercise, we will be using the breast cancer toy dataset to calculate the evaluation metrics using the scikit-learn library. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook to implement this exercise and import all the required elements:<p class="source-code">from sklearn.datasets import load_breast_cancer</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn import tree</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">from sklearn.metrics import precision_score</p><p class="source-code">from sklearn.metrics import recall_score</p><p>The fourth line imports the <strong class="source-inline">tree</strong> module from scikit-learn, which will be used to train a decision tree model on the training data in this exercise. The lines of code below that will import the different evaluation metrics that will be calculated during this exercise.</p></li>
				<li>The breast cancer toy dataset contains the final diagnosis (malignant or benign) of the analysis of masses found in the breasts of 569 women. Load the dataset and create features and target Pandas DataFrames, as follows:<p class="source-code">data = load_breast_cancer()</p><p class="source-code">X = pd.DataFrame(data.data)</p><p class="source-code">Y = pd.DataFrame(data.target)</p></li>
				<li>Split the dataset using the conventional split approach:<p class="source-code">X_train, X_test, \</p><p class="source-code">Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, \</p><p class="source-code">                                   random_state = 0)</p><p>Note that the dataset is divided into two subsets (train and test sets) because the purpose of this exercise is to learn how to calculate the evaluation metrics using the scikit-learn package.</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">random_state</strong> parameter is used to set a seed that will ensure the same results every time you run the code. This guarantees that you will get the same results as the ones reflected in this exercise. Different numbers can be used as the seed; however, use the same number as suggested in the exercises and activities of this chapter to get the same results as the ones shown here.</p></li>
				<li>First, instantiate the <strong class="source-inline">DecisionTreeClassifier</strong> class from scikit-learn's <strong class="source-inline">tree</strong> module. Next, train a decision tree on the train set. Finally, use the model to predict the class label on the test set. Use the following code to do this:<p class="source-code">model = tree.DecisionTreeClassifier(random_state = 0)</p><p class="source-code">model = model.fit(X_train, Y_train)</p><p class="source-code">Y_pred = model.predict(X_test)</p><p>First, the model is instantiated using a <strong class="source-inline">random_state</strong> to set a seed. Then, the <strong class="source-inline">fit</strong> method is used to train the model using the data from the train sets (both <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>). Finally, the <strong class="source-inline">predict</strong> method is used to trigger the predictions on the data in the test set (only <strong class="source-inline">X</strong>). The data from <strong class="source-inline">Y_test</strong> will be used to compare the predictions with the ground truth.</p><p class="callout-heading">Note</p><p class="callout">The steps for training a supervised learning model will be explained further in <em class="italic">Chapter 4</em>, <em class="italic">Supervised Learning Algorithms: Predicting Annual Income</em> and <em class="italic">Chapter 5</em>, <em class="italic">Artificial Neural Networks: Predicting Annual Income</em>.</p></li>
				<li>Use scikit-learn to construct a confusion matrix, as follows:<p class="source-code">confusion_matrix(Y_test, Y_pred)</p><p>The result is as follows, where the ground truth is measured against the prediction:</p><p class="source-code">array([[21, 1],</p><p class="source-code">       [6, 29]])</p></li>
				<li>Calculate the accuracy, precision, and recall of the model by comparing <strong class="source-inline">Y_test</strong> and <strong class="source-inline">Y_pred</strong>:<p class="source-code">accuracy = accuracy_score(Y_test, Y_pred)</p><p class="source-code">print("accuracy:", accuracy)</p><p class="source-code">precision = precision_score(Y_test, Y_pred)</p><p class="source-code">print("precision:", precision)</p><p class="source-code">recall = recall_score(Y_test, Y_pred)</p><p class="source-code">print("recall:", recall)</p><p>The results are displayed as follows:</p><p class="source-code">accuracy: 0.8771</p><p class="source-code">precision: 0.9666</p><p class="source-code">recall: 0.8285</p><p>Given that the positive labels are those where the mass is malignant, it can be concluded that the instances that the model predicts as malignant have a high probability (96.6%) of being malignant, but for the instances predicted as benign, the model has a 17.15% (100%–82.85%) probability of being wrong.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Yw0hiu">https://packt.live/2Yw0hiu</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3e4rRtE">https://packt.live/3e4rRtE</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>You have successfully calculated evaluation metrics on a classification task.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor100"/>Choosing an Evaluation Metric</h2>
			<p>There are several metrics that can be used to measure the performance of a model on classification tasks, and selecting the right one is key to building a model that performs exceptionally well for the purpose of the study.</p>
			<p>Previously, the importance of understanding the purpose of the study was mentioned as a useful insight to determine the pre-processing techniques that need to be performed on the dataset. Moreover, the purpose of the study is also useful to determine the ideal metric for measuring the performance of the model.</p>
			<p>Why is the purpose of the study important for selecting the evaluation metric? Because by understanding the main goal of the study, it is possible to decide whether it is important to focus our attention on the overall performance of the model or only on one of the class labels. </p>
			<p>For instance, a model that has been created to recognize when birds are present in a picture does not need to perform well in recognizing which other animals are present in the picture as long as it does not classify them as birds. This means that the model needs to focus on improving the performance of correctly classifying birds only.</p>
			<p>On the other hand, for a model that has been created to recognize handwritten characters, where no one character is more important than another, the ideal metric would be the one that measures the overall accuracy of the model.</p>
			<p>What would happen if more than one metric was selected? It would become difficult to arrive at the best performance of the model, considering that measuring two metrics simultaneously can result in needing different approaches to improve results.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor101"/>Evaluation Metrics for Regression Tasks</h2>
			<p>Considering that regression tasks are those where the final output is continuous, without a fixed number of output labels, the comparison between the ground truth and the prediction is based on the proximity of the values rather than on them having exactly the same values. For instance, when predicting house prices, a model that predicts a value of USD 299,846 for a house valued at USD 300,000 can be considered to be a good model.</p>
			<p>The two metrics most commonly used for evaluating the accuracy of continuous variables are the <strong class="bold">Mean Absolute Error</strong> (<strong class="bold">MAE</strong>) and the <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>), which are explained here:</p>
			<ul>
				<li><strong class="bold">Mean Absolute Error</strong>: This metric measures the average absolute difference between a prediction and the ground truth, without taking into account the direction of the error. The formula to calculate the MAE is as follows:</li>
			</ul>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B15781_03_09.jpg" alt="Figure 3.9: An equation showing the calculation of MAE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9: An equation showing the calculation of MAE</p>
			<p>Here, <em class="italic">m</em> refers to the total number of instances, <em class="italic">y</em> is the ground truth, and <em class="italic">ŷ</em> is the predicted value.</p>
			<ul>
				<li><strong class="bold">Root Mean Squared Error</strong>: This is a quadratic metric that also measures the average magnitude of error between the ground truth and the prediction. As its name suggests, the RMSE is the square root of the average of the squared differences, as shown in the following formula:<div id="_idContainer062" class="IMG---Figure"><img src="image/B15781_03_10.jpg" alt="Figure 3.10: An equation showing the calculation of RMSE&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 3.10: An equation showing the calculation of RMSE</p>
			<p>Both these metrics express the average error, in a range from 0 to infinity, where the lower the value, the better the performance of the model. The main difference between these two metrics is that the MAE assigns the same weight of importance to all errors, while the RMSE squares the error, assigning higher weights to larger errors. </p>
			<p>Considering this, the RMSE metric is especially useful in cases where larger errors should be penalized, meaning that outliers are taken into account in the measurement of performance. For instance, the RMSE metric can be used when a value that is off by 4 is more than twice as bad as being off by 2. The MAE, on the other hand, is used when a value that is off by 4 is just twice as bad as a value off by 2.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor102"/>Exercise 3.04: Calculating Evaluation Metrics on a Regression Task</h2>
			<p>In this exercise, we will be calculating evaluation metrics on a model that was trained using linear regression. We will use the <strong class="source-inline">boston</strong> toy dataset for this purpose. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook to implement this exercise and import all the required elements, as follows:<p class="source-code">from sklearn.datasets import load_boston</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn import linear_model</p><p class="source-code">from sklearn.metrics import mean_absolute_error</p><p class="source-code">from sklearn.metrics import mean_squared_error</p><p class="source-code">import numpy as np</p><p>The fourth line imports the <strong class="source-inline">linear_model</strong> module from scikit-learn, which will be used to train a linear regression model on the training dataset. The lines of code that follow import two performance metrics that will be evaluated in this exercise.</p></li>
				<li>For this exercise, the <strong class="source-inline">boston</strong> toy dataset will be used. This dataset contains data about 506 house prices in Boston. Use the following code to load and split the dataset, the same as we did for the previous exercises:<p class="source-code">data = load_boston()</p><p class="source-code">X = pd.DataFrame(data.data)</p><p class="source-code">Y = pd.DataFrame(data.target)</p><p class="source-code">X_train, X_test, Y_train, Y_test = train_test_split(X,Y, \</p><p class="source-code">                                   test_size = 0.1, random_state = 0)</p></li>
				<li>Train a linear regression model on the train set. Then, use the model to predict the class label on the test set, as follows:<p class="source-code">model = linear_model.LinearRegression()</p><p class="source-code">model = model.fit(X_train, Y_train)</p><p class="source-code">Y_pred = model.predict(X_test)</p><p>As a general explanation, the <strong class="source-inline">LinearRegression</strong> class from scikit-learn's <strong class="source-inline">linear_model</strong> module is instantiated first. Then, the <strong class="source-inline">fit</strong> method is used to train the model using the data from the train sets (both <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>). Finally, the <strong class="source-inline">predict</strong> method is used to trigger the predictions on the data in the test set (only <strong class="source-inline">X</strong>). The data from <strong class="source-inline">Y_test</strong> will be used to compare the predictions to the ground truth.</p></li>
				<li>Calculate both the MAE and RMSE metrics:<p class="source-code">MAE = mean_absolute_error(Y_test, Y_pred)</p><p class="source-code">print("MAE:", MAE)</p><p class="source-code">RMSE = np.sqrt(mean_squared_error(Y_test, Y_pred))</p><p class="source-code">print("RMSE:", RMSE)</p><p>The results are displayed as follows:</p><p class="source-code">MAE: 3.9357</p><p class="source-code">RMSE: 6.4594</p><p class="callout-heading">Note</p><p class="callout">The scikit-learn library allows you to directly calculate the MSE. To calculate the RMSE, the square root of the value obtained from the <strong class="source-inline">mean_squared_error()</strong> function is calculated. By using the square root, we ensure that the values from MAE and RMSE are comparable. </p><p>From the results, it is possible to conclude that the model performs well on the test set, considering that both values are close to zero. Nevertheless, this also means that the performance can still be improved.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YxVXiU">https://packt.live/2YxVXiU</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2N0Elqy">https://packt.live/2N0Elqy</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>You have now successfully calculated evaluation metrics on a regression task that aimed to calculate the prices of houses based on their characteristics. In the next activity, we will calculate the performance of a classification model that was created to recognize handwritten characters.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor103"/>Activity 3.02: Evaluating the Performance of the Model Trained on a Handwritten Dataset</h2>
			<p>You continue to work on creating a model to recognize handwritten digits. The team has built a model and they want you to evaluate the performance of the model. In this activity, you will calculate different performance evaluation metrics on a trained model. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the required elements to load and split a dataset in order to train a model and evaluate the performance of the classification tasks.</li>
				<li>Load the <strong class="source-inline">digits</strong> toy dataset from scikit-learn and create Pandas DataFrames containing the features and target matrices.</li>
				<li>Split the data into training and testing sets. Use 20% as the size of the testing set.</li>
				<li>Train a decision tree on the train set. Then, use the model to predict the class label on the test set.<p class="callout-heading">Note</p><p class="callout">To train the decision tree, revisit <em class="italic">Exercise 3.04</em>, <em class="italic">Calculating Different Evaluation Metrics on a Classification Task</em>.</p></li>
				<li>Use scikit-learn to construct a confusion matrix.</li>
				<li>Calculate the accuracy of the model.</li>
				<li>Calculate the precision and recall. Considering that both the precision and recall can only be calculated on binary classification problems, we'll assume that we are only interested in classifying instances as the number <strong class="source-inline">6</strong> or <strong class="source-inline">any other number</strong>.<p>To be able to calculate the precision and recall, use the following code to convert <strong class="source-inline">Y_test</strong> and <strong class="source-inline">Y_pred</strong> into a one-hot vector. A one-hot vector consists of a vector that only contains zeros and ones. For this activity, the 0 represents the <em class="italic">number 6</em>, while the 1 represents <strong class="source-inline">any other number</strong>. This converts the class labels (<strong class="source-inline">Y_test</strong> and <strong class="source-inline">Y_pred</strong>) into binary data, meaning that there are only two possible outcomes instead of 10 different ones.</p><p>Then, calculate the precision and recall using the new variables:</p><p class="source-code">Y_test_2 = Y_test[:]</p><p class="source-code">Y_test_2[Y_test_2 != 6] = 1</p><p class="source-code">Y_test_2[Y_test_2 == 6] = 0</p><p class="source-code">Y_pred_2 = Y_pred</p><p class="source-code">Y_pred_2[Y_pred_2 != 6] = 1</p><p class="source-code">Y_pred_2[Y_pred_2 == 6] = 0</p><p>You should obtain the following values as the output:</p><p class="source-code">Accuracy = 84.72%</p><p class="source-code">Precision = 98.41%</p><p class="source-code">Recall = 98.10%</p><p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found on page 230.</p></li>
			</ol>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor104"/>Error Analysis</h1>
			<p>Building an average model, as explained so far, is surprisingly easy through the use of the scikit-learn library. The key aspects of building an exceptional model come from the analysis and decision-making on the part of the researcher.</p>
			<p>As we have seen so far, some of the most important tasks are choosing and pre-processing the dataset, determining the purpose of the study, and selecting the appropriate evaluation metric. After handling all of this and taking into account that a model needs to be fine-tuned in order to reach the highest standards, most data scientists recommend training a simple model, regardless of the hyperparameters, to get the study started.</p>
			<p><strong class="bold">Error analysis</strong> is then introduced as a very useful methodology to turn an average model into an exceptional one. As the name suggests, it consists of analyzing the errors among the different subsets of the dataset in order to target the condition that is affecting the model at a greater scale.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor105"/>Bias, Variance, and Data Mismatch</h2>
			<p>To understand the different conditions that may affect a machine learning model, it is important to understand what the <strong class="bold">Bayes error</strong> is. The Bayes error, also known as the <strong class="bold">irreducible error</strong>, is the lowest possible error rate that can be achieved. </p>
			<p>Before the improvements that were made in technology and artificial intelligence, the Bayes error was considered to be the lowest possible error achievable by humans (<strong class="bold">human error</strong>). For instance, for a process that most humans achieve with an error rate of 0.1, but top experts achieve with an error rate of 0.05, the Bayes error would be 0.05.</p>
			<p>However, the Bayes error has now been redefined as being the lowest possible error that machines can achieve, which is unknown considering that, as humans, we can only understand as far as human error goes. Due to this, when using the Bayes error to analyze errors, it is not possible to know the lowest limit once the model is below the human error.</p>
			<p>The following diagram is useful for analyzing the error rates among the different sets of data and determining the condition that is affecting the model in a greater proportion. The purpose of this diagram is to find the errors that differ to a greater extent from each other so that the model can be diagnosed and improved accordingly. It is important to highlight that the value of the error for each set is calculated by subtracting the evaluation metrics (for instance, the accuracy) of that set from 100%:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B15781_03_11.jpg" alt="Figure 3.11: Error analysis methodology&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11: Error analysis methodology</p>
			<p>Considering the preceding diagram, the process to perform error analysis is as follows:</p>
			<ol>
				<li value="1">The performance evaluation is calculated on all sets of data. This measure is used to calculate the error for each set.</li>
				<li>Starting from the bottom to the top, the difference is calculated as follows:<p>The dev set error (12%) is subtracted from the testing set error (12%). The resulting value (0%) is saved.</p><p>The train-dev error (9%) is subtracted from the dev set error (12%). The resulting value (3%) is saved.</p><p>The training set error (8%) is subtracted from the train-dev error (9%). The resulting value (1%) is saved.</p><p>The Bayes error (2%) is subtracted from the training set error (8%). The resulting value (6%) is saved.</p></li>
				<li>The bigger difference determines the condition that is most seriously affecting the model. In this case, the bigger difference occurs between the Bayes error and the training set error, which, as shown in the preceding diagram, determines that the model is suffering from <em class="italic">high bias</em>. <p class="callout-heading">Note</p><p class="callout">The train/dev set is a combination of data in the training and the validation (dev) sets. It is usually of the same shape as the dev set and contains the same amount of data from both sets. </p></li>
			</ol>
			<p>An explanation of each of the conditions is as follows, along with some techniques to avoid/fix them:</p>
			<ul>
				<li><strong class="bold">High Bias</strong>: Also known as underfitting, this occurs when the model is not learning from the training set, which translates into the model performing poorly for all three sets (training, validation, and testing sets), as well as for unseen data.<p>Underfitting is the easiest condition to detect and it usually requires changing to a different algorithm that may be a better fit for the data available. With regard to neural networks, it can typically be fixed by constructing a bigger network or by training for longer periods of time.</p></li>
				<li><strong class="bold">High Variance</strong>: Also known as overfitting, this condition refers to the model's inability to perform well on data that's different than that of the training set. This basically means that the model has overfitted the training data by learning the details and outliers of the data, without making any generalizations. A model suffering from overfitting will not perform well on the dev or test sets, or on unseen data.<p>Overfitting can be fixed by tuning the different hyperparameters of the algorithm, often with the objective of simplifying the algorithm's approximation of the data. For instance, for decision trees, this can be addressed by pruning the tree to delete some of the details that were learned from the training data. In neural networks, on the other hand, this can be addressed by adding regularization techniques that seek to reduce some of the neuron's influence on the overall result. </p><p>Additionally, adding more data to the training set can also help the model avoid high variance, that is, increasing the dataset that's used for training the model.</p></li>
				<li><strong class="bold">Data mismatch</strong>: This occurs when the training and validation sets do not follow the same distribution. This affects the model as although it generalizes based on the training data. This generalization does not describe the data that was found in the validation set. For instance, a model that's created to describe landscape photographs may suffer from a data mismatch if it is trained using high-definition images, while the actual images that will be used once the model has been built are unprofessional.<p>Logically, the best way to avoid data mismatch is to make sure that the sets follow the same distribution. For example, you can do this by shuffling together the images from both sources (professional and unprofessional images) and then dividing them into the different sets. </p><p>Nevertheless, in cases where there is not enough data that follows the same distribution of unseen data (data that will be used in the future), it is highly recommended to create the dev and test sets entirely out of that data and add the remaining data to the large training set. From the preceding example, the unprofessional images should be used to create the dev and test sets, adding the remaining ones to the training set, along with the professional images. This helps to train a model with a set that contains enough images to make a generalization, but it uses data with the same distribution as the unseen data to fine-tune the model.</p><p>Finally, if the data from all sets comes from the same distribution, this condition actually refers to a problem of high variance and should be handled as such.</p></li>
				<li><strong class="bold">Overfitting to the dev set</strong>: Lastly, similar to the variance condition, this occurs when the model is not generalizing but instead is fitting the dev set too well. <p>It should be addressed using the same approaches that were explained for high variance.</p></li>
			</ul>
			<p>In the next exercise, we will calculate the error rate of the model on the different sets of data, which can be used to perform error analysis.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor106"/>Exercise 3.05: Calculating the Error Rate on Different Sets of Data </h2>
			<p>In this exercise, we will calculate error rates for a model that has been trained using a decision tree. We will use the breast cancer dataset for this purpose. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook to implement this exercise and import all the required elements to load and split the dataset. These will be used to train a model and evaluate its recall:<p class="source-code">from sklearn.datasets import load_breast_cancer</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn import tree</p><p class="source-code">from sklearn.metrics import recall_score</p></li>
				<li>For this exercise, the <strong class="source-inline">breast cancer</strong> dataset will be used. Use the following code to load the dataset and create the Pandas DataFrames containing the features and target matrices:<p class="source-code">breast_cancer = load_breast_cancer()</p><p class="source-code">X = pd.DataFrame(breast_cancer.data)</p><p class="source-code">Y = pd.DataFrame(breast_cancer.target)</p></li>
				<li>Split the dataset into training, validation, and testing sets: <p class="source-code">X_new, X_test, Y_new, Y_test = train_test_split(X, Y, \</p><p class="source-code">                               test_size = 0.1, random_state = 101)</p><p class="source-code">test_size = X_test.shape[0] / X_new.shape[0]</p><p class="source-code">X_train, X_dev, Y_train, Y_dev = train_test_split(X_new, Y_new, \</p><p class="source-code">                                 test_size = test_size, \</p><p class="source-code">                                 random_state = 101)</p><p class="source-code">print(X_train.shape, Y_train.shape, X_dev.shape, \</p><p class="source-code">      Y_dev.shape, X_test.shape, Y_test.shape)</p><p>The resulting shapes are as follows:</p><p class="source-code">(455, 30) (455, 1) (57, 30) (57, 1) (57, 30) (57, 1)</p></li>
				<li>Create a train/dev set that combines data from both the training and validation sets:<p class="source-code">np.random.seed(101)</p><p class="source-code">indices_train = np.random.randint(0, len(X_train), 25)</p><p class="source-code">indices_dev = np.random.randint(0, len(X_dev), 25)</p><p class="source-code">X_train_dev = pd.concat([X_train.iloc[indices_train,:], \</p><p class="source-code">                         X_dev.iloc[indices_dev,:]])</p><p class="source-code">Y_train_dev = pd.concat([Y_train.iloc[indices_train,:], \</p><p class="source-code">                         Y_dev.iloc[indices_dev,:]])</p><p class="source-code">print(X_train_dev.shape, Y_train_dev.shape)</p><p>First, a random seed is set to ensure the reproducibility of the results. Next, the NumPy <strong class="source-inline">random.randint()</strong> function is used to select random indices from the <strong class="source-inline">X_train</strong> set. To do that, 28 random integers are generated in a range between 0 and the total length of <strong class="source-inline">X_train</strong>. The same process is used to generate the random indices of the dev set. Finally, a new variable is created to store the selected values of <strong class="source-inline">X_train</strong> and <strong class="source-inline">X_dev</strong>, as well as a variable to store the corresponding values from <strong class="source-inline">Y_train</strong> and <strong class="source-inline">Y_dev</strong>.</p><p>The variables that have been created contain 25 instances/labels from the train set and 25 instances/labels from the dev set.</p><p>The resulting shapes of the sets are as follows:</p><p class="source-code">(50, 30) (50, 1)</p></li>
				<li>Train a decision tree on the train set, as follows:<p class="source-code">model = tree.DecisionTreeClassifier(random_state = 101)</p><p class="source-code">model = model.fit(X_train, Y_train)</p></li>
				<li>Use the <strong class="source-inline">predict</strong> method to generate the predictions for all of your sets (train, train/dev, dev, and test). Next, considering that the objective of the study is to maximize the model's ability to predict all malignant cases, calculate the recall scores for all predictions. Store all of the scores in a variable named <strong class="source-inline">scores</strong>:<p class="source-code">sets = ["Training", "Train/dev", "Validation", "Testing"]</p><p class="source-code">X_sets = [X_train, X_train_dev, X_dev, X_test]</p><p class="source-code">Y_sets = [Y_train, Y_train_dev, Y_dev, Y_test]</p><p class="source-code">scores = {}</p><p class="source-code">for i in range(0, len(X_sets)):</p><p class="source-code">    pred = model.predict(X_sets[i])</p><p class="source-code">    score = recall_score(Y_sets[i], pred)</p><p class="source-code">    scores[sets[i]] = score</p><p class="source-code">print(scores)</p><p>The error rates for all of the sets of data are as follows:</p><p class="source-code">{'Training': 1.0, 'Train/dev': 0.9705882352941176, 'Validation': 0.9333333333333333, 'Testing': 0.9714285714285714}</p><p>From the preceding values, the following table containing the error rates can be created:</p><div id="_idContainer064" class="IMG---Figure"><img src="image/B15781_03_12.jpg" alt="Figure 3.12: Error rates for all sets of data&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.12: Error rates for all sets of data</p>
			<p>Here, the Bayes error was assumed as <strong class="source-inline">0</strong>, considering that the classification between a malignant and a benign mass is done by taking a biopsy of the mass. </p>
			<p>From the preceding table, it can be concluded that the model performs exceptionally well for the purpose of the study, considering that all error rates are close to 0, which is the lowest possible error. </p>
			<p>The highest difference in error rates is found between the train/dev set and the dev set, which refers to data mismatch. However, taking into account that all the datasets come from the same distribution, this condition is considered a high variance issue, where adding more data to the training set should help reduce the error rate.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3e4Toer">https://packt.live/3e4Toer</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2UJzDkW">https://packt.live/2UJzDkW</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>You have successfully calculated the error rate of all the subsets of data. In the next activity, we will perform an error analysis to define the steps to be taken to improve the performance of a model that was created to recognize handwritten digits.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor107"/>Activity 3.03: Performing Error Analysis on a Model Trained to Recognize Handwritten Digits</h2>
			<p>Based on the different metrics that you have provided to your team to measure the performance of the model, they have selected accuracy as the ideal metric. Considering this, your team has asked you to perform an error analysis to determine how the model could be improved. In this activity, you will perform an error analysis by comparing the error rate of the different sets in terms of the accuracy of the model. Follow these steps to achieve this:</p>
			<ol>
				<li value="1">Import the required elements to load and split a dataset. We will do this to train the model and measure its accuracy.</li>
				<li>Load the <strong class="source-inline">digits</strong> toy dataset from scikit-learn and create Pandas DataFrames containing the features and target matrices.</li>
				<li>Split the data into training, validation, and testing sets. Use 0.1 as the size of the test set, and an equivalent number to build a validation set of the same shape.</li>
				<li>Create a train/dev set for both the features and target values that contains 90 instances/labels of the train set and 90 instances/labels of the dev set.</li>
				<li>Train a decision tree on that training set data.</li>
				<li>Calculate the error rate for all sets of data in terms of the accuracy of the model and determine which condition is affecting the performance of the model.</li>
			</ol>
			<p>By completing this activity, you should obtain the following error rates:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B15781_03_13.jpg" alt="Figure 3.13: Expected error rates&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13: Expected error rates</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 233.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor108"/>Summary</h1>
			<p>This chapter explained the different tasks that can be solved through supervised learning algorithms: classification and regression. Although both of these tasks' goal is to approximate a function that maps a set of features to an output, classification tasks have a discrete number of outputs, while regression tasks can have infinite continuous values as outputs.</p>
			<p>When developing machine learning models to solve supervised learning problems, one of the main goals is for the model to be capable of generalizing so that it will be applicable to future unseen data, instead of just learning a set of instances very well but performing poorly on new data. Accordingly, a methodology for validation and testing was explained in this chapter, which involved splitting the data into three sets: a training set, a dev set, and a test set. This approach eliminates the risk of bias. </p>
			<p>After this, we covered how to evaluate the performance of a model for both classification and regression problems. Finally, we covered how to analyze the performance of a model and perform error analysis on each of the sets to detect the condition affecting the model's performance.</p>
			<p>In the next chapter, we will focus on applying different algorithms to a real-life dataset, with the underlying objective of applying the steps we learned about here to choose the best performing algorithm for the case study.</p>
		</div>
	</body></html>
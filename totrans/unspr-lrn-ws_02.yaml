- en: 1\. Introduction to Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: Finding insights and value in data is the ambitious promise that has been seen
    in the rise of machine learning. Within machine learning, there are predictive
    approaches to understanding dense information in deeper ways, as well as approaches
    to predicting outcomes based on changing inputs. In this chapter, we will learn
    what supervised learning and unsupervised learning are, and how they are applied
    to different use cases. Once you have a deeper understanding of where unsupervised
    learning is useful, we will walk through some foundational techniques that provide
    value quickly.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to implement k-means clustering
    algorithms using built-in Python packages and calculate the silhouette score.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever been asked to take a look at some data and came up empty handed?
    Maybe you weren't familiar with the dataset, or maybe you didn't even know where
    to start. This may have been extremely frustrating, and even embarrassing, depending
    on who asked you to take care of the task.
  prefs: []
  type: TYPE_NORMAL
- en: You are not alone, and, interestingly enough, there are many times the data
    itself is simply too confusing to be made sense of. As you try and figure out
    what all those numbers in your spreadsheet mean, you're most likely mimicking
    what many unsupervised algorithms do when they try to find meaning in data. The
    reality is that many unprocessed real-world datasets may not have any useful insights.
    One example to consider is the fact that these days, individuals generate massive
    amounts of granular data on a daily basis â€“ whether it's their actions on a website,
    their purchase history, or what apps they use on their phone. If you were to look
    at this information on the surface, it would be a big, unorganized mess with no
    hope of clarity. Don't fret, however; this book will prepare you for such tall
    tasks so that you'll never be frustrated again when dealing with data exploration
    tasks, no matter how large.
  prefs: []
  type: TYPE_NORMAL
- en: For this book, we have developed some best-in-class content to help you understand
    how unsupervised algorithms work and where to use them. We'll cover some of the
    foundations of finding clusters in your data, how to reduce the size of your data
    so it's easier to understand, and how each of these sides of unsupervised learning
    can be applied in the real world. We hope you will come away from this book with
    a strong real-world understanding of unsupervised learning, the problems that
    it can solve, and those it cannot.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning versus Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Unsupervised learning** is the field of practice that helps find patterns
    in cluttered data and is one of the most exciting areas of development in machine
    learning today. If you have explored machine learning bookwork before, you are
    probably familiar with the common breakout of problems in either supervised or
    unsupervised learning. **Supervised learning** encompasses the problem set of
    having a labeled dataset that can be used to either classify data (for example,
    predicting smokers and non-smokers, if you''re looking at a lung health dataset)
    or finding a pattern in clearly defined data (for example, predicting the sale
    price of a home based on how many bedrooms it has). This model most closely mirrors
    an intuitive human approach to learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you wanted to learn how to not burn your food with a basic
    understanding of cooking, you could build a dataset by putting your food on the
    burner and seeing how long it takes (input) for your food to burn (output). Eventually,
    as you continue to burn your food, you will build a mental model of when burning
    will occur and how to avoid it in the future. Development in supervised learning
    was once fast paced and valuable, but it has simmered down in recent years. Many
    of the obstacles around getting to know your data have already been tackled and
    are listed in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: Differences between unsupervised and supervised learning'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.1: Differences between unsupervised and supervised learning'
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, unsupervised learning encompasses the problem set of having a tremendous
    amount of data that is unlabeled. Labeled data, in this case, would be data that
    has a supplied "target" outcome that you are trying to find the correlation to
    with supplied data. For instance, in the preceding example, you know that your
    "target outcome" is whether your food was burned; this is an example of labeled
    data. Unlabeled data is when you do not know what the "target" outcome is, and
    you have only supplied input data.
  prefs: []
  type: TYPE_NORMAL
- en: Building upon the previous example, imagine you were just dropped on planet
    Earth with zero knowledge of how cooking works. You are given 100 days, a stove,
    and a fridge full of food without any instructions on what to do. Your initial
    exploration of a kitchen could go in infinite directions. On day 10, you may finally
    learn how to open the fridge; on day 30, you may learn that food can go on the
    stove; and after many more days, you may unwittingly make an edible meal. As you
    can see, trying to find meaning in a kitchen devoid of adequate informational
    structure leads to very noisy data that is completely irrelevant to actually preparing
    a meal.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning can be an answer to this problem. Looking back at your
    100 days of data, you can use **clustering** to find patterns of similar attributes
    across days and deduce which foods are similar and may lead to a "good" meal.
    However, unsupervised learning isn't a magical answer. Simply finding clusters
    can be just as likely to help you find pockets of similar, yet ultimately useless,
    data. Expanding on the cooking example, we can illustrate this shortcoming with
    the concept of the "third variable". Just because you have a cluster of really
    great recipes doesn't mean they are infallible. During your research, you may
    have found a unifying factor that all good meals were cooked on a stove. This
    does not mean that every meal cooked on a stove will be good, and you cannot easily
    jump to that conclusion for all future scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: This challenge is what makes unsupervised learning so exciting. How can we find
    smarter techniques to speed up the process of finding clusters of information
    that are beneficial to our end goals? The following sections would help us answer
    this question.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is the overarching process that involves finding groups of similar
    data that exist in your dataset, which can be extremely valuable if you are trying
    to find its underlying meaning. If you were a store owner and you wanted to understand
    which customers are more valuable without a set idea of what valuable is, clustering
    would be a great place to start to find patterns in your data. You may have a
    few high-level ideas of what denotes a valuable customer, but you aren't entirely
    sure in the face of a large mountain of available data. Through clustering, you
    can find commonalities among similar groups in your data. For example, if you
    look more deeply at a cluster of similar people, you may learn that everyone in
    that group visits your website for longer periods of time than others. This can
    show you what the value is and also provide a clean sample size for future supervised
    learning experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following image shows two scatterplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2: Two distinct scatterplots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.2: Two distinct scatterplots'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image separates the two scatterplots into two distinct clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: Scatterplots clearly showing clusters that exist in a provided
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.3: Scatterplots clearly showing clusters that exist in a provided
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1.2* and *Figure 1.3* display randomly generated number pairs (x and
    y coordinates) pulled from two distinct Gaussian distributions centered at different
    locations. Simply by glancing at the first image, it should be obvious where the
    clusters exist in your data; in real life, it will never be this easy. Now that
    you know that the data can be clearly separated into two clusters, you can start
    to understand what differences exist between the two groups.'
  prefs: []
  type: TYPE_NORMAL
- en: Rewinding a bit from where unsupervised learning fits into the larger machine
    learning environment, let's begin by understanding the building blocks of clustering.
    The most basic definition finds clusters simply as groupings of similar data as
    subsets of a larger dataset. As an example, imagine that you had a room with 10
    people in it and each person had a job either in finance or as a scientist. If
    you told all the financial workers to stand together and all the scientists to
    do the same, you would have effectively formed two clusters based on job types.
    Finding clusters can be immensely valuable in identifying items that are more
    similar and, on the other end of the scale, quite different from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Two-Dimensional Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand this, imagine that you were given a simple 1,000-row dataset
    by your employer that had two columns of numerical data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4: Two-dimensional raw data in an array'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.4: Two-dimensional raw data in an array'
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, this dataset provides no real structure or understanding.
  prefs: []
  type: TYPE_NORMAL
- en: A **dimension** in a dataset is another way of simply counting the number of
    features available. In most organized data tables, you can view the number of
    features as the number of columns. So, using the 1,000-row dataset example of
    size (1,000 x 2), you will have 1,000 observations across two dimensions. Please
    note that dimensions of dataset should not be confused with the dimensions of
    an array.
  prefs: []
  type: TYPE_NORMAL
- en: You begin by plotting the first column against the second column to get a better
    idea of what the data structure looks like. There will be plenty of times where
    the cause of differences between groups will prove to be underwhelming; however,
    the cases that have differences that you can take action on are extremely rewarding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1.01: Identifying Clusters in Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are given two-dimensional plots of data that you suspect have clusters of
    similar data. Please look at the two-dimensional graphs provided in the exercise
    and identify the groups of data points to drive the point home that machine learning
    is important. Without using any algorithmic approaches, identify where these clusters
    exist in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This exercise will help you start building your intuition of how we can identify
    clusters using our own eyes and thought processes. As you complete this exercise,
    think of the rationale of why a group of data points should be considered a cluster
    versus a group that should not be considered a cluster. Follow these steps to
    complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Identify the clusters in the following scatterplot:![Figure 1.5: Two-dimensional
    scatterplot'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15923_01_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.5: Two-dimensional scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The clusters are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.6: Clusters in the scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_01_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.6: Clusters in the scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Identify the clusters in the following scatterplot:![Figure 1.7: Two-dimensional
    scatterplot'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15923_01_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.7: Two-dimensional scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The clusters are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.8: Clusters in the scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_01_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.8: Clusters in the scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Identify the clusters in the following scatterplot:![Figure 1.9: Two-dimensional
    scatterplot'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15923_01_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.9: Two-dimensional scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The clusters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10: Clusters in the scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.10: Clusters in the scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: Most of these examples were likely quite easy for you to understand, and that's
    the point. The human brain and eyes are incredible at finding patterns in the
    real world. Within milliseconds of viewing each plot, you could tell what fitted
    together and what didn't. While it is easy for you, a computer does not have the
    ability to see and process plots in the same manner that we do.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not always a bad thing. Look back at the preceding scatterplot.
    Were you able to find the six discrete clusters in the data just by looking at
    the plot? You probably found only three to four clusters in this scatterplot,
    while a computer would be able to see all six. The human brain is magnificent,
    but it also lacks the nuances that come with a strictly logic-based approach.
    Through algorithmic clustering, you will learn how to build a model that works
    even better than a human at these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at the clustering algorithm in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to k-means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, by now, you can see that finding clusters is extremely valuable in
    a machine learning workflow. But, how can you actually find these clusters? One
    of the most basic yet popular approaches is to use a cluster analysis technique
    called **k-means clustering**. The k-means clustering works by searching for k
    clusters in your data and the workflow is actually quite intuitive. We will start
    with the no-math introduction to k-means, followed by an implementation in Python.
    **Cluster membership** refers to where the points go as the algorithm processes
    the data. Consider it like choosing players for a sports team, where all the players
    are in a pool but, for each successive run, the player is assigned to a team (in
    this case, a cluster).
  prefs: []
  type: TYPE_NORMAL
- en: No-Math k-means Walkthrough
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The no-math algorithm for k-means clustering is pretty simple:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll pick "k" centroids, where "k" would be the expected distinct number
    of clusters. The value of k will be chosen by us and determines the type of clustering
    we obtain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will place the "k" centroids at random places among the existing training
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the distance from each centroid to all the points in the training data
    will be calculated. We will go into detail about distance functions shortly, but
    for now, let's just consider it as how far points are from each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, all the training points will be grouped with their nearest centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Isolating the grouped training points along with their respective centroid,
    calculate the mean data point in the group and move the previous centroid to the
    mean location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is to be repeated until convergence or until maximum iteration
    limit has been achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'And that''s it. The following image represents original raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11: Original raw data charted on x and y coordinates'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.11: Original raw data charted on x and y coordinates'
  prefs: []
  type: TYPE_NORMAL
- en: 'Provided with the original data in the preceding image, we can visualize the
    iterative process of k-means by showing the predicted clusters in each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12: Reading from left to right, red points are randomly initialized
    centroids,'
  prefs: []
  type: TYPE_NORMAL
- en: and the closest data points are assigned to groupings of each centroid
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.12: Reading from left to right, red points are randomly initialized
    centroids, and the closest data points are assigned to groupings of each centroid'
  prefs: []
  type: TYPE_NORMAL
- en: K-means Clustering In-Depth Walkthrough
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand k-means at a deeper level, let''s walk through the example that
    was provided in the introduction again with some of the math that supports k-means.
    The most important math that underpins this algorithm is the distance function.
    A distance function is basically any formula that allows you to quantitatively
    understand how far one object is from another, with the most popular one being
    the Euclidean distance formula. This formula works by subtracting the respective
    components of each point and squaring to remove negatives, followed by adding
    the resulting distances and square rooting them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13: Euclidean distance formula'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.13: Euclidean distance formula'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you notice, the preceding formula holds true for data points having only
    two dimensions (the number of co-ordinates). A generic way of representing the
    preceding equation for higher-dimensional points is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14: Euclidean distance formula for higher dimensional points'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.14: Euclidean distance formula for higher dimensional points'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the terms involved in calculation of Euclidean distance between two
    points *p* and *q* in a higher dimensional space. Here, *n* is the number of dimensions
    of the two points. We compute the difference between the respective components
    of points *p* and *q* (*p*i and *q*i are known as the *i*th component of point
    *p* and *q* respectively) and square each of them. This squared value of the difference
    is summed up for all *n* components, and then square root of this sum is obtained.
    This value represents the Euclidean distance between point *p* and *q*. If you
    substitute n = 2 in the preceding equation, it will decompose to the equation
    represented in *Figure 1.13*.
  prefs: []
  type: TYPE_NORMAL
- en: Now coming back again to our discussion on k-means. Centroids are randomly set
    at the beginning as points in your n-dimensional space. Each of these centers
    is fed into the preceding formula as (*a*, *b*), and a point in your space is
    fed in as (*x*, *y*). Distances are calculated between each point and the coordinates
    of every centroid, with the centroid the shortest distance away chosen as the
    point's group.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s pick three random centroids, an arbitrary point, and,
    using the Euclidean distance formula, calculate the distance from each point to
    the centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Random centroids: [ (2,5), (8,3), (4,5) ].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arbitrary point x: (0, 8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance from point to each centroid: [ 3.61, 9.43, 5.00 ].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the arbitrary point x is closest to the first centroid, it will be assigned
    to the first centroid.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative Distance Metric â€“ Manhattan Distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Euclidean distance is the most common distance metric for many machine learning
    applications and is often known colloquially as the distance metric; however,
    it is not the only, or even the best, distance metric for every situation. Another
    popular distance metric that can be used for clustering is **Manhattan distance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Manhattan distance is called as such because it mirrors the concept of traveling
    through a metropolis (such as New York City) that has many square blocks. Euclidean
    distance relies on diagonals due to its basis in Pythagorean theorem, while Manhattan
    distance constrains distance to only right angles. The formula for Manhattan distance
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.15: Manhattan distance formula'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.15: Manhattan distance formula'
  prefs: []
  type: TYPE_NORMAL
- en: Here *p*i and *q*i are the *i*th component of points *p* and *q*, respectively.
    Building upon our examples of Euclidean distance, where we want to find the distance
    between two points, if our two points were (1,2) and (2,3), then the Manhattan
    distance would equal `|1-2| + |2-3| = 1 + 1 = 2`. This functionality scales to
    any number of dimensions. In practice, Manhattan distance may outperform Euclidean
    distance when it comes to high dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper Dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding examples can be clearly visualized when your data is only two-dimensional.
    This is for convenience, to help drive the point home of how k-means works and
    could lead you into a false understanding of how easy clustering is. In many of
    your own applications, your data will likely be orders of magnitude larger to
    the point that it cannot be perceived by visualization (anything beyond three
    dimensions will be unperceivable to humans). In the previous examples, you could
    mentally work out a few two-dimensional lines to separate the data into its own
    groups. At higher dimensions, you will need to be aided by a computer to find
    an n-dimensional hyperplane that adequately separates the dataset. In practice,
    this is where clustering methods such as k-means provide significant value. The
    following image shows the two-dimensional, three-dimensional, and n-dimensional
    plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.16: Two-dimensional, three-dimensional, and n-dimensional plots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.16: Two-dimensional, three-dimensional, and n-dimensional plots'
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will calculate Euclidean distance. We'll build our
    set of tools by using the `NumPy` and `Math` Python packages. `NumPy` is a scientific
    computing package for Python that pre-packages common mathematical functions in
    highly optimized formats.
  prefs: []
  type: TYPE_NORMAL
- en: As the name implies, the `Math` package is a basic library that makes implementing
    foundational math building blocks, such as exponentials and square roots, much
    easier. By using a package such as `NumPy` or `Math`, we help cut down the time
    spent creating custom math functions from scratch and instead focus on developing
    our solutions. You will see how each of these packages is used in practice in
    the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1.02: Calculating Euclidean Distance in Python'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will create an example point along with three sample centroids
    to help illustrate how Euclidean distance works. Understanding this distance formula
    is the basis for the rest of our work in clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and create a naÃ¯ve formula that captures the direct
    math of Euclidean distance, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This approach is considered naÃ¯ve because it performs element-wise calculations
    on your data points (slow) compared to a more real-world implementation using
    vectors and matrix math to achieve significant performance increases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the data points in Python as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the formula you created to calculate the Euclidean distance in *Step 1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The shortest distance between our point, `x`, and the centroids is `3.61`, which
    is equivalent to the distance between `(0, 8)` and `(2, 5)`. Since this is the
    minimum distance, our example point, `x`, will be assigned as a member of the
    first centroid's group.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this example, our formula was used on a single point, x (0, 8). Beyond this
    single point, the same process will be repeated for every remaining point in your
    dataset until each point is assigned to a cluster. After each point is assigned,
    the mean point is calculated among all of the points within each cluster. The
    calculation of the mean among these points is the same as calculating the mean
    between single integers.
  prefs: []
  type: TYPE_NORMAL
- en: While there was only one point in this example, by completing this process,
    you have effectively assigned a point to its first cluster using Euclidean distance.
    We'll build upon this approach with more than one point in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2VUvCuz](https://packt.live/2VUvCuz).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3ebDwpZ](https://packt.live/3ebDwpZ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1.03: Forming Clusters with the Notion of Distance'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is very intuitive for our human minds to see groups of dots on a plot and
    determine which dots belong to discrete clusters. However, how do we ask a computer
    to repeat this same task? In this exercise, you''ll help teach a computer an approach
    to forming clusters of its own with the notion of distance. We will build upon
    how we use these distance metrics in the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a list of points, [ (0,8), (3,8), (3,4) ], that are assigned to cluster
    one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To find the new centroid among your list of points, calculate the mean point
    between all of the points. Calculation of the mean scales to infinite points,
    as you simply add the integers at each position and divide by the total number
    of points. For example, if your two points are (0,1,2) and (3,4,5), the mean calculation
    would be [ (0+3)/2, (1+4)/2, (2+5)/2 ]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After a new centroid is calculated, repeat the cluster membership calculation
    we looked at in *Exercise 1.02*, *Calculating Euclidean Distance in Python*, and
    then repeat the previous two steps to find the new cluster centroid. Eventually,
    the new cluster centroid will be the same as the centroid before the cluster membership
    calculation and the exercise will be complete. How many times this repeats depends
    on the data you are clustering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you have moved the centroid location to the new mean point of (2, 6.67),
    you can compare it to the initial list of centroids you entered the problem with.
    If the new mean point is different than the centroid that is currently in your
    list, you will have to go through another iteration of the preceding two exercises.
    Once the new mean point you calculate is the same as the centroid you started
    the problem with, you have completed a run of k-means and reached a point called
    **convergence**. However, in practice, sometimes the number of iterations required
    to reach convergence is very large and such large computations may not be practically
    feasible. In such cases, we need to set a maximum limit to the number of iterations.
    Once this iteration limit is reached, we stop furtherÂ processing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3iJ3JiT](https://packt.live/3iJ3JiT).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38CCpOG](https://packt.live/38CCpOG).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next exercise, we will implement k-means from scratch. To do this, we
    will start employing common packages from the Python ecosystem that will serve
    as building blocks for the rest of your career. One of the most popular machine
    learning libraries is called scikit-learn ([https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)),
    which has many built-in algorithms and functions to support your understanding
    of how the algorithms work. We will also be using functions from SciPy ([https://docs.scipy.org/doc/scipy/reference/](https://docs.scipy.org/doc/scipy/reference/)),
    which is a package much like NumPy and abstracts away basic scientific math functions
    that allow for more efficient deployment. Finally, the next exercise will introduce
    `matplotlib` ([https://matplotlib.org/3.1.1/contents.html](https://matplotlib.org/3.1.1/contents.html)),
    which is a plotting library that creates graphical representations of the data
    you are working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1.04: K-means from Scratch â€“ Part 1: Data Generation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next two exercises focus on the creation of exercise data and the implementation
    of k-means from scratch on your training data. This exercise relies on scikit-learn,
    an open source Python package that enables the fast prototyping of popular machine
    learning models. Within scikit-learn, we will be using the `datasets` functionality
    to create a synthetic blob dataset. In addition to harnessing the power of scikit-learn,
    we will also rely on Matplotlib, a popular plotting library for Python that makes
    it easy for us to visualize our data. To do this, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can find more details on the `KMeans` library at [https://scikit-learn.org/stable/modules/clustering.html#k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Generate a random cluster dataset to experiment on X = coordinate points, y
    = cluster labels, and define random centroids. We will achieve this with the `make_blobs`
    function that we imported from `sklearn.datasets`, which, as the name implies,
    generates blobs of data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here the `n_samples` parameter determines the total number of data points generated
    by the blobs. The `centers` parameter determines the number of centroids for the
    blob. The `n_feature` attribute defines the number of dimensions generated by
    the dataset. Here, the data will be two dimensional.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In order to generate the same data points in all the iterations (which in turn
    are generated randomly) for reproducibility of results, we set the `random_state`
    parameter to `800`. Different values of the `random_state` parameter would yield
    different results. If we do not set the `random_state` parameter, each time on
    execution we will obtain different results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the coordinate points using the scatterplot functionality we imported
    from `matplotlib.pyplot`. This function takes input lists of points and presents
    them graphically for ease of understanding. Please review the `matplotlib` documentation
    if you want to explore the parameters provided at a deeper level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot appears as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.17: Plot of the coordinates'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_01_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.17: Plot of the coordinates'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Print the array of `y`, which is the labels provided by scikit-learn and serves
    as the ground truth for comparison.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the coordinate points with the correct cluster labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot appears as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.18: Plot of the coordinates with correct cluster labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_01_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.18: Plot of the coordinates with correct cluster labels'
  prefs: []
  type: TYPE_NORMAL
- en: By completing the preceding steps, you have generated the data and visually
    explored how it is put together. By visualizing the ground truth, you have established
    a baseline that provides a relative metric for algorithm accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2JM8Q1S](https://packt.live/2JM8Q1S).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3ecjKdT](https://packt.live/3ecjKdT).
  prefs: []
  type: TYPE_NORMAL
- en: With data in hand, in the next exercise, we'll continue by building your unsupervised
    learning toolset with an optimized version of the Euclidean distance function
    from the `SciPy` package, `cdist`. You will compare a non-vectorized, clearly
    understandable version of the approach with `cdist`, which has been specially
    tweaked for maximum performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1.05: K-means from Scratch â€“ Part 2: Implementing k-means'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's recreate these results on our own. We will go over an example implementing
    this with some optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This exercise is a continuation of the previous exercise and should be performed
    in the same Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we will rely on SciPy, a Python package that allows easy
    access to highly optimized versions of scientific calculations. In particular,
    we will be implementing Euclidean distance with `cdist`, the functionally of which
    replicates the barebones implementation of our distance metric in a much more
    efficient manner. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basis of this exercise will be comparing a basic implementation of Euclidean
    distance with an optimized version provided in SciPy. First, import the optimized
    Euclidean distance reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Identify a subset of `X` you want to explore. For this example, we are only
    selecting five points to make the lesson clearer; however, this approach scales
    to any number of points. We chose points 105-109, inclusive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the distances and choose the index of the shortest distance as a
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `k_means` function as follows and initialize the k-centroids randomly.
    Repeat this process until the difference between the new/old `centroids` equals
    `0`, using the `while` loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Do not break this code, as it might lead to an error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Zip together the historical steps of centers and their labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plots may differ from what you can see if we haven''t set the
    random seed. The first plot looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.19: First scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_01_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.19: First scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second plot appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.20: Second scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.20: Second scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The third plot appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21: Third scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.21: Third scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fourth plot appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.22: Fourth scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.22: Fourth scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fifth plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.23: Fifth scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.23: Fifth scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: As shown by the preceding images, k-means takes an iterative approach to refine
    optimal clusters based on distance. The algorithm starts with random initialization
    of centroids and, depending on the complexity of the data, quickly finds the separations
    that make the most sense.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2JM8Q1S](https://packt.live/2JM8Q1S).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3ecjKdT](https://packt.live/3ecjKdT).
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Performance â€“ Silhouette Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the performance of unsupervised learning methods is inherently
    much more difficult than supervised learning methods because there is no ground
    truth available. For supervised learning, there are many robust performance metricsâ€”the
    most straightforward of these being accuracy in the form of comparing model-predicted
    labels to actual labels and seeing how many the model got correct. Unfortunately,
    for clustering, we do not have labels to rely on and need to build an understanding
    of how "different" our clusters are. We achieve this with the silhouette score
    metric. We can also use silhouette scores to find the optimal "K" numbers of clusters
    for our unsupervised learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette metric works by analyzing how well a point fits within its cluster.
    The metric ranges from -1 to 1\. If the average silhouette score across your clustering
    is one, then you will have achieved perfect clusters and there will be minimal
    confusion about which point belongs where. For the plots in the previous exercise,
    the silhouette score will be much closer to one since the blobs are tightly condensed
    and there is a fair amount of distance between each blob. This is very rare, though;
    the silhouette score should be treated as an attempt at doing the best you can,
    since hitting one is highly unlikely. If the silhouette score is positive, it
    means that a point is closer to the assigned cluster than it is to the neighboring
    clusters. If the silhouette score is 0, then a point lies on the boundary between
    the assigned cluster and the next closest cluster. If the silhouette score is
    negative, then it indicates that a given point is assigned to an incorrect cluster,
    and the given point in fact likely belongs to a neighboring cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the silhouette score calculation is quite straightforward and
    is obtained using the **Simplified Silhouette Index** (**SSI**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here *a*i is the distance from point *i* to its own cluster centroid, and bi
    is the distance from point *i* to the nearest cluster centroid.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition captured here is that ai represents how cohesive the cluster of
    point *i*' is as a clear cluster, and bi represents how far apart the clusters
    lie. We will use the optimized implementation of `silhouette_score` in scikit-learn
    in *Activity 1.01*, *Implementing k-means Clustering*. Using it is simple and
    only requires that you pass in the feature array and the predicted cluster labels
    from your k-means clustering method.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will use the `pandas` library ([https://pandas.pydata.org/pandas-docs/stable/](https://pandas.pydata.org/pandas-docs/stable/))
    to read a CSV file. Pandas is a Python library that makes data wrangling easier
    through the use of DataFrames. If you look back at the arrays you built with NumPy,
    you probably noticed that the resulting data structures are quite unwieldly. To
    extract subsets from the data, you had to index using brackets and specific numbers
    of rows. Instead of this approach, pandas allows an easier-to-understand approach
    to moving data around and getting it into the format necessary for unsupervised
    learning and other machine learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To read data in Python, you will use `variable_name = pd.read_csv('file_name.csv',
    header=None)`
  prefs: []
  type: TYPE_NORMAL
- en: Here, the parameter `header = None` explicitly mentions that there is no presence
    of column names. If your file contains column names, then retain those default
    values. Also, if you specify `header = None` for a file which contains column
    names, Pandas will treat the row containing names of column as the row containing
    data only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1.06: Calculating the Silhouette Score'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will calculate the silhouette score of a dataset with
    a fixed number of clusters. For this, we will use the seeds dataset, which is
    available at [https://packt.live/2UQA79z](https://packt.live/2UQA79z). The following
    note outlines more information regarding this dataset, in addition to further
    exploration in the next activity. For the purpose of this exercise, please disregard
    the specific details of what this dataset is comprised of as it is of greater
    importance to learn about the silhouette score. As we go into the next activity,
    you will gain more context as needed to create a smart machine learning system.
    Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    It can be accessed at [https://packt.live/2UQA79z](https://packt.live/2UQA79z)
  prefs: []
  type: TYPE_NORMAL
- en: 'Citation: Contributors gratefully acknowledge support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the seeds data file using pandas, a package that makes data wrangling
    much easier through the use of DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate the `X` features, since we want to treat this as an unsupervised learning
    problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Bring back the `k_means` function we made earlier for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert our seeds `X` feature DataFrame into a `NumPy` matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run our `k_means` function on the seeds matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the silhouette score for the `Area (''A'')` and `Length of Kernel
    (''LK'')` columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this exercise, we calculated the silhouette score for the `Area ('A')` and
    `Length of Kernel ('LK')` columns of the seeds dataset. We will use this technique
    in the next activity to determine the performance of our k-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2UOqW9H](https://packt.live/2UOqW9H).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3fbtJ4y](https://packt.live/3fbtJ4y).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1.01: Implementing k-means Clustering'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are implementing a k-means clustering algorithm from scratch to prove that
    you understand how it works. You will be using the seeds dataset provided by the
    UCI ML repository. The seeds dataset is a classic in the data science world and
    contains features of wheat kernels that are used to predict three different types
    of wheat species. The download location can be found later in this activity.
  prefs: []
  type: TYPE_NORMAL
- en: For this activity, you should use Matplotlib, NumPy, scikit-learn metrics, and
    pandas.
  prefs: []
  type: TYPE_NORMAL
- en: By loading and reshaping data easily, you can focus more on learning k-means
    instead of writing data loader functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following seeds data features are provided for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The aim here is to truly understand how k-means works. To do so, you need to
    take what you have learned in the previous sections and implement k-means from
    scratch in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please open your favorite editing platform and try the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `NumPy` or the `math` package and the Euclidean distance formula, write
    a function that calculates the distance between two coordinates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a function that calculates the distance from the centroids to each of
    the points in your dataset and returns the cluster membership.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write a k-means function that takes in a dataset and the number of clusters
    (K) and returns the final cluster centroids, as well as the data points that make
    up that cluster''s membership. After implementing k-means from scratch, apply
    your custom algorithm to the seeds dataset, which is located here: [https://packt.live/2Xh2FdS](https://packt.live/2Xh2FdS).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    It can be accessed at [https://packt.live/2Xh2FdS](https://packt.live/2Xh2FdS).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Citation: Contributors gratefully acknowledge support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Remove the classes supplied in this dataset and see whether your k-means algorithm
    can group the different wheat species into their proper groups just based on plant
    characteristics!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the silhouette score using the scikit-learn implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In completing this exercise, you have gained hands-on experience of tuning
    a k-means clustering algorithm for a real-world dataset. The seeds dataset is
    seen as a classic "hello world"-type problem in the data science space and is
    helpful for testing foundational techniques. Your final clustering algorithm should
    do a decent job of finding the three clusters of wheat species types that exist
    in the data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.24: Expected plot of three clusters of wheat species'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_01_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.24: Expected plot of three clusters of wheat species'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 418.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored what clustering is and why it is important
    in a variety of data challenges. Building upon this foundation of clustering knowledge,
    you implemented k-means, which is one of the simplest, yet most popular, methods
    of unsupervised learning. If you have reached this summary and can repeat what
    k-means does step by step to a friend, then you're ready to move on to more complex
    forms of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: From here, we will be moving on to hierarchical clustering, which, in one configuration,
    reuses the centroid learning approach that we used in k-means. We will build upon
    this approach by outlining additional clustering methodologies and approaches
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL

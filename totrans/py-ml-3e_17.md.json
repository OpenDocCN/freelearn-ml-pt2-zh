["```py\n! pip install -q tensorflow-gpu==2.0.0 \n```", "```py\n>>> import tensorflow as tf\n>>> print(tf.__version__)\n'2.0.0'\n>>> print(\"GPU Available:\", tf.test.is_gpu_available())\nGPU Available: True\n>>> if tf.test.is_gpu_available():\n...     device_name = tf.test.gpu_device_name()\n... else:\n...     device_name = '/CPU:0'\n>>> print(device_name)\n'/device:GPU:0' \n```", "```py\n>>> from google.colab import drive\n>>> drive.mount('/content/drive/') \n```", "```py\n>>> import tensorflow as tf\n>>> import tensorflow_datasets as tfds\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> ## define a function for the generator:\n>>> def make_generator_network(\n...         num_hidden_layers=1,\n...         num_hidden_units=100,\n...         num_output_units=784):\n...\n...     model = tf.keras.Sequential()\n...     for i in range(num_hidden_layers):\n...         model.add(\n...             tf.keras.layers.Dense(\n...                 units=num_hidden_units, use_bias=False))\n...         model.add(tf.keras.layers.LeakyReLU())\n...         \n...     model.add(\n...         tf.keras.layers.Dense(\n...             units=num_output_units, activation='tanh'))\n...     return model\n>>> ## define a function for the discriminator:\n>>> def make_discriminator_network(\n...         num_hidden_layers=1,\n...         num_hidden_units=100,\n...         num_output_units=1):\n...\n...     model = tf.keras.Sequential()\n...     for i in range(num_hidden_layers):\n...         model.add(\n...             tf.keras.layers.Dense(units=num_hidden_units))\n...         model.add(tf.keras.layers.LeakyReLU())\n...         model.add(tf.keras.layers.Dropout(rate=0.5))\n...         \n...     model.add(\n...         tf.keras.layers.Dense(\n...             units=num_output_units, activation=None))\n...     return model \n```", "```py\n>>> image_size = (28, 28)\n>>> z_size = 20\n>>> mode_z = 'uniform' # 'uniform' vs. 'normal'\n>>> gen_hidden_layers = 1\n>>> gen_hidden_size = 100\n>>> disc_hidden_layers = 1\n>>> disc_hidden_size = 100\n>>> tf.random.set_seed(1)\n>>> gen_model = make_generator_network(\n...     num_hidden_layers=gen_hidden_layers,\n...     num_hidden_units=gen_hidden_size,\n...     num_output_units=np.prod(image_size))\n>>> gen_model.build(input_shape=(None, z_size))\n>>> gen_model.summary()\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                multiple                  2000      \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      multiple                  0         \n_________________________________________________________________\ndense_1 (Dense)              multiple                  79184     \n=================================================================\nTotal params: 81,184\nTrainable params: 81,184\nNon-trainable params: 0\n_________________________________________________________________\n>>> disc_model = make_discriminator_network(\n...     num_hidden_layers=disc_hidden_layers,\n...     num_hidden_units=disc_hidden_size)\n>>> disc_model.build(input_shape=(None, np.prod(image_size)))\n>>> disc_model.summary()\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              multiple                  78500     \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    multiple                  0         \n_________________________________________________________________\ndropout (Dropout)            multiple                  0         \n_________________________________________________________________\ndense_3 (Dense)              multiple                  101       \n=================================================================\nTotal params: 78,601\nTrainable params: 78,601\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```py\n>>> mnist_bldr = tfds.builder('mnist')\n>>> mnist_bldr.download_and_prepare()\n>>> mnist = mnist_bldr.as_dataset(shuffle_files=False)\n>>> def preprocess(ex, mode='uniform'):\n...     image = ex['image']\n...     image = tf.image.convert_image_dtype(image, tf.float32)\n...     image = tf.reshape(image, [-1])\n...     image = image*2 - 1.0\n...     if mode == 'uniform':\n...         input_z = tf.random.uniform(\n...             shape=(z_size,), minval=-1.0, maxval=1.0)\n...     elif mode == 'normal':\n...         input_z = tf.random.normal(shape=(z_size,))\n...     return input_z, image\n>>> mnist_trainset = mnist['train']\n>>> mnist_trainset = mnist_trainset.map(preprocess) \n```", "```py\n>>> mnist_trainset = mnist_trainset.batch(32, drop_remainder=True)\n>>> input_z, input_real = next(iter(mnist_trainset))\n>>> print('input-z -- shape:   ', input_z.shape)\n>>> print('input-real -- shape:', input_real.shape)\ninput-z -- shape:    (32, 20)\ninput-real -- shape: (32, 784)\n>>> g_output = gen_model(input_z)\n>>> print('Output of G -- shape:', g_output.shape)\nOutput of G -- shape: (32, 784)\n>>> d_logits_real = disc_model(input_real)\n>>> d_logits_fake = disc_model(g_output)\n>>> print('Disc. (real) -- shape:', d_logits_real.shape)\n>>> print('Disc. (fake) -- shape:', d_logits_fake.shape)\nDisc. (real) -- shape: (32, 1)\nDisc. (fake) -- shape: (32, 1) \n```", "```py\n>>> loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n>>> ## Loss for the Generator\n>>> g_labels_real = tf.ones_like(d_logits_fake)\n>>> g_loss = loss_fn(y_true=g_labels_real, y_pred=d_logits_fake)\n>>> print('Generator Loss: {:.4f}'.format(g_loss))\nGenerator Loss: 0.7505\n>>> ## Loss for the Discriminator\n>>> d_labels_real = tf.ones_like(d_logits_real)\n>>> d_labels_fake = tf.zeros_like(d_logits_fake)\n>>> d_loss_real = loss_fn(y_true=d_labels_real,\n...                       y_pred=d_logits_real)\n>>> d_loss_fake = loss_fn(y_true=d_labels_fake,\n...                       y_pred=d_logits_fake)\n>>> print('Discriminator Losses: Real {:.4f} Fake {:.4f}'\n...       .format(d_loss_real.numpy(), d_loss_fake.numpy()))\nDiscriminator Losses: Real 1.3683 Fake 0.6434 \n```", "```py\n>>> import time\n>>> num_epochs = 100\n>>> batch_size = 64\n>>> image_size = (28, 28)\n>>> z_size = 20\n>>> mode_z = 'uniform'\n>>> gen_hidden_layers = 1\n>>> gen_hidden_size = 100\n>>> disc_hidden_layers = 1\n>>> disc_hidden_size = 100\n>>> tf.random.set_seed(1)\n>>> np.random.seed(1)\n>>> if mode_z == 'uniform':\n...     fixed_z = tf.random.uniform(\n...         shape=(batch_size, z_size),\n...         minval=-1, maxval=1)\n>>> elif mode_z == 'normal':\n...     fixed_z = tf.random.normal(\n...         shape=(batch_size, z_size))\n>>> def create_samples(g_model, input_z):\n...     g_output = g_model(input_z, training=False)\n...     images = tf.reshape(g_output, (batch_size, *image_size))\n...     return (images+1)/2.0\n>>> ## Set-up the dataset\n>>> mnist_trainset = mnist['train']\n>>> mnist_trainset = mnist_trainset.map(\n...     lambda ex: preprocess(ex, mode=mode_z))\n>>> mnist_trainset = mnist_trainset.shuffle(10000)\n>>> mnist_trainset = mnist_trainset.batch(\n...     batch_size, drop_remainder=True)\n>>> ## Set-up the model\n>>> with tf.device(device_name):\n...     gen_model = make_generator_network(\n...         num_hidden_layers=gen_hidden_layers,\n...         num_hidden_units=gen_hidden_size,\n...         num_output_units=np.prod(image_size))\n...     gen_model.build(input_shape=(None, z_size))\n...\n...     disc_model = make_discriminator_network(\n...         num_hidden_layers=disc_hidden_layers,\n...         num_hidden_units=disc_hidden_size)\n...     disc_model.build(input_shape=(None, np.prod(image_size)))\n>>> ## Loss function and optimizers:\n>>> loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n>>> g_optimizer = tf.keras.optimizers.Adam()\n>>> d_optimizer = tf.keras.optimizers.Adam()\n>>> all_losses = []\n>>> all_d_vals = []\n>>> epoch_samples = []\n>>> start_time = time.time()\n>>> for epoch in range(1, num_epochs+1):\n...\n...     epoch_losses, epoch_d_vals = [], []\n...\n...     for i,(input_z,input_real) in enumerate(mnist_trainset):\n...         \n...         ## Compute generator's loss\n...         with tf.GradientTape() as g_tape:\n...             g_output = gen_model(input_z)\n...             d_logits_fake = disc_model(g_output,\n...                                        training=True)\n...             labels_real = tf.ones_like(d_logits_fake)\n...             g_loss = loss_fn(y_true=labels_real,\n...                              y_pred=d_logits_fake)\n...             \n...         ## Compute the gradients of g_loss\n...         g_grads = g_tape.gradient(g_loss,\n...                       gen_model.trainable_variables)\n...\n...         ## Optimization: Apply the gradients\n...         g_optimizer.apply_gradients(\n...             grads_and_vars=zip(g_grads,\n...             gen_model.trainable_variables))\n...\n...         ## Compute discriminator's loss\n...         with tf.GradientTape() as d_tape:\n...             d_logits_real = disc_model(input_real,\n...                                        training=True)\n...\n...             d_labels_real = tf.ones_like(d_logits_real)\n...             \n...             d_loss_real = loss_fn(\n...                 y_true=d_labels_real, y_pred=d_logits_real)\n...\n...             d_logits_fake = disc_model(g_output,\n...                                        training=True)\n...             d_labels_fake = tf.zeros_like(d_logits_fake)\n...\n...             d_loss_fake = loss_fn(\n...                 y_true=d_labels_fake, y_pred=d_logits_fake)\n...\n...             d_loss = d_loss_real + d_loss_fake\n...\n...         ## Compute the gradients of d_loss\n...         d_grads = d_tape.gradient(d_loss,\n...                       disc_model.trainable_variables)\n...         \n...         ## Optimization: Apply the gradients\n...         d_optimizer.apply_gradients(\n...             grads_and_vars=zip(d_grads,\n...             disc_model.trainable_variables))\n...                            \n...         epoch_losses.append(\n...             (g_loss.numpy(), d_loss.numpy(),\n...              d_loss_real.numpy(), d_loss_fake.numpy()))\n...         \n...         d_probs_real = tf.reduce_mean(\n...                            tf.sigmoid(d_logits_real))\n...         d_probs_fake = tf.reduce_mean(\n...                            tf.sigmoid(d_logits_fake))\n...         epoch_d_vals.append((d_probs_real.numpy(),\n...                              d_probs_fake.numpy()))\n...       \n...     all_losses.append(epoch_losses)\n...     all_d_vals.append(epoch_d_vals)\n...     print(\n...         'Epoch {:03d} | ET {:.2f} min | Avg Losses >>'\n...         ' G/D {:.4f}/{:.4f} [D-Real: {:.4f} D-Fake: {:.4f}]'\n...         .format(\n...             epoch, (time.time() - start_time)/60,\n...             *list(np.mean(all_losses[-1], axis=0))))\n...     epoch_samples.append(\n...         create_samples(gen_model, fixed_z).numpy())\nEpoch 001 | ET 0.88 min | Avg Losses >> G/D 2.9594/0.2843 [D-Real: 0.0306 D-Fake: 0.2537]\nEpoch 002 | ET 1.77 min | Avg Losses >> G/D 5.2096/0.3193 [D-Real: 0.1002 D-Fake: 0.2191]\nEpoch ...\nEpoch 100 | ET 88.25 min | Avg Losses >> G/D 0.8909/1.3262 [D-Real: 0.6655 D-Fake: 0.6607] \n```", "```py\n>>> import itertools\n>>> fig = plt.figure(figsize=(16, 6))\n>>> ## Plotting the losses\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> g_losses = [item[0] for item in itertools.chain(*all_losses)]\n>>> d_losses = [item[1]/2.0 for item in itertools.chain(\n...             *all_losses)]\n>>> plt.plot(g_losses, label='Generator loss', alpha=0.95)\n>>> plt.plot(d_losses, label='Discriminator loss', alpha=0.95)\n>>> plt.legend(fontsize=20)\n>>> ax.set_xlabel('Iteration', size=15)\n>>> ax.set_ylabel('Loss', size=15)\n>>> epochs = np.arange(1, 101)\n>>> epoch2iter = lambda e: e*len(all_losses[-1])\n>>> epoch_ticks = [1, 20, 40, 60, 80, 100]\n>>> newpos = [epoch2iter(e) for e in epoch_ticks]\n>>> ax2 = ax.twiny()\n>>> ax2.set_xticks(newpos)\n>>> ax2.set_xticklabels(epoch_ticks)\n>>> ax2.xaxis.set_ticks_position('bottom')\n>>> ax2.xaxis.set_label_position('bottom')\n>>> ax2.spines['bottom'].set_position(('outward', 60))\n>>> ax2.set_xlabel('Epoch', size=15)\n>>> ax2.set_xlim(ax.get_xlim())\n>>> ax.tick_params(axis='both', which='major', labelsize=15)\n>>> ax2.tick_params(axis='both', which='major', labelsize=15)\n>>> ## Plotting the outputs of the discriminator\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> d_vals_real = [item[0] for item in itertools.chain(\n...                *all_d_vals)]\n>>> d_vals_fake = [item[1] for item in itertools.chain(\n...                *all_d_vals)]\n>>> plt.plot(d_vals_real, alpha=0.75,\n...          label=r'Real: $D(\\mathbf{x})$')\n>>> plt.plot(d_vals_fake, alpha=0.75,\n...          label=r'Fake: $D(G(\\mathbf{z}))$')\n>>> plt.legend(fontsize=20)\n>>> ax.set_xlabel('Iteration', size=15)\n>>> ax.set_ylabel('Discriminator output', size=15)\n>>> ax2 = ax.twiny()\n>>> ax2.set_xticks(newpos)\n>>> ax2.set_xticklabels(epoch_ticks)\n>>> ax2.xaxis.set_ticks_position('bottom')\n>>> ax2.xaxis.set_label_position('bottom')\n>>> ax2.spines['bottom'].set_position(('outward', 60))\n>>> ax2.set_xlabel('Epoch', size=15)\n>>> ax2.set_xlim(ax.get_xlim())\n>>> ax.tick_params(axis='both', which='major', labelsize=15)\n>>> ax2.tick_params(axis='both', which='major', labelsize=15)\n>>> plt.show() \n```", "```py\n>>> selected_epochs = [1, 2, 4, 10, 50, 100]\n>>> fig = plt.figure(figsize=(10, 14))\n>>> for i,e in enumerate(selected_epochs):\n...     for j in range(5):\n...         ax = fig.add_subplot(6, 5, i*5+j+1)\n...         ax.set_xticks([])\n...         ax.set_yticks([])\n...         if j == 0:\n...             ax.text(\n...                 -0.06, 0.5, 'Epoch {}'.format(e),\n...                 rotation=90, size=18, color='red',\n...                 horizontalalignment='right',\n...                 verticalalignment='center',\n...                 transform=ax.transAxes)\n...         \n...         image = epoch_samples[e-1][j]\n...         ax.imshow(image, cmap='gray_r')\n...     \n>>> plt.show() \n```", "```py\n>>> def make_dcgan_generator(\n...         z_size=20,\n...         output_size=(28, 28, 1),\n...         n_filters=128,\n...         n_blocks=2):\n...     size_factor = 2**n_blocks\n...     hidden_size = (\n...         output_size[0]//size_factor,\n...         output_size[1]//size_factor)\n...     \n...     model = tf.keras.Sequential([\n...         tf.keras.layers.Input(shape=(z_size,)),\n...         \n...         tf.keras.layers.Dense(\n...             units=n_filters*np.prod(hidden_size),\n...             use_bias=False),\n...         tf.keras.layers.BatchNormalization(),\n...         tf.keras.layers.LeakyReLU(),\n...         tf.keras.layers.Reshape(\n...             (hidden_size[0], hidden_size[1], n_filters)),\n...     \n...         tf.keras.layers.Conv2DTranspose(\n...             filters=n_filters, kernel_size=(5, 5),\n...             strides=(1, 1), padding='same', use_bias=False),\n...         tf.keras.layers.BatchNormalization(),\n...         tf.keras.layers.LeakyReLU()\n...     ])\n...         \n...     nf = n_filters\n...     for i in range(n_blocks):\n...         nf = nf // 2\n...         model.add(\n...             tf.keras.layers.Conv2DTranspose(\n...                 filters=nf, kernel_size=(5, 5),\n...                 strides=(2, 2), padding='same',\n...                 use_bias=False))\n...         model.add(tf.keras.layers.BatchNormalization())\n...         model.add(tf.keras.layers.LeakyReLU())\n...                 \n...     model.add(\n...         tf.keras.layers.Conv2DTranspose(\n...             filters=output_size[2], kernel_size=(5, 5),\n...             strides=(1, 1), padding='same', use_bias=False,\n...             activation='tanh'))\n...         \n...     return model\n>>> def make_dcgan_discriminator(\n...         input_size=(28, 28, 1),\n...         n_filters=64,\n...         n_blocks=2):\n...     model = tf.keras.Sequential([\n...         tf.keras.layers.Input(shape=input_size),\n...         tf.keras.layers.Conv2D(\n...             filters=n_filters, kernel_size=5,\n...             strides=(1, 1), padding='same'),\n...         tf.keras.layers.BatchNormalization(),\n...         tf.keras.layers.LeakyReLU()\n...     ])\n...     \n...     nf = n_filters\n...     for i in range(n_blocks):\n...         nf = nf*2\n...         model.add(\n...             tf.keras.layers.Conv2D(\n...                 filters=nf, kernel_size=(5, 5),\n...                 strides=(2, 2),padding='same'))\n...         model.add(tf.keras.layers.BatchNormalization())\n...         model.add(tf.keras.layers.LeakyReLU())\n...         model.add(tf.keras.layers.Dropout(0.3))\n...         \n...     model.add(\n...         tf.keras.layers.Conv2D(\n...                 filters=1, kernel_size=(7, 7),\n...                 padding='valid'))\n...     \n...     model.add(tf.keras.layers.Reshape((1,)))\n...     \n...     return model \n```", "```py\n>>> mnist_bldr = tfds.builder('mnist')\n>>> mnist_bldr.download_and_prepare()\n>>> mnist = mnist_bldr.as_dataset(shuffle_files=False)\n>>> def preprocess(ex, mode='uniform'):\n...     image = ex['image']\n...     image = tf.image.convert_image_dtype(image, tf.float32)\n...\n...     image = image*2 - 1.0\n...     if mode == 'uniform':\n...         input_z = tf.random.uniform(\n...             shape=(z_size,), minval=-1.0, maxval=1.0)\n...     elif mode == 'normal':\n...         input_z = tf.random.normal(shape=(z_size,))\n...     return input_z, image \n```", "```py\n>>> gen_model = make_dcgan_generator()\n>>> gen_model.summary()\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 6272)              125440    \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 6272)              25088     \n_________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)    (None, 6272)              0         \n_________________________________________________________________\nreshape_2 (Reshape)          (None, 7, 7, 128)         0         \n_________________________________________________________________\nconv2d_transpose_4 (Conv2DTr (None, 7, 7, 128)         409600    \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 7, 7, 128)         512       \n_________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)    (None, 7, 7, 128)         0         \n_________________________________________________________________\nconv2d_transpose_5 (Conv2DTr (None, 14, 14, 64)        204800    \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 14, 14, 64)        256       \n_________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_transpose_6 (Conv2DTr (None, 28, 28, 32)        51200     \n_________________________________________________________________\nbatch_normalization_10 (Batc (None, 28, 28, 32)        128       \n_________________________________________________________________\nleaky_re_lu_10 (LeakyReLU)   (None, 28, 28, 32)        0         \n_________________________________________________________________\nconv2d_transpose_7 (Conv2DTr (None, 28, 28, 1)         800       \n=================================================================\nTotal params: 817,824\nTrainable params: 804,832\nNon-trainable params: 12,992\n_________________________________________________________________ \n```", "```py\n>>> disc_model = make_dcgan_discriminator()\n>>> disc_model.summary()\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 28, 28, 64)        1664      \n_________________________________________________________________\nbatch_normalization_11 (Batc (None, 28, 28, 64)        256       \n_________________________________________________________________\nleaky_re_lu_11 (LeakyReLU)   (None, 28, 28, 64)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 14, 14, 128)       204928    \n_________________________________________________________________\nbatch_normalization_12 (Batc (None, 14, 14, 128)       512       \n_________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)   (None, 14, 14, 128)       0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 7, 7, 256)         819456    \n_________________________________________________________________\nbatch_normalization_13 (Batc (None, 7, 7, 256)         1024      \n_________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)   (None, 7, 7, 256)         0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 7, 7, 256)         0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 1, 1, 1)           12545     \n_________________________________________________________________\nreshape_3 (Reshape)          (None, 1)                 0         \n=================================================================\nTotal params: 1,040,385\nTrainable params: 1,039,489\nNon-trainable params: 896\n_________________________________________________________________ \n```", "```py\n>>> num_epochs = 100\n>>> batch_size = 128\n>>> image_size = (28, 28)\n>>> z_size = 20\n>>> mode_x = 'uniform'\n>>> lambda_gp = 10.0\n>>> tf.random.set_seed(1)\n>>> np.random.seed(1)\n>>> ## Set-up the dataset\n>>> mnist_trainset = mnist['train']\n>>> mnist_trainset = mnist_trainset.map(preprocess)\n>>> mnist_trainset = mnist_trainset.shuffle(10000)\n>>> mnist_trainset = mnist_trainset.batch(\n...     batch_size, drop_remainder=True)\n>>> ## Set-up the model\n>>> with tf.device(device_name):\n...     gen_model = make_dcgan_generator()\n...     gen_model.build(input_shape=(None, z_size))\n...\n...     disc_model = make_dcgan_discriminator()\n...     disc_model.build(input_shape=(None, np.prod(image_size))) \n```", "```py\n>>> import time\n>>> ## Optimizers:\n>>> g_optimizer = tf.keras.optimizers.Adam(0.0002)\n>>> d_optimizer = tf.keras.optimizers.Adam(0.0002)\n>>> if mode_z == 'uniform':\n...     fixed_z = tf.random.uniform(\n...         shape=(batch_size, z_size), minval=-1, maxval=1)\n... elif mode_z == 'normal':\n...     fixed_z = tf.random.normal(shape=(batch_size, z_size))\n...\n>>> def create_samples(g_model, input_z):\n...     g_output = g_model(input_z, training=False)\n...     images = tf.reshape(g_output, (batch_size, *image_size))\n...     return (images+1)/2.0\n>>> all_losses = []\n>>> epoch_samples = []\n>>> start_time = time.time()\n>>> for epoch in range(1, num_epochs+1):\n...\n...     epoch_losses = []\n...\n...     for i,(input_z,input_real) in enumerate(mnist_trainset):\n...         \n...         with tf.GradientTape() as d_tape, tf.GradientTape() \\\n...                 as g_tape:\n... \n...             g_output = gen_model(input_z, training=True)\n...             \n...             d_critics_real = disc_model(input_real,\n...                 training=True)\n...             d_critics_fake = disc_model(g_output,\n...                 training=True)\n...\n...             ## Compute generator's loss:\n...             g_loss = -tf.math.reduce_mean(d_critics_fake)\n...\n...             ## compute discriminator's losses:\n...             d_loss_real = -tf.math.reduce_mean(d_critics_real)\n...             d_loss_fake =  tf.math.reduce_mean(d_critics_fake)\n...             d_loss = d_loss_real + d_loss_fake\n...         \n...             ## Gradient-penalty:\n...             with tf.GradientTape() as gp_tape:\n...                 alpha = tf.random.uniform(\n...                     shape=[d_critics_real.shape[0], 1, 1, 1],\n...                     minval=0.0, maxval=1.0)\n...                 interpolated = (alpha*input_real +\n...                                  (1-alpha)*g_output)\n...                 gp_tape.watch(interpolated)\n...                 d_critics_intp = disc_model(interpolated)\n...             \n...             grads_intp = gp_tape.gradient(\n...                 d_critics_intp, [interpolated,])[0]\n...             grads_intp_l2 = tf.sqrt(\n...                 tf.reduce_sum(tf.square(grads_intp),\n...                               axis=[1, 2, 3]))\n...             grad_penalty = tf.reduce_mean(tf.square(\n...                                grads_intp_l2 - 1.0))\n...         \n...             d_loss = d_loss + lambda_gp*grad_penalty\n...             \n...         ## Optimization: Compute the gradients apply them\n...         d_grads = d_tape.gradient(d_loss,\n...                       disc_model.trainable_variables)\n...         d_optimizer.apply_gradients(\n...             grads_and_vars=zip(d_grads,\n...             disc_model.trainable_variables))\n...\n...         g_grads = g_tape.gradient(g_loss,\n...                       gen_model.trainable_variables)\n...         g_optimizer.apply_gradients(\n...             grads_and_vars=zip(g_grads,\n...             gen_model.trainable_variables))\n...\n...         epoch_losses.append(\n...             (g_loss.numpy(), d_loss.numpy(),\n...              d_loss_real.numpy(), d_loss_fake.numpy()))\n...\n...     all_losses.append(epoch_losses)\n...     print(\n...         'Epoch {:03d} | ET {:.2f} min | Avg Losses >>'\n...         ' G/D {:6.2f}/{:6.2f} [D-Real: {:6.2f}'\n...         ' D-Fake: {:6.2f}]'\n...         .format(\n...             epoch, (time.time() - start_time)/60,\n...             *list(np.mean(all_losses[-1], axis=0))))\n...     epoch_samples.append(\n...         create_samples(gen_model, fixed_z).numpy()) \n```", "```py\n>>> selected_epochs = [1, 2, 4, 10, 50, 100]\n>>> fig = plt.figure(figsize=(10, 14))\n>>> for i,e in enumerate(selected_epochs):\n...     for j in range(5):\n...         ax = fig.add_subplot(6, 5, i*5+j+1)\n...         ax.set_xticks([])\n...         ax.set_yticks([])\n...         if j == 0:\n...             ax.text(-0.06, 0.5, 'Epoch {}'.format(e),\n...                     rotation=90, size=18, color='red',\n...                     horizontalalignment='right',\n...                     verticalalignment='center',\n...                     transform=ax.transAxes)\n...         \n...         image = epoch_samples[e-1][j]\n...         ax.imshow(image, cmap='gray_r')\n>>> plt.show() \n```"]
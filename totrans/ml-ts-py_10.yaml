- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning for Time-Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a widely successful paradigm for control problems
    and function optimization that doesn't require labeled data. It's a powerful framework
    for experience-driven autonomous learning, where an agent interacts directly with
    the environment by taking actions and improves its efficiency by trial and error.
    Reinforcement learning has been especially popular since the breakthrough of the
    London-based Google-owned company DeepMind in complex games.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll discuss a classification of **reinforcement learning**
    (**RL**) approaches in time-series specifically economics, and we'll deal with
    the accuracy and applicability of RL-based time-series models.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with core concepts and algorithms in RL relevant to time-series
    and we'll talk about open issues and challenges in current deep RL models.
  prefs: []
  type: TYPE_NORMAL
- en: 'I am going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Reinforcement Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement Learning for Time-Series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bandit algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python Practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with an introduction to reinforcement learning and the main concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is one of the main paradigms in machine learning alongside
    supervised and unsupervised methods. A major distinction is that supervised or
    unsupervised methods are passive, responding to changes, whereas RL is actively
    changing the environment and seeking out new data. In fact, from a machine learning
    perspective, reinforcement learning algorithms can be viewed as alternating between
    finding good data and doing supervised learning on that data.
  prefs: []
  type: TYPE_NORMAL
- en: Computer programs based on reinforcement learning have been breaking through
    barriers. In a watershed moment for artificial intelligence, in March 2016, DeepMind's
    AlphaGo defeated the professional Go board game player Lee Sedol. Previously,
    the game of Go was considered to be a hallmark of human creativity and intelligence,
    too complex to be learned by a machine.
  prefs: []
  type: TYPE_NORMAL
- en: It has been argued that it is edging us closer toward **Artificial General Intelligence**
    (**AGI**). For example, in their paper "*Reward is enough*" (2021), David Silver,
    Satinder Singh, Doina Precup, and Richard S. Sutton argue that reward-orientated
    learning is enough to acquire knowledge, learn, perceive, socialize, understand
    and produce language, generalize, and imitate. More emphatically, they state that
    reinforcement learning agents could constitute a solution to AGI.
  prefs: []
  type: TYPE_NORMAL
- en: '**Artificial General Intelligence** (**AGI**) is the hypothetical ability of
    an intelligent agent to understand or learn any intellectual task that would require
    intelligence. What is **intelligence** though? Often this is defined as anything
    humans can do or would consider hard. According to Turing Award winning computer
    scientist John McCarthy ("*What Is AI?*" 1998), "*intelligence is the computational
    part of the ability to achieve goals in the world.*"'
  prefs: []
  type: TYPE_NORMAL
- en: 'In reinforcement learning, an agent interacts with the environment through
    actions and gets feedback in the shape of rewards. Contrary to the situation in
    supervised learning, no labeled data is available, but rather the environment
    is explored and exploited on the basis of the expectation of cumulative rewards.
    This feedback cycle of action and reward is illustrated in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Feedback loop in reinforcement learning'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is concerned with the objective of reward maximization.
    By interacting with the environment, the agent gets feedback and learns to take
    better actions. By optimizing the cumulative reward, the agent develops goal-directed
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is an approach where an agent interacts
    directly with the environment by taking actions. The agent learns through trial
    and error to maximize the reward.'
  prefs: []
  type: TYPE_NORMAL
- en: If you've read *Chapter 8*, *Online Learning for Time-Series*, you might be
    confused about the difference between reinforcement learning and online learning,
    and it might be worthwhile to consider the two approaches in comparison. Some
    of the most prominent algorithms for reinforcement learning, Q-learning and Temporal
    Difference (TD) learning, to just name a couple of examples, are online algorithms,
    the way they update the value function.
  prefs: []
  type: TYPE_NORMAL
- en: However, reinforcement learning doesn't focus on predictions, but on the interaction
    with the environment. In **online learning**, information is processed continuously,
    and the problem is clearly defined in terms of what's correct and what's incorrect.
    In reinforcement learning, the goal is the optimization of a delayed reward over
    a number of steps interacting with the environment. This is the main difference
    between the two approaches, although there are many particular details proponents
    of each technique would claim as theirs. Some of these we'll discuss later in
    this chapter, such as exploration versus exploitation and experience replay.
  prefs: []
  type: TYPE_NORMAL
- en: 'A reinforcement problem is defined by three main components: the environment
    ε, the agent *A*, and the cumulative objective. The agent is a decision-making
    entity that can observe the current state of the environment and takes an action.
    By performing an action ![](img/B17577_11_001.png), the agent transitions from
    state to state, ![](img/B17577_11_002.png). Executing an action in a specific
    state provides the agent with a reward, which is a numerical score. The reward
    is an instantaneous measurement of progress towards a goal.'
  prefs: []
  type: TYPE_NORMAL
- en: The environment is in a certain state that depends on some combination of the
    current state and the action taken, although some of the changes could be random.
    It's the agent's objective to maximize a cumulative reward function. This cumulative
    reward objective can be the sum of rewards over a number of steps, a discounted
    sum, or the average reward over time.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, an agent in the context of RL is a system (or program) that receives
    an observation *O*[t] of the environment at time *t* and outputs an action ![](img/B17577_11_003.png)
    given its history of experiences ![](img/B17577_11_004.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, an environment is another system. It receives an action *A*[t] at
    time *t* and changes its state in accordance with the history of actions and past
    states and a random process ![](img/B17577_11_005.png). The state is accessible
    to the agent to a certain degree, and to simplify we can state: ![](img/B17577_11_006.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a reward is a scalar observation that is emitted at every time step
    *t* by the environment that provides momentaneous feedback to the agent on how
    well it is doing.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the core of the reinforcement learning agent is a model that estimates the
    value of an environmental state or suggests the next action in the environment.
    These are the two main categories of reinforcement learning: in **value-based**
    learning, a model approximates the outcomes of actions or the value of environmental
    states with a value function (a model) and the action selection reduces to take
    the action with the best expected outcome. In **policy-based** learning, we focus
    on the more direct goal of choosing the action by predicting an action from the
    environmental state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s another twist to reinforcement learning: the **exploration versus
    exploitation dilemma**. You can decide to keep doing what you know works best
    (exploitation) or try out new avenues (exploration). Trying out new things will
    probably lead to worse results in the short run but might teach you important
    lessons that you can draw from in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple approach to balance the two against each other is **epsilon-greedy**.
    This is a simple method to balance exploration and exploitation by choosing between
    exploration and exploitation randomly: either we follow our model''s advice, or
    we don''t. Epsilon is the parameter for the probability that we do an action that''s
    not recognized as the best by the model; the higher epsilon, the more random the
    model''s actions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep reinforcement learning** (**DRL**) techniques are a subset of reinforcement
    learning methods, where the model is a deep neural network (or, in a looser sense,
    a multilayer-perceptron).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look into how RL can be applied to time-series!
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning for Time-Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) can and has been applied to time-series, however,
    the problem has to be framed in a certain way. For reinforcement learning, we
    need to have significant feedback between predictions and ongoing (actions) of
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: In order to apply RL to time-series forecasting or predictions, the prediction
    has to condition an action, therefore the state evolution depends on the current
    state and the agent's action (and randomness). Hypothetically, rewards could be
    a performance metric about the accuracy of predictions. However, the consequences
    of good or bad predictions do not affect the original environment. Essentially
    this corresponds to a supervised learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: More meaningfully, if we want to frame our situation as an RL problem, the state
    of the systems should be affected by the agents' decisions. For instance, in the
    case of interacting with the stock market, we would buy or sell based on predictions
    of the movements and include something that we influence such as our portfolio
    and funds in the state, or (only really if we are a market maker) the influence
    we have over the stock movements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, RL is very apt for dealing with processes that change over time,
    although RL deals with those that can be controlled or influenced. A core application
    for time-series is in industrial processes and control – this was in fact already
    pointed out by Box and Jenkins in their classic book "*Time-Series Analysis: Forecasting
    and Control*").'
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of applications that we could think of for reinforcement learning.
    Trading on the stock market is a major driver of business growth, and the presence
    of uncertainty and risk recommends it as a reinforcement learning use case. In
    pricing, for example in insurance or retail, reinforcement learning can help explore
    the space of value proposition for customers that would yield high sales, while
    optimizing the margin. Finally auction mechanisms, for example online bidding
    for advertisements, are another domain. In auctions, reinforcement agents have
    to develop responses in the presence of other players.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go more into detail about a few algorithms – first, bandits.
  prefs: []
  type: TYPE_NORMAL
- en: Bandit algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Multi-Armed Bandit** (**MAB**) is a classic reinforcement learning problem,
    in which a player is faced with a slot machine (bandit) that has *k* levers (arms),
    each with a different reward distribution. The agent's goal is to maximize its
    cumulative reward on a trial-by-trial basis. Since MABs are a simple but powerful
    framework for algorithms that make decisions over time under uncertainty, a large
    number of research articles have been dedicated to them.
  prefs: []
  type: TYPE_NORMAL
- en: Bandit learning refers to algorithms that aim to optimize a single unknown stationary
    objective function. An agent chooses an action from a set of actions ![](img/B17577_11_007.png).
    The environment reveals reward ![](img/B17577_11_008.png) of the chosen action
    at time *t*. As information is accumulated over multiple rounds, the agent can
    build a good representation of the value (or reward) distribution for each arm,
    ![](img/B17577_11_009.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, a good policy might converge so that the choice of arm becomes optimal.
    According to one policy, **UCB1** (published by Peter Auer, Nicolò Cesa-Bianchi,
    and Paul Fischer, "*Finite-Time Analysis of the Multi-Armed Bandit Problem*",
    2002), given the expected values for each action, the action is chosen that maximizes
    this criterion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_11_010.png)'
  prefs: []
  type: TYPE_IMG
- en: The second term refers to the upper confidence bound of the reward values based
    on the information we have accumulated. Here, *t* refers to the number of iterations
    so far, the time step, and ![](img/B17577_11_011.png) to the number of times action
    *a* has been executed so far. This means that the nominator in the equation increases
    logarithmically with time and the denominator increases each time we receive reward
    information from the action.
  prefs: []
  type: TYPE_NORMAL
- en: When the available rewards are binary (win or lose, yes or no, charge or no
    charge) then this can be described by a Beta distribution. The Beta distribution
    takes two parameters, ![](img/B17577_05_038.png) and ![](img/B17577_11_013.png),
    for wins and losses, respectively. The mean value is ![](img/B17577_11_014.png).
  prefs: []
  type: TYPE_NORMAL
- en: In **Thompson sampling**, we sample from the Beta distribution of each action
    (arm) and choose the action with the highest estimated return. The Beta distribution
    narrows with the number of tries, therefore actions that have been tried infrequently
    have wide distributions. Therefore, Beta sampling models the estimated mean reward
    and the level of confidence in the estimate. In **Dirichlet sampling**, instead
    of sampling from a Beta distribution, we are sampling from a Dirichlet distribution
    (also called multivariate Beta distribution).
  prefs: []
  type: TYPE_NORMAL
- en: '**Contextual bandits** incorporate information about the environment for updating
    the reward expectation. If you think about ads, this contextual information could
    be if the ad is about traveling. The advantage of contextual bandits is agents
    can encode much richer information about the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contextual bandits, an agent chooses an arm, the reward ![](img/B17577_11_015.png)
    is revealed, and the agent''s expectation of the reward is updated, but with context
    features: ![](img/B17577_11_016.png), where *x* is a set of features encoding
    the environment. In many implementations, the context is often restricted to discrete
    values, however, at least in theory, they could be either categorical or numerical.
    The value function could be any machine learning algorithm such as a neural network
    (NeuralBandit) or a random forest (BanditForest).'
  prefs: []
  type: TYPE_NORMAL
- en: Bandits find applications, among other fields, in information retrieval models
    such as recommender and ranking systems, which are employed in search engines
    or on consumer websites. The **probability ranking principle** (PRP; from S.E.
    Robertson's article "*The probability ranking principle in IR*", 1977) forms the
    theoretical basis for probabilistic models, which have been dominating IR. The
    PRP states that articles should be ranked in decreasing order of relevance probability.
    This is what we'll go through in an exercise in the practice section.
  prefs: []
  type: TYPE_NORMAL
- en: Let's delve into Q-learning and deep Q-learning now.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning, introduced by Chris Watkins in 1989, is an algorithm to learn the
    value of an action in a particular state. Q-learning revolves around representing
    the expected rewards for an action taken in a given state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected reward of the state-action combination ![](img/B17577_11_017.png)
    is approximated by the Q function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_11_018.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Q* is initialized to a fixed value, usually at random. At each time step *t*,
    the agent selects an action ![](img/B17577_11_019.png) and sees a new state of
    the environment ![](img/B17577_11_020.png) as a consequence and receives a reward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The value function *Q* can then be updated according to the Bellman equation
    as the weighted average of the old value and the new information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_11_021.png)'
  prefs: []
  type: TYPE_IMG
- en: The weight is by ![](img/B17577_05_038.png), the learning rate – the higher
    the learning rate, the more adaptive the Q-function. The discount factor ![](img/B17577_11_023.png)
    is weighting the rewards by their immediacy – the higher ![](img/B17577_11_024.png),
    the more impatient (myopic) the agent becomes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_11_025.png) represents the current reward. ![](img/B17577_11_026.png)
    is the reward obtained by ![](img/B17577_11_027.png) weighted by learning rate
    ![](img/B17577_11_028.png), and ![](img/B17577_11_029.png) is the weighted maximum
    reward that can be obtained from state ![](img/B17577_11_030.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This last part can be recursively broken down into simpler sub-problems like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_11_031.png)'
  prefs: []
  type: TYPE_IMG
- en: In the simplest case, *Q* can be a lookup table, called a Q-table.
  prefs: []
  type: TYPE_NORMAL
- en: In 2014, Google DeepMind patented an algorithm called **deep Q-learning**. This
    algorithm was introduced in the Nature paper "*Human-level control through deep
    reinforcement learning*" with an application in Atari 2600 games.
  prefs: []
  type: TYPE_NORMAL
- en: In Deep Q-learning, a neural network is used for the Q-function as a nonlinear
    function approximator. They used a convolutional neural network to learn expected
    rewards from pixel values. They introduced a technique called **experience replay**
    to update Q over a randomly drawn sample of prior actions. This is done to reduce
    the learning instability of the Q updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q-learning can be shown in pseudocode roughly like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This implements an epsilon-greedy policy by which a random (exploratory) choice
    is made according to the probability `epsilon`. A few more variables are assumed
    given. The handle for the environment, `env`, allows us to execute an action.
    We have a learning function, which applies gradient descent on the Q-function
    to learn better values according to the Bellman equation. The parameter `L` is
    the number of previous values that are used for learning.
  prefs: []
  type: TYPE_NORMAL
- en: The memory replay part is obviously simplified. In actuality, we would have
    a maximum size of the memory, and, once the memory capacity is reached, we would
    replace old associations of states, actions, and rewards with new ones.
  prefs: []
  type: TYPE_NORMAL
- en: We'll put some of this into practice now.
  prefs: []
  type: TYPE_NORMAL
- en: Python Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get into modeling. We'll start by giving some recommendations for users
    using MABs.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, we'll take joke preferences by users, and we'll use them to
    simulate feedback on recommended jokes on our website. We'll use this feedback
    to tune our recommendations. We want to select the 10 best jokes to present to
    people visiting our site. The recommendations are going to be produced by 10 MABs
    that each have as many arms as there are jokes.
  prefs: []
  type: TYPE_NORMAL
- en: This is adapted from an example from the `mab-ranking` library on GitHub by
    Kenza-AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a handy library that comes with implementations of different bandits.
    I''ve simplified the installation of this library in my fork of the library, so
    we''ll be using my fork here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After this is finished, we can get right to it!
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll download the `jester` dataset with joke preferences from S3\. Here''s
    the location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll download them using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll make some cosmetic adjustments. The rows refer to users, the columns
    to jokes. We can make this clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The encoding of choices is a bit weird, so we''ll fix this as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'So either people chose the joke or they didn''t. We''ll get rid of people who
    didn''t choose any joke at all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Our dataset looks like this now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_oTdq4A/Screenshot
    2021-09-04 at 16.41.24.png](img/B17577_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: Jester dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll set up our bandits as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We choose independent bandits with Thompson sampling from the Beta distribution.
    We recommend the best 10 jokes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then start our simulation. Our hypothetical website has lots of visits,
    and we''ll get feedback on the 10 jokes that we''ll display as chosen by our independent
    bandits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We are simulating 7,000 iterations (visits). At each visit, we'll change our
    choices according to the updated reward expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the hit rate, the jokes that users are selecting, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'I''ve introduced a rolling average (over 200 iterations) to get a smoother
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: Hit rate over time (Dirichlet sampling)'
  prefs: []
  type: TYPE_NORMAL
- en: The mab-ranking library supports contextual information, so we can try out giving
    additional information. Let's imagine this information as different user groups
    (cohorts). We could think of users who use different search or filter functionality
    on our imaginary website, say "newest jokes" or "most popular." Alternatively,
    they could be from different regions. Or it could be a timestamp category that
    corresponds to the time of the day of visits of users to our website.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s supply the categorical user group information, the context. We''ll cluster
    users by their preferences, and we''ll use the clusters as context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This creates 5 user groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll have to reset our bandits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can redo our simulation. Only now, we''ll supply the user context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the hit rate over time again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![recommendations_dirichlet_context.png](img/B17577_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Hit rate over time (Dirichlet sampling with context)'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the hit rate is a bit higher than before.
  prefs: []
  type: TYPE_NORMAL
- en: This model ignores the order of the recommended jokes on our hypothetical website.
    There are other bandit implementations that model the ranks.
  prefs: []
  type: TYPE_NORMAL
- en: I'll leave it to the reader to play around with this more. A fun exercise is
    to create a probabilistic model of reward expectations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll be playing around with a deep Q-learning trading
    bot. This is a more intricate model and will require more attention. We'll apply
    this to cryptocurrency trading.
  prefs: []
  type: TYPE_NORMAL
- en: Trading with DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is based on a tutorial of the TensorTrade library, which we'll use in this
    example. TensorTrade is a framework for building, training, evaluating, and deploying
    robust trading algorithms using reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorTrade relies on existing tools such as OpenAI Gym, Keras, and TensorFlow
    to enable fast experimentation with algorithmic trading strategies. We''ll install
    it with pip as usual. We''ll make sure we install the latest version from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We could also install the `ta` library, which can provide additional signals
    useful for trading, but we'll leave this out here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get a few imports out of the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: These imports concern utilities for the (simulated) exchange, the portfolio,
    and the environment. Further, there are utilities for data loading and feeding
    it into the simulation, constants for currency conversion, and finally, there's
    a deep Q-agent, which consists of a Deep Q-Network (DQN).
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the matplotlib magic command `(%matplotlib inline`) is needed
    for the Plotly charts to show up as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, let''s load a dataset of historical cryptocurrency prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset consists of hourly Bitcoin prices in US dollars. It looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_uXbUvY/Screenshot
    2021-09-05 at 10.57.42.png](img/B17577_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Crypto dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We'll add a relative strength indicator signal, a technical indicator for the
    financial markets. It measures the strength or weakness of a market by comparing
    the closing prices of a recent trading period. We'll also add a **moving average
    convergence/divergence** (**MACD**) indicator, which is designed to reveal changes
    in the strength, direction, momentum, and duration of a trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'These two are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, here we could be using trading signals from the `ta` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll now set up the feed that goes into our decision making:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We are selecting the closing price as a feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll add our indicators as additional features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Aside from RSI and MACD, we are also adding a trend indicator (LR).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can have a look at the first five lines from the data feed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what our trading signal features look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_y5gWwn/Screenshot
    2021-09-05 at 11.11.53.png](img/B17577_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Data feed for trading'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up the broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The exchange is the interface that will let us execute orders. An exchange needs
    a name, an execution service, and streams of price data. Currently, TensorTrade
    supports a simulated execution service using simulated or stochastic data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need a portfolio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: A portfolio can be any combination of exchanges and instruments that the exchange
    supports.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorTrade includes lots of monitoring tools, called renderers, which can
    be attached to the environment. They can draw a chart (`PlotlyTradingChart`) or
    log to a file (`FileLogger`), for example. Here''s our setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, here''s the trading environment, which is an instance of the OpenAI
    Gym (the OpenAI Gym provides a wide variety of simulated environments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You might be familiar with Gym environments if you've done reinforcement learning
    before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the Gym feed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what comes through:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_GBkklu/Screenshot
    2021-09-05 at 11.40.11.png](img/B17577_11_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: Environment data feed for trading bot'
  prefs: []
  type: TYPE_NORMAL
- en: This is what the trading bot will be able to rely on for making decisions on
    executing trades.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can set up and train our DQN trading agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It might be a good point here to explain the difference between an epoch and
    an episode. Readers will probably be familiar with an epoch, which is a single
    pass over all training examples, whereas an episode is specific to the context
    of reinforcement learning. An episode is a sequence of states, actions, and rewards,
    which ends with a terminal state.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get lots of plotting output from our renderer. Here''s the first output
    I got (yours might differ a bit):'
  prefs: []
  type: TYPE_NORMAL
- en: '![../trading_renderer.png](img/B17577_11_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: PlotlyPlotRenderer – Episode 2/2 Step 51/200'
  prefs: []
  type: TYPE_NORMAL
- en: This plot gives an overview of the market operations of our trading bot. The
    first subplot shows the up and down movements of the prices. Then the second subplot
    charts volumes of stock in the portfolio, and in the bottom-most subplot, you
    can see the portfolio net worth.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to see the net worth over time (not only the first snapshot as
    above), you can plot this as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the portfolio net worth over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![networth.png](img/B17577_11_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: Portfolio worth over time'
  prefs: []
  type: TYPE_NORMAL
- en: It looks like our trading bot could need some more training before getting let
    loose in the wild. I made a loss, so I am happy there wasn't real money on the
    line.
  prefs: []
  type: TYPE_NORMAL
- en: This is all folks. Let's summarize.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While online learning, which we talked about in *Chapter 8*, *Online Learning
    for Time-Series* is tackling traditional supervised learning, reinforcement learning
    tries to deal with the environment. In this chapter, I've introduced reinforcement
    learning concepts relevant to time-series, and we've discussed many algorithms,
    such as deep Q-learning and **MABs**.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning algorithms are very useful in certain contexts like recommendations,
    trading, or – more generally – control scenarios. In the practice section, we
    implemented a recommender using MABs and a trading bot with a DQN.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at case studies with time-series. Among other
    things, we'll look at multivariate forecasts of energy demand.
  prefs: []
  type: TYPE_NORMAL

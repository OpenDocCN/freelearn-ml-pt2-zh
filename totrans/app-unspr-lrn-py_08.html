<html><head></head><body>
		<div class="Content" id="_idContainer298">
			<h1 id="_idParaDest-167"><em class="italics"><a id="_idTextAnchor189"/>Chapter 8</em></h1>
		</div>
		<div class="Content" id="_idContainer299">
			<h1 id="_idParaDest-168"><a id="_idTextAnchor190"/>Market Basket Analysis</h1>
		</div>
		<div class="Content" id="_idContainer300">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Work with transaction-level data</li>
				<li class="bullets">Use market basket analysis in the appropriate context</li>
				<li class="bullets">Run the Apriori algorithm and build association rules</li>
				<li class="bullets">Perform basic visualizations on association rules</li>
				<li class="bullets">Interpret the key metrics of market basket analysis</li>
			</ul>
			<p>In this chapter, we will explore a foundational and reliable algorithm for analyzing transaction data.</p>
		</div>
		<div class="Content" id="_idContainer361">
			<h2 id="_idParaDest-169"><a id="_idTextAnchor191"/>Introduction</h2>
			<p>In this chapter, we are going to change direction entirely. The previous chapter, which explored topic models, focused on natural language processing, text data, and applying relatively recently developed algorithms. Most data science practitioners would agree that natural language processing, including topic models, is toward the cutting edge of data science and is an active research area. We now understand that topic models can, and should, be leveraged wherever text data could potentially drive insights or growth, including in social media analysis, recommendation engines, and news filtering.</p>
			<p>This chapter takes us into the retail space to explore a foundational and reliable algorithm for analyzing transaction data. While this algorithm might not be on the cutting edge or in the catalog of the most popular machine learning algorithms, it is ubiquitous and undeniably impactful in the retail space. The insights it drives are easily interpretable, immediately actionable, and instructive for determining analytical next steps. If you work in the retail space or with transaction data, you would be well-served to dive deep into market basket analysis.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor192"/>Market Basket Analysis</h2>
			<p>Imagine you work for a retailer that sells dozens of products and your boss comes to you and asks the following questions: </p>
			<ul>
				<li>What products are purchased together most frequently? </li>
				<li>How should the products be organized and positioned in the store? </li>
				<li>How do we identify the best products to discount via coupons? </li>
			</ul>
			<p>You might reasonably respond with complete bewilderment, as those questions are very diverse and do not immediately seem answerable using a single algorithm and dataset. However, the answer to all those questions and many more is <strong class="keyword">market basket analysis</strong>. The general idea behind market basket analysis is to identify and quantify which items, or groups of items, are purchased together frequently enough to drive insight into customer behavior and product relationships.</p>
			<p>Before we dive into the analytics, it is worth defining the term market basket. A market basket is a permanent set of products in an economic system. In this case, permanent does not necessarily mean permanent in the traditional sense. It means that until such time as the product is taken out of the catalog, it will consistently be available for purchase. The product referenced in the preceding definition is any good, service, or element of a group, including a bicycle, having your house painted, or a website. Lastly, an economic system could be a company, a collection of activities, or a country. The easiest example of a market basket is a grocery store, which is a system made up of a collection of food and drink items.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer301">
					<img alt="Figure 8.1: An example market basket where the economic system is the butcher shop and the permanent set of items is all the meat products offered by the butcher&#13;&#10;" src="image/C12626_08_01.jpg"/>
				</div>
			</div>
			<h6>Figure 8.1: An example market basket where the economic system is the butcher shop and the permanent set of items is all the meat products offered by the butcher</h6>
			<p>Even without using any models or analyses, certain product relationships are obvious. Let's take the relationship between meat and vegetables. Typically, market basket analysis models return relationships more specific than meat and vegetables, but, for argument's sake, we will generalize to meat and vegetables. Okay, there is a relationship between meat and vegetables. So what? Well, we know these are staple items that are frequently purchased together. We can leverage this information by putting the vegetables and meats on opposite sides of the store, which you will notice is often the positioning of those two items, forcing customers to walk the full distance of the store, and thereby increasing the likelihood that they will buy additional items that they might not have bought if they did not have to traverse the whole store.</p>
			<p>One of the things retail companies struggle with is how to discount items effectively. Let's consider another obvious relationship: peanut butter and jelly. In the United States, peanut butter and jelly sandwiches are incredibly popular, especially among children. When peanut butter is in a shopping basket, the chance jelly is also there can be assumed to be quite high. Since we know peanut butter and jelly are purchased together, it does not make sense to discount them both. If we want customers to buy both items, we can just discount one of the items, knowing that if we can get the customers to buy the discounted item, they will probably buy the other item too, even if it is full price.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer302">
					<img alt="Figure 8.2: A visualization of market basket analysis&#13;&#10;" src="image/C12626_08_02.jpg"/>
				</div>
			</div>
			<h6>Figure 8.2: A visualization of market basket analysis</h6>
			<p>Just like the topic models in the previous chapter, market basket analysis is all about identifying frequently occurring groups. Here, we are looking for frequently occurring groups of products, whereas in topic models, we were looking for frequently occurring groups of words. Thus, as it was to topic models, the word clustering could be applied to market basket analysis. The major differences are that the clusters in market basket analysis are micro, only a few products per cluster, and the order of the items in the cluster matters when it comes to computing probabilistic metrics. We will dive much deeper into these metrics and how they are calculated later in this chapter.</p>
			<p>What has clearly been implied by the previous two examples is that, in market basket analysis, retailers can discover the relationships – obvious and surprising – between the products that customers buy. Once uncovered, the relationships can be used to inform and improve the decision-making process. A great aspect of market basket analysis is that while this analysis was developed in relation to, discussed in terms of, and mostly applied to the retail world, it can be applied to many diverse types of businesses. </p>
			<p>The only requirement for performing this type of analysis is that the data is a list of collections of items. In the retail case, this would be a list of transactions where each transaction is a group of purchased products. One example of an alternative application is analyzing website traffic. With website traffic, we consider the products to be websites, so each element of the list is the collection of websites visited by an individual over a specified time period. Needless to say, the applications of market basket analysis extend well beyond the principal retail application.</p>
			<h3 id="_idParaDest-171"><a id="_idTextAnchor193"/>Use Cases</h3>
			<p>There are three principal use cases in the traditional retail application: pricing enhancement, coupon and discount recommendation, and store layout. As was briefly mentioned previously, by using the product associations uncovered by the model, retailers can strategically place products in their stores to get customers to buy more items and thus spend more money. If any relationship between two or more products is sufficiently strong, meaning the product grouping occurs often in the dataset and the individual products in the grouping appear separate from the group infrequently, then the products could be placed far away from one another in the store without significantly jeopardizing the odds of the customer purchasing both products. By forcing the customer to traverse the whole store to get both products, the retailer increases the chances that the customer will notice and purchase additional products. Likewise, retailers can increase the chances of customers purchasing two weakly related or non-staple products by placing the two items next to each other. Obviously, there are a lot of factors that drive store layout, but market basket analysis is definitely one of those factors:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer303">
					<img alt="Figure 8.3: How product associations can help inform efficient and lucrative store layouts&#13;&#10;" src="image/C12626_08_03.jpg"/>
				</div>
			</div>
			<h6>Figure 8.3: How product associations can help inform efficient and lucrative store layouts</h6>
			<p>Pricing enhancement and coupon and discount recommendation are two sides of the same coin. They can simply be interpreted as where to raise and where to lower prices. Consider the case of two strongly related items. These two items are most likely going to be purchased in the same transaction, so one way to increase the profitability of that transaction would be to increase the price of one of the items. If the association between the two items is sufficiently strong, the price increase can be made with little to no risk of the customer not purchasing both items. In a similar way, retailers can encourage customers to purchase an item weakly associated with another through discounting or couponing. </p>
			<p>For example, retailers could compare the purchase history of individual customers with the results of market basket analysis done on all transactions and find where some of the items certain customers are purchasing are weakly associated to items those customers are not currently purchasing. Using this comparison, retailers could offer discounts to the customers for the as-yet-unpurchased items the model suggested were related to the items previously purchased by those customers. If you have ever had coupons print out with your receipt at the end of a transaction, the chances are high that those items were found to be related to the items involved in your just-completed transaction.</p>
			<p>A non-traditional, but viable, use of market basket analysis would be to enhance online advertising and search engine optimization. Imagine we had access to lists of websites visited by individuals. Using market basket analysis, we could find relationships between websites and use those relationships to both strategically order and group the websites resulting from a search engine query. In many ways, this is similar to the store layout use case.</p>
			<p>With a general sense of what market basket analysis is all about and a clear understanding of its use cases, let's dig into the data used in these models.</p>
			<h3 id="_idParaDest-172"><a id="_idTextAnchor194"/>Important Probabilistic Metrics</h3>
			<p>Market basket analysis is built upon the computation of several probabilistic metrics. The five major metrics covered here are support, confidence, lift, leverage, and conviction. Before digging into transaction data and the specific market basket analysis models, including the <strong class="keyword">Apriori algorithm</strong> and <strong class="keyword">association rules</strong>, we should spend some time defining and exploring these metrics using a small, made-up set of transactions. We start by making up some data to use.</p>
			<h3 id="_idParaDest-173"><a id="_idTextAnchor195"/>Exercise 39: Creating Sample Transaction Data</h3>
			<p>Since this is the first exercise of the chapter, let's set the environment. This chapter will use the same environment requirements that were used in <em class="italics">Chapter 7</em>, <em class="italics">Topic Modeling</em>. If any of the packages do not load, as happened in the previous chapter, use <strong class="inline">pip</strong> to install them via the command line. One of the libraries we will use is <strong class="inline">mlxtend</strong>, which may be unfamiliar to you. It is a machine learning extensions library that contains useful supplemental tools, including ensembling, stacking, and, of book, market basket analysis models. This exercise does not have any real output. We will simply create a sample transaction dataset for use in subsequent exercises.</p>
			<ol>
				<li>Open a Jupyter notebook with Python 3.</li>
				<li>Install the following libraries: <strong class="inline">matplotlib.pyplot</strong>, which is used to plot the results of the models, <strong class="inline">mlxtend.frequent_patterns</strong>, which is used to run the models, <strong class="inline">mlxtend.preprocessing</strong>, which is used to encode and prep the data for the models, <strong class="inline">numpy</strong>, which is used to work with arrays, and <strong class="inline">pandas</strong>, which is used to work with DataFrames:<h4>Note</h4><p class="callout">To install <strong class="inline">mlxtend</strong>, go to the Anaconda prompt and execute <strong class="inline">pip install mlxtend</strong>.</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import mlxtend.frequent_patterns</p><p class="snippet">import mlxtend.preprocessing</p><p class="snippet">import numpy</p><p class="snippet">import pandas</p></li>
				<li>Create 10 fake transactions featuring grocery store items. The data will take the form of a list of lists, a data structure that will be relevant later when discussing formatting transaction data for the models:<p class="snippet">example = [</p><p class="snippet">    ['milk', 'bread', 'apples', 'cereal', 'jelly', </p><p class="snippet">     'cookies', 'salad', 'tomatoes'],</p><p class="snippet">    ['beer', 'milk', 'chips', 'salsa', 'grapes', </p><p class="snippet">     'wine', 'potatoes', 'eggs', 'carrots'],</p><p class="snippet">    ['diapers', 'baby formula', 'milk', 'bread', </p><p class="snippet">     'chicken', 'asparagus', 'cookies'],</p><p class="snippet">    ['milk', 'cookies', 'chicken', 'asparagus', </p><p class="snippet">     'broccoli', 'cereal', 'orange juice'],</p><p class="snippet">    ['steak', 'asparagus', 'broccoli', 'chips', </p><p class="snippet">     'salsa', 'ketchup', 'potatoes', 'salad'],</p><p class="snippet">    ['beer', 'salsa', 'asparagus', 'wine', 'cheese', </p><p class="snippet">     'crackers', 'strawberries', 'cookies'],</p><p class="snippet">    ['chocolate cake', 'strawberries', 'wine', 'cheese', </p><p class="snippet">     'beer', 'milk', 'orange juice'],</p><p class="snippet">    ['chicken', 'peas', 'broccoli', 'milk', 'bread', </p><p class="snippet">     'eggs', 'potatoes', 'ketchup', 'crackers'],</p><p class="snippet">    ['eggs', 'bread', 'cheese', 'turkey', 'salad', </p><p class="snippet">     'tomatoes', 'wine', 'steak', 'carrots'],</p><p class="snippet">    ['bread', 'milk', 'tomatoes', 'cereal', 'chicken', </p><p class="snippet">     'turkey', 'chips', 'salsa', 'diapers']</p><p class="snippet">]</p></li>
			</ol>
			<p>This simple dataset will make explaining and interpreting the probabilistic metrics much easier.</p>
			<h3 id="_idParaDest-174"><a id="_idTextAnchor196"/>Support</h3>
			<p><strong class="bold">Support</strong> is simply the probability that the item set appears in the data, which can be calculated by counting the number of transactions in which the item set appears and dividing that count by the total number of transactions. Note that an item set can be a single item or a group of items. Support is an important metric, despite being very simple, as it is one of the primary metrics used to determine the believability and strength of association between groups of items. For example, it is possible to have two items that only occur with each other, suggesting that their association is very strong, but in a dataset containing 100 transactions, only appearing twice is not very impressive. Because the item set appears in only 2% of the transactions, and 2% is small in terms of the raw number of appearances, the association cannot be considered significant and, thus, is probably unusable in decision making. </p>
			<p>Note that since support is a probability, it will fall in the range [0,1]. The formula takes the following form if the item set is two items, X and Y, and N is the total number of transactions.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer304">
					<img alt="Figure 8.4: Formula for support&#13;&#10;" src="image/C12626_08_04.jpg"/>
				</div>
			</div>
			<h6>Figure 8.4: Formula for support</h6>
			<p>Let's return momentarily to the made-up data from <em class="italics">Exercise 39</em>, <em class="italics">Creating Sample Transaction Data</em> and define an item set as being milk and bread. We can easily look through the 10 transactions and count the number of transactions in which this milk and bread item set occurs – that would be 4 times. Given that there are 10 transactions, the support of milk and bread is 4 divided by 10, or 0.4. Whether this is large enough support depends on the dataset itself, which we will get into in a later section.</p>
			<h3 id="_idParaDest-175"><a id="_idTextAnchor197"/>Confidence</h3>
			<p>The <strong class="bold">confidence</strong> metric can be thought of in terms of conditional probability, as it is basically the probability that product B is purchased given the purchase of product A. Confidence is typically notated as A <img alt="" src="image/C12626_Formula_08_01.png"/> B, and expressed as the proportion of transactions containing A that also contain B. Hence, confidence is found by filtering the full set of transactions down to those containing A, and then computing the proportion of those transactions that contain B. Like support, confidence is a probability, so its range is [0,1]. Using the same variable definitions from the support section, the following is the formula for confidence:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer306">
					<img alt="Figure 8.5: Formula for confidence&#13;&#10;" src="image/C12626_08_05.jpg"/>
				</div>
			</div>
			<h6>Figure 8.5: Formula for confidence</h6>
			<p>To demonstrate confidence, we will use the items beer and wine. Specifically, let's compute the confidence of Beer <img alt="" src="image/C12626_Formula_08_02.png"/> Wine. To start, we need to identify the transactions that contain beer. There are 3 of them, and they are transactions 2, 6, and 7. Now, of those transactions, how many contain wine? The answer is all of them. Thus, the confidence of Beer <img alt="" src="image/C12626_Formula_08_03.png"/> Wine is 1. Every time a customer bought beer, they also bought wine. It might be obvious, but for identifying actionable associations, higher confidence values are better:</p>
			<h3 id="_idParaDest-176"><a id="_idTextAnchor198"/>Lift and Leverage</h3>
			<p>We will discuss the next two metrics, lift and leverage, simultaneously, since despite being calculated differently, both seek to answer the same question. Like confidence, <strong class="bold">lift</strong> and <strong class="bold">leverage</strong> are notated as A <img alt="" src="image/C12626_Formula_08_04.png"/> B. The question to which we seek an answer is, can one item, say A, be used to determine anything about another item, say B? Stated another way, if product A is bought by an individual, can we say anything about whether they will or will not purchase product B with some level of confidence? These questions are answered by comparing the support of A and B under the standard case when A and B are not assumed to be independent with the case where the two products are assumed to be independent. Lift calculates the ratio of these two cases, so its range is [0, Infinity]. When lift equals one, the two products are independent and, hence, no conclusions can be made about product B when product A is purchased:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer310">
					<img alt="Figure 8.6: Formula for lift&#13;&#10;" src="image/C12626_08_06.jpg"/>
				</div>
			</div>
			<h6>Figure 8.6: Formula for lift</h6>
			<p>Leverage calculates the difference between the two cases, so its range is [-1, 1]. Leverage equaling zero can be interpreted the same way as lift equaling one:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer311">
					<img alt="Figure 8.7: Formula for leverage&#13;&#10;" src="image/C12626_08_07.jpg"/>
				</div>
			</div>
			<h6>Figure 8.7: Formula for leverage</h6>
			<p>The values of the metrics measure the strength and direction of the relationship between the items. If the lift value is 0.1, we could say the relationship between the two items is strong in the negative direction. That is, it could be said that when one product is purchased, the chance the second product is purchased is diminished. The positive and negative associations are separated by the points of independence, which, as stated earlier, are 1 for lift and 0 for leverage, and the further away the value gets from these points, the stronger the association.</p>
			<h3 id="_idParaDest-177"><a id="_idTextAnchor199"/>Conviction</h3>
			<p>The last metric to be discussed is conviction, which is a bit less intuitive than the other metrics. Conviction is the ratio of the expected frequency that X occurs without Y, given that X and Y are independent to the frequency of incorrect predictions. The frequency of incorrect predictions is defined as 1 minus the confidence of X <img alt="" src="image/C12626_Formula_08_05.png"/> Y. Remember that confidence can be defined as <img alt="" src="image/C12626_Formula_08_06.png"/>, which means .<img alt="" src="image/C12626_Formula_08_07.png"/>. The numerator could also be thought of as <img alt="" src="image/C12626_Formula_08_08.png"/>. The only difference between the two is that the numerator has the assumption of independence between X and Y, while the denominator does not. A value greater than 1 is ideal because that means the association between products or item sets X and Y is incorrect more often if the association between X and Y is random chance (in other words, X and Y are independent). To reiterate, this stipulates that the association between X and Y is meaningful. A value of 1 applies independence, and a value of less than 1 signifies that the random chance X and Y relationship is correct more often than the X and Y relationship that has been defined as X <img alt="" src="image/C12626_Formula_08_09.png"/> Y. Under this situation, the relationship might go the other way (in other words, Y <img alt="" src="image/C12626_Formula_08_10.png"/> X). Conviction has the range [0, Inf] and the following form:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer318">
					<img alt="Figure 8.8: Formula for conviction&#13;&#10;" src="image/C12626_08_08.jpg"/>
				</div>
			</div>
			<h6>Figure 8.8: Formula for conviction</h6>
			<p>Let's again return to the products beer and wine, but for this explanation, we will consider the opposite association of Wine <img alt="" src="image/C12626_Formula_08_11.png"/> Beer. Support(Y) or, in this case, Support(Beer) is 3/10 and Confidence X <img alt="" src="image/C12626_Formula_08_12.png"/> Y, or, in this case, Confidence(Wine <img alt="" src="image/C12626_Formula_08_13.png"/> Beer), is 3/4. Thus, the Conviction(Wine <img alt="" src="image/C12626_Formula_08_14.png"/> Beer) is (1-3/10) / (1-3/4) = (7/10) * (4/1). We can conclude by saying that Wine <img alt="" src="image/C12626_Formula_08_15.png"/> Beer would be incorrect 2.8 times as often if wine and beer were independent. Thus, the previously articulated association between wine and beer is legitimate.</p>
			<h3 id="_idParaDest-178"><a id="_idTextAnchor200"/>Exercise 40: Computing Metrics</h3>
			<p>In this exercise, we use the fake data in <em class="italics">Exercise 39</em>, <em class="italics">Creating Sample Transaction Data</em>  to compute the five previously described metrics, which we will use again in the covering of the Apriori algorithm and association rules. The association on which these metrics will be evaluated is Milk <img alt="" src="image/C12626_Formula_08_16.png"/> Bread.</p>
			<h4>Note</h4>
			<p class="callout">All exercises in this chapter need to be performed in the same Jupyter notebook.</p>
			<ol>
				<li value="1">Define and print the frequencies that are the basis of all five metrics, which would be Frequency(Milk), Frequency(Bread), and Frequency(Milk, Bread). Also, define N as the total number of transactions in the dataset:<p class="snippet">N = len(example)</p><p class="snippet">f_x = sum(['milk' in i for i in example]) # milk</p><p class="snippet">f_y = sum(['bread' in i for i in example]) # bread</p><p class="snippet">f_x_y = sum([</p><p class="snippet">    all(w in i for w in ['milk', 'bread']) </p><p class="snippet">    for i in example</p><p class="snippet">])</p><p class="snippet">print(</p><p class="snippet">    "N = {}\n".format(N) + </p><p class="snippet">    "Freq(x) = {}\n".format(f_x) + </p><p class="snippet">    "Freq(y) = {}\n".format(f_y) + </p><p class="snippet">    "Freq(x, y) = {}".format(f_x_y)</p><p class="snippet">)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer325"><img alt="Figure 8.9: Screenshot of the frequencies&#13;&#10;" src="image/C12626_08_09.jpg"/></div><h6>Figure 8.9: Screenshot of the frequencies</h6></li>
				<li>Calculate and print Support(Milk <img alt="" src="image/C12626_Formula_08_17.png"/> Bread):<p class="snippet">support = f_x_y / N</p><p class="snippet">print("Support = {}".format(round(support, 4)))</p><p>The support of <strong class="inline">x</strong> to <strong class="inline">y</strong> is <strong class="inline">0.4</strong>. From experience, if we were working with a full transaction dataset, this support value would be considered very large in many cases.</p></li>
				<li>Calculate and print Confidence(Milk <img alt="" src="image/C12626_Formula_08_17.png"/> Bread):<p class="snippet">confidence = support / (f_x / N)</p><p class="snippet">print("Confidence = {}".format(round(confidence, 4)))</p><p>The confidence of <strong class="inline">x</strong> to <strong class="inline">y</strong> is <strong class="inline">0.5714</strong>. This means that the probability of Y being purchased given that <strong class="inline">x</strong> was purchased is just slightly higher than 50%. </p></li>
				<li>Calculate and print Lift(Milk <img alt="" src="image/C12626_Formula_08_19.png"/> Bread):<p class="snippet">lift = confidence / (f_y / N)</p><p class="snippet">print("Lift = {}".format(round(lift, 4)))</p><p>	The lift of <strong class="inline">x</strong> to <strong class="inline">y</strong> is <strong class="inline">1.1429</strong>.</p></li>
				<li>Calculate and print Leverage(Milk <img alt="" src="image/C12626_Formula_08_19.png"/> Bread):<p class="snippet">leverage = support - ((f_x / N) * (f_y / N))</p><p class="snippet">print("Leverage = {}".format(round(leverage, 4)))</p><p>The leverage of <strong class="inline">x</strong> to <strong class="inline">y</strong> is <strong class="inline">0.05</strong>. Both lift and leverage can be used to say that the association <strong class="inline">x</strong> to <strong class="inline">y</strong> is positive (in other words, <strong class="inline">x</strong> implies <strong class="inline">y</strong>), but weak. That is, the values are close to 1 and 0, respectively.</p></li>
				<li>Calculate and print Conviction(Milk <img alt="" src="image/C12626_Formula_08_19.png"/> Bread):<p class="snippet">conviction = (1 - (f_y / N)) / (1 - confidence)</p><p class="snippet">print("Conviction = {}".format(round(conviction, 4)))</p><p>The conviction value of <strong class="inline">1.1667</strong> can be interpreted by saying the Milk <img alt="" src="image/C12626_Formula_08_19.png"/> Bread association would be incorrect <strong class="inline">1.1667</strong> times as often if milk and bread were independent.</p></li>
			</ol>
			<p>Before diving into the Apriori algorithm and association rule learning on actual data, we will explore transaction data and get some retail data loaded and prepped for modeling.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor201"/>Characteristics of Transaction Data</h2>
			<p>The data used in market basket analysis is transaction data or any type of data that resembles transaction data. In its most basic form, transaction data has some sort of transaction identifier, such as an invoice or transaction number, and a list of products associated with said identifier. It just so happens that these two base elements are all that is needed to perform market basket analysis. However, transaction data rarely – it is probably even safe to say never – comes in this basic form. Transaction data typically includes pricing information, dates and times, and customer identifiers, among many other things:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer332">
					<img alt="Figure 8.10: Each available product is going to map back to multiple invoice numbers&#13;&#10;" src="image/C12626_08_10.jpg"/>
				</div>
			</div>
			<h6>Figure 8.10: Each available product is going to map back to multiple invoice numbers</h6>
			<p>Due to the complexity of transaction data, data cleaning is crucial. The goal of data cleaning in the context of market basket analysis is to filter out all the unnecessary information, which includes removing variables in the data that are not relevant, and filtering out problematic transactions. The techniques used to complete these two cleaning steps vary, depending on the particular transaction data file. In an attempt to not get bogged down in data cleaning, the exercises from here on out will use a subset of an online retail dataset from the UCI Machine Learning Repository, and the activities will use the whole dataset. This both limits the data cleaning discussion, but also gives us an opportunity to discuss how the results change when the size of the dataset changes. This is important because if you work for a retailer and run market basket analysis, it will be important to understand and be able to clearly articulate the fact that, as more data is received, product relationships can, and most likely will, shift. Before discussing the specific cleaning process required for this dataset, let's load the online retail dataset.</p>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor202"/>Exercise 41: Loading Data</h3>
			<p>In this exercise, we will load and view an example online retail dataset. This dataset is originally from the UCI Machine Learning Repository and can be found at <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45"><span class="Hyperlink">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45</span></a>. Once you have downloaded the dataset, save it and note the path. Now, let's proceed with the exercise. The output of this exercise is the transaction data that will be used in future modeling exercises and some exploratory figures to help us better understand the data with which we are working.</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="http://archive.ics.uci.edu/ml/datasets/online+retail"><span class="Hyperlink">http://archive.ics.uci.edu/ml/datasets/online+retail#</span></a>. It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45"><span class="Hyperlink">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45</span></a>. Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012.</p>
			<p class="callout">UCI Machine Learning Repository [<span class="Hyperlink">http://archive.ics.uci.edu/ml</span>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Using the <strong class="inline">read_excel</strong> function from <strong class="inline">pandas</strong>, load the data. Note that the first row of the Excel file contains the column names:<p class="snippet">online = pandas.read_excel(</p><p class="snippet">    io="~/Desktop/Online Retail.xlsx", </p><p class="snippet">    sheet_name="Online Retail", </p><p class="snippet">    header=0</p><p class="snippet">)</p><h4>Note</h4><p class="callout">The path to <strong class="inline">Online Retail.xlsx</strong> should be changed as per the location of the file on your system.</p></li>
				<li>Print out the first 10 rows of the DataFrame. Notice that the data contains some columns that will not be relevant to market basket analysis:<p class="snippet">online.head(10)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer333"><img alt="Figure 8.11: The raw online retail data &#13;&#10;" src="image/C12626_08_11.jpg"/></div><h6>Figure 8.11: The raw online retail data </h6></li>
				<li>Print out the data type for each column in the DataFrame. This information will come in handy when trying to perform specific cleaning tasks:<p class="snippet">online.dtypes</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer334"><img alt="Figure 8.12: Data type for each column in the dataset&#13;&#10;" src="image/C12626_08_12.jpg"/></div><h6>Figure 8.12: Data type for each column in the dataset</h6></li>
				<li>Get the dimensions of the DataFrame, as well as the number of unique invoice numbers and customer identifications:<p class="snippet">print(</p><p class="snippet">     "Data dimension (row count, col count): {dim}"</p><p class="snippet">     .format(dim=online.shape)</p><p class="snippet">)</p><p class="snippet">print(</p><p class="snippet">     "Count of unique invoice numbers: {cnt}"</p><p class="snippet">     .format(cnt=online.InvoiceNo.nunique())</p><p class="snippet">)</p><p class="snippet">print(</p><p class="snippet">     "Count of unique customer ids: {cnt}"</p><p class="snippet">     .format(cnt=online.CustomerID.nunique())</p><p class="snippet">)</p><p>The output is as follows:</p><p class="snippet">Data dimension (row count, col count): (541909, 8)</p><p class="snippet">Count of unique invoice numbers: 25900</p><p class="snippet">Count of unique customer ids: 4372</p></li>
			</ol>
			<p>In this exercise, we have loaded the data and performed some exploratory work.</p>
			<h3 id="_idParaDest-181"><a id="_idTextAnchor203"/>Data Cleaning and Formatting</h3>
			<p>With the dataset now loaded, let's delve into the specific data cleaning processes to be performed. Since we are going to filter the data down to just the invoice numbers and items, we focus the data cleaning on these two columns of the dataset. Remember that market basket analysis looks to identify associations between the items purchased by all customers over time. As such, the main focus of the data cleaning involves removing transactions with non-positive numbers of items. This could happen when the transaction involves voiding another transaction, when items are returned, or when the transaction is some administrative task. These types of transactions will be filtered out in two ways. The first is that canceled transactions have invoice numbers that are prefaced with "C," so we will identify those specific invoice numbers and remove them from the data. The other approach is to remove all transactions with either zero or negative numbers of items. After performing these two steps, the data will be subset down to just the invoice number and item description columns, and any row of the now two-column dataset with at least one missing value is removed.</p>
			<p>The next stage of the data cleaning exercise involves putting the data in the appropriate format for modeling. In this and subsequent exercises, we will use a subset of the full data. The subset will be done by taking the first 5,000 unique invoice numbers. Once we have cut the data down to the first 5,000 unique invoice numbers, we change the data structure to that needed to run the models. Note that the data is currently in long format, where each item is on its own row. The desired format is a list of lists, like the made-up data from earlier in the chapter. Each subset list represents a unique invoice number, so in this case, the outer list should contain 5,000 sub-lists. The elements of the sub-lists are all the items belonging to the invoice number that that sub-list represents. With the cleaning process described, let's proceed to the exercise.</p>
			<h3 id="_idParaDest-182"><a id="_idTextAnchor204"/>Exercise 42: Data Cleaning and Formatting</h3>
			<p>In this exercise, we will perform the cleaning steps described previously. As we work through the process, the evolution of the data will be monitored by printing out the current state of the data and computing some basic summary metrics. Be sure to perform data cleaning in the same notebook in which the data is loaded.</p>
			<ol>
				<li value="1">Create an indicator column stipulating whether the invoice number begins with "<strong class="inline">C</strong>":<p class="snippet">online['IsCPresent'] = (</p><p class="snippet">    online['InvoiceNo']</p><p class="snippet">    .astype(str)</p><p class="snippet">    .apply(lambda x: 1 if x.find('C') != -1 else 0)</p><p class="snippet">)</p></li>
				<li>Filter out all transactions having either zero or a negative number of items, remove all invoice numbers starting with "C" using the column created in step one, subset the DataFrame down to <strong class="inline">InvoiceNo</strong> and <strong class="inline">Description</strong>, and lastly, drop all rows with at least one missing value. Rename the DataFrame <strong class="inline">online1</strong>:<p class="snippet">online1 = (</p><p class="snippet">    online</p><p class="snippet">    # filter out non-positive quantity values</p><p class="snippet">    .loc[online["Quantity"] &gt; 0]</p><p class="snippet">    # remove InvoiceNos starting with C</p><p class="snippet">    .loc[online['IsCPresent'] != 1]</p><p class="snippet">    # column filtering</p><p class="snippet">    .loc[:, ["InvoiceNo", "Description"]]</p><p class="snippet">    # dropping all rows with at least one missing value</p><p class="snippet">    .dropna()</p><p class="snippet">)</p></li>
				<li>Print out the first 10 rows of the filtered DataFrame, <strong class="inline">online1</strong>:<p class="snippet">online1.head(10)</p><div class="IMG---Figure" id="_idContainer335"><img alt="Figure 8.13: The cleaned online retail dataset&#13;&#10;" src="image/C12626_08_13.jpg"/></div><h6>Figure 8.13: The cleaned online retail dataset</h6></li>
				<li>Print out the dimensions of the cleaned DataFrame and the number of unique invoice numbers:<p class="snippet">print(</p><p class="snippet">    "Data dimension (row count, col count): {dim}"</p><p class="snippet">    .format(dim=online1.shape)</p><p class="snippet">)</p><p class="snippet">print(</p><p class="snippet">    "Count of unique invoice numbers: {cnt}"</p><p class="snippet">    .format(cnt=online1.InvoiceNo.nunique())</p><p class="snippet">)</p><p>The output is as follows:</p><p class="snippet">Data dimension (row count, col count): (530693, 2)</p><p class="snippet">Count of unique invoice numbers: 20136</p><p>Notice that we have already removed approximately 10,000 rows and 5,800 invoice numbers.</p></li>
				<li>Extract the invoice numbers from the DataFrame as a list. Remove duplicate elements to create a list of unique invoice numbers. Confirm that the process was successful by printing the length of the list of unique invoice numbers. Compare with the output of <em class="italics">Step 4</em>:<p class="snippet">invoice_no_list = online1.InvoiceNo.tolist()</p><p class="snippet">invoice_no_list = list(set(invoice_no_list))</p><p class="snippet">print(</p><p class="snippet">    "Length of list of invoice numbers: {ln}"</p><p class="snippet">    .format(ln=len(invoice_no_list))</p><p class="snippet">)</p><p>The output is as follows:</p><p class="snippet">Length of list of invoice numbers: 20136</p></li>
				<li>Take the list from step five and cut it to only include the first 5,000 elements. Print out the length of the new list to confirm that it is, in fact, the expected length of 5,000:<p class="snippet">subset_invoice_no_list = invoice_no_list[0:5000]</p><p class="snippet">print(</p><p class="snippet">    "Length of subset list of invoice numbers: {ln}"</p><p class="snippet">    .format(ln=len(subset_invoice_no_list))</p><p class="snippet">)</p><p>The output is as follows:</p><p class="snippet">Length of subset list of invoice numbers: 5000</p></li>
				<li>Filter the <strong class="inline">online1</strong> DataFrame down by only keeping the invoice numbers in the list from the previous step:<p class="snippet">online1 = online1.loc[online1["InvoiceNo"].isin(subset_invoice_no_list)]</p></li>
				<li>Print out the first 10 rows of <strong class="inline">online1</strong>:<p class="snippet">online1.head(10)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer336"><img alt="Figure 8.14: The cleaned dataset with only 5,000 unique invoice numbers&#13;&#10;" src="image/C12626_08_14.jpg"/></div><h6>Figure 8.14: The cleaned dataset with only 5,000 unique invoice numbers</h6></li>
				<li>Print out the dimensions of the DataFrame and the number of unique invoice numbers to confirm that the filtering and cleaning process was successful:<p class="snippet">print(</p><p class="snippet">    "Data dimension (row count, col count): {dim}"</p><p class="snippet">    .format(dim=online1.shape)</p><p class="snippet">)</p><p class="snippet">print(</p><p class="snippet">    "Count of unique invoice numbers: {cnt}"</p><p class="snippet">    .format(cnt=online1.InvoiceNo.nunique())</p><p class="snippet">)</p><p>The output is as follows:</p><p class="snippet">Data dimension (row count, col count): (129815, 2)</p><p class="snippet">Count of unique invoice numbers: 5000</p></li>
				<li>Transform the data in <strong class="inline">online1</strong> into the aforementioned list of lists called <strong class="inline">invoice_item_list</strong>. The process for doing this is to iterate over the unique invoice numbers and, at each iteration, extract the item descriptions as a list and append that list to the larger <strong class="inline">invoice_item_list</strong> list. Print out elements one through four of the list:<p class="snippet">invoice_item_list = []</p><p class="snippet">for num in list(set(online1.InvoiceNo.tolist())):</p><p class="snippet">    # filter dataset down to one invoice number</p><p class="snippet">    tmp_df = online1.loc[online1['InvoiceNo'] == num]</p><p class="snippet">    # extract item descriptions and convert to list</p><p class="snippet">    tmp_items = tmp_df.Description.tolist()</p><p class="snippet">    # append list invoice_item_list</p><p class="snippet">    invoice_item_list.append(tmp_items)</p><p class="snippet">    </p><p class="snippet">print(invoice_item_list[1:5])</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer337">
					<img alt="Figure 8.15: Four elements of the list of lists, where each sub-list contains all the items belonging to an individual invoice&#13;&#10;" src="image/C12626_08_15.jpg"/>
				</div>
			</div>
			<h6>Figure 8.15: Four elements of the list of lists, where each sub-list contains all the items belonging to an individual invoice</h6>
			<h4>Note</h4>
			<p class="callout">This step can take some minutes to complete.</p>
			<h3 id="_idParaDest-183"><a id="_idTextAnchor205"/>Data Encoding</h3>
			<p>While cleaning the data is crucial, the most important part of the data preparation process is molding the data into the correct form. Before running the models, the data, currently in the list of lists form, needs to be encoded and recast as a DataFrame. To do this, we will leverage <strong class="inline">TransactionEncoder</strong> from the <strong class="inline">preprocessing</strong> module of <strong class="inline">mlxtend</strong>. Outputted from the encoder is a multidimensional array, where each row is the length of the total number of unique items in the transaction dataset and the elements are Boolean variables, indicating whether that particular item is linked to the invoice number that row represents. With the data encoded, we can recast it as a DataFrame where the rows are the invoice numbers and the columns are the unique items in the transaction dataset.</p>
			<p>In the following exercise, the data encoding will be done using <strong class="inline">mlxtend</strong>, but it is very easy to encode the data without using any package. The first step is to unlist the list of lists and return one list with every value from the original list of lists. Next, the duplicate products are filtered out and, if preferred, the data is sorted in alphabetical order. Before doing the actual encoding, we initialize the final DataFrame by having all elements equal to false, a number of rows equal to the number of invoice numbers in the dataset, and column names equal to the non-duplicated list of product names. </p>
			<p>In this case, we have 5,000 transactions and over 3,100 unique products. Thus, the DataFrame has over 15,000,000 elements. The actual encoding is done by looping over each transaction and each item in each transaction. Change the row <em class="italics">i</em> and column <em class="italics">j</em> cell values in the initialized dataset from false to true if the <img alt="" src="image/C12626_Formula_08_23.png"/> transaction contains the <img alt="" src="image/C12626_Formula_08_24.png"/> product. This double loop is not fast as we need to iterate over 15,000,000 cells. There are ways to improve performance, including some that have been implemented in <strong class="inline">mlxtend</strong>, but to better understand the process, it is helpful to work through the double loop methodology. The following is an example function to do the encoding from scratch without the assistance of any package other than <strong class="inline">pandas</strong>:</p>
			<p class="snippet">def manual_encoding(ll):</p>
			<p class="snippet">    # unlist the list of lists input</p>
			<p class="snippet">    # result is one list with all the elements of the sublists</p>
			<p class="snippet">    list_dup_unsort_items = [element for sub in ll for element in sub]</p>
			<p class="snippet">    # two cleaning steps:</p>
			<p class="snippet">    #     1. remove duplicate items, only want one of each item in list</p>
			<p class="snippet">    #     2. sort items in alphabetical order</p>
			<p class="snippet">    list_nondup_sort_items = sorted(list(set(list_dup_unsort_items)))</p>
			<p class="snippet">    </p>
			<p class="snippet">    # initialize DataFrame with all elements having False value</p>
			<p class="snippet">    # name the columns the elements of list_dup_unsort_items</p>
			<p class="snippet">    manual_df = pandas.DataFrame(</p>
			<p class="snippet">        False, </p>
			<p class="snippet">        index=range(len(ll)), </p>
			<p class="snippet">        columns=list_dup_unsort_items</p>
			<p class="snippet">    )</p>
			<p class="snippet">    </p>
			<p class="snippet">    # change False to True if element is in individual transaction list</p>
			<p class="snippet">    # each row is represents the contains of an individual transaction</p>
			<p class="snippet">    # (sublist from the original list of lists)</p>
			<p class="snippet">    for i in range(len(ll)):</p>
			<p class="snippet">        for j in ll[i]:</p>
			<p class="snippet">            manual_df.loc[i, j] = True</p>
			<p class="snippet">    </p>
			<p class="snippet">    # return the True/False DataFrame</p>
			<p class="snippet">    return manual_df</p>
			<h3 id="_idParaDest-184"><a id="_idTextAnchor206"/>Exercise 43: Data Encoding</h3>
			<p>In this exercise, we continue the data preparation process by taking the list of lists generated in the previous exercise and encoding the data in the specific way required to run the models.</p>
			<ol>
				<li value="1">Initialize and fit the transaction encoder. Print out an example of the resulting data:<p class="snippet">online_encoder = mlxtend.preprocessing.TransactionEncoder()</p><p class="snippet">online_encoder_array = online_encoder.fit_transform(invoice_item_list)</p><p class="snippet">print(online_encoder_array)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer340"><img alt="Figure 8.16: The multi-dimensional array containing the Boolean variables indicating product presence in each transaction&#13;&#10;" src="image/C12626_08_16.jpg"/></div><h6>Figure 8.16: The multi-dimensional array containing the Boolean variables indicating product presence in each transaction</h6></li>
				<li>Recast the encoded array as a DataFrame named <strong class="inline">online_encoder_df</strong>. Print out a predefined subset of the DataFrame that features both true and false values:<p class="snippet">online_encoder_df = pandas.DataFrame(</p><p class="snippet">    online_encoder_array, </p><p class="snippet">    columns=online_encoder.columns_</p><p class="snippet">)</p><p class="snippet"># this is a very big table, so for more </p><p class="snippet"># easy viewing only a subset is printed</p><p class="snippet">online_encoder_df.loc[</p><p class="snippet">    4970:4979, </p><p class="snippet">    online_encoder_df.columns.tolist()[0:8]</p><p class="snippet">]</p><p>The output will be similar to the following:</p><div class="IMG---Figure" id="_idContainer341"><img alt="Figure 8.17: A small section of the encoded data recast as a DataFrame&#13;&#10;" src="image/C12626_08_17.jpg"/></div><h6> </h6><h6>Figure 8.17: A small section of the encoded data recast as a DataFrame</h6></li>
				<li>Print out the dimensions of the encoded DataFrame. It should have 5,000 rows because the data used to generate it was previously filtered down to 5,000 unique invoice numbers:<p class="snippet">print(</p><p class="snippet">    "Data dimension (row count, col count): {dim}"</p><p class="snippet">    .format(dim=online_encoder_df.shape)</p><p class="snippet">)</p><p>The output will be similar to the following:</p><p class="snippet">Data dimension (row count, col count): (5000, 3334)</p></li>
			</ol>
			<p>The data is now prepped for modeling. In the next section, we will explore the Apriori algorithm.</p>
			<h3 id="_idParaDest-185"><a id="_idTextAnchor207"/>Activity 18: Loading and Preparing Full Online Retail Data</h3>
			<p>In this activity, we are charged with loading and preparing a large transaction dataset for modeling. The final output will be an appropriately encoded dataset that has one row for each unique transaction in the dataset, and one column for each unique item in the dataset. If an item appears in an individual transaction, that element of the DataFrame will be marked true.</p>
			<p>This activity will largely repeat the last few exercises, but will use the complete online retail dataset file. No new downloads need to be executed, but you will need the path to the file downloaded previously. Perform this activity in a separate Jupyter notebook.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Load the online retail dataset file:<h4>Note</h4><p class="callout">This dataset is taken from <a href="http://archive.ics.uci.edu/ml/datasets/online+retail"><span class="Hyperlink">http://archive.ics.uci.edu/ml/datasets/online+retail#</span></a>. It can be downloaded from <span class="Hyperlink">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20</span>. Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012.</p><p class="callout">UCI Machine Learning Repository [<span class="Hyperlink">http://archive.ics.uci.edu/ml</span>]. Irvine, CA: University of California, School of Information and Computer Science.</p></li>
				<li>Clean and prep the data for modeling, including turning the cleaned data into a list of lists.</li>
				<li>Encode the data and recast it as a DataFrame:<h4>Note</h4><p class="callout">The solution for this activity can be found on page 366.</p></li>
			</ol>
			<p>The output will be similar to the following:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer342">
					<img alt="Figure 8.18: A subset of the cleaned, encoded, and recast DataFrame built from the complete online retail dataset&#13;&#10;" src="image/C12626_08_18.jpg"/>
				</div>
			</div>
			<h6>Figure 8.18: A subset of the cleaned, encoded, and recast DataFrame built from the complete online retail dataset</h6>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor208"/>Apriori Algorithm</h2>
			<p>The <strong class="keyword">Apriori</strong> algorithm is a data mining methodology for identifying and quantifying frequent item sets in transaction data, and is the foundational component of association rule learning. Extending the results of the Apriori algorithm to association rule learning will be discussed in the next section. The minimum value to qualify as frequent in the Apriori algorithm is an input into the model and, as such, is adjustable. Frequency is quantified here as support, so the value inputted into the model is the minimum support acceptable for the analysis being done. The model then identifies all item sets whose support is greater than, or equal to, the minimum support provided to the model. Note that the minimum support parameter is not a parameter that can be optimized via a grid search because there is no evaluation metric for the Apriori algorithm. Instead, the minimum support parameter is set based on the data, the use case, and domain expertise.</p>
			<p>The main idea behind the Apriori algorithm is the Apriori principle: any subset of a frequent item set must itself be frequent.</p>
			<p>Another aspect worth mentioning is the corollary: no superset of an infrequent item set can be frequent.</p>
			<p>Let's take some examples. If the item set {hammer, saw, and nail} is frequent, then, according to the Apriori principle and what is hopefully obvious, any less complex item set, say {hammer, saw}, is also frequent. On the contrary, if that same item set, {hammer, saw, nail}, is infrequent, then adding complexity, such as incorporating wood in the item set {hammer, saw, nail, wood}, is not going to result in the item set becoming frequent.</p>
			<p>It might seem straightforward to calculate the support value for every item set in a transactional database and only return those item sets whose support is greater than or equal to the prespecified minimum support threshold, but it is not because of the number of computations that need to happen. For example, take an item set with 10 unique items. This would result in 1,023 individual item sets for which support would need to be calculated. Now, try to extrapolate out to our working dataset that has 3,135 unique items. That is going to be an enormous number of item sets for which we need to compute a support value. Computational efficiency is a major issue:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer343">
					<img alt="Figure 8.19: A mapping of how item sets are built and how the Apriori principle can greatly decrease the computational requirements (all the grayed-out nodes are infrequent)&#13;&#10;" src="image/C12626_08_19.jpg"/>
				</div>
			</div>
			<h6>Figure 8.19: A mapping of how item sets are built and how the Apriori principle can greatly decrease the computational requirements (all the grayed-out nodes are infrequent)</h6>
			<p>In order to address the computational demands, the Apriori algorithm is defined as a bottom-up model that has two steps. These steps involve generating candidate item sets by adding items to already existing frequent item sets and testing these candidate item sets against the dataset to determine whether these candidate datasets are also frequent. No support value is computed for item sets that contain infrequent item sets. This process repeats until no further candidate item sets exist:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer344">
					<img alt="Figure 8.20: Assuming a minimum support threshold of 0.4, the diagram shows the general Apriori algorithm structure&#13;&#10;" src="image/C12626_08_20.jpg"/>
				</div>
			</div>
			<h6>Figure 8.20: Assuming a minimum support threshold of 0.4, the diagram shows the general Apriori algorithm structure</h6>
			<p>The preceding structure includes establishing an item set, computing support values, filtering out infrequent item sets, creating new item sets, and repeating the process.</p>
			<p>There is a clear tree-like structure that serves as the path for identifying candidate item sets. The specific search technique used, which was built for traversing tree-like data structures, is called a breadth-first search, which means that each step of the search process focuses on completely searching one level of the tree before moving on instead of searching branch by branch.</p>
			<p>The high-level steps of the algorithm are to:</p>
			<ol>
				<li value="1">Define the set of frequent items. To start, this is typically the set of individual items.</li>
				<li>Derive candidate item sets by combining frequent item sets together. Move up in size one item at a time. That is, go from item sets with one item to two, two to three, and so on.</li>
				<li>Compute the support value for each candidate item set.</li>
				<li>Create a new frequent item set made up of the candidate item sets whose support value exceeded the specified threshold. </li>
			</ol>
			<p>Repeat <em class="italics">Steps 1</em> to <em class="italics">4</em> until there are no more frequent item sets; that is, until we have worked through all the combinations.</p>
			<p>The pseudo code for the Apriori algorithm is as follows:</p>
			<p class="snippet">L1 = {frequent items}</p>
			<p class="snippet">For k = 1 and L1 != empty set do</p>
			<p class="snippet">    Ck+1 = candidate item sets derived from Lk</p>
			<p class="snippet">    For each transaction t in the dataset do</p>
			<p class="snippet">        Increment the count of the candidates in Ck+1 that appear in t</p>
			<p class="snippet">    Compute the support for the candidates in Ck+1 using the appearance counts</p>
			<p class="snippet">    Lk+1 = the candidates in Ck+1 meeting the minimum support requirement</p>
			<p class="snippet">        End</p>
			<p class="snippet">Return L = UkLk = all frequent item sets with corresponding support values</p>
			<p>Despite the Apriori principle, this algorithm can still face significant computational challenges depending on the size of the transaction dataset. There are several strategies currently accepted to further reduce the computational demands.</p>
			<h3 id="_idParaDest-187"><a id="_idTextAnchor209"/>Computational Fixes</h3>
			<p>Transaction reduction is an easy way to reduce the computational load. Note that after each candidate set of item sets is generated, the entirety of the transaction data needs to be scanned in order to count the number of appearances of each candidate item set. If we could shrink the size of the transaction dataset, the size of the dataset scans would decrease dramatically. The shrinking of the transaction dataset is done by realizing that any transaction containing no frequent item sets in the <em class="italics">ith</em> iteration is not going to contain any frequent item sets in subsequent iterations. Therefore, once each transaction contains no frequent item sets, it can be removed from the transaction dataset used for future scans.</p>
			<p>Sampling the transaction dataset and testing each candidate item set against it is another approach to reducing the computational requirements associated with scanning the transaction dataset to calculate the support of each item set. When this approach is implemented it is important to lower the minimum support requirement to guarantee that no item sets that should be present in the final data are left out. Given that the sampled transaction dataset will naturally cause the support values to be smaller, leaving the minimum support at its original value will incorrectly remove what should be frequent item sets from the output of the model.</p>
			<p>A similar approach is partitioning. In this case, the dataset is partitioned into several individual datasets on which the evaluation of each candidate item set is executed. Item sets are deemed frequent in the full transaction dataset if frequent in one of the partitions. Each partition is scanned consecutively until frequency for an item set is established.</p>
			<p>Regardless of whether or not one of these techniques is employed, the computational requirements are always going to be fairly substantial when it comes to the Apriori algorithm. As should now be clear, the essence of the algorithm, the computation of support, is not as complex as other models discussed in this text.</p>
			<h3 id="_idParaDest-188"><a id="_idTextAnchor210"/>Exercise 44: Executing the Apriori algorithm</h3>
			<p>The execution of the Apriori algorithm is made easy with <strong class="inline">mlxtend</strong>. As a result, this exercise will focus on how to manipulate the outputted dataset and to interpret the results. You will recall that the cleaned and encoded transaction data was defined as <strong class="inline">online_encoder_df</strong>. Perform this exercise in the same notebook that all previous exercises were run as we will continue using the environment, data, and results already established in that notebook. (So, you should be using the notebook that contains the reduced dataset of 5,000 entries, not the full dataset as used in the activity.)</p>
			<ol>
				<li value="1">Run the Apriori algorithm using <strong class="inline">mlxtend</strong> without changing any of the default parameter values:<p class="snippet">mod = mlxtend.frequent_patterns.apriori(online_encoder_df)</p><p class="snippet">mod</p><p>The output is an empty DataFrame. The default minimum support value is set to 0.5, so since an empty DataFrame was returned, we know that all item sets have a support of less than 0.5. Depending on the number of transactions and the diversity of available items, having no item set with a plus 0.5 support is not unusual.</p></li>
				<li>Rerun the Apriori algorithm, but with the minimum support set to 0.01. This minimum support value is the same as saying that when analyzing 5,000 transactions, we need an item set to appear 50 times to be considered frequent. As mentioned previously, the minimum support can be set to any value in the range [0,1]. There is no best minimum support value; the setting of this value is entirely subjective. Many businesses have their own specific thresholds for significance, but there is no industry standard or method for optimizing this value: <p class="snippet">mod_minsupport = mlxtend.frequent_patterns.apriori(</p><p class="snippet">    online_encoder_df,</p><p class="snippet">    min_support=0.01</p><p class="snippet">)</p><p class="snippet">mod_minsupport.loc[0:6]</p><p>The output will be similar to the following:</p><div class="IMG---Figure" id="_idContainer345"><img alt="Figure 8.21: Basic output of the Apriori algorithm run using mlxtend&#13;&#10;" src="image/C12626_08_21.jpg"/></div><h6>Figure 8.21: Basic output of the Apriori algorithm run using mlxtend</h6><p>Notice that the item sets are designated numerically in the output, which makes the results hard to interpret.</p></li>
				<li>Rerun the Apriori algorithm with the same minimum support as in <em class="italics">Step 2</em>, but this time set <strong class="inline">use_colnames</strong> to True. This will replace the numerical designations with the actual item names:<p class="snippet">mod_colnames_minsupport = mlxtend.frequent_patterns.apriori(</p><p class="snippet">    online_encoder_df, </p><p class="snippet">    min_support=0.01,</p><p class="snippet">    use_colnames=True</p><p class="snippet">)</p><p class="snippet">mod_colnames_minsupport.loc[0:6]</p><p>The output will be similar to the following:</p><div class="IMG---Figure" id="_idContainer346"><img alt="Figure 8.22: The output of the Apriori algorithm with the actual item names instead of numerical designations&#13;&#10;" src="image/C12626_08_22.jpg"/></div><h6>Figure 8.22: The output of the Apriori algorithm with the actual item names instead of numerical designations</h6><p>This DataFrame contains every item set whose support value is greater than the specified minimum support value. That is, these item sets occur with sufficient frequency to potentially be meaningful and therefore actionable.</p></li>
				<li>Add an additional column to the output of <em class="italics">Step 3</em> that contains the size of the item set, which will help with filtering and further analysis:<p class="snippet">mod_colnames_minsupport['length'] = (</p><p class="snippet">    mod_colnames_minsupport['itemsets'].apply(lambda x: len(x))</p><p class="snippet">)</p><p class="snippet">mod_colnames_minsupport.loc[0:6]</p><p>The output will be similar to the following:</p><div class="IMG---Figure" id="_idContainer347"><img alt="" src="image/C12626_08_23.jpg"/></div><h6>Figure 8.23: The Apriori algorithm output plus an additional column containing the lengths of the item sets</h6></li>
				<li>Find the support of the item set containing '<strong class="inline">10 COLOUR SPACEBOY PEN</strong>':<p class="snippet">mod_colnames_minsupport[</p><p class="snippet">    mod_colnames_minsupport['itemsets'] == frozenset(</p><p class="snippet">        {'10 COLOUR SPACEBOY PEN'}</p><p class="snippet">    )</p><p class="snippet">]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer348"><img alt="Figure 8.24: The output DataFrame filtered down to a single item set&#13;&#10;" src="image/C12626_08_24.jpg"/></div><h6>Figure 8.24: The output DataFrame filtered down to a single item set</h6><p>This single row DataFrame gives us the support value for this specific item set that contains one item. The support value says that this specific item set appears in 1.5% of the transactions.</p></li>
				<li>Return all item sets of length 2 whose support is in the range [0.02, 0.021]<p class="snippet">mod_colnames_minsupport[</p><p class="snippet">    (mod_colnames_minsupport['length'] == 2) &amp; </p><p class="snippet">    (mod_colnames_minsupport['support'] &gt;= 0.02) &amp;</p><p class="snippet">    (mod_colnames_minsupport['support'] &lt; 0.021)</p><p class="snippet">] </p><p>The output will be similar to the following:</p><div class="IMG---Figure" id="_idContainer349"><img alt="Figure 8.25: The Apriori algorithm output DataFrame filtered by length and support&#13;&#10;" src="image/C12626_08_25.jpg"/></div><h6>Figure 8.25: The Apriori algorithm output DataFrame filtered by length and support</h6><p>This DataFrame contains all the item sets (pairs of items bought together) whose support value is in the range specified at the start of the step. Each of these item sets appears in between 2.0% and 2.1% of transactions.</p><p>Note that when filtering on <strong class="inline">support</strong>, it is wise to specify a range instead of a specific value since it is quite possible to pick a value for which there are no item sets. The preceding output has 18 item sets. Keep note of that and the particular items in the item sets because we will be running this same filter when we scale up to the full data and we will want to execute a comparison.</p></li>
				<li>Plot the support values. Note that this plot will have no support values less than 0.01 because that was the value used as the minimum support:<p class="snippet">mod_colnames_minsupport.hist("support", grid=False, bins=30)</p><p class="snippet">plt.title("Support")</p><p>The output will be similar to the following plot:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer350">
					<img alt="Figure 8.26: Distribution of the support values returned by the Apriori algorithm&#13;&#10;" src="image/C12626_08_26.jpg"/>
				</div>
			</div>
			<h6>Figure 8.26: Distribution of the support values returned by the Apriori algorithm</h6>
			<p>The maximum support value is approximately 0.14, which is approximately 700 transactions. What might appear to be a small value may not be given the number of products available. Larger numbers of products tend to result in lower support values because the variability of item combinations increases.</p>
			<p>Hopefully, you can think of more ways in which this data could be used and with a view to supporting retail businesses. We will generate even more useful information in the next section by using the Apriori algorithm results to generate association rules.</p>
			<h3 id="_idParaDest-189">Activity 19: Apriori on the Complete Online Retail<a id="_idTextAnchor211"/> Dataset</h3>
			<p>Imagine you work for an online retailer. You are given all the transaction data from the last month and told to find all the item sets appearing in at least 1% of the transactions. Once the qualifying item sets are identified, you are subsequently told to identify the distribution of the support values. The distribution of support values will tell all interested parties whether groups of items exist that are purchased together with high probability as well as the mean of the support values. Let's collect all the information for the company leadership and strategists.</p>
			<p>In this activity, you will run the Apriori algorithm on the full online retail dataset. </p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="http://archive.ics.uci.edu/ml/datasets/online+retail"><span class="Hyperlink">http://archive.ics.uci.edu/ml/datasets/online+retail#</span></a>. It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20"><span class="Hyperlink">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20</span></a>. Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012.</p>
			<p class="callout">UCI Machine Learning Repository [<span class="Hyperlink">http://archive.ics.uci.edu/ml</span>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<p>Ensure that you complete this activity in the same notebook as the previous activity (in other words, the notebook that uses the full dataset, not the notebook that uses the subset of 5,000 items that you're using for the exercises).</p>
			<p>This will also provide you with an opportunity to compare the results with those generated using only 5,000 transactions. This is an interesting activity, as it provides some insight into the ways in which the data may change as more data is collected, as well as some insight into how support values change when the partitioning technique is employed. Note that what was done in the exercises is not a perfect representation of the partitioning technique because 5,000 was an arbitrary number of transactions to sample.</p>
			<h4>Note</h4>
			<p class="callout">All the activities in this chapter need to be performed in <a id="_idTextAnchor212"/>the same notebook.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Run the Apriori algorithm on the full data with reasonable parameter settings.</li>
				<li>Filter the results down to the item set containing <strong class="inline">10 COLOUR SPACEBOY PEN</strong>. Compare the support value to that of <em class="italics">Exercise 44</em>, <em class="italics">Executing the Apriori algorithm</em>.</li>
				<li>Add another column containing the item set length. Then, filter down to those item sets whose length is two and whose support is in the range [0.02, 0.021]. Compare this to the result from <em class="italics">Exercise 44</em>, <em class="italics">Executing the Apriori algorithm</em>.</li>
				<li>Plot the <strong class="inline">support</strong> values.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 367.</p></li>
			</ol>
			<p>The output of this activity will be similar to the following:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer351">
					<img alt="Figure 8.27: Distribution of support values&#13;&#10;" src="image/C12626_08_27.jpg"/>
				</div>
			</div>
			<h6>Figure 8.27: Distribution of support values</h6>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor213"/>Association Rules</h2>
			<p>Association rule learning is a machine learning model that seeks to unearth the hidden patterns (in other words, relationships) in transaction data that describe the shopping habits of the customers of any retailer. The definition of an association rule was hinted at when the common probabilistic metrics were defined and explained previously. </p>
			<p>Consider the imaginary frequent item set {Milk, Bread}. Two association rules can be formed from that item set: Milk <img alt="" src="image/C12626_Formula_08_19.png"/> Bread and Bread <img alt="" src="image/C12626_Formula_08_19.png"/> Milk. For simplicity, the first item set in the association rule is referred to as the antecedent, while the second item set in the association rule is referred to as the consequent. Once the association rules have been identified, all the previously discussed metrics can be computed to evaluate the validity of the association rules determining whether or not the rules can be leveraged in the decision-making process.</p>
			<p>The establishment of an association rule is based on support and confidence. Support, as we discussed in the last section, identifies which item sets are frequent, while confidence measures the frequency of truthfulness for a particular rule. Confidence is typically referred to as the measure of interestingness, as it is the metric that determines whether an association should be formed. Thus, the establishment of an association rule is a two-step process. Identify frequent datasets and then evaluate the confidence of a candidate association rule and, if that confidence value exceeds some arbitrary threshold, the result is an association rule.</p>
			<p>A major issue of association rule learning is the discovery of spurious associations, which are highly likely given the huge numbers of potential rules. Spurious associations are defined as associations that occur with surprising regularity in the data given that the association occurs entirely by chance. To clearly articulate the idea, assume we are in a situation where we have 100 candidate rules. If we run a statistical test for independence at the 0.05 significance level, we are still faced with a 5% chance that an association is found when no association exists. Let's further assume that all 100 candidate rules are not valid associations. Given the 5% chance, we should still expect to find 5 valid association rules. Now scale the imaginary candidate rule list up to millions or billions, so that that 5% amounts to an enormous number of associations. This problem is not unlike the issue of statistical significance and error faced by virtually every model. It is worth calling out that some techniques exist to combat the spurious association issue, but they are neither consistently incorporated in the frequently used association rule libraries nor in the scope of this chapter.</p>
			<p>Let's now apply our working knowledge of association rule learning to the online retail dataset.</p>
			<h3 id="_idParaDest-191"><a id="_idTextAnchor214"/>Exercise 45: Deriving Association Rules</h3>
			<p>In this exercise, we will derive association rules for the online retail dataset and explore the associated metrics.  Ensure that you complete this exercise in the same notebook as the previous exercises (in other words, the notebook that uses the 5,000-item subset, not the full dataset from the activities).</p>
			<ol>
				<li value="1">Use the <strong class="inline">mlxtend</strong> library to derive association rules for the online retail dataset. Use confidence as the measure of interestingness, set the minimum threshold to 0.6, and return all the metrics, not just support. Count the number of returned association rules:<p class="snippet">rules = mlxtend.frequent_patterns.association_rules(</p><p class="snippet">    mod_colnames_minsupport, </p><p class="snippet">    metric="confidence",</p><p class="snippet">    min_threshold=0.6, </p><p class="snippet">    support_only=False</p><p class="snippet">)</p><p class="snippet">rules.loc[0:6]</p><p>The output is similar to the following:</p><div class="IMG---Figure" id="_idContainer354"><img alt="Figure 8.28: The first 7 rows of the association rules generated using only 5,000 transactions&#13;&#10;" src="image/C12626_08_28.jpg"/></div><h6>Figure 8.28: The first 7 rows of the association rules generated using only 5,000 transactions</h6></li>
				<li>Print the number of associations as follows:<p class="snippet">print("Number of Associations: {}".format(rules.shape[0]))</p><p>5,070 association rules were found.</p><h4>Note</h4><p class="callout">The number of association rules may differ.</p></li>
				<li>Try running another version of the model. Choose any minimum threshold and any measure of interestingness. Count and explore the returned rules:<p class="snippet">rules2 = mlxtend.frequent_patterns.association_rules(</p><p class="snippet">    mod_colnames_minsupport, </p><p class="snippet">    metric="lift",</p><p class="snippet">    min_threshold=50, </p><p class="snippet">    support_only=False</p><p class="snippet">)</p><p class="snippet">rules2.loc[0:6]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer355"><img alt="Figure 8.29: The first 7 rows of the association rules&#13;&#10;" src="image/C12626_08_29.jpg"/></div><h6>Figure 8.29: The first 7 rows of the association rules</h6></li>
				<li>Print the number of associations as follows:<p class="snippet">print("Number of Associations: {}".format(rules2.shape[0]))</p><p>The number of association rules found using the metric lift and the minimum threshold value of 50 is 26, which is significantly lower than in <em class="italics">Step 2</em>. We will see in the following that 50 is quite a high threshold value, so it is not surprising that we returned fewer association rules.</p></li>
				<li>Plot confidence against support and identify specific trends in the data:<p class="snippet">rules.plot.scatter("support", "confidence", alpha=0.5, marker="*")</p><p class="snippet">plt.xlabel("Support")</p><p class="snippet">plt.ylabel("Confidence")</p><p class="snippet">plt.title("Association Rules")</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer356"><img alt="Figure 8.30: A plot of confidence against support&#13;&#10;" src="image/C12626_08_30.jpg"/></div><h6>Figure 8.30: A plot of confidence against support</h6><p>Notice that there are not any association rules with both extremely high confidence and extremely high support. This should hopefully make sense. If an item set has high support, the items are likely to appear with many other items, making the chances of high confidence very low.</p></li>
				<li>Look at the distribution of confidence:<p class="snippet">rules.hist("confidence", grid=False, bins=30)</p><p class="snippet">plt.title("Confidence")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer357"><img alt="Figure 8.31: The distribution of confidence values&#13;&#10;" src="image/C12626_08_31.jpg"/></div><h6>Figure 8.31: The distribution of confidence values</h6></li>
				<li>Now, look at the distribution of lift:<p class="snippet">rules.hist("lift", grid=False, bins=30)</p><p class="snippet">plt.title("Lift")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer358"><img alt="Figure 8.32: The distribution of lift values&#13;&#10;" src="image/C12626_08_32.jpg"/></div><h6>Figure 8.32: The distribution of lift values</h6><p>As mentioned previously, this plot shows that 50 is a high threshold value in that there are not many points above that value.</p></li>
				<li>Now, look at the distribution of leverage:<p class="snippet">rules.hist("leverage", grid=False, bins=30)</p><p class="snippet">plt.title("Leverage")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer359"><img alt="Figure 8.33: The distribution of leverage values&#13;&#10;" src="image/C12626_08_33.jpg"/></div><h6>Figure 8.33: The distribution of leverage values</h6></li>
				<li>Now, look at the distribution of conviction:<p class="snippet">plt.hist(</p><p class="snippet">    rules[numpy.isfinite(rules['conviction'])].conviction.values, </p><p class="snippet">    bins = 30</p><p class="snippet">)</p><p class="snippet">plt.title("Conviction")</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer360">
					<img alt="Figure 8.34: The distribution of conviction values&#13;&#10;" src="image/C12626_08_34.jpg"/>
				</div>
			</div>
			<h6>Figure 8.34: The distribution of conviction values</h6>
			<p>What is interesting about the four distributions is that spikes of varying sizes appear at the upper ends of the plots, implying that there are a few very strong association rules. The distribution of confidence tails off as the confidence values get larger, but, at the very end, around the highest values, the distribution jumps up a little. The lift distribution has the most obvious spike. The conviction distribution plot shows a small spike, perhaps more accurately described as a bump, around 50. Lastly, the leverage distribution does not really show any spike in the higher values, but it does feature a long tail with some very high leverage values.</p>
			<p>Take some time to explore the association rules found by the model. Do the product pairings make sense to you? What happened to the number of association rules when you changed the model parameter values? Do you appreciate the impact that these rules would have when attempting to improve any retail business?</p>
			<h3 id="_idParaDest-192">Activity 20: Finding the Association Rules on the Complete O<a id="_idTextAnchor215"/>nline Retail Dataset</h3>
			<p>Let's pick up the scenario set out in <em class="italics">Activity 19</em> <em class="italics">Apriori on the Complete Online Retail Dataset</em>. The company leadership comes back to you and says it is great that we know how frequently each item set occurs in the dataset, but which item sets can we act upon? Which item sets can we use to change the store layout or adjust pricing? To find these answers, we derive the full association rules.</p>
			<p>In this activity, let's derive association rules from the complete online retail transaction dataset. Ensure that you complete this activity in the notebook that uses the full dataset (in other words, the notebook with the complete retail dataset, not the notebook from the exercises that uses the 5,000-item subset).</p>
			<p>These steps will help us to perform the activity:</p>
			<ol>
				<li value="1">Fit the association rule model on the full dataset. Use metric confidence and a minimum threshold of 0.6.</li>
				<li>Count the number of association rules. Is the number different to that found in <em class="italics">step 1</em> of Exercise 45, <em class="italics">Deriving Association Rules</em>?</li>
				<li>Plot confidence against support.</li>
				<li>Look at the distributions of confidence, lift, leverage, and conviction.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 370.</p></li>
			</ol>
			<p>By the end of this activity, you will have a plot of lift, leverage, and conviction.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor216"/>Summary</h2>
			<p>Market basket analysis is used to analyze and extract insights from transaction or transaction-like data that can be used to help drive growth in many industries, most famously the retail industry. These decisions can include how to layout the retail space, what products to discount, and how to price products. One of the central pillars of market basket analysis is the establishment of association rules. Association rule learning is a machine learning approach to uncovering the associations between the products individuals purchase that are strong enough to be leveraged in business decisions. Association rule learning relies on the Apriori algorithm to find frequent item sets in a computationally efficient way. These models are atypical of machine learning models because no prediction is being done, the results cannot really be evaluated using any one metric, and the parameter values are selected not by grid search, but by domain requirements specific to the question of interest. That being said, the goal of pattern extraction that is at the heart of all machine learning models is most definitely present here. At the conclusion of this chapter, you should feel comfortable evaluating and interpreting the probabilistic metrics, be able to run and adjust the Apriori algorithm and association rule learning model using <strong class="inline">mlxtend</strong>, and know how these models are applied in business. Know that there is a decent chance the positioning and pricing of items in your neighborhood grocery store were chosen based on the past actions made by you and many other customers in that store! </p>
			<p>In the next chapter, we explore hotspot analysis using kernel density estimation, arguably one of the most frequently used algorithms in all of statistics and machine learning.</p>
		</div>
		<div>
			<div class="Content" id="_idContainer362">
			</div>
		</div>
	</body></html>
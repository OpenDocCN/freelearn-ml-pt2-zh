["```py\ncross_val_score(SVC(), X_train, y_train, cv = 10, scoring='neg_log_loss')\n```", "```py\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:,2:]\ny = iris.target\n```", "```py\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,random_state = 7)\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\n\nkn_3 = KNeighborsClassifier(n_neighbors = 3)\nkn_5 = KNeighborsClassifier(n_neighbors = 5)\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\nkn_3_scores = cross_val_score(kn_3, X_train, y_train, cv=4)\nkn_5_scores = cross_val_score(kn_5, X_train, y_train, cv=4)\nkn_3_scores\n\narray([ 0.9 , 0.92857143, 0.92592593, 1\\. ])\n```", "```py\nkn_5_scores\n\narray([ 0.96666667, 0.96428571, 0.88888889, 1\\. ])\n```", "```py\nprint \"Mean of kn_3: \",kn_3_scores.mean()\nprint \"Mean of kn_5: \",kn_5_scores.mean()\n\nMean of kn_3: 0.938624338624\nMean of kn_5: 0.95496031746\n```", "```py\nprint \"Std of kn_3: \",kn_3_scores.std()\nprint \"Std of kn_5: \",kn_5_scores.std()\n\nStd of kn_3: 0.037152126551\nStd of kn_5: 0.0406755710299\n```", "```py\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8],[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 2, 1, 2, 1, 2, 1, 2])\n```", "```py\nfrom sklearn.model_selection import KFold\n\nkf= KFold(n_splits = 4)\n```", "```py\ncc = 1\nfor train_index, test_index in kf.split(X):\n print \"Round : \",cc,\": \",\n print \"Training indices :\", train_index,\n print \"Testing indices :\", test_index\n cc += 1\n\nRound 1 : Training indices : [2 3 4 5 6 7] Testing indices : [0 1]\nRound 2 : Training indices : [0 1 4 5 6 7] Testing indices : [2 3]\nRound 3 : Training indices : [0 1 2 3 6 7] Testing indices : [4 5]\nRound 4 : Training indices : [0 1 2 3 4 5] Testing indices : [6 7]\n```", "```py\nkf.get_n_splits()\n\n4\n```", "```py\nindices_list = list(kf.split(X))\n```", "```py\nindices_list[3] #the list is indexed from 0 to 3\n\n(array([0, 1, 2, 3, 4, 5], dtype=int64), array([6, 7], dtype=int64))\n```", "```py\ntrain_indices, test_indices = indices_list[3]\n\nX[train_indices]\n\narray([[1, 2],\n [3, 4],\n [5, 6],\n [7, 8],\n [1, 2],\n [3, 4]])\n\ny[train_indices]\n\narray([1, 2, 1, 2, 1, 2])\n```", "```py\nX[test_indices]\n\narray([[5, 6],\n [7, 8]])\n\ny[test_indices]\n\narray([1, 2])\n```", "```py\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8],[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 1, 1, 1, 2, 2, 2, 2])\n```", "```py\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits = 4)\n```", "```py\ncc = 1\nfor train_index, test_index in skf.split(X,y):\n print \"Round\",cc,\":\",\n print \"Training indices :\", train_index,\n print \"Testing indices :\", test_index\n cc += 1\n\nRound 1 : Training indices : [1 2 3 5 6 7] Testing indices : [0 4]\nRound 2 : Training indices : [0 2 3 4 6 7] Testing indices : [1 5]\nRound 3 : Training indices : [0 1 3 4 5 7] Testing indices : [2 6]\nRound 4 : Training indices : [0 1 2 4 5 6] Testing indices : [3 7]\n```", "```py\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits = 5,test_size=0.25)\n\ncc = 1\nfor train_index, test_index in sss.split(X,y):\n print \"Round\",cc,\":\",\n print \"Training indices :\", train_index,\n print \"Testing indices :\", test_index\n cc += 1\n\nRound 1 : Training indices : [1 6 5 7 0 2] Testing indices : [4 3]\nRound 2 : Training indices : [3 2 6 7 5 0] Testing indices : [1 4]\nRound 3 : Training indices : [2 1 4 7 0 6] Testing indices : [3 5]\nRound 4 : Training indices : [4 2 7 6 0 1] Testing indices : [5 3]\nRound 5 : Training indices : [1 2 0 5 4 7] Testing indices : [6 3]\nRound 6 : Training indices : [0 6 5 1 7 3] Testing indices : [2 4]\nRound 7 : Training indices : [1 7 3 6 2 5] Testing indices : [0 4]\n```", "```py\n%matplotlib inline\n\nimport numpy as np\ntrue_mean = 1000\ntrue_std = 10\nN = 1000\ndataset = np.random.normal(loc= true_mean, scale = true_std, size=N)\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize=(10, 7))\nax.hist(dataset, color='k', alpha=.65, histtype='stepfilled',bins=50)\nax.set_title(\"Histogram of dataset\")\n```", "```py\nholdout_set = dataset[:500]\nfitting_set = dataset[500:]\nestimate = fitting_set[:N/2].mean()\nestimate\n\n999.69789261486721\n```", "```py\ndata_mean = dataset.mean()\ndata_mean\n\n999.55177343767843\n```", "```py\nfrom sklearn.model_selection import ShuffleSplit\nshuffle_split = ShuffleSplit(n_splits=100, test_size=.5, random_state=0)\nmean_p = []\nestimate_closeness = []\nfor train_index, not_used_index in shuffle_split.split(fitting_set):\n mean_p.append(fitting_set[train_index].mean())\n shuf_estimate = np.mean(mean_p)\n estimate_closeness.append(np.abs(shuf_estimate - dataset.mean()))\n\nplt.figure(figsize=(10,5))\nplt.plot(estimate_closeness)\n```", "```py\nfrom sklearn.model_selection import TimeSeriesSplit\nimport numpy as np\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4],[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4, 1, 2, 3, 4])\n```", "```py\ntscv = TimeSeriesSplit(n_splits=7)\n```", "```py\nfor train_index, test_index in tscv.split(X):\n\n X_train, X_test = X[train_index], X[test_index]\n y_train, y_test = y[train_index], y[test_index]\n\n print \"Training indices:\", train_index, \"Testing indices:\", test_index\n\nTraining indices: [0] Testing indices: [1]\nTraining indices: [0 1] Testing indices: [2]\nTraining indices: [0 1 2] Testing indices: [3]\nTraining indices: [0 1 2 3] Testing indices: [4]\nTraining indices: [0 1 2 3 4] Testing indices: [5]\nTraining indices: [0 1 2 3 4 5] Testing indices: [6]\nTraining indices: [0 1 2 3 4 5 6] Testing indices: [7]\n```", "```py\ntscv_list = list(tscv.split(X))\n```", "```py\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:,2:]\ny = iris.target\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,random_state = 7)\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier()\n```", "```py\nparam_grid = {'n_neighbors': list(range(3,9,1))}\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\ngs = GridSearchCV(knn_clf,param_grid,cv=10)\n```", "```py\ngs.fit(X_train, y_train)\n```", "```py\ngs.best_params_\n\n{'n_neighbors': 3}\n\ngs.cv_results_['mean_test_score']\n\nzip(gs.cv_results_['params'],gs.cv_results_['mean_test_score'])\n\n[({'n_neighbors': 3}, 0.9553571428571429),\n ({'n_neighbors': 4}, 0.9375),\n ({'n_neighbors': 5}, 0.9553571428571429),\n ({'n_neighbors': 6}, 0.9553571428571429),\n ({'n_neighbors': 7}, 0.9553571428571429),\n ({'n_neighbors': 8}, 0.9553571428571429)]\n```", "```py\nall_scores = []\nfor n_neighbors in range(3,9,1):\n knn_clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n all_scores.append((n_neighbors, cross_val_score(knn_clf, X_train, y_train, cv=10).mean()))\n sorted(all_scores, key = lambda x:x[1], reverse = True)\n\n[(3, 0.95666666666666667),\n (5, 0.95666666666666667),\n (6, 0.95666666666666667),\n (7, 0.95666666666666667),\n (8, 0.95666666666666667),\n (4, 0.94000000000000006)]\n```", "```py\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:,2:]\ny = iris.target\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,random_state = 7)\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier()\n```", "```py\nparam_dist = {'n_neighbors': list(range(3,9,1))}\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\nrs = RandomizedSearchCV(knn_clf,param_dist,cv=10,n_iter=6)\n```", "```py\nrs.fit(X_train, y_train)\n```", "```py\nrs.best_params_\n\n{'n_neighbors': 3}\n\nzip(rs.cv_results_['params'],rs.cv_results_['mean_test_score'])\n\n[({'n_neighbors': 3}, 0.9553571428571429),\n ({'n_neighbors': 4}, 0.9375),\n ({'n_neighbors': 5}, 0.9553571428571429),\n ({'n_neighbors': 6}, 0.9553571428571429),\n ({'n_neighbors': 7}, 0.9553571428571429),\n ({'n_neighbors': 8}, 0.9553571428571429)]\n```", "```py\nparam_dist = {'n_neighbors': list(range(3,50,1))}\nrs = RandomizedSearchCV(knn_clf,param_dist,cv=10,n_iter=15)\nrs.fit(X_train,y_train)\n\nrs.best_params_\n\n{'n_neighbors': 16}\n```", "```py\n%timeit rs.fit(X_train,y_train)\n\n1 loop, best of 3: 1.06 s per loop\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_neighbors': list(range(3,50,1))}\ngs = GridSearchCV(knn_clf,param_grid,cv=10)\n\n%timeit gs.fit(X_train,y_train)\n\n1 loop, best of 3: 3.24 s per loop\n```", "```py\ngs.best_params_ \n\n{'n_neighbors': 3}\n```", "```py\nzip(gs.cv_results_['params'],gs.cv_results_['mean_test_score'])\n\n[({'n_neighbors': 3}, 0.9553571428571429),\n ({'n_neighbors': 4}, 0.9375),\n ...\n ({'n_neighbors': 14}, 0.9553571428571429),\n ({'n_neighbors': 15}, 0.9553571428571429),\n ({'n_neighbors': 16}, 0.9553571428571429),\n ({'n_neighbors': 17}, 0.9464285714285714),\n ...\n```", "```py\nimport pandas as pd\n\ndata_web_address = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n\ncolumn_names = ['pregnancy_x',\n'plasma_con',\n'blood_pressure',\n'skin_mm',\n'insulin',\n'bmi',\n'pedigree_func',\n'age',\n'target']\n\nfeature_names = column_names[:-1]\nall_data = pd.read_csv(data_web_address , names=column_names)\n```", "```py\nimport numpy as np\nimport pandas as pd\n\nX = all_data[feature_names]\ny = all_data['target']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nknn_clf = KNeighborsClassifier()\n\nparam_dist = {'n_neighbors': list(range(3,20,1))}\n\nrs = RandomizedSearchCV(knn_clf,param_dist,cv=10,n_iter=17)\nrs.fit(X_train, y_train)\n```", "```py\nrs.best_score_\n\n0.75407166123778502\n```", "```py\ny_pred = rs.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)\n\narray([[84, 16],\n [27, 27]])\n```", "```py\nfrom sklearn.metrics import recall_score\n\nrecall_score(y_test, y_pred)\n\n0.5\n```", "```py\nfrom sklearn.metrics import make_scorer\n\nrecall_scorer = make_scorer(recall_score, greater_is_better=True)\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nknn_clf = KNeighborsClassifier()\n\nparam_dist = {'n_neighbors': list(range(3,20,1))}\n\nrs = RandomizedSearchCV(knn_clf,param_dist,cv=10,n_iter=17,scoring=recall_scorer)\n\nrs.fit(X_train, y_train)\n```", "```py\nrs.best_score_\n\n0.5649632669176643\n```", "```py\ny_pred = rs.predict(X_test)\nrecall_score(y_test,y_pred)\n\n0.5\n```", "```py\nfrom sklearn.metrics import roc_auc_score\n\nrs = RandomizedSearchCV(knn_clf,param_dist,cv=10,n_iter=17,scoring=make_scorer(roc_auc_score,greater_is_better=True))\n\nrs.fit(X_train, y_train)\n\nrs.best_score_\n\n0.7100264217324479\n```", "```py\ncosts_array = confusion_matrix(y_test, y_pred) * np.array([[1,2],\n [100,20]])\ncosts_array\n\narray([[  84,   32],\n [2700,  540]])\n```", "```py\ncosts_array.sum()\n\n3356\n```", "```py\ndef costs_total(y_test, y_pred):\n\n return (confusion_matrix(y_test, y_pred) * np.array([[1,2],\n [100,20]])).sum()\n\ncosts_scorer = make_scorer(costs_total, greater_is_better=False)\n\nrs = RandomizedSearchCV(knn_clf,param_dist,cv=10,n_iter=17,scoring=costs_scorer)\n\nrs.fit(X_train, y_train)\n\nrs.best_score_\n\n-1217.5879478827362\n```", "```py\n costs_total(y_test,rs.predict(X_test)) \n\n3356\n```", "```py\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n\nX = boston.data\ny = boston.target\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n```", "```py\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nknn_reg = KNeighborsRegressor()\nparam_dist = {'n_neighbors': list(range(3,20,1))}\nrs = RandomizedSearchCV(knn_reg,param_dist,cv=10,n_iter=17)\nrs.fit(X_train, y_train)\nrs.best_score_\n\n0.46455839325055914\n```", "```py\nfrom sklearn.linear_model import Ridge\ncross_val_score(Ridge(),X_train,y_train,cv=10).mean()\n\n0.7439511908709866\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\ncross_val_score(GradientBoostingRegressor(max_depth=7),X_train,y_train,cv=10).mean()\n\n0.83082671732165492\n```", "```py\ncross_val_score(RandomForestRegressor(),X_train,y_train,cv=10).mean()\n\n0.82474734196711685\n```", "```py\nparam_dist = {'n_estimators': [4000], 'learning_rate': [0.01], 'max_depth':[1,2,3,5,7]}\nrs_inst_a = RandomizedSearchCV(GradientBoostingRegressor(), param_dist, n_iter = 5, n_jobs=-1)\nrs_inst_a.fit(X_train, y_train)\n```", "```py\nrs_inst_a.best_params_\n\n{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 4000}\n\nrs_inst_a.best_score_\n\n0.88548410382780185\n```", "```py\ndef mape_score(y_test, y_pred):\n return (np.abs(y_test - y_pred)/y_test).mean()\n```", "```py\nfrom numba import autojit\n\n@autojit\ndef mape_score(y_test, y_pred):\n sum_total = 0\n y_vec_length = len(y_test)\n for index in range(y_vec_length):\n sum_total += (1 - (y_pred[index]/y_test[index]))\n\n return sum_total/y_vec_length\n```", "```py\nfrom sklearn.metrics import make_scorer\nmape_scorer = make_scorer(mape_score, greater_is_better=False)\n```", "```py\nparam_dist = {'n_estimators': [4000], 'learning_rate': [0.01], 'max_depth':[1,2,3,4,5]}\nrs_inst_b = RandomizedSearchCV(GradientBoostingRegressor(), param_dist, n_iter = 3, n_jobs=-1,scoring = mape_scorer)\nrs_inst_b.fit(X_train, y_train)\nrs_inst_b.best_score_\n\n0.021086502313661441\n\nrs_inst_b.best_params_\n\n{'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 4000}\n```", "```py\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\niris = load_iris()\nX = iris.data\ny = np.where(iris.target == 0,0,1)\n```", "```py\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2,random_state=0)\nkmeans.fit(X) \n```", "```py\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(kmeans,X,y,cv=10,scoring=make_scorer(adjusted_rand_score)).mean()\n\n0.8733695652173914\n```", "```py\nfrom sklearn.datasets import make_regression, make_classification\n\nX, y = make_regression()\nfrom sklearn import dummy\ndumdum = dummy.DummyRegressor()\ndumdum.fit(X, y)\n\nDummyRegressor(constant=None, quantile=None, strategy='mean')\n```", "```py\ndumdum.predict(X)[:5]\n\n>array([-25.0450033, -25.0450033, -25.0450033, -25.0450033, -25.0450033])\n```", "```py\npredictors = [(\"mean\", None),\n(\"median\", None),\n(\"constant\", 10)]\nfor strategy, constant in predictors:\n dumdum = dummy.DummyRegressor(strategy=strategy,\n constant=constant)\n dumdum.fit(X, y)\n print \"strategy: {}\".format(strategy), \",\".join(map(str, dumdum.predict(X)[:5]))\n\nstrategy: mean -25.0450032962,-25.0450032962,-25.0450032962,-25.0450032962,-25.0450032962\nstrategy: median -37.734448002,-37.734448002,-37.734448002,-37.734448002,-37.734448002\nstrategy: constant 10.0,10.0,10.0,10.0,10.0\n```", "```py\npredictors = [(\"constant\", 0),(\"stratified\", None),(\"uniform\", None),(\"most_frequent\", None)]\n#We'll also need to create some classification data:\nX, y = make_classification()\nfor strategy, constant in predictors:\n dumdum = dummy.DummyClassifier(strategy=strategy,\n constant=constant)\n dumdum.fit(X, y)\n print \"strategy: {}\".format(strategy), \",\".join(map(str,dumdum.predict(X)[:5]))\n\nstrategy: constant 0,0,0,0,0\nstrategy: stratified 1,0,1,1,1\nstrategy: uniform 1,0,1,0,1\nstrategy: most_frequent 0,0,0,0,0\n```", "```py\nX, y = make_classification(20000, weights=[.95, .05])\ndumdum = dummy.DummyClassifier(strategy='most_frequent')\ndumdum.fit(X, y)\n\nDummyClassifier(constant=None, random_state=None, strategy='most_frequent')\n\nfrom sklearn.metrics import accuracy_score\nprint accuracy_score(y, dumdum.predict(X))\n\n0.94615\n```", "```py\nfrom sklearn import datasets\n X, y = datasets.make_regression(1000, 10000)\n```", "```py\nfrom sklearn import feature_selection\nf, p = feature_selection.f_regression(X, y)\n```", "```py\nf[:5]\n\narray([ 1.23494617, 0.70831694, 0.09219176, 0.14583189, 0.78776466])\n\np[:5]\n\narray([ 0.26671492, 0.40020473, 0.76147235, 0.7026321 , 0.37499074])\n```", "```py\nimport numpy as np\nidx = np.arange(0, X.shape[1])\nfeatures_to_keep = idx[p < .05]\nlen(features_to_keep)\n\n496\n```", "```py\nvar_threshold = feature_selection.VarianceThreshold(np.median(np.var(X, axis=1)))\nvar_threshold.fit_transform(X).shape\n\n(1000L, 4888L)\n```", "```py\nX, y = datasets.make_regression(10000, 20)\nf, p = feature_selection.f_regression(X, y)\n```", "```py\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nf, ax = plt.subplots(figsize=(7, 5))\nax.bar(np.arange(20), p, color='k')\nax.set_title(\"Feature p values\")\n```", "```py\nimport sklearn.datasets as ds\ndiabetes = ds.load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n```", "```py\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import cross_val_score,ShuffleSplit\n\nshuff = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\nscore_before = cross_val_score(lr,X,y,cv=shuff,scoring=make_scorer(mean_squared_error,greater_is_better=False)).mean()\n\nscore_before\n\n-3053.393446308266\n```", "```py\nfrom sklearn.linear_model import LassoCV\n\nlasso_cv = LassoCV()\nlasso_cv.fit(X,y)\nlasso_cv.coef_\n\narray([ -0\\. , -226.2375274 , 526.85738059, 314.44026013,\n -196.92164002, 1.48742026, -151.78054083, 106.52846989,\n 530.58541123, 64.50588257])\n```", "```py\nimport numpy as np\ncolumns = np.arange(X.shape[1])[lasso_cv.coef_ != 0]\ncolumns\n```", "```py\nscore_afterwards = cross_val_score(lr,X[:,columns],y,cv=shuff, scoring=make_scorer(mean_squared_error,greater_is_better=False)).mean()\nscore_afterwards\n\n-3033.5012859289677\n```", "```py\nX, y = ds.make_regression(noise=5)\n```", "```py\nshuff = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n\nscore_before = cross_val_score(lr,X,y,cv=shuff, scoring=make_scorer(mean_squared_error,greater_is_better=False)).mean()\n```", "```py\nlasso_cv = LassoCV()\nlasso_cv.fit(X,y)\n```", "```py\ncolumns = np.arange(X.shape[1])[lasso_cv.coef_ != 0]\nscore_afterwards = cross_val_score(lr,X[:,columns],y,cv=shuff, scoring=make_scorer(mean_squared_error,greater_is_better=False)).mean()\nprint \"Score before:\",score_before\nprint \"Score after: \",score_afterwards\n\nScore before: -8891.35368845\nScore after: -22.3488585347\n```", "```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = make_classification()\ndt = DecisionTreeClassifier()\ndt.fit(X, y)\n```", "```py\nfrom sklearn.externals import joblib\njoblib.dump(dt, \"dtree.clf\")\n\n['dtree.clf']\n```", "```py\nfrom sklearn.externals import joblib\npulled_model = joblib.load(\"dtree.clf\")\ny_pred = pulled_model.predict(X)\n```", "```py\nimport cPickle as pickle #Python 2.x\n# import pickle          # Python 3.x\n```", "```py\nf = open(\"dtree.save\", 'wb')\npickle.dump(dt,f, protocol = pickle.HIGHEST_PROTOCOL)\nf.close()\n```", "```py\nf = open(\"dtree.save\", 'rb')\nreturn_tree = pickle.load(f)\nf.close()\n```", "```py\nreturn_tree\n\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            presort=False, random_state=None, splitter='best'\n```"]
- en: Predicting Numeric Outcomes with Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: graph_from_dot_data() function on the Linear regression is used to predict a
    continuous numeric value from a set of input features. This machine learning algorithm
    is fundamental to statisticians when it comes to predicting numeric outcomes.
    Although advanced algorithms such as neural networks and deep learning have taken
    the place of linear regression in modern times, the algorithm is still key when
    it comes to providing you with the foundations for neural networks and deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: The key benefit of building machine learning models with the linear regression
    algorithm, as opposed to neural networks and deep learning, is that it is highly
    interpretable. Interpretability helps you, as the machine learning practitioner,
    to understand how the different input variables behave when it comes to predicting
    output.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression algorithm is applied in the financial industry (in order
    to predict stock prices) and in the real estate industry (in order to predict
    housing prices). In fact, the linear regression algorithm can be applied in any
    field where there is a need to predict a numeric value, given a set of input features.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The inner mechanics of the linear regression algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and evaluating your first linear regression algorithm, using scikit-learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling your data for a potential performance improvement
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing your linear regression model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, and Matplotlib ≥ 3.0.0 installed on your system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_05.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_05.ipynb)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2Ay95cJ](http://bit.ly/2Ay95cJ)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The inner mechanics of the linear regression algorithm
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In its most fundamental form, the expression for the linear regression algorithm
    can be written as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/f5050bc0-a8db-43a6-983e-d963e7b87ea5.png)*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding equation, the output of the model is a numeric outcome. In
    order to obtain this numeric outcome, we require that each input feature be multiplied
    with a parameter called *Parameter1*, and we add the second parameter, *Parameter2*,
    to this result.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in other words, our task is to find the values of the two parameters that
    can predict the value of the numeric outcome as accurately as possible. In visual
    terms, consider the following diagram:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4c43234-60c1-4a0a-a28b-b9c0fba0b94c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Two-dimensional plot between the target and input feature
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows a two-dimensional plot between the target that we
    want to predict on the *y* axis (numeric output) and the input feature, which
    is along the *x* axis. The goal of linear regression is to find the optimal values
    of the two parameters mentioned in the preceding equation, in order to fit a line
    through the given set of points.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: This line is known as the **line of best fit**. A line of best fit is one that
    fits the given sets of points very well, so that it can make accurate predictions
    for us. Therefore, in order to find the optimal values of the parameters that
    will result in the line of best fit, we need to define a function that can do
    it for us.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is known as the **loss function**. The goal of the loss function,
    as the name suggests, is to minimize the loss/errors as much as possible, so that
    we can obtain a line of best fit. In order to understand how this works, consider
    the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/deeb026a-8eed-4df2-9931-dc9c7990f3a4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Line of best fit
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the line is fit through the set of data points, and
    the features can be defined as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The distance between each point in the plot and the line is known as the **residual**.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss/error function is the sum of the squares of these residuals.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of the linear regression algorithm is to minimize this value. The sum
    of the squares of the residuals is known as **ordinary least squares** (**OLS**).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing linear regression in scikit-learn
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will implement your first linear regression algorithm
    in scikit-learn. To make this easy to follow, the section will be divided into
    three subsections, in which you will learn about the following topics:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Implementing and visualizing a simple linear regression model in two dimensions
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing linear regression to predict the mobile transaction amount
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling your data for a potential increase in performance
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression in two dimensions
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this subsection, you will learn how to implement your first linear regression
    algorithm, in order to predict the amount of a mobile transaction by using one
    input feature: the old balance amount of the account holder. We will be using
    the same fraudulent mobile transaction dataset that we used in [*Chapter 2*](08e4b04a-e866-4754-9b0b-1486016dce2c.xhtml),
    *Predicting Categories with K-Nearest Neighbors*, of this book.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to read in the dataset and define the feature and target
    variable. This can be done by using the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will create a simple scatter plot between the amount of the mobile
    transaction on the *y* axis (which is the outcome of the linear regression model)
    and the old balance of the account holder along the *x* axis (which is the input
    feature). This can be done by using the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, we use the `plt.scatter()` function to create a scatter
    plot between the featureon the *x* axis and the targeton the *y* axis. This results
    in the scatter plot illustrated in the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97a7b42f-9f3f-4193-9500-d1f4f02a58c7.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Two-dimensional space of the linear regression model
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will fit a linear regression model into the two-dimensional space illustrated
    in the preceding diagram. Note that, in the preceding diagram, the data is not
    entirely linear. In order to do this, we use the following code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This results in a line of best fit, as illustrated in the following diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3b641d3-a7bc-462d-9263-31a9c7bca3c7.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Line of best fit
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, first, we initialize a linear regression model and fit
    the training data into that model. Since we only have a single feature, we need
    to reshape the feature and target for scikit-learn. Next, we define the upper
    and lower limits of the *x* axis, which contains our feature variable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we create a scatter plot between the feature and the target variable
    and include the line of best fit with the color red, as indicated in the preceding
    diagram.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Using linear regression to predict mobile transaction amount
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have visualized how a simple linear regression model works in two
    dimensions, we can use the linear regression algorithm to predict the total amount
    of a mobile transaction, using all of the other features in our mobile transaction
    dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to import our fraud prediction dataset into our workspace
    and divide it into training and test sets. This can be done by using the following
    code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now fit the linear regression model and evaluate the initial accuracy
    score of the model by using the following code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding code, first, we initialize a linear regression model, which
    we can then fit into the training data by using the `.fit()`function. Then, we
    evaluate the accuracy score on the test data by using the `.score()`function.
    This results in an accuracy score of 98%, which is fantastic!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your data
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scaling your data and providing a level of standardization is a vital step
    in any linear regression pipeline, as it could offer a way to enhance the performance
    of your model. In order to scale the data, we use the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We use the same scaling pipeline that we used in all of the previous chapters.
    In the preceding code, we replace the model name with the linear regression model
    and evaluate the scaled accuracy scores on the test data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: In this case, scaling the data did not lead to any improvements in the accuracy
    score, but it is vital to implement scaling into your linear regression pipeline,
    as it does lead to an improvement in the accuracy scores in most cases.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fundamental objective of the linear regression algorithm is to minimize
    the loss/cost function. In order to do this, the algorithm tries to optimize the
    values of the coefficients of each feature (*Parameter1*),such that the loss function
    is minimized.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, this leads to overfitting, as the coefficients of each variable are
    optimized for the data that the variable is trained on. This means that your linear
    regression model will not generalize beyond your current training data very well.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The process by which we penalize hyper-optimized coefficients in order to prevent
    this type of overfitting is called **regularization**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two broad types of regularization methods, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasso regression
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following subsections, the two types of regularization techniques will
    be discussed in detail, and you will learn about how you can implement them into
    your model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The equation for ridge regression is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bf0db77-ab6c-43a2-bd5c-6c5afeef7676.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the ridge loss function is equal to the ordinary
    least squares loss function, plus the product of the square of *Parameter1* of
    each feature and `alpha`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '`alpha` is a parameter that we can optimize in order to control the amount
    by which the ridge loss function penalizes the coefficients, in order to prevent
    overfitting. Obviously, if `alpha` is equal to `0`, the ridge loss function is
    equal to the ordinary least squares loss function, thereby making no difference
    to the initial overfit model.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, optimizing this value of `alpha` provides the optimal model that
    can generalize beyond the data that it has trained on.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to implement ridge regression into the fraud prediction dataset, we
    use the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, first, we read in the dataset and divide it into training
    and test sets (as usual). Next, we initialize a ridge regression model by using
    the `Ridge()` function, with the parameters of `alpha` set to `0` and `normalize`
    set to `True`, in order to standardize the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Next, the ridge model is fit into the training data, and the accuracy score
    is extracted from the test data. The accuracy of this model is exactly the same
    as the accuracy of the model that we built without the ridge regression as the
    parameter that controls how the model is optimized; `alpha` is set to `0`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to obtain the optimal value of `alpha` with the `GridSearchCV` algorithm,
    we use the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding code, the following applies:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize a ridge regression model, and then, we use the `GridSearchCV`
    algorithm to search for the optimal value of `alpha`, from a range of values.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After we obtain this optimal value of `alpha`, we build a new ridge regression
    model with this optimal value in the training data, and we evaluate the accuracy
    score on the test data.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since our initial model was already well optimized, the accuracy score did not
    increase by an observable amount. However, on datasets with larger dimensions/features,
    ridge regression holds immense value for providing you with a model that generalizes
    well, without overfitting.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to verify the results that the `GridSearchCV` algorithm has provided
    us with, we will construct a plot between the accuracy scores on the *y* axis
    and the different values of `alpha` along the *x* axis, for both the training
    and test data. In order to do this, we use the following code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15bc0a4c-ab68-41c0-8439-c4ec352b9748.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Accuracy versus alpha
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding plot, it is clear that a value of 0.01 or lower provides the
    highest value of accuracy for both the training and test data, and therefore,
    the results from the `GridSearchCV` algorithm make logical sense.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, first, we initialize two empty lists, to store the accuracy
    scores for both the training and test data. We then evaluate the accuracy scores
    for both the training and test sets for different values of `alpha`, and we create
    the preceding plot.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regression
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The equation for lasso regression is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8213a04-3fa1-4d9c-9d65-d4d9f6ff399d.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the lasso loss function is equal to the ordinary
    least squares loss function plus the product of the absolute value of the coefficients
    of each feature and `alpha`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '`alpha` is a parameter that we can optimize to control the amount by which
    the lasso loss function penalizes the coefficients, in order to prevent overfitting.
    Once again, if `alpha` is equal to `0`, the lasso loss function is equal to the
    ordinary least squares loss function, thereby making no difference to the initial
    overfit model.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, optimizing this value of `alpha` provides the optimal model that
    generalizes well beyond the data that it has trained on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to implement lasso regression into the fraud prediction dataset, we
    use the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding code is very similar to the code that we used to build the ridge
    regression model; the only difference is the `Lasso()`function which we use to
    initialize a lasso regression model. Additionally, the `warnings` package is used,
    in order to suppress the warning that is generated as we set the value of `alpha`
    to `0`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to optimize the value of `alpha`, we use the `GridSearchCV` algorithm.
    This is done by using the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding code is similar to the `alpha` optimization that we implemented
    for the ridge regression. Here, we use the lasso regression model instead of the
    ridge regression model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to verify the results of the `GridSearchCV` algorithm, we construct
    a plot between the accuracy scores and the value of `alpha` for the training and
    test sets. This is shown in the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9b6f0c2-31c7-43c4-9f34-ad1679b7e0f3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Accuracy versus alpha
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性与 alpha 的关系
- en: All of the values of `alpha` provide the same values of accuracy scores, and
    we can thus pick the value given to us by the `GridSearchCV` algorithm.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的`alpha`值都提供相同的准确度分数，因此我们可以选择由`GridSearchCV`算法提供的值。
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about how the linear regression algorithm works
    internally, through key concepts such as residuals and ordinary least squares.
    You also learned how to visualize a simple linear regression model in two dimensions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了线性回归算法的内部工作原理，通过残差和普通最小二乘等关键概念。你还学会了如何在二维空间中可视化简单的线性回归模型。
- en: We also covered implementing the linear regression model to predict the amount
    of a mobile transaction, along with scaling your data in an effective pipeline,
    to bring potential improvements to your performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了如何实现线性回归模型来预测移动交易的金额，并在有效的管道中对数据进行缩放，以带来潜在的性能提升。
- en: Finally, you learned how to optimize your model by using the concept of regularization,
    in the form of ridge and lasso regression.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学会了如何通过使用正则化概念来优化你的模型，正则化形式为岭回归和套索回归。

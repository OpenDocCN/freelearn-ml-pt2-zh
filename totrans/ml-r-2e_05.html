<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Divide and Conquer &#x2013; Classification Using Decision Trees and Rules"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules</h1></div></div></div><p>While deciding between several job offers with various levels of pay and benefits, many people begin by making lists of pros and cons, and eliminate options based on simple rules. For instance, ''if I have to commute for more than an hour, I will be unhappy.'' Or, ''if I make less than $50k, I won't be able to support my family.'' In this way, the complex and difficult decision of predicting one's future happiness can be reduced to a series of simple decisions.</p><p>This chapter covers decision trees and rule learners—two machine learning methods that also make complex decisions from sets of simple choices. These methods then present their knowledge in the form of logical structures that can be understood with no statistical knowledge. This aspect makes these models particularly useful for business strategy and process improvement.</p><p>By the end of this chapter, you will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How trees and rules "greedily" partition data into interesting segments</li><li class="listitem" style="list-style-type: disc">The most common decision tree and classification rule learners, including the C5.0, 1R, and RIPPER algorithms</li><li class="listitem" style="list-style-type: disc">How to use these algorithms to perform real-world classification tasks, such as identifying risky bank loans and poisonous mushrooms</li></ul></div><p>We will begin by examining decision trees, followed by a look at classification rules. Then, we will summarize what we've learned by previewing later chapters, which discuss methods that use trees and rules as a foundation for more advanced machine learning techniques.</p><div class="section" title="Understanding decision trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec24"/>Understanding decision trees</h1></div></div></div><p>Decision tree <a id="id351" class="indexterm"/>learners are powerful classifiers, which utilize a <span class="strong"><strong>tree structure</strong></span> to model the relationships among the features and the potential outcomes. As illustrated in the following figure, this structure earned its name due to the fact that it mirrors how a literal tree begins at a wide trunk, which if followed upward, splits into narrower and narrower branches. In much the same way, a decision tree classifier uses a structure of branching decisions, which channel examples into a final predicted class value.</p><p>To better understand how this works in practice, let's consider the following tree, which predicts whether a job offer should be accepted. A job offer to be considered begins at the <span class="strong"><strong>root node</strong></span>, where it is <a id="id352" class="indexterm"/>then passed through <span class="strong"><strong>decision nodes</strong></span> that require choices to be made based on the attributes of the job. These choices split the data across <span class="strong"><strong>branches</strong></span> <a id="id353" class="indexterm"/>that indicate potential outcomes of a decision, depicted here as yes or no outcomes, though in some cases there may be more than two possibilities. In <a id="id354" class="indexterm"/>the case a final decision can be made, the tree is terminated by <a id="id355" class="indexterm"/>
<span class="strong"><strong>leaf nodes</strong></span> (also known as <span class="strong"><strong>terminal nodes</strong></span>) that denote the action to be taken as the result of the series of decisions. In the case of a predictive model, the leaf nodes provide the expected result given the series of events in the tree.</p><div class="mediaobject"><img src="graphics/B03905_05_01.jpg" alt="Understanding decision trees"/></div><p>A great benefit of decision tree algorithms is that the flowchart-like tree structure is not necessarily exclusively for the learner's internal use. After the model is created, many decision tree algorithms output the resulting structure in a human-readable format. This provides tremendous insight into how and why the model works or doesn't work well for a particular task. This also makes decision trees particularly appropriate for applications in which the classification mechanism needs to be transparent for legal reasons, or in case the results need to <a id="id356" class="indexterm"/>be shared with others in order to inform future business practices. With this in mind, some potential uses include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Credit scoring <a id="id357" class="indexterm"/>models in which the criteria that causes an applicant to be rejected need to be clearly documented and free from bias</li><li class="listitem" style="list-style-type: disc">Marketing studies of customer behavior such as satisfaction or churn, which will be shared with management or advertising agencies</li><li class="listitem" style="list-style-type: disc">Diagnosis of medical conditions based on laboratory measurements, symptoms, or the rate of disease progression</li></ul></div><p>Although the previous applications illustrate the value of trees in informing decision processes, this is not to suggest that their utility ends here. In fact, decision trees are perhaps the single most widely used machine learning technique, and can be applied to model almost any type of data—often with excellent out-of-the-box applications.</p><p>This said, in spite of their wide applicability, it is worth noting some scenarios where trees may not be an ideal fit. One such case might be a task where the data has a large number of nominal features with many levels or it has a large number of numeric features. These cases may result in a very large number of decisions and an overly complex tree. They may also contribute to the tendency of decision trees to overfit data, though as we will soon see, even this weakness can be overcome by adjusting some simple parameters.</p><div class="section" title="Divide and conquer"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec45"/>Divide and conquer</h2></div></div></div><p>Decision trees are built using a <a id="id358" class="indexterm"/>heuristic called <span class="strong"><strong>recursive partitioning</strong></span>. This approach is also commonly known as <span class="strong"><strong>divide and conquer</strong></span> because it splits the <a id="id359" class="indexterm"/>data into subsets, which are then split repeatedly <a id="id360" class="indexterm"/>into even smaller subsets, and so on and so forth until the process stops when the algorithm determines the data within the subsets are sufficiently homogenous, or another stopping criterion has been met.</p><p>To see how splitting a dataset can create a decision tree, imagine a bare root node that will grow into a mature tree. At first, the root node represents the entire dataset, since no splitting has transpired. Next, the decision tree algorithm must choose a feature to split upon; ideally, it chooses the feature most predictive of the target class. The examples are then partitioned into groups according to the distinct values of this feature, and the first set of tree branches are formed.</p><p>Working down each branch, the algorithm continues to divide and conquer the data, choosing the best candidate feature each time to create another decision node, until a stopping criterion is reached. Divide and conquer might stop at a node in a case that:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">All (or nearly all) of the examples at the node have the same class</li><li class="listitem" style="list-style-type: disc">There are no remaining features to distinguish among the examples</li><li class="listitem" style="list-style-type: disc">The tree has grown to a predefined size limit</li></ul></div><p>To illustrate the tree <a id="id361" class="indexterm"/>building process, let's consider a simple example. Imagine that you work for a Hollywood studio, where your role is to decide whether the <a id="id362" class="indexterm"/>studio should move forward with producing the screenplays pitched by promising new authors. After returning from a vacation, your desk is piled high with proposals. Without the time to read each proposal cover-to-cover, you decide to develop a decision tree algorithm to predict whether a potential movie would fall into one of three categories: <span class="strong"><strong>Critical Success</strong></span>, <span class="strong"><strong>Mainstream Hit</strong></span>, or <span class="strong"><strong>Box Office Bust</strong></span>.</p><p>To build the decision tree, you turn to the studio archives to examine the factors leading to the success and failure of the company's 30 most recent releases. You quickly notice a relationship between the film's estimated shooting budget, the number of A-list celebrities lined up for starring roles, and the level of success. Excited about this finding, you produce a scatterplot to illustrate the pattern:</p><div class="mediaobject"><img src="graphics/B03905_05_02.jpg" alt="Divide and conquer"/></div><p>Using the divide and conquer strategy, we can build a simple decision tree from this data. First, to create the tree's root node, we split the feature indicating the number of celebrities, partitioning the <a id="id363" class="indexterm"/>movies into groups with and without a <a id="id364" class="indexterm"/>significant number of A-list stars:</p><div class="mediaobject"><img src="graphics/B03905_05_03.jpg" alt="Divide and conquer"/></div><p>Next, among the group <a id="id365" class="indexterm"/>of movies with a larger number of celebrities, we can make another split between movies with and without a high budget:</p><div class="mediaobject"><img src="graphics/B03905_05_04.jpg" alt="Divide and conquer"/></div><p>At this point, we have partitioned the data into three groups. The group at the top-left corner of the diagram is composed entirely of critically acclaimed films. This group is distinguished by a high number of celebrities and a relatively low budget. At the top-right corner, majority of movies are box office hits with high budgets and a large number of celebrities. The final group, which has little star power but budgets ranging from small to large, contains the flops.</p><p>If we wanted, we could continue to divide and conquer the data by splitting it based on the increasingly specific ranges of budget and celebrity count, until each of the currently misclassified values resides in its own tiny partition, and is correctly classified. However, it is not advisable to overfit a decision tree in this way. Though there is nothing to stop us from splitting the data indefinitely, overly specific decisions do not always generalize more broadly. We'll avoid the problem of overfitting by stopping the algorithm here, since more than 80 percent of the examples in each group are from a single class. This forms the basis of <a id="id366" class="indexterm"/>our stopping criterion.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip61"/>Tip</h3><p>You might have <a id="id367" class="indexterm"/>noticed that diagonal lines might have split the data even more cleanly. This is one limitation of the decision tree's knowledge representation, which uses <span class="strong"><strong>axis-parallel splits</strong></span>. The fact that each split considers one feature at a time prevents the decision tree from forming more complex decision boundaries. For example, a diagonal line could be created by a decision that asks, "is the number of celebrities is greater than the estimated budget?" If so, then "it will be a critical success."</p></div></div><p>Our model for predicting the future success of movies can be represented in a simple tree, as shown in the following diagram. To evaluate a script, follow the branches through each decision until the script's success or failure has been predicted. In no time, you will be able to identify the most promising options among the backlog of scripts and get back to more important work, such as writing an Academy Awards acceptance speech.</p><div class="mediaobject"><img src="graphics/B03905_05_05.jpg" alt="Divide and conquer"/></div><p>Since real-world data <a id="id368" class="indexterm"/>contains more than two features, decision <a id="id369" class="indexterm"/>trees quickly become far more complex than this, with many more nodes, branches, and leaves. In the next section, you will learn about a popular algorithm to build decision tree models automatically.</p></div><div class="section" title="The C5.0 decision tree algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec46"/>The C5.0 decision tree algorithm</h2></div></div></div><p>There are numerous <a id="id370" class="indexterm"/>implementations of decision trees, but one of the most well-known implementations is the <span class="strong"><strong>C5.0 algorithm</strong></span>. This algorithm was developed by computer scientist J. Ross Quinlan as an improved version of his prior algorithm, <span class="strong"><strong>C4.5</strong></span>, which <a id="id371" class="indexterm"/>itself is an improvement over his <span class="strong"><strong>Iterative Dichotomiser 3</strong></span> (<span class="strong"><strong>ID3</strong></span>) algorithm. Although Quinlan markets C5.0 to commercial clients (see <a class="ulink" href="http://www.rulequest.com/">http://www.rulequest.com/</a> for details), the source code for a single-threaded version of the algorithm was made publically available, and it has therefore been incorporated into programs such as R.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>To further confuse matters, a popular Java-based open source alternative to C4.5, titled <span class="strong"><strong>J48</strong></span>, is included in R's <code class="literal">RWeka</code> package. Because the differences among C5.0, C4.5, and J48 are minor, the principles in this chapter will apply to any of these three methods, and the algorithms should be considered synonymous.</p></div></div><p>The C5.0 algorithm has become the industry standard to produce decision trees, because it does well for most types of problems directly out of the box. Compared to other advanced machine learning models, such as those described in <a class="link" href="ch07.html" title="Chapter 7. Black Box Methods – Neural Networks and Support Vector Machines">Chapter 7</a>, <span class="emphasis"><em>Black Box Methods – Neural Networks and Support Vector Machines</em></span>, the decision trees built by C5.0 generally perform nearly as well, but are much easier to understand and deploy. Additionally, as shown in the following table, the algorithm's weaknesses are relatively minor and can be largely avoided:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strengths</p>
</th><th style="text-align: left" valign="bottom">
<p>Weaknesses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An all-purpose classifier that does well on most problems</li><li class="listitem" style="list-style-type: disc">Highly automatic learning process, which can handle numeric or nominal features, as well as missing data</li><li class="listitem" style="list-style-type: disc">Excludes unimportant features</li><li class="listitem" style="list-style-type: disc">Can be used on both small and large datasets</li><li class="listitem" style="list-style-type: disc">Results in a model that can be interpreted without a mathematical background (for relatively small trees)</li><li class="listitem" style="list-style-type: disc">More efficient than other complex models</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Decision tree models are often biased toward splits on features having a large number of levels</li><li class="listitem" style="list-style-type: disc">It is easy to overfit or underfit the model</li><li class="listitem" style="list-style-type: disc">Can have trouble modeling some relationships due to reliance on axis-parallel splits</li><li class="listitem" style="list-style-type: disc">Small changes in the training data can result in large changes to decision logic</li><li class="listitem" style="list-style-type: disc">Large trees can be difficult to interpret and the decisions they make may seem counterintuitive</li></ul></div>
</td></tr></tbody></table></div><p>To keep things simple, our earlier decision tree example ignored the mathematics involved in how a machine would employ a divide and conquer strategy. Let's explore this in more detail to examine how this heuristic works in practice.</p><div class="section" title="Choosing the best split"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec28"/>Choosing the best split</h3></div></div></div><p>The first challenge <a id="id372" class="indexterm"/>that a decision tree will face is to identify which feature to split upon. In the previous example, we looked for a way to split the data such that the <a id="id373" class="indexterm"/>resulting partitions contained examples primarily of a single class. The degree to which a subset of examples contains only a single class is known as <span class="strong"><strong>purity</strong></span>, and any <a id="id374" class="indexterm"/>subset composed of only a single class is called <span class="strong"><strong>pure</strong></span>.</p><p>There are various measurements of purity that can be used to identify the best decision tree splitting candidate. C5.0 uses <span class="strong"><strong>entropy</strong></span>, a concept borrowed from information theory that quantifies the randomness, or disorder, within a set of class values. Sets with high entropy are very diverse and provide little information about other items that may also belong in the set, as there is no apparent commonality. The decision tree hopes to find splits that reduce entropy, ultimately increasing homogeneity within the groups.</p><p>Typically, entropy is <a id="id375" class="indexterm"/>measured in <span class="strong"><strong>bits</strong></span>. If there are only two possible classes, entropy values can range from 0 to 1. For <span class="emphasis"><em>n</em></span> classes, entropy ranges from 0 to <span class="emphasis"><em>log<sub>2</sub>(n)</em></span>. In each case, the minimum value indicates that the sample is completely homogenous, while the maximum value indicates that the data are as diverse as possible, and no group has even a small plurality.</p><p>In the mathematical notion, entropy is specified as follows:</p><div class="mediaobject"><img src="graphics/B03905_05_06.jpg" alt="Choosing the best split"/></div><p>In this formula, for a given segment of data <span class="emphasis"><em>(S)</em></span>, the term <span class="emphasis"><em>c</em></span> refers to the number of class levels and <span class="emphasis"><em>p<sub>i</sub></em></span> refers to the proportion of values falling into class level <span class="emphasis"><em>i</em></span>. For example, suppose we have a partition of data with two classes: red (60 percent) and white (40 percent). We can calculate the entropy as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; -0.60 * log2(0.60) - 0.40 * log2(0.40)</strong></span>
<span class="strong"><strong>[1] 0.9709506</strong></span>
</pre></div><p>We can examine the entropy for all the possible two-class arrangements. If we know that the proportion of examples in one class is <span class="emphasis"><em>x</em></span>, then the proportion in the other class is <span class="emphasis"><em>(1 – x)</em></span>. Using the <code class="literal">curve()</code> function, we can then plot the entropy for all the possible values of <span class="emphasis"><em>x</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; curve(-x * log2(x) - (1 - x) * log2(1 - x),</strong></span>
<span class="strong"><strong>        col = "red", xlab = "x", ylab = "Entropy", lwd = 4)</strong></span>
</pre></div><p>This results in the following figure:</p><div class="mediaobject"><img src="graphics/B03905_05_07.jpg" alt="Choosing the best split"/></div><p>As illustrated by the peak in entropy at <span class="emphasis"><em>x = 0.50</em></span>, a 50-50 split results in maximum entropy. As one class increasingly dominates the other, the entropy reduces to zero.</p><p>To use entropy to <a id="id376" class="indexterm"/>determine the optimal feature to split upon, the algorithm calculates the change in homogeneity that would result from a split on each possible <a id="id377" class="indexterm"/>feature, which is a measure known as <span class="strong"><strong>information gain</strong></span>. The information gain for a feature <span class="emphasis"><em>F</em></span> is calculated as the difference between the entropy in the segment before the split <span class="emphasis"><em>(S<sub>1</sub>)</em></span> and the partitions resulting from the split <span class="emphasis"><em>(S<sub>2</sub>)</em></span>:</p><div class="mediaobject"><img src="graphics/B03905_05_08.jpg" alt="Choosing the best split"/></div><p>One complication is that after a split, the data is divided into more than one partition. Therefore, the function to calculate <span class="emphasis"><em>Entropy(S<sub>2</sub>)</em></span> needs to consider the total entropy across all of the partitions. It does this by weighing each partition's entropy by the proportion of records falling into the partition. This can be stated in a formula as:</p><div class="mediaobject"><img src="graphics/B03905_05_09.jpg" alt="Choosing the best split"/></div><p>In simple terms, the total entropy resulting from a split is the sum of the entropy of each of the <span class="emphasis"><em>n</em></span> partitions weighted by the proportion of examples falling in the partition (<span class="emphasis"><em>w<sub>i</sub></em></span>).</p><p>The higher the information gain, the better a feature is at creating homogeneous groups after a split on this feature. If the information gain is zero, there is no reduction in entropy for splitting on this feature. On the other hand, the maximum information gain is equal to the entropy prior to the split. This would imply that the entropy after the split is zero, which means that the split results in completely homogeneous groups.</p><p>The previous formulae assume nominal features, but decision trees use information gain for splitting on numeric features as well. To do so, a common practice is to test various splits that divide the values <a id="id378" class="indexterm"/>into groups greater than or less than a numeric threshold. This reduces the numeric feature into a two-level categorical feature that allows information gain to be calculated as usual. The numeric cut point yielding the largest information gain is chosen for the split. </p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Though it is used by C5.0, information gain is not the only splitting criterion that can be used to build decision trees. Other commonly used criteria are <span class="strong"><strong>Gini index</strong></span>, <span class="strong"><strong>Chi-Squared statistic</strong></span>, and <span class="strong"><strong>gain ratio</strong></span>. For a review of these (and many more) criteria, refer to Mingers J. <span class="emphasis"><em>An Empirical Comparison of Selection Measures for Decision-Tree Induction</em></span>. Machine Learning. 1989; 3:319-342.</p></div></div></div><div class="section" title="Pruning the decision tree"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec29"/>Pruning the decision tree</h3></div></div></div><p>A decision tree can <a id="id379" class="indexterm"/>continue to grow indefinitely, choosing splitting features and <a id="id380" class="indexterm"/>dividing the data into smaller and smaller partitions until each example is perfectly classified or the algorithm runs out of features to split on. However, if the tree grows overly large, many of the decisions it makes will be overly specific and the model will be overfitted to the training data. The process of <span class="strong"><strong>pruning</strong></span> a decision tree involves reducing its size such that it generalizes better to unseen data.</p><p>One solution to this problem is to stop the tree from growing once it reaches a certain number of decisions or when the <a id="id381" class="indexterm"/>decision nodes contain only a small number of <a id="id382" class="indexterm"/>examples. This is called <span class="strong"><strong>early stopping</strong></span> or <span class="strong"><strong>pre-pruning</strong></span> the decision tree. As the tree avoids doing needless work, this is an appealing strategy. However, one downside to this approach is that there is no way to know whether the tree will miss subtle, but important patterns that it would have learned had it grown to a larger size.</p><p>An alternative, called <span class="strong"><strong>post-pruning</strong></span>, involves growing a tree that is intentionally too large and pruning leaf nodes to <a id="id383" class="indexterm"/>reduce the size of the tree to a more appropriate level. This is often a more effective approach than pre-pruning, because it is quite difficult to determine the optimal depth of a decision tree without growing it first. Pruning the tree later on allows the algorithm to be certain that all the important data structures were discovered.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>The implementation details of pruning operations are very technical and beyond the scope of this book. For a comparison of some of the available methods, see Esposito F, Malerba D, Semeraro G. <span class="emphasis"><em>A Comparative Analysis of Methods for Pruning Decision Trees</em></span>. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1997;19: 476-491.</p></div></div><p>One of the benefits of the C5.0 algorithm is that it is opinionated about pruning—it takes care of many decisions automatically using fairly reasonable defaults. Its overall strategy is to post-prune the tree. It first grows a large tree that overfits the training data. Later, the nodes and branches that have little effect on the classification errors are removed. In some cases, entire <a id="id384" class="indexterm"/>branches are moved further up the tree or replaced by simpler <a id="id385" class="indexterm"/>decisions. These processes of grafting <a id="id386" class="indexterm"/>branches are known as <span class="strong"><strong>subtree raising</strong></span> and <span class="strong"><strong>subtree replacement</strong></span><a id="id387" class="indexterm"/>, respectively.</p><p>Balancing overfitting and underfitting a decision tree is a bit of an art, but if model accuracy is vital, it may be worth investing some time with various pruning options to see if it improves the performance on test data. As you will soon see, one of the strengths of the C5.0 algorithm is that it is very easy to adjust the training options.</p></div></div></div></div>
<div class="section" title="Example &#x2013; identifying risky bank loans using C5.0 decision trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec25"/>Example – identifying risky bank loans using C5.0 decision trees</h1></div></div></div><p>The global <a id="id388" class="indexterm"/>financial crisis of 2007-2008 <a id="id389" class="indexterm"/>highlighted the importance of transparency and rigor in banking practices. As the availability of credit was limited, banks tightened their lending systems and turned to machine learning to more accurately identify risky loans.</p><p>Decision trees are <a id="id390" class="indexterm"/>widely used in the banking industry due to their high accuracy and ability to formulate a statistical model in plain language. Since government organizations in many countries carefully monitor lending practices, executives must be able to explain why one applicant was rejected for a loan while the others were approved. This information is also useful for customers hoping to determine why their credit rating is unsatisfactory.</p><p>It is likely that <a id="id391" class="indexterm"/>automated credit scoring models are <a id="id392" class="indexterm"/>employed to instantly approve credit applications on the telephone and web. In this section, we will develop a simple credit approval model using C5.0 decision trees. We will also see how the results of the model can be tuned to minimize errors that result in a financial loss for the institution.</p><div class="section" title="Step 1 – collecting data"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec47"/>Step 1 – collecting data</h2></div></div></div><p>The idea behind our <a id="id393" class="indexterm"/>credit model is to identify factors that are predictive of higher risk of default. Therefore, we need to obtain data on a large number of past bank loans and whether the loan went into default, as well as information on the applicant.</p><p>Data with <a id="id394" class="indexterm"/>these characteristics is available in a dataset donated to the UCI Machine Learning Data Repository (<a class="ulink" href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>) by Hans Hofmann of the University of Hamburg. The dataset contains information on loans obtained from a credit agency in Germany. </p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip62"/>Tip</h3><p>The dataset presented in this chapter has been modified slightly from the original in order to eliminate some preprocessing steps. To follow along with the examples, download the <code class="literal">credit.csv</code> file from Packt Publishing's website and save it to your R working directory.</p></div></div><p>The credit dataset includes 1,000 examples on loans, plus a set of numeric and nominal features indicating the characteristics of the loan and the loan applicant. A class variable indicates whether the loan went into default. Let's see whether we can determine any patterns that predict this outcome.</p></div><div class="section" title="Step 2 – exploring and preparing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec48"/>Step 2 – exploring and preparing the data</h2></div></div></div><p>As we <a id="id395" class="indexterm"/>did previously, we <a id="id396" class="indexterm"/>will import data using the <code class="literal">read.csv()</code> function. We will ignore the <code class="literal">stringsAsFactors</code> option and, therefore, use the default value of <code class="literal">TRUE</code>, as the majority of the features in the data are nominal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit &lt;- read.csv("credit.csv")</strong></span>
</pre></div><p>The first several lines of output from the <code class="literal">str()</code> function are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(credit)</strong></span>
<span class="strong"><strong>'data.frame':1000 obs. of  17 variables:</strong></span>
<span class="strong"><strong> $ checking_balance : Factor w/ 4 levels "&lt; 0 DM","&gt; 200 DM",..</strong></span>
<span class="strong"><strong> $ months_loan_duration: int  6 48 12 ...</strong></span>
<span class="strong"><strong> $ credit_history      : Factor w/ 5 levels "critical","good",..</strong></span>
<span class="strong"><strong> $ purpose             : Factor w/ 6 levels "business","car",..</strong></span>
<span class="strong"><strong> $ amount              : int  1169 5951 2096 ...</strong></span>
</pre></div><p>We see the expected 1,000 observations and 17 features, which are a combination of factor and integer data types.</p><p>Let's take a look at the <code class="literal">table()</code> output for a couple of loan features that seem likely to predict a default. The applicant's checking and savings account balance are recorded as categorical variables:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(credit$checking_balance)</strong></span>
<span class="strong"><strong>    &lt; 0 DM   &gt; 200 DM 1 - 200 DM    unknown </strong></span>
<span class="strong"><strong>       274         63        269        394</strong></span>
<span class="strong"><strong>&gt; table(credit$savings_balance)</strong></span>
<span class="strong"><strong>     &lt; 100 DM &gt; 1000 DM  100 - 500 DM 500 - 1000 DM   unknown </strong></span>
<span class="strong"><strong>          603        48           103            63       183</strong></span>
</pre></div><p>The checking <a id="id397" class="indexterm"/>and savings account balance may prove to be important predictors of loan default status. Note that since the <a id="id398" class="indexterm"/>loan data was obtained from Germany, the currency is recorded in Deutsche Marks (DM).</p><p>Some of the loan's features are numeric, such as its duration and the amount of credit requested:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(credit$months_loan_duration)</strong></span>
<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </strong></span>
<span class="strong"><strong>    4.0    12.0    18.0    20.9    24.0    72.0 </strong></span>
<span class="strong"><strong>&gt; summary(credit$amount)</strong></span>
<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </strong></span>
<span class="strong"><strong>    250    1366    2320    3271    3972   18420</strong></span>
</pre></div><p>The loan amounts ranged from 250 DM to 18,420 DM across terms of 4 to 72 months with a median duration of 18 months and an amount of 2,320 DM.</p><p>The <code class="literal">default</code> vector indicates whether the loan applicant was unable to meet the agreed payment terms and went into default. A total of 30 percent of the loans in this dataset went into default:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(credit$default)</strong></span>
<span class="strong"><strong> no yes </strong></span>
<span class="strong"><strong>700 300</strong></span>
</pre></div><p>A high rate <a id="id399" class="indexterm"/>of default is undesirable for a bank, because it means that the bank is unlikely to fully recover its <a id="id400" class="indexterm"/>investment. If we are successful, our model will identify applicants that are at high risk to default, allowing the bank to refuse credit requests.</p><div class="section" title="Data preparation – creating random training and test datasets"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec30"/>Data preparation – creating random training and test datasets</h3></div></div></div><p>As we have <a id="id401" class="indexterm"/>done in the previous chapters, we will split our data into two portions: a training dataset to build the decision tree and a test dataset to evaluate the performance of the model <a id="id402" class="indexterm"/>on new data. We will use 90 percent of the data for training and 10 percent for testing, which will provide us with 100 records to simulate new applicants.</p><p>As prior chapters used data that had been sorted in a random order, we simply divided the dataset into two portions, by taking the first 90 percent of records for training, and the remaining 10 percent for testing. In contrast, the credit dataset is not randomly ordered, making the prior approach unwise. Suppose that the bank had sorted the data by the loan amount, with the largest loans at the end of the file. If we used the first 90 percent for training and the remaining 10 percent for testing, we would be training a model on only the small loans and testing the model on the big loans. Obviously, this could be problematic.</p><p>We'll solve this problem by using a <span class="strong"><strong>random sample</strong></span> of the credit data for training. A random sample is simply a process that selects a subset of records at random. In R, the <code class="literal">sample()</code> function is used to perform random sampling. However, before putting it in action, a common practice is to set a <span class="strong"><strong>seed</strong></span> value, which causes the randomization process to follow a sequence that can be replicated later on if desired. It may seem that this defeats the purpose of generating random numbers, but there is a good reason for doing it this way. Providing a seed value via the <code class="literal">set.seed()</code> function ensures that if the analysis is repeated in the future, an identical result is obtained.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip63"/>Tip</h3><p>You may wonder how a so-called random process can be seeded to produce an identical result. This is due to the fact that computers use a mathematical function called a <span class="strong"><strong>pseudorandom number generator</strong></span> to create random number sequences that appear to act very random, but are actually quite predictable given knowledge of the previous values in the sequence. In practice, modern pseudorandom number sequences are virtually indistinguishable from true random sequences, but have the benefit that computers can generate them quickly and easily.</p></div></div><p>The following commands use the <code class="literal">sample()</code> function to select 900 values at random out of the sequence of integers from 1 to 1000. Note that the <code class="literal">set.seed()</code> function uses the arbitrary value <code class="literal">123</code>. Omitting this seed will cause your training and testing split to differ from those shown in the remainder of this chapter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(123)</strong></span>
<span class="strong"><strong>&gt; train_sample &lt;- sample(1000, 900)</strong></span>
</pre></div><p>As expected, the resulting <code class="literal">train_sample</code> object is a vector of 900 random integers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(train_sample)</strong></span>
<span class="strong"><strong> int [1:900] 288 788 409 881 937 46 525 887 548 453 ... </strong></span>
</pre></div><p>By using this vector to select rows from the credit data, we can split it into the 90 percent training and <a id="id403" class="indexterm"/>10 percent <a id="id404" class="indexterm"/>test datasets we desired. Recall that the dash operator used in the selection of the test records tells R to select records that are not in the specified rows; in other words, the test data includes only the rows that are not in the training sample.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_train &lt;- credit[train_sample, ]</strong></span>
<span class="strong"><strong>&gt; credit_test  &lt;- credit[-train_sample, ]</strong></span>
</pre></div><p>If all went well, we should have about 30 percent of defaulted loans in each of the datasets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; prop.table(table(credit_train$default))</strong></span>
<span class="strong"><strong>       no       yes </strong></span>
<span class="strong"><strong>0.7033333 0.2966667 </strong></span>

<span class="strong"><strong>&gt; prop.table(table(credit_test$default))</strong></span>
<span class="strong"><strong>  no  yes </strong></span>
<span class="strong"><strong>0.67 0.33</strong></span>
</pre></div><p>This appears to be a fairly even split, so we can now build our decision tree.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip64"/>Tip</h3><p>If your results do not match exactly, ensure that you ran the command <code class="literal">set.seed(123)</code> immediately prior to creating the <code class="literal">train_sample</code> vector.</p></div></div></div></div><div class="section" title="Step 3 – training a model on the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec49"/>Step 3 – training a model on the data</h2></div></div></div><p>We will <a id="id405" class="indexterm"/>use the C5.0 algorithm in the <code class="literal">C50</code> package to train our decision tree model. If you have not done so already, install the package with <code class="literal">install.packages("C50")</code> and load it to your R session, using <code class="literal">library(C50)</code>.</p><p>The following syntax box lists some of the most commonly used commands to build decision trees. Compared to the machine learning approaches we used previously, the C5.0 algorithm offers many more ways to tailor the model to a particular learning problem, but more options are available. Once the <code class="literal">C50</code> package has been loaded, the <code class="literal">?C5.0Control</code> command displays the help page for more details on how to finely-tune the algorithm.</p><div class="mediaobject"><img src="graphics/B03905_05_10.jpg" alt="Step 3 – training a model on the data"/></div><p>For the first <a id="id406" class="indexterm"/>iteration of our credit approval model, we'll use the default C5.0 configuration, as shown in the following code. The 17th column in <code class="literal">credit_train</code> is the <code class="literal">default</code> class variable, so we need to exclude it from the training data frame, but supply it as the target factor vector for classification:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_model &lt;- C5.0(credit_train[-17], credit_train$default)</strong></span>
</pre></div><p>The <code class="literal">credit_model</code> object now contains a C5.0 decision tree. We can see some basic data about the tree by typing its name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_model</strong></span>

<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>C5.0.default(x = credit_train[-17], y = credit_train$default)</strong></span>

<span class="strong"><strong>Classification Tree</strong></span>
<span class="strong"><strong>Number of samples: 900 </strong></span>
<span class="strong"><strong>Number of predictors: 16 </strong></span>

<span class="strong"><strong>Tree size: 57 </strong></span>

<span class="strong"><strong>Non-standard options: attempt to group attributes</strong></span>
</pre></div><p>The preceding text shows some simple facts about the tree, including the function call that generated it, the number of features (labeled <code class="literal">predictors</code>), and examples (labeled <code class="literal">samples</code>) used to grow the tree. Also listed is the tree size of 57, which indicates that the tree is 57 decisions deep—quite a bit larger than the example trees we've considered so far!</p><p>To see the tree's decisions, we can call the <code class="literal">summary()</code> function on the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(credit_model)</strong></span>
</pre></div><p>This results in the following output:</p><div class="mediaobject"><img src="graphics/B03905_05_11.jpg" alt="Step 3 – training a model on the data"/></div><p>The preceding output shows some of the first branches in the decision tree. The first three lines could be represented in plain language as:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">If the checking account balance is unknown or greater than 200 DM, then classify as "not likely to default."</li><li class="listitem">Otherwise, if the checking account balance is less than zero DM or between one and 200 DM.</li><li class="listitem">And the credit history is perfect or very good, then classify as "likely to default."</li></ol></div><p>The numbers in <a id="id407" class="indexterm"/>parentheses indicate the number of examples meeting the criteria for that decision, and the number incorrectly classified by the decision. For instance, on the first line, <code class="literal">412/50</code> indicates that of the 412 examples reaching the decision, 50 were incorrectly classified as not likely to default. In other words, 50 applicants actually defaulted, in spite of the model's prediction to the contrary.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip65"/>Tip</h3><p>Sometimes a tree results in decisions that make little logical sense. For example, why would an applicant whose credit history is very good be likely to default, while those whose checking balance is unknown are not likely to default? Contradictory rules like this occur sometimes. They might reflect a real pattern in the data, or they may be a statistical anomaly. In either case, it is important to investigate such strange decisions to see whether the tree's logic makes sense for business use.</p></div></div><p>After the tree, the <code class="literal">summary(credit_model)</code> output displays a confusion matrix, which is a cross-tabulation that indicates the model's incorrectly classified records in the training data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Evaluation on training data (900 cases):</strong></span>

<span class="strong"><strong>      Decision Tree   </strong></span>
<span class="strong"><strong>    ----------------  </strong></span>
<span class="strong"><strong>    Size      Errors  </strong></span>
<span class="strong"><strong>      56  133(14.8%)   &lt;&lt;</strong></span>

<span class="strong"><strong>     (a)   (b)    &lt;-classified as</strong></span>
<span class="strong"><strong>    ----  ----</strong></span>
<span class="strong"><strong>     598    35    (a): class no</strong></span>
<span class="strong"><strong>      98   169    (b): class yes</strong></span>
</pre></div><p>The Errors output notes that the model correctly classified all but 133 of the 900 training instances for an error rate of 14.8 percent. A total of 35 actual no values were incorrectly classified as yes (false positives), while 98 yes values were misclassified as no (false negatives).</p><p>Decision trees are <a id="id408" class="indexterm"/>known for having a tendency to overfit the model to the training data. For this reason, the error rate reported on training data may be overly optimistic, and it is especially important to evaluate decision trees on a test dataset.</p></div><div class="section" title="Step 4 – evaluating model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec50"/>Step 4 – evaluating model performance</h2></div></div></div><p>To apply our <a id="id409" class="indexterm"/>decision tree to the test dataset, we use the <code class="literal">predict()</code> function, as shown in the following line of code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_pred &lt;- predict(credit_model, credit_test)</strong></span>
</pre></div><p>This creates a vector of predicted class values, which we can compare to the actual class values using the <code class="literal">CrossTable()</code> function in the <code class="literal">gmodels</code> package. Setting the <code class="literal">prop.c</code> and <code class="literal">prop.r</code> parameters to <code class="literal">FALSE</code> removes the column and row percentages from the table. The remaining percentage (<code class="literal">prop.t</code>) indicates the proportion of records in the cell out of the total number of records:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(gmodels)</strong></span>
<span class="strong"><strong>&gt; CrossTable(credit_test$default, credit_pred,</strong></span>
<span class="strong"><strong>             prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,</strong></span>
<span class="strong"><strong>             dnn = c('actual default', 'predicted default'))</strong></span>
</pre></div><p>This results in the following table:</p><div class="mediaobject"><img src="graphics/B03905_05_12.jpg" alt="Step 4 – evaluating model performance"/></div><p>Out of the 100 test loan application records, our model correctly predicted that 59 did not default and 14 did default, resulting in an accuracy of 73 percent and an error rate of 27 percent. This is somewhat worse than its performance on the training data, but not unexpected, given <a id="id410" class="indexterm"/>that a model's performance is often worse on unseen data. Also note that the model only correctly predicted 14 of the 33 actual loan defaults in the test data, or 42 percent. Unfortunately, this type of error is a potentially very costly mistake, as the bank loses money on each default. Let's see if we can improve the result with a bit more effort.</p></div><div class="section" title="Step 5 – improving model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec51"/>Step 5 – improving model performance</h2></div></div></div><p>Our model's error <a id="id411" class="indexterm"/>rate is likely to be too high to deploy it in a real-time credit scoring application. In fact, if the model had predicted "no default" for every test case, it would have been correct 67 percent of the time—a result not much worse than our model's, but requiring much less effort! Predicting loan defaults from 900 examples seems to be a challenging problem.</p><p>Making matters even worse, our model performed especially poorly at identifying applicants who do default on their loans. Luckily, there are a couple of simple ways to adjust the C5.0 algorithm that may help to improve the performance of the model, both overall and for the more costly type of mistakes.</p><div class="section" title="Boosting the accuracy of decision trees"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec31"/>Boosting the accuracy of decision trees</h3></div></div></div><p>One way the <a id="id412" class="indexterm"/>C5.0 algorithm improved upon the C4.5 algorithm was <a id="id413" class="indexterm"/>through the addition of <span class="strong"><strong>adaptive boosting</strong></span>. This is a process in which many decision trees are built and the trees vote on the best class for each example.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>The idea of boosting is based largely upon the research by Rob Schapire and Yoav Freund. For more information, try searching the web for their publications or their recent textbook <span class="emphasis"><em>Boosting: Foundations and Algorithms</em></span>. The MIT Press (2012).</p></div></div><p>As boosting can be applied more generally to any machine learning algorithm, it is covered in detail later in this book in <a class="link" href="ch11.html" title="Chapter 11. Improving Model Performance">Chapter 11</a>, <span class="emphasis"><em>Improving Model Performance</em></span>. For now, it suffices to say that boosting is rooted in the notion that by combining a number of weak performing learners, you can create a team that is much stronger than any of the learners alone. Each of the models has a unique set of strengths and weaknesses and they may be better or worse in solving certain problems. Using a combination of several learners with complementary strengths and weaknesses can therefore dramatically improve the accuracy of a classifier.</p><p>The <code class="literal">C5.0()</code> function makes it easy to add boosting to our C5.0 decision tree. We simply need to add an additional <code class="literal">trials</code> parameter indicating the number of separate decision trees to use in the boosted team. The <code class="literal">trials</code> parameter sets an upper limit; the algorithm will stop adding trees <a id="id414" class="indexterm"/>if it recognizes that additional trials do not seem to be improving the accuracy. We'll start with 10 trials, a number that has become the de facto standard, as research suggests that this reduces error rates on test data by about 25 percent:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_boost10 &lt;- C5.0(credit_train[-17], credit_train$default,</strong></span>
<span class="strong"><strong>                         trials = 10)</strong></span>
</pre></div><p>While examining the resulting model, we can see that some additional lines have been added, indicating the changes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_boost10</strong></span>
<span class="strong"><strong>Number of boosting iterations: 10 </strong></span>
<span class="strong"><strong>Average tree size: 47.5</strong></span>
</pre></div><p>Across the 10 iterations, our tree size shrunk. If you would like, you can see all 10 trees by typing <code class="literal">summary(credit_boost10)</code> at the command prompt. It also lists the model's performance on the training data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(credit_boost10)</strong></span>

<span class="strong"><strong>     (a)   (b)    &lt;-classified as</strong></span>
<span class="strong"><strong>    ----  ----</strong></span>
<span class="strong"><strong>     629     4    (a): class no</strong></span>
<span class="strong"><strong>      30   237    (b): class yes</strong></span>
</pre></div><p>The classifier made 34 mistakes on 900 training examples for an error rate of 3.8 percent. This is quite an improvement over the 13.9 percent training error rate we noted before adding boosting! However, it remains to be seen whether we see a similar improvement on the test data. Let's take a look:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_boost_pred10 &lt;- predict(credit_boost10, credit_test)</strong></span>
<span class="strong"><strong>&gt; CrossTable(credit_test$default, credit_boost_pred10,</strong></span>
<span class="strong"><strong>             prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,</strong></span>
<span class="strong"><strong>             dnn = c('actual default', 'predicted default'))</strong></span>
</pre></div><p>The resulting table is as follows:</p><div class="mediaobject"><img src="graphics/B03905_05_13.jpg" alt="Boosting the accuracy of decision trees"/></div><p>Here, we reduced the total error rate from 27 percent prior to boosting down to 18 percent in the boosted model. It does not seem like a large gain, but it is in fact larger than the 25 percent reduction we expected. On the other hand, the model is still not doing well at predicting defaults, predicting only <span class="emphasis"><em>20/33 = 61%</em></span> correctly. The lack of an even greater improvement may be a function of our relatively small training dataset, or it may just be a very difficult problem to solve.</p><p>This said, if boosting can be added this easily, why not apply it by default to every decision tree? The <a id="id415" class="indexterm"/>reason is twofold. First, if building a decision tree once takes a great deal of computation time, building many trees may be computationally impractical. Secondly, if the training data is very noisy, then boosting might not result in an improvement at all. Still, if greater accuracy is needed, it's worth giving it a try.</p></div><div class="section" title="Making mistakes more costlier than others"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec32"/>Making mistakes more costlier than others</h3></div></div></div><p>Giving a loan out to an applicant who is likely to default can be an expensive mistake. One solution to reduce the number of false negatives may be to reject a larger number of borderline applicants, under the assumption that the interest the bank would earn from a risky loan is far outweighed by the massive loss it would incur if the money is not paid back at all.</p><p>The C5.0 algorithm allows us to assign a penalty to different types of errors, in order to discourage a tree from making more costly mistakes. The penalties are designated in a <span class="strong"><strong>cost matrix</strong></span>, which specifies how much costlier each error is, relative to any other prediction.</p><p>To begin constructing the cost matrix, we need to start by specifying the dimensions. Since the predicted and actual values can both take two values, <code class="literal">yes</code> or <code class="literal">no</code>, we need to describe a 2 x 2 matrix, using a list of two vectors, each with two values. At the same time, we'll also name the matrix dimensions to avoid confusion later on:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; matrix_dimensions &lt;- list(c("no", "yes"), c("no", "yes"))</strong></span>
<span class="strong"><strong>&gt; names(matrix_dimensions) &lt;- c("predicted", "actual")</strong></span>
</pre></div><p>Examining the new object shows that our dimensions have been set up correctly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; matrix_dimensions</strong></span>
<span class="strong"><strong>$predicted</strong></span>
<span class="strong"><strong>[1] "no"  "yes"</strong></span>

<span class="strong"><strong>$actual</strong></span>
<span class="strong"><strong>[1] "no"  "yes"</strong></span>
</pre></div><p>Next, we need to assign the penalty for the various types of errors by supplying four values to fill the matrix. Since R fills a matrix by filling columns one by one from top to bottom, we need to supply the values in a specific order:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Predicted no, actual no</li><li class="listitem" style="list-style-type: disc">Predicted yes, actual no</li><li class="listitem" style="list-style-type: disc">Predicted no, actual yes</li><li class="listitem" style="list-style-type: disc">Predicted yes, actual yes</li></ul></div><p>Suppose we believe that a loan default costs the bank four times as much as a missed opportunity. Our penalty values could then be defined as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; error_cost &lt;- matrix(c(0, 1, 4, 0), nrow = 2,</strong></span>
<span class="strong"><strong>    dimnames = matrix_dimensions)</strong></span>
</pre></div><p>This creates the following matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; error_cost</strong></span>
<span class="strong"><strong>         actual</strong></span>
<span class="strong"><strong>predicted no yes</strong></span>
<span class="strong"><strong>      no   0   4</strong></span>
<span class="strong"><strong>      yes  1   0</strong></span>
</pre></div><p>As defined by this matrix, there is no cost assigned when the algorithm classifies a no or yes correctly, but a false negative has a cost of 4 versus a false positive's cost of 1. To see how this impacts classification, let's apply it to our decision tree using the <code class="literal">costs</code> parameter of the <code class="literal">C5.0()</code> function. We'll otherwise use the same steps as we did earlier:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_cost &lt;- C5.0(credit_train[-17], credit_train$default,</strong></span>
<span class="strong"><strong>                            costs = error_cost)</strong></span>
<span class="strong"><strong>&gt; credit_cost_pred &lt;- predict(credit_cost, credit_test)</strong></span>
<span class="strong"><strong>&gt; CrossTable(credit_test$default, credit_cost_pred,</strong></span>
<span class="strong"><strong>             prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,</strong></span>
<span class="strong"><strong>             dnn = c('actual default', 'predicted default'))</strong></span>
</pre></div><p>This produces the following confusion matrix:</p><div class="mediaobject"><img src="graphics/B03905_05_14.jpg" alt="Making mistakes more costlier than others"/></div><p>Compared to our boosted model, this version makes more mistakes overall: 37 percent error here versus 18 percent in the boosted case. However, the types of mistakes are very different. Where the previous models incorrectly classified only 42 and 61 percent of defaults correctly, in this model, 79 percent of the actual defaults were predicted to be non-defaults. This trade resulting in a reduction of false negatives at the expense of increasing false positives may be acceptable if our cost estimates were accurate.</p></div></div></div>
<div class="section" title="Understanding classification rules"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec26"/>Understanding classification rules</h1></div></div></div><p>Classification rules <a id="id416" class="indexterm"/>represent knowledge in the form of logical if-else statements that assign a class to unlabeled examples. They are specified in terms of an <span class="strong"><strong>antecedent</strong></span> and a <span class="strong"><strong>consequent</strong></span>; these form a hypothesis stating that "if this happens, then that happens." A simple rule might state, "if the hard drive is making a clicking sound, then it is about to fail." The antecedent comprises certain combinations of feature values, while the consequent specifies the class value to assign when the rule's conditions are met.</p><p>Rule learners are often used in a manner similar to decision tree learners. Like decision trees, they can be used for applications that generate knowledge for future action, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Identifying conditions that lead to a hardware failure in mechanical devices</li><li class="listitem" style="list-style-type: disc">Describing the key characteristics of groups of people for customer segmentation</li><li class="listitem" style="list-style-type: disc">Finding conditions that precede large drops or increases in the prices of shares on the stock market</li></ul></div><p>On the other hand, rule learners offer some distinct advantages over trees for some tasks. Unlike a tree, which must be applied from top-to-bottom through a series of decisions, rules are propositions that can be read much like a statement of fact. Additionally, for reasons that will be discussed later, the results of a rule learner can be more simple, direct, and easier to understand <a id="id417" class="indexterm"/>than a decision tree built on the same data.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip66"/>Tip</h3><p>As you will see later in this chapter, rules can be generated using decision trees. So, why bother with a separate group of rule learning algorithms? The reason is that decision trees bring a particular set of biases to the task that a rule learner avoids by identifying the rules directly.</p></div></div><p>Rule learners are generally applied to problems where the features are primarily or entirely nominal. They do well at identifying rare events, even if the rare event occurs only for a very specific interaction among feature values.</p><div class="section" title="Separate and conquer"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec52"/>Separate and conquer</h2></div></div></div><p>Classification rule <a id="id418" class="indexterm"/>learning algorithms utilize a heuristic known as <span class="strong"><strong>separate and conquer</strong></span>. The process involves identifying a rule that covers a subset of <a id="id419" class="indexterm"/>examples in the training data, and then separating this partition from the remaining data. As the rules are added, additional subsets of the data are separated until the entire dataset has been covered and no more examples remain.</p><p>One way to imagine the rule learning process is to think about drilling down into the data by creating increasingly specific rules to identify class values. Suppose you were tasked with creating rules to identify whether or not an animal is a mammal. You could depict the set of all animals as a large space, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B03905_05_15.jpg" alt="Separate and conquer"/></div><p>A rule learner begins by using the available features to find homogeneous groups. For example, using a <a id="id420" class="indexterm"/>feature that indicates whether the species <a id="id421" class="indexterm"/>travels via land, sea, or air, the first rule might suggest that any land-based animals are mammals:</p><div class="mediaobject"><img src="graphics/B03905_05_16.jpg" alt="Separate and conquer"/></div><p>Do you notice any problems with this rule? If you're an animal lover, you might have realized that frogs are amphibians, not mammals. Therefore, our rule needs to be a bit more specific. Let's drill down further by suggesting that mammals must walk on land and have a tail:</p><div class="mediaobject"><img src="graphics/B03905_05_17.jpg" alt="Separate and conquer"/></div><p>An additional rule can be defined to separate out the bats, the only remaining mammal. Thus, this subset can be separated from the other data.</p><p>An additional rule can be <a id="id422" class="indexterm"/>defined to separate out the bats, the only remaining <a id="id423" class="indexterm"/>mammal. A potential feature distinguishing bats from the other remaining animals would be the presence of fur. Using a rule built around this feature, we have then correctly identified all the animals:</p><div class="mediaobject"><img src="graphics/B03905_05_18.jpg" alt="Separate and conquer"/></div><p>At this point, since all of the training instances have been classified, the rule learning process would stop. We learned a total of three rules:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Animals that walk on land and have tails are mammals</li><li class="listitem" style="list-style-type: disc">If the animal does not have fur, it is not a mammal</li><li class="listitem" style="list-style-type: disc">Otherwise, the animal is a mammal</li></ul></div><p>The previous example illustrates how rules gradually consume larger and larger segments of data to eventually classify all instances.</p><p>As the rules seem to cover portions of the data, separate and conquer algorithms are also known as <span class="strong"><strong>covering algorithms</strong></span>, and the resulting rules are called covering rules. In the next section, we will learn how covering rules are applied in practice by examining a simple rule learning algorithm. We will then examine a more complex rule learner, and apply both to a real-world problem.</p></div><div class="section" title="The 1R algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec53"/>The 1R algorithm</h2></div></div></div><p>Suppose a television game show has a wheel <a id="id424" class="indexterm"/>with ten evenly sized colored slices. Three of the segments were colored red, three were blue, and four were white. Prior to spinning the wheel, you are asked to choose one of these colors. When the wheel stops spinning, if the color shown matches your prediction, you will win a large cash prize. What color should you pick?</p><p>If you choose white, you are, of course, more likely to win the prize—this is the most common color on the wheel. Obviously, this game show is a bit ridiculous, but it demonstrates the simplest <a id="id425" class="indexterm"/>classifier, <span class="strong"><strong>ZeroR</strong></span>, a rule learner that literally learns no rules (hence the name). For every unlabeled example, regardless of the values of its features, it predicts the most common class.</p><p>The <span class="strong"><strong>1R algorithm</strong></span> (<span class="strong"><strong>One Rule</strong></span> or <span class="strong"><strong>OneR</strong></span>), improves over ZeroR by selecting a single rule. Although this may seem overly simplistic, it tends to perform better than you might expect. As demonstrated in empirical studies, the accuracy of this algorithm can approach that of much more sophisticated algorithms for many real-world tasks.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>For an in-depth look at the surprising performance of 1R, see Holte RC. <span class="emphasis"><em>Very simple classification rules perform well on most commonly used datasets</em></span>. Machine Learning. 1993; 11:63-91.</p></div></div><p>The strengths and weaknesses <a id="id426" class="indexterm"/>of the 1R algorithm are shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strengths</p>
</th><th style="text-align: left" valign="bottom">
<p>Weaknesses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Generates a single, easy-to-understand, human-readable rule of thumb</li><li class="listitem" style="list-style-type: disc">Often performs surprisingly well</li><li class="listitem" style="list-style-type: disc">Can serve as a benchmark for more complex algorithms</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Uses only a single feature</li><li class="listitem" style="list-style-type: disc">Probably overly simplistic</li></ul></div>
</td></tr></tbody></table></div><p>The way this algorithm works is simple. For each feature, 1R divides the data into groups based on similar values <a id="id427" class="indexterm"/>of the feature. Then, for each segment, the algorithm predicts the majority class. The error rate for the rule based on each feature is calculated and the rule with the fewest errors is chosen as the one rule.</p><p>The following tables show how this would work for the animal data we looked at earlier in this section:</p><div class="mediaobject"><img src="graphics/B03905_05_19.jpg" alt="The 1R algorithm"/></div><p>For the <span class="strong"><strong>Travels By</strong></span> feature, the dataset was divided into three groups: <span class="strong"><strong>Air</strong></span>, <span class="strong"><strong>Land</strong></span>, and <span class="strong"><strong>Sea</strong></span>. Animals in the <span class="strong"><strong>Air</strong></span> and <span class="strong"><strong>Sea</strong></span> groups were predicted to be non-mammal, while animals in the <span class="strong"><strong>Land</strong></span> group were predicted to be mammals. This resulted in two errors: bats and frogs. The <span class="strong"><strong>Has Fur</strong></span> feature divided animals into two groups. Those with fur were predicted to be mammals, while those without fur were not predicted to be mammals. Three errors were counted: pigs, elephants, and rhinos. As the <span class="strong"><strong>Travels By</strong></span> feature results in fewer errors, the 1R algorithm will return the following "one rule" based on <span class="strong"><strong>Travels By</strong></span>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If the animal travels by air, it is not a mammal</li><li class="listitem" style="list-style-type: disc">If the animal travels by land, it is a mammal</li><li class="listitem" style="list-style-type: disc">If the animal <a id="id428" class="indexterm"/>travels by sea, it is not a mammal</li></ul></div><p>The algorithm stops here, having found the single most important rule.</p><p>Obviously, this rule learning algorithm may be too basic for some tasks. Would you want a medical diagnosis system to consider only a single symptom, or an automated driving system to stop or accelerate your car based on only a single factor? For these types of tasks, a more sophisticated rule learner might be useful. We'll learn about one in the following section.</p></div><div class="section" title="The RIPPER algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec54"/>The RIPPER algorithm</h2></div></div></div><p>Early rule learning <a id="id429" class="indexterm"/>algorithms were plagued by a couple of <a id="id430" class="indexterm"/>problems. First, they were notorious for being slow, which made them ineffective for the increasing number of large datasets. Secondly, they were often prone to being inaccurate on noisy data.</p><p>A first step <a id="id431" class="indexterm"/>toward solving these problems was proposed in 1994 by Johannes Furnkranz and Gerhard Widmer. Their <span class="strong"><strong>Incremental Reduced Error Pruning (IREP) algorithm</strong></span> uses a combination of pre-pruning and post-pruning methods that grow very complex rules and prune them before separating the instances from the full dataset. Although this strategy helped the performance of rule learners, decision trees often still performed better.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>For more information on IREP, see Furnkranz J, Widmer G. <span class="emphasis"><em>Incremental Reduced Error Pruning</em></span>. Proceedings of the 11<sup>th</sup> International Conference on Machine Learning. 1994: 70-77.</p></div></div><p>Rule <a id="id432" class="indexterm"/>learners took another step forward in 1995 when William W. Cohen introduced the <span class="strong"><strong>Repeated Incremental Pruning to Produce Error Reduction (RIPPER) algorithm</strong></span>, which improved upon IREP to generate rules that match or exceed the performance of decision trees.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>For more detail on RIPPER, see Cohen WW. <span class="emphasis"><em>Fast effective rule induction</em></span>. Proceedings of the 12<sup>th</sup> International Conference on Machine Learning. 1995:115-123.</p></div></div><p>As outlined in the following table, the strengths and weaknesses of RIPPER are generally comparable to decision trees. The chief benefit is that they may result in a slightly more parsimonious model:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strengths</p>
</th><th style="text-align: left" valign="bottom">
<p>Weaknesses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Generates easy-to-understand, human-readable rules</li><li class="listitem" style="list-style-type: disc">Efficient on large and noisy datasets</li><li class="listitem" style="list-style-type: disc">Generally produces a simpler model than a comparable decision tree</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">May result in rules that seem to defy common sense or expert knowledge</li><li class="listitem" style="list-style-type: disc">Not ideal for working with numeric data</li><li class="listitem" style="list-style-type: disc">Might not perform as well as more complex models</li></ul></div>
</td></tr></tbody></table></div><p>Having evolved from several iterations of rule learning algorithms, the RIPPER algorithm is a patchwork of efficient heuristics for rule learning. Due to its complexity, a discussion of the technical implementation details is beyond the scope of this book. However, it can be understood in general terms as a three-step process:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Grow</li><li class="listitem">Prune</li><li class="listitem">Optimize</li></ol></div><p>The growing phase uses the separate and conquer technique to greedily add conditions to a rule until it perfectly classifies a subset of data or runs out of attributes for splitting. Similar to decision trees, the information gain criterion is used to identify the next splitting attribute. When increasing a rule's specificity no longer reduces entropy, the rule is immediately pruned. Steps one and two are repeated until it reaches a stopping criterion, at which point the entire set of rules is optimized using a variety of heuristics.</p><p>The RIPPER algorithm can create much more complex rules than can the 1R algorithm, as in can consider more <a id="id433" class="indexterm"/>than one feature. This means that it can create rules with multiple antecedents such as "if an animal flies and has fur, then it is a mammal." This improves the algorithm's ability to model complex data, but just like decision trees, it means that the rules can quickly become more difficult to comprehend.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>The evolution of classification rule learners didn't stop with RIPPER. New rule learning algorithms are being proposed rapidly. A survey of literature shows algorithms called IREP++, SLIPPER, TRIPPER, among many others.</p></div></div></div><div class="section" title="Rules from decision trees"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec55"/>Rules from decision trees</h2></div></div></div><p>Classification <a id="id434" class="indexterm"/>rules can also be obtained directly <a id="id435" class="indexterm"/>from decision trees. Beginning at a leaf node and following the branches back to the root, you will have obtained a series of decisions. These can be combined into a single rule. The following figure shows how rules could be constructed from the decision tree to predict movie success:</p><div class="mediaobject"><img src="graphics/B03905_05_20.jpg" alt="Rules from decision trees"/></div><p>Following the paths from the root node down to each leaf, the rules would be:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">If the number of celebrities is low, then the movie will be a <span class="strong"><strong>Box Office Bust</strong></span>.</li><li class="listitem">If the number of celebrities is high and the budget is high, then the movie will be a <span class="strong"><strong>Mainstream Hit</strong></span>.</li><li class="listitem">If the number of celebrities is high and the budget is low, then the movie will be a <span class="strong"><strong>Critical Success</strong></span>.</li></ol></div><p>For reasons that will be <a id="id436" class="indexterm"/>made clear in the following <a id="id437" class="indexterm"/>section, the chief downside to using a decision tree to generate rules is that the resulting rules are often more complex than those learned by a rule learning algorithm. The divide and conquer strategy employed by decision trees biases the results differently than that of a rule learner. On the other hand, it is sometimes more computationally efficient to generate rules from trees.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip67"/>Tip</h3><p>The <code class="literal">C5.0()</code> function in the <code class="literal">C50</code> package will generate a model using classification rules if you specify <code class="literal">rules = TRUE</code> when training the model.</p></div></div></div><div class="section" title="What makes trees and rules greedy?"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec56"/>What makes trees and rules greedy?</h2></div></div></div><p>Decision trees and <a id="id438" class="indexterm"/>rule learners are known as <span class="strong"><strong>greedy learners</strong></span> because they use data on a first-come, first-served basis. Both the divide and conquer heuristic used by decision trees and the separate and conquer heuristic used by rule learners attempt to make partitions one at a time, finding the most homogeneous partition first, followed by the next best, and so on, until all examples have been classified.</p><p>The downside to the greedy approach is that greedy algorithms are not guaranteed to generate the optimal, most accurate, or smallest number of rules for a particular dataset. By taking the low-hanging fruit early, a greedy learner may quickly find a single rule that is accurate for one subset of data; however, in doing so, the learner may miss the opportunity to develop a more nuanced set of rules with better overall accuracy on the entire set of data. However, without using the greedy approach to rule learning, it is likely that for all but the smallest of datasets, rule learning would be computationally infeasible.</p><div class="mediaobject"><img src="graphics/B03905_05_21.jpg" alt="What makes trees and rules greedy?"/></div><p>Though both trees and rules employ greedy learning heuristics, there are subtle differences in how they build rules. Perhaps the best way to distinguish them is to note that once divide and conquer splits on a feature, the partitions created by the split may not be re-conquered, only further subdivided. In this way, a tree is permanently limited by its history of past decisions. In contrast, once separate and conquer finds a rule, any examples not covered by all of the rule's conditions may be re-conquered.</p><p>To illustrate this contrast, consider the previous case in which we built a rule learner to determine whether an animal was a mammal. The rule learner identified three rules that perfectly classify the example animals:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Animals that walk on land and have tails are mammals (bears, cats, dogs, elephants, pigs, rabbits, rats, rhinos)</li><li class="listitem" style="list-style-type: disc">If the animal does not has fur, it is not a mammal (birds, eels, fish, frogs, insects, sharks)</li><li class="listitem" style="list-style-type: disc">Otherwise, the animal is a mammal (bats)</li></ul></div><p>In contrast, a decision tree built on the same data might have come up with four rules to achieve the same perfect classification:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If an animal walks <a id="id439" class="indexterm"/>on land and has fur, then it is a mammal (bears, cats, dogs, elephants, pigs, rabbits, rats, rhinos)</li><li class="listitem" style="list-style-type: disc">If an animal walks on land and does not have fur, then it is not a mammal (frogs)</li><li class="listitem" style="list-style-type: disc">If the animal does not walk on land and has fur, then it is a mammal (bats)</li><li class="listitem" style="list-style-type: disc">If the animal does not walk on land and does not have fur, then it is not a mammal (birds, insects, sharks, fish, eels)<div class="mediaobject"><img src="graphics/B03905_05_22.jpg" alt="What makes trees and rules greedy?"/></div></li></ul></div><p>The different result across these two approaches has to do with what happens to the frogs after they are separated by the "walk on land" decision. Where the rule learner allows frogs to be re-conquered by the "does not have fur" decision, the decision tree cannot modify the existing partitions, and therefore must place the frog into its own rule.</p><p>On one hand, because rule learners can reexamine cases that were considered but ultimately not covered as part of prior rules, rule learners often find a more parsimonious set of rules than those generated <a id="id440" class="indexterm"/>from decision trees. On the other hand, this reuse of data means that the computational cost of rule learners may be somewhat higher than for decision trees.</p></div></div>
<div class="section" title="Example &#x2013; identifying poisonous mushrooms with rule learners"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec27"/>Example – identifying poisonous mushrooms with rule learners</h1></div></div></div><p>Each <a id="id441" class="indexterm"/>year, many people fall ill and sometimes even die from ingesting poisonous wild mushrooms. Since many mushrooms are very similar to each other in appearance, occasionally even experienced mushroom gatherers are poisoned.</p><p>Unlike the identification of harmful plants such as a poison oak or poison ivy, there are no clear rules such as "leaves of three, let them be" to identify whether a wild mushroom is poisonous or edible. Complicating matters, many traditional rules, such as "poisonous mushrooms are brightly colored," provide dangerous or misleading information. If simple, clear, and consistent rules were available to identify poisonous mushrooms, they could save the lives of foragers.</p><p>Because one of the strengths of rule learning algorithms is the fact that they generate easy-to-understand rules, they seem like an appropriate fit for this classification task. However, the rules will only be as useful as they are accurate.</p><div class="section" title="Step 1 – collecting data"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec57"/>Step 1 – collecting data</h2></div></div></div><p>To identify <a id="id442" class="indexterm"/>rules for distinguishing poisonous mushrooms, we will utilize the Mushroom dataset by Jeff Schlimmer of Carnegie Mellon University. The raw dataset is available freely at the UCI Machine Learning Repository (<a class="ulink" href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>).</p><p>The dataset includes information on 8,124 mushroom samples from 23 species of gilled mushrooms listed in <span class="emphasis"><em>Audubon Society Field Guide to North American Mushrooms</em></span> (1981). In the Field Guide, each of the mushroom species is identified "definitely edible," "definitely poisonous," or "likely poisonous, and not recommended to be eaten." For the purposes of this dataset, the latter group was combined with the "definitely poisonous" group to make two classes: poisonous and nonpoisonous. The data dictionary available on the UCI website describes the 22 features of the mushroom samples, including characteristics such as <a id="id443" class="indexterm"/>cap shape, cap color, odor, gill size and color, stalk shape, and habitat.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip68"/>Tip</h3><p>This chapter uses a slightly modified version of the mushroom data. If you plan on following along with the example, download the <code class="literal">mushrooms.csv</code> file from the Packt Publishing website and save it in your R working directory.</p></div></div></div><div class="section" title="Step 2 – exploring and preparing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec58"/>Step 2 – exploring and preparing the data</h2></div></div></div><p>We begin by <a id="id444" class="indexterm"/>using <code class="literal">read.csv()</code>, to import the data for our analysis. Since all the 22 features and the target class are <a id="id445" class="indexterm"/>nominal, in this case, we will set <code class="literal">stringsAsFactors = TRUE</code> and take advantage of the automatic factor conversion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mushrooms &lt;- read.csv("mushrooms.csv", stringsAsFactors = TRUE)</strong></span>
</pre></div><p>The output of the <code class="literal">str(mushrooms)</code> command notes that the data contain 8,124 observations of 23 variables as the data dictionary had described. While most of the <code class="literal">str()</code> output is unremarkable, one feature is worth mentioning. Do you notice anything peculiar about the <code class="literal">veil_type</code> variable in the following line?</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ veil_type : Factor w/ 1 level "partial": 1 1 1 1 1 1 ...</strong></span>
</pre></div><p>If you think it is odd that a factor has only one level, you are correct. The data dictionary lists two levels for this feature: partial and universal. However, all the examples in our data are classified as partial. It is likely that this data element was somehow coded incorrectly. In any case, since the veil type does not vary across samples, it does not provide any useful information for prediction. We will drop this variable from our analysis using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mushrooms$veil_type &lt;- NULL</strong></span>
</pre></div><p>By assigning <code class="literal">NULL</code> to the veil type vector, R eliminates the feature from the <code class="literal">mushrooms</code> data frame.</p><p>Before going much further, we should take a quick look at the distribution of the mushroom <code class="literal">type</code> class variable in our dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(mushrooms$type)</strong></span>
<span class="strong"><strong>   edible poisonous </strong></span>
<span class="strong"><strong>     4208      3916</strong></span>
</pre></div><p>About 52 percent of the mushroom samples (<span class="emphasis"><em>N = 4,208</em></span>) are edible, while 48 percent (<span class="emphasis"><em>N = 3,916</em></span>) are poisonous. </p><p>For the purposes of this experiment, we will consider the 8,214 samples in the mushroom data to be an <a id="id446" class="indexterm"/>exhaustive set of all the possible wild mushrooms. This is an important assumption, because it means <a id="id447" class="indexterm"/>that we do not need to hold some samples out of the training data for testing purposes. We are not trying to develop rules that cover unforeseen types of mushrooms; we are merely trying to find rules that accurately depict the complete set of known mushroom types. Therefore, we can build and test the model on the same data.</p></div><div class="section" title="Step 3 – training a model on the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec59"/>Step 3 – training a model on the data</h2></div></div></div><p>If we trained a <a id="id448" class="indexterm"/>hypothetical ZeroR classifier on this data, what would it predict? Since ZeroR ignores all of the features and simply predicts the target's mode, in plain language, its rule would state that all the mushrooms are edible. Obviously, this is not a very helpful classifier, because it would leave a mushroom gatherer sick or dead with nearly half of the mushroom samples bearing the possibility of being poisonous. Our rules will need to do much better than this in order to provide safe advice that can be published. At the same time, we need simple rules that are easy to remember.</p><p>Since simple rules can often be extremely predictive, let's see how a very simple rule learner performs on the mushroom data. Toward the end, we will apply the 1R classifier, which will identify the most predictive single feature of the target class and use it to construct a set of rules.</p><p>We will use the 1R implementation in the <code class="literal">RWeka</code> package called <code class="literal">OneR()</code>. You may recall that we had installed <code class="literal">RWeka</code> in <a class="link" href="ch01.html" title="Chapter 1. Introducing Machine Learning">Chapter 1</a>, <span class="emphasis"><em>Introducing Machine Learning</em></span>, as a part of the tutorial on installing and loading packages. If you haven't installed the package per these instructions, you will need to use the <code class="literal">install.packages("RWeka")</code> command and have Java installed on your system (refer to the installation instructions for more details). With these steps complete, load the package by typing <code class="literal">library(RWeka)</code>:</p><div class="mediaobject"><img src="graphics/B03905_05_23.jpg" alt="Step 3 – training a model on the data"/></div><p>The <code class="literal">OneR()</code> implementation uses the R formula syntax to specify the model to be trained. The formula syntax uses the <code class="literal">~</code> operator (known as the tilde) to express the relationship between a target variable and its predictors. The class variable to be learned goes to the left of the tilde, and the predictor features are written on the right, separated by <code class="literal">+</code> operators. If you like to model the relationship between the <code class="literal">y</code> class and predictors <code class="literal">x1</code> and <code class="literal">x2</code>, you could write the formula as <code class="literal">y ~ x1 + x2</code>. If you like to include all the variables in the model, the special term <code class="literal">.</code> can be used. For example, <code class="literal">y ~ .</code> specifies the relationship between <code class="literal">y</code> and all the other features in the dataset.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip69"/>Tip</h3><p>The R formula syntax is used across many R functions and offers some powerful features to describe the relationships among predictor variables. We will explore some of these features in the later chapters. However, if you're eager for a sneak peek, feel free to read the documentation using the <code class="literal">?formula</code> command.</p></div></div><p>Using the <code class="literal">type ~ .</code> formula, we will allow our first <code class="literal">OneR()</code> rule learner to consider all the possible features in the mushroom data while constructing its rules to predict type:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mushroom_1R &lt;- OneR(type ~ ., data = mushrooms)</strong></span>
</pre></div><p>To examine the <a id="id449" class="indexterm"/>rules it created, we can type the name of the classifier object, in this case, <code class="literal">mushroom_1R</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mushroom_1R</strong></span>

<span class="strong"><strong>odor:</strong></span>
<span class="strong"><strong>  almond  -&gt; edible</strong></span>
<span class="strong"><strong>  anise  -&gt; edible</strong></span>
<span class="strong"><strong>  creosote  -&gt; poisonous</strong></span>
<span class="strong"><strong>  fishy  -&gt; poisonous</strong></span>
<span class="strong"><strong>  foul  -&gt; poisonous</strong></span>
<span class="strong"><strong>  musty  -&gt; poisonous</strong></span>
<span class="strong"><strong>  none  -&gt; edible</strong></span>
<span class="strong"><strong>  pungent  -&gt; poisonous</strong></span>
<span class="strong"><strong>  spicy  -&gt; poisonous</strong></span>
<span class="strong"><strong>(8004/8124 instances correct)</strong></span>
</pre></div><p>In the first line of the output, we see that the odor feature was selected for rule generation. The categories of odor, such as almond, anise, and so on, specify rules for whether the mushroom is likely to be edible or poisonous. For instance, if the mushroom smells fishy, foul, musty, pungent, spicy, or like creosote, the mushroom is likely to be poisonous. On the other hand, mushrooms with more pleasant smells like almond and anise, and those with no smell at all are predicted to be edible. For the purposes of a field guide for mushroom gathering, these rules could be summarized in a simple rule of thumb: "if the mushroom smells unappetizing, then it is likely to be poisonous."</p></div><div class="section" title="Step 4 – evaluating model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec60"/>Step 4 – evaluating model performance</h2></div></div></div><p>The last line <a id="id450" class="indexterm"/>of the output notes that the rules correctly predicted the edibility of 8,004 of the 8,124 mushroom samples or nearly 99 percent of the mushroom samples. We can obtain additional details about the classifier using the <code class="literal">summary()</code> function, as <a id="id451" class="indexterm"/>shown in the following example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(mushroom_1R)</strong></span>

<span class="strong"><strong>=== Summary ===</strong></span>
<span class="strong"><strong>Correctly Classified Instances        8004  98.5229 %</strong></span>
<span class="strong"><strong>Incorrectly Classified Instances       120  1.4771 %</strong></span>
<span class="strong"><strong>Kappa statistic                          0.9704</strong></span>
<span class="strong"><strong>Mean absolute error                      0.0148</strong></span>
<span class="strong"><strong>Root mean squared error                  0.1215</strong></span>
<span class="strong"><strong>Relative absolute error                  2.958  %</strong></span>
<span class="strong"><strong>Root relative squared error             24.323  %</strong></span>
<span class="strong"><strong>Coverage of cases (0.95 level)          98.5229 %</strong></span>
<span class="strong"><strong>Mean rel. region size (0.95 level)      50      %</strong></span>
<span class="strong"><strong>Total Number of Instances             8124     </strong></span>

<span class="strong"><strong>=== Confusion Matrix ===</strong></span>
<span class="strong"><strong>    a    b   &lt;-- classified as</strong></span>
<span class="strong"><strong> 4208    0 |    a = edible</strong></span>
<span class="strong"><strong>  120 3796 |    b = poisonous</strong></span>
</pre></div><p>The section labeled <code class="literal">Summary</code> lists a number of different ways to measure the performance of our 1R classifier. We will cover many of these statistics later on in <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span>, so we will ignore them for now.</p><p>The section labeled <code class="literal">Confusion Matrix</code> is similar to those used before. Here, we can see where our rules went wrong. The key is displayed on the right, with <code class="literal">a = edible</code> and <code class="literal">b = poisonous</code>. Table columns indicate the predicted class of the mushroom while the table rows separate the 4,208 edible mushrooms from the 3,916 poisonous mushrooms. Examining the table, we can see that although the 1R classifier did not classify any edible mushrooms as poisonous, it did classify 120 poisonous mushrooms as edible—which makes for an incredibly dangerous mistake! </p><p>Considering that the learner utilized only a single feature, it did reasonably well; if one avoids unappetizing smells when foraging for mushrooms, they will almost avoid a trip to the hospital. That said, close does not cut it when lives are involved, not to mention the field guide publisher might not be happy about the prospect of a lawsuit when its readers fall ill. Let's see if we can add a few more rules and develop an even better classifier.</p></div><div class="section" title="Step 5 – improving model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec61"/>Step 5 – improving model performance</h2></div></div></div><p>For a more <a id="id452" class="indexterm"/>sophisticated rule learner, we will use <code class="literal">JRip()</code>, a Java-based implementation of the RIPPER rule learning algorithm. As with the 1R implementation we used previously, <code class="literal">JRip()</code> is included in the <code class="literal">RWeka</code> package. If you have not done so yet, be sure to load the package using the <code class="literal">library(RWeka)</code> command:</p><div class="mediaobject"><img src="graphics/B03905_05_24.jpg" alt="Step 5 – improving model performance"/></div><p>As shown in the syntax box, the process of training a <code class="literal">JRip()</code> model is very similar to how we previously trained a <code class="literal">OneR()</code> model. This is one of the pleasant benefits of the functions in the <code class="literal">RWeka</code> package; the syntax is consistent across algorithms, which makes the process of comparing a number of different models very simple.</p><p>Let's train the <code class="literal">JRip()</code> rule learner as we did with <code class="literal">OneR()</code>, allowing it to choose rules from all the available features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mushroom_JRip &lt;- JRip(type ~ ., data = mushrooms)</strong></span>
</pre></div><p>To examine the rules, type the name of the classifier:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mushroom_JRip</strong></span>

<span class="strong"><strong>JRIP rules:</strong></span>
<span class="strong"><strong>===========</strong></span>
<span class="strong"><strong>(odor = foul) =&gt; type=poisonous (2160.0/0.0)</strong></span>
<span class="strong"><strong>(gill_size = narrow) and (gill_color = buff) =&gt; type=poisonous (1152.0/0.0)</strong></span>
<span class="strong"><strong>(gill_size = narrow) and (odor = pungent) =&gt; type=poisonous (256.0/0.0)</strong></span>
<span class="strong"><strong>(odor = creosote) =&gt; type=poisonous (192.0/0.0)</strong></span>
<span class="strong"><strong>(spore_print_color = green) =&gt; type=poisonous (72.0/0.0)</strong></span>
<span class="strong"><strong>(stalk_surface_below_ring = scaly) and (stalk_surface_above_ring = silky) =&gt; type=poisonous (68.0/0.0)</strong></span>
<span class="strong"><strong>(habitat = leaves) and (cap_color = white) =&gt; type=poisonous (8.0/0.0)</strong></span>
<span class="strong"><strong>(stalk_color_above_ring = yellow) =&gt; type=poisonous (8.0/0.0)</strong></span>
<span class="strong"><strong> =&gt; type=edible (4208.0/0.0)</strong></span>
<span class="strong"><strong>Number of Rules : 9</strong></span>
</pre></div><p>The <code class="literal">JRip()</code> classifier learned a total of nine rules from the mushroom data. An easy way to read these rules is to think of them as a list of if-else statements, similar to programming <a id="id453" class="indexterm"/>logic. The first three rules could be expressed as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If the odor is foul, then the mushroom type is poisonous</li><li class="listitem" style="list-style-type: disc">If the gill size is narrow and the gill color is buff, then the mushroom type is poisonous</li><li class="listitem" style="list-style-type: disc">If the gill size is narrow and the odor is pungent, then the mushroom type is poisonous</li></ul></div><p>Finally, the ninth rule implies that any mushroom sample that was not covered by the preceding eight rules is edible. Following the example of our programming logic, this can be read as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Else, the mushroom is edible</li></ul></div><p>The numbers next to each rule indicate the number of instances covered by the rule and a count of misclassified instances. Notably, there were no misclassified mushroom samples using these nine rules. As a result, the number of instances covered by the last rule is exactly equal to the number of edible mushrooms in the data (<span class="emphasis"><em>N = 4,208</em></span>).</p><p>The following <a id="id454" class="indexterm"/>figure provides a rough illustration of how the rules are applied to the mushroom data. If you imagine everything within the oval as all the species of mushroom, the rule learner identified features or sets of features, which separate homogeneous segments from the larger group. First, the algorithm found a large group of poisonous mushrooms uniquely distinguished by their foul odor. Next, it found smaller and more specific groups of poisonous mushrooms. By identifying covering rules for each of the varieties of poisonous mushrooms, all of the remaining mushrooms were found to be edible. Thanks to Mother Nature, each variety of mushrooms was unique enough that the classifier was able to achieve 100 percent accuracy.</p><div class="mediaobject"><img src="graphics/B03905_05_25.jpg" alt="Step 5 – improving model performance"/></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Summary</h1></div></div></div><p>This chapter covered two classification methods that use so-called "greedy" algorithms to partition the data according to feature values. Decision trees use a divide and conquer strategy to create flowchart-like structures, while rule learners separate and conquer data to identify logical if-else rules. Both methods produce models that can be interpreted without a statistical background.</p><p>One popular and highly configurable decision tree algorithm is C5.0. We used the C5.0 algorithm to create a tree to predict whether a loan applicant will default. Using options for boosting and cost-sensitive errors, we were able to improve our accuracy and avoid risky loans that would cost the bank more money.</p><p>We also used two rule learners, 1R and RIPPER, to develop rules to identify poisonous mushrooms. The 1R algorithm used a single feature to achieve 99 percent accuracy in identifying potentially fatal mushroom samples. On the other hand, the set of nine rules generated by the more sophisticated RIPPER algorithm correctly identified the edibility of each mushroom.</p><p>This chapter merely scratched the surface of how trees and rules can be used. In <a class="link" href="ch06.html" title="Chapter 6. Forecasting Numeric Data – Regression Methods">Chapter 6</a>, <span class="emphasis"><em>Forecasting Numeric Data – Regression Methods</em></span>, we will learn techniques known as regression trees and model trees, which use decision trees for numeric prediction rather than classification. In <a class="link" href="ch11.html" title="Chapter 11. Improving Model Performance">Chapter 11</a>, <span class="emphasis"><em>Improving Model Performance</em></span>, we will discover how the performance of decision trees can be improved by grouping them together in a model known as a random forest. In <a class="link" href="ch08.html" title="Chapter 8. Finding Patterns – Market Basket Analysis Using Association Rules">Chapter 8</a>, <span class="emphasis"><em>Finding Patterns – Market Basket Analysis Using Association Rules</em></span>, we will see how association rules—a relative of classification rules—can be used to identify groups of items in transactional data.</p></div></body></html>
["```py\nJupyter Notebook\n```", "```py\npip3 install pandas\n```", "```py\nconda install pandas\n```", "```py\n#Package Imports\n\nimport pandas as pd\n\n#Reading in the dataset\n\ndf = pd.read_csv('PS_20174392719_1491204439457_log.csv')\n\n#Viewing the first 5 rows of the dataset\n\ndf.head()\n```", "```py\n#Dropping the redundant features\n\ndf = df.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)\n```", "```py\n#Storing the fraudulent data into a dataframe\n\ndf_fraud = df[df['isFraud'] == 1]\n\n#Storing the non-fraudulent data into a dataframe \n\ndf_nofraud = df[df['isFraud'] == 0]\n\n#Storing 12,000 rows of non-fraudulent data\n\ndf_nofraud = df_nofraud.head(12000)\n\n#Joining both datasets together \n\ndf = pd.concat([df_fraud, df_nofraud], axis = 0)\n```", "```py\n#Package Imports\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n#Converting the type column to categorical\n\ndf['type'] = df['type'].astype('category')\n\n#Integer Encoding the 'type' column\n\ntype_encode = LabelEncoder()\n\n#Integer encoding the 'type' column\n\ndf['type'] = type_encode.fit_transform(df.type)\n```", "```py\n#One hot encoding the 'type' column\n\ntype_one_hot = OneHotEncoder()\n\ntype_one_hot_encode = type_one_hot.fit_transform(df.type.values.reshape(-1,1)).toarray()\n\n#Adding the one hot encoded variables to the dataset \n\nohe_variable = pd.DataFrame(type_one_hot_encode, columns = [\"type_\"+str(int(i)) for i in range(type_one_hot_encode.shape[1])])\n\ndf = pd.concat([df, ohe_variable], axis=1)\n\n#Dropping the original type variable \n\ndf = df.drop('type', axis = 1)\n\n#Viewing the new dataframe after one-hot-encoding \n\ndf.head()\n```", "```py\n#Checking every column for missing values\n\ndf.isnull().any()\n```", "```py\n#Imputing the missing values with a 0\n\ndf = df.fillna(0)\n```", "```py\ndf.to_csv('fraud_prediction.csv')\n```", "```py\n#Creating the features \n\nfeatures = df.drop('isFraud', axis = 1).values\ntarget = df['isFraud'].values\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)\n```", "```py\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Initializing the kNN classifier with 3 neighbors \n\nknn_classifier = KNeighborsClassifier(n_neighbors=3)\n\n#Fitting the classifier on the training data \n\nknn_classifier.fit(X_train, y_train)\n\n#Extracting the accuracy score from the test sets\n\nknn_classifier.score(X_test, y_test)\n```", "```py\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\n\n#Initializing a grid with possible number of neighbors from 1 to 24\n\ngrid = {'n_neighbors' : np.arange(1, 25)}\n\n#Initializing a k-NN classifier \n\nknn_classifier = KNeighborsClassifier()\n\n#Using cross validation to find optimal number of neighbors \n\nknn = GridSearchCV(knn_classifier, grid, cv = 10)\n\nknn.fit(X_train, y_train)\n\n#Extracting the optimal number of neighbors \n\nknn.best_*params_\n\n#Extracting the accuracy score for optimal number of neighbors\n\nknn.best_score_* \n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n#Setting up the scaling pipeline \n\npipeline_order = [('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors = 1))]\n\npipeline = Pipeline(pipeline_order)\n\n#Fitting the classfier to the scaled dataset \n\nknn_classifier_scaled = pipeline.fit(X_train, y_train)\n\n#Extracting the score \n\nknn_classifier_scaled.score(X_test, y_test)\n```"]
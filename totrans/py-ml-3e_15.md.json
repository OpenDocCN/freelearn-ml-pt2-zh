["```py\n>>> import numpy as np\n>>> def conv1d(x, w, p=0, s=1):\n...     w_rot = np.array(w[::-1])\n...     x_padded = np.array(x)\n...     if p > 0:\n...         zero_pad = np.zeros(shape=p)\n...         x_padded = np.concatenate([zero_pad,\n...                                    x_padded,\n...                                    zero_pad])\n...     res = []\n...     for i in range(0, int(len(x)/s),s):\n...         res.append(np.sum(x_padded[i:i+w_rot.shape[0]] *\n...                           w_rot))\n...     return np.array(res)\n>>> ## Testing:\n>>> x = [1, 3, 2, 4, 5, 6, 1, 3]\n>>> w = [1, 0, 3, 1, 2]\n>>> print('Conv1d Implementation:',\n...       conv1d(x, w, p=2, s=1))\nConv1d Implementation: [ 5\\. 14\\. 16\\. 26\\. 24\\. 34\\. 19\\. 22.]\n>>> print('NumPy Results:',\n...       np.convolve(x, w, mode='same'))\nNumPy Results: [ 5 14 16 26 24 34 19 22] \n```", "```py\n>>> import numpy as np\n>>> import scipy.signal\n>>> def conv2d(X, W, p=(0, 0), s=(1, 1)):\n...     W_rot = np.array(W)[::-1,::-1]\n...     X_orig = np.array(X)\n...     n1 = X_orig.shape[0] + 2*p[0]\n...     n2 = X_orig.shape[1] + 2*p[1]\n...     X_padded = np.zeros(shape=(n1, n2))\n...     X_padded[p[0]:p[0]+X_orig.shape[0],\n...              p[1]:p[1]+X_orig.shape[1]] = X_orig\n...\n...     res = []\n...     for i in range(0, int((X_padded.shape[0] - \\\n...                            W_rot.shape[0])/s[0])+1, s[0]):\n...         res.append([])\n...         for j in range(0, int((X_padded.shape[1] - \\\n...                                W_rot.shape[1])/s[1])+1, s[1]):\n...             X_sub = X_padded[i:i+W_rot.shape[0],\n...                              j:j+W_rot.shape[1]]\n...             res[-1].append(np.sum(X_sub * W_rot))\n...     return(np.array(res))\n>>> X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\n>>> W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n>>> print('Conv2d Implementation:\\n',\n...       conv2d(X, W, p=(1, 1), s=(1, 1)))\nConv2d Implementation:\n[[ 11\\.  25\\.  32\\.  13.]\n [ 19\\.  25\\.  24\\.  13.]\n [ 13\\.  28\\.  25\\.  17.]\n [ 11\\.  17\\.  14\\.   9.]]\n>>> print('SciPy Results:\\n',\n...       scipy.signal.convolve2d(X, W, mode='same'))\nSciPy Results:\n[[11 25 32 13]\n [19 25 24 13]\n [13 28 25 17]\n [11 17 14  9]] \n```", "```py\n>>> import tensorflow as tf\n>>> img_raw = tf.io.read_file('example-image.png')\n>>> img = tf.image.decode_image(img_raw)\n>>> print('Image shape:', img.shape)\nImage shape: (252, 221, 3) \n```", "```py\n> conda install imageio \n```", "```py\n> pip install imageio \n```", "```py\n>>> import imageio\n>>> img = imageio.imread('example-image.png')\n>>> print('Image shape:', img.shape)\nImage shape: (252, 221, 3)\n>>> print('Number of channels:', img.shape[2])\nNumber of channels: 3\n>>> print('Image data type:', img.dtype)\nImage data type: uint8\n>>> print(img[100:102, 100:102, :])\n[[[179 134 110]\n [182 136 112]]\n[[180 135 11]\n [182 137 113]]] \n```", "```py\n>>> from tensorflow import keras \n>>> conv_layer = keras.layers.Conv2D(\n...     filters=16,\n...     kernel_size=(3,3),\n...     kernel_regularizer=keras.regularizers.l2(0.001))\n>>> fc_layer = keras.layers.Dense(\n...     units=16,\n...     kernel_regularizer=keras.regularizers.l2(0.001)) \n```", "```py\n>>> import tensorflow_datasets as tfds\n>>> ####### Binary Crossentropy\n>>> bce_probas = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n>>> bce_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n>>> logits = tf.constant([0.8])\n>>> probas = tf.keras.activations.sigmoid(logits)\n>>> tf.print(\n...     'BCE (w Probas): {:.4f}'.format(\n...     bce_probas(y_true=[1], y_pred=probas)),\n...     '(w Logits): {:.4f}'.format(\n...     bce_logits(y_true=[1], y_pred=logits)))\nBCE (w Probas): 0.3711 (w Logits): 0.3711\n>>> ####### Categorical Crossentropy\n>>> cce_probas = tf.keras.losses.CategoricalCrossentropy(\n...     from_logits=False)\n>>> cce_logits = tf.keras.losses.CategoricalCrossentropy(\n...     from_logits=True)\n>>> logits = tf.constant([[1.5, 0.8, 2.1]])\n>>> probas = tf.keras.activations.softmax(logits)\n>>> tf.print(\n...     'CCE (w Probas): {:.4f}'.format(\n...     cce_probas(y_true=[0, 0, 1], y_pred=probas)),\n...     '(w Logits): {:.4f}'.format(\n...     cce_logits(y_true=[0, 0, 1], y_pred=logits)))\nCCE (w Probas): 0.5996 (w Logits): 0.5996\n>>> ####### Sparse Categorical Crossentropy\n>>> sp_cce_probas = tf.keras.losses.SparseCategoricalCrossentropy(\n...     from_logits=False)\n>>> sp_cce_logits = tf.keras.losses.SparseCategoricalCrossentropy(\n...     from_logits=True)\n>>> tf.print(\n...     'Sparse CCE (w Probas): {:.4f}'.format(\n...     sp_cce_probas(y_true=[2], y_pred=probas)),\n...     '(w Logits): {:.4f}'.format(\n...     sp_cce_logits(y_true=[2], y_pred=logits)))\nSparse CCE (w Probas): 0.5996 (w Logits): 0.5996 \n```", "```py\n>>> import tensorflow_datasets as tfds\n>>> ## Loading the data\n>>> mnist_bldr = tfds.builder('mnist')\n>>> mnist_bldr.download_and_prepare()\n>>> datasets = mnist_bldr.as_dataset(shuffle_files=False)\n>>> mnist_train_orig = datasets['train']\n>>> mnist_test_orig = datasets['test'] \n```", "```py\n>>> BUFFER_SIZE = 10000\n>>> BATCH_SIZE = 64\n>>> NUM_EPOCHS = 20\n>>> mnist_train = mnist_train_orig.map(\n...     lambda item: (tf.cast(item['image'], tf.float32)/255.0,\n...                   tf.cast(item['label'], tf.int32)))\n>>> mnist_test = mnist_test_orig.map(\n...     lambda item: (tf.cast(item['image'], tf.float32)/255.0,\n...                   tf.cast(item['label'], tf.int32)))\n>>> tf.random.set_seed(1)\n>>> mnist_train = mnist_train.shuffle(buffer_size=BUFFER_SIZE,\n...                   reshuffle_each_iteration=False)\n>>> mnist_valid = mnist_train.take(10000).batch(BATCH_SIZE)\n>>> mnist_train = mnist_train.skip(10000).batch(BATCH_SIZE) \n```", "```py\n>>> model = tf.keras.Sequential()\n>>> model.add(tf.keras.layers.Conv2D(\n...     filters=32, kernel_size=(5, 5),\n...     strides=(1, 1), padding='same',\n...     data_format='channels_last',\n...     name='conv_1', activation='relu'))\n>>> model.add(tf.keras.layers.MaxPool2D(\n...     pool_size=(2, 2), name='pool_1'))\n>>> model.add(tf.keras.layers.Conv2D(\n...     filters=64, kernel_size=(5, 5),\n...     strides=(1, 1), padding='same',\n...     name='conv_2', activation='relu'))\n>>> model.add(tf.keras.layers.MaxPool2D(\n...     pool_size=(2, 2), name='pool_2')) \n```", "```py\n>>> model.compute_output_shape(input_shape=(16, 28, 28, 1))\nTensorShape([16, 7, 7, 64]) \n```", "```py\n>>> model.add(tf.keras.layers.Flatten())\n>>> model.compute_output_shape(input_shape=(16, 28, 28, 1))\nTensorShape([16, 3136]) \n```", "```py\n>>> model.add(tf.keras.layers.Dense(\n...     units=1024, name='fc_1',\n...     activation='relu'))\n>>> model.add(tf.keras.layers.Dropout(\n...     rate=0.5))\n>>> model.add(tf.keras.layers.Dense(\n...     units=10, name='fc_2',\n...     activation='softmax')) \n```", "```py\n>>> tf.random.set_seed(1)\n>>> model.build(input_shape=(None, 28, 28, 1))\n>>> model.compile(\n...     optimizer=tf.keras.optimizers.Adam(),\n...     loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n...     metrics=['accuracy']) \n```", "```py\n>>> history = model.fit(mnist_train, epochs=NUM_EPOCHS,\n...                     validation_data=mnist_valid,\n...                     shuffle=True)\nEpoch 1/20\n782/782 [==============================] - 35s 45ms/step - loss: 0.1450 - accuracy: 0.8882 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 2/20\n782/782 [==============================] - 34s 43ms/step - loss: 0.0472 - accuracy: 0.9833 - val_loss: 0.0507 - val_accuracy: 0.9839\n..\nEpoch 20/20\n782/782 [==============================] - 34s 44ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0488 - val_accuracy: 0.9920 \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> hist = history.history\n>>> x_arr = np.arange(len(hist['loss'])) + 1\n>>> fig = plt.figure(figsize=(12, 4))\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n>>> ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n>>> ax.legend(fontsize=15)\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n>>> ax.plot(x_arr, hist['val_accuracy'], '--<', \n...         label='Validation acc.')\n>>> ax.legend(fontsize=15)\n>>> plt.show() \n```", "```py\n>>> test_results = model.evaluate(mnist_test.batch(20))\n>>> print('Test Acc.: {:.2f}\\%'.format(test_results[1]*100))\nTest Acc.: 99.39% \n```", "```py\n>>> batch_test = next(iter(mnist_test.batch(12)))\n>>> preds = model(batch_test[0])\n>>> tf.print(preds.shape)\nTensorShape([12, 10])\n>>> preds = tf.argmax(preds, axis=1)\n>>> print(preds)\ntf.Tensor([6 2 3 7 2 2 3 4 7 6 6 9], shape=(12,), dtype=int64)\n>>> fig = plt.figure(figsize=(12, 4))\n>>> for i in range(12):\n...     ax = fig.add_subplot(2, 6, i+1)\n...     ax.set_xticks([]); ax.set_yticks([])\n...     img = batch_test[0][i, :, :, 0]\n...     ax.imshow(img, cmap='gray_r')\n...     ax.text(0.9, 0.1, '{}'.format(preds[i]),\n...             size=15, color='blue',\n...             horizontalalignment='center',\n...             verticalalignment='center',\n...             transform=ax.transAxes)\n>>> plt.show() \n```", "```py\n>>> import tensorflow as tf\n>>> import tensorflow_datasets as tfds\n>>> celeba_bldr = tfds.builder('celeb_a')\n>>> celeba_bldr.download_and_prepare()\n>>> celeba = celeba_bldr.as_dataset(shuffle_files=False)\n>>> celeba_train = celeba['train']\n>>> celeba_valid = celeba['validation']\n>>> celeba_test = celeba['test']\n>>>\n>>> def count_items(ds):\n...     n = 0\n...     for _ in ds:\n...         n += 1\n...     return n\n>>> print('Train set:  {}'.format(count_items(celeba_train)))\nTrain set:  162770\n>>> print('Validation: {}'.format(count_items(celeba_valid)))\nValidation: 19867\n>>> print('Test set:   {}'.format(count_items(celeba_test)))\nTest set:   19962 \n```", "```py\n>>> celeba_train = celeba_train.take(16000)\n>>> celeba_valid = celeba_valid.take(1000)\n>>> print('Train set:  {}'.format(count_items(celeba_train)))\nTrain set:  16000\n>>> print('Validation: {}'.format(count_items(celeba_valid)))\nValidation: 1000 \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> # take 5 examples\n>>> examples = []\n>>> for example in celeba_train.take(5):\n...     examples.append(example['image'])\n>>> fig = plt.figure(figsize=(16, 8.5))\n>>> ## Column 1: cropping to a bounding-box\n>>> ax = fig.add_subplot(2, 5, 1)\n>>> ax.set_title('Crop to a \\nbounding-box', size=15)\n>>> ax.imshow(examples[0])\n>>> ax = fig.add_subplot(2, 5, 6)\n>>> img_cropped = tf.image.crop_to_bounding_box(\n...     examples[0], 50, 20, 128, 128)\n>>> ax.imshow(img_cropped)\n>>> ## Column 2: flipping (horizontally)\n>>> ax = fig.add_subplot(2, 5, 2)\n>>> ax.set_title('Flip (horizontal)', size=15)\n>>> ax.imshow(examples[1])\n>>> ax = fig.add_subplot(2, 5, 7)\n>>> img_flipped = tf.image.flip_left_right(examples[1])\n>>> ax.imshow(img_flipped)\n>>> ## Column 3: adjust contrast\n>>> ax = fig.add_subplot(2, 5, 3)\n>>> ax.set_title('Adjust constrast', size=15)\n>>> ax.imshow(examples[2])\n>>> ax = fig.add_subplot(2, 5, 8)\n>>> img_adj_contrast = tf.image.adjust_contrast(\n...     examples[2], contrast_factor=2)\n>>> ax.imshow(img_adj_contrast)\n>>> ## Column 4: adjust brightness\n>>> ax = fig.add_subplot(2, 5, 4)\n>>> ax.set_title('Adjust brightness', size=15)\n>>> ax.imshow(examples[3])\n>>> ax = fig.add_subplot(2, 5, 9)\n>>> img_adj_brightness = tf.image.adjust_brightness(\n...     examples[3], delta=0.3)\n>>> ax.imshow(img_adj_brightness)\n>>> ## Column 5: cropping from image center\n>>> ax = fig.add_subplot(2, 5, 5)\n>>> ax.set_title('Centeral crop\\nand resize', size=15)\n>>> ax.imshow(examples[4])\n>>> ax = fig.add_subplot(2, 5, 10)\n>>> img_center_crop = tf.image.central_crop(\n...     examples[4], 0.7)\n>>> img_resized = tf.image.resize(\n...     img_center_crop, size=(218, 178))\n>>> ax.imshow(img_resized.numpy().astype('uint8'))\n>>> plt.show() \n```", "```py\n>>> tf.random.set_seed(1)\n>>> fig = plt.figure(figsize=(14, 12))\n>>> for i,example in enumerate(celeba_train.take(3)):\n...     image = example['image']\n...\n...     ax = fig.add_subplot(3, 4, i*4+1)\n...     ax.imshow(image)\n...     if i == 0:\n...         ax.set_title('Orig', size=15)\n...\n...     ax = fig.add_subplot(3, 4, i*4+2)\n...     img_crop = tf.image.random_crop(image, size=(178, 178, 3))\n...     ax.imshow(img_crop)\n...     if i == 0:\n...         ax.set_title('Step 1: Random crop', size=15)\n...\n...     ax = fig.add_subplot(3, 4, i*4+3)\n...     img_flip = tf.image.random_flip_left_right(img_crop)\n...     ax.imshow(tf.cast(img_flip, tf.uint8))\n...     if i == 0:\n...         ax.set_title('Step 2: Random flip', size=15)\n...\n...     ax = fig.add_subplot(3, 4, i*4+4)\n...     img_resize = tf.image.resize(img_flip, size=(128, 128))\n...     ax.imshow(tf.cast(img_resize, tf.uint8))\n...     if i == 0:\n...         ax.set_title('Step 3: Resize', size=15)\n>>> plt.show() \n```", "```py\n>>> def preprocess(example, size=(64, 64), mode='train'):\n...     image = example['image']\n...     label = example['attributes']['Male']\n...     if mode == 'train':\n...         image_cropped = tf.image.random_crop(\n...             image, size=(178, 178, 3))\n...         image_resized = tf.image.resize(\n...             image_cropped, size=size)\n...         image_flip = tf.image.random_flip_left_right(\n...             image_resized)\n...         return image_flip/255.0, tf.cast(label, tf.int32)\n...     else: # use center- instead of \n...           # random-crops for non-training data\n...         image_cropped = tf.image.crop_to_bounding_box(\n...             image, offset_height=20, offset_width=0,\n...             target_height=178, target_width=178)\n...         image_resized = tf.image.resize(\n...             image_cropped, size=size)\n...         return image_resized/255.0, tf.cast(label, tf.int32) \n```", "```py\n>>> tf.random.set_seed(1)\n>>> ds = celeba_train.shuffle(1000, reshuffle_each_iteration=False)\n>>> ds = ds.take(2).repeat(5)\n>>> ds = ds.map(lambda x:preprocess(x, size=(178, 178), mode='train'))\n>>> fig = plt.figure(figsize=(15, 6))\n>>> for j,example in enumerate(ds):\n...     ax = fig.add_subplot(2, 5, j//2+(j%2)*5+1)\n...     ax.set_xticks([])\n...     ax.set_yticks([])\n...     ax.imshow(example[0])\n>>> plt.show() \n```", "```py\n>>> import numpy as np\n>>> BATCH_SIZE = 32\n>>> BUFFER_SIZE = 1000\n>>> IMAGE_SIZE = (64, 64)\n>>> steps_per_epoch = np.ceil(16000/BATCH_SIZE)\n>>> ds_train = celeba_train.map(\n...     lambda x: preprocess(x, size=IMAGE_SIZE, mode='train'))\n>>> ds_train = ds_train.shuffle(buffer_size=BUFFER_SIZE).repeat()\n>>> ds_train = ds_train.batch(BATCH_SIZE)\n>>> ds_valid = celeba_valid.map(\n...     lambda x: preprocess(x, size=IMAGE_SIZE, mode='eval'))\n>>> ds_valid = ds_valid.batch(BATCH_SIZE) \n```", "```py\n>>> model = tf.keras.Sequential([\n...     tf.keras.layers.Conv2D(\n...         32, (3, 3), padding='same', activation='relu'),\n...     tf.keras.layers.MaxPooling2D((2, 2)),\n...     tf.keras.layers.Dropout(rate=0.5),\n...     \n...     tf.keras.layers.Conv2D(\n...         64, (3, 3), padding='same', activation='relu'),\n...     tf.keras.layers.MaxPooling2D((2, 2)),\n...     tf.keras.layers.Dropout(rate=0.5),\n...     \n...     tf.keras.layers.Conv2D(\n...         128, (3, 3), padding='same', activation='relu'),\n...     tf.keras.layers.MaxPooling2D((2, 2)),\n...     \n...     tf.keras.layers.Conv2D(\n...         256, (3, 3), padding='same', activation='relu')\n>>>     ]) \n```", "```py\n>>> model.compute_output_shape(input_shape=(None, 64, 64, 3))\nTensorShape([None, 8, 8, 256]) \n```", "```py\n>>> model.add(tf.keras.layers.GlobalAveragePooling2D())\n>>> model.compute_output_shape(input_shape=(None, 64, 64, 3))\nTensorShape([None, 256]) \n```", "```py\n>>> model.add(tf.keras.layers.Dense(1, activation=None))\n>>> tf.random.set_seed(1)\n>>> model.build(input_shape=(None, 64, 64, 3))\n>>> model.summary()\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              multiple                  896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) multiple                  0         \n_________________________________________________________________\ndropout (Dropout)            multiple                  0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            multiple                  18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 multiple                  0         \n_________________________________________________________________\ndropout_1 (Dropout)          multiple                  0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            multiple                  73856     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 multiple                  0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            multiple                  295168    \n_________________________________________________________________\nglobal_average_pooling2d (Gl multiple                  0         \n_________________________________________________________________\ndense (Dense)                multiple                  257       \n=================================================================\nTotal params: 388,673\nTrainable params: 388,673\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```py\n>>> model.compile(optimizer=tf.keras.optimizers.Adam(),\n...     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n...               metrics=['accuracy'])\n>>> history = model.fit(ds_train, validation_data=ds_valid,\n...                     epochs=20,\n...                     steps_per_epoch=steps_per_epoch) \n```", "```py\n>>> hist = history.history\n>>> x_arr = np.arange(len(hist['loss'])) + 1\n>>> fig = plt.figure(figsize=(12, 4))\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n>>> ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n>>> ax.legend(fontsize=15)\n>>> ax.set_xlabel('Epoch', size=15)\n>>> ax.set_ylabel('Loss', size=15)\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n>>> ax.plot(x_arr, hist['val_accuracy'], '--<',\n...         label='Validation acc.')\n>>> ax.legend(fontsize=15)\n>>> ax.set_xlabel('Epoch', size=15)\n>>> ax.set_ylabel('Accuracy', size=15)\n>>> plt.show() \n```", "```py\n>>> history = model.fit(ds_train, validation_data=ds_valid,\n...                     epochs=30, initial_epoch=20,\n...                     steps_per_epoch=steps_per_epoch) \n```", "```py\n>>> ds_test = celeba_test.map(\n...   lambda x:preprocess(x, size=IMAGE_SIZE, mode='eval')).batch(32)\n>>> test_results = model.evaluate(ds_test)\n>>> print('Test Acc: {:.2f}%'.format(test_results[1]*100))\nTest Acc: 94.75% \n```", "```py\n>>> ds = ds_test.unbatch().take(10)\n>>> pred_logits = model.predict(ds.batch(10))\n>>> probas = tf.sigmoid(pred_logits)\n>>> probas = probas.numpy().flatten()*100\n>>> fig = plt.figure(figsize=(15, 7))\n>>> for j,example in enumerate(ds):\n...     ax = fig.add_subplot(2, 5, j+1)\n...     ax.set_xticks([]); ax.set_yticks([])\n...     ax.imshow(example[0])\n...     if example[1].numpy() == 1:\n...         label='M'\n...     else:\n...         label = 'F'\n...     ax.text(\n...         0.5, -0.15, 'GT: {:s}\\nPr(Male)={:.0f}%'\n...         ''.format(label, probas[j]),\n...         size=16,\n...         horizontalalignment='center',\n...         verticalalignment='center',\n...         transform=ax.transAxes)\n>>> plt.tight_layout()\n>>> plt.show() \n```"]
["```py\nimport numpy as np \nimport pandas as pd \nimport requests \nimport matplotlib.pyplot as plt \n%matplotlib inline \n```", "```py\nr = requests.get('https://www.renthop.com/nyc/apartments-for-rent') \nr.content \n```", "```py\nfrom bs4 import BeautifulSoup \n\nsoup = BeautifulSoup(r.content, \"html5lib\") \n```", "```py\nlisting_divs = soup.select('div[class*=search-info]') \nlisting_divs \n```", "```py\nlen(listing_divs) \n```", "```py\nlisting_divs[0] \n```", "```py\nlisting_divs[0].select('a[id*=title]')[0]['href'] \n```", "```py\nhref = current_listing.select('a[id*=title]')[0]['href'] \naddy = current_listing.select('a[id*=title]')[0].string \nhood = current_listing.select('div[id*=hood]')[0]\\ \n       .string.replace('\\n','') \n```", "```py\nprint(href) \nprint(addy) \nprint(hood) \n```", "```py\nlisting_specs = listing_divs[0].select('table[id*=info] tr') \nfor spec in listing_specs: \n    spec_data = spec.text.strip().replace(' ', '_').split() \n    print(spec_data) \n```", "```py\nlisting_list = [] \nfor idx in range(len(listing_divs)): \n    indv_listing = [] \n    current_listing = listing_divs[idx] \n    href = current_listing.select('a[id*=title]')[0]['href'] \n    addy = current_listing.select('a[id*=title]')[0].string \n    hood = current_listing.select('div[id*=hood]')[0]\\ \n    .string.replace('\\n','') \n  indv_listing.append(href) \n    indv_listing.append(addy) \n    indv_listing.append(hood) \n\n    listing_specs = current_listing.select('table[id*=info] tr') \n    for spec in listing_specs: \n        try: \n            indv_listing.extend(spec.text.strip()\\ \n                                .replace(' ', '_').split()) \n        except: \n            indv_listing.extend(np.nan) \n    listing_list.append(indv_listing) \n```", "```py\nlisting_list \n```", "```py\nurl_prefix = \"https://www.renthop.com/search/nyc?max_price=50000&min_price=0&page=\" \npage_no = 1 \nurl_suffix = \"&sort=hopscore&q=&search=0\" \n\nfor i in range(3): \n    target_page = url_prefix + str(page_no) + url_suffix \n    print(target_page) \n    page_no += 1 \n```", "```py\ndef parse_data(listing_divs): \n    listing_list = [] \n    for idx in range(len(listing_divs)): \n        indv_listing = [] \n        current_listing = listing_divs[idx] \n        href = current_listing.select('a[id*=title]')[0]['href'] \n        addy = current_listing.select('a[id*=title]')[0].string \n        hood = current_listing.select('div[id*=hood]')[0]\\ \n        .string.replace('\\n','') \n\n        indv_listing.append(href) \n        indv_listing.append(addy) \n        indv_listing.append(hood) \n\n        listing_specs = current_listing.select('table[id*=info] tr') \n        for spec in listing_specs: \n            try: \n                values = spec.text.strip().replace(' ', '_').split() \n                clean_values = [x for x in values if x != '_'] \n                indv_listing.extend(clean_values) \n            except: \n                indv_listing.extend(np.nan) \n        listing_list.append(indv_listing) \n    return listing_list \n```", "```py\nall_pages_parsed = [] \nfor i in range(100): \n    target_page = url_prefix + str(page_no) + url_suffix \n    print(target_page) \n    r = requests.get(target_page) \n\n    soup = BeautifulSoup(r.content, 'html5lib') \n\n    listing_divs = soup.select('div[class*=search-info]') \n\n    one_page_parsed = parse_data(listing_divs) \n\n    all_pages_parsed.extend(one_page_parsed) \n\n    page_no += 1 \n```", "```py\ndf = pd.DataFrame(all_pages_parsed, columns=['url', 'address', 'neighborhood', 'rent', 'beds', 'baths']) \n\ndf \n```", "```py\ndf['beds'].unique() \n```", "```py\ndf['baths'].unique() \n```", "```py\ndf['beds'] = df['beds'].map(lambda x: x[1:] if x.startswith('_') else x) \ndf['baths'] = df['baths'].map(lambda x: x[1:] if x.startswith('_') else x) \n```", "```py\ndf['beds'].unique() \n```", "```py\ndf['baths'].unique() \n```", "```py\ndf.describe() \n```", "```py\ndf['rent'] = df['rent'].map(lambda x: str(x).replace('$','').replace(',','')).astype('int') \ndf['beds'] = df['beds'].map(lambda x: x.replace('_Bed', '')) \ndf['beds'] = df['beds'].map(lambda x: x.replace('Studio', '0')) \ndf['beds'] = df['beds'].map(lambda x: x.replace('Loft', '0')).astype('int') \ndf['baths'] = df['baths'].map(lambda x: x.replace('_Bath', '')).astype('float') \n```", "```py\ndf.dtypes \n```", "```py\ndf.groupby('neighborhood')['rent'].count().to_frame('count')\\ \n.sort_values(by='count', ascending=False) \n```", "```py\ndf[df['neighborhood'].str.contains('Upper East Side')]['neighborhood'].value_counts() \n```", "```py\ndf['neighborhood'] = df['neighborhood'].map(lambda x: x.strip()) \n```", "```py\ndf[df['neighborhood'].str.contains('Upper East Side')]['neighborhood'].value_counts() \n```", "```py\ndf.groupby('neighborhood')['rent'].mean().to_frame('mean')\\ \n.sort_values(by='mean', ascending=False) \n```", "```py\nimport googlemaps \n\ngmaps = googlemaps.Client(key='YOUR_API_KEY_GOES_HERE') \n\nta = df.loc[3,['address']].values[0] + ' '\\ \n+ df.loc[3,['neighborhood']].values[0].split(', ')[-1] \n\nta \n```", "```py\ngeocode_result = gmaps.geocode(ta) \n\ngeocode_result \n```", "```py\nfor piece in geocode_result[0]['address_components']: \n    if 'postal_code' in piece['types'] : \n        print(piece['short_name']) \n```", "```py\nimport re \ndef get_zip(row): \n    try: \n        addy = row['address'] + ' ' + row['neighborhood'].split(', ')[-1] \n        print(addy) \n        if re.match('^\\d+\\s\\w', addy): \n            geocode_result = gmaps.geocode(addy) \n            for piece in geocode_result[0]['address_components']: \n                if 'postal_code' in piece['types']: \n                    return piece['short_name'] \n                else: \n                    pass \n        else: \n            return np.nan \n    except: \n        return np.nan \n\ndf['zip'] = df.apply(get_zip, axis=1) \n```", "```py\ndf[df['zip'].notnull()].count() \n```", "```py\ndf.to_csv('apts_with_zip.csv') \n```", "```py\nzdf = df[df['zip'].notnull()].copy() \n```", "```py\nzdf_mean = zdf.groupby('zip')['rent'].mean().to_frame('avg_rent')\\ \n.sort_values(by='avg_rent', ascending=False).reset_index() \nzdf_mean \n```", "```py\nimport folium \n\nm = folium.Map(location=[40.748817, -73.985428], zoom_start=13) \n\nm.choropleth( \n    geo_data=open('nyc.json').read(), \n    data=zdf_mean, \n    columns=['zip', 'avg_rent'], \n    key_on='feature.properties.postalCode', \n    fill_color='YlOrRd', fill_opacity=0.7, line_opacity=0.2, \n    ) \n\nm \n```", "```py\nimport patsy \nimport statsmodels.api as sm \n\nf = 'rent ~ zip + beds' \ny, X = patsy.dmatrices(f, zdf, return_type='dataframe') \n\nresults = sm.OLS(y, X).fit() \nresults.summary() \n```", "```py\nX \n```", "```py\nX.head() \n```", "```py\nto_pred_idx = X.iloc[0].index \nto_pred_zeros = np.zeros(len(to_pred_idx)) \ntpdf = pd.DataFrame(to_pred_zeros, index=to_pred_idx, columns=['value']) \n\ntpdf \n```", "```py\ntpdf.loc['Intercept'] = 1 \ntpdf.loc['beds'] = 1 \ntpdf.loc['zip[T.10009]'] = 1 \n\ntpdf \n```", "```py\nresults.predict(tpdf['value'].to_frame().T) \n```", "```py\ntpdf['value'] = 0 \ntpdf.loc['Intercept'] = 1 \ntpdf.loc['beds'] = 2 \ntpdf.loc['zip[T.10009]'] = 1 \n```", "```py\nresults.predict(tpdf['value'].to_frame().T) \n```", "```py\ntpdf['value'] = 0 \ntpdf.loc['Intercept'] = 1 \ntpdf.loc['beds'] = 2 \ntpdf.loc['zip[T.10069]'] = 1 \n\nresults.predict(tpdf['value'].to_frame().T) \n```"]
- en: Getting Started with Data Mining Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2003, Linden, Smith, and York of Amazon.com published a paper entitled Item-to-Item
    Collaborative Filtering, which explained how product recommendations at Amazon
    work. Since then, this class of algorithmg has gone on to dominate the industry
    standard for recommendations. Every website or app with a sizeable user base,
    be it Netflix, Amazon, or Facebook, makes use of some form of collaborative filters
    to suggest items (which may be movies, products, or friends):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a10e3bca-ab33-450c-a0f4-ddf773ad5652.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As described in the first chapter, collaborative filters try to leverage the
    power of the community to give reliable, relevant, and sometime, even surprising
    recommendations. If Alice and Bob largely like the same movies (say The Lion King,
    Aladdin, and Toy Story) and Alice also likes Finding Nemo, it is extremely likely
    that Bob, who hasn't watched Finding Nemo, will like it too.
  prefs: []
  type: TYPE_NORMAL
- en: We will be building powerful collaborative filters in the next chapter. However,
    before we do that, it is important that we have a good grasp of the underlying
    techniques, principles, and algorithms that go into building collaborative filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarity measures**: Given two items, how do we mathematically quantify
    how different or similar they are to each other? Similarity measures help us in
    answering this question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already made use of a similarity measure (the cosine score) while building
    our content recommendation engine. In this chapter, we will be looking at a few
    other popular similarity scores.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: When building collaborative filters, we are usually
    dealing with millions of users rating millions of items. In such cases, our user
    and item vectors are going to be of a dimension in the order of millions. To improve
    performance, speed up calculations, and avoid the curse of dimensionality, it
    is often a good idea to reduce the number of dimensions considerably, while retaining
    most of the information. This section of the chapter will describe techniques
    that do just that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervised learning**: Supervised learning is a class of machine learning
    algorithm that makes use of label data to infer a mapping function that can then
    be used to predict the label (or class) of unlabeled data. We will be looking
    at some of the most popular supervised learning algorithms, such as support vector
    machines, logistic regression, decision trees, and ensembling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: Clustering is a type of unsupervised learning where the algorithm
    tries to divide all the data points into a certain number of clusters. Therefore,
    without the use of a label dataset, the clustering algorithm is able to assign
    classes to all the unlabel points. In this section, we will be looking at k-means
    clustering, a simple but powerful algorithm popularly used in collaborative filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation methods and metrics**: We will take a look at a few evaluation
    metrics that are used to gauge the performance of these algorithms. The metrics
    include accuracy, precision, and recall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The topics covered in this chapter merit an entire textbook. Since this is a
    hands-on recommendation engine tutorial, we will not be delving too deeply into
    the functioning of most of the algorithms. Nor will we code them up from scratch.
    What we will do is gain an understanding of how and when they work, their advantages
    and disadvantages, and their easy-to-use implementations using the scikit-learn
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Collaborative filtering algorithms try to solve the prediction problem (as
    described in the [Chapter 1](c4bff0e9-57b3-44ec-90cb-9e5950696b27.xhtml), *Getting
    Started with Recommender Systems*). In other words, we are given a matrix of i
    users and j items. The value in the ith row and the jth column (denoted by rij)
    denotes the rating given by user i to item j:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6fd11cf-ee20-4af0-bcd8-bd9a3c3acc5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix of i users and j items
  prefs: []
  type: TYPE_NORMAL
- en: Our job is to complete this matrix. In other words, we need to predict all the
    cells in the matrix that we have no data for. For example, in the preceding diagram,
    we are asked to predict whether user E will like the music player item. To accomplish
    this task, some ratings are available (such as User A liking the music player
    and video games) whereas others are not (for instance, we do not know whether
    Users C and D like video games).
  prefs: []
  type: TYPE_NORMAL
- en: Similarity measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the rating matrix in the previous section, we see that every user can be
    represented as a j-dimensional vector where the kth dimension denotes the rating
    given by that user to the kth item. For instance, let 1 denote a like, -1 denote
    a dislike, and 0 denote no rating. Therefore, user B can be represented as (0,
    1, -1, -1). Similarly, every item can also be represented as an i-dimensional
    vector where the kth dimension denotes the rating given to that item by the kth user.
    The video games item is therefore represented as (1, -1, 0, 0, -1).
  prefs: []
  type: TYPE_NORMAL
- en: We have already computed a similarity score for like-dimensional vectors when
    we built our content-based recommendation engine. In this section, we will take
    a look at the other similarity measures and also revisit the cosine similarity
    score in the context of the other scores.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Euclidean distance can be defined as the length of the line segment joining
    the two data points plotted on an *n*-dimensional Cartesian plane. For example,
    consider two points plotted in a 2D plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c808a35-3c9d-4bbe-a6ae-e858a3961159.png)'
  prefs: []
  type: TYPE_IMG
- en: Euclidean distance
  prefs: []
  type: TYPE_NORMAL
- en: The distance, d, between the two points gives us the Euclidean distance and
    its formula in the 2D space is given in the preceding graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'More generally, consider two *n*-dimensional points (or vectors):'
  prefs: []
  type: TYPE_NORMAL
- en: '**v1**: (q1, q2,...., qn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**v2**: (r1, r2,....., rn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, the Euclidean score is mathematically defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5327766a-fae3-4b13-8664-dfec476932a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Euclidean scores can take any value between 0 and infinity. The lower the Euclidean
    score (or distance), the more similar the two vectors are to each other. Let''s
    now define a simple function using NumPy, which allows us to compute the Euclidean
    distance between two *n*-dimensional vectors using the aforementioned formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s define three users who have rated five different movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From the ratings, we can see that users 1 and 2 have extremely different tastes,
    whereas the tastes of users 1 and 3 are largely similar. Let''s see whether the
    Euclidean distance metric is able to capture this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The Euclidean distance between users 1 and 2 comes out to be approximately
    7.48:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Users 1 and 3 have a much smaller Euclidean score between them than users 1
    and 2\. Therefore, in this case, the Euclidean distance was able to satisfactorily
    capture the relationships between our users.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider two users, Alice and Bob, who have rated the same five movies. Alice
    is extremely stingy with her ratings and never gives more than a 4 to any movie.
    On the other hand, Bob is more liberal and never gives anything below a 2 when
    rating movies. Let''s define the matrices representing Alice and Bob and compute
    their Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We get a Euclidean distance of about 2.23\. However, on closer inspection, we
    see that Bob always gives a rating that is one higher than Alice. Therefore, we
    can say that Alice and Bob's ratings are extremely correlated. In other words,
    if we know Alice's rating for a movie, we can compute Bob's rating for the same
    movie with high accuracy (in this case, by just adding 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider another user, Eve, who has the polar opposite tastes to Alice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We get a very high score of 6.32, which indicates that the two people are very
    dissimilar. If we used Euclidean distances, we would not be able to do much beyond
    this. However, on inspection, we see that the sum of Alice's and Eve's ratings
    for a movie always add up to 6\. Therefore, although very different people, one's
    rating can be used to accurately predict the corresponding rating of the other.
    Mathematically speaking, we say Alice's and Eve's ratings are strongly negatively
    correlated.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distances place emphasis on magnitude, and in the process, are not
    able to gauge the degree of similarity or dissimilarity well. This is where the
    Pearson correlation comes into the picture. The Pearson correlation is a score
    between -1 and 1, where -1 indicates total negative correlation (as in the case
    with Alice and Eve) and 1 indicates total positive correlation (as in the case
    with Alice and Bob), whereas 0 indicates that the two entities are in no way correlated
    with each other (or are independent of each other).
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the Pearson correlation is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8d9ac65-0fe2-452f-a394-ecf52c6be691.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/cf6be388-6b92-4140-9b16-06c11f0d25d7.png) denotes the mean of
    all the elements in vector *i*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SciPy package gives us access to a function that computes the Pearson Similarity
    Scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first element of our list output is the Pearson score. We see that Alice
    and Bob have the highest possible similarity score, whereas Alice and Eve have
    the lowest possible score.
  prefs: []
  type: TYPE_NORMAL
- en: Can you guess the similarity score for Bob and Eve?
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we mathematically defined the cosine similarity score
    and used it extensively while building our content-based recommenders:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d9a8b2c-2455-46e6-af8d-24e85bb2a810.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Mathematically, the Cosine similarity is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2d6d4ef-6faa-45c4-a75d-db6d6faa9690.png)'
  prefs: []
  type: TYPE_IMG
- en: The cosine similarity score computes the cosine of the angle between two vectors
    in an *n*-dimensional space. When the cosine score is 1 (or angle is 0), the vectors
    are exactly similar. On the other hand, a cosine score of -1 (or angle 180 degrees)
    denotes that the two vectors are exactly dissimilar to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider two vectors, x and y, both with zero mean. We see that when this
    is the case, the Pearson correlation score is exactly the same as the cosine similarity
    Score. In other words, for centered vectors with zero mean, the Pearson correlation
    is the cosine similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: Different similarity scores are appropriate in different scenarios. For cases
    where the magnitude is important, the Euclidean distance is an appropriate metric
    to use. However, as we saw in the case described in the Pearson correlation subsection,
    magnitude is not as important to us as correlation. Therefore, we will be using
    the Pearson and the cosine similarity scores when building our filters.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main ideas behind collaborative filtering is that if user A has the
    same opinion of a product as user B, then A is also more likely to have the same
    opinion as B on another product than that of a randomly chosen user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering is one of the most popular techniques used in collaborative-filtering
    algorithms. It is a type of unsupervised learning that groups data points into
    different classes in such a way that data points belonging to a particular class
    are more similar to each other than data points belonging to different classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f30db156-43ab-479f-aa83-cdb67d2f265f.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, imagine that all our users were plotted on a two-dimensional Cartesian
    plane, as shown in the preceding graph. The job of a clustering algorithm is to
    assign classes to every point on this plane. Just like the similarity measures,
    there is no one clustering algorithm to rule them all. Each algorithm has its
    specific use case and is suitable only in certain problems. In this section, we
    will be looking only at the k-means clustering algorithm, which will perform a
    satisfactory job is assigning classes to the collection of preceding points. We
    will also see a case where k-means will not prove to be suitable.
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means algorithm is one of the simplest yet most popular machine learning
    algorithms. It takes in the data points and the number of clusters (k) as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it randomly plots k different points on the plane (called centroids).
    After the k centroids are randomly plotted, the following two steps are repeatedly
    performed until there is no further change in the set of k centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: Assignment of points to the centroids: Every data point is assigned to the centroid
    that is the closest to it. The collection of data points assigned to a particular
    centroid is called a cluster. Therefore, the assignment of points to k centroids
    results in the formation of k clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reassignment of centroids: In the next step, the centroid of every cluster
    is recomputed to be the center of the cluster (or the average of all the points
    in the cluster). All the data points are then reassigned to the new centroids:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/df507754-0da0-43a7-8918-9ec46d18fe33.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows a visualization of the steps involved in a k-means
    clustering algorithm, with the number of assigned clusters as two.
  prefs: []
  type: TYPE_NORMAL
- en: Some sections in this chapter make use of the Matplotlib and Seaborn libraries
    for visualizations. You don't need to understand the plotting code written as
    part of this book, but if you're still interested, you can find the official matplotlib
    tutorial at [https://matplotlib.org/users/pyplot_tutorial.html](https://matplotlib.org/users/pyplot_tutorial.html)
    and the official seaborn tutorial at [https://seaborn.pydata.org/tutorial.html](https://seaborn.pydata.org/tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not be implementing the k-means algorithm from scratch. Instead, we
    will use its implementation provided by scikit-learn. As a first step, let''s
    access the data points as plotted in the beginning of this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the most important steps while using the k-means algorithm is determining
    the number of clusters. In this case, it can be clearly seen from the plot (and
    the code) that we''ve plotted the points in such a way that they form three clearly
    separable clusters. Let''s now apply the k-means algorithm via scikit-learn and
    assess its performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the algorithm proves to be extremely successful in identifying
    the three clusters. The three final centroids are also marked with an X on the
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60b43706-0325-4a32-a270-ec3193c351f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Choosing k
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated in the previous subsection, choosing a good value of k is vital to
    the success of the k-means clustering algorithm. The number of clusters can be
    anywhere between 1 and the total number of data points (where each point is assigned
    to its own cluster).
  prefs: []
  type: TYPE_NORMAL
- en: Data in the real world is seldom of the type explored previously, where the
    points formed well defined, visually separable clusters on a two-dimensional plane.
    There are several methods available to determine a good value of K. In this section,
    we will explore the Elbow method of determining k.
  prefs: []
  type: TYPE_NORMAL
- en: The Elbow method computes the sum of squares for each value of  k and chooses
    the elbow point of the sum-of-squares v/s  K plot as the best value for k. The
    elbow point is defined as the value of k at which the sum-of-squares value for
    every subsequent k starts decreasing much more slowly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sum of squares value is defined as the sum of the distances of each data
    point to the centroid of the cluster to which it was assigned. Mathematically,
    it is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/209c345a-a658-4081-9f33-8d91c766aa37.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, Ck is the kth cluster and uk is the corresponding centroid of Ck.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately for us, scikit-learn''s implementation of k-means automatically
    computes the value of sum-of-squares when it is computing the clusters. Let''s
    now visualize the Elbow plot for our data and determine the best value of K:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6b410c80-031b-44d6-9e5a-8bb5f1450210.png)'
  prefs: []
  type: TYPE_IMG
- en: From the plot, it is clear that the Elbow is at K=3\. From what we visualized
    earlier, we know that this is indeed the optimum number of clusters for this data.
  prefs: []
  type: TYPE_NORMAL
- en: Other clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means algorithm, although very powerful, is not ideal for every use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let''s construct a plot with two half moons. Like the preceding
    blobs, scikit-learn gives us a convenient function to plot half-moon clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c23431ea-c235-4672-8fbc-12a742546d04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Will the k-means algorithm be able to figure out the two half moons correctly?
    Let''s find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now visualize what k-means thinks the two clusters that exist for this
    set of data points are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f399828f-41b5-48ce-b1c3-e37c170b798a.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the k-means algorithm doesn't do a very good job of identifying
    the correct clusters. For clusters such as these half moons, another algorithm,
    called spectral clustering, with nearest-neighbor, affinity performs much better.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not go into the workings of spectral clustering. Instead, we will use
    its scikit-learn implementation and assess its performance directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/43fa2b76-f6b8-46fb-8c2d-4535e14d548e.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that spectral clustering does a very good job of identifying the half-moon
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen that different clustering algorithms are appropriate in different
    cases. The same applies to cases of collaborative filters. For instance, the surprise package,
    which we will visit in the next chapter, has an implementation of a collaborative
    filter that makes use of yet another clustering algorithm, called co-clustering.
    We will wrap up our discussion of clustering and move on to another important
    data mining technique: dimensionality reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most machine learning algorithms tend to perform poorly as the number of dimensions
    in the data increases. This phenomenon is often known as the curse of dimensionality.
    Therefore, it is a good idea to reduce the number of features available in the
    data, while retaining the maximum amount of information possible. There are two
    ways to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection**: This method involves identifying the features that have
    the least predictive power and dropping them altogether. Therefore, feature selection
    involves identifying a subset of features that is most important for that particular
    use case. An important distinction of feature selection is that it maintains the
    original meaning of every retained feature. For example, let''s say we have a
    housing dataset with price, area, and number of roomsas features. Now, if we were
    to drop the areafeature, the remaining price and number of roomsfeatures will
    still mean what they did originally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: Feature extraction takes in *m*-dimensional data and
    transforms it into an *n*-dimensional output space (usually where *m* >> *n*),
    while retaining most of the information. However, in doing so, it creates new
    features that have no inherent meaning. For example, if we took the same housing
    dataset and used feature extraction to output it into a 2D space, the new features
    won''t mean price, area, or number of rooms.They will be devoid of any meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will take a look at an important feature-extraction method: **Principal
    component analysis** (or **PCA**).
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Principal component analysis** is an unsupervised feature extraction algorithm
    that takes in *m*-dimensional input to create a set of *n* (*m* >> *n*) linearly
    uncorrelated variables (called principal components) in such a way that the *n *dimensions
    lose as little variance (or information) as possible due to the loss of the (*m*-*n*)
    dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: The linear transformation in PCA is done in such a way that the first principal
    component holds the maximum variance (or information). It does so by considering
    those variables that are highly correlated to each other. Every principal component
    has more variance than every succeeding component and is orthogonal to the preceding
    component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a three-dimensional space where two features are highly correlated
    to each other and relatively uncorrelated to the third:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f8c85c1-cbc2-44e8-8b2a-9c4a34fcd4e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's say that we want to convert this into a two-dimensional space. To do this,
    PCA tries to identify the first principal component, which will hold the maximum
    possible variance. It does so by defining a new dimension using the two highly
    correlated variables. Now, it tries to define the next dimension in such a way
    that it holds the maximum variance, is orthogonal to the first principal component
    constructed, and also is uncorrelated to it. The two new dimensions (or principal
    components), PC 1 and PC 2, are shown in the preceding figure.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the PCA algorithm requires linear algebraic concepts that are
    beyond the scope of this book. Instead, we will use the black box implementation
    of PCA that `scikit-learn` gives us and consider a use case with the well-known
    Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load the Iris dataset from the UCI machine learning repository
    into a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6a8745dd-6316-4c05-9afb-81f42946e2d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The PCA algorithm is extremely sensitive to scale. Therefore, we are going
    to scale all the features in such a way that they have a mean of 0 and a variance
    of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0365714a-25dc-45f9-a203-f1eddb655995.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''re now in a good place to apply the PCA algorithm. Let''s transform our
    data into the two-dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d5b1f7a0-f605-4645-a932-062be42e66e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `scikit-Learn`''s PCA implementation also gives us information about the
    ratio of variance contained by each principal component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We see that the first principal component holds about 72.8% of the information,
    whereas the second principal component holds about 23.3%. In total, 95.8% of the
    information is retained, whereas 4.2% of the information is lost in removing two
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s visualize our data points by class in the new 2D plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d48fdcaf-da25-44dc-826b-40695edab808.png)'
  prefs: []
  type: TYPE_IMG
- en: Other dimensionality reduction techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear-discriminant analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like PCA, linear-discriminant analysis is a linear transformation method that
    aims to transform *m*-dimensional data into an *n*-dimensional output space.
  prefs: []
  type: TYPE_NORMAL
- en: However, unlike PCA, which tries to retain the maximum information, LDA aims
    to identify a set of *n* features that result in the maximum separation (or discrimination)
    of classes. Since LDA requires labeled data in order to determine its components,
    it is a type of supervised learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now apply the LDA algorithm to the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/902577ac-de3d-4433-89f0-5f0c0ee1f823.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the classes are much more separable than in PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Singular value decomposition, or SVD, is a type of matrix analysis technique
    that allows us to represent a high-dimensional matrix in a lower dimension. SVD
    achieves this by identifying and removing the less important parts of the matrix
    and producing an approximation in the desired number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The SVD approach to collaborative filtering was first proposed by Simon Funk
    and proved to be extremely popular and effective during the Netflix prize competition.
    Unfortunately, understanding SVD requires a grasp of linear algebraic topics that
    are beyond the scope of this book. However, we will use a black box implementation
    of the SVD collaborative filter as provided by the `surprise`package in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised learning is a class of machine learning algorithm that takes in a
    series of vectors and their corresponding output (a continuous value or a class)
    as input, and produces an inferred function that can be used to map new examples.
  prefs: []
  type: TYPE_NORMAL
- en: An important precondition for using supervised learning is the availability
    of labeled data. In other words, it is necessary that we have access to input
    for which we already know the correct output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning can be classified into two types: classification and regression.
    A classification problem has a discrete set of values as the target variable (for
    instance, a likeand a dislike), whereas a regression problem has a continuous
    value as its target (for instance, an average rating between one and five).'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the rating matrix defined earlier. It is possible to treat (*m-1*)
    columns as the input and the m^(th) column as the target variable. In this way,
    it should be possible to predict an unavailable value in the m^(th) column by
    passing in the corresponding (m-1) dimensional vector.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is one of the most mature subfields of machine learning
    and, as a result, there are plenty of potent algorithms available for performing
    accurate predictions. In this section, we will look at some of the most popular
    algorithms used successfully in a variety of applications (including collaborative
    filters).
  prefs: []
  type: TYPE_NORMAL
- en: k-nearest neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**k-nearest neighbors** (**k-NN**) is perhaps the simplest machine learning
    algorithm. In the case of classification, it assigns a class to a particular data
    point by a majority vote of its *k* nearest neighbors. In other words, the data
    point is assigned the class that is the most common among its k-nearest neighbors.
    In the case of regression, it computes the average value for the target variable
    based on its k-nearest neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike most machine learning algorithms, k-NN is non-parametric and lazy in
    nature. The former means that k-NN does not make any underlying assumptions about
    the distribution of the data. In other words, the model structure is determined
    by the data. The latter means that k-NN undergoes virtually no training. It only
    computes the k-nearest neighbors of a particular point in the prediction phase.
    This also means that the k-NN model needs to have access to the training data
    at all times and cannot discard it during prediction like its sister algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/d37d07d3-2ce0-4165-ba28-5157422bb1f5.png)'
  prefs: []
  type: TYPE_IMG
- en: k-NN classification is best explained with the help of an example. Consider
    a dataset that has binary classes (represented as the blue squares and the red
    triangles). k-NN now plots this into *n*-dimensional space (in this case, two
    dimensions).
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we want to predict the class of the green circle. Before the k-NN
    algorithm can make predictions, it needs to know the number of nearest neighbors
    that have to be taken into consideration (the value of *k*). *k* is usually odd
    (to avoid ties in the case of binary classification).
  prefs: []
  type: TYPE_NORMAL
- en: Consider the case where *k=3*.
  prefs: []
  type: TYPE_NORMAL
- en: k-NN computes the distance metric (usually the Euclidean distance) from the
    green circle to every other point in the training dataset and selects the three
    data points that are closest to it. In this case, these are the points contained
    in the solid inner circle.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to determine the majority class among the three points. There
    are two red triangles and one blue square. Therefore, the green circle is assigned
    the class of red triangle.
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider the case where *k=5*.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the nearest neighbors are all the points contained within the
    dotted outer circle. This time around, we have two red triangles and three blue
    squares. Therefore, the green circle is assigned the class of blue square.
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding case, it is clear that the value of *k* is extremely significant
    in determining the final class assigned to a data point. It is often a good practice
    to test different values of *k* and assess its performance with your cross-validation
    and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: k-NN regression works in almost the same way. Instead of classes, we compute
    the property values of the k-NN.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we have a housing dataset and we're trying to predict the price
    of a house. The price of a particular house will therefore be determined by the
    average of the prices of the houses of its *k* nearest neighbors. As with classification,
    the final target value may differ depending on the value of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of the algorithms in this section, we will go through only the
    classification process. However, just like k-NN, most algorithms require only
    very slight modifications to be suitable for use in a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The support vector machine is one of the most popular classification algorithms
    used in the industry. It takes in an *n*-dimensional dataset as input and constructs
    an (*n-1*) dimensional hyperplane in such a way that there is maximum separation
    of  classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the visualization of a binary dataset in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cb564ba-4cc1-4bb3-aa45-4849836ccaf5.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph shows three possible hyperplanes (the straight lines) that
    separate the two classes. However, the solid line is the one with the maximum
    margin. In other words, it is the hyperplane that maximally separates the two
    classes. Also, it divides the entire plane into two regions. Any point below the
    hyperplane will be classified as a red square, and any point above will be classified
    as a blue circle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SVM model is only dependent on `support vectors`*; *these are the points
    that determine the maximum margin possible between the two classes. In the preceding
    graph, the filled squares and circles are the support vectors. The rest of the
    points do not have an effect on the workings of the SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/122d0c7b-0c42-4c81-8483-c8a36025727e.png)'
  prefs: []
  type: TYPE_IMG
- en: SVMs are also capable of separating classes that are not linearly separable
    (such as in the preceding figure). It does so with special tools, called radial
    kernel functions, that plot the points in a higher dimension and attempt to construct
    a maximum margin hyperplane there.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are extremely fast and simple tree-based algorithms that branch
    out on features that result in the largest information gain*. *Decision trees,
    although not very accurate, are extremely interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not delve into the inner workings of the decision tree, but we will
    see it in action via a visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc4b45e3-bf2a-43ab-84ad-b2fda443aa18.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's say we want to classify the Iris dataset using a decision tree. A decision
    tree performing the classification is shown in the preceding diagram. We start
    at the top and go deeper into the tree until we reach a leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the petal width of a flower is less than 0.8 cm, we reach a
    leaf node and it gets classified as setosa*. *If not, it goes into the other branch
    and the process continues until a leaf node is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees have an element of randomness in their workings and come up with
    different conditions in different iterations. As stated before, they are also
    not very accurate in their predictions. However, their randomness and fast execution
    make them extremely popular in ensemble models, which will be explained in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main idea behind ensembling is that the predictive power of multiple algorithms
    is much greater than a single algorithm. Decision trees are the most common base
    algorithm used when building ensembling models.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging is short for bootstrap aggregating. Like most other ensemble methods,
    it averages over a large number of base classification models and averages their
    results to deliver its final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps involved in building a bagging model:'
  prefs: []
  type: TYPE_NORMAL
- en: A certain percentage of the data points are sampled (say 10%). The Sampling
    is done with replacement. In other words, a particular data point can appear in
    multiple iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A baseline classification model (typically a decision tree) is trained on this
    sampled data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is repeated until *n* number of models are trained. The final prediction
    delivered by the bagging model is the average of all the predictions of all the
    base models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An improvement on the bagging model is the random forest model. In addition
    to sampling data points, the random forest ensemble method also forces each baseline
    model to randomly select a subset of the features (usually a number equal to the
    square root of the total number of features):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef09911f-dd49-461f-9d27-baa0ad54e2a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting a subset of samples, as well as features, to build the baseline decision
    trees greatly enhances the randomness of each individual tree. This, in turn,
    increases the robustness of the random forest and allows it to perform extremely
    well with noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, building baseline models from a subset of features and analyzing
    their contribution to the final prediction also allows the random forest to determine
    the importance of each feature. It is therefore possible to perform feature-selectionusing
    random forests (recall that feature-selection is a type of dimensionality reduction).
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bagging and the random forest models train baseline models that are completely
    independent of each other. Therefore, they do not learn from the mistakes that
    each learner has made. This is where boosting comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Like random forests, boosting models build a baseline model using a subset of
    samples and features. However, while building the next learners, the boosting
    model tries to rectify the mistakes that the previous learners made. Different
    boosting algorithms do this in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the original boosting algorithm simply added 50% of the misclassified
    samples to the second learner, and all the samples that the first two learners
    disagree upon to build the third and final learner. This ensemble of three learners
    was then used to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting algorithms are extremely robust and routinely provide high performance.
    This makes them extremely popular in data science competitions and, as far as
    we are concerned, in building powerful collaborative filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scikit-learn` gives us access to implementations of all the algorithms
    described in this section. The usage of every algorithm is almost the same. As
    an illustration, let''s apply gradient boosting to classify the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the classifier achieves a 97.3% accuracy on the unseen test data.
    Like random forests, gradient boosting machines are able to gauge the predictive
    power of each feature. Let''s plot the feature importances of the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/66f0e4eb-ffbc-4c0d-9549-39053234113c.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will take a look at a few metrics that will allow us to
    mathematically quantify the performance of our classifiers, regressors, and filters.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Accuracy is the most widely used metric to gauge the performance of a classification
    model. It is the ratio of the number of correct predictions to the total number
    of predictions made by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4e870eb-dca2-45be-900d-2439cd5b75f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Root mean square error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Root Mean Square Error** (or **RMSE**) is a metric widely used to gauge
    the performance of regressors. Mathematically, it is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/619cb11f-b47a-428e-92d9-05bf08b0a841.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/7ca4e738-dbfe-4c82-9f9c-d6ac7ff621a0.png) is the i^(th) real target
    value and ![](img/7ae9ea8f-9150-4c9a-8028-df08658eff7e.png) is the i^(th) predicted
    target value.
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, accuracy does not give us a good estimate of the performance of a
    model.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider a binary class dataset where 99% of the data belongs
    to one class and only 1% of the data belongs to the other class. Now, if a classifier
    were to always predict the majority class for every data point, it would have
    99% accuracy. But that wouldn't mean that the classifier is performing well.
  prefs: []
  type: TYPE_NORMAL
- en: 'For such cases, we make use of other metrics. To understand them, we first
    need to define a few terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive** (**TP**): True positive refers to all cases where the actual
    and the predicted classes are both positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negative** (**TN**): True negative refers to all cases where the actual
    and the predicted classes are both negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive** (**FP**):These are all the cases where the actual class
    is negative but the predicted class is positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative** (**FN**):These are all the cases where the actual class
    is positive but the predicted class is negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To illustrate, consider a test that tries to determine whether a person has
    cancer. If the test predicts that a person does have cancer when in fact they
    don't, it is a false positive. On the other hand, if the test fails to detect
    cancer in a person actually suffering from it, it is a false negative.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The precision is the ratio of the number of positive cases that were correct
    to all the cases that were identified as positive. Mathematically, it looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5aa14fe-1440-4bad-bc9a-f602f717df1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recall is the ratio of the number of positive cases that were identified
    to the all positive cases present in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44d25d38-9af6-489d-8231-4f4aaa40ae3a.png)'
  prefs: []
  type: TYPE_IMG
- en: F1 score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The F1 score is a metric that conveys the balance between precision and recall.
    It is the harmonic mean of the precision and recall. An F1 score of 1 implies
    perfect precision and recall, whereas a score of 0 implies precision and recall
    are not possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcd94ad1-96f6-4e27-84c9-d6f42e1efee2.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered a lot of topics that will help us to build
    powerful collaborative filters. We took a look at clustering, a form of unsupervised
    learning algorithm that could help us to segregate users into well defined clusters.
    Next, we went through a few dimensionality reduction techniques to overcome the
    curse of dimensionality and improve the performance of our learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The subsequent section dealt with supervised learning algorithms, and finally
    we ended the chapter with a brief overview of various evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The topics covered in this chapter merit an entire book and we did not analyze
    the techniques in the depth usually required of machine learning engineers. However,
    what we have learned in this chapter should be sufficient to help us build and
    understand collaborative filters, which is one of the main objectives of this
    book. In case you're interested, a more detailed treatment of the topics presented
    in this chapter is available in an excellent book entitled *Python Machine Learning *by
    Sebastian Thrun.
  prefs: []
  type: TYPE_NORMAL

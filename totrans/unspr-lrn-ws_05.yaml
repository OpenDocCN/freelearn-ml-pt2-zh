- en: 4\. Dimensionality Reduction Techniques and PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will apply dimension reduction techniques and describe the
    concepts behind principal components and dimensionality reduction. We will apply
    **Principal Component Analysis** (**PCA**) when solving problems using scikit-learn.
    We will also compare manual PCA versus scikit-learn. By the end of this chapter,
    you will be able to reduce the size of a dataset by extracting only the most important
    components of variance within the data.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed clustering algorithms and how they can
    be helpful to find underlying meaning in large volumes of data. This chapter investigates
    the use of different feature sets (or spaces) in our unsupervised learning algorithms,
    and we will start with a discussion regarding dimensionality reduction, specifically,
    **Principal Component Analysis** (**PCA**). We will then extend our understanding
    of the benefits of the different feature spaces through an exploration of two
    independently powerful machine learning architectures in neural network-based
    autoencoders. Neural networks certainly have a well-deserved reputation for being
    powerful models in supervised learning problems. Furthermore, through the use
    of an autoencoder stage, neural networks have been shown to be sufficiently flexible
    for their application to unsupervised learning problems. Finally, we will build
    on our neural network implementation and dimensionality reduction as we cover
    t-distributed nearest neighbors in *Chapter 6*, *t-Distributed Stochastic Neighbor
    Embedding*. These techniques will prove helpful when dealing with high-dimensional
    data, such as image processing or datasets with many features. One strong business
    benefit of some types of dimension reduction is that it helps to remove features
    that do not have much impact on final outputs. This creates opportunities to make
    your algorithms more efficient without any loss in performance.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Dimensionality Reduction?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dimensionality reduction is an important tool in any data scientist''s toolkit,
    and due to its wide variety of use cases, is essentially assumed knowledge within
    the field. So, before we can consider reducing the dimensionality and why we would
    want to reduce it, we must first have a good understanding of what dimensionality
    is. To put it simply, dimensionality is the number of dimensions, features, or
    variables associated with a sample of data. Often, this can be thought of as a
    number of columns in a spreadsheet, where each sample is on a new row, and each
    column describes an attribute of the sample. The following table is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Two samples of data with three different features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1: Two samples of data with three different features'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding table, we have two samples of data, each with three independent
    features or dimensions. Depending on the problem being solved, or the origin of
    this dataset, we may want to reduce the number of dimensions per sample without
    losing the provided information. This is where dimensionality reduction can be
    helpful. But how exactly can dimensionality reduction help us to solve problems?
    We will cover the applications in more detail in the following section. However,
    let''s say that we had a very large dataset of time series data, such as echocardiogram
    or ECG (also known as an EKG in some countries) signals, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Electrocardiogram (ECG or EKG)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.2: Electrocardiogram (ECG or EKG)'
  prefs: []
  type: TYPE_NORMAL
- en: 'These signals were captured from your company''s new model of watch, and we
    need to look for signs of a heart attack or stroke. After looking through the
    dataset, we can make a few observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the individual heartbeats are very similar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is some noise in the data from the recording system or from the patient
    moving during the recording.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the noise, the heartbeat signals are still visible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a lot of data – too much to be able to process using the hardware available
    on the watch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is in such a situation that dimensionality reduction really shines. By using
    dimensionality reduction, we are able to remove much of the noise from the signal,
    which, in turn, will assist with the performance of the algorithms that are applied
    to the data as well as reduce the size of the dataset to allow for reduced hardware
    requirements. The techniques that we are going to discuss in this chapter, in
    particular, PCA and autoencoders, have been well applied in research and industry
    to effectively process, cluster, and classify such datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Dimensionality Reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start a detailed investigation of dimensionality reduction and PCA,
    we will discuss some of the common applications for these techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing/feature engineering**: One of the most common applications
    is in the preprocessing or feature engineering stages of developing a machine
    learning solution. The quality of the information provided during the algorithm
    development, as well as the correlation between the input data and the desired
    result, is critical in order for a high-performing solution to be designed. In
    this situation, PCA can provide assistance, as we are able to isolate the most
    important components of information from the data and provide this to the model
    so that only the most relevant information is being provided. This can also have
    a secondary benefit in that we have reduced the number of features being provided
    to the model, so there can be a corresponding reduction in the number of calculations
    to be completed. This can reduce the overall training time for the system. An
    example use case of this feature engineering would be predicting whether a transaction
    is at risk of credit card theft. In this scenario, you may be presented with millions
    of transactions that each have tens or hundreds of features. This would be resource-intensive
    or even impossible to run a predictive algorithm on in real time; however, by
    using feature preprocessing, we can distill the many features down to just the
    top 3-4 most important ones, thereby reducing runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise reduction**: Dimensionality reduction can also be used as an effective
    noise reduction/filtering technique. It is expected that the noise within a signal
    or dataset does not comprise a large component of the variation within the data.
    Thus, we can remove some of the noise from the signal by removing the smaller
    components of variation and then restoring the data back to the original dataspace.
    In the following example, the image on the left has been filtered to the first
    20 most significant sources of data, which gives us the image on the right. We
    can see that the quality of the image has been reduced, but the critical information
    is still there:![Figure 4.3: An image filtered with dimensionality reduction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B15923_04_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.3: An image filtered with dimensionality reduction'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This photograph was taken by Arthur Brognoli from Pexels and is available for
    free use under [https://www.pexels.com/photo-license/](https://www.pexels.com/photo-license/).
    In this case, we have the original image on the left and the filtered image on
    the right.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating plausible artificial datasets**: As PCA divides the dataset into
    the components of information (or variation), we can investigate the effects of
    each component or generate new dataset samples by adjusting the ratios between
    the eigenvalues. We will cover more on eigenvalues later on in this chapter. We
    can scale these components, which, in effect, increases or decreases the importance
    of that specific component. This is also referred to as **statistical shape modeling**,
    as one common method is to use it to create plausible variants of shapes. It is
    also used to detect facial landmarks in images in the process of **active shape
    modeling**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial modeling/risk analysis**: Dimensionality reduction provides a useful
    toolkit for the finance industry, since being able to consolidate a large number
    of individual market metrics or signals into a smaller number of components allows
    for faster, and more efficient, computations. Similarly, the components can be
    used to highlight those higher-risk products/companies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Curse of Dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can understand the benefits of using dimensionality reduction techniques,
    we must first understand why the dimensionality of feature sets needs to be reduced
    at all. The **curse of dimensionality** is a phrase commonly used to describe
    issues that arise when working with data that has a high number of dimensions
    in the feature space; for example, the number of attributes that are collected
    for each sample. Consider a dataset of point locations within a game of *Pac-Man*.
    Your character, Pac-Man, occupies a position within the virtual world defined
    by two dimensions or coordinates (*x*, *y*). Let''s say that we are creating a
    new computer enemy: an AI-driven ghost to play against, and that it requires some
    information regarding our character to make its own game logic decisions. For
    the bot to be effective, we require the player''s position (*x*, *y*) and their
    velocity in each of the directions (*vx*, *vy*) in addition to the players last
    five (*x*, *y*) positions, the number of remaining hearts, and the number of remaining
    power pellets in the maze (power pellets temporarily allow Pac-Man to eat ghosts).
    Now, for each moment in time, our bot requires 16 individual features (or dimensions)
    to make its decisions. These 16 features correspond to 5 previous positions times
    the 2 *x* and *y* coordinates + the 2 *x* and *y* coordinates of the player''s
    current position + the 2 *x* and *y* coordinates of player''s velocity + 1 feature
    for the number of hearts + 1 feature for the power pellets = 16\. This is clearly
    a lot more than just the two dimensions as provided by the position:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Dimensions in a PacMan game'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.4: Dimensions in a PacMan game'
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain the concept of dimensionality reduction, we will consider a fictional
    dataset (see *Figure 4.5*) of *x* and *y* coordinates as features, giving two
    dimensions in the feature space. It should be noted that this example is by no
    means a mathematical proof but is rather intended to provide a means of visualizing
    the effect of increased dimensionality. In this dataset, we have six individual
    samples (or points), and we can visualize the currently occupied volume within
    the feature space of approximately *(3 – 1) x (4 – 2) = 2 x 2 = 4* squared units:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: Data in a 2D feature space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.5: Data in a 2D feature space'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the dataset comprises the same number of points, but with an additional
    feature (the *z* coordinate) to each sample. The occupied data volume is now approximately
    *2 x 2 x 2 = 8* cubed units. So, we now have the same number of samples, but the
    space enclosing the dataset is now larger. As such, the data takes up less relative
    volume in the available space and is now sparser. This is the curse of dimensionality;
    as we increase the number of available features, we increase the sparsity of the
    data, and, in turn, make statistically valid correlations more difficult. Looking
    back to our example of creating a video game bot to play against a human player,
    we have 16 features that are a mix of different feature types: positions, velocity,
    power-ups, and hearts. Depending on the range of possible values for each of these
    features and the variance to the dataset provided by each feature, the data could
    be extremely sparse. Even within the constrained world of Pac-Man, the potential
    variance of each of the features could be quite large, some much larger than others.'
  prefs: []
  type: TYPE_NORMAL
- en: So, without dealing with the sparsity of the dataset, we have more information
    with the additional feature(s), but may not be able to improve the performance
    of our machine learning model, as the statistical correlations are more difficult.
    What we would like to do is to keep the useful information provided by the extra
    features but minimize the negative effect of sparsity. This is exactly what dimensionality
    reduction techniques are designed to do and these can be extremely powerful in
    increasing the performance of your machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will discuss a number of different dimensionality
    reduction techniques and will cover one of the most important and useful methods,
    PCA, in greater detail with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Dimensionality Reduction Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of any dimensionality reduction technique is to manage the sparsity
    of the dataset while keeping any useful information that is provided. In our case
    of classification, dimensionality reduction is typically used as an important
    preprocessing step used before the actual classification. Most dimensionality
    reduction techniques aim to complete this task using a process of **feature projection**,
    which adjusts the data from the higher-dimensional space into a space with fewer
    dimensions to remove the sparsity from the data. Again, as a means of visualizing
    the projection process, consider a sphere in a 3D space. We can project the sphere
    into a lower 2D space into a circle with some information loss (the value for
    the *z* coordinate), but retaining much of the information that describes its
    original shape. We still know the origin, radius, and manifold (outline) of the
    shape, and it is still very clear that it is a circle. So, depending on the problem
    that we are trying to solve, we may have reduced the dimensionality while retaining
    the important information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: A projection of a 3D sphere into a 2D space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.6: A projection of a 3D sphere into a 2D space'
  prefs: []
  type: TYPE_NORMAL
- en: The secondary benefit that can be obtained by preprocessing the dataset with
    a dimensionality reduction stage is the improved computational performance that
    can be achieved. As the data has been projected into a lower-dimensional space,
    it will contain fewer, but potentially more powerful, features. The fact that
    there are fewer features means that, during later classification or regression
    stages, the size of the dataset being processed is significantly smaller. This
    will potentially reduce the required system resources and processing time for
    classification/regression, and, in some cases, the dimensionality reduction technique
    can also be used directly to complete the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: This analogy also introduces one of the important considerations of dimensionality
    reduction. We are always trying to balance the information loss resulting from
    the projection into lower dimensional space by reducing the sparsity of the data.
    Depending on the nature of the problem and the dataset being used, the correct
    balance could present itself and be relatively straightforward. In some applications,
    this decision may rely on the outcome of additional validation methods, such as
    cross-validation (particularly in supervised learning problems) or the assessment
    of experts in your problem domain. In this scenario, cross-validation refers to
    the practice of partitioning a rolling section of the data to test on, with the
    inverse serving as the train set until all parts of the dataset are used. This
    approach allows for the reduction of bias within a machine learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: One way we like to think about this trade-off in dimensionality reduction is
    to consider compressing a file or image on a computer for transfer. Dimensionality
    reduction techniques, such as PCA, are essentially methods of compressing information
    into a smaller size for transfer, and, in many compression methods, some losses
    occur as a result of the compression process. Sometimes, these losses are acceptable;
    if we are transferring a 50 MB image and need to shrink it to 5 MB for transfer,
    we can expect to still be able to see the main subject of the image, but perhaps
    some smaller background features will become too blurry to see. We would also
    not expect to be able to restore the original image to a pixel-perfect representation
    from the compressed version, but we could expect to restore it with some additional
    artifacts, such as blurring.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dimensionality reduction techniques have many uses in machine learning, as
    the ability to extract the useful information of a dataset can provide performance
    boosts in many machine learning problems. They can be particularly useful in unsupervised
    as opposed to supervised learning methods because the dataset does not contain
    any ground truth labels or targets to achieve. In unsupervised learning, the training
    environment is being used to organize the data in a way that is appropriate for
    the problem being solved (for example, classification via clustering), which is
    typically based on the most important information in the dataset. Dimensionality
    reduction provides an effective means of extracting important information, and,
    as there are a number of different methods that we could use, it is beneficial
    to review some of the available options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Discriminant Analysis** (**LDA**): This is a particularly handy technique
    that can be used for both classification as well as dimensionality reduction.
    LDA will be covered in more detail in *Chapter 7*, *Topic Modeling*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-negative matrix factorization** (**NMF**): Like many of the dimensionality
    reduction techniques, this relies on the properties of linear algebra to reduce
    the number of features in the dataset. NMF will also be covered in more detail
    in *Chapter 7*, *Topic Modeling*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**): This is somewhat related to PCA
    (which is covered in more detail in this chapter) and is also a matrix decomposition
    process not too dissimilar to NMF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independent Component Analysis** (**ICA**): This also shares some similarities
    to SVD and PCA, but relaxing the assumption of the data being a Gaussian distribution
    allows for non-Gaussian data to be separated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the methods described so far all use linear transformation to reduce
    the sparsity of the data in their original implementation. Some of these methods
    also have variants that use non-linear kernel functions in the separation process,
    providing the ability to reduce the sparsity in a non-linear fashion. Depending
    on the dataset being used, a non-linear kernel may be more effective at extracting
    the most useful information from the signal.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described previously, PCA is a commonly used and very effective dimensionality
    reduction technique, which often forms a preprocessing stage for a number of machine
    learning models and techniques. For this reason, we will dedicate this section
    of the book to looking at PCA in more detail than any of the other methods. PCA
    reduces the sparsity in the dataset by separating the data into a series of components
    where each component represents a source of information within the data. As its
    name suggests, the first component produced in PCA, the **principal component**,
    comprises the majority of information or variance within the data. The principal
    component can often be thought of as contributing the most amount of interesting
    information in addition to the mean. With each subsequent component, less information,
    but more subtlety, is contributed to the compressed data. If we consider all of
    these components together, there will be no benefit of using PCA, as the original
    dataset will be returned. To clarify this process and the information returned
    by PCA, we will use a worked example, completing the PCA calculations by hand.
    But first, we must review some foundational statistical concepts, which are required
    to execute the PCA calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mean, or the average value, is simply the addition of all values divided
    by the number of values in the set.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Deviation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often referred to as the spread of the data and related to the variance, the
    standard deviation is a measure of how much of the data lies within proximity
    to the mean. In a normally distributed dataset, approximately 68% of the dataset
    lies within one standard deviation of the mean, (that is, between (mean - 1*std)
    to (mean + 1*std), you can find 68% of the data if it is normally distributed.)
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between the variance and standard deviation is quite a simple
    one – the variance is the standard deviation squared.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Where standard deviation or variance is the spread of the data calculated on
    a single dimension, the covariance is the variance of one dimension (or feature)
    against another. When the covariance of a dimension is computed against itself,
    the result is the same as simply calculating the variance for the dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A covariance matrix is a matrix representation of the possible covariance values
    that can be computed for a dataset. Other than being particularly useful in data
    exploration, covariance matrices are also required to execute the PCA of a dataset.
    To determine the variance of one feature with respect to another, we simply look
    up the corresponding value in the covariance matrix. In the following diagram,
    we can see that, in column 1, row 2, the value is the variance of feature or dataset
    *Y* with respect to *X* (*cov(Y, X))*. We can also see that there is a diagonal
    column of covariance values computed against the same feature or dataset; for
    example, *cov(X, X)*. In this situation, the value is simply the variance of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: The covariance matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.7: The covariance matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the exact values of each of the covariances are not as interesting
    as looking at the magnitude and relative size of each of the covariances within
    the matrix. A large value of the covariance of one feature against another would
    suggest that one feature changes significantly with respect to the other, while
    a value close to zero would signify very little change. The other interesting
    aspect of the covariance to look for is the sign associated with the covariance;
    a positive value indicates that as one feature increases or decreases, then so
    does the other, while a negative covariance indicates that the two features diverge
    from one another, with one increasing as the other decreases or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, `numpy` and `scipy` provide functions to efficiently perform these
    calculations for you. In the next exercise, we will compute these values in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.01: Computing Mean, Standard Deviation, and Variance Using the pandas
    Library'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will briefly review how to compute some of the foundational
    statistical concepts using both the `numpy` and `pandas` Python packages. In this
    exercise, we will use a dataset of the measurements of seeds from different varieties
    of wheat, created using X-ray imaging. The dataset, which can be found in the
    accompanying source code, comprises seven individual measurements (`area A`, `perimeter
    P`, `compactness C`, `length of kernel LK`, `width of kernel WK`, `asymmetry coefficient`
    `A_Coef`, and `length of kernel groove LKG`) of three different wheat varieties:
    Kama, Rosa, and Canadian.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seed](https://archive.ics.uci.edu/ml/datasets/seed)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/2RjpDxk](https://packt.live/2RjpDxk).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `numpy`, and `matplotlib` packages for use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset and preview the first five lines of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8: The head of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.8: The head of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We only require the area, `A`, and the length of the kernel `LK` features,
    so remove the other columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9: The head after cleaning the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.9: The head after cleaning the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the dataset by plotting the `A` versus `LK` values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10: Plot of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.10: Plot of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the mean value using the `pandas` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the mean value using the `numpy` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the standard deviation value using the `pandas` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the standard deviation value using the `numpy` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the variance values using the `pandas` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the variance values using the `numpy` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the covariance matrix using the `pandas` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.11: Covariance matrix using the pandas method'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.11: Covariance matrix using the pandas method'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the covariance matrix using the `numpy` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we know how to compute the foundational statistic values, we will turn
    our attention to the remaining components of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2BHiLFz](https://packt.live/2BHiLFz).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2O80UtW](https://packt.live/2O80UtW).
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalues and Eigenvectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mathematical concept of eigenvalues and eigenvectors is a very important
    one in the fields of physics and engineering, and they also form the final steps
    in computing the principal components of a dataset. The exact mathematical definition
    of eigenvalues and eigenvectors is outside the scope of this book, as it is quite
    involved and requires a reasonable understanding of linear algebra. Any square
    matrix *A* of dimensions *n x n* has a vector, *x*, of shape *n x 1* in such a
    way that it satisfies the following relation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: Equation representing PCA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.12: Equation representing PCA'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the term ![C:\Users\user\Downloads\B15923_04_Formula_01.PNG](img/B15923_04_Formula_01.png)
    is a numerical value and denotes the eigenvalue, whereas *x* denotes the corresponding
    eigenvector. *N* denotes the order of the matrix, *A*. There will be exactly *n*
    eigenvalue and eigenvectors for matrix *A*. Without diving into the mathematical
    details of PCA, let''s take a look at another way of representing the preceding
    equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13: Alternative equation representing PCA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.13: Alternative equation representing PCA'
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting this simply into the context of PCA, we can derive the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariance Matrix** (*A*): As discussed in the preceding section, matrix
    *A* should be a square matrix before it can undergo eigenvalue decomposition.
    Since, in the case of our dataset, it has rows greater than the number of columns
    (let the shape of dataset be *m x n* where *m* is the number of rows and *n* is
    the number of columns). Therefore, we cannot perform eigenvalue decomposition
    directly. To perform eigenvalue decomposition on a rectangular matrix, it is first
    converted to a square matrix by computing its covariance matrix. A covariance
    matrix has a shape of *n x n*, that is, it is a square matrix of order ''*n*''.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eigenvectors** (*U*) are the components contributing information to the dataset
    as described in the first paragraph of this section on principal components called
    eigenvectors. Each eigenvector describes some amount of variability within the
    dataset. This variability is indicated by the corresponding eigenvalue. The larger
    the eigenvalue, the greater its contribution. An eigenvectors matrix has a shape
    of *n x n*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eigenvalues** (![C:\Users\user\Downloads\B15923_04_Formula_02.PNG](img/B15923_04_Formula_02.png))
    are the individual values that describe how much contribution each eigenvector
    provides to the dataset. As described previously, the single eigenvector that
    describes the largest contribution is referred to as the principal component,
    and, as such, will have the largest eigenvalue. Accordingly, the eigenvector with
    the smallest eigenvalue contributes the least amount of variance or information
    to the data. Eigenvalues are a diagonal matrix, which has the diagonal elements
    representing eigenvalues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that even the SVD of a covariance matrix of data produces eigenvalue
    decomposition, which we will see in *Exercise 4.04*, *scikit-learn PCA*. However,
    SVD uses a different process for the decomposition of the matrix. Remember that
    eigenvalue decomposition can be done for a square matrix only, whereas SVD can
    be done for a rectangular matrix as well.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**Square matrix**: A square matrix has the same number of rows and columns.
    The number of rows in a square matrix is called the order of the matrix. A matrix
    that has an unequal number of rows and columns is known as a rectangular matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Diagonal matrix**: A diagonal matrix has all non-diagonal elements as zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.02: Computing Eigenvalues and Eigenvectors'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed previously, deriving and computing the eigenvalues and eigenvectors
    manually is a little involved and is not within the scope of this book. Thankfully,
    `numpy` provides all the functionality for us to compute these values. Again,
    we will use the Seeds dataset for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/34gOQ0B](https://packt.live/34gOQ0B).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` and `numpy` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.14: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Again, we only require the `A` and `LK` features, so remove the other columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.15: The area and length of the kernel features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.15: The area and length of the kernel features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From the linear algebra module of `numpy`, use the `eig` function to compute
    the `eigenvalues` and `eigenvectors` characteristic vectors. Note the use of the
    covariance matrix of data here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `numpy` function, `cov`, can be used to calculate the covariance matrix
    of data. It produces a square matrix of order equal to the number of features
    of data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Look at the eigenvalues; we can see that the first value is the largest, so
    the first eigenvector contributes the most information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is handy to look at eigenvalues as a percentage of the total variance within
    the dataset. We will use a cumulative sum function to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Divide by the last or maximum value to convert eigenvalues into a percentage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see here that the first (or principal) component comprises 99% of the
    variation within the data, and, therefore, most of the information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at `eigenvectors`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Confirm that the shape of the eigenvector matrix is in (`n x n`) format; that
    is, `2` x `2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, from the eigenvalues, we saw that the principal component was the first
    eigenvector. Look at the values for the first eigenvector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have decomposed the dataset down into the principal components, and, using
    the eigenvectors, we can further reduce the dimensionality of the available data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3e5x3N3](https://packt.live/3e5x3N3).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3f5Skrk](https://packt.live/3f5Skrk).
  prefs: []
  type: TYPE_NORMAL
- en: In later examples, we will consider PCA and apply this technique to an example
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Process of PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we have all of the pieces ready to complete PCA in order to reduce the
    number of dimensions in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall algorithm for completing PCA is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required Python packages (`numpy` and `pandas`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the entire dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the available data, select the features that you wish to use in dimensionality
    reduction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If there is a significant difference in the scale between the features of the
    dataset; for example, one feature ranges in values between 0 and 1, and another
    between 100 and 1,000, you may need to normalize one of the features, as such
    differences in magnitude can eliminate the effect of the smaller features. In
    such a situation, you may need to divide the larger feature by its maximum value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As an example, take a look at this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`x1 = [0.1, 0.23, 0.54, 0.76, 0.78]`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`x2 = [121, 125, 167, 104, 192]`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`# Normalise x2 to be between 0 and 1`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`x2 = (x2-np.min(x2)) / (np.max(x2)-np.min(x2))`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the `covariance` matrix of the selected (and possibly normalized) data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the eigenvalues and eigenvectors of the `covariance` matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues (and corresponding eigenvectors) from the highest to the
    lowest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the eigenvalues as a percentage of the total variance within the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the number of eigenvalues and corresponding eigenvectors. They will be
    required to comprise a predetermined value of a minimum composition variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this stage, the sorted eigenvalues represent a percentage of the total variance
    within the dataset. As such, we can use these values to select the number of eigenvectors
    required, either for the problem being solved or to sufficiently reduce the size
    of the dataset being applied to the model. For example, say that we required at
    least 90% of the variance to be accounted for within the output of PCA. We would
    then select the number of eigenvalues (and corresponding eigenvectors) that comprise
    at least 90% of the variance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Multiply the dataset by the selected eigenvectors and you have completed a PCA,
    thereby reducing the number of features representing the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before moving on to the next exercise, note that **transpose** is a term from
    linear algebra that means to swap the rows with the columns and vice versa. Let's
    say we
  prefs: []
  type: TYPE_NORMAL
- en: have a matrix of `X=[1, 2, 3]`, then, the transpose of *X* would be ![C:\Users\user\Downloads\B15923_04_Formula_03.PNG](img/B15923_04_Formula_03.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.03: Manually Executing PCA'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this exercise, we will be completing PCA manually, again using the Seeds
    dataset. For this example, we want to sufficiently reduce the number of dimensions
    within the dataset to comprise at least 75% of the available variance:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/2Xe7cxO](https://packt.live/2Xe7cxO).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` and `numpy` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.16: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.16: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Again, we only require the `A` and `LK` features, so remove the other columns.
    In this example, we are not normalizing the selected dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.17: The area and length of the kernel features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.17: The area and length of the kernel features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the `covariance` matrix for the selected data. Note that we need to
    take the transpose of the `covariance` matrix to ensure that it is based on the
    number of features (2) and not samples (150):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the eigenvectors and eigenvalues for the covariance matrix, Again,
    use the `full_matrices` function argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Eigenvalues are returned, sorted from the highest value to the lowest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Eigenvectors are returned as a matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the eigenvalues as a percentage of the variance within the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As per the introduction to the exercise, we need to describe the data with
    at least 75% of the available variance. As per *Step 7*, the principal component
    comprises 99% of the available variance. As such, we require only the principal
    component from the dataset. What are the principal components? Let''s take a look:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we can apply the dimensionality reduction process. Execute a matrix multiplication
    of the principal component with the transpose of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The dimensionality reduction process is a matrix multiplication of the selected
    eigenvectors and the data to be transformed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Without taking the transpose of the `df.values` matrix, multiplication could
    not occur:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.18: The result of matrix multiplication'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.18: The result of matrix multiplication'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The transpose of the dataset is required to execute matrix multiplication, as
    the **inner dimensions of the matrix must be the same** for matrix multiplication
    to occur. For **A** ("A dot B") to be valid, **A** must have the shape of *m x
    n*, and **B** must have the shape of *n x p*. In this example, the inner dimensions
    of **A** and **B** are both *n*. The resulting matrix would have dimensions of
    *m x p*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following example, the output of the PCA is a single-column, 210-sample
    dataset. As such, we have just reduced the size of the initial dataset by half,
    comprising approximately 99% of the variance within the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the values of the principal component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows, and shows the new component values of the 210-sample
    dataset, as seen printed in the preceding step:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.19: The Seeds dataset transformed using a manual PCA'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.19: The Seeds dataset transformed using a manual PCA'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we simply computed the covariance matrix of the dataset without
    applying any transformations to the dataset beforehand. If the two features have
    roughly the same mean and standard deviation, this is perfectly fine. However,
    if one feature is much larger in value (and has a somewhat different mean) than
    the other, then this feature may dominate the other when decomposing into components.
    This could have the effect of removing the information provided by the smaller
    feature altogether. One simple normalization technique before computing the covariance
    matrix would be to subtract the respective means from the features, thus centering
    the dataset around zero. We will demonstrate this in *Exercise 4.05*, *Visualizing
    Variance Reduction with Manual PCA*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fa8X57](https://packt.live/3fa8X57).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3iOvg2P](https://packt.live/3iOvg2P).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.04: scikit-learn PCA'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, we will not complete PCA manually, especially when scikit-learn
    provides an optimized API with convenient methods that allow us to easily transform
    the data to and from the reduced-dimensional space. In this exercise, we will
    look at using a scikit-learn PCA on the Seeds dataset in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    also can be downloaded from [https://packt.live/2Ri6VGk](https://packt.live/2Ri6VGk).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `numpy`, and `PCA` modules from the `sklearn` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.20: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Again, we only require the `A` and `LK` features, so remove the other columns.
    In this example, we are not normalizing the selected dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.21: The area and length of the kernel features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.21: The area and length of the kernel features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the data to a scikit-learn PCA model of the covariance data. Using the
    default values, as we have here, produces the maximum number of eigenvalues and
    eigenvectors possible for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `copy` indicates that the data fit within the model is copied before any
    calculations are applied. If `copy` was set to `False`, data passed to PCA is
    overwritten. `iterated_power` shows that the `A` and `LK` features are the number
    of principal components to keep. The default value is `None`, which selects the
    number of components as one less than the minimum of either the number of samples
    or the number of features. `random_state` allows the user to specify a seed for
    the random number generator used by the SVD solver. `svd_solver` specifies the
    SVD solver to be used during PCA. `tol` is the tolerance value used by the SVD
    solver. With `whiten`, the component vectors are multiplied by the square root
    of the number of samples. This will remove some information but can improve the
    performance of some downstream estimators.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The percentage of variance described by the components (eigenvalues) is contained
    within the `explained_variance_ratio_` property. Display the values for `explained_variance_ratio_`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the eigenvectors via the `components_` property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this exercise, we will again only use the primary component, so we will
    create a new `PCA` model, this time specifying the number of components (eigenvectors/eigenvalues)
    to be `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `fit` method to fit the `covariance` matrix to the `PCA` model and
    generate the corresponding eigenvalues/eigenvectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model is fitted using a number of default parameters, as listed in the preceding
    output. `copy = True` is the data provided to the `fit` method, which is copied
    before PCA is applied. `iterated_power='auto'` is used to define the number of
    iterations by the internal SVD solver. `n_components=1` specifies that the PCA
    model is to return only the principal component. `random_state=None` specifies
    the random number generator to be used by the internal SVD solver if required.
    `svd_solver='auto'` is the type of SVD solver used. `tol=0.0` is the tolerance
    value for the SVD solver. `whiten=False` specifies that the eigenvectors are not
    to be modified. If set to `True`, whitening modifies the components further by
    multiplying by the square root of the number of samples and dividing by the singular
    values. This can help to improve the performance of later algorithm steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Typically, you will not need to worry about adjusting any of these parameters,
    other than the number of components (`n_components`), which you can pass while
    declaring the PCA object as `model = PCA(n_components=1)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the eigenvectors using the `components_` property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the Seeds dataset into the lower space by using the `fit_transform`
    method of the model on the dataset. Assign the transformed values to the `data_t`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the transformed values to visualize the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.22: The seeds dataset transformed using the scikit-learn PCA'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.22: The seeds dataset transformed using the scikit-learn PCA'
  prefs: []
  type: TYPE_NORMAL
- en: You have just reduced the dimensionality of the Seeds dataset using manual PCA,
    along with the scikit-learn API. But before we celebrate too early, compare *Figure
    4.19* and *Figure 4.22*; these plots should be identical, shouldn't they? We used
    two separate methods to complete a PCA on the same dataset and selected the principal
    component for both. In the next activity, we will investigate why there are differences
    between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZQV85c](https://packt.live/2ZQV85c).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2VSG99R](https://packt.live/2VSG99R).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.01: Manual PCA versus scikit-learn'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose that you have been asked to port some legacy code from an older application
    executing PCA manually to a newer application that uses scikit-learn. During the
    porting process, you observe some differences between the output of the manual
    PCA and that of your port. Why is there a difference between the output of our
    manual PCA and scikit-learn? Compare the results of the two approaches on the
    Seeds dataset. What are the differences between them?
  prefs: []
  type: TYPE_NORMAL
- en: 'The aim of this activity is to truly dive into understanding how PCA works
    by doing it from scratch, and then comparing your implementation against the one
    included in scikit-learn to see whether there are any major differences:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/2JIH1qT](https://packt.live/2JIH1qT).'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `pandas`, `numpy`, and `matplotlib` plotting libraries and the scikit-learn
    `PCA` model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the dataset and select only the kernel features as per the previous exercises.
    Display the first five rows of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the `covariance` matrix for the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the data using the scikit-learn API and only the first principal component.
    Store the transformed data in the `sklearn_pca` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the data using the manual PCA and only the first principal component.
    Store the transformed data in the `manual_pca` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the `sklearn_pca` and `manual_pca` values on the same plot to visualize
    the difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice that the two plots look almost identical, but with some key differences.
    What are these differences?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See whether you can modify the output of the manual PCA process to bring it
    in line with the scikit-learn version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Hint: The scikit-learn API subtracts the mean of the data prior to the transform.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Expected output**: By the end of this activity, you will have transformed
    the dataset using both the manual and scikit-learn PCA methods. You will have
    produced a plot demonstrating that the two reduced datasets are, in fact, identical,
    and you should have an understanding of why they initially looked quite different.
    The final plot should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23: The expected final plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.23: The expected final plot'
  prefs: []
  type: TYPE_NORMAL
- en: This plot will demonstrate that the dimensionality reduction completed by the
    two methods are, in fact, the same.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 437.
  prefs: []
  type: TYPE_NORMAL
- en: Restoring the Compressed Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have covered a few different examples of transforming a dataset
    into a lower-dimensional space, we should consider what practical effect this
    transformation has had on the data. Using PCA as a preprocessing step to condense
    the number of features in the data will result in some of the variance being discarded.
    The following exercise will walk us through this process so that we can see how
    much information has been discarded by the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.05: Visualizing Variance Reduction with Manual PCA'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most important aspects of dimensionality reduction is understanding
    how much information has been removed from the dataset as a result of the dimensionality
    reduction process. Removing too much information will add additional challenges
    to later processing, while not removing enough defeats the purpose of PCA or other
    techniques. In this exercise, we will visualize the amount of information that
    has been removed from the Seeds dataset as a result of PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    also can be downloaded from [https://packt.live/2RhnDFS](https://packt.live/2RhnDFS).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `numpy`, and `matplotlib` plotting libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the `wheat kernel` features from the Seeds dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.24: Kernel features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.24: Kernel features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Center the dataset around zero by subtracting the respective means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To calculate the data and print the results, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.25: Section of the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.25: Section of the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use manual PCA to transform the data on the basis of the first principal component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the data into the lower-dimensional space by doing a dot product
    of the preceding `P` with a transposed version of the data matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the principal components for later use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To compute the inverse transform of the reduced dataset, we need to restore
    the selected eigenvectors to the higher-dimensional space. To do this, we will
    invert the matrix. Matrix inversion is another linear algebra technique that we
    will only cover very briefly. A square matrix, *A*, is said to be invertible if
    there is another square matrix, *B*, and if *AB=BA=I*, where *I* is a special
    matrix known as an identity matrix, consisting of values of `1` only through the
    center diagonal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the transformed data for use in the matrix multiplication:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the inverse transform of the reduced data and plot the result to visualize
    the effect of removing the variance from the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.26: The inverse transform of the reduced data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.26: The inverse transform of the reduced data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the `means` array back to the transformed data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the result by plotting the original and the transformed datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.27: The inverse transform after removing variance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.27: The inverse transform after removing variance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are only two components of variation in this dataset. If we do not remove
    any of the components, what will be the result of the inverse transform? Again,
    transform the data into the lower-dimensional space, but this time, use all of
    the eigenvectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transpose `data_transformed` to put it in the correct shape for matrix multiplication:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, restore the data back to the higher-dimensional space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.28: The restored data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.28: The restored data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the means back to the restored data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the restored data in the context of the original dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.29: The inverse transform after removing the variance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.29: The inverse transform after removing the variance'
  prefs: []
  type: TYPE_NORMAL
- en: If we compare the two plots produced in this exercise, we can see that the PCA
    went down, and the restored dataset is essentially a negative linear trend line
    between the two feature sets. We can compare this to the dataset restored from
    all of the available components, where we have recreated the original dataset
    as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/38EztBu](https://packt.live/38EztBu).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3f8LDVC](https://packt.live/3f8LDVC).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.06: Visualizing Variance Reduction with scikit-learn'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will again visualize the effect of reducing the dimensionality
    of the dataset; however, this time, we will be using the scikit-learn API. This
    is this method that you will commonly use in practical applications due to the
    power and simplicity of the scikit-learn model:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/3bVlJm4](https://packt.live/3bVlJm4).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `numpy`, and `matplotlib` plotting libraries and the `PCA`
    model from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the `Wheat Kernel` features from the Seeds dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.30: The Wheat Kernel features from the Seeds dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.30: The Wheat Kernel features from the Seeds dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the scikit-learn API to transform the data on the basis of the first principal
    component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the inverse transform of the reduced data and plot the result to visualize
    the effect of removing the variance from the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.31: The inverse transform after removing the variance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.31: The inverse transform after removing the variance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are only two components of variation in this dataset. If we do not remove
    any of the components, what will the result of the inverse transform be? Let''s
    find out by computing the inverse transform and seeing how the results change
    without removing any components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.32: The inverse transform after removing the variance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.32: The inverse transform after removing the variance'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see here, if we don't remove any of the components in the PCA, it
    will recreate the original data when performing inverse transform. We have demonstrated
    the effect of removing information from the dataset and the ability to recreate
    the original data using all of the available eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2O362zv](https://packt.live/2O362zv).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3fdWYDU](https://packt.live/3fdWYDU).
  prefs: []
  type: TYPE_NORMAL
- en: The previous exercises specified the reduction in dimensionality using PCA to
    two dimensions, partly to allow the results to be visualized easily. We can, however,
    use PCA to reduce the dimensions to any value less than that of the original set.
    The following example demonstrates how PCA can be used to reduce a dataset to
    three dimensions, thereby allowing visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.07: Plotting 3D Plots in Matplotlib'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating 3D scatter plots in `matplotlib` is unfortunately not as simple as
    providing a series of (*x*, *y*, *z*) coordinates to a scatter plot. In this exercise,
    we will work through a simple 3D plotting example using the Seeds dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Citation: Contributors gratefully acknowledge the support of their work by
    the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The
    dataset can also be downloaded from [https://packt.live/3c2tAhT](https://packt.live/3c2tAhT).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and `matplotlib`. To enable 3D plotting, you will also need
    to import `Axes3D`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the dataset and select the `A`, `LK`, and `C` columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.33: The first five rows of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.33: The first five rows of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the data in three dimensions and use the `projection=''3d''` argument
    with the `add_subplot` method to create the 3D plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot will appear as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.34: The expanded Seeds dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_04_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.34: The expanded Seeds dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While the `Axes3D` library was imported but not directly used, it is required
    for configuring the plot window in three dimensions. If the import of `Axes3D`
    was omitted, the `projection='3d'` argument would return an `AttributeError` exception.
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3gPM1J9](https://packt.live/3gPM1J9).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2AFVXFr](https://packt.live/2AFVXFr).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.02: PCA Using the Expanded Seeds Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we are going to use the complete Seeds dataset to look at
    the effect of selecting a differing number of components in the PCA decomposition.
    This activity aims to simulate the process that is typically completed in a real-world
    problem as we try to determine the optimum number of components to select, attempting
    to balance the extent of dimensionality reduction and information loss. Therefore,
    we will be using the scikit-learn PCA model:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    also can be downloaded from [https://packt.live/3aPY0nj](https://packt.live/3aPY0nj).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pandas` and `matplotlib`. To enable 3D plotting, you will also need
    to import `Axes3D`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read in the dataset and select the `Area of Kernel`, `Length of Kernel`, and
    `Compactness of Kernel` columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the data in three dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `PCA` model without specifying the number of components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the eigenvalues or `explained_variance_ratio_`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We want to reduce the dimensionality of the dataset but still keep at least
    90% of the variance. What is the minimum number of components required to keep
    90% of the variance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new `PCA` model, this time specifying the number of components required
    to keep at least 90% of the variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the data using the new model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the transformed data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore the transformed data to the original dataspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Plot the restored data in three dimensions in one subplot and the original
    data in a second subplot to visualize the effect of removing some of the variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output**: The final plot will appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.35: Expected plots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15923_04_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.35: Expected plots'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 443.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the process of dimensionality reduction and PCA.
    We completed a number of exercises and developed the skills to reduce the size
    of a dataset by extracting only the most important components of variance within
    the data, using both a manual PCA process and the model provided by scikit-learn.
    During this chapter, we also returned the reduced datasets back to the original
    dataspace and observed the effect of removing the variance on the original data.
    Finally, we discussed a number of potential applications for PCA and other dimensionality
    reduction processes. In our next chapter, we will introduce neural network-based
    autoencoders and use the Keras package to implement them.
  prefs: []
  type: TYPE_NORMAL

<html><head></head><body>
		<div class="Content" id="_idContainer397">
			<h1 id="_idParaDest-215"><em class="italics"><a id="_idTextAnchor239"/>Appendix</em></h1>
		</div>
		<div>
			<div class="Content" id="_idContainer398">
			</div>
		</div>
		<div class="Content" id="_idContainer399">
			<h2>About</h2>
			<p>This section is included to assist the students to perform the activities present in the book. It includes detailed steps that are to be performed by the students to complete and achieve the objectives of the book.</p>
		</div>
		<div class="Content" id="_idContainer490">
			<h2 id="_idParaDest-216"><a id="_idTextAnchor240"/>Chapter 1: Introduction to Clustering</h2>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor241"/>Activity 1: Implementing k-means Clustering</h3>
			<p>Solution:</p>
			<ol>
				<li>Load the Iris data file using pandas, a package that makes data wrangling much easier through the use of DataFrames:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.metrics import silhouette_score</p><p class="snippet">from scipy.spatial.distance import cdist</p><p class="snippet">iris = pd.read_csv('iris_data.csv', header=None)</p><p class="snippet">iris.columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'species']</p></li>
				<li>Separate out the <strong class="inline">X</strong> features and the provided <strong class="inline">y</strong> species labels, since we want to treat this as an unsupervised learning problem:<p class="snippet">X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]</p><p class="snippet">y = iris['species']</p></li>
				<li>Get an idea of what our features look like:<p class="snippet">X.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer400"><img alt="Figure 1.22: First five rows of the data&#13;&#10;" src="image/C12626_01_22.jpg"/></div><h6>Figure 1.22: First five rows of the data</h6></li>
				<li>Bring back the <strong class="inline">k_means</strong> function we made earlier for reference:<p class="snippet">def k_means(X, K):</p><p class="snippet">#Keep track of history so you can see k-means in action</p><p class="snippet">    centroids_history = []</p><p class="snippet">    labels_history = []</p><p class="snippet">    rand_index = np.random.choice(X.shape[0], K)  </p><p class="snippet">    centroids = X[rand_index]</p><p class="snippet">    centroids_history.append(centroids)</p><p class="snippet">    while True:</p><p class="snippet"># Euclidean distances are calculated for each point relative to centroids, #and then np.argmin returns</p><p class="snippet"># the index location of the minimal distance - which cluster a point    is #assigned to</p><p class="snippet">        labels = np.argmin(cdist(X, centroids), axis=1)</p><p class="snippet">        labels_history.append(labels)</p><p class="snippet">#Take mean of points within clusters to find new centroids:</p><p class="snippet">        new_centroids = np.array([X[labels == i].mean(axis=0)</p><p class="snippet">                                for i in range(K)])</p><p class="snippet">        centroids_history.append(new_centroids)</p><p class="snippet">        </p><p class="snippet">        # If old centroids and new centroids no longer change, k-means is complete and end. Otherwise continue</p><p class="snippet">        if np.all(centroids == new_centroids):</p><p class="snippet">            break</p><p class="snippet">        centroids = new_centroids</p><p class="snippet">    </p><p class="snippet">    return centroids, labels, centroids_history, labels_history</p></li>
				<li>Convert our Iris <strong class="inline">X</strong> feature DataFrame to a <strong class="inline">NumPy</strong> matrix:<p class="snippet">X_mat = X.values</p></li>
				<li>Run our <strong class="inline">k_means</strong> function on the Iris matrix:<p class="snippet">centroids, labels, centroids_history, labels_history = k_means(X_mat, 3)</p></li>
				<li>See what labels we get by looking at just the list of predicted species per sample:<p class="snippet">print(labels)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer401"><img alt="Figure 1.23: List of predicted species&#13;&#10;" src="image/C12626_01_23.jpg"/></div><h6>Figure 1.23: List of predicted species</h6></li>
				<li>Visualize how our k-means implementation performed on the dataset:<p class="snippet">plt.scatter(X['SepalLengthCm'], X['SepalWidthCm'])</p><p class="snippet">plt.title('Iris - Sepal Length vs Width')</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer402"><img alt="Figure 1.24: Plot of performed k-means implementation&#13;&#10;" src="image/C12626_01_24.jpg"/></div><h6>Figure 1.24: Plot of performed k-means implementation</h6><p>Visualize the clusters of Iris species as follows:</p><p class="snippet">plt.scatter(X['SepalLengthCm'], X['SepalWidthCm'], c=labels, cmap='tab20b')</p><p class="snippet">plt.title('Iris - Sepal Length vs Width - Clustered')</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer403"><img alt="Figure 1.25: Clusters of Iris species&#13;&#10;" src="image/C12626_01_25.jpg"/></div><h6>Figure 1.25: Clusters of Iris species</h6></li>
				<li>Calculate the Silhouette Score using scikit-learn implementation:<p class="snippet"># Calculate Silhouette Score</p><p class="snippet">silhouette_score(X[['SepalLengthCm','SepalWidthCm']], labels)</p><p>You will get an SSI roughly equal to 0.369. Since we are only using two features, this is acceptable, combined with the visualization of cluster memberships seen in the final plot.</p></li>
			</ol>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor242"/>Chapter 2: Hierarchical Clustering</h2>
			<p>Activity 2: Applying Linkage Criteria</p>
			<p>Solution: </p>
			<ol>
				<li value="1">Visualize the <strong class="inline">x</strong> dataset that we created in <em class="italics">Exercise 7</em>, <em class="italics">Building a Hierarchy</em>:<p class="snippet">from scipy.cluster.hierarchy import linkage, dendrogram, fcluster</p><p class="snippet">from sklearn.datasets import make_blobs</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">%matplotlib inline</p><p class="snippet"># Generate a random cluster dataset to experiment on. X = coordinate points, y = cluster labels (not needed)</p><p class="snippet">X, y = make_blobs(n_samples=1000, centers=8, n_features=2, random_state=800)</p><p class="snippet"># Visualize the data</p><p class="snippet">plt.scatter(X[:,0], X[:,1])</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer404"><img alt="Figure 2.20: A scatter plot of the generated cluster dataset&#13;&#10;" src="image/C12626_02_20.jpg"/></div><h6>Figure 2.20: A scatter plot of the generated cluster dataset</h6></li>
				<li>Create a list with all the possible linkage method hyperparameters:<p class="snippet">methods = ['centroid', 'single', 'complete', 'average', 'weighted']</p></li>
				<li>Loop through each of the methods in the list that you just created and display the effect that they have on the same dataset:<p class="snippet">for method in methods:</p><p class="snippet">    distances = linkage(X, method=method, metric="euclidean")</p><p class="snippet">    clusters = fcluster(distances, 3, criterion="distance") </p><p class="snippet">    plt.title('linkage: ' + method)</p><p class="snippet">    plt.scatter(X[:,0], X[:,1], c=clusters, cmap='tab20b')</p><p class="snippet">    plt.show()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer405">
					<img alt="Figure 2.21: A scatter plot for all the methods&#13;&#10;" src="image/C12626_02_21.jpg"/>
				</div>
			</div>
			<h6>Figure 2.21: A scatter plot for all the methods</h6>
			<p>Analysis: </p>
			<p>As you can see from the preceding plots, by simply changing the linkage criteria, you can dramatically change the efficacy of your clustering. In this dataset, centroid and average linkage work best at finding discrete clusters that make sense. This is clear from the fact that we generated a dataset of eight clusters, and centroid and average linkage are the only ones that show the clusters that are represented using eight different colors. The other linkage types fall short – most noticeably, single linkage.</p>
			<h3 id="_idParaDest-219"><a id="_idTextAnchor243"/>Activity 3: Comparing k-means with Hierarchical Clustering</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the necessary packages from scikit-learn (<strong class="inline">KMeans</strong>, <strong class="inline">AgglomerativeClustering</strong>, and <strong class="inline">silhouette_score</strong>), as follows:<p class="snippet">from sklearn.cluster import KMeans</p><p class="snippet">from sklearn.cluster import AgglomerativeClustering</p><p class="snippet">from sklearn.metrics import silhouette_score</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Read the wine dataset into the pandas DataFrame and print a small sample:<p class="snippet">wine_df = pd.read_csv("wine_data.csv")</p><p class="snippet">print(wine_df.head)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer406"><img alt="Figure 2.22: The output of the wine dataset&#13;&#10;" src="image/C12626_02_22.jpg"/></div><h6>Figure 2.22: The output of the wine dataset</h6></li>
				<li>Visualize the wine dataset to understand the data structure:<p class="snippet">plt.scatter(wine_df.values[:,0], wine_df.values[:,1])</p><p class="snippet">plt.title("Wine Dataset")</p><p class="snippet">plt.xlabel("OD Reading")</p><p class="snippet">plt.ylabel("Proline")</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><h6> </h6><div class="IMG---Figure" id="_idContainer407"><img alt="Figure 2.23: A plot of raw wine data&#13;&#10;" src="image/C12626_02_23.jpg"/></div><h6>Figure 2.23: A plot of raw wine data</h6></li>
				<li>Use the sklearn implementation of k-means on the wine dataset, knowing that there are three wine types:<p class="snippet">km = KMeans(3)</p><p class="snippet">km_clusters = km.fit_predict(wine_df)</p></li>
				<li>Use the sklearn implementation of hierarchical clustering on the wine dataset:<p class="snippet">ac = AgglomerativeClustering(3, linkage='average')</p><p class="snippet">ac_clusters = ac.fit_predict(wine_df)</p></li>
				<li>Plot the predicted clusters from k-means, as follows:<p class="snippet">plt.scatter(wine_df.values[:,0], wine_df.values[:,1], c=km_clusters)</p><p class="snippet">plt.title("Wine Clusters from Agglomerative Clustering")</p><p class="snippet">plt.xlabel("OD Reading")</p><p class="snippet">plt.ylabel("Proline")</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer408"><img alt="Figure 2.24: A plot of clusters from k-means clustering&#13;&#10;" src="image/C12626_02_24.jpg"/></div><h6>Figure 2.24: A plot of clusters from k-means clustering</h6></li>
				<li>Plot the predicted clusters from hierarchical clustering, as follows:<p class="snippet">plt.scatter(wine_df.values[:,0], wine_df.values[:,1], c=ac_clusters)</p><p class="snippet">plt.title("Wine Clusters from Agglomerative Clustering")</p><p class="snippet">plt.xlabel("OD Reading")</p><p class="snippet">plt.ylabel("Proline")</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer409"><img alt="Figure 2.25: A plot of clusters from agglomerative clustering&#13;&#10;" src="image/C12626_02_25.jpg"/></div><h6>Figure 2.25: A plot of clusters from agglomerative clustering</h6></li>
				<li>Compare the silhouette score of each clustering method:<p class="snippet">print("Silhouette Scores for Wine Dataset:\n")</p><p class="snippet">print("k-means Clustering: ", silhouette_score(X[:,11:13], km_clusters))</p><p class="snippet">print("Agg Clustering: ", silhouette_score(X[:,11:13], ac_clusters))</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer410">
					<img alt="Figure 2.26: Silhouette scores for the wine dataset&#13;&#10;" src="image/C12626_02_26.jpg"/>
				</div>
			</div>
			<h6>Figure 2.26: Silhouette scores for the wine dataset</h6>
			<p>As you can see from the preceding silhouette metric, agglomerative clustering narrowly beats k-means clustering when it comes to separating the clusters by mean intra-cluster distance. This is not the case for every version of agglomerative clustering, however. Instead, try different linkage types and examine how the silhouette score and clustering changes between each!</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor244"/>Chapter 3: Neighborhood Approaches and DBSCAN</h2>
			<h3 id="_idParaDest-221"><a id="_idTextAnchor245"/>Activity 4: Implement DBSCAN from Scratch</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Generate a random cluster dataset as follows:<p class="snippet">from sklearn.cluster import DBSCAN</p><p class="snippet">from sklearn.datasets import make_blobs</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import numpy as np</p><p class="snippet">%matplotlib inline</p><p class="snippet">X_blob, y_blob = make_blobs(n_samples=500, centers=4, n_features=2, random_state=800)</p></li>
				<li>Visualize the generated data:<p class="snippet">plt.scatter(X_blob[:,0], X_blob[:,1])</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer411"><img alt="Figure 3.14: Plot of generated data&#13;&#10;" src="image/C12626_03_14.jpg"/></div><h6>Figure 3.14: Plot of generated data</h6></li>
				<li>Create functions from scratch that allow you to call DBSCAN on a dataset:<p class="snippet">def scratch_DBSCAN(x, eps, min_pts):</p><p class="snippet">    """</p><p class="snippet">    param x (list of vectors): your dataset to be clustered</p><p class="snippet">    param eps (float): neigborhood radius threshold</p><p class="snippet">    param min_pts (int): minimum number of points threshold for a nieghborhood to be a cluster</p><p class="snippet">    """</p><p class="snippet">     # Build a label holder that is comprised of all 0s</p><p class="snippet">    labels = [0]* x.shape[0]</p><p class="snippet">    # Arbitrary starting "current cluster" ID    </p><p class="snippet">    C = 0</p><p class="snippet">    </p><p class="snippet">    # For each point p in x...</p><p class="snippet">    # ('p' is the index of the datapoint, rather than the datapoint itself.)</p><p class="snippet">    for p in range(0, x.shape[0]):</p><p class="snippet">    </p><p class="snippet">        # Only unvisited points can be evaluated as neighborhood centers</p><p class="snippet">        if not (labels[p] == 0):</p><p class="snippet">            continue</p><p class="snippet">        </p><p class="snippet">        # Find all of p's neighbors.</p><p class="snippet">        neighbors = neighborhood_search(x, p, eps)</p><p class="snippet">        </p><p class="snippet">        # If there are not enough neighbor points, then it is classified as noise (-1).</p><p class="snippet">        # Otherwise we can use this point as a neighborhood cluster</p><p class="snippet">        if len(neighbors) &lt; min_pts:</p><p class="snippet">            labels[p] = -1    </p><p class="snippet">        else: </p><p class="snippet">            C += 1</p><p class="snippet">            neighbor_cluster(x, labels, p, neighbors, C, eps, min_pts)</p><p class="snippet">    </p><p class="snippet">    return labels</p><p class="snippet">def neighbor_cluster(x, labels, p, neighbors, C, eps, min_pts):</p><p class="snippet">    # Assign the cluster label to original point</p><p class="snippet">    labels[p] = C</p><p class="snippet">    </p><p class="snippet">    # Look at each neighbor of p (by index, not the points themselves) and evaluate</p><p class="snippet">    i = 0</p><p class="snippet">    while i &lt; len(neighbors):    </p><p class="snippet">        </p><p class="snippet">        # Get the next point from the queue.        </p><p class="snippet">        potential_neighbor_ix = neighbors[i]</p><p class="snippet">       </p><p class="snippet">        # If potential_neighbor_ix is noise from previous runs, we can assign it to current cluster</p><p class="snippet">        if labels[potential_neighbor_ix] == -1:</p><p class="snippet">            labels[potential_neighbor_ix] = C</p><p class="snippet">        </p><p class="snippet">        # Otherwise, if potential_neighbor_ix is unvisited, we can add it to current cluster</p><p class="snippet">        elif labels[potential_neighbor_ix] == 0:</p><p class="snippet">            labels[potential_neighbor_ix] = C</p><p class="snippet">            </p><p class="snippet">            # Further find neighbors of potential neighbor</p><p class="snippet">            potential_neighbors_cluster = neighborhood_search(x, potential_neighbor_ix, eps)</p><p class="snippet">            </p><p class="snippet">            if len(potential_neighbors_cluster) &gt;= min_pts:</p><p class="snippet">                neighbors = neighbors + potential_neighbors_cluster      </p><p class="snippet">        </p><p class="snippet">        # Evaluate next neighbor</p><p class="snippet">        i += 1        </p><p class="snippet">def neighborhood_search(x, p, eps):</p><p class="snippet">    neighbors = []</p><p class="snippet">    </p><p class="snippet">    # For each point in the dataset...</p><p class="snippet">    for potential_neighbor in range(0, x.shape[0]):</p><p class="snippet">        </p><p class="snippet">        # If a nearby point falls below the neighborhood radius threshold, add to neighbors list</p><p class="snippet">        if np.linalg.norm(x[p] - x[potential_neighbor]) &lt; eps:</p><p class="snippet">            neighbors.append(potential_neighbor)</p><p class="snippet">            </p><p class="snippet">    return neighbors</p></li>
				<li>Use your created DBSCAN implementation to find clusters in the generated dataset. Feel free to use hyperparameters as you see fit, tuning them based on their performance in step five:<p class="snippet">labels = scratch_DBSCAN(X_blob, 0.6, 5)</p></li>
				<li>Visualize the clustering performance of your DBSCAN implementation from scratch:<p class="snippet">plt.scatter(X_blob[:,0], X_blob[:,1], c=labels)</p><p class="snippet">plt.title("DBSCAN from Scratch Performance")</p><p class="snippet">plt.show()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer412">
					<img alt="Figure 3.15: Plot of DBSCAN implementation&#13;&#10;" src="image/C12626_03_15.jpg"/>
				</div>
			</div>
			<h6>Figure 3.15: Plot of DBSCAN implementation</h6>
			<p>As you may have noticed, it takes quite some time for a custom implementation to run. This is because we explored the non-vectorized version of this algorithm for the sake of clarity. Moving forward, you should aim to use the DBSCAN implementation provided by scikit-learn, as it is highly optimized.</p>
			<h3 id="_idParaDest-222"><a id="_idTextAnchor246"/>Activity 5: Comparing DBSCAN with k-means and Hierarchical Clustering</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the necessary packages:<p class="snippet">from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN</p><p class="snippet">from sklearn.metrics import silhouette_score</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">%matplotlib inline</p></li>
				<li>Load the wine dataset from <em class="italics">Chapter 2</em>, <em class="italics">Hierarchical Clustering</em> and familiarize yourself again with what the data looks like:<p class="snippet"># Load Wine data set</p><p class="snippet">wine_df = pd.read_csv("../CH2/wine_data.csv")</p><p class="snippet"># Show sample of data set</p><p class="snippet">print(wine_df.head())</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer413"><img alt="Figure 3.16: First five rows of wine dataset&#13;&#10;" src="image/C12626_03_16.jpg"/></div><h6>Figure 3.16: First five rows of wine dataset</h6></li>
				<li>Visualize the data:<p class="snippet">plt.scatter(wine_df.values[:,0], wine_df.values[:,1])</p><p class="snippet">plt.title("Wine Dataset")</p><p class="snippet">plt.xlabel("OD Reading")</p><p class="snippet">plt.ylabel("Proline")</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer414"><img alt="Figure 3.17: Plot of the data&#13;&#10;" src="image/C12626_03_17.jpg"/></div><h6>Figure 3.17: Plot of the data</h6></li>
				<li>Generate clusters using k-means, agglomerative clustering, and DBSCAN:<p class="snippet"># Generate clusters from K-Means</p><p class="snippet">km = KMeans(3)</p><p class="snippet">km_clusters = km.fit_predict(wine_df)</p><p class="snippet"># Generate clusters using Agglomerative Hierarchical Clustering</p><p class="snippet">ac = AgglomerativeClustering(3, linkage='average')</p><p class="snippet">ac_clusters = ac.fit_predict(wine_df)</p></li>
				<li>Evaluate a few different options for DSBSCAN hyperparameters and their effect on the silhouette score:<p class="snippet">db_param_options = [[20,5],[25,5],[30,5],[25,7],[35,7],[35,3]]</p><p class="snippet">for ep,min_sample in db_param_options:</p><p class="snippet">    # Generate clusters using DBSCAN</p><p class="snippet">    db = DBSCAN(eps=ep, min_samples = min_sample)</p><p class="snippet">    db_clusters = db.fit_predict(wine_df)</p><p class="snippet">    print("Eps: ", ep, "Min Samples: ", min_sample)</p><p class="snippet">    print("DBSCAN Clustering: ", silhouette_score(wine_df, db_clusters))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer415"><img alt="Figure 3.18: Printing the silhouette score for clusters&#13;&#10;" src="image/C12626_03_18.jpg"/></div><h6>Figure 3.18: Printing the silhouette score for clusters</h6></li>
				<li>Generate the final clusters based on the highest silhouette score (<strong class="inline">eps</strong>: 35, <strong class="inline">min_samples</strong>: 3):<p class="snippet"># Generate clusters using DBSCAN</p><p class="snippet">db = DBSCAN(eps=35, min_samples = 3)</p><p class="snippet">db_clusters = db.fit_predict(wine_df)</p></li>
				<li>Visualize clusters generated using each of the three methods:<p class="snippet">plt.title("Wine Clusters from K-Means")</p><p class="snippet">plt.scatter(wine_df['OD_read'], wine_df['Proline'], c=km_clusters,s=50, cmap='tab20b')</p><p class="snippet">plt.show()</p><p class="snippet">plt.title("Wine Clusters from Agglomerative Clustering")</p><p class="snippet">plt.scatter(wine_df['OD_read'], wine_df['Proline'], c=ac_clusters,s=50, cmap='tab20b')</p><p class="snippet">plt.show()</p><p class="snippet">plt.title("Wine Clusters from DBSCAN")</p><p class="snippet">plt.scatter(wine_df['OD_read'], wine_df['Proline'], c=db_clusters,s=50, cmap='tab20b')</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer416"><img alt="Figure 3.19: Plot of clusters using different algorithms&#13;&#10;" src="image/C12626_03_19.jpg"/></div><h6>Figure 3.19: Plot of clusters using different algorithms</h6></li>
				<li>Evaluate the silhouette score of each approach:<p class="snippet"># Calculate Silhouette Scores</p><p class="snippet">print("Silhouette Scores for Wine Dataset:\n")</p><p class="snippet">print("K-Means Clustering: ", silhouette_score(wine_df, km_clusters))</p><p class="snippet">print("Agg Clustering: ", silhouette_score(wine_df, ac_clusters))</p><p class="snippet">print("DBSCAN Clustering: ", silhouette_score(wine_df, db_clusters))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer417">
					<img alt="Figure 3.20: Silhouette score&#13;&#10;" src="image/C12626_03_20.jpg"/>
				</div>
			</div>
			<h6>Figure 3.20: Silhouette score</h6>
			<p>As you can see, DBSCAN isn't automatically the best choice for your clustering needs. One key trait that makes it different form other algorithms is the use of noise as a potential clustering. In some cases, this is great, as it removes outliers, however, there may be situations where it is not tuned well enough and classifies too many points as noise. Can you improve the silhouette score by tuning the hyperparameters?</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor247"/>Chapter 4: Dimension Reduction and PCA</h2>
			<h3 id="_idParaDest-224"><a id="_idTextAnchor248"/>Activity 6: Manual PCA versus scikit-learn</h3>
			<p>Solution</p>
			<ol>
				<li value="1">Import the <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong> plotting libraries and the scikit-learn <strong class="inline">PCA</strong> model:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.decomposition import PCA</p></li>
				<li>Load the dataset and select only the sepal features as per the previous exercises. Display the first five rows of the data:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df = df[['Sepal Length', 'Sepal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer418"><img alt="Figure 4.43: The first five rows of the data&#13;&#10;" src="image/C12626_04_43.jpg"/></div><h6>Figure 4.43: The first five rows of the data</h6></li>
				<li>Compute the <strong class="inline">covariance</strong> matrix for the data:<p class="snippet">cov = np.cov(df.values.T)</p><p class="snippet">cov</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer419"><img alt="Figure 4.44: The covariance matrix for the data&#13;&#10;" src="image/C12626_04_44.jpg"/></div><h6>Figure 4.44: The covariance matrix for the data</h6></li>
				<li>Transform the data using the scikit-learn API and only the first principal component. Store the transformed data in the <strong class="inline">sklearn_pca</strong> variable:<p class="snippet">model = PCA(n_components=1)</p><p class="snippet">sklearn_pca = model.fit_transform(df.values)</p></li>
				<li>Transform the data using the manual PCA and only the first principal component. Store the transformed data in the <strong class="inline">manual_pca</strong> variable.<p class="snippet">eigenvectors, eigenvalues, _ = np.linalg.svd(cov, full_matrices=False)</p><p class="snippet">P = eigenvectors[0]</p><p class="snippet">manual_pca = P.dot(df.values.T)</p></li>
				<li>Plot the <strong class="inline">sklearn_pca</strong> and <strong class="inline">manual_pca</strong> values on the same plot to visualize the difference:<p class="snippet">plt.figure(figsize=(10, 7));</p><p class="snippet">plt.plot(sklearn_pca, label='Scikit-learn PCA');</p><p class="snippet">plt.plot(manual_pca, label='Manual PCA', linestyle='--');</p><p class="snippet">plt.xlabel('Sample');</p><p class="snippet">plt.ylabel('Transformed Value');</p><p class="snippet">plt.legend();</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer420"><img alt="Figure 4.45: A plot of the data&#13;&#10;" src="image/C12626_04_45.jpg"/></div><h6>Figure 4.45: A plot of the data</h6></li>
				<li>Notice that the two plots look almost identical, except that one is a mirror image of another and there is an offset between the two. Display the components of the <strong class="inline">sklearn_pca</strong> and <strong class="inline">manual_pca</strong> models:<p class="snippet">model.components_</p><p>The output is as follows:</p><p class="snippet">array([[ 0.99693955, -0.07817635]])</p><p>Now print <strong class="inline">P</strong>:</p><p class="snippet">P</p><p>The output is as follows:</p><p class="snippet">array([-0.99693955,  0.07817635])</p><p>Notice the difference in the signs; the values are identical, but the signs are different, producing the mirror image result. This is just a difference in convention, nothing meaningful.</p></li>
				<li>Multiply the <strong class="inline">manual_pca</strong> models by <strong class="inline">-1</strong> and re-plot:<p class="snippet">manual_pca *= -1</p><p class="snippet">plt.figure(figsize=(10, 7));</p><p class="snippet">plt.plot(sklearn_pca, label='Scikit-learn PCA');</p><p class="snippet">plt.plot(manual_pca, label='Manual PCA', linestyle='--');</p><p class="snippet">plt.xlabel('Sample');</p><p class="snippet">plt.ylabel('Transformed Value');</p><p class="snippet">plt.legend();</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer421"><img alt="Figure 4.46: Re-plotted data&#13;&#10;" src="image/C12626_04_46.jpg"/></div><h6>Figure 4.46: Re-plotted data</h6></li>
				<li>Now, all we need to do is deal with the offset between the two. The scikit-learn API subtracts the mean of the data prior to the transform. Subtract the mean of each column from the dataset before completing the transform with manual PCA:<p class="snippet">mean_vals = np.mean(df.values, axis=0)</p><p class="snippet">offset_vals = df.values - mean_vals</p><p class="snippet">manual_pca = P.dot(offset_vals.T)</p></li>
				<li>Multiply the result by <strong class="inline">-1</strong>:<p class="snippet">manual_pca *= -1</p></li>
				<li>Re-plot the individual <strong class="inline">sklearn_pca</strong> and <strong class="inline">manual_pca</strong> values:<p class="snippet">plt.figure(figsize=(10, 7));</p><p class="snippet">plt.plot(sklearn_pca, label='Scikit-learn PCA');</p><p class="snippet">plt.plot(manual_pca, label='Manual PCA', linestyle='--');</p><p class="snippet">plt.xlabel('Sample');</p><p class="snippet">plt.ylabel('Transformed Value');</p><p class="snippet">plt.legend();</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer422">
					<img alt="Figure 4.47: Re-plotting the data&#13;&#10;" src="image/C12626_04_47.jpg"/>
				</div>
			</div>
			<h6>Figure 4.47: Re-plotting the data</h6>
			<p>The final plot will demonstrate that the dimensionality reduction completed by the two methods are, in fact, the same. The differences lie in the differences in the signs of the <strong class="inline">covariance</strong> matrices, as the two methods simply use a different feature as the baseline for comparison. Finally, there is also an offset between the two datasets, which is attributed to the mean samples being subtracted before executing the transform in the scikit-learn PCA.</p>
			<h3 id="_idParaDest-225"><a id="_idTextAnchor249"/>Activity 7: PCA Using the Expanded Iris Dataset</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="inline">pandas</strong> and <strong class="inline">matplotlib</strong>. To enable 3D plotting, you will also need to import <strong class="inline">Axes3D</strong>:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.decomposition import PCA</p><p class="snippet">from mpl_toolkits.mplot3d import Axes3D # Required for 3D plotting</p></li>
				<li>Read in the dataset and select the columns <strong class="inline">Sepal Length</strong>, <strong class="inline">Sepal Width</strong>, and <strong class="inline">Petal Width</strong>:<p class="snippet">df = pd.read_csv('iris-data.csv')[['Sepal Length', 'Sepal Width', 'Petal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer423"><img alt="Figure 4.48: Sepal Length, Sepal Width, and Petal Width&#13;&#10;" src="image/C12626_04_48.jpg"/></div><h6>Figure 4.48: Sepal Length, Sepal Width, and Petal Width</h6></li>
				<li>Plot the data in three dimensions:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_subplot(111, projection='3d')</p><p class="snippet">ax.scatter(df['Sepal Length'], df['Sepal Width'], df['Petal Width']);</p><p class="snippet">ax.set_xlabel('Sepal Length (mm)');</p><p class="snippet">ax.set_ylabel('Sepal Width (mm)');</p><p class="snippet">ax.set_zlabel('Petal Width (mm)');</p><p class="snippet">ax.set_title('Expanded Iris Dataset');</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer424"><img alt="Figure 4.49: Expanded Iris dataset plot&#13;&#10;" src="image/C12626_04_49.jpg"/></div><h6>Figure 4.49: Expanded Iris dataset plot</h6></li>
				<li>Create a <strong class="inline">PCA</strong> model without specifying the number of components:<p class="snippet">model = PCA()</p></li>
				<li>Fit the model to the dataset:<p class="snippet">model.fit(df.values)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer425"><img alt="Figure 4.50: The model fitted to the dataset&#13;&#10;" src="image/C12626_04_50.jpg"/></div><h6>Figure 4.50: The model fitted to the dataset</h6></li>
				<li>Display the eigenvalues or <strong class="inline">explained_variance_ratio_</strong>:<p class="snippet">model.explained_variance_ratio_</p><p>The output is as follows:</p><p class="snippet">array([0.8004668 , 0.14652357, 0.05300962])</p></li>
				<li>We want to reduce the dimensionality of the dataset, but still keep at least 90% of the variance. What are the minimum number of components required to keep 90% of the variance?<p>The first two components are required for at least 90% variance. The first two components provide 94.7% of the variance within the dataset.</p></li>
				<li>Create a new <strong class="inline">PCA</strong> model, this time specifying the number of components required to keep at least 90% of the variance:<p class="snippet">model = PCA(n_components=2)</p></li>
				<li>Transform the data using the new model:<p class="snippet">data_transformed = model.fit_transform(df.values)</p></li>
				<li>Plot the transformed data:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.scatter(data_transformed[:,0], data_transformed[:,1]);</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer426"><img alt="Figure 4.51: Plot of the transformed data&#13;&#10;" src="image/C12626_04_51.jpg"/></div><h6>Figure 4.51: Plot of the transformed data</h6></li>
				<li>Restore the transformed data to the original dataspace:<p class="snippet">data_restored = model.inverse_transform(data_transformed)</p></li>
				<li>Plot the restored data in three dimensions in one subplot and the original data in a second subplot to visualize the effect of removing some of the variance:<p class="snippet">fig = plt.figure(figsize=(10, 14))</p><p class="snippet"># Original Data</p><p class="snippet">ax = fig.add_subplot(211, projection='3d')</p><p class="snippet">ax.scatter(df['Sepal Length'], df['Sepal Width'], df['Petal Width'], label='Original Data');</p><p class="snippet">ax.set_xlabel('Sepal Length (mm)');</p><p class="snippet">ax.set_ylabel('Sepal Width (mm)');</p><p class="snippet">ax.set_zlabel('Petal Width (mm)');</p><p class="snippet">ax.set_title('Expanded Iris Dataset');</p><p class="snippet"># Transformed Data</p><p class="snippet">ax = fig.add_subplot(212, projection='3d')</p><p class="snippet">ax.scatter(data_restored[:,0], data_restored[:,1], data_restored[:,2], label='Restored Data');</p><p class="snippet">ax.set_xlabel('Sepal Length (mm)');</p><p class="snippet">ax.set_ylabel('Sepal Width (mm)');</p><p class="snippet">ax.set_zlabel('Petal Width (mm)');</p><p class="snippet">ax.set_title('Restored Iris Dataset');</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer427">
					<img alt="Figure 4.52: Plot of the expanded and the restored Iris datasets&#13;&#10;" src="image/C12626_04_52.jpg"/>
				</div>
			</div>
			<h6>Figure 4.52: Plot of the expanded and the restored Iris datasets</h6>
			<p>Looking at <em class="italics">Figure 4.52</em>, we can see that, as we did with the 2D plots, we have removed much of the noise within the data, but retained the most important information regarding the trends within the data. It can be seen that in general, the sepal length increases with the petal width and that there seems to be two clusters of data within the plots, one sitting above the other.</p>
			<h4>Note</h4>
			<p class="callout">When applying PCA, it is important to keep in mind the size of the data being modelled, as well as the available system memory. The singular value decomposition process involves separating the data into the eigenvalues and eigenvectors, and can be quite memory intensive. If the dataset is too large, you may either be unable to complete the process, suffer significant performance loss, or lock up your system.</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor250"/>Chapter 5: Autoencoders</h2>
			<h3 id="_idParaDest-227"><a id="_idTextAnchor251"/>Activity 8: Modeling Neurons with a ReLU Activation Function</h3>
			<p>Solution: </p>
			<ol>
				<li value="1">Import <strong class="inline">numpy</strong> and matplotlib:<p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Allow latex symbols to be used in labels:<p class="snippet">plt.rc('text', usetex=True)</p></li>
				<li>Define the ReLU activation function as a Python function:<p class="snippet">def relu(x):</p><p class="snippet">    return np.max((0, x))</p></li>
				<li>Define the inputs (<strong class="inline">x</strong>) and tunable weights (<strong class="inline">theta</strong>) for the neuron. In this example, the inputs (<strong class="inline">x</strong>) will be 100 numbers linearly spaced between -5 and 5. Set <strong class="inline">theta = 1</strong>:<p class="snippet">theta = 1</p><p class="snippet">x = np.linspace(-5, 5, 100)</p><p class="snippet">x</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer428"><img alt="Figure 5.35: Printing the inputs&#13;&#10;" src="image/C12626_05_35.jpg"/></div><h6>Figure 5.35: Printing the inputs</h6></li>
				<li>Compute the output (<strong class="inline">y</strong>):<p class="snippet">y = [relu(_x * theta) for _x in x]</p></li>
				<li>Plot the output of the neuron versus the input:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_subplot(111)</p><p class="snippet">ax.plot(x, y)</p><p class="snippet">ax.set_xlabel('$x$', fontsize=22);</p><p class="snippet">ax.set_ylabel('$h(x\Theta)$', fontsize=22);</p><p class="snippet">ax.spines['left'].set_position(('data', 0));</p><p class="snippet">ax.spines['top'].set_visible(False);</p><p class="snippet">ax.spines['right'].set_visible(False);</p><p class="snippet">ax.tick_params(axis='both', which='major', labelsize=22)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer429"><img alt="Figure 5.36: Plot of the neuron versus input&#13;&#10;" src="image/C12626_05_36.jpg"/></div><h6>Figure 5.36: Plot of the neuron versus input</h6></li>
				<li>Now, set <strong class="inline">theta = 5</strong> and recompute and store the output of the neuron:<p class="snippet">theta = 5</p><p class="snippet">y_2 = [relu(_x * theta) for _x in x]</p></li>
				<li>Now, set <strong class="inline">theta = 0.2</strong> and recompute and store the output of the neuron:<p class="snippet">theta = 0.2</p><p class="snippet">y_3 = [relu(_x * theta) for _x in x]</p></li>
				<li>Plot the three different output curves of the neuron (<strong class="inline">theta = 1</strong>, <strong class="inline">theta = 5</strong>, <strong class="inline">theta = 0.2</strong>) on one graph:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_subplot(111)</p><p class="snippet">ax.plot(x, y, label='$\Theta=1$');</p><p class="snippet">ax.plot(x, y_2, label='$\Theta=5$', linestyle=':');</p><p class="snippet">ax.plot(x, y_3, label='$\Theta=0.2$', linestyle='--');</p><p class="snippet">ax.set_xlabel('$x\Theta$', fontsize=22);</p><p class="snippet">ax.set_ylabel('$h(x\Theta)$', fontsize=22);</p><p class="snippet">ax.spines['left'].set_position(('data', 0));</p><p class="snippet">ax.spines['top'].set_visible(False);</p><p class="snippet">ax.spines['right'].set_visible(False);</p><p class="snippet">ax.tick_params(axis='both', which='major', labelsize=22);</p><p class="snippet">ax.legend(fontsize=22);</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer430">
					<img alt="Figure 5.37: Three output curves of the neuron&#13;&#10;" src="image/C12626_05_37.jpg"/>
				</div>
			</div>
			<h6>Figure 5.37: Three output curves of the neuron</h6>
			<p>In this activity, we created a model of a ReLU-based artificial neural network neuron. We can see that the output of this neuron is very different to the sigmoid activation function. There is no saturation region for values greater than 0 because it simply returns the input value of the function. In the negative direction, there is a saturation region where only 0 will be returned if the input is less than 0. The ReLU function is an extremely powerful and commonly used activation function that has shown to be more powerful than the sigmoid function in some circumstances. ReLU is often a good first-choice activation function.</p>
			<h3 id="_idParaDest-228"><a id="_idTextAnchor252"/>Activity 9: MNIST Neural Network</h3>
			<p>Solution:</p>
			<p>In this activity, you will train a neural network to identify images in the MNIST dataset and reinforce your skills in training neural networks:</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, <strong class="inline">matplotlib</strong>, and the <strong class="inline">Sequential</strong> and <strong class="inline">Dense</strong> classes from Keras:<p class="snippet">import pickle</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Dense</p></li>
				<li>Load the <strong class="inline">mnist.pkl</strong> file, which contains the first 10,000 images and corresponding labels from the MNIST dataset that are available in the accompanying source code. The MNIST dataset is a series of 28 x 28 grayscale images of handwritten digits 0 through 9. Extract the images and labels:<p class="snippet">with open('mnist.pkl', 'rb') as f:</p><p class="snippet">    data = pickle.load(f)</p><p class="snippet">    </p><p class="snippet">images = data['images']</p><p class="snippet">labels = data['labels']</p></li>
				<li>Plot the first 10 samples along with the corresponding labels:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for i in range(10):</p><p class="snippet">    plt.subplot(2, 5, i + 1)</p><p class="snippet">    plt.imshow(images[i], cmap='gray')</p><p class="snippet">    plt.title(labels[i])</p><p class="snippet">    plt.axis('off')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer431"><img alt="Figure 5.38: First 10 samples&#13;&#10;" src="image/C12626_05_38.jpg"/></div><h6>Figure 5.38: First 10 samples</h6></li>
				<li>Encode the labels using one hot encoding:<p class="snippet">one_hot_labels = np.zeros((images.shape[0], 10))</p><p class="snippet">for idx, label in enumerate(labels):</p><p class="snippet">    one_hot_labels[idx, label] = 1</p><p class="snippet">    </p><p class="snippet">one_hot_labels</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer432"><img alt="Figure 5.39: Result of one hot encoding&#13;&#10;" src="image/C12626_05_39.jpg"/></div><h6>Figure 5.39: Result of one hot encoding</h6></li>
				<li>Prepare the images for input into a neural network. As a hint, there are two separate steps in this process:<p class="snippet">images = images.reshape((-1, 28 ** 2))</p><p class="snippet">images = images / 255.</p></li>
				<li>Construct a neural network model in Keras that accepts the prepared images, has a hidden layer of 600 units with a ReLU activation function, and an output of the same number of units as classes. The output layer uses a <strong class="inline">softmax</strong> activation function:<p class="snippet">model = Sequential([</p><p class="snippet">    Dense(600, input_shape=(784,), activation='relu'),</p><p class="snippet">    Dense(10, activation='softmax'),</p><p class="snippet">])</p></li>
				<li>Compile the model using multiclass cross-entropy, stochastic gradient descent, and an accuracy performance metric:<p class="snippet">model.compile(loss='categorical_crossentropy',</p><p class="snippet">              optimizer='sgd',</p><p class="snippet">              metrics=['accuracy'])</p></li>
				<li>Train the model. How many epochs are required to achieve at least 95% classification accuracy on the training data? Let's have a look:<p class="snippet">model.fit(images, one_hot_labels, epochs=20)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer433">
					<img alt="Figure 5.40: Training the model&#13;&#10;" src="image/C12626_05_40.jpg"/>
				</div>
			</div>
			<h6>Figure 5.40: Training the model</h6>
			<p>15 epochs are required to achieve at least 95% classification accuracy on the training set.</p>
			<p>In this example, we have measured the performance of the neural network classifier using the data that the classifier was trained with. In general, this method should not be used as it typically reports a higher level of accuracy than one should expect from the model. In supervised learning problems, there are a number of <strong class="keyword">cross-validation</strong> techniques that should be used instead. As this is a book on unsupervised learning, cross-validation lies outside the scope of this book.</p>
			<h3 id="_idParaDest-229"><a id="_idTextAnchor253"/>Activity 10: Simple MNIST Autoencoder</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong>, and the <strong class="inline">Model</strong>, <strong class="inline">Input</strong>, and <strong class="inline">Dense</strong> classes from Keras:<p class="snippet">import pickle</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from keras.models import Model</p><p class="snippet">from keras.layers import Input, Dense</p></li>
				<li>Load the images from the supplied sample of the MNIST dataset that is provided with the accompanying source code (<strong class="inline">mnist.pkl</strong>):<p class="snippet">with open('mnist.pkl', 'rb') as f:</p><p class="snippet">    images = pickle.load(f)['images']</p></li>
				<li>Prepare the images for input into a neural network. As a hint, there are <strong class="bold">two</strong> separate steps in this process:<p class="snippet">images = images.reshape((-1, 28 ** 2))</p><p class="snippet">images = images / 255.</p></li>
				<li>Construct a simple autoencoder network that reduces the image size to 10 x 10 after the encoding stage:<p class="snippet">input_stage = Input(shape=(784,))</p><p class="snippet">encoding_stage = Dense(100, activation='relu')(input_stage)</p><p class="snippet">decoding_stage = Dense(784, activation='sigmoid')(encoding_stage)</p><p class="snippet">autoencoder = Model(input_stage, decoding_stage)</p></li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and <strong class="inline">adadelta</strong> gradient descent:<p class="snippet">autoencoder.compile(loss='binary_crossentropy',</p><p class="snippet">              optimizer='adadelta')</p></li>
				<li>Fit the encoder model:<p class="snippet">autoencoder.fit(images, images, epochs=100)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer434"><img alt="Figure 5.41: Training the model&#13;&#10;" src="image/C12626_05_41.jpg"/></div><h6>Figure 5.41: Training the model</h6></li>
				<li>Calculate and store the output of the encoding stage for the first five samples:<p class="snippet">encoder_output = Model(input_stage, encoding_stage).predict(images[:5])</p></li>
				<li>Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by 255:<p class="snippet">encoder_output = encoder_output.reshape((-1, 10, 10)) * 255</p></li>
				<li>Calculate and store the output of the decoding stage for the first five samples:<p class="snippet">decoder_output = autoencoder.predict(images[:5])</p></li>
				<li>Reshape the output of the decoder to 28 x 28 and multiply by 255:<p class="snippet">decoder_output = decoder_output.reshape((-1, 28, 28)) * 255</p></li>
				<li>Plot the original image, the encoder output, and the decoder:<p class="snippet">images = images.reshape((-1, 28, 28))</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for i in range(5):</p><p class="snippet">    plt.subplot(3, 5, i + 1)</p><p class="snippet">    plt.imshow(images[i], cmap='gray')</p><p class="snippet">    plt.axis('off')</p><p class="snippet">    plt.subplot(3, 5, i + 6)</p><p class="snippet">    plt.imshow(encoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')   </p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 11)</p><p class="snippet">    plt.imshow(decoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')    </p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer435">
					<img alt="Figure 5.42: The original image, the encoder output, and the decoder&#13;&#10;" src="image/C12626_05_42.jpg"/>
				</div>
			</div>
			<h6>Figure 5.42: The original image, the encoder output, and the decoder</h6>
			<p>So far, we have shown how a simple single hidden layer in both the encoding and decoding stage can be used to reduce the data to a lower dimension space. We can also make this model more complicated by adding additional layers to both the encoding and the decoding stages.</p>
			<h3 id="_idParaDest-230"><a id="_idTextAnchor254"/>Activity 11: MNIST Convolutional Autoencoder</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, <strong class="inline">matplotlib</strong>, and the <strong class="inline">Model</strong> class from <strong class="inline">keras.models</strong> and import <strong class="inline">Input</strong>, <strong class="inline">Conv2D</strong>, <strong class="inline">MaxPooling2D</strong>, and <strong class="inline">UpSampling2D</strong> from <strong class="inline">keras.layers</strong>:<p class="snippet">import pickle</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from keras.models import Model</p><p class="snippet">from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D</p></li>
				<li>Load the data:<p class="snippet">with open('mnist.pkl', 'rb') as f:</p><p class="snippet">    images = pickle.load(f)['images']</p></li>
				<li>Rescale the images to have values between 0 and 1:<p class="snippet">images = images / 255.</p></li>
				<li>We need to reshape the images to add a single depth channel for use with convolutional stages. Reshape the images to have a shape of 28 x 28 x 1:<p class="snippet">images = images.reshape((-1, 28, 28, 1))</p></li>
				<li>Define an input layer. We will use the same shape input as an image:<p class="snippet">input_layer = Input(shape=(28, 28, 1,))</p></li>
				<li>Add a convolutional stage with 16 layers or filters, a 3 x 3 weight matrix, a ReLU activation function, and using same padding, which means the output has the same length as the input image:<p class="snippet">hidden_encoding = Conv2D(</p><p class="snippet">    16, # Number of layers or filters in the weight matrix</p><p class="snippet">    (3, 3), # Shape of the weight matrix</p><p class="snippet">    activation='relu',</p><p class="snippet">    padding='same', # How to apply the weights to the images</p><p class="snippet">)(input_layer)</p></li>
				<li>Add a max pooling layer to the encoder with a 2 x 2 kernel:<p class="snippet">encoded = MaxPooling2D((2, 2))(hidden_encoding)</p></li>
				<li>Add a decoding convolutional layer:<p class="snippet">hidden_decoding = Conv2D(</p><p class="snippet">    16, # Number of layers or filters in the weight matrix</p><p class="snippet">    (3, 3), # Shape of the weight matrix</p><p class="snippet">    activation='relu',</p><p class="snippet">    padding='same', # How to apply the weights to the images</p><p class="snippet">)(encoded)</p></li>
				<li>Add an upsampling layer:<p class="snippet">upsample_decoding = UpSampling2D((2, 2))(hidden_decoding)</p></li>
				<li>Add the final convolutional stage, using one layer as per the initial image depth:<p class="snippet">decoded = Conv2D(</p><p class="snippet">    1, # Number of layers or filters in the weight matrix</p><p class="snippet">    (3, 3), # Shape of the weight matrix</p><p class="snippet">    activation='sigmoid',</p><p class="snippet">    padding='same', # How to apply the weights to the images</p><p class="snippet">)(upsample_decoding)</p></li>
				<li>Construct the model by passing the first and last layers of the network to the <strong class="inline">Model</strong> class:<p class="snippet">autoencoder = Model(input_layer, decoded)</p></li>
				<li>Display the structure of the model:<p class="snippet">autoencoder.summary()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer436"><img alt="Figure 5.43: Structure of model&#13;&#10;" src="image/C12626_05_43.jpg"/></div><h6>Figure 5.43: Structure of model</h6></li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and <strong class="inline">adadelta</strong> gradient descent:<p class="snippet">autoencoder.compile(loss='binary_crossentropy',</p><p class="snippet">              optimizer='adadelta')</p></li>
				<li>Now, let's fit the model; again, we pass the images as the training data and as the desired output.  Train for 20 epochs as convolutional networks take a lot longer to compute:<p class="snippet">autoencoder.fit(images, images, epochs=20)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer437"><img alt="Figure 5.44: Training the model&#13;&#10;" src="image/C12626_05_44.jpg"/></div><h6>Figure 5.44: Training the model</h6></li>
				<li>Calculate and store the output of the encoding stage for the first five samples:<p class="snippet">encoder_output = Model(input_layer, encoded).predict(images[:5])</p></li>
				<li>Reshape the encoder output for visualization, where each image is X*Y in size:<p class="snippet">encoder_output = encoder_output.reshape((-1, 14 * 14, 16))</p></li>
				<li>Get the output of the decoder for the first five images:<p class="snippet">decoder_output = autoencoder.predict(images[:5])</p></li>
				<li>Reshape the decoder output to 28 x 28 in size:<p class="snippet">decoder_output = decoder_output.reshape((-1, 28, 28))</p></li>
				<li>Reshape the original images back to 28 x 28 in size:<p class="snippet">images = images.reshape((-1, 28, 28))</p></li>
				<li>Plot the original image, the mean encoder output, and the decoder:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for i in range(5):</p><p class="snippet">    plt.subplot(3, 5, i + 1)</p><p class="snippet">    plt.imshow(images[i], cmap='gray')</p><p class="snippet">    plt.axis('off')</p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 6)</p><p class="snippet">    plt.imshow(encoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')   </p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 11)</p><p class="snippet">    plt.imshow(decoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')        </p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer438">
					<img alt="Figure 5.45: The original image, the encoder output, and the decoder&#13;&#10;" src="image/C12626_05_45.jpg"/>
				</div>
			</div>
			<h6>Figure 5.45: The original image, the encoder output, and the decoder</h6>
			<p>At the end of this activity, you will have developed an autoencoder comprising convolutional layers within the neural network. Note the improvements made in the decoder representations. This architecture has a significant performance benefit over fully-connected neural network layers and is extremely useful in working with image-based datasets and generating artificial data samples.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor255"/>Chapter 6: t-Distributed Stochastic Neighbor Embedding (t-SNE)</h2>
			<h3 id="_idParaDest-232"><a id="_idTextAnchor256"/>Activity 12: Wine t-SNE</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, <strong class="inline">matplotlib</strong>, and the <strong class="inline">t-SNE</strong> and <strong class="inline">PCA</strong> models from scikit-learn:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.decomposition import PCA</p><p class="snippet">from sklearn.manifold import TSNE</p></li>
				<li>Load the Wine dataset using the <strong class="inline">wine.data</strong> file included in the accompanying source code and display the first five rows of data:<p class="snippet">df = pd.read_csv('wine.data', header=None)</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer439"><img alt="Figure 6.24: The first five rows of the wine dataset.&#13;&#10;" src="image/C12626_06_24.jpg"/></div><h6>Figure 6.24: The first five rows of the wine dataset.</h6></li>
				<li>The first column contains the labels; extract this column and remove it from the dataset:<p class="snippet">labels = df[0]</p><p class="snippet">del df[0]</p></li>
				<li>Execute PCA to reduce the dataset to the first six components:<p class="snippet">model_pca = PCA(n_components=6)</p><p class="snippet">wine_pca = model_pca.fit_transform(df)</p></li>
				<li>Determine the amount of variance within the data described by these six components:<p class="snippet">np.sum(model_pca.explained_variance_ratio_)</p><p>The output is as follows:</p><p class="snippet">0.99999314824536</p></li>
				<li>Create a t-SNE model using a specified random state and a <strong class="inline">verbose</strong> value of 1:<p class="snippet">tsne_model = TSNE(random_state=0, verbose=1)</p><p class="snippet">tsne_model</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer440"><img alt="Figure 6.25: Creating t-SNE model.&#13;&#10;" src="image/C12626_06_25.jpg"/></div><h6>Figure 6.25: Creating t-SNE model.</h6></li>
				<li>Fit the PCA data to the t-SNE model:<p class="snippet">wine_tsne = tsne_model.fit_transform(wine_pca.reshape((len(wine_pca), -1)))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer441"><img alt="Figure 6.26: Fitting PCA data t-SNE model&#13;&#10;" src="image/C12626_06_26.jpg"/></div><h6>Figure 6.26: Fitting PCA data t-SNE model</h6></li>
				<li>Confirm that the shape of the t-SNE fitted data is two dimensional:<p class="snippet">wine_tsne.shape</p><p>The output is as follows:</p><p class="snippet">(172, 8)</p></li>
				<li>Create a scatter plot of the two-dimensional data:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.scatter(wine_tsne[:,0], wine_tsne[:,1]);</p><p class="snippet">plt.title('Low Dimensional Representation of Wine');</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer442"><img alt="Figure 6.27: Scatterplot of two-dimensional data&#13;&#10;" src="image/C12626_06_27.jpg"/></div><h6>Figure 6.27: Scatterplot of two-dimensional data</h6></li>
				<li>Create a secondary scatter plot of the two-dimensional data with the class labels applied to visualize any clustering that may be present:<p class="snippet">MARKER = ['o', 'v', '^',]</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.title('Low Dimensional Representation of Wine');</p><p class="snippet">for i in range(1, 4):</p><p class="snippet">    selections = wine_tsne[labels == i]</p><p class="snippet">    plt.scatter(selections[:,0], selections[:,1], marker=MARKER[i-1], label=f'Wine {i}', s=30);</p><p class="snippet">    plt.legend();</p><p class="snippet">plt.show()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer443">
					<img alt="Figure 6.28: Secondary plot of two-dimensional data&#13;&#10;" src="image/C12626_06_28.jpg"/>
				</div>
			</div>
			<h6>Figure 6.28: Secondary plot of two-dimensional data</h6>
			<p>Note that while there is an overlap between the wine classes, it can also be seen that there is some clustering within the data. The first wine class is predominantly positioned in the top left-hand corner of the plot, the second wine class in the bottom-right, and the third wine class between the first two. This representation certainly couldn't be used to classify individual wine samples with great confidence, but it shows an overall trend and series of clusters contained within the high-dimensional data that we were unable to see earlier.</p>
			<h3 id="_idParaDest-233"><a id="_idTextAnchor257"/>Activity 13: t-SNE Wine and Perplexity</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, <strong class="inline">matplotlib</strong>, and the <strong class="inline">t-SNE</strong> and <strong class="inline">PCA</strong> models from scikit-learn:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.decomposition import PCA</p><p class="snippet">from sklearn.manifold import TSNE</p></li>
				<li>Load the Wine dataset and inspect the first five rows:<p class="snippet">df = pd.read_csv('wine.data', header=None)</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer444"><img alt="Figure 6.29: The first five rows of wine data.&#13;&#10;" src="image/C12626_06_29.jpg"/></div><h6>Figure 6.29: The first five rows of wine data.</h6></li>
				<li>The first column provides the labels; extract them from the DataFrame and store them in a separate variable. Ensure that the column is removed from the DataFrame:<p class="snippet">labels = df[0]</p><p class="snippet">del df[0]</p></li>
				<li>Execute PCA on the dataset and extract the first six components:<p class="snippet">model_pca = PCA(n_components=6)</p><p class="snippet">wine_pca = model_pca.fit_transform(df)</p><p class="snippet">wine_pca = wine_pca.reshape((len(wine_pca), -1))</p></li>
				<li>Construct a loop that iterates through the perplexity values (1, 5, 20, 30, 80, 160, 320). For each loop, generate a t-SNE model with the corresponding perplexity and print a scatter plot of the labeled wine classes. Note the effect of different perplexity values:<p class="snippet">MARKER = ['o', 'v', '^',]</p><p class="snippet">for perp in [1, 5, 20, 30, 80, 160, 320]:</p><p class="snippet">    tsne_model = TSNE(random_state=0, verbose=1, perplexity=perp)</p><p class="snippet">    wine_tsne = tsne_model.fit_transform(wine_pca)</p><p class="snippet">    plt.figure(figsize=(10, 7))</p><p class="snippet">    plt.title(f'Low Dimensional Representation of Wine. Perplexity {perp}');</p><p class="snippet">    for i in range(1, 4):</p><p class="snippet">        selections = wine_tsne[labels == i]</p><p class="snippet">        plt.scatter(selections[:,0], selections[:,1], marker=MARKER[i-1], label=f'Wine {i}', s=30);</p><p class="snippet">        plt.legend();</p><p>A perplexity value of 1 fails to separate the data into any particular structure:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer445">
					<img alt="Figure 6.30: Plot for perplexity value 1&#13;&#10;" src="image/C12626_06_30.jpg"/>
				</div>
			</div>
			<h6>Figure 6.30: Plot for perplexity value 1</h6>
			<p>Increasing the perplexity to 5 leads to a very non-linear structure that is difficult to separate, and it's hard to identify any clusters or patterns:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer446">
					<img alt="Figure 6.31: Plot for perplexity of 5&#13;&#10;" src="image/C12626_06_31.jpg"/>
				</div>
			</div>
			<h6>Figure 6.31: Plot for perplexity of 5</h6>
			<p>A perplexity of 20 finally starts to show some sort of horse-shoe structure. While visually obvious, this can still be tricky to implement:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer447">
					<img alt="Figure 6.32: Plot for perplexity of 20&#13;&#10;" src="image/C12626_06_32.jpg"/>
				</div>
			</div>
			<h6>Figure 6.32: Plot for perplexity of 20</h6>
			<p>A perplexity of 30 demonstrates quite good results. There is a linear relationship between the projected structure with some separation between the types of wine:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer448">
					<img alt="Figure 6.33: Plot for perplexity of 30&#13;&#10;" src="image/C12626_06_33.jpg"/>
				</div>
			</div>
			<h6>Figure 6.33: Plot for perplexity of 30</h6>
			<p>Finally, the last two images in the activity show the extent to which the plots can become increasingly complex and non-linear with increasing perplexity:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer449">
					<img alt="Figure 6.34: Plot for perplexity of 80&#13;&#10;" src="image/C12626_06_34.jpg"/>
				</div>
			</div>
			<h6>Figure 6.34: Plot for perplexity of 80</h6>
			<p>Here's the plot for a perplexity of 160:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer450">
					<img alt="Figure 6.35: Plot for perplexity of 160&#13;&#10;" src="image/C12626_06_35.jpg"/>
				</div>
			</div>
			<h6>Figure 6.35: Plot for perplexity of 160</h6>
			<p>Looking at the individual plots for each of the perplexity values, the effect perplexity has on the visualization of data is immediately obvious. Very small or very large perplexity values produces a range of unusual shapes that don't indicate the presence of any persistent pattern. The most plausible value seems to be 30, which produced the most linear plot we saw in the previous activity.</p>
			<p>In this activity, we demonstrated the need to be careful when selecting the perplexity and that some iteration may be required to determine the correct value.</p>
			<h3 id="_idParaDest-234"><a id="_idTextAnchor258"/>Activity 14: t-SNE Wine and Iterations</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, <strong class="inline">matplotlib</strong>, and the <strong class="inline">t-SNE</strong> and <strong class="inline">PCA</strong> models from scikit-learn:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.decomposition import PCA</p><p class="snippet">from sklearn.manifold import TSNE</p></li>
				<li>Load the Wine dataset and inspect the first five rows:<p class="snippet">df = pd.read_csv('wine.data', header=None)</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer451"><img alt="Figure 6.36: The first five rows of wine dataset&#13;&#10;" src="image/C12626_06_36.jpg"/></div><h6>Figure 6.36: The first five rows of wine dataset</h6></li>
				<li>The first column provides the labels; extract these from the DataFrame and store them in a separate variable. Ensure that the column is removed from the DataFrame:<p class="snippet">labels = df[0]</p><p class="snippet">del df[0]</p></li>
				<li>Execute PCA on the dataset and extract the first six components:<p class="snippet">model_pca = PCA(n_components=6)</p><p class="snippet">wine_pca = model_pca.fit_transform(df)</p><p class="snippet">wine_pca = wine_pca.reshape((len(wine_pca), -1))</p></li>
				<li>Construct a loop that iterates through the iteration values (<strong class="inline">250</strong>, <strong class="inline">500</strong>, <strong class="inline">1000</strong>). For each loop, generate a t-SNE model with the corresponding number of iterations and identical number of iterations without progress values:<p class="snippet">MARKER = ['o', 'v', '1', 'p' ,'*', '+', 'x', 'd', '4', '.']</p><p class="snippet">for iterations in [250, 500, 1000]:</p><p class="snippet">    model_tsne = TSNE(random_state=0, verbose=1, n_iter=iterations, n_iter_without_progress=iterations)</p><p class="snippet">    mnist_tsne = model_tsne.fit_transform(mnist_pca)</p></li>
				<li>Construct a scatter plot of the labeled wine classes<a id="_idTextAnchor259"/>. Note the effect of different iteration values:<p class="snippet">    plt.figure(figsize=(10, 7))</p><p class="snippet">    plt.title(f'Low Dimensional Representation of MNIST (iterations = {iterations})');</p><p class="snippet">    for i in range(10):</p><p class="snippet">        selections = mnist_tsne[mnist['labels'] == i]</p><p class="snippet">        plt.scatter(selections[:,0], selections[:,1], alpha=0.2, marker=MARKER[i], s=5);</p><p class="snippet">        x, y = selections.mean(axis=0)</p><p class="snippet">        plt.text(x, y, str(i), fontdict={'weight': 'bold', 'size': 30}) </p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer452">
					<img alt="Figure 6.37: Scatterplot of wine classes with 250 iterations&#13;&#10;" src="image/C12626_06_37.jpg"/>
				</div>
			</div>
			<h6>Figure 6.37: Scatterplot of wine classes with 250 iterations</h6>
			<p>Here's the plot for 500 iterations:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer453">
					<img alt="Figure 6.38: Scatterplot of wine classes with 500 iterations&#13;&#10;" src="image/C12626_06_38.jpg"/>
				</div>
			</div>
			<h6>Figure 6.38: Scatterplot of wine classes with 500 iterations</h6>
			<p>Here's the plot for 1,000 iterations:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer454">
					<img alt="Figure 6.39: Scatterplot of wine classes with 1,000 iterations&#13;&#10;" src="image/C12626_06_39.jpg"/>
				</div>
			</div>
			<h6>Figure 6.39: Scatterplot of wine classes with 1,000 iterations</h6>
			<p>Again, we can see the improvement in the structure of the data as the number of iterations increase. Even in a relatively simple dataset such as this, 250 iterations are not sufficient to project any structure of data into the lower-dimensional space.</p>
			<p>As we observed in the corresponding activity, there is a balance to find in setting the iteration parameter. In this example, 250 iterations were insufficient, and at least 1,000 iterations were required for the final stabilization of the data.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor260"/>Chapter 7: Topic Modeling</h2>
			<h3 id="_idParaDest-236"><a id="_idTextAnchor261"/>Activity 15: Loading and Cleaning Twitter Data</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the necessary libraries:<p class="snippet">import langdetect</p><p class="snippet">import matplotlib.pyplot</p><p class="snippet">import nltk</p><p class="snippet">import numpy</p><p class="snippet">import pandas</p><p class="snippet">import pyLDAvis</p><p class="snippet">import pyLDAvis.sklearn</p><p class="snippet">import regex</p><p class="snippet">import sklearn</p></li>
				<li>Load the LA Times health Twitter data (<strong class="inline">latimeshealth.txt</strong>) from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17</a>:<h4>Note</h4><p class="callout">Pay close attention to the delimiter (it is neither a comma nor a tab) and double-check the header status.</p><p class="snippet">path = '&lt;Path&gt;/latimeshealth.txt'</p><p class="snippet">df = pandas.read_csv(path, sep="|", header=None)</p><p class="snippet">df.columns = ["id", "datetime", "tweettext"]</p></li>
				<li>Run a quick exploratory analysis to ascertain the data size and structure:<p class="snippet">def dataframe_quick_look(df, nrows):</p><p class="snippet">print("SHAPE:\n{shape}\n".format(shape=df.shape))</p><p class="snippet">print("COLUMN NAMES:\n{names}\n".format(names=df.columns))</p><p class="snippet">print("HEAD:\n{head}\n".format(head=df.head(nrows)))</p><p class="snippet">dataframe_quick_look(df, nrows=2)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer455"><img alt="Figure 7.54: Shape, column names, and head of data&#13;&#10;" src="image/C12626_07_54.jpg"/></div><h6>Figure 7.54: Shape, column names, and head of data</h6></li>
				<li>Extract the tweet text and convert it to a list object:<p class="snippet">raw = df['tweettext'].tolist()</p><p class="snippet">print("HEADLINES:\n{lines}\n".format(lines=raw[:5]))</p><p class="snippet">print("LENGTH:\n{length}\n".format(length=len(raw)))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer456"><img alt="Figure 7.55: Headlines and their length&#13;&#10;" src="image/C12626_07_55.jpg"/></div><h6>Figure 7.55: Headlines and their length</h6></li>
				<li>Write a function to perform language detection, tokenization on whitespaces, and replace screen names and URLs with <strong class="inline">SCREENNAME</strong> and <strong class="inline">URL</strong>, respectively. The function should also remove punctuation, numbers, and the <strong class="inline">SCREENNAME</strong> and <strong class="inline">URL</strong> replacements. Convert everything to lowercase, except <strong class="inline">SCREENNAME</strong> and <strong class="inline">URL</strong>. It should remove all stop words, perform lemmatization, and keep words with five or more letters:<h4>Note</h4><p class="callout">Screen names start with the <strong class="inline">@</strong> symbol.</p><p class="snippet">def do_language_identifying(txt):</p><p class="snippet">    	try:</p><p class="snippet">           the_language = langdetect.detect(txt)</p><p class="snippet">    	except:</p><p class="snippet">        	the_language = 'none'</p><p class="snippet">    	return the_language</p><p class="snippet">def do_lemmatizing(wrd):</p><p class="snippet">    	out = nltk.corpus.wordnet.morphy(wrd)</p><p class="snippet">    	return (wrd if out is None else out)</p><p class="snippet">def do_tweet_cleaning(txt):</p><p class="snippet"># identify language of tweet</p><p class="snippet"># return null if language not english</p><p class="snippet">    	lg = do_language_identifying(txt)</p><p class="snippet">    	if lg != 'en':</p><p class="snippet">        	return None</p><p class="snippet"># split the string on whitespace</p><p class="snippet">    	out = txt.split(' ')</p><p class="snippet"># identify screen names</p><p class="snippet"># replace with SCREENNAME</p><p class="snippet">    	out = ['SCREENNAME' if i.startswith('@') else i for i in out]</p><p class="snippet"># identify urls</p><p class="snippet"># replace with URL</p><p class="snippet">    	out = ['URL' if bool(regex.search('http[s]?://', i)) else i for i in out]</p><p class="snippet">      # remove all punctuation</p><p class="snippet">    	out = [regex.sub('[^\\w\\s]|\n', '', i) for i in out]</p><p class="snippet">      # make all non-keywords lowercase</p><p class="snippet">    	keys = ['SCREENNAME', 'URL']</p><p class="snippet">    	out = [i.lower() if i not in keys else i for i in out]</p><p class="snippet">      # remove keywords</p><p class="snippet">    	out = [i for i in out if i not in keys]</p><p class="snippet">      # remove stopwords</p><p class="snippet">    	list_stop_words = nltk.corpus.stopwords.words('english')</p><p class="snippet">    	list_stop_words = [regex.sub('[^\\w\\s]', '', i) for i in list_stop_words]</p><p class="snippet">    	out = [i for i in out if i not in list_stop_words]</p><p class="snippet">      # lemmatizing</p><p class="snippet">    	out = [do_lemmatizing(i) for i in out]</p><p class="snippet">      # keep words 4 or more characters long</p><p class="snippet">    	out = [i for i in out if len(i) &gt;= 5]</p><p class="snippet">    	return out</p></li>
				<li>Apply the function defined in <em class="italics">step 5</em> to every tweet:<p class="snippet">clean = list(map(do_tweet_cleaning, raw))</p></li>
				<li>Remove elements of output list equal to <strong class="inline">None</strong>:<p class="snippet">clean = list(filter(None.__ne__, clean))</p><p class="snippet">print("HEADLINES:\n{lines}\n".format(lines=clean[:5]))</p><p class="snippet">print("LENGTH:\n{length}\n".format(length=len(clean)))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer457"><img alt="Figure 7.56: Headline and length after removing None&#13;&#10;" src="image/C12626_07_56.jpg"/></div><h6>Figure 7.56: Headline and length after removing None</h6></li>
				<li>Turn the elements of each tweet back into a string. Concatenate using white space:<p class="snippet">clean_sentences = [" ".join(i) for i in clean]</p><p class="snippet">print(clean_sentences[0:10])</p><p>The first 10 elements of the output list should resemble the following:</p><div class="IMG---Figure" id="_idContainer458"><img alt="Figure 7.57: Tweets cleaned for modeling&#13;&#10;" src="image/C12626_07_57.jpg"/></div><h6>Figure 7.57: Tweets cleaned for modeling</h6></li>
				<li>Keep the notebook open for future modeling.</li>
			</ol>
			<h3 id="_idParaDest-237"><a id="_idTextAnchor262"/>Activity 16: Latent Dirichlet Allocation and Health Tweets</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Specify the <strong class="inline">number_words</strong>, <strong class="inline">number_docs</strong>, and <strong class="inline">number_features</strong> variables:<p class="snippet">number_words = 10</p><p class="snippet">number_docs = 10</p><p class="snippet">number_features = 1000</p></li>
				<li>Create a bag-of-words model and assign the feature names to another variable for use later on:<p class="snippet">vectorizer1 = sklearn.feature_extraction.text.CountVectorizer(</p><p class="snippet">    analyzer=»word»,</p><p class="snippet">    max_df=0.95, </p><p class="snippet">    min_df=10, </p><p class="snippet">    max_features=number_features</p><p class="snippet">)</p><p class="snippet">clean_vec1 = vectorizer1.fit_transform(clean_sentences)</p><p class="snippet">print(clean_vec1[0])</p><p class="snippet">feature_names_vec1 = vectorizer1.get_feature_names()</p><p>The output is as follows:</p><p class="snippet">(0, 320)    1</p></li>
				<li>Identify the optimal number of topics:<p class="snippet">def perplexity_by_ntopic(data, ntopics):</p><p class="snippet">    output_dict = {</p><p class="snippet">        «Number Of Topics": [], </p><p class="snippet">        «Perplexity Score»: []</p><p class="snippet">    }</p><p class="snippet">    for t in ntopics:</p><p class="snippet">        lda = sklearn.decomposition.LatentDirichletAllocation(</p><p class="snippet">            n_components=t,</p><p class="snippet">            learning_method="online",</p><p class="snippet">            random_state=0</p><p class="snippet">        )</p><p class="snippet">        lda.fit(data)</p><p class="snippet">        output_dict["Number Of Topics"].append(t)</p><p class="snippet">        output_dict["Perplexity Score"].append(lda.perplexity(data))</p><p class="snippet">    output_df = pandas.DataFrame(output_dict)</p><p class="snippet">    index_min_perplexity = output_df["Perplexity Score"].idxmin()</p><p class="snippet">    output_num_topics = output_df.loc[</p><p class="snippet">        index_min_perplexity,  # index</p><p class="snippet">        «Number Of Topics"  # column</p><p class="snippet">    ]</p><p class="snippet">    return (output_df, output_num_topics)</p><p class="snippet">df_perplexity, optimal_num_topics = perplexity_by_ntopic(</p><p class="snippet">    clean_vec1, </p><p class="snippet">    ntopics=[i for i in range(1, 21) if i % 2 == 0]</p><p class="snippet">)</p><p class="snippet">print(df_perplexity)</p><p>The output is as follows:</p><h6> </h6><div class="IMG---Figure" id="_idContainer459"><img alt="Figure 7.58: Number of topics versus perplexity score data frame&#13;&#10;" src="image/C12626_07_58.jpg"/></div><h6>Figure 7.58: Number of topics versus perplexity score data frame</h6></li>
				<li>Fit the LDA model using the optimal number of topics:<p class="snippet">lda = sklearn.decomposition.LatentDirichletAllocation(</p><p class="snippet">    n_components=optimal_num_topics,</p><p class="snippet">    learning_method="online",</p><p class="snippet">    random_state=0</p><p class="snippet">)</p><p class="snippet">lda.fit(clean_vec1)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer460"><img alt="Figure 7.59: LDA model&#13;&#10;" src="image/C12626_07_59.jpg"/></div><h6>Figure 7.59: LDA model</h6></li>
				<li>Create and print the word-topic table:<p class="snippet">def get_topics(mod, vec, names, docs, ndocs, nwords):</p><p class="snippet">    # word to topic matrix</p><p class="snippet">    W = mod.components_</p><p class="snippet">    W_norm = W / W.sum(axis=1)[:, numpy.newaxis]</p><p class="snippet">    # topic to document matrix</p><p class="snippet">    H = mod.transform(vec)</p><p class="snippet">    W_dict = {}</p><p class="snippet">    H_dict = {}</p><p class="snippet">    for tpc_idx, tpc_val in enumerate(W_norm):</p><p class="snippet">        topic = «Topic{}".format(tpc_idx)</p><p class="snippet">        # formatting w</p><p class="snippet">        W_indices = tpc_val.argsort()[::-1][:nwords]</p><p class="snippet">        W_names_values = [</p><p class="snippet">            (round(tpc_val[j], 4), names[j]) </p><p class="snippet">            for j in W_indices</p><p class="snippet">        ]</p><p class="snippet">        W_dict[topic] = W_names_values</p><p class="snippet">        # formatting h</p><p class="snippet">        H_indices = H[:, tpc_idx].argsort()[::-1][:ndocs]</p><p class="snippet">        H_names_values = [</p><p class="snippet">        (round(H[:, tpc_idx][j], 4), docs[j]) </p><p class="snippet">            for j in H_indices</p><p class="snippet">        ]</p><p class="snippet">        H_dict[topic] = H_names_values</p><p class="snippet">    W_df = pandas.DataFrame(</p><p class="snippet">        W_dict, </p><p class="snippet">        index=["Word" + str(i) for i in range(nwords)]</p><p class="snippet">    )</p><p class="snippet">    H_df = pandas.DataFrame(</p><p class="snippet">        H_dict,</p><p class="snippet">        index=["Doc" + str(i) for i in range(ndocs)]</p><p class="snippet">    )</p><p class="snippet">    return (W_df, H_df)</p><p class="snippet">W_df, H_df = get_topics(</p><p class="snippet">    mod=lda,</p><p class="snippet">    vec=clean_vec1,</p><p class="snippet">    names=feature_names_vec1,</p><p class="snippet">    docs=raw,</p><p class="snippet">    ndocs=number_docs, </p><p class="snippet">    nwords=number_words</p><p class="snippet">)</p><p class="snippet">print(W_df)</p><p>The output is as follows:</p><h6> </h6><div class="IMG---Figure" id="_idContainer461"><img alt="Figure 7.60: Word-topic table for the health tweet data&#13;&#10;" src="image/C12626_07_60.jpg"/></div><h6>Figure 7.60: Word-topic table for the health tweet data</h6></li>
				<li>Print the document-topic table:<p class="snippet">print(H_df)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer462"><img alt="Figure 7.61: Document topic table&#13;&#10;" src="image/C12626_07_61.jpg"/></div><h6>Figure 7.61: Document topic table</h6></li>
				<li>Create a biplot visualization:<p class="snippet">lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, vectorizer1, R=10)</p><p class="snippet">pyLDAvis.display(lda_plot)</p><div class="IMG---Figure" id="_idContainer463"><img alt="Figure 7.62: A histogram and biplot for the LDA model trained on health tweets&#13;&#10;" src="image/C12626_07_62.jpg"/></div><h6>Figure 7.62: A histogram and biplot for the LDA model trained on health tweets</h6></li>
				<li>Keep the notebook open for future modeling.</li>
			</ol>
			<h3 id="_idParaDest-238"><a id="_idTextAnchor263"/>Activity 17: Non-Negative Matrix Factorization</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Create the appropriate bag-of-words model and output the feature names as another variable:<p class="snippet">vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer(</p><p class="snippet">    analyzer="word",</p><p class="snippet">    max_df=0.5, </p><p class="snippet">    min_df=20, </p><p class="snippet">    max_features=number_features,</p><p class="snippet">    smooth_idf=False</p><p class="snippet">)</p><p class="snippet">clean_vec2 = vectorizer2.fit_transform(clean_sentences)</p><p class="snippet">print(clean_vec2[0])</p><p class="snippet">feature_names_vec2 = vectorizer2.get_feature_names()</p></li>
				<li>Define and fit the NMF algorithm using the number of topics (<strong class="inline">n_components</strong>) value from activity two:<p class="snippet">nmf = sklearn.decomposition.NMF(</p><p class="snippet">    n_components=optimal_num_topics,</p><p class="snippet">    init="nndsvda",</p><p class="snippet">    solver="mu",</p><p class="snippet">    beta_loss="frobenius",</p><p class="snippet">    random_state=0, </p><p class="snippet">    alpha=0.1, </p><p class="snippet">    l1_ratio=0.5</p><p class="snippet">)</p><p class="snippet">nmf.fit(clean_vec2)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer464"><img alt="Figure 7.63: Defining the NMF model&#13;&#10;" src="image/C12626_07_63.jpg"/></div><h6>Figure 7.63: Defining the NMF model</h6></li>
				<li>Get the topic-document and word-topic result tables. Take a few minutes to explore the word groupings and try to define the abstract topics:<p class="snippet">W_df, H_df = get_topics(</p><p class="snippet">    mod=nmf,</p><p class="snippet">    vec=clean_vec2,</p><p class="snippet">    names=feature_names_vec2,</p><p class="snippet">    docs=raw,</p><p class="snippet">    ndocs=number_docs, </p><p class="snippet">    nwords=number_words</p><p class="snippet">)</p><p class="snippet">print(W_df)</p><h6> </h6><div class="IMG---Figure" id="_idContainer465"><img alt="Figure 7.64: The word-topic table with probabilities&#13;&#10;" src="image/C12626_07_64.jpg"/></div><h6>Figure 7.64: The word-topic table with probabilities</h6></li>
				<li>Adjust the model parameters and rerun <em class="italics">step 3</em> and <em class="italics">step 4</em>.</li>
			</ol>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor264"/>Chapter 8: Market Basket Analysis</h2>
			<h3 id="_idParaDest-240"><a id="_idTextAnchor265"/>Activity 18: Loading and Preparing Full Online Retail Data</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Load the online retail dataset file:<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import mlxtend.frequent_patterns</p><p class="snippet">import mlxtend.preprocessing</p><p class="snippet">import numpy</p><p class="snippet">import pandas</p><p class="snippet">online = pandas.read_excel(</p><p class="snippet">    io="Online Retail.xlsx", </p><p class="snippet">    sheet_name="Online Retail", </p><p class="snippet">    header=0</p><p class="snippet">)</p></li>
				<li>Clean and prep the data for modeling, including turning the cleaned data into a list of lists:<p class="snippet">online['IsCPresent'] = (</p><p class="snippet">    online['InvoiceNo']</p><p class="snippet">    .astype(str)</p><p class="snippet">    .apply(lambda x: 1 if x.find('C') != -1 else 0)</p><p class="snippet">)</p><p class="snippet">online1 = (</p><p class="snippet">    online</p><p class="snippet">    .loc[online["Quantity"] &gt; 0]</p><p class="snippet">    .loc[online['IsCPresent'] != 1]</p><p class="snippet">    .loc[:, ["InvoiceNo", "Description"]]</p><p class="snippet">    .dropna()</p><p class="snippet">)</p><p class="snippet">invoice_item_list = []</p><p class="snippet">for num in list(set(online1.InvoiceNo.tolist())):</p><p class="snippet">    tmp_df = online1.loc[online1['InvoiceNo'] == num]</p><p class="snippet">    tmp_items = tmp_df.Description.tolist()</p><p class="snippet">    invoice_item_list.append(tmp_items)</p></li>
				<li>Encode the data and recast it as a DataFrame:<p class="snippet">online_encoder = mlxtend.preprocessing.TransactionEncoder()</p><p class="snippet">online_encoder_array = online_encoder.fit_transform(invoice_item_list)</p><p class="snippet">online_encoder_df = pandas.DataFrame(</p><p class="snippet">    online_encoder_array, </p><p class="snippet">    columns=online_encoder.columns_</p><p class="snippet">)</p><p class="snippet">online_encoder_df.loc[</p><p class="snippet">    20125:20135, </p><p class="snippet">    online_encoder_df.columns.tolist()[100:110]</p><p class="snippet">]</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer466">
					<img alt="Figure 8.35: A subset of the cleaned, encoded, and recast DataFrame built from the complete online retail dataset&#13;&#10;" src="image/C12626_08_35.jpg"/>
				</div>
			</div>
			<h6>Figure 8.35: A subset of the cleaned, encoded, and recast DataFrame built from the complete online retail dataset</h6>
			<h3 id="_idParaDest-241"><a id="_idTextAnchor266"/>Activity 19: Apriori on the Complete Online Retail Dataset</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Run the Apriori algorithm on the full data with reasonable parameter settings:<p class="snippet">mod_colnames_minsupport = mlxtend.frequent_patterns.apriori(</p><p class="snippet">    online_encoder_df, </p><p class="snippet">    min_support=0.01,</p><p class="snippet">    use_colnames=True</p><p class="snippet">)</p><p class="snippet">mod_colnames_minsupport.loc[0:6]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer467"><img alt="Figure 8.36: The Apriori algorithm results using the complete online retail dataset&#13;&#10;" src="image/C12626_08_36.jpg"/></div><h6>Figure 8.36: The Apriori algorithm results using the complete online retail dataset</h6></li>
				<li>Filter the results down to the item set containing <strong class="inline">10 COLOUR SPACEBOY PEN</strong>. Compare the support value with that under <em class="italics">Exercise 44</em>, <em class="italics">Executing the Apriori algorithm</em>:<p class="snippet">mod_colnames_minsupport[</p><p class="snippet">    mod_colnames_minsupport['itemsets'] == frozenset(</p><p class="snippet">        {'10 COLOUR SPACEBOY PEN'}</p><p class="snippet">    )</p><p class="snippet">]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer468"><img alt="Figure 8.37: Result of item set containing 10 COLOUR SPACEBOY PEN&#13;&#10;" src="image/C12626_08_37.jpg"/></div><h6>Figure 8.37: Result of item set containing 10 COLOUR SPACEBOY PEN</h6><p>The support value does change. When the dataset is expanded to include all transactions, the support for this item set increases from 0.015 to 0.015793. That is, in the reduced dataset used for the exercises, this item set appears in 1.5% of the transactions, while in the full dataset, it appears in approximately 1.6% of transactions.</p></li>
				<li>Add another column containing the item set length. Then, filter down to those item sets whose length is two and whose support is in the range [0.02, 0.021]. Are the item sets the same as those found in <em class="italics">Exercise 44</em>, <em class="italics">Executing the Apriori algorithm,</em> <em class="italics">Step 6</em>?<p class="snippet">mod_colnames_minsupport['length'] = (</p><p class="snippet">    mod_colnames_minsupport['itemsets'].apply(lambda x: len(x))</p><p class="snippet">)</p><p class="snippet">mod_colnames_minsupport[</p><p class="snippet">    (mod_colnames_minsupport['length'] == 2) &amp; </p><p class="snippet">    (mod_colnames_minsupport['support'] &gt;= 0.02) &amp;</p><p class="snippet">    (mod_colnames_minsupport['support'] &lt; 0.021)</p><p class="snippet">]</p><div class="IMG---Figure" id="_idContainer469"><img alt="Figure 8.38: The section of the results of filtering based on length and support&#13;&#10;" src="image/C12626_08_38.jpg"/></div><h6>Figure 8.38: The section of the results of filtering based on length and support</h6><p>The results did change. Before even looking at the particular item sets and their support values, we see that this filtered DataFrame has fewer item sets than the DataFrame in the previous exercise. When we use the full dataset, there are fewer item sets that match the filtering criteria; that is, only 14 item sets contain 2 items and have a support value greater than or equal to 0.02, and less than 0.021. In the previous exercise, 17 item sets met these criteria.</p></li>
				<li>Plot the <strong class="inline">support</strong> values:<p class="snippet">mod_colnames_minsupport.hist("support", grid=False, bins=30)</p><p class="snippet">plt.title("Support")</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer470">
					<img alt="Figure 8.39: The distribution of support values&#13;&#10;" src="image/C12626_08_27.jpg"/>
				</div>
			</div>
			<h6>Figure 8.39: The distribution of support values</h6>
			<p>This plot shows the distribution of support values for the full transaction dataset. As you might have assumed, the distribution is right skewed; that is, most of the item sets have lower support values and there is a long tail of support values on the higher end of the spectrum. Given how many unique item sets exist, it is not surprising that no single item set appears in a high percentage of the transactions. With this information, we could tell management that even the most prominent item set only appears in approximately 10% of the transactions, and that the vast majority of item sets appear in less than 2% of transactions. These results may not support changes in store layout, but could very well inform pricing and discounting strategies. We would gain more information on how to build these strategies by formalizing some association rules.</p>
			<h3 id="_idParaDest-242"><a id="_idTextAnchor267"/>Activity 20: Finding the Association Rules on the Complete Online Retail Dataset</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Fit the association rule model on the full dataset. Use metric confidence and a minimum threshold of 0.6:<p class="snippet">rules = mlxtend.frequent_patterns.association_rules(</p><p class="snippet">    mod_colnames_minsupport, </p><p class="snippet">    metric="confidence",</p><p class="snippet">    min_threshold=0.6, </p><p class="snippet">    support_only=False</p><p class="snippet">)</p><p class="snippet">rules.loc[0:6]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer471"><img alt="Figure 8.40: The association rules based on the complete online retail dataset&#13;&#10;" src="image/C12626_08_40.jpg"/></div><h6>Figure 8.40: The association rules based on the complete online retail dataset</h6></li>
				<li>Count the number of association rules. Is the number different to that found in <em class="italics">Exercise 45</em>, <em class="italics">Deriving Association Rules</em>, <em class="italics">Step 1</em>?<p class="snippet">print("Number of Associations: {}".format(rules.shape[0]))</p><p>There are <strong class="inline">498</strong> association rules.</p></li>
				<li>Plot confidence against support:<p class="snippet">rules.plot.scatter("support", "confidence", alpha=0.5, marker="*")</p><p class="snippet">plt.xlabel("Support")</p><p class="snippet">plt.ylabel("Confidence")</p><p class="snippet">plt.title("Association Rules")</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer472"><img alt="Figure 8.41: The plot of confidence against support&#13;&#10;" src="image/C12626_08_41.jpg"/></div><h6>Figure 8.41: The plot of confidence against support</h6><p>The plot reveals that there are some association rules featuring relatively high support and confidence values for this dataset.</p></li>
				<li>Look at the distributions of lift, leverage, and conviction:<p class="snippet">rules.hist("lift", grid=False, bins=30)</p><p class="snippet">plt.title("Lift")</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer473">
					<img alt="Figure 8.42: The distribution of lift values&#13;&#10;" src="image/C12626_08_42.jpg"/>
				</div>
			</div>
			<h6>Figure 8.42: The distribution of lift values</h6>
			<p class="snippet">rules.hist("leverage", grid=False, bins=30)</p>
			<p class="snippet">plt.title("Leverage")</p>
			<p>The output is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer474">
					<img alt="Figure 8.43: The distribution of leverage values&#13;&#10;" src="image/C12626_08_43.jpg"/>
				</div>
			</div>
			<h6>Figure 8.43: The distribution of leverage values</h6>
			<p class="snippet">plt.hist(</p>
			<p class="snippet">    rules[numpy.isfinite(rules['conviction'])].conviction.values, </p>
			<p class="snippet">    bins = 30</p>
			<p class="snippet">)</p>
			<p class="snippet">plt.title("Conviction")</p>
			<p>The output is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer475">
					<img alt="Figure 8.44: The distribution of conviction values&#13;&#10;" src="image/C12626_08_44.jpg"/>
				</div>
			</div>
			<h6>Figure 8.44: The distribution of conviction values</h6>
			<p>Having derived association rules, we can return to management with additional information, the most important of which would be that there are roughly seven item sets that have reasonably high values for both support and confidence. Look at the scatterplot of confidence against support to see the seven item sets that are separated from all the others. These seven item sets also have high lift values, as can be seen in the lift histogram. It seems that we have identified some actionable association rules, rules that we can use to drive business decisions.</p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor268"/>Chapter 9: Hotspot Analysis</h2>
			<h3 id="_idParaDest-244"><a id="_idTextAnchor269"/>Activity 21: Estimating Density in One Dimension</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Open a new notebook and install all the necessary libraries.<p class="snippet">get_ipython().run_line_magic('matplotlib', 'inline')</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import numpy</p><p class="snippet">import pandas</p><p class="snippet">import seaborn</p><p class="snippet">import sklearn.datasets</p><p class="snippet">import sklearn.model_selection</p><p class="snippet">import sklearn.neighbors</p><p class="snippet">seaborn.set()</p></li>
				<li>Sample 1,000 data points from the standard normal distribution. Add 3.5 to each of the last 625 values of the sample (that is, the indices between 375 and 1,000). To do this, set a random state of 100 using <strong class="inline">numpy.random.RandomState</strong> to guarantee the same sampled values, and then randomly generate the data points using the <strong class="inline">randn(1000)</strong> call:<p class="snippet">rand = numpy.random.RandomState(100)</p><p class="snippet">vals = rand.randn(1000)  # standard normal</p><p class="snippet">vals[375:] += 3.5</p></li>
				<li>Plot the 1,000-point sample data as a histogram and add a scatterplot below it:<p class="snippet">fig, ax = plt.subplots(figsize=(14, 10))</p><p class="snippet">ax.hist(vals, bins=50, density=True, label='Sampled Values')</p><p class="snippet">ax.plot(vals, -0.005 - 0.01 * numpy.random.random(len(vals)), '+k', label='Individual Points')</p><p class="snippet">ax.legend(loc='upper right')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer476"><img alt="Figure 9.29: A histogram of the random sample with a scatterplot underneath&#13;&#10;" src="image/C12626_09_29.jpg"/></div><h6>Figure 9.29: A histogram of the random sample with a scatterplot underneath</h6></li>
				<li>Define a grid of bandwidth values. Then, define and fit a grid search cross-validation algorithm:<p class="snippet">bandwidths = 10 ** numpy.linspace(-1, 1, 100)</p><p class="snippet">grid = sklearn.model_selection.GridSearchCV(</p><p class="snippet">    estimator=sklearn.neighbors.KernelDensity(kernel="gaussian"),</p><p class="snippet">    param_grid={"bandwidth": bandwidths},</p><p class="snippet">    cv=10</p><p class="snippet">)</p><p class="snippet">grid.fit(vals[:, None])</p></li>
				<li>Extract the optimal bandwidth value:<p class="snippet">best_bandwidth = grid.best_params_["bandwidth"]</p><p class="snippet">print(</p><p class="snippet">    "Best Bandwidth Value: {}"</p><p class="snippet">    .format(best_bandwidth)</p><p class="snippet">)</p></li>
				<li>Replot the histogram from <em class="italics">Step 3</em> and overlay the estimated density:<p class="snippet">fig, ax = plt.subplots(figsize=(14, 10))</p><p class="snippet">ax.hist(vals, bins=50, density=True, alpha=0.75, label='Sampled Values')</p><p class="snippet">x_vec = numpy.linspace(-4, 8, 10000)[:, numpy.newaxis]</p><p class="snippet">log_density = numpy.exp(grid.best_estimator_.score_samples(x_vec))</p><p class="snippet">ax.plot(</p><p class="snippet">     x_vec[:, 0], log_density, </p><p class="snippet">     '-', linewidth=4, label='Kernel = Gaussian'</p><p class="snippet">)</p><p class="snippet">ax.legend(loc='upper right')</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer477">
					<img alt="Figure 9.30: A histogram of the random sample with the optimal estimated density overlaid&#13;&#10;" src="image/C12626_09_30.jpg"/>
				</div>
			</div>
			<h6>Figure 9.30: A histogram of the random sample with the optimal estimated density overlaid</h6>
			<h3 id="_idParaDest-245"><a id="_idTextAnchor270"/>Activity 22: Analyzing Crime in London</h3>
			<p>Solution:</p>
			<ol>
				<li value="1">Load the crime data. Use the path where you saved the downloaded directory, create a list of the year-month tags, use the <strong class="inline">read_csv</strong> command to load the individual files iteratively, and then concatenate these files together:<p class="snippet">base_path = (</p><p class="snippet">    "~/Documents/packt/unsupervised-learning-python/"</p><p class="snippet">    "lesson-9-hotspot-models/metro-jul18-dec18/"</p><p class="snippet">    "{yr_mon}/{yr_mon}-metropolitan-street.csv"</p><p class="snippet">)</p><p class="snippet">print(base_path)</p><p class="snippet">yearmon_list = [</p><p class="snippet">    "2018-0" + str(i) if i &lt;= 9 else "2018-" + str(i) </p><p class="snippet">    for i in range(7, 13)</p><p class="snippet">]</p><p class="snippet">print(yearmon_list)</p><p class="snippet">data_yearmon_list = []</p><p class="snippet">for idx, i in enumerate(yearmon_list):</p><p class="snippet">    df = pandas.read_csv(</p><p class="snippet">        base_path.format(yr_mon=i), </p><p class="snippet">        header=0</p><p class="snippet">    )</p><p class="snippet">    </p><p class="snippet">    data_yearmon_list.append(df)</p><p class="snippet">    </p><p class="snippet">    if idx == 0:</p><p class="snippet">        print("Month: {}".format(i))</p><p class="snippet">        print("Dimensions: {}".format(df.shape))</p><p class="snippet">        print("Head:\n{}\n".format(df.head(2)))</p><p class="snippet">london = pandas.concat(data_yearmon_list)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer478"><img alt="Figure 9.31: An example of one of the individual crime files&#13;&#10;" src="image/C12626_09_31.jpg"/></div><h6>Figure 9.31: An example of one of the individual crime files</h6><p>This printed information is just for the first of the loaded files, which will be the criminal information from the Metropolitan Police Service for July 2018. This one file has nearly 100,000 entries. You will notice that there is a great deal of interesting information in this dataset, but we will focus on <strong class="inline">Longitude</strong>, <strong class="inline">Latitude</strong>, <strong class="inline">Month</strong>, and <strong class="inline">Crime type</strong>.</p></li>
				<li>Print diagnostics of the complete (six months) and concatenated dataset:<p class="snippet">print(</p><p class="snippet">    "Dimensions - Full Data:\n{}\n"</p><p class="snippet">    .format(london.shape)</p><p class="snippet">)</p><p class="snippet">print(</p><p class="snippet">    "Unique Months - Full Data:\n{}\n"</p><p class="snippet">    .format(london["Month"].unique())</p><p class="snippet">)</p><p class="snippet">print(</p><p class="snippet">    "Number of Unique Crime Types - Full Data:\n{}\n"</p><p class="snippet">    .format(london["Crime type"].nunique())</p><p class="snippet">)</p><p class="snippet">print(</p><p class="snippet">    "Unique Crime Types - Full Data:\n{}\n"</p><p class="snippet">    .format(london["Crime type"].unique())</p><p class="snippet">)</p><p class="snippet">print(</p><p class="snippet">    "Count Occurrences Of Each Unique Crime Type - Full Type:\n{}\n"</p><p class="snippet">    .format(london["Crime type"].value_counts())</p><p class="snippet">)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer479"><img alt="Figure 9.32: Descriptors of the full crime dataset&#13;&#10;" src="image/C12626_09_32.jpg"/></div><h6>Figure 9.32: Descriptors of the full crime dataset</h6></li>
				<li>Subset the DataFrame down to four variables (<strong class="inline">Longitude</strong>, <strong class="inline">Latitude</strong>, <strong class="inline">Month</strong>, and <strong class="inline">Crime type</strong>):<p class="snippet">london_subset = london[["Month", "Longitude", "Latitude", "Crime type"]]</p><p class="snippet">london_subset.head(5)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer480"><img alt="Figure 9.33: Crime data in DataFrame form subset down to the Longitude, Latitude, Month, and Crime type columns&#13;&#10;" src="image/C12626_09_33.jpg"/></div><h6>Figure 9.33: Crime data in DataFrame form subset down to the Longitude, Latitude, Month, and Crime type columns</h6></li>
				<li>Using the <strong class="inline">jointplot</strong> function from <strong class="inline">seaborn</strong>, fit and visualize three kernel density estimation models for bicycle theft in July, September, and December 2018:<p class="snippet">crime_bicycle_jul = london_subset[</p><p class="snippet">    (london_subset["Crime type"] == "Bicycle theft") &amp; </p><p class="snippet">    (london_subset["Month"] == "2018-07")</p><p class="snippet">]</p><p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_bicycle_jul, kind="kde")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer481"><img alt="Figure 9.34: The estimated joint and marginal densities for bicycle thefts in July 2018&#13;&#10;" src="image/C12626_09_34.jpg"/></div><h6>Figure 9.34: The estimated joint and marginal densities for bicycle thefts in July 2018</h6><p class="snippet">crime_bicycle_sept = london_subset[</p><p class="snippet">    (london_subset["Crime type"] == "Bicycle theft") &amp; </p><p class="snippet">    (london_subset["Month"] == "2018-09")</p><p class="snippet">]</p><p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_bicycle_sept, kind="kde")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer482"><img alt="Figure 9.35: The estimated joint and marginal densities for bicycle thefts in September 2018&#13;&#10;" src="image/C12626_09_35.jpg"/></div><h6>Figure 9.35: The estimated joint and marginal densities for bicycle thefts in September 2018</h6><p class="snippet">crime_bicycle_dec = london_subset[</p><p class="snippet">    (london_subset["Crime type"] == "Bicycle theft") &amp; </p><p class="snippet">    (london_subset["Month"] == "2018-12")</p><p class="snippet">]</p><p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_bicycle_dec, kind="kde")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer483"><img alt="Figure 9.36: The estimated joint and marginal densities for bicycle thefts in December 2018&#13;&#10;" src="image/C12626_09_36.jpg"/></div><h6>Figure 9.36: The estimated joint and marginal densities for bicycle thefts in December 2018</h6><p>From month to month, the density of bicycle thefts stays quite constant. There are slight differences between the densities, which is to be expected given that the data that is the foundation of these estimated densities is three one-month samples. Given these results, police or criminologists should be confident in predicting where future bicycle thefts are most likely to occur.</p></li>
				<li>Repeat <em class="italics">Step 4</em>; this time, use shoplifting crimes for the months of August, October, and November 2018:<p class="snippet">crime_shoplift_aug = london_subset[</p><p class="snippet">    (london_subset["Crime type"] == "Shoplifting") &amp; </p><p class="snippet">    (london_subset["Month"] == "2018-08")</p><p class="snippet">]</p><p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_shoplift_aug, kind="kde")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer484"><img alt="Figure 9.37: The estimated joint and marginal densities for shoplifting incidents in August 2018&#13;&#10;" src="image/C12626_09_37.jpg"/></div><h6>Figure 9.37: The estimated joint and marginal densities for shoplifting incidents in August 2018</h6><p class="snippet">crime_shoplift_oct = london_subset[</p><p class="snippet">    (london_subset["Crime type"] == "Shoplifting") &amp; </p><p class="snippet">    (london_subset["Month"] == "2018-10")</p><p class="snippet">]</p><p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_shoplift_oct, kind="kde")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer485"><img alt="Figure 9.38: The estimated joint and marginal densities for shoplifting incidents in October 2018&#13;&#10;" src="image/C12626_09_38.jpg"/></div><h6>Figure 9.38: The estimated joint and marginal densities for shoplifting incidents in October 2018</h6><p class="snippet">crime_shoplift_nov = london_subset[</p><p class="snippet">    (london_subset["Crime type"] == "Shoplifting") &amp; </p><p class="snippet">    (london_subset["Month"] == "2018-11")</p><p class="snippet">]</p><p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_shoplift_nov, kind="kde")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer486"><img alt="Figure 9.39: The estimated joint and marginal densities for shoplifting incidents in November 2018&#13;&#10;" src="image/C12626_09_39.jpg"/></div><h6>Figure 9.39: The estimated joint and marginal densities for shoplifting incidents in November 2018</h6><p>Like the bicycle theft results, the shoplifting densities are quite stable across the months. The density from August 2018 looks different from the other two months; however, if you look at the longitude and latitude values, you will notice that the density is very similar, but it has just shifted and scaled. The reason for this is that there were probably a number of outliers forcing the creation of a much larger plotting region.</p></li>
				<li>Repeat <em class="italics">Step 5</em>; this time use burglary crimes for the months of July, October, and December 2018:<p class="snippet">crime_burglary_jul = london_subset[</p><p class="snippet">    (london_subset["Crime type"] == "Burglary") &amp; </p><p class="snippet">    (london_subset["Month"] == "2018-07")</p><p class="snippet">]</p><p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_burglary_jul, kind="kde")</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer487">
					<img alt="Figure 9.40: The estimated joint and marginal densities for burglaries in July 2018&#13;&#10;" src="image/C12626_09_40.jpg"/>
				</div>
			</div>
			<h6>Figure 9.40: The estimated joint and marginal densities for burglaries in July 2018</h6>
			<p class="snippet">crime_burglary_oct = london_subset[</p>
			<p class="snippet">    (london_subset["Crime type"] == "Burglary") &amp; </p>
			<p class="snippet">    (london_subset["Month"] == "2018-10")</p>
			<p class="snippet">]</p>
			<p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_burglary_oct, kind="kde")</p>
			<p>The output is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer488">
					<img alt="Figure 9.41: The estimated joint and marginal densities for burglaries in October 2018&#13;&#10;" src="image/C12626_09_41.jpg"/>
				</div>
			</div>
			<h6>Figure 9.41: The estimated joint and marginal densities for burglaries in October 2018</h6>
			<p class="snippet">crime_burglary_dec = london_subset[</p>
			<p class="snippet">    (london_subset["Crime type"] == "Burglary") &amp; </p>
			<p class="snippet">    (london_subset["Month"] == "2018-12")</p>
			<p class="snippet">]</p>
			<p class="snippet">seaborn.jointplot("Longitude", "Latitude", crime_burglary_dec, kind="kde")</p>
			<p>The output is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer489">
					<img alt="Figure 9.42: The estimated joint and marginal densities for burglaries in December 2018&#13;&#10;" src="image/C12626_09_42.jpg"/>
				</div>
			</div>
			<h6>Figure 9.42: The estimated joint and marginal densities for burglaries in December 2018</h6>
			<p>Once again, we can see that the distributions are quite similar across the months. The only difference is that the densities seem to widen or spread from July to December. As always, the noise and inherent lack of information contained in the sample data is causing small shifts in the estimated densities.</p>
		</div>
	</body></html>
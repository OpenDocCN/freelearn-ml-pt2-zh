["```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> vectorizer = CountVectorizer(min_df=1)\n\n```", "```py\n>>> print(vectorizer)CountVectorizer(analyzer='word', binary=False, charset=None,\n charset_error=None, decode_error='strict',\n dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n lowercase=True, max_df=1.0, max_features=None, min_df=1,\n ngram_range=(1, 1), preprocessor=None, stop_words=None,\n strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n tokenizer=None, vocabulary=None)\n\n```", "```py\n>>> content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n\n```", "```py\n>>> X = vectorizer.fit_transform(content)\n>>> vectorizer.get_feature_names()[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']\n\n```", "```py\n>>> print(X.toarray().transpose())\n[[1 1]\n [1 1]\n [1 1]\n [1 0]\n [1 0]\n [0 1]\n [1 0]]\n\n```", "```py\n>>> posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> vectorizer = CountVectorizer(min_df=1)\n\n```", "```py\n>>> X_train = vectorizer.fit_transform(posts)\n>>> num_samples, num_features = X_train.shape\n>>> print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n#samples: 5, #features: 25\n\n```", "```py\n>>> print(vectorizer.get_feature_names())\n[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'this', u'toy']\n\n```", "```py\n>>> new_post = \"imaging databases\"\n>>> new_post_vec = vectorizer.transform([new_post])\n\n```", "```py\n>>> print(new_post_vec)\n (0, 7)  1\n (0, 5)  1\n\n```", "```py\n>>> print(new_post_vec.toarray())\n[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n\n```", "```py\n>>> import scipy as sp\n>>> def dist_raw(v1, v2):\n...     delta = v1-v2\n...     return sp.linalg.norm(delta.toarray())\n\n```", "```py\n>>> import sys\n>>> best_doc = None\n>>> best_dist = sys.maxint\n>>> best_i = None\n>>> for i, post in enumerate(num_samples):\n...     if post == new_post:\n...         continue\n...     post_vec = X_train.getrow(i)\n...     d = dist_raw(post_vec, new_post_vec)\n...     print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n...     if d<best_dist:\n...         best_dist = d\n...         best_i = i\n>>> print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n\n=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n=== Post 3 with dist=1.41: Imaging databases store data.\n=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\nBest post is 3 with dist=1.41\n\n```", "```py\n>>> print(X_train.getrow(3).toarray())\n[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n>>> print(X_train.getrow(4).toarray())\n[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n\n```", "```py\n>>> def dist_norm(v1, v2):\n...    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n...    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n...    delta = v1_normalized - v2_normalized\n...    return sp.linalg.norm(delta.toarray())\n\n```", "```py\n=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n=== Post 3 with dist=0.77: Imaging databases store data.\n=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\nBest post is 3 with dist=0.77\n\n```", "```py\n>>> vectorizer = CountVectorizer(min_df=1, stop_words='english')\n\n```", "```py\n>>> sorted(vectorizer.get_stop_words())[0:20]\n['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n\n```", "```py\n[u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'learning', u'machine', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'toy']\n\n```", "```py\n=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n=== Post 3 with dist=0.77: Imaging databases store data.\n=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\nBest post is 3 with dist=0.77\n\n```", "```py\n>>> import nltk\n\n```", "```py\n>>> import nltk.stem\n>>> s = nltk.stem.SnowballStemmer('english')\n>>> s.stem(\"graphics\")\nu'graphic'\n>>> s.stem(\"imaging\")\nu'imag'\n>>> s.stem(\"image\")\nu'imag'\n>>> s.stem(\"imagination\")\nu'imagin'\n>>> s.stem(\"imagine\")\nu'imagin'\n\n```", "```py\n>>> s.stem(\"buys\")\nu'buy'\n>>> s.stem(\"buying\")\nu'buy'\n\n```", "```py\n>>> s.stem(\"bought\")\nu'bought'\n\n```", "```py\n>>> import nltk.stem\n>>> english_stemmer = nltk.stem.SnowballStemmer('english'))\n>>> class StemmedCountVectorizer(CountVectorizer):\n...     def build_analyzer(self):\n...         analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n...         return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n>>> vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n\n```", "```py\n[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'save', u'storag', u'store', u'stuff', u'toy']\n\n```", "```py\n=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n\n```", "```py\n=== Post 2 with dist=0.63: Most imaging databases save images permanently.\n=== Post 3 with dist=0.77: Imaging databases store data.\n=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\nBest post is 2 with dist=0.63\n\n```", "```py\n>>> import scipy as sp\n>>> def tfidf(term, doc, corpus):\n...     tf = doc.count(term) / len(doc)\n...     num_docs_with_term = len([d for d in corpus if term in d])\n...     idf = sp.log(len(corpus) / num_docs_with_term)\n...     return tf * idf\n\n```", "```py\n>>> a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n>>> D = [a, abb, abc]\n>>> print(tfidf(\"a\", a, D))\n0.0\n>>> print(tfidf(\"a\", abb, D))\n0.0\n>>> print(tfidf(\"a\", abc, D))\n0.0\n>>> print(tfidf(\"b\", abb, D))\n0.270310072072\n>>> print(tfidf(\"a\", abc, D))\n0.0\n>>> print(tfidf(\"b\", abc, D))\n0.135155036036\n>>> print(tfidf(\"c\", abc, D))\n0.366204096223\n\n```", "```py\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> class StemmedTfidfVectorizer(TfidfVectorizer):\n...     def build_analyzer(self):\n...         analyzer = super(TfidfVectorizer,\n self).build_analyzer()\n...         return lambda doc: (\n english_stemmer.stem(w) for w in analyzer(doc))\n>>> vectorizer = StemmedTfidfVectorizer(min_df=1,\n stop_words='english', decode_error='ignore')\n\n```", "```py\n>>> import sklearn.datasets\n>>> all_data = sklearn.datasets.fetch_20newsgroups(subset='all')\n>>> print(len(all_data.filenames))\n18846\n>>> print(all_data.target_names)\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n\n```", "```py\n>>> train_data = sklearn.datasets.fetch_20newsgroups(subset='train', categories=groups)\n>>> print(len(train_data.filenames))\n11314\n>>> test_data = sklearn.datasets.fetch_20newsgroups(subset='test')\n>>> print(len(test_data.filenames))\n7532\n\n```", "```py\n>>> groups = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n>>> train_data = sklearn.datasets.fetch_20newsgroups(subset='train', categories=groups)\n>>> print(len(train_data.filenames))\n3529\n\n>>> test_data = sklearn.datasets.fetch_20newsgroups(subset='test', categories=groups)\n>>> print(len(test_data.filenames))\n2349\n\n```", "```py\n>>> vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n...              stop_words='english', decode_error='ignore')\n>>> vectorized = vectorizer.fit_transform(train_data.data)\n>>> num_samples, num_features = vectorized.shape\n>>> print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n#samples: 3529, #features: 4712\n\n```", "```py\n>>> num_clusters = 50\n>>> from sklearn.cluster import KMeans\n>>> km = KMeans(n_clusters=num_clusters, init='random', n_init=1,\nverbose=1, random_state=3)\n>>> km.fit(vectorized)\n\n```", "```py\n>>> print(km.labels_)\n[48 23 31 ...,  6  2 22]\n>>> print(km.labels_.shape)\n3529\n\n```", "```py\n>>> new_post_vec = vectorizer.transform([new_post])\n>>> new_post_label = km.predict(new_post_vec)[0]\n\n```", "```py\n>>> similar_indices = (km.labels_==new_post_label).nonzero()[0]\n\n```", "```py\n>>> similar = []\n>>> for i in similar_indices:\n...    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n...    similar.append((dist, dataset.data[i]))\n>>> similar = sorted(similar)\n>>> print(len(similar))\n131\n\n```", "```py\n>>> show_at_1 = similar[0]\n>>> show_at_2 = similar[int(len(similar)/10)]\n>>> show_at_3 = similar[int(len(similar)/2)]\n\n```", "```py\n>>> post_group = zip(train_data.data, train_data.target)\n>>> all = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]\n>>> graphics = sorted([post for post in all if post[2]=='comp.graphics'])\n>>> print(graphics[5])\n(245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization: The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk<…snip…>', 'comp.graphics')\n\n```", "```py\n>>> noise_post = graphics[5][1]\n>>> analyzer = vectorizer.build_analyzer()\n>>> print(list(analyzer(noise_post)))\n['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test', 'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line', 'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']\n\n```", "```py\n>>> useful = set(analyzer(noise_post)).intersection(vectorizer.get_feature_names())\n>>> print(sorted(useful))\n['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']\n\n```", "```py\n>>> for term in sorted(useful):\n...     print('IDF(%s)=%.2f'%(term, vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))\nIDF(ac)=3.51\nIDF(birmingham)=6.77\nIDF(host)=1.74\nIDF(kingdom)=6.68\nIDF(nntp)=1.77\nIDF(sorri)=4.14\nIDF(test)=3.83\nIDF(uk)=3.70\nIDF(unit)=4.42\nIDF(univers)=1.91\n\n```"]
<html><head></head><body>
<section id="chapter-7-mixture-models" class="level2 chapterHead" data-number="1.11">&#13;
<h1 class="chapterHead" data-number="1.11">Chapter 7<br/>&#13;
<span id="x1-1370007"/>Mixture Models</h1>&#13;
<blockquote>&#13;
<p>...the father has the form of a lion, the mother of an ant; the father eats flesh and the mother herbs. And these breed the ant-lion... –The Book of Imaginary Beings</p>&#13;
</blockquote>&#13;
<p>The River Plate (also known as La Plata River or Río de la Plata) is the widest river on Earth and a natural border between Argentina and Uruguay. During the late 19<sup>th</sup> century, the port area along this river was a place where indigenous people mixed with Africans (most of them slaves) and European immigrants. One consequence of this encounter was the mix of European music, such as the waltz and mazurka, with the African candombe and Argentinian milonga (which, in turn, is a mix of Afro-American rhythms), giving origin to the dance and music we now call the tango.</p>&#13;
<p>Mixing previously existing elements is a great way to create new things, not only in the context of music. In statistics, mixture models are one common approach to model building. These models are built by mixing simpler distributions to obtain more complex ones. For example, we can combine two Gaussians to describe a bimodal distribution or many Gaussians to describe arbitrary distributions. While using Gaussians is very common, in principle we can mix any family of distributions we want. Mixture models are used for different purposes, such as directly modeling sub-populations or as a useful trick for handling complicated distributions that cannot be described with simpler distributions.</p>&#13;
<p>In this chapter, we will cover the following topics:</p>&#13;
<ul>&#13;
<li><p>Finite mixture models</p></li>&#13;
<li><p>Zero-inflated and hurdle models</p></li>&#13;
<li><p>Infinite mixture models</p></li>&#13;
<li><p>Continuous mixture models</p></li>&#13;
</ul>&#13;
<p><span id="x1-137001r331"/></p>&#13;
<section id="understanding-mixture-models" class="level3 sectionHead" data-number="1.11.1">&#13;
<h2 class="sectionHead" data-number="1.11.1">7.1 <span id="x1-1380001"/>Understanding mixture models</h2>&#13;
<p><span id="dx1-138001"/></p>&#13;
<p>Mixture models naturally arise when the overall population is a combination of distinct sub-populations. A familiar example is the distribution of heights in a given adult human population, which can be described as a mixture of female and male sub-populations. Another classical example is the clustering of handwritten digits. In this case, it is very reasonable to expect 10 sub-populations, at least in a base 10 system! If we know to which sub-population each observation belongs, it is generally a good idea to use that information to model each sub-population as a separate group. However, when we do not have direct access to this information, mixture models come in handy.</p>&#13;
<div id="tcolobox-14" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Blends of Distributions</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>Many datasets cannot be properly described using a single probability distribution, but they can be described as a mixture of such distributions. Models that assume data comes from a mixture of distributions are known as mixture models.</p>&#13;
</div>&#13;
</div>&#13;
<p>When building a mixture model, it is not necessary to believe we are describing true sub-populations in the data. Mixture models can also be used as a statistical trick to add flexibility to our toolbox. Take, for example, the Gaussian distribution. We can use it as a reasonable approximation for many unimodal and approximately symmetrical distributions. But what about multimodal or skewed distributions? Can we use Gaussian distributions to model them? Yes, we can, if we use a mixture of Gaussians.</p>&#13;
<p>In a Gaussian mixture model, each component will be a Gaussian with a different mean and, generally (but not necessarily), a different standard deviation. By combining Gaussians, we can add flexibility to our models and fit complex data distributions. In fact, we can approximate practically any distribution we want by using a proper combination of Gaussians. The exact number of distributions will depend on the accuracy of the approximation and the details of the data. Actually, we have been using this idea in many of the plots throughout this book. The Kernel Density Estimation (KDE) technique is a non-Bayesian implementation of this idea. Conceptually, when we call <code>az.plot_kde</code>, the function places a Gaussian, with a fixed variance, on top of each data point and then sums all the individual Gaussians to approximate the empirical distribution of the data. <em>Figure <a href="#x1-138002r1">7.1</a></em> shows an example of how we can mix 8 Gaussians to represent a complex distribution, like a boa constrictor digesting an elephant, or a hat, depending on your perspective.</p>&#13;
<p>In <em>Figure <a href="#x1-138002r1">7.1</a></em>, all Gaussians have the same variance and they are centered at the gray dots, which represent sample points from a possible unknown population. If you look carefully, you may notice that two of the Gaussians are on top of each other.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file196.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-138002r1"/><strong>Figure 7.1</strong>: Example of a KDE as a Gaussian mixture model</p>&#13;
<p>Whether we believe in sub-populations or use them for mathematical convenience (or even something in the middle), mixture models are a useful way of adding flexibility to our models by using a mixture of distributions to describe the data. <span id="x1-138003r334"/></p>&#13;
</section>&#13;
<section id="finite-mixture-models" class="level3 sectionHead" data-number="1.11.2">&#13;
<h2 class="sectionHead" data-number="1.11.2">7.2 <span id="x1-1390002"/>Finite mixture models</h2>&#13;
<p><span id="dx1-139001"/></p>&#13;
<p>One way to build mixture models is to consider a finite weighted mixture of two or more distributions. Then the probability density of the observed data is a weighted sum of the probability density of <em>K</em> subgroups:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file197.jpg" class="math-display" alt=" ∑K p(y) = wip (y | θi) i=1 "/>&#13;
</div>&#13;
<p>We can interpret <em>w</em><sub><em>i</em></sub> as the probability of the component <em>i</em>, and thus its values are restricted to the interval [0, 1] and they need to sum up to 1. The components <em>p</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>θ</em><sub><em>i</em></sub>) are usually simple distributions, such as a Gaussian or a Poisson. If <em>K</em> is finite, we have a finite mixture model. To fit such a model, we need to provide a value of <em>K</em>, either because we know the correct value beforehand or because we can make an educated guess.</p>&#13;
<p>Conceptually, to solve a mixture model, all we need to do is properly assign each data point to one of the components. In a probabilistic model, we can do this by introducing a random variable, whose function is to specify to which component a particular observation is assigned. This variable is generally referred to as a <span id="dx1-139002"/><strong>latent variable</strong> because we cannot directly observe it.</p>&#13;
<p>For a mixture of only two components (<em>K</em> = 2), we can use the coin-flipping problem model as a building block. For that model, we have two possible outcomes and we use the Bernoulli distribution to describe them. Since we do not know the probability of getting heads or tails, we use a Beta distribution as a prior distribution. If instead of head or tails, we think of any two groups (or components or classes), we can assign the observation to one group if we get 0 and to the other if we get 1. This is all very nice, but in a mixture model, we can have two or more groups, so we need to make this idea more general. We can do it by noticing that the generalization of the Bernoulli distribution to <em>K</em> outcomes is the Categorical distribution and the generalization of the Beta distribution to higher dimensions is the Dirichlet distribution. If the components we are assigning are Gaussians, like in <em>Figure <a href="#x1-138002r1">7.1</a></em>, then <em>Figure <a href="#x1-139003r2">7.2</a></em> shows a Kruschke-style diagram of such a model. The rounded-corner box indicates that we have <em>K</em> components and the Categorical variables decide which of them we use to describe a given data point. Notice that only <em>μ</em><sub><em>k</em></sub> depends on the different components, while <em>σ</em><sub><em>μ</em></sub> and <em>σ</em><sub><em>σ</em></sub> are shared for all of them. This is just a modeling choice; if necessary, we can change it and allow other parameters to be conditioned on each component.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file198.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-139003r2"/><strong>Figure 7.2</strong>: Kruschke-style diagram of a mixture model with Gaussian components</p>&#13;
<p>We are going to implement this model in PyMC, but before doing that, let me introduce the Categorical and Dirichlet distributions. If you are already familiar with these distributions, you can skip the next two sections and jump to the example. <span id="x1-139004r321"/></p>&#13;
<section id="the-categorical-distribution" class="level4 subsectionHead" data-number="1.11.2.1">&#13;
<h3 class="subsectionHead" data-number="1.11.2.1">7.2.1 <span id="x1-1400001"/>The Categorical distribution</h3>&#13;
<p><span id="dx1-140001"/> <span id="dx1-140002"/></p>&#13;
<p>The Categorical distribution is the most general discrete distribution and is parameterized by a vector where each element specifies the probabilities of each possible outcome. <em>Figure <a href="#x1-140003r3">7.3</a></em> represents two possible instances of the Categorical distribution. The dots represent the values of the Categorical distribution, while the continuous dashed lines are a visual aid to help us easily grasp the <em>shape</em> of the distribution:</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file199.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-140003r3"/><strong>Figure 7.3</strong>: Two members of the Categorical distribution family. The dashed lines are just a visual aid.</p>&#13;
<p><span id="x1-140004r336"/></p>&#13;
</section>&#13;
<section id="the-dirichlet-distribution" class="level4 subsectionHead" data-number="1.11.2.2">&#13;
<h3 class="subsectionHead" data-number="1.11.2.2">7.2.2 <span id="x1-1410002"/>The Dirichlet distribution</h3>&#13;
<p><span id="dx1-141001"/> <span id="dx1-141002"/></p>&#13;
<p>The Dirichlet distribution lives in the simplex, which you can think of as an n-dimensional triangle; a 1-simplex is a line, a 2-simplex is a triangle, a 3-simplex is a tetrahedron, and so on. Why a simplex? Intuitively, because the output of this distribution is a <em>K</em>-length vector, whose elements are restricted to be on the interval [0<em>,</em>1] and sum up to 1. As we said, the Dirichlet distribution is the generalization of the Beta distribution. Thus, a good way to understand the former is to compare it to the latter. We use the Beta for problems with two outcomes: one with probability <em>p</em> and the other 1 <span class="cmsy-10x-x-109">− </span><em>p</em>. As we can see, <em>p</em> + (1 <span class="cmsy-10x-x-109">− </span><em>p</em>) = 1. The Beta returns a two-element vector, (<em>p,q</em> = 1 <span class="cmsy-10x-x-109">− </span><em>p</em>), but in practice, we omit <em>q</em> as the outcome is entirely determined once we know the value of <em>p</em>. If we want to extend the Beta distribution to three outcomes, we need a three-element vector (<em>p,q,r</em>), where each element is in the interval [0<em>,</em>1] and their values sum up to 1. Similar to the Beta distribution, we could use three scalars to parameterize such a distribution, and we may call them <em>α</em>, <em>β</em>, and <em>γ</em>; however, we could easily run out of Greek letters as there are only 24 of them. Instead, we can just use a vector named <em>α</em> of length <em>K</em>. Note that we can think of the Beta and Dirichlet as distributions over proportions. <em>Figure <a href="#x1-141003r4">7.4</a></em> shows 4 members of the Dirichlet distribution when <em>k</em> = 3. On the top, we have the pdf and on the bottom, we have samples from the distribution.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file200.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-141003r4"/><strong>Figure 7.4</strong>: Four members of the Dirichlet distribution family</p>&#13;
<p>Okay, now we have all the <em>components</em> to implement our first mixture model. <span id="x1-141004r337"/></p>&#13;
</section>&#13;
<section id="chemical-mixture" class="level4 subsectionHead" data-number="1.11.2.3">&#13;
<h3 class="subsectionHead" data-number="1.11.2.3">7.2.3 <span id="x1-1420003"/>Chemical mixture</h3>&#13;
<p><span id="dx1-142001"/></p>&#13;
<p>To keep things very concrete, let’s work on an example. We are going to use the chemical shifts data we already saw in <em>Chapter <a href="CH02.xhtml#x1-440002">2</a></em>. <em>Figure <a href="#x1-142002r5">7.5</a></em> shows a histogram of this data.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file201.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-142002r5"/><strong>Figure 7.5</strong>: Histogram for the chemical shifts data</p>&#13;
<p>We can see that this data cannot be properly described using a single distribution like a Gaussian, but we can do it if we use many as we already showed in <em>Figure <a href="#x1-138002r1">7.1</a></em>; maybe three or four would do the trick. There are good theoretical reasons, which we will ignore and not discuss here, indicating this chemical shifts data comes from a mixture of 40 sub-populations. But just by looking at the data, it seems impossible to recover the true groups as there is a lot of overlap between them.</p>&#13;
<p>The following code block shows the implementation of a Gaussian mixture model of two components in PyMC:</p>&#13;
<p><span id="x1-142003r1"/> <span id="x1-142004"/><strong>Code 7.1</strong></p>&#13;
<pre id="listing-94" class="source-code"><code>K = 2 </code>&#13;
<code>with pm.Model() as model_kg: </code>&#13;
<code>   p = pm.Dirichlet('p', a=np.ones(K)) </code>&#13;
<code>   z = pm.Categorical('z', p=p, shape=len(cs_exp)) </code>&#13;
<code>   means = pm.Normal('means', mu=cs_exp.mean(), sigma=10, shape=K) </code>&#13;
<code>   sd = pm.HalfNormal('sd', sigma=10) </code>&#13;
<code> </code>&#13;
<code>   y = pm.Normal('y', mu=means[z], sigma=sd, observed=cs_exp) </code>&#13;
<code>   idata_kg = pm.sample()</code></pre>&#13;
<p>If you run this code, you will find that it is very slow and the trace looks very bad (refer to <em>Chapter <a href="CH10.xhtml#x1-18900010">10</a></em> to learn more about diagnostics). Can we make this model run faster? Yes, let’s see how.</p>&#13;
<p>In <code>model_kg</code>, we have explicitly included the latent variable <em>z</em> in the model. Sampling this discrete variable usually leads to poor posterior sampling. One way to solve this issue is to reparametrize the model so <em>z</em> is no longer explicitly a part of the model. This type of reparametrization is called marginalization <span id="dx1-142014"/>. Marginalizing out discrete variables usually provides speed-up and better sampling. Unfortunately, it requires some mathematical skills that not everyone has. Luckily for us, we don’t need to do this ourselves as PyMC includes a <code>NormalMixture </code>distribution. So, we can write the mixture model as follows:</p>&#13;
<p><span id="x1-142015r2"/> <span id="x1-142016"/><strong>Code 7.2</strong></p>&#13;
<pre id="listing-95" class="source-code"><code>with pm.Model() as model_mg: </code>&#13;
<code>    p = pm.Dirichlet('p', a=np.ones(K)) </code>&#13;
<code>    means = pm.Normal('means', mu=cs_exp.mean(), sigma=10, shape=K) </code>&#13;
<code>    sd = pm.HalfNormal('sd', sigma=5) </code>&#13;
<code>    y = pm.NormalMixture('y', w=p, mu=means, sigma=sd, observed=cs_exp) </code>&#13;
<code>    idata_mg = pm.sample()</code></pre>&#13;
<p>Let’s check the results with a forest plot. <em>Figure <a href="#x1-142023r6">7.6</a></em> shows something really funny going on. Before moving on to the next section, take some time to think about it. Can you spot it? We will discuss it in the next section.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file202.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-142023r6"/><strong>Figure 7.6</strong>: Forest plot of the means from <code>model_mg</code></p>&#13;
<p><span id="x1-142024r335"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="the-non-identifiability-of-mixture-models" class="level3 sectionHead" data-number="1.11.3">&#13;
<h2 class="sectionHead" data-number="1.11.3">7.3 <span id="x1-1430003"/>The non-identifiability of mixture models</h2>&#13;
<p><span id="dx1-143001"/> <span id="dx1-143002"/></p>&#13;
<p>The <code>means </code>parameter has shape 2, and from <em>Figure <a href="#x1-142023r6">7.6</a></em> we can see that one of its values is around 47 and the other is close to 57.5. The funny thing is that we have one chain saying that <code>means[0] </code>is 47 and the other 3 saying it is 57.5, and the opposite for <code>mmeans[1]</code>. Thus, if we compute the mean of <code>mmeans[0]</code>, we will get some value close to 55 (57<em>.</em>5 <span class="cmsy-10x-x-109">× </span>3 + 47 <span class="cmsy-10x-x-109">× </span>1), which is not the correct value. What we are seeing is an example of a phenomenon known as parameter non-identifiability. This happens because, from the perspective of the model, there is no difference if component 1 has a mean of 47 and component 2 has a mean of 57.5 or vice versa; both scenarios are equivalent. In the context of mixture models, this is also known as the label-switching problem.</p>&#13;
<div id="tcolobox-15" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Non-Identifiability</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>A statistical model is non-identifiable if one or more of its parameters cannot be uniquely determined. Parameters in a model are not identified if the same likelihood function is obtained for more than one choice of the model parameters. It could happen that the data does not contain enough information to estimate the parameters. In other cases, parameters may not be identifiable because the model is not structurally identifiable, meaning that the parameters cannot be uniquely determined even if all the necessary data is available.</p>&#13;
</div>&#13;
</div>&#13;
<p>With mixture models, there are at least two ways of parameterizing a model to remove the non-identifiability issue. We can force the components to be ordered; for example, arrange the means of the components in strictly increasing order and/or use informative priors.</p>&#13;
<p>Using PyMC, we can implement the first option with a transformation as in the next code block. Notice that we also provide initial values for the means; anything to ensure that the first mean is smaller than the second one will work.</p>&#13;
<p><span id="x1-143003r3"/> <span id="x1-143004"/><strong>Code 7.3</strong></p>&#13;
<pre id="listing-96" class="source-code"><code>with pm.Model() as model_mgo: </code>&#13;
<code>    p = pm.Dirichlet('p', a=np.ones(K)) </code>&#13;
<code>    means = pm.Normal('means', mu=cs_exp.mean(), sigma=10, shape=K, </code>&#13;
<code>                      transform=pm.distributions.transforms.ordered, </code>&#13;
<code>                      initval=np.array([cs_exp.mean()-1, cs_exp.mean()+1])) </code>&#13;
<code>    sd = pm.HalfNormal('sd', sigma=10) </code>&#13;
<code>    y = pm.NormalMixture('y', w=p, mu=means, sigma=sd, observed=cs_exp) </code>&#13;
<code> </code>&#13;
<code>    idata_mgo = pm.sample()</code></pre>&#13;
<p>Let’s check the new results with a forest plot. <em>Figure <a href="#x1-143014r7">7.7</a></em> confirms that we have removed the non-identifiability issue:</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file203.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-143014r7"/><strong>Figure 7.7</strong>: Forest plot of the means from <code>model_mgo</code></p>&#13;
<p><span id="x1-143015r343"/></p>&#13;
</section>&#13;
<section id="how-to-choose-k" class="level3 sectionHead" data-number="1.11.4">&#13;
<h2 class="sectionHead" data-number="1.11.4">7.4 <span id="x1-1440004"/>How to choose K</h2>&#13;
<p>One of the main concerns with finite mixture models is how to decide on the number of components. A rule of thumb is to begin with a relatively small number of components and then increase it to improve the model-fit evaluation. As we already know from <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>, model fit can be evaluated using posterior-predictive checks, metrics such as the ELPD, and the expertise of the modeler(s).</p>&#13;
<p>Let us compare the model for <em>K</em> = <span class="cmsy-10x-x-109">{</span>2<em>,</em>3<em>,</em>4<em>,</em>5<span class="cmsy-10x-x-109">}</span>. To do this, we are going to fit the model four times and then save the data and model objects for later use:</p>&#13;
<p><span id="x1-144001r4"/> <span id="x1-144002"/><strong>Code 7.4</strong></p>&#13;
<pre id="listing-97" class="source-code"><code>Ks = [2, 3, 4, 5] </code>&#13;
<code> </code>&#13;
<code>models = [] </code>&#13;
<code>idatas = [] </code>&#13;
<code>for k in Ks: </code>&#13;
<code>    with pm.Model() as model: </code>&#13;
<code>        p = pm.Dirichlet('p', a=np.ones(k)) </code>&#13;
<code>        means = pm.Normal('means', </code>&#13;
<code>                          mu=np.linspace(cs_exp.min(), cs_exp.max(), k), </code>&#13;
<code>                          sigma=cs_exp.var() / k, shape=k, </code>&#13;
<code>                          transform=pm.distributions.transforms.ordered, </code>&#13;
<code>                          ) </code>&#13;
<code>        sd = pm.HalfNormal('sd', sigma=5) </code>&#13;
<code>        y = pm.NormalMixture('y', w=p, mu=means, sigma=sd, observed=cs_exp) </code>&#13;
<code>        idata = pm.sample(random_seed=123, </code>&#13;
<code>                          idata_kwargs={"log_likelihood":True} </code>&#13;
<code>                         ) </code>&#13;
<code> </code>&#13;
<code>        idatas.append(idata) </code>&#13;
<code>        models.append(model)</code></pre>&#13;
<p><em>Figure <a href="#x1-144023r8">7.8</a></em> shows the mixture models for <em>K</em> number of Gaussians. The black solid lines are the posterior means, and the gray lines are samples from the posterior. The mean-Gaussian components are represented using a black dashed line.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file204.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-144023r8"/><strong>Figure 7.8</strong>: Gaussian mixture models for different numbers of Gaussians (<em>K</em>)</p>&#13;
<p>Visually, it seems <em>K</em> = 2 is too low, but how do we choose a better value? As we have already discussed in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>, we can use posterior predictive checks of test quantities of interest and compute Bayesian p-values. <em>Figure <a href="#x1-144024r9">7.9</a></em> shows an example of such a calculation and visualization. <em>K</em> = 5 is the best solution and <em>K</em> = 4 comes closes.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file205.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-144024r9"/><strong>Figure 7.9</strong>: Posterior predictive check to choose <em>K</em></p>&#13;
<p>To complement posterior predictive checks, we can compute the ELPD as approximated with the LOO method. This is shown in <em>Figure <a href="#x1-144025r10">7.10</a></em>. We are comparing the same model but with different values of <em>K</em>. We can see that <em>K</em> = 5 is the best solution and <em>K</em> = 4 comes close. This is in agreement with the Bayesian p-values shown in <em>Figure <a href="#x1-144024r9">7.9</a></em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file206.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-144025r10"/><strong>Figure 7.10</strong>: Model selection with LOO to choose <em>K</em></p>&#13;
<p>The chemical shifts example, while simple, shows the main ideas about finite mixture models. For this example, we used Gaussians as they provide a good approximation to model the data. However, we are free to use non-Gaussian components if needed. For example, we could use a:</p>&#13;
<ul>&#13;
<li><p><strong>Poisson mixture model</strong>: Suppose you are monitoring the number of customers entering a store every hour. A Poisson mixture model can help to identify different patterns of customer traffic, such as peak hours or days, by assuming that the data follows a mixture of Poisson distributions.</p></li>&#13;
<li><p><strong>Exponential mixture model</strong>: Imagine you are studying the lifetimes of a certain type of light bulb. An Exponential mixture model can assist in identifying different groups of light bulbs with varying lifetimes, suggesting potential differences in manufacturing quality or environmental factors.</p></li>&#13;
</ul>&#13;
<p>In the next section, we will explore a very particular type of mixture model, one that involves two processes: one generating zeros and the other generating zeros or non-zeros. <span id="x1-144026r346"/></p>&#13;
</section>&#13;
<section id="zero-inflated-and-hurdle-models" class="level3 sectionHead" data-number="1.11.5">&#13;
<h2 class="sectionHead" data-number="1.11.5">7.5 <span id="x1-1450005"/>Zero-Inflated and hurdle models</h2>&#13;
<p><span id="dx1-145001"/></p>&#13;
<p>When counting things, like cars on a road, stars in the sky, moles on your skin, or virtually anything else, one option is to not count a thing, that is, to get zero. The number zero can generally occur for many reasons; we get a zero because we were counting red cars and a red car did not go down the street or because we missed it. If we use a Poisson or NegativeBinomial distribution to model such data, we will notice that the model generates fewer zeros compared to the data. How do we fix that? We may try to address the exact cause of our model predicting fewer zeros than the observed and include that factor in the model. But, as is often the case, it may be enough, and simpler, to assume that we have a mixture of two processes:</p>&#13;
<ul>&#13;
<li><p>One modeled by a discrete distribution with probability <img src="../media/Phi_01.png" style="width:0.75em; vertical-align: -0.30em;"/></p></li>&#13;
<li><p>One giving extra zeros with probability 1 <span class="cmsy-10x-x-109">− </span><img src="../media/Phi_01.png" style="width:0.75em; vertical-align: -0.30em;"/></p></li>&#13;
</ul>&#13;
<p>In some texts, you will find that <img src="../media/Phi_01.png" style="width:0.75em; vertical-align: -0.30em;"/> represents the extra zeros instead of 1 <span class="cmsy-10x-x-109">− </span><img src="../media/Phi_01.png" style="width:0.75em; vertical-align: -0.30em;"/>. This is not a big deal; just pay attention to which is which for a concrete example.</p>&#13;
<p>The family of distributions allowing for ”extra” zeros is known as a Zero-Inflated distribution. The most common members of that family are:</p>&#13;
<ul>&#13;
<li><p>Zero-Inflated Poisson</p></li>&#13;
<li><p>Zero-Inflated NegativeBinomial</p></li>&#13;
<li><p>Zero-Inflated Binomial</p></li>&#13;
</ul>&#13;
<p>In the next section, we are going to use Zero-Inflated Poisson to solve a regression problem. Once you understand how to work with this distribution, working with the Zero-Inflated NegativeBinomial or Zero-Inflated Binomial will become very easy. <span id="x1-145002r338"/></p>&#13;
<section id="zero-inflated-poisson-regression" class="level4 subsectionHead" data-number="1.11.5.1">&#13;
<h3 class="subsectionHead" data-number="1.11.5.1">7.5.1 <span id="x1-1460001"/>Zero-Inflated Poisson regression</h3>&#13;
<p><span id="dx1-146001"/></p>&#13;
<p>To exemplify a Zero-Inflated Poisson regression model, we are going to work with a dataset taken from the Institute for Digital Research and Education ( <a href="http://www.ats.ucla.edu/stat/data" class="url">http://www.ats.ucla.edu/stat/data</a>). We have 250 groups of visitors to a park. Here are some parts of the data per group: the number of fish they caught (<code>count</code>), how many children were in the group (<code>child</code>), and whether they brought a camper to the park (<code>camper</code>). Using this data, we are going to build a model that predicts the number of caught fish as a function of the child and camper variables.</p>&#13;
<p>Using PyMC we can write a model for this data like:</p>&#13;
<p><span id="x1-146002r5"/> <span id="x1-146003"/><strong>Code 7.5</strong></p>&#13;
<pre id="listing-98" class="source-code"><code>with pm.Model() as ZIP_reg: </code>&#13;
<code>    <img src="../media/Phi_02.png" style="width:0.75em; vertical-align: -0.30em;"/> = pm.Beta('<img src="../media/Phi_02.png" style="width:0.75em; vertical-align: -0.30em;"/>', 1, 1) </code>&#13;
<code>    <em>α</em> = pm.Normal('<em>α</em>', 0, 1) </code>&#13;
<code>    <em>β</em> = pm.Normal('<em>β</em>', 0, 1, shape=2) </code>&#13;
<code>    <em>θ</em> = pm.math.exp(<em>α</em> + <em>β</em>[0] * fish_data['child'] + <em>β</em>[1] * fish_data['camper']) </code>&#13;
<code>    yl = pm.ZeroInflatedPoisson('yl', <img src="../media/Phi_02.png" style="width:0.75em; vertical-align: -0.30em;"/>, <em>θ</em>, observed=fish_data['count']) </code>&#13;
<code>    trace_ZIP_reg = pm.sample()</code></pre>&#13;
<p><code>camper </code>is a binary variable with 0 for not-camper and 1 for camper. A variable indicating the absence/presence of an attribute is usually denoted as a dummy variable or indicator variable. Note that when <code>camper </code>takes the value of 0, the term involving <em>β</em><sub>1</sub> will also be 0 and the model reduces to a regression with a single independent variable. We already discussed this in <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em> when talking about Categorical predictors.</p>&#13;
<p>The results are shown in <em>Figure <a href="#x1-146011r11">7.11</a></em>. We can see that the higher the number of children, the lower the number of fish caught. Also, people who travel with a camper generally catch more fish. If you check the coefficients for <code>child </code>and <code>camper</code>, you will see that we can say the following:</p>&#13;
<ul>&#13;
<li><p>For each additional child, the expected count of the fish caught decreases by <span class="cmsy-10x-x-109">≈ </span>0<em>.</em>4</p></li>&#13;
<li><p>Camping with a camper increases the expected count of the fish caught by <span class="cmsy-10x-x-109">≈ </span>2</p></li>&#13;
</ul>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file207.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-146011r11"/><strong>Figure 7.11</strong>: Fish caught as a function of the number of children and use of a camper</p>&#13;
<p>Zero-Inflated models are closely associated with hurdle models, making it beneficial to learn about hurdle models while the concept of Zero-Inflated models is still fresh in our minds. <span id="x1-146012r350"/></p>&#13;
</section>&#13;
<section id="hurdle-models" class="level4 subsectionHead" data-number="1.11.5.2">&#13;
<h3 class="subsectionHead" data-number="1.11.5.2">7.5.2 <span id="x1-1470002"/>Hurdle models</h3>&#13;
<p><span id="dx1-147001"/></p>&#13;
<p>In hurdle models, the Bernoulli probability determines whether a count variable has a value of zero or something greater. If it’s greater than zero, we consider the <em>hurdle</em> crossed, and the distribution of these positive values is determined using a distribution truncated at zero.</p>&#13;
<p>In terms of mixture models, we can think of a Zero-Inflated model as a mixture of zeros and something else, which can be zero or non-zero. Instead, a hurdle model is a mixture of zeros and non-zeros. As a consequence, a Zero-Inflated model can only increase the probability of <em>P</em>(<em>x</em> = 0), but for hurdle models, the probability can be smaller or larger.</p>&#13;
<p>The available distributions in PyMC and Bambi for hurdle models are:</p>&#13;
<ul>&#13;
<li><p>Hurdle Poisson</p></li>&#13;
<li><p>Hurdle NegativeBinomial</p></li>&#13;
<li><p>Hurdle Gamma</p></li>&#13;
<li><p>Hurdle LogNormal</p></li>&#13;
</ul>&#13;
<p>To illustrate hurdle models, we are going to use the horseshoe crab dataset [<a href="Bibliography.xhtml#XBrockmann_1996">Brockmann</a>, <a href="Bibliography.xhtml#XBrockmann_1996">1996</a>]. Horseshoe crabs arrive at the beach in pairs for their spawning ritual. Additionally, solitary males also make their way to the shoreline, gathering around the nesting couples and vying for the opportunity to fertilize the eggs. These individuals, known as satellite males, often congregate in sizable groups near certain nesting pairs while disregarding others. We want to model the number of male <code>satellites</code>. We suspect this number is related to the properties of the female crabs. As predictors, we are going to use the carapace <code>width </code>and <code>color</code>. The carapace is the hard upper shell of crabs. The color is encoded using the integers 1 to 4, from lighter to darker tones.</p>&#13;
<p>We are going to use Bambi to encode and fit four models. The main difference between the four models is that we are going to use four different likelihoods, or families, namely Poisson, Hurdle Poisson, NegativeBinomial, and Hurdle NegativeBinomial. The models are shown in the next code block:</p>&#13;
<p><span id="x1-147002r6"/> <span id="x1-147003"/><strong>Code 7.6</strong></p>&#13;
<pre id="listing-99" class="source-code"><code>model_crab_p = bmb.Model("satellite ∼ width + C(color)", </code>&#13;
<code>                         family="poisson", data=crab) </code>&#13;
<code>model_crab_hp = bmb.Model("satellite ∼ width + C(color)", </code>&#13;
<code>                          family="hurdle_poisson", data=crab) </code>&#13;
<code>model_crab_nb = bmb.Model("satellite ∼ width + C(color)", </code>&#13;
<code>                          family="negativebinomial", data=crab) </code>&#13;
<code>model_crab_hnb = bmb.Model("satellite ∼ width + C(color)", </code>&#13;
<code>                           family="hurdle_negativebinomial", data=crab)</code></pre>&#13;
<p>Notice that we have encoded color as <code>C(color) </code>to indicate to Bambi that it should treat it as a Categorical variable, and not a numeric one.</p>&#13;
<p><em>Figure <a href="#x1-147012r12">7.12</a></em> shows a posterior predictive check for the four models we fitted to the horseshoe data. The gray bars represent the frequency of the observed data. The dots are the expected values, according to the model. The dashed line is just a visual aid. We can see that the NegativeBinomial is an improvement on the Poisson and the hurdle model is an improvement on the non-inflated models.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file208.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-147012r12"/><strong>Figure 7.12</strong>: Posterior predictive check for 4 models for the horseshoe data</p>&#13;
<p><em>Figure <a href="#x1-147013r13">7.13</a></em> shows a model comparison in terms of the ELPD as computed with LOO. The Hurdle NegativeBinomial is the best model and the Poisson one is the worst.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file209.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-147013r13"/><strong>Figure 7.13</strong>: Model comparison using LOO for 4 models for the horseshoe data</p>&#13;
<p><em>Figure <a href="#x1-147012r12">7.12</a></em> is nice, but there is an alternative representation called hanging rootograms [<a href="Bibliography.xhtml#Xkleiber_2016">Kleiber and Zeileis</a>, <a href="Bibliography.xhtml#Xkleiber_2016">2016</a>], which is particularly useful for diagnosing and treating issues such as overdispersion and/or excess zeros in count data models. See <em>Figure <a href="#x1-147014r14">7.14</a></em> for an example of the horseshoe data and our four models.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file210.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-147014r14"/><strong>Figure 7.14</strong>: Posterior predictive check, with rootograms, for 4 models for the horseshoe data</p>&#13;
<p>In hanging rootograms, we plot the square roots of the observed and predicted values. This is a quick way of approximately adjusting for the scale differences across the different counts. In other words, it makes it easier to compare observed and expected frequencies even for low frequencies. Second, the bars for the observed data are <em>hanging</em> from the expected values, instead of <em>growing</em> from zero, as in <em>Figure <a href="#x1-147012r12">7.12</a></em>. Because the bars are hanging, if a bar doesn’t reach zero (dashed gray line), then the model is overpredicting a count, and if the bar goes below zero, then the model is underpredicting that count.</p>&#13;
<p>Let’s summarize each of the subplots from <em>Figure <a href="#x1-147012r12">7.12</a></em>:</p>&#13;
<ul>&#13;
<li><p>Poisson: The zeros are underpredicted, and counts 1 to 4 are overpredicted. Most counts from 6 onward are also underpredicted. This pattern is an indication of overdispersion in the data, and the huge difference for 0 indicates an excess of zeros.</p></li>&#13;
<li><p>NegativeBinomial: We see that overdispersion is much better handled compared to the Poisson model. We still see that the zeros are underpredicted and counts 1 and 2 are overpredicted, probably indicating an excess of zeros.</p></li>&#13;
<li><p>Hurdle Poisson: As expected for a hurdle model, we get a perfect fit for the zeros. For the positive values, we still get some deviations.</p></li>&#13;
<li><p>Hurdle NegativeBinomial: We see that the model can fit the data very well, with the deviations being very small for most of the counts.</p></li>&#13;
</ul>&#13;
<p><span id="x1-147015r349"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="mixture-models-and-clustering" class="level3 sectionHead" data-number="1.11.6">&#13;
<h2 class="sectionHead" data-number="1.11.6">7.6 <span id="x1-1480006"/>Mixture models and clustering</h2>&#13;
<p><span id="dx1-148001"/> <span id="dx1-148002"/></p>&#13;
<p>Clustering or cluster analysis is the data analysis task of grouping objects in such a way that objects in a given group are closer to each other than to those in the other groups. The groups are called clusters and the degree of closeness can be computed in many different ways, for example, by using metrics, such as the Euclidean distance. If instead we take the probabilistic route, then a mixture model arises as a natural candidate to solve clustering tasks.</p>&#13;
<p>Performing clustering using probabilistic models is usually known as model-based clustering. Using a probabilistic model allows us to compute the probability of each data point belonging to each one of the clusters. This is known as soft clustering instead of hard clustering, where each data point belongs to a cluster with a probability of 0 or 1. We can turn soft clustering into hard clustering by introducing some rule or boundary. In fact, you may remember that this is exactly what we do to turn logistic regression into a classification method, where we use as the default boundary the value of 0.5. For clustering, a reasonable choice is to assign a data point to the cluster with the highest probability.</p>&#13;
<p>In summary, when people talk about clustering, they are generally talking about grouping objects, and when people talk about mixture models, they are talking about using a mix of simple distributions to model a more complex distribution, either to identify subgroups or just to have a more flexible model to describe the data. <span id="x1-148003r356"/></p>&#13;
</section>&#13;
<section id="non-finite-mixture-model" class="level3 sectionHead" data-number="1.11.7">&#13;
<h2 class="sectionHead" data-number="1.11.7">7.7 <span id="x1-1490007"/>Non-finite mixture model</h2>&#13;
<p><span id="dx1-149001"/></p>&#13;
<p>For some problems, such as trying to cluster handwritten digits, it is easy to justify the number of groups we expect to find in the data. For other problems, we can have good guesses; for example, we may know that our sample of Iris flowers was taken from a region where only three species of Iris grow, thus using three components is a reasonable starting point. When we are not that sure about the number of components, we can use model selection to help us choose the number of groups. Nevertheless, for other problems, choosing the number of groups a priori can be a shortcoming, or we may instead be interested in estimating this number directly from the data. A Bayesian solution for this type of problem is related to the Dirichlet process. <span id="x1-149002r353"/></p>&#13;
<section id="dirichlet-process" class="level4 subsectionHead" data-number="1.11.7.1">&#13;
<h3 class="subsectionHead" data-number="1.11.7.1">7.7.1 <span id="x1-1500001"/>Dirichlet process</h3>&#13;
<p><span id="dx1-150001"/> <span id="dx1-150002"/></p>&#13;
<p>All the models that we have seen so far have been parametric models, meaning models with a fixed number of parameters that we are interested in estimating, like a fixed number of clusters. We can also have non-parametric models. A better name for these models would probably be non-fixed-parametric models or models with a variable number of parameters, but someone already decided the name for us. We can think of non-parametric models as models with a theoretically infinite number of parameters. In practice, we somehow let the data reduce the theoretically infinite number of parameters to some finite number. As the data <em>decides</em> on the actual number of parameters, non-parametric models are very flexible and potentially robust against underfitting and overfitting. In this book, we are going to see three examples of such models: the Gaussian process (GP), Bayesian Additive Regression Trees (BART), and the Dirichlet process (DPs). While the upcoming chapters will focus on GPs and BART individually, our immediate attention will be directed toward exploring DPs.</p>&#13;
<p>As the Dirichlet distribution is the n-dimensional generalization of the beta distribution, the Dirichlet process is the infinite-dimensional generalization of the Dirichlet distribution. I know this can be puzzling at first, so let’s take the time to re-read the previous sentence before continuing.</p>&#13;
<p>The Dirichlet distribution is a probability distribution on the space of probabilities, while the DP is a probability distribution on the space of distributions. This means that a single draw from a DP is actually a distribution. For finite mixture models, we used the Dirichlet distribution to assign a prior for the fixed number of clusters or groups. A DP is a way to assign a prior distribution to a non-fixed number of clusters. We can think of a DP as a way to sample from a prior distribution of distributions.</p>&#13;
<p>Before we move on to the actual non-parametric mixture model, let us take a moment to discuss some of the details of the DP. The formal definition of a DP is somewhat obscure unless you know probability theory very well, so instead let me describe some of the properties of a DP that are relevant to understanding its role in non-finite mixture models:</p>&#13;
<ul>&#13;
<li><p>A DP is a distribution whose realizations are probability distributions. For instance, from a Gaussian distribution, you sample numbers, while from a DP, you sample distributions.</p></li>&#13;
<li><p>A DP is specified by a base distribution <span class="cmsy-10x-x-109"><img src="../media/H.PNG" style="width:1.1em; vertical-align: -0.10em;"/> </span>and <em>α</em>, a positive real number, called the concentration parameter. <em>α</em> is analog to the concentration parameter in the Dirichlet distribution.</p></li>&#13;
<li><p><span class="cmsy-10x-x-109"><img src="../media/H.PNG" style="width:1.1em; vertical-align: -0.10em;"/> </span>is the expected value of the DP. This means that a DP will generate distributions around the base distribution. This is somehow equivalent to the mean of a Gaussian distribution.</p></li>&#13;
<li><p>As <em>α</em> increases, the realizations become less and less concentrated.</p></li>&#13;
<li><p>In practice, a DP always generates discrete distributions.</p></li>&#13;
<li><p>In the limit <em>α</em> <span class="cmsy-10x-x-109">→∞</span>, the realizations from a DP will be equal to the base distribution, thus if the base distribution is continuous, the DP will generate a continuous distribution. For this reason, mathematicians say that the distributions generated from a DP are almost surely discrete. In practice, <em>alpha</em> is a finite number, thus we always work with discrete distributions.</p></li>&#13;
</ul>&#13;
<div id="tcolobox-16" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Priors Over Distributions</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>We can think of a DP as the prior on a random distribution <em>f</em>, where the base distribution <span class="cmsy-10x-x-109"><img src="../media/H_gray.PNG" style="width:1.1em; vertical-align: -0.10em;"/> </span>is what we expected <em>f</em> to be and the concentration parameter <em>α</em> represents how confident we are about our prior guess.</p>&#13;
</div>&#13;
</div>&#13;
<p>To make these properties more concrete, let us take a look again at the Categorical distribution in <em>Figure <a href="#x1-140003r3">7.3</a></em>. We can completely specify this distribution by indicating the position on the x-axis and the height on the y-axis. For the Categorical distribution, the positions on the x-axis are restricted to be integers and the sum of the heights has to be 1. Let’s keep the last restriction but relax the former one. To generate the positions on the x-axis, we are going to sample from a base distribution <span class="cmsy-10x-x-109"><img src="../media/H.PNG" style="width:1.1em; vertical-align: -0.10em;"/></span>. In principle, it can be any distribution we want; thus if we choose a Gaussian, the locations would be any value from the real line. Instead, if we choose a Beta, the locations will be restricted to the interval [0, 1], and if we choose a Poisson as the base distribution, the locations will be restricted to be non-negative integers 0, 1, 2, ....</p>&#13;
<p>So far so good, but how do we choose the values on the y-axis? We follow a <em>Gedanken experiment</em> known as the stick-breaking process. Imagine we have a stick of length 1, then we break it into two parts (not necessarily equal). We set one part aside and break the other part into two, and then we just keep doing this forever and ever. In practice, as we cannot really repeat the process infinitely, we truncate it at some predefined value <em>K</em>, but the general idea holds, at least in practice. To control the stick-breaking process, we use a parameter <em>α</em>. As we increase the value of <em>α</em>, we will break the stick into smaller and smaller portions. Thus, for <em>α</em> = 0, we don’t break the stick, and for <em>α</em> = <span class="cmsy-10x-x-109">∞</span>, we break it into infinite pieces. <em>Figure <a href="#x1-150003r15">7.15</a></em> shows four draws from a DP, for four different values of <em>α</em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file211.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-150003r15"/><strong>Figure 7.15</strong>: Stick-breaking process with a Gaussian as the base distribution</p>&#13;
<p>We can see from <em>Figure <a href="#x1-150003r15">7.15</a></em> that the DP is a discrete distribution. When <em>α</em> increases, we obtain smaller pieces from the initial unit-length stick; notice the change on the scale of the y-axis. The base distribution, a Normal(0, 1) in this figure, controls the locations. With increasing <em>α</em>, the sticks progressively resemble the base distribution more. In the accompanying notebook for this chapter, you will find the code to generate <em>Figure <a href="#x1-150003r15">7.15</a></em>. I highly recommend you play with this code to gain a better intuition of DPs.</p>&#13;
<p><em>Figure <a href="#x1-138002r1">7.1</a></em> shows that if you place a Gaussian on top of each data point and then sum all the Gaussians, you can approximate the distribution of the data. We can use a DP to do something similar, but instead of placing a Gaussian on top of each data point, we can place a Gaussian at the location of each piece of the original unit-length stick. We then weigh each Gaussian by the length of each piece. This procedure provides a general recipe for a non-finite Gaussian-mixture model.</p>&#13;
<p>Alternatively, we can replace the Gaussian for any other distribution and we will have a general recipe for a non-finite mixture model. <em>Figure <a href="#x1-150004r16">7.16</a></em> shows an example of such a model. I used a mixture of Laplace distributions, just to reinforce the idea that you are by no means restricted to just using Gaussian mixture models:</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file212.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-150004r16"/><strong>Figure 7.16</strong>: Laplace mixture model using a DP</p>&#13;
<p>Now we are more than ready to try to implement a DP in PyMC. Let’s first define a <code>stick_breaking </code>function that works with PyMC:</p>&#13;
<p><span id="x1-150005r7"/> <span id="x1-150006"/><strong>Code 7.7</strong></p>&#13;
<pre id="listing-100" class="source-code"><code>K = 10 </code>&#13;
<code> </code>&#13;
<code>def stick_breaking(<em>α</em>, K): </code>&#13;
<code>    <em>β</em> = pm.Beta('<em>β</em>', 1., <em>α</em>, shape=K) </code>&#13;
<code>    w = <em>β</em> * pt.concatenate([[1.], pt.extra_ops.cumprod(1. - <em>β</em>)[:-1]]) + 1E-6 </code>&#13;
<code>    return w/w.sum()</code></pre>&#13;
<p>Instead of fixing the value of <em>α</em>, the concentration parameter, we are going to define a prior for it. A common choice for this is a Gamma distribution, as shown in the following code block:</p>&#13;
<p><span id="x1-150013r8"/> <span id="x1-150014"/><strong>Code 7.8</strong></p>&#13;
<pre id="listing-101" class="source-code"><code>with pm.Model() as model_DP: </code>&#13;
<code>    <em>α</em> = pm.Gamma('<em>α</em>', 2, 1) </code>&#13;
<code>    w = pm.Deterministic('w', stick_breaking(<em>α</em>, K)) </code>&#13;
<code>    means = pm.Normal('means', </code>&#13;
<code>                      mu=np.linspace(cs_exp.min(), cs_exp.max(), K), </code>&#13;
<code>                      sigma=5, shape=K, </code>&#13;
<code>                      transform=pm.distributions.transforms.ordered, </code>&#13;
<code>                     ) </code>&#13;
<code> </code>&#13;
<code>    sd = pm.HalfNormal('sd', sigma=10, shape=K) </code>&#13;
<code>    obs = pm.NormalMixture('obs', w, means, sigma=sd, observed=cs_exp.values) </code>&#13;
<code>    idata = pm.sample()</code></pre>&#13;
<p>Because we are approximating the infinite DP with a truncated stick-breaking procedure, it is important to check that the truncation value (<em>K</em> = 10 in this example) is not introducing any bias. A simple way to do this is to compute the average weight of each component, sort them, and then plot their cumulative sum. To be on the safe side, we should have at least a few components with negligible weight; otherwise, we must increase the truncation value. An example of this type of plot is <em>Figure <a href="#x1-150027r17">7.17</a></em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file213.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-150027r17"/><strong>Figure 7.17</strong>: Ordered cumulative distribution of average weights of the DP components</p>&#13;
<p>We can see that only the first 7 components are somewhat important. The first 7 components represent more than 99.9% of the total weight (gray dashed line in <em>Figure <a href="#x1-150027r17">7.17</a></em>) and thus we can be confident that the chosen upper value (<em>K</em> = 10) is large enough for this data.</p>&#13;
<p><em>Figure <a href="#x1-150028r18">7.18</a></em> shows the mean density estimated using the DP model (black line) together with samples from the posterior (gray lines) to reflect the uncertainty in the estimation.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file214.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-150028r18"/><strong>Figure 7.18</strong>: DP mixture model for the chemical shifts data</p>&#13;
<p><span id="x1-150029r357"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="continuous-mixtures" class="level3 sectionHead" data-number="1.11.8">&#13;
<h2 class="sectionHead" data-number="1.11.8">7.8 <span id="x1-1510008"/>Continuous mixtures</h2>&#13;
<p><span id="dx1-151001"/></p>&#13;
<p>The focus of this chapter was on discrete mixture models, but we can also have continuous mixture models. And indeed we already know some of them. For instance, hierarchical models can also be interpreted as continuous mixture models where the parameters in each group come from a continuous distribution in the upper level. To make it more concrete, think about performing linear regression for several groups. We can assume that each group has its own slope or that all the groups share the same slope. Alternatively, instead of framing our problem as two extreme discrete options, a hierarchical model allows us to effectively model a continuous mixture of these two options. <span id="x1-151002r358"/></p>&#13;
<section id="some-common-distributions-are-mixtures" class="level4 subsectionHead" data-number="1.11.8.1">&#13;
<h3 class="subsectionHead" data-number="1.11.8.1">7.8.1 <span id="x1-1520001"/>Some common distributions are mixtures</h3>&#13;
<p><span id="dx1-152001"/></p>&#13;
<p>The BetaBinomial is a discrete distribution generally used to describe the number of successes <em>y</em> for <em>n</em> Bernoulli trials when the probability of success <em>p</em> at each trial is unknown and assumed to follow a beta distribution with parameters <em>α</em> and <em>β</em>:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file215.jpg" class="math-display" alt=" ∫ 1 BetaBinomial(y | n,𝛼,𝛽 ) = Bin(y | p,n ) Beta(p | 𝛼,𝛽)dp 0 "/>&#13;
</div>&#13;
<p>That is, to find the probability of observing the outcome <em>y</em>, we average over all the possible (and continuous) values of <em>p</em>. Thus, the BetaBinomial can be considered as a continuous mixture model. If the BetaBinomial model sounds familiar to you, it is because you were paying attention in the first two chapters of the book! This is the model we used for the coin-flipping problem, although we explicitly used a Beta and Binomial distribution, instead of using the already <em>mixed</em> Beta-Binomial distribution.</p>&#13;
<p>In a similar fashion, we have the NegativeBinomial distribution, which can be understood as a Gamma-Poisson mixture. That is, a mixture of Poisson distributions where the rate parameter is Gamma distributed. The Negative-Binomial distribution is often used to circumvent a common problem encountered when dealing with count data. This problem is known as over-dispersion. Suppose you are using a Poisson distribution to model count data, and then you realize that the variance in your data exceeds that of the model; the problem with using a Poisson distribution is that mean and variance are described by the same parameter. One way to account for over-dispersion is to model the data as a (continuous) mixture of Poisson distributions. By considering a mixture of distributions, our model has more flexibility and can better accommodate the mean and variance of the data.</p>&#13;
<p>Another example of a mixture of distributions is the Student’s t-distribution. We introduced this distribution as a robust alternative to the Gaussian distribution. In this case, the t-distribution results from a mixture of Gaussian distributions with mean <em>μ</em> and unknown variance distributed as an InverseGamma distribution. <span id="x1-152002r363"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="summary-6" class="level3 sectionHead" data-number="1.11.9">&#13;
<h2 class="sectionHead" data-number="1.11.9">7.9 <span id="x1-1530009"/>Summary</h2>&#13;
<p>Many problems can be described as an overall population composed of distinct sub-populations. When we know to which sub-population each observation belongs, we can specifically model each sub-population as a separate group. However, many times we do not have direct access to this information, thus it may be appropriate to model that data using mixture models. We can use mixture models to try to capture true sub-populations in the data or as a general statistical trick to model complex distributions by combining simpler distributions.</p>&#13;
<p>In this chapter, we divided mixture models into three classes: finite mixture models, non-finite mixture models, and continuous mixture models. A finite mixture model is a finite weighted mixture of two or more distributions, each distribution or component representing a subgroup of the data. In principle, the components can be virtually anything we may consider useful from simple distributions, such as a Gaussian or a Poisson, to more complex objects, such as hierarchical models or neural networks. Conceptually, to solve a mixture model, all we need to do is to properly assign each data point to one of the components. We can do this by introducing a latent variable <em>z</em>. We use a Categorical distribution for <em>z</em>, which is the most general discrete distribution, with a Dirichlet prior, which is the n-dimensional generalization of the Beta distribution. Sampling the discrete variable <em>z</em> can be problematic, thus it may be convenient to marginalize it. PyMC includes a normal mixture distribution and a mixture distribution that performs this marginalization for us, making it easier to build mixture models with PyMC.</p>&#13;
<p>One common problem we looked at in this chapter when working with mixture models is that this model can lead to the label-switching problem, a form of non-identifiability. One way to remove non-identifiability is to force the components to be ordered. One challenge with finite mixture models is how to decide on the number of components. One solution is to perform a model comparison for a set of models around an estimated number of components. That estimation should be guided, when possible, by our knowledge of the problem at hand. Another option is to try to automatically estimate the number of components from the data. For this reason, we introduced the concept of the Dirichlet process as an infinite-dimensional version of the Dirichlet distribution that we can use to build a non-parametric mixture model.</p>&#13;
<p>Finally, to close the chapter, we briefly discussed how many models, such as the BetaBinomial (the one used for the coin-flipping problem), the NegativeBinomial, the Student’s t-distribution, and even hierarchical models, can be interpreted as continuous mixture models. <span id="x1-153001r365"/></p>&#13;
</section>&#13;
<section id="exercises-6" class="level3 sectionHead" data-number="1.11.10">&#13;
<h2 class="sectionHead" data-number="1.11.10">7.10 <span id="x1-15400010"/>Exercises</h2>&#13;
<ol>&#13;
<li><div id="x1-154002x1">&#13;
<p>Generate synthetic data from a mixture of 3 Gaussians. Check the accompanying Jupyter notebook for this chapter for an example of how to do this. Fit a finite Gaussian mixture model with 2, 3, or 4 components.</p>&#13;
</div></li>&#13;
<li><div id="x1-154004x2">&#13;
<p>Use LOO to compare the results from exercise 1.</p>&#13;
</div></li>&#13;
<li><div id="x1-154006x3">&#13;
<p>Read and run through the following examples about mixture models from the PyMC documentation:</p>&#13;
<ul>&#13;
<li><p>Marginalized Gaussian mixture model: <a href="https://www.pymc.io/projects/examples/en/latest/mixture_models/marginalized_gaussian_mixture_model.html" class="url">https://www.pymc.io/projects/examples/en/latest/mixture_models/marginalized_gaussian_mixture_model.html</a></p></li>&#13;
<li><p>Dependent density regression: <a href="https://www.pymc.io/projects/examples/en/latest/mixture_models/dependent_density_regression.html" class="url">https://www.pymc.io/projects/examples/en/latest/mixture_models/dependent_density_regression.html</a></p></li>&#13;
</ul>&#13;
</div></li>&#13;
<li><div id="x1-154008x4">&#13;
<p>Refit <code>fish_data </code>using a NegativeBinomial and a Hurdle NegativeBinomial model. Use rootograms to compare these two models with the Zero-Inflated Poisson model shown in this chapter.</p>&#13;
</div></li>&#13;
<li><div id="x1-154010x5">&#13;
<p>Repeat exercise 1 using a Dirichlet process.</p>&#13;
</div></li>&#13;
<li><div id="x1-154012x6">&#13;
<p>Assuming for a moment that you do not know the correct species/labels for the iris dataset, use a mixture model to cluster the three Iris species, using one feature of your choice (like the length of the sepal).</p>&#13;
</div></li>&#13;
<li><div id="x1-154014x7">&#13;
<p>Repeat exercise 6 but this time use two features.</p>&#13;
</div></li>&#13;
</ol>&#13;
</section>&#13;
<section id="join-our-community-discord-space-7" class="level3 likesectionHead" data-number="1.11.11">&#13;
<h2 class="likesectionHead" data-number="1.11.11"><span id="x1-15500010"/>Join our community Discord space</h2>&#13;
<p>Join our Discord community to meet like-minded people and learn alongside more than 5000 members at: <a href="https://packt.link/bayesian">https://packt.link/bayesian</a></p>&#13;
<p><img src="../media/file1.png" alt="PIC"/></p>&#13;
<p><span id="x1-155001r333"/></p>&#13;
</section>&#13;
</section>&#13;
</body></html>
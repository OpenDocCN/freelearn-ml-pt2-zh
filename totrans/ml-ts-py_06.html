<html><head></head><body>
  <div id="_idContainer274">
    <h1 class="chapterNumber">7</h1>
    <h1 id="_idParaDest-113" class="chapterTitle">Machine Learning Models for¬†Time-Series</h1>
    <p class="normal">Machine learning has come a long way in recent years, and this is reflected in the methods available to time-series predictions. We've introduced a few state-of-the-art machine learning methods for time-series in <em class="chapterRef">Chapter 4</em>, <em class="italic">Introduction to Machine Learning for Time-Series</em>. In the current chapter, we'll introduce several more machine learning methods.</p>
    <p class="normal">We'll go through methods that are commonly used as baseline methods, or that stand out in terms of either performance, ease of use, or their applicability. I'll introduce k-nearest neighbors with dynamic time warping and gradient boosting for time-series as a baseline and we'll go over other methods, such as Silverkite and gradient boosting. Finally, we'll go through an applied exercise with some of these methods.</p>
    <p class="normal">We're going to cover the following topics:</p>
    <ul>
      <li class="bullet">More machine learning methods for time-series</li>
      <li class="bullet">K-nearest neighbors with dynamic time warping</li>
      <li class="bullet">Silverkite</li>
      <li class="bullet">Gradient boosting</li>
      <li class="bullet">Python exercise</li>
    </ul>
    <p class="normal">If you are looking for a discussion of state-of-the-art machine learning algorithms, please refer to <em class="chapterRef">Chapter 4</em>, <em class="italic">Introduction to Machine Learning for Time-Series</em>. The discussion of algorithms will assume some of the information of that chapter. The algorithms that we'll cover in the next sections are all highly competitive for forecasting and prediction tasks. </p>
    <p class="normal">We'll discuss algorithms here in more detail.</p>
    <h1 id="_idParaDest-114" class="title">More machine learning methods for time-series</h1>
    <p class="normal">The algorithms that we'll cover in this section are all highly competitive for forecasting and prediction tasks. If you are looking for a discussion of state-of-the-art machine learning algorithms, please refer to <em class="chapterRef">Chapter 4</em>, <em class="italic">Introduction to Machine Learning for Time-Series</em>.</p>
    <p class="normal">In the aforementioned chapter, we've briefly discussed a few of these algorithms, but¬†we'll discuss <a id="_idIndexMarker559"/>them here in more detail and we will also introduce other algorithms that we haven't discussed before, such as Silverkite, gradient boosting, and k-nearest neighbors.</p>
    <p class="normal">We'll dedicate <a id="_idIndexMarker560"/>a separate practice section to a library that was released in 2021, which¬†is facebook's Kats. Kats provides many advanced features, including hyperparameter tuning and ensemble learning. On top of these features, they implement feature extraction based on the TSFresh library and include many models, including Prophet, SARIMA, and others. They claim that their hyperparameter tuning for time-series is about 6-20 times faster in benchmarks compared with other¬†hyperparameter tuning algorithms.</p>
    <p class="normal">This graph provides an overview of the popularity of selected time-series machine learning libraries:</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_01.png" alt="more_machine_learning-star_history.png"/></figure>
    <p class="packt_figref">Figure 7.1: Popularity of time-series machine learning libraries</p>
    <p class="normal">As of mid-2021, Kats and GreyKite have been released very recently, and although they have been garnering stars on GitHub, they haven't accumulated enough to rival TSFresh's popularity. I've included TSFresh even though it is a library for feature generation, and not prediction. I found it interesting to see how important it is in relation to <a id="_idIndexMarker561"/>other libraries that we use in this <a id="_idIndexMarker562"/>chapter. After TSFresh, SKTime is second, and it has been attracting a lot of stars over a relatively short time period. </p>
    <p class="normal">We'll use a few of these libraries in the practical examples in this chapter.</p>
    <p class="normal">Another important issue is validation, and it's worth covering this separately.</p>
    <h2 id="_idParaDest-115" class="title">Validation</h2>
    <p class="normal">We've discussed validation before in <em class="chapterRef">Chapter 4</em>, <em class="italic">Introduction to Machine Learning for Time-Series</em>. Often, in machine learning tasks, we use k-fold cross-validation, where splits of the <a id="_idIndexMarker563"/>data are performed pseudo-randomly, so the training and the test/validation datasets can come from any part of the data as long as it hasn't been used for training (<strong class="keyword">out-of-sample data</strong>). </p>
    <p class="normal">With time-series data, this way of validation can lead to an overconfidence in the model's performance because, realistically, time-series tend to change over time according to trend, seasonality, and changes to the time-series characteristics. </p>
    <p class="normal">Therefore, with time-series, validation is often performed in a so-called <strong class="keyword">walk-forward validation</strong>. This means <a id="_idIndexMarker564"/>that we train the model on past data, and we'll test it on the newest slice of data. This will take out the optimistic bias and give us a more realistic estimate of performance once the model is deployed.</p>
    <p class="normal">In terms of training, validation, and test datasets, this means that we'll adjust model parameters entirely on training and validation datasets, and we'll benchmark our test based on a set of data that's more advanced in time, as illustrated in the following diagram (source: Greykite library's GitHub repository):</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_02.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document Being Saved By screencaptureui 20)/Screenshot 2021-06-06 at 21.39.47.png"/></figure>
    <p class="packt_figref">Figure 7.2: Walk-forward validation</p>
    <p class="normal">In walk-forward validation, we train on an initial segment of the data and then test on a period after <a id="_idIndexMarker565"/>the training set. Next, we roll forward and repeat the process. This way, we have multiple out-of-sample periods and can combine the results over these periods. With walk-forward, we are less likely to suffer from overfitting.</p>
    <h1 id="_idParaDest-116" class="title">K-nearest neighbors with dynamic time warping</h1>
    <p class="normal">K-nearest neighbors is a well-known machine learning method (sometimes also going under the <a id="_idIndexMarker566"/>guise of case-based reasoning). In kNN, we can use a distance measure to find similar data points. We can then take <a id="_idIndexMarker567"/>the known labels of these nearest neighbors as the output and integrate them in some way using a function. </p>
    <p class="normal"><em class="italic">Figure 7.3</em> illustrates the basic idea of kNN for classification (source ‚Äì WikiMedia Commons: <a href="https://commons.wikimedia.org/wiki/File:KnnClassification.svg"><span class="url">https://commons.wikimedia.org/wiki/File:KnnClassification.svg</span></a>):</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_03.png" alt="/Users/ben/Downloads/Machine-Learning for Time-Series with Python/knn.png"/></figure>
    <p class="packt_figref">Figure 7.3: K-nearest neighbor for classification</p>
    <p class="normal">We know a few data points already. In the preceding illustration, these points are indicated as squares and triangles, and they represent data points of two different classes, respectively. Given a new data point, indicated by a circle, we find the closest known data points to it. In this example, we find that the new point is similar to triangles, so we might assume that the new point is of the triangle class as well. </p>
    <p class="normal">While this <a id="_idIndexMarker568"/>method is conceptually very simple, it often serves as a strong baseline method, or is sometimes even competitive with more sophisticated machine learning algorithms, even when we only compare the closest neighbor <em class="italic">(</em><em class="italic">ùëò</em><em class="italic">=1)</em>.</p>
    <p class="normal">The important hyperparameters in this algorithm are:</p>
    <ul>
      <li class="bullet">The number of neighbors (k) you want to base your output on</li>
      <li class="bullet">The integration function (for example, the average or the most frequent value)</li>
      <li class="bullet">The distance function to use to find the nearest data points</li>
    </ul>
    <p class="normal">We talked about dynamic time warping in <em class="chapterRef">Chapter 4</em>, <em class="italic">Introduction to Machine Learning for Time-Series</em>, as a measure that can be used to compare the similarity (or, equivalently, the distance) between two time-series. These sequences can even be of different lengths. Dynamic time warping has proven itself to be an exceptionally strong distance measure for time-series. </p>
    <p class="normal">We can use kNN in combination with dynamic time warping as a distance measure to find similar time-series, and this method has proven itself hard to beat, although the state of the art has since surpassed it.</p>
    <h1 id="_idParaDest-117" class="title">Silverkite</h1>
    <p class="normal">The Silverkite algorithm ships together with the Greykite library released by LinkedIn. It was explicitly <a id="_idIndexMarker569"/>designed with the goals in mind of being fast, accurate, and intuitive. The algorithm is described in a 2021 publication ("<em class="italic">A flexible forecasting model for production systems</em>", by Reza Hosseini and others). </p>
    <p class="normal">According to LinkedIn, it can handle different kinds of trends and seasonalities such as hourly, daily, weekly, repeated events, and holidays, and short-range effects. Within LinkedIn, it is used for both short-term, for example, a 1-day head, and long-term forecast horizons, such as 1 year ahead. </p>
    <p class="normal">Use cases within LinkedIn include optimizing budget decisions, setting business metric targets, and providing sufficient infrastructure to handle peak traffic. Furthermore, a use case has been to model recoveries from the COVID-19 pandemic. </p>
    <p class="normal">The time-series is modeled as an additive composite of trends, change points, and seasonality, where seasonality includes holiday/event effects. The trend is then modeled as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_001.png" alt="" style="height: 3.5em;"/></figure>
    <p class="normal">where K is the number of change points, and <em class="italic">t</em><sub class="" style="font-style: italic;">i</sub> is the time index for the i-th change point. Therefore, <img src="../Images/B17577_07_002.png" alt="" style="height: 1em;"/> is an indicator function for the i-th change point. The function f(t) can be linear, square root, quadratic, any combination, or completely custom. </p>
    <p class="normal">Silverkite also constructs indicator variables for holidays. Holidays can be specified by name or by country, or can even be completely custom.</p>
    <p class="normal">Change points can be specified manually or candidates can be automatically detected with a regression model and subsequently selected using the Adaptive Lasso algorithm (Hui Zhou, 2006).</p>
    <p class="normal">In addition to trend, seasonality, and holiday, Silverkite includes an autoregressive term that is calculated based on windowed averages rather than taking lags independently ("<em class="italic">Selecting a binary Markov model for a precipitation process</em>", by Reza Hosseini and others, 2011).</p>
    <p class="normal">This autoregressive <a id="_idIndexMarker570"/>term is specified using the Pasty library, using a formula mini-language, in a form like this string:</p>
    <pre class="programlisting gen"><code class="hljs">y ~ a + a:b + np.log(x)
</code></pre>
    <p class="normal">In this formula, y on the left-hand side is defined as the sum of three terms, <code class="Code-In-Text--PACKT-">a</code>, <code class="Code-In-Text--PACKT-">a:b</code>, and <code class="Code-In-Text--PACKT-">np.log(x)</code>. The term <code class="Code-In-Text--PACKT-">a:b</code> is an interaction between two factors, a and b. The model template itself in Pasty is highly customizable, so this interface provides a high degree of flexibility.</p>
    <p class="normal">Finally, Silverkite comes with several model types, such as ridge regression, elastic net, and boosted trees, with supported loss functions, MSE, and quantile loss for robust regression.</p>
    <p class="normal">According to a LinkedIn benchmark on several datasets, Silverkite outperforms both auto-Arima (the pmdarima library) and Prophet in terms of prediction error. Yet, Silverkite was about four times as fast as Prophet, which we'll introduce in <em class="chapterRef">Chapter 9</em>, <em class="italic">Probabilistic Models</em>.</p>
    <h1 id="_idParaDest-118" class="title">Gradient boosting</h1>
    <p class="normal"><strong class="keyword">XGBoost</strong> (short for <strong class="keyword">eXtreme Gradient Boosting</strong>) is <a id="_idIndexMarker571"/>an efficient implementation of gradient boosting (Jerome Friedman, "<em class="italic">Greedy function approximation: a gradient boosting machine</em>", 2001) for <a id="_idIndexMarker572"/>classification and regression problems. Gradient boosting is also known as <strong class="keyword">Gradient Boosting Machine</strong> (<strong class="keyword">GBM</strong>) or <strong class="keyword">Gradient Boosted Regression Tree</strong> (<strong class="keyword">GBRT</strong>). A special case is LambdaMART for ranking applications. Apart from XGBoost; other <a id="_idIndexMarker573"/>implementations are Microsoft's Light Gradient Boosting Machine (LightGBM), and Yandex's Catboost.</p>
    <p class="normal">Gradient <a id="_idIndexMarker574"/>Boosted Trees is an ensemble of trees. This is similar to Bagging algorithms such as Random Forest; however, since this is a boosting algorithm, each tree is computed to incrementally reduce the error. With each new iteration a tree is greedily chosen and its prediction is added to the previous predictions based on a weight term. There is also a regularization term that <a id="_idIndexMarker575"/>penalizes complexity and reduces overfitting, similar to the Regularized Greedy Forest (RGF).</p>
    <p class="normal">The <strong class="keyword">XGBoost</strong> algorithm was published in 2016 by Tianqi Chen and Carlos Guestrin ("<em class="italic">XGBoost: A Scalable Tree Boosting System</em>") and pushed the envelope on many classification and regression benchmarks. It was used in many winning solutions to Kaggle problems. In fact, in 2015, of the 29 challenge-winning solutions, 17 solutions used XGBoost.</p>
    <p class="normal">It was designed <a id="_idIndexMarker576"/>to be highly scalable and features extensions of the gradient boosting algorithm for weighted quantiles, along with improvements for scalability and parallelization based on smarter caching patterns, sharding, and the handling of sparsity.</p>
    <p class="normal">As a special case of regression, XGBoost can be used for forecasting. In this scenario, the model is trained based on past values to predict future values, and this can be applied to univariate as well as multivariate time-series. </p>
    <h1 id="_idParaDest-119" class="title">Python exercise</h1>
    <p class="normal">Let's put into practice what we've learned in this chapter so far. </p>
    <p class="normal">As for <a id="_idIndexMarker577"/>requirements, in this chapter, we'll be installing requirements for each section separately. The installation can be performed from the terminal, the notebook, or from the anaconda navigator. </p>
    <p class="normal">In a few of the following sections, we'll demonstrate classification in a forecast, so some of these approaches will not be comparable. The reader is invited to do forecasts and classification using each approach and then compare results.</p>
    <p class="normal">As a note of caution, both Kats and Greykite (at the time of writing) are very new libraries, so there <a id="_idIndexMarker578"/>might still be frequent changes to dependencies. They might pin your NumPy version or other commonly used libraries. Therefore, I'd recommend you install them in virtual environments separately for each section. </p>
    <p class="normal">We'll go through this setup in the next section.</p>
    <h2 id="_idParaDest-120" class="title">Virtual environments</h2>
    <p class="normal">In a Python virtual environment, all libraries, binaries, and scripts installed into it are¬†isolated from those installed in other virtual environments and from those installed in the system. This means <a id="_idIndexMarker579"/>that we can have different libraries, such as Kats and Greykite, installed without having to bother with compatibility issues between them or with other libraries installed on our computer.</p>
    <p class="normal">Let's go through <a id="_idIndexMarker580"/>a quick tutorial introduction to using virtual environments with Jupyter notebooks using anaconda (similarly, you can use tools such as virtualenv or¬†pipenv).</p>
    <p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to Time-Series with Python</em>, we went through the installation of Anaconda, so we'll skip the installation. Please refer to that chapter or¬†go to conda.io for instructions.</p>
    <p class="normal">To create a virtual environment, you have to specify a name: </p>
    <pre class="programlisting con"><code class="hljs-con">conda create --name myenv
</code></pre>
    <p class="normal">This will create an eponymous directory (<code class="Code-In-Text--PACKT-">myenv</code>), where all libraries and scripts will be installed.</p>
    <p class="normal">If we want to use this environment, we have to activate it first, which means that we set the <code class="Code-In-Text--PACKT-">PATH</code> variable to include our newly created directory:</p>
    <pre class="programlisting con"><code class="hljs-con">conda activate myenv
</code></pre>
    <p class="normal">We can now use tools such as pip, which will default to the one bundled with conda, or the conda command directly to install libraries.</p>
    <p class="normal">We can install Jupyter or Jupyter labs into our environment and then start it. This means that our Jupyter environment will include all dependencies as we've installed them in isolation. </p>
    <p class="normal">Let's start <a id="_idIndexMarker581"/>with a kNN algorithm with dynamic <a id="_idIndexMarker582"/>time warping. As I've mentioned, this¬†algorithm often serves as a decent baseline for comparisons.</p>
    <h2 id="_idParaDest-121" class="title">K-nearest neighbors with dynamic time warping in Python</h2>
    <p class="normal">In this section, we'll classify failures from force and torque measurements of a robot over time. </p>
    <p class="normal">We'll use <a id="_idIndexMarker583"/>a very simple <a id="_idIndexMarker584"/>classifier, kNN, and <a id="_idIndexMarker585"/>perhaps we should give a heads-up that this method involves taking point-wise distances, which can often be a bottleneck for computations.</p>
    <p class="normal">In this section, we'll combine TSFresh's feature extraction in a pipeline with a kNN algorithm. The time-series pipeline can really help make things easy, as you'll find when reading the code snippets.</p>
    <p class="normal">Let's install tsfresh and tslearn:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install tsfresh tslearn
</code></pre>
    <p class="normal">We'll use the kNN classifier in tslearn. We could even have used the kNN classifier in scikit-learn, which allows a custom metric to be specified.</p>
    <p class="normal">In the example, we will download a dataset of robotic execution failures from the UCI machine learning repository and store it locally. This dataset contains force and torque measurements on a robot after failure detection. For each sample, the task is to classify whether the robot will report a failure:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tsfresh.examples <span class="hljs-keyword">import</span> load_robot_execution_failures
<span class="hljs-keyword">from</span> tsfresh.examples.robot_execution_failures <span class="hljs-keyword">import</span> download_robot_execution_failures
download_robot_execution_failures()
df_ts, y = load_robot_execution_failures()
</code></pre>
    <p class="normal">The columns include the time and six time-series with signals from the sensors, <code class="Code-In-Text--PACKT-">F_x</code>,<code class="Code-In-Text--PACKT-"> F_y</code>,<code class="Code-In-Text--PACKT-"> F_z</code>,<code class="Code-In-Text--PACKT-"> T_x</code>,<code class="Code-In-Text--PACKT-"> T_y</code>, and<code class="Code-In-Text--PACKT-"> T_z</code>. The target variable, <code class="Code-In-Text--PACKT-">y</code>, which can take the values True or False, indicates if there was a failure. </p>
    <p class="normal">It's always important to check the frequency of the two classes:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">f"</span><span class="hljs-subst">{y.mean():</span><span class="hljs-number">.2</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">The mean of y is 0.24.</p>
    <p class="normal">We can <a id="_idIndexMarker586"/>then extract <a id="_idIndexMarker587"/>time-series features <a id="_idIndexMarker588"/>using TSFresh, as discussed in <em class="italic">Chapter 3, Preprocessing Time-Series</em>. We can impute missing values and select features based on relevance to the target. In TSFresh, the p-value from a statistical test is used to calculate the feature significance:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tsfresh <span class="hljs-keyword">import</span> extract_features
<span class="hljs-keyword">from</span> tsfresh <span class="hljs-keyword">import</span> select_features
<span class="hljs-keyword">from</span> tsfresh.utilities.dataframe_functions <span class="hljs-keyword">import</span> impute
extracted_features = impute(extract_features(df_ts, column_id=<span class="hljs-string">"id"</span>, column_sort=<span class="hljs-string">"time"</span>))
features_filtered = select_features(extracted_features, y)
</code></pre>
    <p class="normal">We can continue working with the <code class="Code-In-Text--PACKT-">features_filtered</code> DataFrame, which contains our features ‚Äì sensor signals from before and TSFresh features.</p>
    <p class="normal">Let's find some good values for the number of neighbors by doing a grid search:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> TimeSeriesSplit, GridSearchCV
<span class="hljs-keyword">from</span> tslearn.neighbors <span class="hljs-keyword">import</span> KNeighborsTimeSeriesClassifier
knn = KNeighborsTimeSeriesClassifier()
param_search = {
    <span class="hljs-string">'metric'</span> : [<span class="hljs-string">'dtw'</span>],
    <span class="hljs-string">'n_neighbors'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]
}
tscv = TimeSeriesSplit(n_splits=<span class="hljs-number">2</span>)
gsearch = GridSearchCV(
    estimator=knn,
    cv=tscv,
    param_grid=param_search
)
gsearch.fit(
    features_filtered,
    y
)
</code></pre>
    <p class="normal">We are <a id="_idIndexMarker589"/>using scikit-learn's <code class="Code-In-Text--PACKT-">TimeSeriesSplit</code> to split the time-series. This is for the GridSearch. </p>
    <p class="normal">Alternatively, we <a id="_idIndexMarker590"/>could have <a id="_idIndexMarker591"/>just split based on an index.</p>
    <p class="normal">There are many parameters we could have tried, especially for the distance metric in¬†the kNN classifier. If you want to have a play with this, please see <code class="Code-In-Text--PACKT-">TSLEARN_VALID_METRICS</code> for a complete list of metrics supported by tslearn. </p>
    <p class="normal">Let's do a few forecasts of COVID cases. In the next section, we'll start with the Silverkite algorithm. Silverkite comes with the Greykite library released by LinkedIn¬†in 2021. </p>
    <h2 id="_idParaDest-122" class="title">Silverkite</h2>
    <p class="normal">At the time of writing, Greykite is in version 0.1.1 ‚Äì it's not fully stable yet. Its dependencies <a id="_idIndexMarker592"/>might conflict with newer versions of commonly <a id="_idIndexMarker593"/>used libraries, including Jupyter Notebooks. Do not worry though if you install the library in your¬†virtual environment or on Google Colab. </p>
    <p class="normal">Just go ahead and install the library with all its dependencies:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install greykite
</code></pre>
    <p class="normal">Now that Greykite is installed, we can use it.</p>
    <p class="normal">We'll load up the COVID cases from the <em class="italic">Our World in Data</em> dataset, probably one of the best sources of available COVID data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
owid_covid = pd.read_csv(<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">https://covid.ourworldindata.org/data/owid-covid-data.csv</strong></span><span class="hljs-string">"</span>)
owid_covid[<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>] = pd.to_datetime(owid_covid[<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>])
df = owid_covid[owid_covid.location == <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">France</strong></span><span class="hljs-string">"</span>].set_index(<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>, drop=<span class="hljs-literal">True</span>).resample(<span class="hljs-string">'</span><span class="code-highlight"><strong class="hljs-slc">D</strong></span><span class="hljs-string">'</span>).interpolate(method=<span class="hljs-string">'</span><span class="code-highlight"><strong class="hljs-slc">linear</strong></span><span class="hljs-string">'</span>)
</code></pre>
    <p class="normal">We are concentrating on cases in France. </p>
    <p class="normal">We start by setting up the Greykite metadata parameters. We'll then pass this object into the forecaster configuration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> greykite.framework.templates.autogen.forecast_config <span class="hljs-keyword">import</span> (
    ForecastConfig, MetadataParam
)
metadata = MetadataParam(
    time_col=<span class="hljs-string">"date"</span>,
    value_col=<span class="hljs-string">"new_cases"</span>,
    freq=<span class="hljs-string">"D"</span>
)
</code></pre>
    <p class="normal">Our time column is <code class="Code-In-Text--PACKT-">date</code> and our value column is <code class="Code-In-Text--PACKT-">new_cases</code>. </p>
    <p class="normal">We'll now create the <code class="Code-In-Text--PACKT-">forecaster</code> object, which creates forecasts and stores the result: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> warnings
<span class="hljs-keyword">from</span> greykite.framework.templates.forecaster <span class="hljs-keyword">import</span> Forecaster
<span class="hljs-keyword">from</span> greykite.framework.templates.model_templates <span class="hljs-keyword">import</span> ModelTemplateEnum
forecaster = Forecaster()
    warnings.filterwarnings(<span class="hljs-string">"ignore"</span>, category=UserWarning)
    result = forecaster.run_forecast_config(
        df=yahoo_df,
        config=ForecastConfig(
            model_template=ModelTemplateEnum.SILVERKITE_DAILY_90.name,
            forecast_horizon=<span class="hljs-number">90</span>,
            coverage=<span class="hljs-number">0.95</span>,
            metadata_param=metadata
        )
    ) 
</code></pre>
    <p class="normal">The forecast <a id="_idIndexMarker594"/>horizon is 90 days; we will forecast 90 days ahead. Our prediction interval is 95%. Both Silverkite and Prophet support quantifying <a id="_idIndexMarker595"/>uncertainty by means of prediction intervals. A coverage of 95% means that 95% of actuals should fall within the prediction interval. In Greykite, the _<code class="Code-In-Text--PACKT-">components.uncertainty</code> model provides additional configuration options about uncertainty.</p>
    <p class="normal">I've added a line to ignore warnings of the <code class="Code-In-Text--PACKT-">UserWarning</code> type during training since otherwise, there are about 500 lines of warnings about 0s in the target column.</p>
    <p class="normal">Let's plot the original time-series from the result object. We can overlay our forecasts: </p>
    <pre class="programlisting code"><code class="hljs-code">forecast = result.forecast
forecast.plot().show(renderer="<span class="code-highlight"><strong class="hljs-slc">colab</strong></span>")
</code></pre>
    <p class="normal">Please leave out the <code class="Code-In-Text--PACKT-">renderer</code> argument if you are not on Google Colab!</p>
    <p class="normal">We get the following plot: </p>
    <figure class="mediaobject"><img src="../Images/B17577_07_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 7.4: Forecast versus actual time-series (Silverkite)</p>
    <p class="normal">The forecasts <a id="_idIndexMarker596"/>are in the <code class="Code-In-Text--PACKT-">df</code> attribute of the <code class="Code-In-Text--PACKT-">forecast</code> <a id="_idIndexMarker597"/>object:</p>
    <pre class="programlisting code"><code class="hljs-code">forecast.df.head().<span class="hljs-built_in">round</span>(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">These are the upper and lower confidence intervals of the forecasts:</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_05.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_wsZ8St/Screenshot 2021-08-30 at 21.25.06.png"/></figure>
    <p class="packt_figref">Figure 7.5: Table of forecast versus actual time-series (Silverkite)</p>
    <p class="normal">We might <a id="_idIndexMarker598"/>want to get some performance metrics for our model. We <a id="_idIndexMarker599"/>can get the performance of the historical forecast on the holdout test set like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict
backtest = result.backtest
backtest_eval = defaultdict(<span class="hljs-built_in">list</span>)
<span class="hljs-keyword">for</span> metric, value <span class="hljs-keyword">in</span> backtest.train_evaluation.items():
    backtest_eval[metric].append(value)
    backtest_eval[metric].append(backtest.test_evaluation[metric])
metrics = pd.DataFrame(backtest_eval, index=[<span class="hljs-string">"train"</span>, <span class="hljs-string">"test"</span>]).T
metrics.head()
</code></pre>
    <p class="normal">Our performance metrics look like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_06.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_UgQpeb/Screenshot 2021-08-30 at 21.28.22.png"/></figure>
    <p class="packt_figref">Figure 7.6: Performance metrics on the hold-out data (Silverkite)</p>
    <p class="normal">I've <a id="_idIndexMarker600"/>truncated the metrics to the first five.</p>
    <p class="normal">We can <a id="_idIndexMarker601"/>apply our model conveniently to new data like this:</p>
    <pre class="programlisting code"><code class="hljs-code">model = result.model
future_df = result.timeseries.make_future_dataframe(
    periods=<span class="hljs-number">4</span>,
    include_history=<span class="hljs-literal">False</span>
)
model.predict(future_df)
</code></pre>
    <p class="normal">The predictions look like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_07.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_LxjFCi/Screenshot 2021-08-30 at 21.31.08.png"/></figure>
    <p class="packt_figref">Figure 7.7: Forecast DataFrame (Silverkite)</p>
    <p class="normal">Please note that your result might vary.</p>
    <p class="normal">We can <a id="_idIndexMarker602"/>use other forecaster models by changing the <code class="Code-In-Text--PACKT-">model_template</code> argument in the run configuration of the forecaster. For instance, we could set <a id="_idIndexMarker603"/>it to <code class="Code-In-Text--PACKT-">ModelTemplateEnum.PROPHET.name</code> in order to take Facebook's Prophet model.</p>
    <p class="normal">This concludes our tour of Silverkite. Next, we will forecast by applying a supervised regression method with XGBoost. Let's do some gradient boosting!</p>
    <h2 id="_idParaDest-123" class="title">Gradient boosting</h2>
    <p class="normal">We can use supervised machine learning for time-series forecasting as well. For this, we can use <a id="_idIndexMarker604"/>the dates and the previous values to predict the future. </p>
    <p class="normal">First, we need to install XGBoost: </p>
    <pre class="programlisting con"><code class="hljs-con">pip install xgboost
</code></pre>
    <p class="normal">We'll use <a id="_idIndexMarker605"/>the Yahoo daily closing data in this example, as in other practice sections of¬†this chapter. </p>
    <p class="normal">Let's go through the preparation and modeling step by step. </p>
    <p class="normal">We first need to featurize the data. Here, we'll do this by extracting date features, but please see the section on kNNs, where TSFresh's feature extraction is used instead. You might want to change this example by combining the two feature extraction strategies or by relying on TSFresh entirely. </p>
    <p class="normal">We will reload the new COVID cases from the <em class="italic">Our World in Data</em> dataset as before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
owid_covid = pd.read_csv(<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">https://covid.ourworldindata.org/data/owid-covid-data.csv</strong></span><span class="hljs-string">"</span>)
owid_covid[<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>] = pd.to_datetime(owid_covid[<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>])
df = owid_covid[owid_covid.location == <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">France</strong></span><span class="hljs-string">"</span>].set_index(<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>, drop=<span class="hljs-literal">True</span>).resample(<span class="hljs-string">'</span><span class="code-highlight"><strong class="hljs-slc">D</strong></span><span class="hljs-string">'</span>).interpolate(method=<span class="hljs-string">'</span><span class="code-highlight"><strong class="hljs-slc">linear</strong></span><span class="hljs-string">'</span>).reset_index()
</code></pre>
    <p class="normal">For feature extraction, transformers are handy. A transformer is basically a class with <code class="Code-In-Text--PACKT-">fit()</code> and <code class="Code-In-Text--PACKT-">transform()</code> methods that can make the transformer adapt to a dataset and transform <a id="_idIndexMarker606"/>the data accordingly. Here's the code for the <code class="Code-In-Text--PACKT-">DateFeatures</code> transformer <a id="_idIndexMarker607"/>that annotates a dataset according to a date:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.base <span class="hljs-keyword">import</span> TransformerMixin, BaseEstimator
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">DateFeatures</span><span class="hljs-class">(</span><span class="hljs-params">TransformerMixin, BaseEstimator</span><span class="hljs-class">):</span>
    features = [
        <span class="hljs-string">"hour"</span>,
        <span class="hljs-string">"year"</span>,
        <span class="hljs-string">"day"</span>,
        <span class="hljs-string">"weekday"</span>,
        <span class="hljs-string">"month"</span>,
        <span class="hljs-string">"quarter"</span>,
    ]
    
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>().__init__()
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">transform</span><span class="hljs-function">(</span><span class="hljs-params">self, df: pd.DataFrame</span><span class="hljs-function">):</span>
        Xt = []
        <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> df.columns:
            <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> self.features:
                date_feature = <span class="hljs-built_in">getattr</span>(
                    <span class="hljs-built_in">getattr</span>(
                        df[col], <span class="hljs-string">"dt"</span>
                    ), feature
                )
                date_feature.name = <span class="hljs-string">f"</span><span class="hljs-subst">{col}</span><span class="hljs-string">_</span><span class="hljs-subst">{feature}</span><span class="hljs-string">"</span>
                Xt.append(date_feature)
        
        df2 = pd.concat(Xt, axis=<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> df2
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">fit</span><span class="hljs-function">(</span><span class="hljs-params">self, df: pd.DataFrame, y=</span><span class="hljs-literal">None</span><span class="hljs-params">, **fit_params</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">return</span> self
</code></pre>
    <p class="normal">This transformer is relatively simple in that it extracts a range of features for a date column such as hours, years, days, weekday, months, week of year, and quarter. These features can potentially be very powerful for describing or annotating the time-series data in a machine learning context.</p>
    <p class="normal">You can <a id="_idIndexMarker608"/>find the complete code for this example on GitHub. I am providing an additional transformer for cyclical features there that are omitted from this chapter.</p>
    <p class="normal">We apply <a id="_idIndexMarker609"/>the transformers as follows to the <code class="Code-In-Text--PACKT-">date</code> column of the DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.compose <span class="hljs-keyword">import</span> ColumnTransformer
<span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> Pipeline, make_pipeline
preprocessor = ColumnTransformer(
    transformers=[(
        <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>,
        make_pipeline(
            DateFeatures(),
            ColumnTransformer(transformers=[
                (<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">cyclical</strong></span><span class="hljs-string">"</span>, CyclicalFeatures(),
                  [<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date_day</strong></span><span class="hljs-string">"</span>, <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date_weekday</strong></span><span class="hljs-string">"</span>, <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date_month</strong></span><span class="hljs-string">"</span>]
                )
            ], remainder=<span class="hljs-string">"passthrough"</span>)
        ), [<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>],
  ),], remainder=<span class="hljs-string">"passthrough"</span>
)
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">remainder="passthrough"</code> argument is set in case we want to provide additional exogenous features for prediction.<code class="Code-In-Text--PACKT-"> </code></p>
    <p class="normal">We can define a pipeline of these preprocessing steps together with a model so that it can be fitted and applied to prediction:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> xgboost <span class="hljs-keyword">import</span> XGBRegressor
pipeline = Pipeline(
    [
        (<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">preprocessing</strong></span><span class="hljs-string">"</span>, preprocessor),
         (<span class="hljs-string">"xgb"</span>, XGBRegressor(objective=<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">reg:squarederror</strong></span><span class="hljs-string">"</span>, n_estimators=<span class="code-highlight"><strong class="hljs-slc">1000</strong></span>))
    ]
)
</code></pre>
    <p class="normal">The predictor <a id="_idIndexMarker610"/>is an XGBoost regressor. I didn't make much <a id="_idIndexMarker611"/>of an effort in terms of tweaking. The only parameter that we'll change is the number of estimators. We'll use an ensemble size (number of trees) of 1,000. </p>
    <p class="normal">Now it's time to split the dataset into training and test sets. This includes two issues:</p>
    <ul>
      <li class="bullet">We need to align the features with values ahead of time</li>
      <li class="bullet">We need to split the dataset into two by a cutoff time</li>
    </ul>
    <p class="normal">Let's first set the basic parameters for this. First, we want to predict into the future given a time horizon. Second, we need to decide how many data points we use for training and for testing:</p>
    <pre class="programlisting code"><code class="hljs-code">TRAIN_SIZE = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(df) * <span class="code-highlight"><strong class="hljs-slc">0.9</strong></span>)
HORIZON = <span class="code-highlight"><strong class="hljs-slc">1</strong></span>
TARGET_COL = <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">new_cases</strong></span><span class="hljs-string">"</span>
</code></pre>
    <p class="normal">We take 90% of points for training, and we predict 90 days into the future:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train, X_test = df.iloc[HORIZON:TRAIN_SIZE], df.iloc[TRAIN_SIZE+HORIZON:]
y_train = df.shift(periods=HORIZON).iloc[HORIZON:TRAIN_SIZE][TARGET_COL]
y_test = df.shift(periods=HORIZON).iloc[TRAIN_SIZE+HORIZON:][TARGET_COL]
</code></pre>
    <p class="normal">This does both the alignment and horizon. Therefore, we have the datasets for testing and training, both with features and labels that we want to predict with XGBoost.</p>
    <p class="normal">Now we <a id="_idIndexMarker612"/>can train our XGBoost regression model <a id="_idIndexMarker613"/>to predict values within our HORIZON into the future based on the features we produced with our transformer and the current values.</p>
    <p class="normal">We can fit our pipeline as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">FEATURE_COLS = [<span class="hljs-string">"date"</span>]
pipeline.fit(X_train[FEATURE_COLS], y_train)
</code></pre>
    <p class="normal">We can see the following pipeline parameters:</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_08.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_VPuhp5/Screenshot 2021-08-30 at 23.08.52.png"/></figure>
    <p class="packt_figref">Figure 7.8: Pipeline parameters</p>
    <p class="normal">If we create a series of the dates from beginning to end, we can get the predictions of the model for the whole time period:</p>
    <pre class="programlisting code"><code class="hljs-code">MAX_HORIZON = <span class="code-highlight"><strong class="hljs-slc">90</strong></span>
X_test_horizon = pd.Series(pd.date_range(
    start=df.date.<span class="code-highlight"><strong class="hljs-slc">min()</strong></span>, 
    periods=<span class="code-highlight"><strong class="hljs-slc">len</strong></span>(df) + MAX_HORIZON,
    name=<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>
)).reset_index()
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">predict()</code> method of the pipeline applied to <code class="Code-In-Text--PACKT-">X_test</code> gives us the forecast: </p>
    <pre class="programlisting code"><code class="hljs-code">forecasted = pd.concat(
    [pd.Series(pipeline.predict(X_test_horizon[FEATURE_COLS])), pd.Series(X_test_horizon.date)],
    axis=<span class="hljs-number">1</span>
)
forecasted.columns = [TARGET_COL, <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>]
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker614"/>do the same for the actual cases:</p>
    <pre class="programlisting code"><code class="hljs-code">actual = pd.concat(
    [pd.Series(df[TARGET_COL]), pd.Series(df.date)],
    axis=<span class="hljs-number">1</span>
)
actual.columns = [TARGET_COL, <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>]
</code></pre>
    <p class="normal">Now, we <a id="_idIndexMarker615"/>can contrast the forecast with the actual values, <code class="Code-In-Text--PACKT-">y_test</code>, in a plot:</p>
    <pre class="programlisting code"><code class="hljs-code">fig, ax = plt.subplots(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
forecasted.set_index(<span class="hljs-string">"date"</span>).plot(linestyle=<span class="hljs-string">'--'</span>, ax=ax)
actual.set_index(<span class="hljs-string">"date"</span>).plot(linestyle=<span class="hljs-string">'-.'</span>, ax=ax)
plt.legend([<span class="hljs-string">"forecast"</span>, <span class="hljs-string">"actual"</span>])
</code></pre>
    <p class="normal">This is the plot we are getting:</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_09.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_Vnji3v/Screenshot 2021-08-30 at 23.29.17.png"/></figure>
    <p class="packt_figref">Figure 7.9: Forecast versus actual (XGBoost)</p>
    <p class="normal">We can <a id="_idIndexMarker616"/>extract performance metrics over the test <a id="_idIndexMarker617"/>period like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error
test_data = actual.merge(forecasted, on=<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>, suffixes=(<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">_actual</strong></span><span class="hljs-string">"</span>, <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">_predicted</strong></span><span class="hljs-string">"</span>))
mse = mean_squared_error(test_data.new_cases_actual, test_data.new_cases_predicted, squared=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># RMSE</span>
<span class="code-highlight"><strong class="hljs-slc">print("The root mean squared error (RMSE) on test set: {:.2f}".format(mse))</strong></span>
</code></pre>
    <p class="normal">We should be seeing something like this:</p>
    <pre class="programlisting con"><code class="hljs-con">The root mean squared error (RMSE) on test set: 12753.41
</code></pre>
    <p class="normal">Next, we'll create an ensemble model for time-series forecasting in Kats.</p>
    <h2 id="_idParaDest-124" class="title">Ensembles with Kats</h2>
    <p class="normal">The Kats installation should be very easy in two steps. First, we install fbprophet, an old <a id="_idIndexMarker618"/>version of Facebook's Prophet library:</p>
    <pre class="programlisting con"><code class="hljs-con">conda install -c conda-forge fbprophet 
</code></pre>
    <p class="normal">Now we <a id="_idIndexMarker619"/>install Kats with pip:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install kats
</code></pre>
    <p class="normal">Alternatively, on Colab, we can install Kats like this:</p>
    <pre class="programlisting con"><code class="hljs-con">!MINIMAL=1 pip install kats
!pip install "numpy==1.20"
</code></pre>
    <p class="normal">We'll load the COVID cases dataset as before. Here's just the last line:</p>
    <pre class="programlisting code"><code class="hljs-code">df = owid_covid[owid_covid.location == <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">France</strong></span><span class="hljs-string">"</span>].set_index(<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">date</strong></span><span class="hljs-string">"</span>, drop=<span class="hljs-literal">True</span>).resample(<span class="hljs-string">'</span><span class="code-highlight"><strong class="hljs-slc">D</strong></span><span class="hljs-string">'</span>).interpolate(method=<span class="hljs-string">'</span><span class="code-highlight"><strong class="hljs-slc">linear</strong></span><span class="hljs-string">'</span>).reset_index()
</code></pre>
    <p class="normal">We'll configure our ensemble model, fit it, and then do a forecast.</p>
    <p class="normal">First, the configuration of our ensemble:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> kats.models.ensemble.ensemble <span class="hljs-keyword">import</span> EnsembleParams, BaseModelParams
<span class="hljs-keyword">from</span> kats.models.ensemble.kats_ensemble <span class="hljs-keyword">import</span> KatsEnsemble
<span class="hljs-keyword">from</span> kats.models <span class="hljs-keyword">import</span> linear_model, quadratic_model
model_params = EnsembleParams(
            [
                BaseModelParams(<span class="hljs-string">"linear"</span>, linear_model.LinearModelParams()),
                BaseModelParams(<span class="hljs-string">"quadratic"</span>, quadratic_model.QuadraticModelParams()),
            ]
        )
</code></pre>
    <p class="normal">Here, we include only two different models, but we could have included other and more models, and we could have defined better parameters. This is an example only; for a more realistic exercise, which I leave to the reader, I'd suggest adding ARIMA and Theta models. We need to define hyperparameters for each forecasting model. </p>
    <p class="normal">We also need to create ensemble parameters that define how the ensemble aggregate is to be calculated and how the decomposition should work:</p>
    <pre class="programlisting code"><code class="hljs-code">KatsEnsembleParam = {
    <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">models</strong></span><span class="hljs-string">"</span>: model_params,
    <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">aggregation</strong></span><span class="hljs-string">"</span>: <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">weightedavg</strong></span><span class="hljs-string">"</span>,
    <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">seasonality_length</strong></span><span class="hljs-string">"</span>: <span class="hljs-number">30</span>,
    <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">decomposition_method</strong></span><span class="hljs-string">"</span>: <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">additive</strong></span><span class="hljs-string">"</span>,
}
</code></pre>
    <p class="normal">To use <a id="_idIndexMarker620"/>a time-series with Kats, we have to convert <a id="_idIndexMarker621"/>our data from a DataFrame or series to a Kats time-series object. We can convert our COVID case data as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> kats.consts <span class="hljs-keyword">import</span> TimeSeriesData
TARGET_COL = <span class="hljs-string">"new_cases"</span>
df_ts = TimeSeriesData(
    value=df[TARGET_COL], time=df[<span class="hljs-string">"date"</span>]
)
</code></pre>
    <p class="normal">What is important for the conversion is the fact that Kats can infer the frequency of the index. This can be tested with <code class="Code-In-Text--PACKT-">pd.infer_freq()</code>. In our case, <code class="Code-In-Text--PACKT-">pd.infer_freq(df["date"])</code> should return <code class="Code-In-Text--PACKT-">D </code>for daily frequency.</p>
    <p class="normal">Now we can create our <code class="Code-In-Text--PACKT-">KatsEnsemble </code>and fit it:</p>
    <pre class="programlisting code"><code class="hljs-code">m = KatsEnsemble(
    data=df_ts, 
    params=KatsEnsembleParam
).fit()
</code></pre>
    <p class="normal">We can get separate predictions for each model using the <code class="Code-In-Text--PACKT-">predict()</code> method. If we want to get the ensemble output, we have to call <code class="Code-In-Text--PACKT-">aggregate()</code> after <code class="Code-In-Text--PACKT-">predict()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">m.predict(steps=<span class="hljs-number">90</span>)
m.aggregate()
m.plot()
plt.ylabel(TARGET_COL)
</code></pre>
    <p class="normal">We predict 90 days ahead. These predictions are stored as part of the model, so we don't need to capture the returned forecast. We can then aggregate the forecast from each model. Again, we don't need to get the returned DataFrame because this is stored inside the <a id="_idIndexMarker622"/>model object (<code class="Code-In-Text--PACKT-">m.fcst_df</code>).</p>
    <p class="normal">In the end, we <a id="_idIndexMarker623"/>plot the aggregated DataFrame using a Kats convenience function:</p>
    <figure class="mediaobject"><img src="../Images/B17577_07_10.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_Ou1y0x/Screenshot 2021-08-30 at 23.38.36.png"/></figure>
    <p class="packt_figref">Figure 7.10: Kats ensemble model forecast</p>
    <p class="normal">Since we can tweak this ensemble model by changing the base model parameter and adding <a id="_idIndexMarker624"/>new models, this can give us lots of room <a id="_idIndexMarker625"/>for improvement.</p>
    <p class="normal">It's time to conclude this chapter with a summary of what we've learned.</p>
    <h1 id="_idParaDest-125" class="title">Summary</h1>
    <p class="normal">In this chapter, we've discussed popular time-series machine learning libraries in Python. We then discussed and tried out a k-nearest neighbor algorithm with dynamic time warping for the classification of robotic failures. We talked about validation in time-series forecasting and we tried three different methods for forecasting COVID cases: Silverkite, Gradient Boosting with XGBoost, and ensemble¬†models in Kats.</p>
  </div>
</body></html>
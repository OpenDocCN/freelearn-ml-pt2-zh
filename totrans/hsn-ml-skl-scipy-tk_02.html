<html><head></head><body>
    <section>
      <header class="header-title chapter-title">
                    Introduction to Machine Learning
                </header>
      <article>
        <p class="mce-root">Machine learning is everywhere. When you book a flight ticket, an algorithm decides the price you are going to pay for it. When you apply for a loan, machine learning may decide whether you are going to get it or not. When you scroll through your Facebook timeline, it picks which advertisements to show to you. Machine learning also plays a big role in your Google search results. It organizes your email's inbox and filters out spam, it goes through your resumé before recruiters when you apply for a job, and, more recently, it has also started to play the role of your personal assistant in the form of Siri and other virtual assistants. </p>
        <p class="mce-root">In this book, we will learn about the theory and practice of machine learning. We will understand when and how to apply it. To get started, we will look at a high-level introduction to how machine learning works. You will then be able to differentiate between the different machine learning paradigms and know when to use each of them. Then, you'll be taken through the model development life cycle and the different steps practitioners take to solve problems. Finally, we will introduce you to scikit-learn, and learn why it is the <em>de facto</em> tool for many practitioners.  </p>
        <p>Here is a list of the topics that will be covered in this first chapter:</p>
        <ul>
          <li>Understanding machine learning</li>
          <li>The model development life cycle</li>
          <li>Introduction to scikit-learn</li>
          <li>Installing the packages you need</li>
        </ul>
        <p/>
        <p/>
        <h1 id="uuid-c3c677d0-ab33-43f8-9ff9-cd8a23406828" class="p3">Understanding machine learning</h1>
        <p class="p3">You may be wondering how machines actually learn. To get the answer to this query, let's take the following example of a fictional company. <strong>Space Shuttle Corporation</strong> has a few space vehicles to rent. They get applications every day from clients who want to travel to Mars. They are not sure whether those clients will ever return the vehicles—maybe they'll decide to continue living on Mars and never come back again. Even worse, some of the clients may be lousy pilots and crash their vehicles on the way. So, the company decides to hire shuttle rent-approval officers whose job is to go through the applications and decide who is worthy of a shuttle ride. Their business, however, grows so big that they need to formulate the shuttle-approval process.</p>
        <p class="p3">A traditional shuttle company would start by having business rules and hiring junior employees to execute those rules. For example, if you are an alien, then sorry, you cannot rent a shuttle from us. If you are a human and you have kids that are in school on Earth, then you are more than welcome to rent one of our shuttles. As you can see, those rules are too broad. What about aliens who love living on Earth and just want to go to Mars for a quick holiday? To come up with a better business policy, the company starts hiring analysts. Their job is to go through historical data and try to come up with detailed rules or business logic. These analysts can come up with very detailed rules. If you are an alien, one of your parents is from Neptune, your age is between 0.1 and 0.2 Neptunian years, and you have 3 to 4 kids and one of them is 80% or more human, then you are allowed to rent a shuttle. To be able to come up with suitable rules, the analysts also need a way to measure how good this business logic is. For example, what percentage of the shuttles return if certain rules are applied? They use historic data to evaluate these measures, and only then can wesay that these rules are actually learned from data.</p>
        <p class="p3">Machine learning works in almost the same way. You want to use historic data to come up with some business logic (an algorithm) in order to optimize some measure of how good the logic is (an objective or loss function). Throughout this book, we will learn about numerous machine learning algorithms; they differ from each other in how they represent business logic, what objective functions they use, and what optimization techniques they utilize to reach a model that maximizes (or sometimes minimizes) the objective function. Like the analysts in the previous example, you should pick an objective function that is as close as possible to your business objective. Any time you hear people saying data scientists should have a good understanding of their business, a significant part of that is their choice of a good objective function and ways to evaluate the models they build. In my example, I quickly picked the percentage of shuttles returned as my objective.</p>
        <p class="p3">But if you think about it, is this really an accurate one-to-one mapping of the shuttle company's revenue? Is the revenue made by allowing a trip equal to the cost of losing a shuttle? Furthermore, rejecting a trip may also cost your company angry calls to the customer care center and negative word-of-mouth advertising. You have to understand all of this well enough before picking your objective function.</p>
        <p class="p3">Finally, a key benefit to using machine learning is that it can iterate over a vast amount of business logic cases until it reaches the optimum objective function, unlike the case of the analysts in our space shuttle company who can only go so far with their rules. The machine learning approach is also automated in the sense that it keeps updating the business logic whenever new data arrives. These two aspects make it scalable, more accurate, and adaptable to change.</p>
        <h2 id="uuid-a108d651-d866-44f2-9f4c-c15e321aa326">Types of machine learning algorithms</h2>
        <div class="p3 packt_quote">"Society is changing, one learning algorithm at a time."</div>
        <div class="p3 packt_quote CDPAlignRight CDPAlign">– Pedro Domingos</div>
        <p class="p3">In this book, we are going to cover the two main paradigms of machine learning—supervised learning and unsupervised learning. Each of these two paradigms has its own sub-branches that will be discussed in the next section. Although it is not covered in this book, reinforcement learning will also be introduced in the next section:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/1ca6ecc7-0e48-4729-9e46-188cf3242dfe.png" style="width:56.50em;"/>
        </p>
        <p class="mce-root"/>
        <p class="p3">Let's use our fictional Space Shuttle Corporation company once more to explain the differences between the different machine learning paradigms.</p>
        <h3 id="uuid-051e0d2e-934f-4f64-b7ee-6c0e5d7bd24f" class="p3">Supervised learning</h3>
        <p class="p3">Remember those old good days at school when you were given examples to practice on, along with the correct answers to them at the end to validate whether you are doing a good job? Then, at exam time, you were left on your own. That's basically what supervised learning is. Say our fictional space vehicle company wants to predict whether travelers will return their space vehicles. Luckily, the company has worked with many travelers in the past, and they already know which of them returned their vehicles and who did not. Think of this data as a spreadsheet, where each column has some information about the travelers—their financial statements, the number of kids they have, whether they are humans or aliens, and maybe their age (in Neptunian years, of course). Machine learners call these columns <strong>features</strong>. There is one extra column for previous travelers that states whether they returned or not; we call this column the <strong>label</strong> or <strong>target</strong> column. In the learning phase, we build a model using the features and targets. The aim of the algorithm while learning is to minimize the differences between its predictions and the actual targets. The difference is what we call the error. Once a model is constructed so that its error is minimal, we then use it to make predictions for newer data points. For new travelers, we only know their features, but we use the model we've just built to predict their corresponding targets. In a nutshell, the presence of the target in our historic data is what makes this process supervised.</p>
        <h4 id="uuid-ea75c451-dd14-48f1-bedb-7a825b11df84">Classification versus regression</h4>
        <p class="p3">Supervised learning is furthersubdivided into classification and regression. For cases where we only have a few predefined labels to predict, we use a classifier—for example, <em>return</em>versus<em>no return</em> or<em>human</em> versus<em>Martian</em> versus<em>Venusian</em>. If what we want to predict is a wide-range number—say, how many years a traveler will take to come back—then it is a regression problem since these values can be anything from 1 or 2 years to 3 years, 5 months, and 7 days.</p>
        <h4 id="uuid-c96768f2-9fe4-4e25-8d0e-9d93db0c073c">Supervised learning evaluation</h4>
        <p class="p3">Due to their differences, the metrics we use to evaluate these classifiers are usually different from ones we use with regression:</p>
        <ul>
          <li><strong>Classifier evaluation metrics</strong>: Suppose we are using a classifier to determine whether a traveler is going to return. Then, of those travelers that the classifier predicted to return, we want to measure what percentage of them actually did return. We call this measure <strong>precision</strong>. Also, of all travelers who did return, we want to measure what percentage of them the classifier correctly predicted to return. We call this <strong>recall</strong>. Precision and recall can be calculated for each class—that is, we can also calculate precision and recall for the travelers who did not return.</li>
        </ul>
        <p style="padding-left: 60px" class="p3"><strong>Accuracy</strong> is another commonly used, and sometimes abused, measure. For each case in our historic data, we know whether a traveler actually returned (<strong>actuals</strong>) and we can also generate <strong>predictions</strong> of whether they will return. The accuracy calculates what percentage of cases of the predictions and actuals match. As you can see, it is labeled <strong>agnostic</strong>, so it can sometimesbe misleading when the classes are highly imbalanced. In our example business, say 99% of our travelers actually return. We can build a dummy classifier that predicts whether every single traveler returns; it will be accurate 99% of the time. This 99% accuracy value doesn't tell us much, especially if you know that in these cases, the recall value for non-returning travelers is 0%. As we are going to see later on in this book, each measure has its pros and cons, and a measure is only as good as how close it is to our business objectives. We are also going to learn about other metrics, such as <strong>F<sub>1</sub> score</strong>, <strong>AUC</strong>, and <strong>log loss</strong>.</p>
        <ul>
          <li><strong>Regressor evaluation metrics</strong>: If we are using a regressor to tell how long a traveler will stay, then we need to determine how far the numbers that the regression evaluation is predicting are from reality. Let's say for three users, the regressor expected them to stay for 6, 9, and 20 years, respectively, while they actually stayed for 5, 10, and 26 years, respectively. One solution is to calculate the average of the differences between the prediction and the reality—the average of 6–5, 9–10, and 20–25, so the average of 1, -1, and -6 is -2. One problem with these calculations is that 1 and -1 cancel each other out. If you think about it, both 1 and -1 are mistakes that the model made, and the sign might not matter much here.</li>
        </ul>
        <p style="padding-left: 60px">So, we will need to use <strong>Mean Absolute Error</strong>(<strong>MAE</strong>) instead. This calculates the average of the absolute values of the differences—so, the average of 1, 1, and 6 is 2.67. This makes more sense now, but what if we can tolerate a 1-year difference more than a 6-year difference? We can then use<strong>Mean Squared Error</strong>(<strong>MSE</strong>) to calculate the average of the differences squared—so, the average of 1, 1, and 36 is 12.67. Clearly, each measure has its pros and cons here as well. Additionally, we can also use different variations of these metrics, such as median absolute error or max error. Furthermore, sometimes your business objective can dictate other measures. Say we want to penalize the model if it predicts that a traveler will arrive 1 year later twice as often as when it predicts them to arrive 1 year earlier—what metric can you come up with then?</p>
        <p class="p3">In practice, the lines between classification and regression problems can get blurred sometimes. For the case of how many years a traveler will take to return, you can still decide to bucket the range into 1–5 years, 5–10 years, and 10+ years. Then, you end up with a classification problem to solve instead. Conversely, classifiers return probabilities along with their predicted targets. For the case of whether a user will return, a predicted value of 60% and 95% means the same thing from a binary classifier's point of view, but the classifier is more confident that the traveler will return in the second case compared to the first case. Although this is still a classification problem, we can use the <strong>Brier score</strong> to evaluate our classifier here, which is actually <strong>MSE</strong> in disguise. More on the Brier score will be covered in <a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=31&amp;action=edit">Chapter 9</a>, <em>The Y is as important as the X</em>. Most of the time, it is clear whether you are facing a classification or regression problem, but always keep your eyes open to the possibility of reformulating your problem if needed. </p>
        <h3 id="uuid-ce5a4ed1-ff4b-4239-ba3e-5220e7e94fab" class="p3">Unsupervised learning</h3>
        <p class="p3">Life doesn't always provide us with correct answers as was the case when we were in school. We have been told that space travelers like it when they are traveling with like-minded passengers. We already know a lot about our travelers, but of course, no traveler will say <em>by the way, I am a type A, B, or C traveler</em>. So, to group our clients, we use a form of unsupervised learning called <strong>clustering</strong>. Clustering algorithms try to come up with groups and put our travelers into them without us telling them what groups may exist. Unsupervised learning lacks targets, but this doesn't mean that we cannot evaluate our clustering algorithms. We want the members of a cluster to be similar to each other, but we also want them to be dissimilar from the members of adjacent clusters. The <strong>silhouette coefficient</strong> basically measures that. We will come across other measures for clustering, such as the <strong>Davies-Bouldin</strong><strong>index</strong> and the <strong>Calinski-Harabasz</strong><strong>index</strong>, later in this book</p>
        <h3 id="uuid-c3c54645-2493-497f-b74d-133408e99af2" class="p3">Reinforcement learning</h3>
        <p class="p3">Reinforcement learning is beyond the scope of this book and is not implemented in <kbd>scikit-learn</kbd>. Nevertheless, I will briefly talk about it here. In the supervised learning examples we have looked at, we treated each traveler separately. If we want to know which travelers are going to return their space vehicles the earliest, our aim then is to pick the best travelers for our business. But if you think about it, the behavior of one traveler affects the experience of the others as well. We only allow space vehicles to stay up to 20 years in space. However, we haven't explored the effect of allowing some travelers to stay longer or the effect of having a stricter rent period for other travelers. Reinforcement learning is the answer to that, where the key to it is exploration and exploitation.</p>
        <p class="p3">Rather than dealing with each action separately, we may want to explore sub-optimal actions in order to reach an overall optimumset of actions. Reinforcement learning is used in robotics, where a robot has a goal and it can only reach it through a sequence of steps—2 steps to the right, 5 steps forward, and so on. We can't tell whether a right versus left step is better on its own; the whole sequence must be found to reach the best outcome. Reinforcement learningis also used in gaming, as well as in recommendation engines. If Netflix only recommended to a user what matches their taste best, a user may end up with nothing but Star Wars movies on their home screen. Reinforcement learning is thenneeded to explore less-optimum matches to enrich the user's overall experience.</p>
        <h1 id="uuid-a2fdf13a-fc0b-4062-b9fb-47c8ce99f477" class="p3">The model development life cycle</h1>
        <p>When asked to solve a problem using machine learning, data scientists achieve this by following a sequence of steps. In this section, we are going to discuss those iterative steps.</p>
        <h2 id="uuid-d77b8486-a98f-498e-9cbb-d860c3d187ff" class="p3">Understanding a problem</h2>
        <div class="p3 packt_quote">"All models are wrong, but some are useful."</div>
        <div class="p3 packt_quote CDPAlignRight CDPAlign">– George Box</div>
        <p class="p3">The first thing to do when developing a model is to understand the problem you are trying to solve thoroughly. This not only involves understanding what problem you are solving, but also why you are solving it, what impact are you expecting to have, and what the currently available solution isthat you are comparing your new solution to. My understanding of what Box said when he stated that all models are wrong is that a model is just an approximation of reality by modeling one or more angles of it. By understanding the problem you are trying to solve, you can decide which angles of reality you need to model, and which ones you can tolerate ignoring.</p>
        <p class="p3">You also need to understand the problem well to decide how to split your data for training and evaluation (more on that in the next section). You can then decide what kind of model to use. Is the problem suitable for supervised or unsupervised learning? Are we better off using classification or regression algorithms for this problem? What kind of classification algorithm will serve us best? Is a linear model good enough to approximate our reality? Do we need the most accurate model or one that we can easily explain to its users and to the business stakeholders?</p>
        <p>Minimal exploratory data analysis can be done here, where you can check whether you have labels and check the cardinality of the labels, if present, to decide whether you are dealing with a classification or a regression problem. I would still save any further data analysis until after the dataset is split into training and test sets. It is important to limit advanced data analysis to the training set only to ensure your model's generalizability. </p>
        <p class="p3">Finally, we need to understand what we are comparing our model to. What is the current baseline that we need to improve on? If there are already business rules in place, then our model has to be better at solving the problem at hand than these rules. To be able to decide how much better it is at solving the problem, we need to use evaluation metrics—metrics that are suitable for our model and also as close as possible to our business requirements. If our aim is to increase revenue, then our metric should be good at estimating the increase in revenue when our model is used, compared to the current status quo. If our aim is to increase repeat purchases regardless of the revenue, then other metrics may be more suitable.</p>
        <h2 id="uuid-881a08b5-8ea7-4f86-adf4-caaeb703cc20" class="p3">Splitting our data</h2>
        <p class="p3">As we have seen in supervised learning, we train our model on a set of data where the correct answers (labels) are given. Learning, however, is only half of the problem. We also want to be able to tell whether the model we built is going to do a good job when used on future data. We cannot foresee the future, but we can use the data we already have to evaluate our model.</p>
        <p class="p3">We do this by splitting our data into parts. We use one part of the data to train the model (the training set) and then use a separate part to evaluate the model (the test set). Since we want our test set to be as close as possible to the future data, there are two key points discussed in the following subsections to keep in mind when splitting our data:</p>
        <ul class="ul1">
          <li class="li3">Finding the best manner to split the data</li>
          <li class="li3">Making sure the training and test datasets are separate</li>
        </ul>
        <h3 id="uuid-277b7294-1995-4cac-949d-79a990dc6bb4" class="p3">Finding the best manner to split the data</h3>
        <p class="p3">Say your users' data is sorted according to their country in alphabetical order. If you just take the first <em>N</em> records for training and the rest for testing, you will end up training the model on users from certain countries and will never let it learn anything about users from, say, Zambia and Zimbabwe. So, one common solution is to randomize your data before splitting it. Random split is not always the best option, however. Say we want to build a model to predict the stock prices or climate change phenomena a few years ahead. To be confident that our system will capture temporal trends such as global warming, we need to split our data based on time. We can train on earlier data and see whether the model can do a good job in predicting more recent data.</p>
        <p class="p3">Sometimes, we just predict rare incidents. It can be that the number of fraud cases that occur in your payment system is 0.1%. If you randomly split your data, you may be unlucky and have the vast majority of the fraud cases in the training data and very few cases in the test data, or vice versa. So, it is advised that you use stratification when it comes to highly unbalanced data. Stratification makes sure that the distribution of your targets is more or less the same in both the training and test datasets.</p>
        <div class="p3 packt_infobox">A stratified sampling strategy is used to make sure that the different subgroups in our population are represented in our samples. If my dataset is made up of 99% males and 1% females, a random sample of the population may end up having only males in it. So, you should separate the male and female populations first, and then take a sample from each one of the two and combine them later to make sure they are both represented in the final sample. The same concept is applied here if we want to make sure all the class labels are present in our training and test sets. Later on in this book, we will be splitting our data using the <kbd>train_test_split()</kbd>function. This function uses the class labels to stratify its samples by default. </div>
        <h3 id="uuid-0608266d-4a35-472b-a7a1-5017e4c1cf63" class="p3">Making sure the training and the test datasets are separate</h3>
        <p class="p3">One of the most common mistakes new data scientists may fall prey to is the look-ahead bias. We use the test dataset to simulate the data we will see in the future, but usually, the test dataset contains information that we can only know after time has passed. Take the case of our example space vehicles; we may have two columns—one saying whether the vehicle returns, and the other saying how long the vehicle will take to return. If we are to build a classifier to predict whether a vehicle will return, we will use the former column as our target, but we will never use the latter column as a feature. We can only know how long a vehicle stayed in outer space once it is actually back. This example looks trivial, but believe me, look-ahead bias is a very common mistake, especially when dealing with less obvious cases than this one.</p>
        <p class="p3">Besides training, you also learn things from the data in order to preprocess it. Say, instead of users' heights in centimeters, you want to have a feature stating whether a user's height is above or below the median. To do that, you need to go through the data and calculate the median. Now, since anything that we learn has to come from the training set itself, we also need to learn this median from the training set and not from the entire dataset. Luckily, in all the data preprocessing functions of scikit-learn, there are separate methods for the <kbd>fit()</kbd>, <kbd>predict()</kbd>, and <kbd>transform()</kbd> functions. This makes sure that anything learned from the data (via the <kbd>fit()</kbd> method) is only learned from the training dataset, and then it can be applied to the test set (via the <kbd>predict()</kbd> and/or <kbd>transform()</kbd> methods).</p>
        <h3 id="uuid-edc914cc-12a7-4732-9ad7-823d01f6076e" class="p3">Development set</h3>
        <p class="p3">When developing a model, we need to try multiple configurations of the model to decide which configuration gives the best results. To be able to do so, we usually split the training dataset further into training and development sets. Having two new subsets allows us to try different configurations when training on one of the two subsets and evaluating the effect of those configuration changes on the other. Once we find the best configuration, we evaluate our model with its final configuration on the test set. In <a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=25&amp;action=edit">Chapter 2</a>, <em>Making Decisions with Trees</em>, we will do all this in practice. Note that I will be using the terms <em>model configuration</em> and <em>hyperparameters</em> interchangeably.</p>
        <h2 id="uuid-660e349c-9f13-4278-bffa-dffd223cb545" class="p3">Evaluating our model</h2>
        <p class="p3">Evaluating your model's performance is essential in picking the best algorithm for the job and to be able to estimate how your model will perform in real life. As Box said, a model that is wrong can still be useful. Take the example of a web start-up. They run an ad campaign where they are paid $1 for each view they get, and they know that for every 100 viewers, only one viewer signs up and buys stuff for $50. In other words, they have to spend $100 to make $50. Obviously, that's a bad <strong>Return of Investment</strong> (<strong>ROI</strong>) for their business. Now, what if you create a model for them that can pick users for them to target, but your new model is only correct 10% of the time? Is 10% precision good or bad, in this case? Well, of course, this model is wrong 90% of the time, which may sound like a very bad model, but if we calculate ROI now, then for every $100 they spend, they make $500. Well, I would definitely pay you to build me this model that is quite wrong, yet quite useful!</p>
        <p class="p3">scikit-learn provides a large number of evaluation metrics that we will be using to evaluate the models we build in this book. But remember, a metric is only useful if you really understand the problem you are solving and its business impact.</p>
        <h2 id="uuid-67e8132c-aa66-4213-b76a-279761424c7d" class="p3">Deploying in production and monitoring</h2>
        <p class="p3">The main reason that many data scientists use Python for machine learning instead of R, for example, is that it makes it easier to productionize your code. Python has plenty of web frameworks to build APIs with and put the machine learning models behind. It is also supported by all cloud providers. I find it important that the team developing a model is also responsible for deploying it in production. Building your model in one language and then asking another team to port it into another language is error-prone. Of course, having one person or team building and deploying models may not be feasible in larger companies or due to other implementation constraints.</p>
        <p class="p3">However, keeping the two teams in close contact and making sure that the ones developing the model can still understand the production code is essential and helps to minimize errors on account of development and production code inconsistency.</p>
        <p class="p3">We try our best not to have any look-ahead bias when training our models. We hope data doesn't change after our models are trained, and we want our code to be bug-free. However, we cannot guarantee any of this. We may overlook the fact that the user's credit score is only added to the database after they make their first purchase. We may not know that our developers decided to switch to the metric system to specify our inventory's weights while it was saved in pounds when the model was trained. Because of that, it is important to log all the predictions your model makes to be able to monitor its performance in real life and compare it to the test set's performance. You can also log the test set's performance every time you retrain the model or keep track of the target's distribution over time.</p>
        <h2 id="uuid-951d9528-bd5d-41bb-ab87-29070e76123b" class="p3">Iterating</h2>
        <p class="p3">Often, when you deploy a model, you end up with more data. Furthermore, the performance of your model is not guaranteed to be the same when deployed in production. This can be due to some implementation issues or mistakes that took place during the evaluation process. Those two points mean that the first version of your solution is always up for improvement. Starting with simple solutions (that can be improved via iterations) is an important concept for agile programming and is a paramount concept for machine learning.</p>
        <p>This whole process, from understanding theproblemto monitoring the ongoing improvements on the solution, requires tools that allow us to iterate quickly and efficiently. In the next section, we will introduce you to scikit-learn and explain why many machine learning practitioners consider it the right tool for the job.</p>
        <h2 id="uuid-bae7e65a-a5f3-4bca-b7d0-81fed5e64ff9" class="mce-root">When to use machine learning</h2>
        <div class="packt_quote">"Pretty much anything that a normal person can do in less than 1 second, we can now automate with AI."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– Andrew Ng</div>
        <p>One additional note before moving on to the next section is that when faced with a problem, you have to decide whether machine learning is apt for the task. Andrew Ng's 1-second rule is a good heuristic for you to estimate whether a machine learning-based solution will work. The main reason behind this is that computers are good with patterns. They are way better than humans at picking repeated patterns and acting on them.</p>
        <p class="mce-root"/>
        <p>Once they identify the same pattern over and over again, it is easy to codify them to make the same decisions every time. In the same manner, computers are also good with tactics. In 1908, Richard Teichmann stated that a game of chess is 99% basedontactics. Maybe that's why computers have beat humans in chess since 1997. If we are to believe Teichmann's statement, then the remaining 1% is strategy. Unlike tactics, strategy is the arena where humans beat machines. If the problem you want to solve can be formulated as a set of tactics, then go for machine learning and leave the strategic decisions for humans to make. In the end, most of our day-to-day decisions are tactical. Furthermore, one man's strategy is often someone else's tactics.  </p>
        <h1 id="uuid-eef5e16f-f194-473a-98e3-9a6b5b21037d" class="p3">Introduction to scikit-learn</h1>
        <p class="p3">Since you have already picked up this book, you probably don't need me to convince you why machine learning is important. However, you may still have doubts about why to use scikit-learn in particular. You may encounter names such as TensorFlow, PyTorch, and Spark more often during your daily news consumption than scikit-learn. So, let me convince you of my preference for the latter.</p>
        <h2 id="uuid-54a10c9e-4546-41c7-84a5-c4b05df70d8c" class="p3">It plays well with the Python data ecosystem</h2>
        <p class="p3">scikit-learn is a Python toolkit built on top of NumPy, SciPy, and Matplotlib. These choices mean that it fits well into your daily data pipeline. As a data scientist, Python is most likely your language of choice since it is good for both offline analysis and real-time implementations. You will also be using tools such as <kbd>pandas</kbd> to load data from your database, which allows you to perform a vast amount of transformation to your data. Since both <kbd>pandas</kbd> and scikit-learn are built on top of NumPy, they play very well with each other. Matplotlib is the <em>de facto</em> data visualization tool for Python, which means you can use its sophisticated data visualization capabilities to explore your data and unravel your model's ins and outs. </p>
        <p class="p3">Since it is an open source tool that is heavily used in the community, it is very common to see other data tools use an almost identical interface to scikit-learn. Many of these tools are built on top of the same scientific Python libraries, and they are collectively known as <strong>SciKits</strong> (short for <strong>SciPy</strong><strong>Toolkits</strong>)—hence, the <em>scikit</em> prefix in scikit-learn. For example, <kbd>scikit-image</kbd> is a library for image processing, while <kbd>categorical-encoding</kbd> and <kbd>imbalanced-learn</kbd> are separate libraries for data preprocessing that are built as add-ons to scikit-learn.</p>
        <p class="p3">We are going to use some of these tools in this book, and you will notice how easy it is to integrate these different tools into your workflow when using scikit-learn.</p>
        <p class="p3">Being a key player in the Python data ecosystem is what makes scikit-learn the <em>de facto</em> toolset for machine learning. This is the tool that you will most likely hand your job application assignment to, as well as use for Kaggle competitions and to solve most of your professional day-to-day machine learning problems for your job.</p>
        <h2 id="uuid-5dad6045-0d23-4e31-bb4b-5865b362f6fa" class="p3">Practical level of abstraction</h2>
        <p class="p3">scikit-learn implements a vast amount of machine learning, data processing, and model selection algorithms. These implementations are abstract enough, so you only need to apply minor changes when switching from one algorithm to another. This is a key feature since you will need to quickly iterate between different algorithms when developing a model to pick the best one for your problem. Having that said, this abstraction doesn't shield you from the algorithms' configurations. In other words, you are still in full control of your hyperparameters and settings.</p>
        <h2 id="uuid-a30460a3-83e5-4f44-b924-a0073f4713f1" class="p3">When not to use scikit-learn</h2>
        <p class="p3">Most likely, the reasons to not use scikit-learn will include combinations of deep learning or scale. scikit-learn's implementation of neural networks is limited. Unlike scikit-learn, TensorFlow and PyTorch allow you to use a custom architecture, and they support GPUs for a massive training scale. All of scikit-learn's implementations run in memory on a single machine. I'd say that way more than 90% of businesses are at a scale where these constraints are fine. Data scientists can still fit their data in memory in large enough machines thanks to the cloud optionsavailable. They can cleverly engineer workarounds to deal with scaling issues, but if these limitations become something that they can no longer deal with, then they will need other tools to do the trick for them.</p>
        <div class="packt_infobox">There are solutions being developed that allow scikit-learn to scale to multiple machines, such as Dask. Many scikit-learn algorithms allow parallel execution using <kbd>joblib</kbd>, which natively provides thread-based and process-based parallelism. Dask can scale these <kbd>joblib</kbd>-backed algorithms out to a cluster of machines by providing an alternative <kbd>joblib</kbd> backend. </div>
        <h1 id="uuid-d9d6a247-2092-495b-a73b-e8f7e28d9955">Installing the packages you need</h1>
        <p>It's time to install the packages we will need in this book, but first of all, make sure you have Python installed on your computer. In this book, we will be using Python version 3.6. If your computer comes with Python 2.x installed, then you should upgrade Python to version 3.6 or later. I will show you how to install the required packages using <kbd>pip</kbd>, Python's <em>de facto</em> package-management system. If you use other package-management systems, such as Anaconda, you can easily find the equivalent installation commands for each of the following packages online.</p>
        <p>To install <kbd>scikit-learn</kbd>, run the following command:</p>
        <pre>
          <strong>$ pip install --upgrade scikit-learn==0.22<span class="n"/></strong>
        </pre>
        <p><span class="n">I will be using version <kbd>0.22</kbd> of <kbd>scikit-learn</kbd> here. You can add the </span><kbd>--user</kbd>switch to the <kbd>pip</kbd> command to limit the installation to your own directories. This is important if you do not have root access to your machine or if you do not want to install the libraries globally. Furthermore, I prefer to create a virtual environment for each project I work on and install all the libraries I need for this project into that environment. You can check the documentation for Anaconda or Python's <kbd>venv</kbd> module to see how to create virtual environments. </p>
        <p>Along with scikit-learn, we will need to install <kbd>pandas</kbd>. I will briefly introduce <kbd>pandas</kbd> in the next section, but for now, you can use the following command to install it:</p>
        <pre>
          <strong>$ pip install --upgrade pandas==0.25.3</strong>
        </pre>
        <p>Optionally, you may need to install <strong>Jupyter</strong>. Jupyter notebooks allow you to write code in your browser and run bits of it in whichever order you want. This makes it ideal for experimentation and trying different parameters without the need to rerun the entire code every time. You can also plot graphs in your notebooks with the help of Matplotlib. Use the following commands to install both Jupyter and Matplotlib:</p>
        <pre>
          <strong>$ pip install jupyter</strong>
          <br/>
          <strong>$ pip install matplotlib</strong>
        </pre>
        <p>To start your Jupyter server, you can run <kbd>jupyter notebook</kbd>in your terminal, and then visit <kbd>http://localhost:8888/</kbd>in your browser.</p>
        <p>We will make use of other libraries later on in the book. I'd rather introduce you to them when we need them and show you how to install each of them then. </p>
        <h2 id="uuid-db767f6a-d123-4c96-97f6-e48c82bd59f0" class="p3">Introduction to pandas</h2>
        <p><kbd>pandas</kbd> is an open source library that provides data analysis tools for the Python programming language. If this definition doesn't tell you much, then you may think of <kbd>pandas</kbd> as Python's response to spreadsheets. I have decided to dedicate this section to <kbd>pandas</kbd> since you will be using it to create and load the data you are going to use in this book. You will also use <kbd>pandas</kbd> to analyze and visualize your data and alter the value of its columns before applying machine learning algorithms to it.</p>
        <p>Tables in <kbd>pandas</kbd> are referred to as DataFrames. If you are an R programmer, then this name should be familiar to you. Now, let's start by creating a DataFrame for some polygon names and the number of sides each has:</p>
        <pre># It's customary to call pandas pd when importing it<br/>import pandas as pd<br/><br/>polygons_data_frame = pd.DataFrame(<br/>    {<br/>         'Name': ['Triangle', 'Quadrilateral', 'Pentagon', 'Hexagon'],<br/>         'Sides': [3, 4, 5, 6],<br/>     }<br/>)</pre>
        <p>You can then use the <kbd>head</kbd> method to print the first <em>N</em> rows of your newly created DataFrame:</p>
        <pre>polygons_data_frame.head(3)</pre>
        <p>Here, you can see the first three rows of the DataFrame. In addition to the columns we specified, <kbd>pandas</kbd> add a default index:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/36f71f93-9cce-4b4f-b03c-f4fc88b098f1.png" style="width:8.67em;"/>
          <br/>
        </p>
        <p>Since we are programming in Python, we can also use the language's built-in function or even use our custom-built functions when creating a DataFrame. Here, we will use the <kbd>range</kbd> generator, rather than typing in all the possible side counts ourselves:</p>
        <pre>polygons = {<br/>    'Name': [<br/>        'Triangle', 'Quadrilateral', 'Pentagon', 'Hexagon', 'Heptagon', 'Octagon', 'Nonagon', 'Decagon', 'Hendecagon', 'Dodecagon', 'Tridecagon', 'Tetradecagon'<br/>     ],<br/>     # Range parameters are the start, the end of the range and the step<br/>     'Sides': range(3, 15, 1), <br/>}<br/>polygons_data_frame = pd.DataFrame(polygons)</pre>
        <p>You can also sort your DataFrame by column. Here, we will sort it by polygon name in alphabetical order, and then print the first five polygons:</p>
        <pre>polygons_data_frame.sort_values('Name').head(5)</pre>
        <p>This time, we can see the first five rows of the DataFrame after it has been ordered by the names of the polygons in alphabetical order:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/03cb45bc-2fdf-4b29-bfe4-10c1c1e15f76.png" style="width:10.00em;"/>
        </p>
        <p>Feature engineering is the art of deriving new features by manipulating existing data. This is something that <kbd>pandas</kbd> is good at. In the following example, we are creating a new column, <kbd>Length of Name</kbd>, and adding the character lengths of each polygon's name:</p>
        <pre>polygons_data_frame[<br/>   'Length of Name'<br/>] = polygons_data_frame['Name'].str.len()</pre>
        <p>We use <kbd>str</kbd> to be able to access the string functions to apply them to the values in the <kbd>Name</kbd> column. We then use the <kbd>len</kbd> method of a string. One other way to achieve the same result is to use the <kbd>apply()</kbd> function. If you call <kbd>apply()</kbd> on a column, you can get access to the values in the column. You can then apply any Python built-in or custom functions there. Here are two examples of how to use the <kbd>apply()</kbd> function. </p>
        <p>Example 1 is as follows:</p>
        <pre>polygons_data_frame[<br/>   'Length of Name'<br/>] = polygons_data_frame['Name'].apply(len)</pre>
        <p>Example 2 is as follows:</p>
        <pre>polygons_data_frame[<br/>   'Length of Name'<br/>] = polygons_data_frame['Name'].apply(lambda n: len(n))</pre>
        <p>The good thing about the <kbd>apply()</kbd> method is that it allows you to run your own custom code anywhere, which is something you will need to use a lot when performing complex feature engineering. Nevertheless, the code you run using the <kbd>apply()</kbd> method isn't as optimized as the code in the first example. This is a clear case of flexibility versus performance trade-off that you should be aware of.</p>
        <p>Finally, we can use the plotting capabilities provided by <kbd>pandas</kbd> and Matplotlib to see whether there is any correlation between the number of sides a polygon has and the length of its name:</p>
        <pre># We use the DataFrame's plot method here, <br/># where we specify that this is a scatter plot<br/># and also specify which columns to use for x and y<br/>polygons_data_frame.plot(<br/>    title='Sides vs Length of Name',<br/>    kind='scatter',<br/>    x='Sides',<br/>    y='Length of Name',<br/>)</pre>
        <p class="CDPAlignLeft CDPAlign">Once we run the previous code, the following scatter plots will be displayed:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/7debbcb9-35d9-40a6-aa6d-d0d85e5146ef.png" style="width:39.58em;"/>
        </p>
        <p>Scatter plots are generally useful for seeing correlations between two features. In the following plot, there is no clear correlation to be seen.</p>
        <h3 id="uuid-ae849db6-8e89-4eb7-8b09-9c195a6d6789">Python's scientific computing ecosystem conventions</h3>
        <p>Throughout this book, I will be using <kbd>pandas</kbd>, NumPy, SciPy, Matplotlib, and Seaborn. Any time you see the <kbd>np</kbd>, <kbd>sp</kbd>, <kbd>pd</kbd>, <kbd>sns</kbd>, and <kbd>plt</kbd>prefixes,you should assume that I have run the following import statements prior to the code:</p>
        <pre>import numpy as np<br/>import scipy as sp<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</pre>
        <p>This is the <em>de facto</em> way of importing the scientific computing ecosystem into Python. If any of these libraries is missing on your computer, here is how to install them using <kbd>pip</kbd>:</p>
        <pre>
          <strong>$ pip install --upgrade numpy==1.17.3</strong>
          <br/>
          <strong>$ pip install --upgrade scipy==1.3.1</strong>
          <br/>
          <strong>$ pip install --upgrade pandas==0.25.3</strong>
          <br/>
          <strong>$ pip install --upgrade scikit-learn==0.22</strong>
          <br/>
          <strong>$ pip install --upgrade matplotlib==3.1.2</strong>
          <br/>
          <strong>$ pip install --upgrade seaborn==0.9.0</strong>
        </pre>
        <p>Usually, you do not need to specify the versions for each library; running <kbd>pip install numpy</kbd> will just install the latest stable version of the library. Nevertheless, pinning the version is good practice for reproducibility. It ensures the same results from the same code when it runs on different machines. </p>
        <p>The code used in this book is written in Jupyter notebooks. I advise you to do the same on your machine. In general, the code should run smoothly in any other environment with very few changes when it comes to printing and displaying the results. If the figures are not shown in your  Jupyter notebook, you may need to run the following line at least once in any cell at the beginning of your notebook:</p>
        <pre>
          <strong>%matplotlib inline</strong>
        </pre>
        <p>Furthermore, randomness is quite common in many machine learning tasks. We may need to create random data to use with our algorithms. We may also randomly split this data into training and test sets. The algorithms themselves may use random values for initialization. There are tricks to make sure we all get the exact same results by using pseudo-random numbers. I will be using these tricks when needed sometimes, but other times, it would be better to make sure we get slightly different results to give you an idea of how things are not always deterministic and how to find ways to deal with underlying uncertainties. More on this later. </p>
        <h1 id="uuid-c59d45cc-2668-4aad-b3b1-8d29edf822ea" class="p3">Summary</h1>
        <p>Mastering machine learning is a desirable skill nowadays given its vast application everywhere, from business to academia. Nevertheless, just understanding the theory of it will only take you so far since practitioners also need to understand their tools to be self-sufficient and capable.</p>
        <p>In this chapter, we started with a high-level introduction to machine learning and learned when to use each of the machine learning types; from classification and regression to clustering and reinforcement learning. We then learned about scikit-learn and why practitioners recommend it when solving both supervised and unsupervised learning problems. To keep this book self-sufficient, we also covered the basics of data manipulation for those who haven't used libraries such as <kbd>pandas</kbd> and Matplotlib before. In the following chapters, we will continue to combine our understanding of the underlying theory of machine learning with more practical examples using scikit-learn.</p>
        <p class="p3">The first two parts of this book will cover supervised machine learning algorithms. The first part will cover basic algorithms, as well as some other machine learning basics, such as data splitting and preprocessing. Then, we will move on to more advanced topics in the second part. The third and final part will cover unsupervised learning and topics such as anomaly detection and recommendation engines.</p>
        <p class="p3">So that this book remains a practical guide, I have made sure to provide examples in each chapter. I also did not want to separate the data preparation from model creation. Although topics such as data splitting, feature selection, data scaling, and model evaluation are key concepts to know about, we usually deal with them as part of an overall whole solution. I also feel that those concepts are best understood in their correct context. That's why, within each chapter, I will be covering one main algorithm but will use some examples to shed light on some other concepts along the way.</p>
        <p class="p3">This means that it is up to you whether you read this book from cover to cover or use it as a reference and jump straight to the algorithms you want to know about when you need them. Nevertheless, I advise that you skim through all the chapters, even if you already know about the algorithm covered there or don't need to know about it at the moment.</p>
        <p class="p3">I hope that you are now ready for the next chapter, where we will start by looking at decision trees and learn how to use them to solve different classification and regression problems. </p>
        <h1 id="uuid-b1ebb170-087e-4402-a61a-b8867f99e722" class="mce-root">Further reading</h1>
        <p>For more information on the relative topics of this chapter, please refer to the following links:</p>
        <ul>
          <li>
            <p class="a-size-large a-spacing-none"><em>Learn Python Programming – Second Edition</em>, by <em>Fabrizio Romano</em>: <a href="https://www.packtpub.com/application-development/learn-python-programming-second-edition">https://www.packtpub.com/application-development/learn-python-programming-second-edition</a></p>
          </li>
        </ul>
        <ul>
          <li class="mt0 ng-binding">
            <p class="a-size-large a-spacing-none"><em>Hands-On Data Analysis with Pandas</em>, by <em>Stefanie Molin</em>: <a href="https://www.packtpub.com/big-data-and-business-intelligence/hands-data-analysis-pandas">https://www.packtpub.com/big-data-and-business-intelligence/hands-data-analysis-pandas</a></p>
          </li>
        </ul>
      </article>
    </section>
  </body></html>
- en: Chapter 8. Recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recommendations have become one of the staples of online services and commerce.
    This type of automated system can provide each user with a personalized list of
    suggestions (be it a list of products to purchase, features to use, or new connections).
    In this chapter, we will see the basic ways in which automated recommendation
    generation systems work. The field of recommendation based on consumer inputs
    is often called collaborative filtering, as the users collaborate through the
    system to find the best items for each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first part of this chapter, we will see how we can use past product
    ratings from consumers to predict new ratings. We start with a few ideas that
    are helpful and then combine all of them. When combining, we use regression to
    learn the best way in they can be combined. This will also allow us to explore
    a generic concept in machine learning: ensemble learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second part of this chapter, we will take a look at a different way
    of learning recommendations: basket analysis. Unlike the case in which we have
    numeric ratings, in the basket analysis setting, all we have is information about
    the shopping baskets, that is, what items were bought together. The goal is to
    learn about recommendations. You have probably already seen features of the form
    "people who bought X also bought Y" in online shopping. We will develop a similar
    feature of our own.'
  prefs: []
  type: TYPE_NORMAL
- en: Rating predictions and recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have used any online shopping system in the last 10 years, you have probably
    seen these recommendations. Some are like Amazon's "costumers who bought X also
    bought Y". These will be dealt with later in the chapter in the *Basket analysis*
    section. Other recommendations are based on predicting the rating of a product,
    such as a movie.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of learning recommendations based on past product ratings was made
    famous by the Netflix Prize, a million-dollar machine learning public challenge
    by Netflix. Netflix (well-known in the USA and UK and in a process of international
    expansion) is a movie rental company. Traditionally, you would receive DVDs in
    the mail; more recently, Netflix has focused on the online streaming of movies
    and TV shows. From the start, one of the distinguishing features of the service
    was that it gives users the option to rate the films they have seen. Netflix then
    uses these ratings to recommend other films to its customers. In this machine
    learning problem, you not only have the information about which films the user
    saw but also about how the user rated them.
  prefs: []
  type: TYPE_NORMAL
- en: In 2006, Netflix made a large number of customer ratings of films in its database
    available for a public challenge. The goal was to improve on their in-house algorithm
    for rating prediction. Whoever would be able to beat it by 10 percent or more
    would win 1 million dollars. In 2009, an international team named BellKor's Pragmatic
    Chaos was able to beat this mark and take the prize. They did so just 20 minutes
    before another team, The Ensemble, and passed the 10 percent mark as well—an exciting
    photo finish for a competition that lasted several years.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Machine learning in the real world**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Much has been written about the Netflix Prize, and you may learn a lot by reading
    up on it. The techniques that won were a mixture of advanced machine learning
    and a lot of work put into preprocessing the data. For example, some users like
    to rate everything very highly, while others are always more negative; if you
    do not account for this in preprocessing, your model will suffer. Other normalizations
    were also necessary for a good result: how old is the film and how many ratings
    did it receive. Good algorithms are a good thing, but you always need to "get
    your hands dirty" and tune your methods to the properties of the data you have
    in front of you. Preprocessing and normalizing the data is often the most time-consuming
    part of the machine learning process. However, this is also the place where one
    can have the biggest impact on the final performance of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to note about the Netflix Prize is how hard it was. Roughly
    speaking, the internal system that Netflix used was about 10 percent better than
    no recommendations (that is, assigning each movie just the average value for all
    users). The goal was to obtain just another 10 percent improvement on this. In
    total, the winning system was roughly just 20 percent better than no personalization.
    Yet, it took a tremendous amount of time and effort to achieve this goal. And
    even though 20 percent does not seem like much, the result is a system that is
    useful in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, for legal reasons, this dataset is no longer available. Although
    the data was anonymous, there were concerns that it might be possible to discover
    who the clients were and reveal private details of movie rentals. However, we
    can use an academic dataset with similar characteristics. This data comes from
    GroupLens, a research laboratory at the University of Minnesota.
  prefs: []
  type: TYPE_NORMAL
- en: How can we solve a Netflix style ratings prediction question? We will see two
    different approaches, neighborhood approaches and regression approaches. We will
    also see how to combine these to obtain a single prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting into training and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At a high level, splitting the dataset into training and testing data in order
    to obtain a principled estimate of the system''s performance is performed as in
    previous chapters: we take a certain fraction of our data points (we will use
    10 percent) and reserve them for testing; the rest will be used for training.
    However, because the data is structured differently in this context, the code
    is different. The first step is to load the data from disk, for which we use the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that zero entries in this matrix represent missing ratings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We now use the standard random module to choose indices to test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we build the `train` matrix, which is like `reviews`, but with the testing
    entries set to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `test` matrix contains just the testing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: From now on, we will work on taking the training data, and try to predict all
    the missing entries in the dataset. That is, we will write code that assigns each
    (user, movie) pair a recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing the training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed, it is best to normalize the data to remove obvious movie or
    user-specific effects. We will just use one very simple type of normalization,
    which we used before: conversion to z-scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, we cannot simply use scikit-learn's normalization objects as
    we have to deal with the missing values in our data (that is, not all movies were
    rated by all users). Thus, we want to normalize by the mean and standard deviation
    of the values that are, in fact, present.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will write our own class, which ignores missing values. This class will
    follow the scikit-learn preprocessing API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to choose the axis of normalization. By default, we normalize along
    the first axis, but sometimes it will be useful to normalize along the second
    one. This follows the convention of many other NumPy-related functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important method is the fit method. In our implementation, we compute
    the mean and standard deviation of the values that are not zero. Recall that zeros
    indicate "missing values":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If the axis is 1, we operate on the transposed array as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We add 0.1 to the direct estimate of the standard deviation to avoid underestimating
    the value of the standard deviation when there are only a few samples, all of
    which may be exactly the same. The exact value used does not matter much for the
    final result, but we need to avoid division by zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `transform` method needs to take care of maintaining the binary structure
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how we took care of transposing the input matrix when the axis is 1
    and then transformed it back so that the return value has the same shape as the
    input. The `inverse_transform` method performs the inverse operation to transform
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add the `fit_transform` method which, as the name indicates, combines
    both the `fit` and `transform` operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The methods that we defined (`fit`, `transform`, `transform_inverse`, and `fit_transform`)
    were the same as the objects defined in the `sklearn.preprocessing` module. In
    the following sections, we will first normalize the inputs, generate normalized
    predictions, and finally apply the inverse transformation to obtain the final
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: A neighborhood approach to recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The neighborhood concept can be implemented in two ways: user neighbors or
    movie neighbors. User neighborhoods are based on a very simple concept: to know
    how a user will rate a movie, find the users most similar to them, and look at
    their ratings. We will only consider user neighbors for the moment. At the end
    of this section, we will discuss how the code can be adapted to compute movie
    neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the interesting techniques that we will now explore is to just see which
    movies each user has rated, even without taking a look at what rating was given.
    Even with a binary matrix where we have an entry equal to 1 when a user rates
    a movie, and 0 when they did not, we can make useful predictions. In hindsight,
    this makes perfect sense; we do not completely randomly choose movies to watch,
    but instead pick those where we already have an expectation of liking them. We
    also do not make random choices of which movies to rate, but perhaps only rate
    those we feel most strongly about (naturally, there are exceptions, but on average
    this is probably true).
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the values of the matrix as an image, where each rating is
    depicted as a little square. Black represents the absence of a rating and the
    gray levels represent the rating value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to visualize the data is very simple (you can adapt it to show a larger
    fraction of the matrix than is possible to show in this book), as shown in the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the output of this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A neighborhood approach to recommendations](img/2772OS_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the matrix is sparse—most of the squares are black. We can also
    see that some users rate a lot more movies than others and that some movies are
    the target of many more ratings than others.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now going to use this binary matrix to make predictions of movie ratings.
    The general algorithm will be (in pseudo code) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For each user, rank every other user in terms of closeness. For this step, we
    will use the binary matrix and use correlation as the measure of closeness (interpreting
    the binary matrix as zeros and ones allows us to perform this computation).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When we need to estimate a rating for a (user, movie) pair, we look at all
    the users who have rated that movie and split them into two: the most similar
    half and the most dissimilar half. We then use the average of the most similar
    half as the prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can use the `scipy.spatial.distance.pdist` function to obtain the distance
    between all the users as a matrix. This function returns the correlation distance,
    which transforms the correlation value by inverting it so that larger numbers
    mean less similar. Mathematically, the correlation distance is ![A neighborhood
    approach to recommendations](img/2772OS_08_06.jpg), where ![A neighborhood approach
    to recommendations](img/2772OS_11_15.jpg) is the correlation value. The code is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can use this matrix to select the nearest neighbors of each user. These are
    the users that most resemble it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we iterate over all users to estimate predictions for all inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The tricky part in the preceding snippet is indexing by the right values to
    select the neighbors who have rated the movie. Then, we choose the half that is
    closest to the user (in the `rev[:n]` line) and average those. Because some films
    have many reviews and others very few, it is hard to find a single number of users
    for all cases. Choosing half of the available data is a more generic approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the final result, we need to un-normalize the predictions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the same metrics we learned about in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code computes the result for user neighbors, but we can use it
    to compute the movie neighbors by simply transposing the input matrix. In fact,
    the code computes neighbors of whatever are the rows of its input matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'So we can rerun the following code, by just inserting the following line at
    the top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Thus we can see that the results are not that different.
  prefs: []
  type: TYPE_NORMAL
- en: In this book's code repository, the neighborhood code has been wrapped into
    a simple function, which makes it easier to reuse.
  prefs: []
  type: TYPE_NORMAL
- en: A regression approach to recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative to neighborhoods is to formulate recommendations as a regression
    problem and apply the methods that we learned in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also consider why this problem is not a good fit for a classification formulation.
    We could certainly attempt to learn a five-class model, using one class for each
    possible movie rating. There are two problems with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The different possible errors are not at all the same. For example, mistaking
    a 5-star movie for a 4-star one is not as serious a mistake as mistaking a 5-star
    movie for a 1-star one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intermediate values make sense. Even if our inputs are only integer values,
    it is perfectly meaningful to say that the prediction is 4.3\. We can see that
    this is a different prediction than 3.5, even if they both round to 4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two factors together mean that classification is not a good fit to the
    problem. The regression framework is a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a basic approach, we again have two choices: we can build movie-specific
    or user-specific models. In our case, we are going to first build user-specific
    models. This means that, for each user, we take the movies that the user has rated
    as our target variable. The inputs are the ratings of other users. We hypothesize
    that this will give a high value to users who are similar to our user (or a negative
    value to users who like the same movies that our user dislikes).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up the `train` and `test` matrices is as before (including running
    the normalization steps). Therefore, we jump directly to the learning step. First,
    we instantiate a regressor as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We build a data matrix, which will contain a rating for every (user, movie)
    pair. We initialize it as a copy of the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we iterate over all the users, and each time learn a regression model
    based only on the data that that user has given us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluating the method can be done exactly as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As before, we can adapt this code to perform movie regression by using the transposed
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiple methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now combine the aforementioned methods in a single prediction. This seems
    intuitively a good idea, but how can we do this in practice? Perhaps, the first
    thought that comes to mind is that we can average the predictions. This might
    give decent results, but there is no reason to think that all estimated predictions
    should be treated the same. It might be that one is better than others.
  prefs: []
  type: TYPE_NORMAL
- en: We can try a weighted average, multiplying each prediction by a given weight
    before summing it all up. How do we find the best weights, though? We learn them
    from the data, of course!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Ensemble learning**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using a general technique in machine learning, which is not only applicable
    in regression: **ensemble learning**. We learn an ensemble (that is, a set) of
    predictors. Then, we to combine them to obtain a single output. What is interesting
    is that we can see each prediction as being a new feature, and we are now just
    combining features based on training data, which is what we have been doing all
    along. Note that we are doing so for regression here, but the same reasoning is
    applicable to classification: you learn several classifiers, then a master classifier,
    which takes the output of all of them and gives a final prediction. Different
    forms of ensemble learning differ on how you combine the base predictors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to combine the methods, we will use a technique called **stacked learning**.
    The idea is you learn a set of predictors, then you use the output of these predictors
    as features for another predictor. You can even have several layers, where each
    layer learns by using the output of the previous layer as features for its prediction.
    Have a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Combining multiple methods](img/2772OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In order to fit this combination model, we will split the training data into
    two. Alternatively, we could have used cross-validation (the original stacked
    learning model worked like this). However, in this case, we have enough data to
    obtain good estimates by leaving some aside.
  prefs: []
  type: TYPE_NORMAL
- en: 'As when fitting hyperparameters, though, we need two layers of training/testing
    splits: a first, higher-level split, and then, inside the training split, a second
    split to be able to fit the stacked learner, as show in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we apply the whole process to the testing split and evaluate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluation is as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The result of stacked learning is better than what any single method had achieved.
    It is quite typical that combining methods is a simple way to obtain a small performance
    boost, but that the results are not earth shattering.
  prefs: []
  type: TYPE_NORMAL
- en: By having a flexible way to combine multiple methods, we can simply try any
    idea we wish by adding it into the mix of learners and letting the system fold
    it into the prediction. We can, for example, replace the neighborhood criterion
    in the nearest neighbor code.
  prefs: []
  type: TYPE_NORMAL
- en: However, we do have to be careful to not overfit our dataset. In fact, if we
    randomly try too many things, some of them will work well on this dataset, but
    will not generalize. Even though we are splitting our data, we are not rigorously
    cross-validating our design decisions. In order to have a good estimate, and if
    data is plentiful, you should leave a portion of the data untouched until you
    have a final model that is about to go into production. Then, testing your model
    on this held out data gives you an unbiased prediction of how well you should
    expect it to work in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Basket analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The methods we have discussed so far work well when you have numeric ratings
    of how much a user liked a product. This type of information is not always available,
    as it requires active behavior on the part of consumers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basket analysis is an alternative mode of learning recommendations. In this
    mode, our data consists only of what items were bought together; it does not contain
    any information on whether individual items were enjoyed or not. Even if users
    sometimes buy items they regret, on average, knowing their purchases gives you
    enough information to build good recommendations. It is often easier to get this
    data rather than rating data, as many users will not provide ratings, while the
    basket data is generated as a side effect of shopping. The following screenshot
    shows you a snippet of Amazon.com''s web page for Tolstoy''s classic book *War
    and Peace*, which demonstrates a common way to use these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basket analysis](img/2772OS_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This mode of learning is not only applicable to actual shopping baskets, naturally.
    It is applicable in any setting where you have groups of objects together and
    need to recommend another. For example, recommending additional recipients to
    a user writing an e-mail is done by Gmail and could be implemented using similar
    techniques (we do not know what Gmail uses internally; perhaps, they combine multiple
    techniques, as we did earlier). Or, we could use these methods to develop an app
    to recommend web pages to visit based on your browsing history. Even if we are
    handling purchases, it may make sense to group all purchases by a customer into
    a single basket, independently of whether the items were bought together or on
    separate transactions. This depends on the business context, but keep in mind
    that the techniques are flexible and can be useful in many settings.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beer and diapers. One of the stories that is often mentioned in the context
    of basket analysis is the *diapers and beer* story. It says that when supermarkets
    first started to look at their data, they found that diapers were often bought
    together with beer. Supposedly, it was the father who would go out to the supermarket
    to buy diapers and then would pick up some beer as well. There has been much discussion
    of whether this is true or just an urban myth. In this case, it seems that it
    is true. In the early 1990s, Osco Drug did discover that in the early evening,
    beer and diapers were bought together, and it did surprise the managers who had,
    until then, never considered these two products to be similar. What is not true
    is that this led the store to move the beer display closer to the diaper section.
    Also, we have no idea whether it was really that fathers were buying beer and
    diapers together more than mothers (or grandparents).
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining useful predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not just "customers who bought X also bought Y" even though that is how
    many online retailers phrase it (see the Amazon.com screenshot given earlier);
    a real system cannot work like this. Why not? Because such a system would get
    fooled by very frequently bought items and would simply recommend that which is
    popular without any personalization.
  prefs: []
  type: TYPE_NORMAL
- en: For example, at a supermarket, many customers buy bread every time they shop
    or close to it (for the sake of argument, let us say that 50 percent of visits
    include bread). So, if you focus on any particular item, say dishwasher soap and
    look at what is frequently bought with dishwasher soap, you might find that bread
    is frequently bought with soap. In fact, just by random chance, 50 percent of
    the times someone buys dishwasher soap, they buy bread. However, bread is frequently
    bought with anything else just because everybody buys bread very often.
  prefs: []
  type: TYPE_NORMAL
- en: What we are really looking for is "customers who bought X, are statistically
    more likely to buy Y than the average customer who has not bought X". If you buy
    dishwasher soap, you are likely to buy bread, but not more so than the baseline.
    Similarly, a bookstore that simply recommended bestsellers no matter which books
    you had already bought would not be doing a good job of personalizing recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing supermarket shopping baskets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an example, we will look at a dataset consisting of anonymous transactions
    at a supermarket in Belgium. This dataset was made available by Tom Brijs at Hasselt
    University. Due to privacy concerns, the data has been anonymized, so we only
    have a number for each product and a basket is a set of numbers. The data file
    is available from several online sources (including this book's companion website).
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by loading the dataset and looking at some statistics (this is always
    a good idea):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the resulting counts summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| # of times bought | # of products |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Just once | 2,224 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 or 3 | 2,438 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 to 7 | 2,508 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 to 15 | 2,251 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 to 31 | 2,182 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 to 63 | 1,940 |'
  prefs: []
  type: TYPE_TB
- en: '| 64 to 127 | 1,523 |'
  prefs: []
  type: TYPE_TB
- en: '| 128 to 511 | 1,225 |'
  prefs: []
  type: TYPE_TB
- en: '| 512 or more | 179 |'
  prefs: []
  type: TYPE_TB
- en: There are many products that have only been bought a few times. For example,
    33 percent of products were bought four or fewer times. However, this represents
    only 1 percent of purchases. This phenomenon that many products are only purchased
    a small number of times is sometimes labeled *the long tail* and has only become
    more prominent as the Internet made it cheaper to stock and sell niche items.
    In order to be able to provide recommendations for these products, we would need
    a lot more data.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few open source implementations of basket analysis algorithms out
    there, but none that are well integrated with scikit-learn or any of the other
    packages we have been using. Therefore, we are going to implement one classic
    algorithm ourselves. This algorithm is called the Apriori algorithm, and it is
    a bit old (it was published in 1994 by Rakesh Agrawal and Ramakrishnan Srikant),
    but it still works (algorithms, of course, never stop working, they just get superceded
    by better ideas).
  prefs: []
  type: TYPE_NORMAL
- en: Formally, the Apriori algorithm takes a collection of sets (that is, your shopping
    baskets) and returns sets that are very frequent as subsets (that is, items that
    together are part of many shopping baskets).
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm works using a bottom-up approach: starting with the smallest
    candidates (those composed of one single element), it builds up, adding one element
    at a time. Formally, the algorithm takes a set of baskets and the minimum input
    that should be considered (a parameter we will call minsupport). The first step
    is to consider all baskets with just one element with minimal support. Then, these
    are combined in all possible ways to build up two-element baskets. These are filtered
    in order to keep only those that have minimal support. Then, all possible three-element
    baskets are considered and those with minimal support are kept, and so on. The
    trick of Apriori is that when building a larger basket, *it only needs to consider
    those that are built up of smaller sets*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram presents a schematic view of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing supermarket shopping baskets](img/2772OS_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We shall now implement this algorithm in code. We need to define the minimum
    support we are looking for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Support is the number of times a set of products was purchased together. The
    goal of Apriori is to find itemsets with high support. Logically, any itemset
    with more than minimal support can only be composed of items which themselves
    have at least minimal support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Our initial itemsets are singletons (sets with a single element). In particular,
    all singletons that have at least minimal support are frequent itemsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our loop is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This works correctly, but it is slow. A better implementation has more infrastructure
    to avoid having to loop over all the datasets to get the count (`support_c`).
    In particular, we keep track of which shopping baskets have which frequent itemsets.
    This accelerates the loop but makes the code harder to follow. Therefore we will
    not show it here. As usual, you can find both the implementations on this book's
    companion website. The code there is also wrapped into a function that can be
    applied to other datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The Apriori algorithm returns frequent itemsets, that is, baskets that are present
    above a certain threshold (given by the `minsupport` variable in the code).
  prefs: []
  type: TYPE_NORMAL
- en: Association rule mining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Frequent itemsets are not very useful by themselves. The next step is to build
    **association rules**. Because of this final goal, the whole field of basket analysis
    is sometimes called association rule mining.
  prefs: []
  type: TYPE_NORMAL
- en: 'An association rule is a statement of the type "If X, then Y", for example,
    "if a customer bought War and Peace, then they will buy Anna Karenina". Note that
    the rule is not deterministic (not all customers who buy X will buy Y), but it
    is rather cumbersome to always spell it out: "if a customer bought X, he is more
    likely than baseline to buy Y"; thus, we say "if X, then Y", but we mean it in
    a probabilistic sense.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, both the antecedent and the conclusion may contain multiple
    objects: costumers who bought X, Y, and Z also bought A, B, and C. Multiple antecedents
    may allow you to make more specific predictions than are possible from a single
    item.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get from a frequent set to a rule by just trying all the possible combinations
    of X implies Y. It is easy to generate many of these rules. However, you only
    want to have valuable rules. Therefore, we need to measure the value of a rule.
    A commonly used measure is called the **lift**. The lift is the ratio between
    the probability obtained by applying the rule and the baseline, as in the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Association rule mining](img/2772OS_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, P(Y) is the fraction of all the transactions that
    include Y, while P(Y|X) is the fraction of transactions that include Y, given
    that they also include X. Using the lift helps avoid the problem of recommending
    bestsellers; for a bestseller, both P(Y) and P(Y|X) will be large. Therefore,
    the lift will be close to one and the rule will be deemed irrelevant. In practice,
    we wish to have values of lift of at least 10, perhaps even 100.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Some of the results are shown in the following table. The counts are the number
    of transactions which include the **consequent alone** (that is, the base rate
    at which that product is bought), **all the items in the antecedent**, and **all
    the items in the antecedent and the consequent**.
  prefs: []
  type: TYPE_NORMAL
- en: '| Antecedent | Consequent | Consequent count | Antecedent count | Antecedent
    & consequent count | Lift |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1,378, 1,379, 1,380 | 1,269 | 279 (0.3 percent) | 80 | 57 | 225 |'
  prefs: []
  type: TYPE_TB
- en: '| 48, 41, 976 | 117 | 1026 (1.1 percent) | 122 | 51 | 35 |'
  prefs: []
  type: TYPE_TB
- en: '| 48, 41, 1,6011 | 16,010 | 1316 (1.5 percent ) | 165 | 159 | 64 |'
  prefs: []
  type: TYPE_TB
- en: We can see, for example, that there were 80 transactions in which 1,378, 1,379,
    and 1,380 were bought together. Of these, 57 also included 1,269, so the estimated
    conditional probability is 57/80 ≈ 71 percent. Compared to the fact that only
    0.3 percent of all transactions included 1,269, this gives us a lift of 255.
  prefs: []
  type: TYPE_NORMAL
- en: The need to have a decent number of transactions in these counts in order to
    be able to make relatively solid inferences is why we must first select frequent
    itemsets. If we were to generate rules from an infrequent itemset, the counts
    would be very small; due to this, the relative values would be meaningless (or
    subject to very large error bars).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that there are many more association rules discovered from this dataset:
    the algorithm discovers 1,030 rules (requiring support for the baskets of at least
    80 and a minimum lift of 5). This is still a small dataset when compared to what
    is now possible with the web. With datasets containing millions of transactions,
    you can expect to generate many thousands of rules, even millions.'
  prefs: []
  type: TYPE_NORMAL
- en: However, for each customer or each product, only a few rules will be relevant
    at any given time. So each costumer only receives a small number of recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: More advanced basket analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are now other algorithms for basket analysis that run faster than Apriori.
    The code we saw earlier was simple and it was good enough for us, as we only had
    circa 100 thousand transactions. If we had many millions, it might be worthwhile
    to use a faster algorithm. Note, though, that learning association rules can often
    be done offline, where efficiency is not as great a concern.
  prefs: []
  type: TYPE_NORMAL
- en: There are also methods to work with temporal information, leading to rules that
    take into account the order in which you have made your purchases. Consider, as
    an example, that someone buying supplies for a large party may come back for trash
    bags. Thus, it may make sense to propose trash bags on the first visit. However,
    it would not make sense to propose party supplies to everyone who buys a trash
    bag.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by using regression for rating predictions. We saw
    a couple of different ways in which to do so, and then combined them all in a
    single prediction by learning a set of weights. This technique, ensemble learning,
    in particular stacked learning, is a general technique that can be used in many
    situations, not just for regression. It allows you to combine different ideas
    even if their internal mechanics are completely different; you can combine their
    final outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second half of the chapter, we switched gears and looked at another
    mode of producing recommendations: shopping basket analysis or association rule
    mining. In this mode, we try to discover (probabilistic) association rules of
    the "customers who bought X are likely to be interested in Y" form. This takes
    advantage of the data that is generated from sales alone without requiring users
    to numerically rate items. This is not available in scikit-learn at this moment,
    so we wrote our own code.'
  prefs: []
  type: TYPE_NORMAL
- en: Association rule mining needs to be careful to not simply recommend bestsellers
    to every user (otherwise, what is the point of personalization?). In order to
    do this, we learned about measuring the value of rules in relation to the baseline,
    using a measure called the lift of a rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point in the book, we have seen the major modes of machine learning:
    classification. In the next two chapters, we will look at techniques used for
    two specific kinds of data, music and images. Our first goal will be to build
    a music genre classifier.'
  prefs: []
  type: TYPE_NORMAL

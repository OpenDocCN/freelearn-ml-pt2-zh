<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer120">
			<h1 id="_idParaDest-110"><a id="_idTextAnchor108"/>Chapter 6: Training Natural Language Processing Models</h1>
			<p>In the previous chapter, you learned how to use SageMaker's built-in algorithms for <strong class="bold">computer vision</strong> (<strong class="bold">CV</strong>) to solve problems including image classification, object detection, and semantic segmentation. </p>
			<p><strong class="bold">Natural language processing</strong> (<strong class="bold">NLP</strong>) is another very promising field in ML. Indeed, NLP algorithms have proven very effective in modeling language and extracting context from unstructured text. Thanks to this, applications such as search and translation applications and chatbots are now commonplace.</p>
			<p>In this chapter, you will learn about built-in algorithms designed specifically for NLP tasks and we'll discuss the types of problems that you can solve with them. As in the previous chapter, we'll also cover in great detail how to prepare real-life datasets such as Amazon customer reviews. Of course, we'll train and deploy models too. We will cover all of this under the following topics:</p>
			<ul>
				<li>Discovering the NLP built-in algorithms in Amazon SageMaker</li>
				<li>Preparing natural language datasets</li>
				<li>Using the built-in algorithms for NLP</li>
			</ul>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor109"/>Technical requirements</h1>
			<p>You will need an <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) account to run the examples included in this chapter. If you haven't got one already, please browse to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create it. You should also familiarize yourself with the AWS Free Tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the AWS <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) tool for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).</p>
			<p>You will need a working Python 3.x environment. Installing the Anaconda distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory but strongly encouraged, as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>The code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor110"/>Discovering the NLP built-in algorithms in Amazon SageMaker</h1>
			<p>SageMaker <a id="_idIndexMarker497"/>includes four NLP algorithms, enabling <strong class="bold">supervised learning</strong> (<strong class="bold">SL</strong>) and <strong class="bold">unsupervised learning</strong> (<strong class="bold">UL</strong>) scenarios. In this<a id="_idIndexMarker498"/> section, you'll learn about these <a id="_idIndexMarker499"/>algorithms, what<a id="_idIndexMarker500"/> kinds of problems they solve, and what their training scenarios are. Let's have a look at an overview of the algorithms we'll be discussing:</p>
			<ul>
				<li><strong class="bold">BlazingText</strong> builds text classification models (SL) or computes word vectors (UL). <strong class="bold">BlazingText</strong> is an <a id="_idIndexMarker501"/>Amazon-invented algorit<a id="_idTextAnchor111"/>hm. </li>
				<li><strong class="bold">LDA</strong> builds UL <a id="_idIndexMarker502"/>models that <a id="_idIndexMarker503"/>group a collection of text documents into topics. This technique is called <strong class="bold">topic modeling</strong>.</li>
				<li><strong class="bold">NTM</strong> is <a id="_idIndexMarker504"/>another <strong class="bold">topic modeling</strong> algorithm based on neural networks, and it gives you more insight into how topics are built.</li>
				<li><strong class="bold">Sequence to Sequence</strong> (<strong class="bold">seq2seq</strong>) builds <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) models, predicting a sequence of<a id="_idIndexMarker505"/> output<a id="_idIndexMarker506"/> tokens from a sequence of input tokens.</li>
			</ul>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor112"/>Discovering the BlazingText algorithm</h2>
			<p>The <a id="_idIndexMarker507"/>BlazingText algorithm <a id="_idIndexMarker508"/>was invented by Amazon. You can read more<a id="_idIndexMarker509"/> about it at <a href="https://dl.acm.org/doi/10.1145/3146347.3146354">https://dl.acm.org/doi/10.1145/3146347.3146354</a>. BlazingText is an evolution of <strong class="bold">FastText</strong>, a library for <a id="_idIndexMarker510"/>efficient text classification and representation learning developed by Facebook (<a href="https://fasttext.cc">https://fasttext.cc</a>). </p>
			<p>It lets you train text classification models, as well as<a id="_idIndexMarker511"/> computing <strong class="bold">word vectors</strong>. Also called <strong class="bold">embeddings</strong>, <strong class="bold">word vectors</strong> are the cornerstone of many NLP tasks, such as finding word<a id="_idIndexMarker512"/> similarities, word analogies, and so on. <strong class="bold">Word2Vec</strong> is one of the<a id="_idIndexMarker513"/> leading algorithms to compute these vectors (<a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>), and it's the one BlazingText implements.</p>
			<p>The main <a id="_idIndexMarker514"/>improvement of BlazingText is <a id="_idIndexMarker515"/>its ability to train on <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) instances, where FastText only <a id="_idIndexMarker516"/>supports <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) instances. </p>
			<p>The speed gain is <a id="_idIndexMarker517"/>significant, and this is where its name comes from: "blazing" is faster than "fast"! If you're curious about benchmarks, you'll certainly enjoy this blog post: <a href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/">https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/</a>.</p>
			<p>Finally, BlazingText is fully compatible with FastText. Models can be very easily exported and tested, as you will see later in the chapter.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor113"/>Discovering the LDA algorithm</h2>
			<p>This <a id="_idIndexMarker518"/>UL algorithm uses a generative technique, named <strong class="bold">topic modeling</strong>, to identify <a id="_idIndexMarker519"/>topics present in a large collection of text documents. It was first applied to ML in 2003 (<a href="http://jmlr.csail.mit.edu/papers/v3/blei03a.html">http://jmlr.csail.mit.edu/papers/v3/blei03a.html</a>). </p>
			<p>Please note that LDA is<a id="_idIndexMarker520"/> not a classification algorithm. You pass it the number of topics to build, not the list of topics you expect. To paraphrase Forrest Gump: "<em class="italic">Topic modeling is like a box of chocolates, you never know what you're gonna get."</em> </p>
			<p>LDA assumes that every text document in a collection was generated from several latent (meaning "hidden") topics. A topic is represented by a word probability distribution. For each word present in a collection of documents, this distribution gives the probability that the word appears in documents generated by this topic. For example, in a "finance" topic, the distribution would yield high probabilities for words such as "revenue," "quarter," or "earnings," and low probabilities for "ballista" or "platypus" (or so I should think).</p>
			<p>Topic distributions are not considered independently. They are represented by a <strong class="bold">Dirichlet distribution</strong>, a <a id="_idIndexMarker521"/>multivariate generalization of univariate distributions (<a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">https://en.wikipedia.org/wiki/Dirichlet_distribution</a>). This mathematical object gives the algorithm its name.</p>
			<p>Given the number of <a id="_idIndexMarker522"/>words in the vocabulary and the number of latent topics, the<a id="_idIndexMarker523"/> purpose of the LDA algorithm is to build a model that is as close as possible to an ideal Dirichlet distribution. In other words, it will try to group words so that distributions are as well formed as possible and match the specified number of topics.</p>
			<p>Training data needs to be carefully prepared. Each document needs to be converted into a <strong class="bold">bag-of-words</strong> (<strong class="bold">BoW</strong>) representation: each<a id="_idIndexMarker524"/> word is replaced by a pair of integers, representing a <a id="_idIndexMarker525"/>unique word <strong class="bold">identifier </strong>(<strong class="bold">ID</strong>) and the word count in the document. The resulting dataset can be saved either to <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) format <a id="_idIndexMarker526"/>or to <strong class="bold">RecordIO-wrapped protobuf</strong> format, a <a id="_idIndexMarker527"/>technique we already studied<a id="_idIndexMarker528"/> with <strong class="bold">factorization machines</strong> in <a href="B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Machine Learning Models</em>.</p>
			<p>Once the model has been trained, we can score any document and get a score per topic. The expectation is that documents containing similar words should have similar scores, making it possible to identify their top topics.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor114"/>Discovering the NTM algorithm</h2>
			<p>NTM is another <a id="_idIndexMarker529"/>algorithm for topic <a id="_idIndexMarker530"/>modeling. You can read more about it at <a href="https://arxiv.org/abs/1511.06038">https://arxiv.org/abs/1511.06038</a>. The following blog post also sums up the key elements of the paper: <a href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-neural-topic-model-now-supports-auxiliary-vocabulary-channel-new-topic-evaluation-metrics-and-training-subsampling/">https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-neural-topic-model-now-supports-auxiliary-vocabulary-channel-new-topic-evaluation-metrics-and-training-subsampling/</a>.</p>
			<p>As with LDA, documents<a id="_idIndexMarker531"/> need to be converted to a BoW representation, and the dataset can be saved to either CSV or RecordIO-wrapped protobuf format.</p>
			<p>For training, NTM uses a completely different approach based on neural networks and—more precisely—on an encoder architecture (<a href="https://en.wikipedia.org/wiki/Autoencoder">https://en.wikipedia.org/wiki/Autoencoder</a>). In true DL fashion, the encoder trains on mini-batches of documents. It tries to learn their latent features by adjusting network parameters through <a id="_idIndexMarker532"/>backpropagation and optimization.</p>
			<p>Unlike LDA, NTM can tell <a id="_idIndexMarker533"/>us which words are the most impactful in each topic. It also gives us two <a id="_idIndexMarker534"/>per-topic metrics, <strong class="bold">word embedding topic coherence</strong> (<strong class="bold">WETC</strong>) and <strong class="bold">topic uniqueness</strong> (<strong class="bold">TU</strong>). These are outlined in more detail here:</p>
			<ul>
				<li>WETC tells us how semantically close the topic words are. This value is between 0 and 1; the higher, the better. It's computed <a id="_idIndexMarker535"/>using the <strong class="bold">cosine similarity</strong> (<a href="https://en.wikipedia.org/wiki/Cosine_similarity">https://en.wikipedia.org/wiki/Cosine_similarity</a>) of the corresponding word vectors in a <a id="_idIndexMarker536"/>pretrained <strong class="bold">Global Vectors</strong> (<strong class="bold">GloVe</strong>) model (another algorithm similar to Word2Vec).</li>
				<li>TU tells us how unique the topic is—that is to say, whether its words are found in other topics or not. Again, the value is between 0 and 1, and the higher the score, the more unique the topic is.</li>
			</ul>
			<p>Once the model has been trained, we can score documents and get a score per topic.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor115"/>Discovering the seq2sea algorithm</h2>
			<p>The seq2seq<a id="_idIndexMarker537"/> algorithm<a id="_idIndexMarker538"/> is based <a id="_idIndexMarker539"/>on <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) neural networks (<a href="https://arxiv.org/abs/1409.3215">https://arxiv.org/abs/1409.3215</a>). As its name implies, seq2seq can be trained to map one sequence of tokens to another. Its main application is machine translation, training on<a id="_idIndexMarker540"/> large bilingual corpora of text, such as the <strong class="bold">Workshop on Statistical Machine Translation</strong> (<strong class="bold">WMT</strong>) dataset (<a href="http://www.statmt.org/wmt20/">http://www.statmt.org/wmt20/</a>). </p>
			<p>In addition to the implementation available in<a id="_idIndexMarker541"/> SageMaker, AWS has also packaged the seq2seq algorithm into an open source project, <strong class="bold">AWS Sockeye</strong> (<a href="https://github.com/awslabs/sockeye">https://github.com/awslabs/sockeye</a>), which also includes tools for dataset preparation.</p>
			<p>I won't cover <a id="_idIndexMarker542"/>seq2seq in this chapter. It would take too many pages to get into the appropriate level of detail, and there's no point in just <a id="_idIndexMarker543"/>repeating what's already available in the Sockeye documentation. </p>
			<p>You can find a seq2seq example in the notebook available at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/seq2seq_translation_en-de">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/seq2seq_translation_en-de</a>. Unfortunately, it uses the low-level <strong class="source-inline">boto3</strong> <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>), which we will cover in <a href="B17705_12_Final_JM_ePub.xhtml#_idTextAnchor260"><em class="italic">Chapter 12</em></a>, <em class="italic">Automating Machine Learning Workflows</em>. Still, it's a valuable read, and you won't have too much trouble figuring things out.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor116"/>Training with NLP algorithms</h2>
			<p>Just as for CV <a id="_idIndexMarker544"/>algorithms, training is the easy part, especially with the SageMaker <strong class="bold">software development kit</strong> (<strong class="bold">SDK</strong>). By now, you should be familiar with the workflow and the APIs, and we'll keep using them in this chapter.</p>
			<p>Preparing data for NLP algorithms is another story. First, real-life datasets are generally pretty bulky. In this chapter, we'll work with millions of samples and hundreds of millions of words. Of course, they need to be cleaned, processed, and converted to the format expected by the algorithm.</p>
			<p>As we go through the chapter, we'll use the following techniques:</p>
			<ul>
				<li>Loading and cleaning data with <a id="_idIndexMarker545"/>the <strong class="source-inline">pandas</strong> library (<a href="https://pandas.pydata.org">https://pandas.pydata.org</a>)</li>
				<li>Removing stop words and lemmatizing<a id="_idIndexMarker546"/> with the <strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>) library (<a href="https://www.nltk.org">https://www.nltk.org</a>)</li>
				<li>Tokenizing with <a id="_idIndexMarker547"/>the <strong class="source-inline">spaCy</strong> library (<a href="https://spacy.io/">https://spacy.io/</a>)</li>
				<li>Building vocabularies and generating BoW representations <a id="_idIndexMarker548"/>with the <strong class="source-inline">gensim</strong> library (<a href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a>)</li>
				<li>Running data processing jobs with <strong class="bold">Amazon SageMaker Processing</strong>, which we studied in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em></li>
			</ul>
			<p>Granted—this isn't an NLP book, and we won't go extremely far in processing data. Still, this will be quite fun, and hopefully an opportunity to learn about popular open source tools for NLP. </p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor117"/>Preparing natural language datasets</h1>
			<p>For the CV algorithms<a id="_idIndexMarker549"/> in the previous chapter, data preparation focused on the technical format required for the dataset (<strong class="bold">image</strong> format, <strong class="bold">RecordIO</strong>, or <strong class="bold">augmented manifest</strong>). The images themselves weren't processed. </p>
			<p>Things are quite different for NLP algorithms. The text needs to be heavily processed, converted, and saved in the right format. In most learning resources, these steps are abbreviated or even ignored. Data is already "automagically" ready for training, leaving the reader frustrated and sometimes dumbfounded on how to prepare their own datasets.</p>
			<p>No such thing here! In this section, you'll learn how to prepare NLP datasets in different formats. Once again, get ready to learn a lot!</p>
			<p>Let's start with preparing data for BlazingText.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor118"/>Preparing data for classification with BlazingText</h2>
			<p>BlazingText<a id="_idIndexMarker550"/> expects labeled input data in the same <a id="_idIndexMarker551"/>format as FastText, outlined here:</p>
			<ul>
				<li>A plaintext file, with one sample per line.</li>
				<li>Each line has two fields, as follows:<p>a) A label in the form of <strong class="source-inline">__label__LABELNAME__</strong> </p><p>b) The text itself, formed into space-separated tokens (words and punctuation)</p></li>
			</ul>
			<p>Let's get to work and prepare a customer review dataset for sentiment analysis (positive, neutral, or negative). We'll use the <strong class="bold">Amazon Customer Reviews</strong> dataset available at <a href="https://s3.amazonaws.com/amazon-reviews-pds/readme.html">https://s3.amazonaws.com/amazon-reviews-pds/readme.html</a>. That should be more than enough real-life data.</p>
			<p>Before starting, please make sure that you have enough storage space. Here, I'm using a notebook instance with 10 <strong class="bold">gigabytes</strong> (<strong class="bold">GB</strong>) of storage. I've also picked a C5 instance type to run the processing steps faster. We'll proceed as follows:</p>
			<ol>
				<li>Let's download the camera reviews by running the following code:<p class="source-code">%%sh</p><p class="source-code">aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Camera_v1_00.tsv.gz /tmp</p></li>
				<li>We load the data with <strong class="source-inline">pandas</strong>, ignoring any line that causes an error. We also drop any line with missing values. The code is illustrated in the following snippet:<p class="source-code">data = pd.read_csv(</p><p class="source-code">    '/tmp/amazon_reviews_us_Camera_v1_00.tsv.gz', </p><p class="source-code">    sep='\t', compression='gzip', </p><p class="source-code">    error_bad_lines=False, dtype='str')</p><p class="source-code">data.dropna(inplace=True)</p></li>
				<li>We print the data shape and the column names, like this:<p class="source-code">print(data.shape)</p><p class="source-code">print(data.columns)</p><p>This gives us the following output:</p><p class="source-code"><strong class="bold">(1800755, 15)</strong></p><p class="source-code"><strong class="bold">Index(['marketplace','customer_id','review_id','product_id','product_parent', 'product_title','product_category',  'star_rating','helpful_votes','total_votes','vine', 'verified_purchase','review_headline','review_body', </strong></p><p class="source-code"><strong class="bold">'review_date'], dtype='object')</strong></p></li>
				<li>1.8 million lines! We <a id="_idIndexMarker552"/>keep 100,000, which is enough for our purpose. We also drop all columns <a id="_idIndexMarker553"/>except <strong class="source-inline">star_rating</strong> and <strong class="source-inline">review_body</strong>, as illustrated in the following code snippet:<p class="source-code">data = data[:100000]</p><p class="source-code">data = data[['star_rating', 'review_body']]</p></li>
				<li>Based on star ratings, we add a new column named <strong class="source-inline">label</strong>, with labels in the proper format. You have to love how <strong class="source-inline">pandas</strong> makes this so simple. Then, we drop the <strong class="source-inline">star_rating</strong> column, as illustrated in the following code snippet:<p class="source-code">data['label'] = data.star_rating.map({</p><p class="source-code">    '1': '__label__negative__',</p><p class="source-code">    '2': '__label__negative__',</p><p class="source-code">    '3': '__label__neutral__',</p><p class="source-code">    '4': '__label__positive__',</p><p class="source-code">    '5': '__label__positive__'})</p><p class="source-code">data = data.drop(['star_rating'], axis=1)</p></li>
				<li>BlazingText expects labels at the beginning of each line, so we move the <strong class="source-inline">label</strong> column to the front, as follows:<p class="source-code">data = data[['label', 'review_body']]</p></li>
				<li>The data should<a id="_idIndexMarker554"/> now look<a id="_idIndexMarker555"/> like this:<div id="_idContainer112" class="IMG---Figure"><img src="Images/B17705_06_1.jpg" alt="Figure 6.1 – Viewing the dataset&#13;&#10;" width="442" height="170"/></div><p class="figure-caption">Figure 6.1 – Viewing the dataset</p></li>
				<li>BlazingText expects <a id="_idIndexMarker556"/>space-separated tokens: each word and each punctuation sign must be space-separated from the next. Let's use the handy <strong class="source-inline">punkt</strong> tokenizer from the <strong class="source-inline">nltk</strong> library. Depending on the instance type you're using, this could take a couple of minutes. Here's the code you'll need:<p class="source-code">!pip -q install nltk</p><p class="source-code">import nltk</p><p class="source-code">nltk.download('punkt')</p><p class="source-code">data['review_body'] = data['review_body'].apply(nltk.word_tokenize)</p></li>
				<li>We join tokens into a single string, which we also convert to lowercase, as follows:<p class="source-code">data['review_body'] = </p><p class="source-code">    data.apply(lambda row: "".join(row['review_body'])</p><p class="source-code">        .lower(), axis=1)</p></li>
				<li>The data should now look like this (notice that all tokens are correctly space-separated):<div id="_idContainer113" class="IMG---Figure"><img src="Images/B17705_06_2.jpg" alt="Figure 6.2 – Viewing the tokenized dataset&#13;&#10;" width="417" height="170"/></div><p class="figure-caption">Figure 6.2 – Viewing the tokenized dataset</p></li>
				<li>Finally, we split the <a id="_idIndexMarker557"/>dataset for training (95%) and validation (5%), and we save both splits as plaintext files, as <a id="_idIndexMarker558"/>illustrated in the following code snippet:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">training, validation = train_test_split(data, test_size=0.05)</p><p class="source-code">np.savetxt('/tmp/training.txt', training.values, fmt='%s')</p><p class="source-code">np.savetxt('/tmp/validation.txt', validation.values, fmt='%s')</p></li>
				<li>Opening one of the files, you should see plenty of lines similar to this one:<p class="source-code"><strong class="bold">__label__neutral__ really works for me , especially on the streets of europe . wished it was less expensive though . the rain cover at the base really works . the padding which comes in contact with your back though will suffocate &amp; make your back sweaty .</strong></p></li>
			</ol>
			<p>The data preparation wasn't too bad, was it? Still, tokenization ran for a minute or two. Now, imagine running it on millions of samples. Sure, you could fire up a larger environment in <strong class="bold">SageMaker Studio</strong>. You'd also pay more for as long as you're using it, which would probably be wasteful if only this one step required extra computing muscle. In addition, imagine having to run the same script on many other datasets. Do you want to do this manually again and again, waiting 20 minutes every time and hoping your notebook doesn't crash? Certainly not, I should say!</p>
			<p>You already know the answer to both problems. It's <strong class="bold">Amazon SageMaker Processing</strong>, which we studied in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>. You should have the best of both worlds, using the smallest and least-expensive environment possible for experimentation, and running on-demand jobs when you need more resources. Day in, day out, you'll save money and get the job done faster.</p>
			<p>Let's move this processing code to SageMaker Processing.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor119"/>Preparing data for classification with BlazingText, version 2</h2>
			<p>We've <a id="_idIndexMarker559"/>covered this in detail in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>, so I'll go faster this time. We'll <a id="_idIndexMarker560"/>proceed as follows:</p>
			<ol>
				<li value="1">We upload the dataset to <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>), as follows:<p class="source-code">import sagemaker</p><p class="source-code">session = sagemaker.Session()</p><p class="source-code">prefix = 'amazon-reviews-camera'</p><p class="source-code">input_data = session.upload_data(</p><p class="source-code">    path='/tmp/amazon_reviews_us_Camera_v1_00.tsv.gz', </p><p class="source-code">    key_prefix=prefix)</p></li>
				<li>We define the processor by running the following code:<p class="source-code">from sagemaker.sklearn.processing import SKLearnProcessor</p><p class="source-code">sklearn_processor = SKLearnProcessor(</p><p class="source-code">   framework_version='0.23-1',</p><p class="source-code">   role= sagemaker.get_execution_role(),</p><p class="source-code">   instance_type='ml.c5.2xlarge',</p><p class="source-code">   instance_count=1)</p></li>
				<li>We run the <a id="_idIndexMarker561"/>processing job, passing<a id="_idIndexMarker562"/> the processing script and its arguments, as follows:<p class="source-code">from sagemaker.processing import ProcessingInput, ProcessingOutput</p><p class="source-code">sklearn_processor.run(</p><p class="source-code">    code='preprocessing.py',</p><p class="source-code">    inputs=[</p><p class="source-code">        ProcessingInput(</p><p class="source-code">            source=input_data,</p><p class="source-code">            destination='/opt/ml/processing/input')</p><p class="source-code">    ],</p><p class="source-code">    outputs=[</p><p class="source-code">        ProcessingOutput(</p><p class="source-code">            output_name='train_data',</p><p class="source-code">            source='/opt/ml/processing/train'),</p><p class="source-code">        ProcessingOutput(</p><p class="source-code">            output_name='validation_data',</p><p class="source-code">            source='/opt/ml/processing/validation')</p><p class="source-code">    ],</p><p class="source-code">    arguments=[</p><p class="source-code">        '--filename', 'amazon_reviews_us_Camera_v1_00.tsv.gz',</p><p class="source-code">        '--num-reviews', '100000',</p><p class="source-code">        '--split-ratio', '0.05'</p><p class="source-code">    ]</p><p class="source-code">)</p></li>
				<li>The abbreviated <a id="_idIndexMarker563"/>preprocessing script is shown in the following code snippet. The full version is in the GitHub repository for the book. We first install the <strong class="source-inline">nltk</strong> package, as follows:<p class="source-code">import argparse, os, subprocess, sys</p><p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">def install(package):</p><p class="source-code">    subprocess.call([sys.executable, "-m", "pip",  </p><p class="source-code">                     "install", package]) </p><p class="source-code">if __name__=='__main__':</p><p class="source-code">    install('nltk')</p><p class="source-code">    import nltk</p></li>
				<li>We read the<a id="_idIndexMarker564"/> command-line arguments, as follows:<p class="source-code">    parser = argparse.ArgumentParser()</p><p class="source-code">    parser.add_argument('--filename', type=str)</p><p class="source-code">    parser.add_argument('--num-reviews', type=int)</p><p class="source-code">    parser.add_argument('--split-ratio', type=float, </p><p class="source-code">                        default=0.1)</p><p class="source-code">    args, _ = parser.parse_known_args()</p><p class="source-code">    filename = args.filename</p><p class="source-code">    num_reviews = args.num_reviews</p><p class="source-code">    split_ratio = args.split_ratio</p></li>
				<li>We read the<a id="_idIndexMarker565"/> input dataset and process it, as follows:<p class="source-code">    input_data_path = </p><p class="source-code">    os.path.join('/opt/ml/processing/input', filename)</p><p class="source-code">    data = pd.read_csv(input_data_path, sep='\t', </p><p class="source-code">           compression='gzip', error_bad_lines=False,</p><p class="source-code">           dtype='str')</p><p class="source-code">    # Process data</p><p class="source-code">    . . . </p></li>
				<li>Finally, we split it <a id="_idIndexMarker566"/>for training and validation, and save it into two text files, as follows:<p class="source-code">    training, validation = train_test_split(</p><p class="source-code">                           data, </p><p class="source-code">                           test_size=split_ratio)</p><p class="source-code">    training_output_path = os.path.join('</p><p class="source-code">    /opt/ml/processing/train', 'training.txt')    </p><p class="source-code">    validation_output_path = os.path.join(</p><p class="source-code">    '/opt/ml/processing/validation', 'validation.txt')</p><p class="source-code">    np.savetxt(training_output_path, </p><p class="source-code">    training.values, fmt='%s')</p><p class="source-code">    np.savetxt(validation_output_path, </p><p class="source-code">    validation.values, fmt='%s')</p></li>
			</ol>
			<p>As you can see, it doesn't take much to convert manual processing code into a SageMaker Processing job. You can actually reuse most of the code too, as it deals with generic topics such as command-line arguments, inputs, and outputs. The only trick is using <strong class="source-inline">subprocess.call</strong> to install dependencies inside the processing container.</p>
			<p>Equipped with this script, you can now process data at scale as often as you want, without having to run and manage long-lasting notebooks.</p>
			<p>Now, let's prepare data for the other BlazingText scenario: word vectors!</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor120"/>Preparing data for word vectors with BlazingText</h2>
			<p>BlazingText lets you compute<a id="_idIndexMarker567"/> word vectors easily and at <a id="_idIndexMarker568"/>scale. It expects input data in the following format:</p>
			<ul>
				<li>A plaintext file, with one sample per line.</li>
				<li>Each sample must have space-separated tokens (words and punctuations).</li>
			</ul>
			<p>Let's process the same dataset as in the previous section, as follows:</p>
			<ol>
				<li value="1">We'll need the <strong class="source-inline">spaCy</strong> library, so let's install it along with its English language model, like this:<p class="source-code">%%sh</p><p class="source-code">pip -q install spacy</p><p class="source-code">python -m spacy download en_core_web_sm</p><p class="source-code">python -m spacy validate</p></li>
				<li>We load the data with <strong class="source-inline">pandas</strong>, ignoring any line that causes an error. We also drop any line with missing values. We should have more than enough data anyway. Here's the code you'll need:<p class="source-code">data = pd.read_csv(</p><p class="source-code">    '/tmp/amazon_reviews_us_Camera_v1_00.tsv.gz', </p><p class="source-code">    sep='\t', compression='gzip', </p><p class="source-code">    error_bad_lines=False, dtype='str')</p><p class="source-code">data.dropna(inplace=True)</p></li>
				<li>We keep 100,000 lines, and we also drop all columns except <strong class="source-inline">review_body</strong>, as illustrated in the following code snippet:<p class="source-code">data = data[:100000]</p><p class="source-code">data = data[['review_body']]</p><p>We write a function to tokenize reviews with <strong class="source-inline">spaCy</strong>, and we apply it to the <strong class="source-inline">DataFrame</strong>. This step should be noticeably faster than <strong class="source-inline">nltk</strong> tokenization in the previous <a id="_idIndexMarker569"/>example, as <strong class="source-inline">spaCy</strong> is<a id="_idIndexMarker570"/> based on <strong class="source-inline">cython</strong> (<a href="https://cython.org">https://cython.org</a>). The code is illustrated in the following snippet: </p><p class="source-code">import spacy</p><p class="source-code">spacy_nlp = spacy.load('en_core_web_sm')</p><p class="source-code">def tokenize(text):</p><p class="source-code">    tokens = spacy_nlp.tokenizer(text)</p><p class="source-code">    tokens = [ t.text for t in tokens ]</p><p class="source-code">    return " ".join(tokens).lower()</p><p class="source-code">data['review_body'] = </p><p class="source-code">    data['review_body'].apply(tokenize)</p><p>The data should now look like this:</p><div id="_idContainer114" class="IMG---Figure"><img src="Images/B17705_06_3.jpg" alt="Figure 6.3 – Viewing the tokenized dataset&#13;&#10;" width="297" height="170"/></div><p class="figure-caption">Figure 6.3 – Viewing the tokenized dataset</p></li>
				<li>Finally, we save <a id="_idIndexMarker571"/>the reviews to a plaintext file, as follows:<p class="source-code">import numpy as np</p><p class="source-code">np.savetxt('/tmp/training.txt', data.values, fmt='%s')</p></li>
				<li>Opening this file, you should see one tokenized review per line, as illustrated in the following code snippet:<p class="source-code"><strong class="bold">Ok</strong></p><p class="source-code"><strong class="bold">perfect , even sturdier than the original !</strong></p></li>
			</ol>
			<p>Here too, we should really be running these steps using SageMaker Processing. You'll find the corresponding notebook and preprocessing script in the GitHub repository for the book.</p>
			<p>Now, let's prepare data for the LDA and NTM algorithms.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/>Preparing data for topic modeling with LDA and NTM</h2>
			<p>In this <a id="_idIndexMarker572"/>example, we will use the <strong class="bold">Million News Headlines</strong> dataset (<a href="https://doi.org/10.7910/DVN/SYBGZL">https://doi.org/10.7910/DVN/SYBGZL</a>), which is also available in the GitHub<a id="_idIndexMarker573"/> repository. As the <a id="_idIndexMarker574"/>name implies, it contains a million <a id="_idIndexMarker575"/>news headlines from the Australian news source <em class="italic">ABC</em>. Unlike product reviews, headlines are in very short sentences. Building a topic model should be an interesting challenge!</p>
			<h3>Tokenizing data</h3>
			<p>As you would expect, both<a id="_idIndexMarker576"/> algorithms require a tokenized dataset, so we'll proceed as follows:</p>
			<ol>
				<li value="1">We'll need the <strong class="source-inline">nltk</strong> and <strong class="source-inline">gensim</strong> libraries, so let's install them, as follows:<p class="source-code">%%sh</p><p class="source-code">pip -q install nltk gensim</p></li>
				<li>Once we've downloaded the dataset, we load it entirely with <strong class="source-inline">pandas</strong>, like this:<p class="source-code">num_lines = 1000000</p><p class="source-code">data = pd.read_csv('abcnews-date-text.csv.gz', </p><p class="source-code">                   compression='gzip', error_bad_lines=False, </p><p class="source-code">                   dtype='str', nrows=num_lines)</p></li>
				<li>The data should look like this:<div id="_idContainer115" class="IMG---Figure"><img src="Images/B17705_06_4.jpg" alt="Figure 6.4 – Viewing the tokenized dataset&#13;&#10;" width="396" height="166"/></div><p class="figure-caption">Figure 6.4 – Viewing the tokenized dataset</p></li>
				<li>It's sorted by date, and we shuffle it as a precaution. We then drop the <strong class="source-inline">date</strong> column by running the following code:<p class="source-code">data = data.sample(frac=1)</p><p class="source-code">data = data.drop(['publish_date'], axis=1)</p></li>
				<li>We write a function to clean up and process the headlines. First, we get rid of all punctuation signs and digits. Using <strong class="source-inline">nltk</strong>, we also remove stop words—namely, words that are <a id="_idIndexMarker577"/>extremely common and don't add any context, such as "this," "any," and so on. In order to reduce the vocabulary size while keeping context, we could apply<a id="_idIndexMarker578"/> either <strong class="bold">stemming</strong> (https://en.wikipedia.org/wiki/Stemming) or <strong class="bold">lemmatization</strong> (<a href="https://en.wikipedia.org/wiki/Lemmatisation">https://en.wikipedia.org/wiki/Lemmatisation</a>), two <a id="_idIndexMarker579"/>popular NLP techniques. Let's go with the latter here. Depending on your instance type, this could run for several minutes. Here's the code you'll need:<p class="source-code">import string</p><p class="source-code">import nltk</p><p class="source-code">from nltk.corpus import stopwords</p><p class="source-code">#from nltk.stem.snowball import SnowballStemmer</p><p class="source-code">from nltk.stem import WordNetLemmatizer </p><p class="source-code">nltk.download('stopwords')</p><p class="source-code">stop_words = stopwords.words('english')</p><p class="source-code">#stemmer = SnowballStemmer("english")</p><p class="source-code">wnl = WordNetLemmatizer()</p><p class="source-code">def process_text(text):</p><p class="source-code">    for p in string.punctuation:</p><p class="source-code">        text = text.replace(p, '')</p><p class="source-code">        text = ''.join([c for c in text if not </p><p class="source-code">                        c.isdigit()])</p><p class="source-code">        text = text.lower().split()</p><p class="source-code">        text = [w for w in text if not w in </p><p class="source-code">                stop_words]</p><p class="source-code">        #text = [stemmer.stem(w) for w in text]</p><p class="source-code">        text = [wnl.lemmatize(w) for w in text]</p><p class="source-code">        return text</p><p class="source-code">data['headline_text'] = </p><p class="source-code">    data['headline_text'].apply(process_text)</p></li>
				<li>Once processed, the <a id="_idIndexMarker580"/>data should look like this:</li>
			</ol>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="Images/B17705_06_5.jpg" alt="Figure 6.5 – Viewing the lemmatized dataset&#13;&#10;" width="301" height="168"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Viewing the lemmatized dataset</p>
			<p>Now the reviews have been tokenized, we need to convert them to a BoW representation, replacing each word with a unique integer ID and its frequency count.</p>
			<h3>Converting data to a BoW representation</h3>
			<p>We will convert the reviews into a <a id="_idIndexMarker581"/>BoW representation using the<a id="_idIndexMarker582"/> following steps:</p>
			<ol>
				<li value="1">The <strong class="source-inline">gensim</strong> library has exactly what we need! We build a <strong class="bold">dictionary</strong>, a list of all words present in the document collection, using the following code:<p class="source-code">from gensim import corpora</p><p class="source-code">dictionary = corpora.Dictionary(data['headline_text'])</p><p class="source-code">print(dictionary)</p><p>The dictionary looks like this:</p><p class="source-code"><strong class="bold">Dictionary(83131 unique tokens: ['aba', 'broadcasting', 'community', 'decides', 'licence']...)</strong></p><p>This number feels very high. If we have too many dimensions, training will be very long, and the algorithm may have trouble fitting the data; for example, NTM is based on a neural network architecture. The input layer will be sized based on the number of tokens, so we need to keep them reasonably low. It will speed up training and help the encoder learn a manageable number of latent features.</p></li>
				<li>We could go <a id="_idIndexMarker583"/>back and clean the headlines some <a id="_idIndexMarker584"/>more. Instead, we use a <strong class="source-inline">gensim</strong> function that removes extreme words—outlier words that are either extremely rare or extremely frequent. Then, taking a bold bet, we decide to restrict the vocabulary to the top 512 remaining words. Yes—that's less than 1%. Here's the code to do this:<p class="source-code">dictionary.filter_extremes(keep_n=512)</p></li>
				<li>We write the vocabulary to a text file. Not only does this help us check what the top words are, but we'll also pass this file to the NTM algorithm as an extra <strong class="bold">channel</strong>. You'll see why this is important when we train the model. The code to do this is illustrated in the following snippet:<p class="source-code">with open('vocab.txt', 'w') as f:</p><p class="source-code">    for index in range(0,len(dictionary)):</p><p class="source-code">        f.write(dictionary.get(index)+'\n')</p></li>
				<li>We use the dictionary to build a BoW for each headline. It's stored in a new column called <strong class="source-inline">tokens</strong>. When we're done, we drop the text review. The code is illustrated in the following snippet:<p class="source-code">data['tokens'] = data.apply(lambda row: dictionary.doc2bow(row['headline_text']), axis=1)</p><p class="source-code">data = data.drop(['headline_text'], axis=1)</p></li>
				<li>The data should now look like this: </li>
			</ol>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="Images/B17705_06_6.jpg" alt="Figure 6.6 – Viewing the BoW dataset&#13;&#10;" width="273" height="164"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Viewing the BoW dataset</p>
			<p>As you can see, each<a id="_idIndexMarker585"/> word has been replaced with its<a id="_idIndexMarker586"/> unique ID and its frequency count in the review. For instance, the last line tells us that word #11 is present once, word #12 is present once, and so on.</p>
			<p>The data processing is now complete. The last step is to save it to the appropriate input format.</p>
			<h3>Saving input data</h3>
			<p>NTM and LDA expect <a id="_idIndexMarker587"/>data in either a CSV format or a RecordIO-wrapped protobuf format. Just as with the <strong class="bold">factorization matrix</strong> example in <a href="B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Machine Learning Models</em>, the data we're working with is quite sparse. Any given review only contains a small number of words from the vocabulary. As CSV is a dense format, we would end up with a huge amount of zero-frequency words. Not a good idea! </p>
			<p>Once again, we'll use <strong class="source-inline">lil_matrix</strong>, a <strong class="bold">sparse matrix</strong> object<a id="_idIndexMarker588"/> available in <strong class="source-inline">SciPy</strong>. It will have as many lines as we have reviews, and as many columns as we have words in the dictionary. We'll proceed as follows:</p>
			<ol>
				<li value="1">We create the sparse matrix, like this:<p class="source-code">from scipy.sparse import lil_matrix</p><p class="source-code">num_lines = data.shape[0]</p><p class="source-code">num_columns = len(dictionary)</p><p class="source-code">token_matrix = lil_matrix((num_lines,num_columns))</p><p class="source-code">               .astype('float32')</p></li>
				<li>We write a function to add a headline to the matrix. For each token, we simply write its frequency in the appropriate column, as follows:<p class="source-code">def add_row_to_matrix(line, row):</p><p class="source-code">    for token_id, token_count in row['tokens']:</p><p class="source-code">        token_matrix[line, token_id] = token_count</p><p class="source-code">    return</p></li>
				<li>We then <a id="_idIndexMarker589"/>iterate over headlines and add them to the matrix. Quick note: we can't use row index values, as they might be larger than the number of lines. The code is illustrated in the following snippet:<p class="source-code">line = 0</p><p class="source-code">for _, row in data.iterrows():</p><p class="source-code">    add_row_to_matrix(line, row)</p><p class="source-code">    line+=1</p></li>
				<li>The last step is to write this matrix into a memory buffer in <strong class="source-inline">protobuf</strong> format and upload it to S3 for future use, as follows:<p class="source-code">import io, boto3</p><p class="source-code">import sagemaker</p><p class="source-code">import sagemaker.amazon.common as smac</p><p class="source-code">buf = io.BytesIO()</p><p class="source-code">smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)</p><p class="source-code">buf.seek(0)</p><p class="source-code">bucket = sagemaker.Session().default_bucket()</p><p class="source-code">prefix = 'headlines-lda-ntm'</p><p class="source-code">train_key = 'reviews.protobuf'</p><p class="source-code">obj = '{}/{}'.format(prefix, train_key))</p><p class="source-code">s3 = boto3.resource('s3')</p><p class="source-code">s3.Bucket(bucket).Object(obj).upload_fileobj(buf)</p><p class="source-code">s3_train_path = 's3://{}/{}'.format(bucket,obj)</p></li>
				<li>Building <a id="_idIndexMarker590"/>the (1000000, 512) matrix takes a few minutes. Once it's been uploaded to S3, we can see that it's only 42 <strong class="bold">megabytes</strong> (<strong class="bold">MB</strong>). Lil' matrix indeed. The code is illustrated in the following snippet:<p class="source-code"><strong class="bold">$ aws s3 ls s3://sagemaker-eu-west-1-123456789012/amazon-reviews-ntm/training.protobuf</strong></p><p class="source-code"><strong class="bold">43884300 training.protobuf</strong></p></li>
			</ol>
			<p>This concludes the data preparation for LDA and NTM. Now, let's see how we can use text datasets prepared with <strong class="bold">SageMaker Ground Truth</strong>.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor122"/>Using datasets labeled with SageMaker Ground Truth</h2>
			<p>As discussed in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>, SageMaker Ground Truth<a id="_idIndexMarker591"/> supports text classification tasks. We<a id="_idIndexMarker592"/> could definitely use its output to build a dataset for FastText or BlazingText.</p>
			<p>First, I ran a quick text classification job on a few sentences, applying one of two labels: "<strong class="source-inline">aws_service</strong>" if the sentence mentions an AWS service, and "<strong class="source-inline">no_aws_service</strong>" if it doesn't.</p>
			<p>Once the job is <a id="_idIndexMarker593"/>complete, I can fetch the <strong class="bold">augmented manifest</strong> from S3. It's in <strong class="bold">JavaScript Object Notation Lines</strong> (<strong class="bold">JSON Lines</strong>) format, and here's one of its entries:</p>
			<p class="source-code">{"source":"With great power come great responsibility. The second you create AWS resources, you're responsible for them: security of course, but also cost and scaling. This makes monitoring and alerting all the more important, which is why we built services like Amazon CloudWatch, AWS Config and AWS Systems Manager.","my-text-classification-job":0,"my-text-classification-job-metadata":{"confidence":0.84,"job-name":"labeling-job/my-text-classification-job","class-name":"aws_service","human-annotated":"yes","creation-date":"2020-05-11T12:44:50.620065","type":"groundtruth/text-classification"}}</p>
			<p>Shall we write a bit <a id="_idIndexMarker594"/>of Python code to put <a id="_idIndexMarker595"/>this in BlazingText format? Of course! Here we go:</p>
			<ol>
				<li value="1">We load the augmented manifest directly from S3, as follows:<p class="source-code">import pandas as pd</p><p class="source-code">bucket = 'sagemaker-book'</p><p class="source-code">prefix = 'chapter2/classif/output/my-text-classification-job/manifests/output'</p><p class="source-code">manifest = 's3://{}/{}/output.manifest'.format(bucket, prefix)</p><p class="source-code">data = pd.read_json(manifest, lines=True)</p></li>
				<li>The data looks like this:<div id="_idContainer118" class="IMG---Figure"><img src="Images/B17705_06_7.jpg" alt="Figure 6.7 – Viewing the labeled dataset&#13;&#10;" width="727" height="145"/></div><p class="figure-caption">Figure 6.7 – Viewing the labeled dataset</p></li>
				<li>The label is buried in the <strong class="source-inline">my-text-classification-job-metadata</strong> column. We extract it into a new column, as follows:<p class="source-code">def get_label(metadata):</p><p class="source-code">    return metadata['class-name']</p><p class="source-code">data['label'] = </p><p class="source-code">data['my-text-classification-job-metadata'].apply(get_label)</p><p class="source-code">data = data[['label', 'source']]</p></li>
				<li>The data now looks like that shown in the following screenshot. From then on, we can <a id="_idIndexMarker596"/>apply tokenization, and<a id="_idIndexMarker597"/> so on. That was easy, wasn't it?</li>
			</ol>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="Images/B17705_06_8.jpg" alt="Figure 6.8 – Viewing the processed dataset&#13;&#10;" width="414" height="134"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Viewing the processed dataset</p>
			<p>Now, let's build NLP models!</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Using the built-in algorithms for NLP</h1>
			<p>In this section, we're going to train and deploy models with BlazingText, LDA, and NTM. Of course, we'll use the datasets prepared in the previous section.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor124"/>Classifying text with BlazingText</h2>
			<p>BlazingText <a id="_idIndexMarker598"/>makes it extremely easy to build a text classification <a id="_idIndexMarker599"/>model, especially if you have no NLP skills. Let's see how, as follows:</p>
			<ol>
				<li value="1">We upload the training and validation datasets to S3. Alternatively, we could use the output paths returned by a SageMaker Processing job. The code is illustrated in the following snippet:<p class="source-code">import sagemaker</p><p class="source-code">session = sagemaker.Session()</p><p class="source-code">bucket = session.default_bucket()</p><p class="source-code">prefix = 'amazon-reviews'</p><p class="source-code">s3_train_path = session.upload_data(path='/tmp/training.txt', bucket=bucket, key_prefix=prefix+'/input/train')</p><p class="source-code">s3_val_path = session.upload_data(</p><p class="source-code">    path='/tmp/validation.txt', bucket=bucket,   </p><p class="source-code">    key_prefix=prefix+'/input/validation')</p><p class="source-code">s3_output = 's3://{}/{}/output/'.format(bucket, </p><p class="source-code">    prefix)</p></li>
				<li>We <a id="_idIndexMarker600"/>configure the <strong class="source-inline">Estimator</strong> function for BlazingText, as<a id="_idIndexMarker601"/> follows:<p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region_name = session.boto_session.region_name</p><p class="source-code">container = retrieve('blazingtext', region)</p><p class="source-code">bt = sagemaker.estimator.Estimator(container, </p><p class="source-code">     sagemaker.get_execution_role(),</p><p class="source-code">     instance_count=1,</p><p class="source-code">     instance_type='ml.p3.2xlarge',</p><p class="source-code">     output_path=s3_output)</p></li>
				<li>We set a single hyperparameter, telling BlazingText to train in supervised mode, as follows:<p class="source-code">bt.set_hyperparameters(mode='supervised')</p></li>
				<li>We define channels, setting the content type to <strong class="source-inline">text/plain</strong>, and then we launch the training, as follows:<p class="source-code">from sagemaker import TrainingInput</p><p class="source-code">train_data = TrainingInput(</p><p class="source-code">    s3_train_path, content_type='text/plain')</p><p class="source-code">validation_data = TrainingInput(</p><p class="source-code">    s3_val_path, content_type='text/plain')</p><p class="source-code">s3_channels = {'train': train_data, </p><p class="source-code">               'validation': validation_data}</p><p class="source-code">bt.fit(inputs=s3_channels)</p></li>
				<li>We get a <a id="_idIndexMarker602"/>validation accuracy of 88.4%, which is quite<a id="_idIndexMarker603"/> good in the absence of any hyperparameter tweaking. We then deploy the model to a small CPU instance, as follows:<p class="source-code">bt_predictor = bt.deploy(initial_instance_count=1, </p><p class="source-code">                         instance_type='ml.t2.medium')</p></li>
				<li>Once the endpoint is up, we send three tokenized samples for prediction, asking for all three labels, as follows:<p class="source-code">import json</p><p class="source-code">sentences = ['This is a bad camera it doesnt work at all , i want a refund  . ' , 'The camera works , the pictures are decent quality, nothing special to say about it . ' , 'Very happy to have bought this , exactly what I needed . ']</p><p class="source-code">payload = {"instances":sentences, </p><p class="source-code">           "configuration":{"k": 3}}</p><p class="source-code">bt_predictor.serializer =  </p><p class="source-code">    sagemaker.serializers.JSONSerializer()</p><p class="source-code">response = bt_predictor.predict(json.dumps(payload))</p></li>
				<li>Printing the <a id="_idIndexMarker604"/>response, we see that the three samples were<a id="_idIndexMarker605"/> correctly categorized, as illustrated here:<p class="source-code"><strong class="bold">[{'prob': [0.9758228063583374, 0.023583529517054558, 0.0006236258195713162], 'label': ['__label__negative__', '__label__neutral__', '__label__positive__']}, </strong></p><p class="source-code"><strong class="bold">{'prob': [0.5177792906761169, 0.2864232063293457, 0.19582746922969818], 'label': ['__label__neutral__', '__label__positive__', '__label__negative__']}, </strong></p><p class="source-code"><strong class="bold">{'prob': [0.9997835755348206, 0.000205090589588508, 4.133415131946094e-05], 'label': ['__label__positive__', '__label__neutral__', '__label__negative__']}]</strong></p></li>
				<li>As usual, we delete the endpoint once we're done by running the following code:<p class="source-code">bt_predictor.delete_endpoint()</p></li>
			</ol>
			<p>Now, let's train BlazingText to compute word vectors.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor125"/>Computing word vectors with BlazingText</h2>
			<p>The code is almost <a id="_idIndexMarker606"/>identical to the previous example, with<a id="_idIndexMarker607"/> only two differences. First, there is only one channel, containing training data. Second, we need to set BlazingText to UL mode.</p>
			<p>BlazingText supports the<a id="_idIndexMarker608"/> training modes implemented in <a id="_idIndexMarker609"/>Word2Vec: <strong class="bold">skipgram</strong> and <strong class="bold">continuous BoW</strong> (<strong class="bold">CBOW</strong>). It adds a<a id="_idIndexMarker610"/> third mode, <strong class="bold">batch_skipgram</strong>, for faster distributed training. It also supports <strong class="bold">subword embeddings</strong>, a <a id="_idIndexMarker611"/>technique that makes it possible to return a word vector for words that are misspelled or not part of the vocabulary.</p>
			<p>Let's go for skipgram with subword embeddings. We leave the dimension of vectors unchanged (the default is 100). Here's the code you'll need:</p>
			<p class="source-code">bt.set_hyperparameters(mode='skipgram', subwords=True)</p>
			<p>Unlike other <a id="_idIndexMarker612"/>algorithms, there is nothing to deploy <a id="_idIndexMarker613"/>here. The model artifact is in S3 and can be used for downstream NLP applications.</p>
			<p>Speaking of which, BlazingText is compatible with FastText, so how about trying to load the models we just trained into FastText?</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor126"/>Using BlazingText models with FastText</h2>
			<p>First, we need to <a id="_idIndexMarker614"/>compile FastText, which is extremely <a id="_idIndexMarker615"/>simple. You can even do it on a notebook instance without having to install anything. Here's the code you'll need:</p>
			<p class="source-code">$ git clone https://github.com/facebookresearch/fastText.git</p>
			<p class="source-code">$ cd fastText</p>
			<p class="source-code">$ make</p>
			<p>Let's first try our classification model.</p>
			<h3>Using a BlazingText classification model with FastText</h3>
			<p>We <a id="_idIndexMarker616"/>will try the model using the following <a id="_idIndexMarker617"/>steps:</p>
			<ol>
				<li value="1">We copy the model artifact from S3 and extract it as follows:<p class="source-code"><strong class="bold">$ aws s3 cp s3://sagemaker-eu-west-1-123456789012/amazon-reviews/output/JOB_NAME/output/model.tar.gz .</strong></p><p class="source-code"><strong class="bold">$ tar xvfz model.tar.gz</strong></p></li>
				<li>We load <strong class="source-inline">model.bin</strong> with FastText, as follows:<p class="source-code"><strong class="bold">$ ./fasttext predict model.bin -</strong></p></li>
				<li>We <a id="_idIndexMarker618"/>predict samples and view their top class, as follows:<p class="source-code"><strong class="bold">This is a bad camera it doesnt work at all , i want a refund  .</strong></p><p class="source-code"><strong class="bold">__label__negative__</strong></p><p class="source-code"><strong class="bold">The camera works , the pictures are decent quality, nothing</strong></p><p class="source-code"><strong class="bold">special to say about it .</strong></p><p class="source-code"><strong class="bold">__label__neutral__</strong></p><p class="source-code"><strong class="bold">Very happy to have bought this , exactly what I needed</strong></p><p class="source-code"><strong class="bold">__label__positive__</strong></p></li>
			</ol>
			<p>We exit <a id="_idIndexMarker619"/>with <em class="italic">Ctrl</em> + <em class="italic">C</em>. Now, let's explore our vectors.</p>
			<h3>Using BlazingText word vectors with FastText</h3>
			<p>We will <a id="_idIndexMarker620"/>now use FastText with the vectors, as <a id="_idIndexMarker621"/>follows:</p>
			<ol>
				<li value="1">We copy the model artifact from S3 and we extract it, like this:<p class="source-code"><strong class="bold">$ aws s3 cp s3://sagemaker-eu-west-1-123456789012/amazon-reviews-word2vec/output/JOB_NAME/output/model.tar.gz .</strong></p><p class="source-code"><strong class="bold">$ tar xvfz model.tar.gz</strong></p></li>
				<li>We can explore word similarities. For example, let's look for words that are closest to "telephoto". This could help us improve how we handle search queries or how we recommend similar products. Here's the code you'll need:<p class="source-code"><strong class="bold">$ ./fasttext nn vectors.bin</strong></p><p class="source-code"><strong class="bold">Query word? Telephoto</strong></p><p class="source-code"><strong class="bold">telephotos 0.951023</strong></p><p class="source-code"><strong class="bold">75-300mm 0.79659</strong></p><p class="source-code"><strong class="bold">55-300mm 0.788019</strong></p><p class="source-code"><strong class="bold">18-300mm 0.782396</strong></p><p class="source-code"><strong class="bold">. . .</strong></p></li>
				<li>We can also<a id="_idIndexMarker622"/> look for analogies. For<a id="_idIndexMarker623"/> example, let's ask our model the following question: What's the Canon equivalent for the Nikon D3300 camera? The code is illustrated in the following snippet:<p class="source-code"><strong class="bold">$ ./fasttext analogies vectors.bin</strong></p><p class="source-code"><strong class="bold">Query triplet (A - B + C)? nikon d3300 canon</strong></p><p class="source-code"><strong class="bold">xsi 0.748873</strong></p><p class="source-code"><strong class="bold">700d 0.744358</strong></p><p class="source-code"><strong class="bold">100d 0.735871</strong></p><p>According to our model, you should consider the XSI and 700D cameras!</p></li>
			</ol>
			<p>As you can see, word vectors are amazing and BlazingText makes it easy to compute them at any scale. Now, let's move on to topic modeling, another fascinating subject.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor127"/>Modeling topics with LDA</h2>
			<p>In a previous section, we <a id="_idIndexMarker624"/>prepared a million news headlines, and we're now<a id="_idIndexMarker625"/> going to use them for topic modeling with LDA, as follows:</p>
			<ol>
				<li value="1">First, we define useful paths by running the following code:<p class="source-code">import sagemaker</p><p class="source-code">session = sagemaker.Session()</p><p class="source-code">bucket = session.default_bucket()</p><p class="source-code">prefix = reviews-lda-ntm'</p><p class="source-code">train_key = 'reviews.protobuf'</p><p class="source-code">obj = '{}/{}'.format(prefix, train_key)</p><p class="source-code">s3_train_path = 's3://{}/{}'.format(bucket,obj)</p><p class="source-code">s3_output = 's3://{}/{}/output/'.format(bucket, prefix)</p></li>
				<li>We <a id="_idIndexMarker626"/>configure<a id="_idIndexMarker627"/> the <strong class="source-inline">Estimator</strong> function, like this:<p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region_name = session.boto_session.region_name</p><p class="source-code">container = retrieve('lda', region)</p><p class="source-code">lda = sagemaker.estimator.Estimator(container, </p><p class="source-code">      role = sagemaker.get_execution_role(), </p><p class="source-code">      instance_count=1,                                </p><p class="source-code">      instance_type='ml.c5.2xlarge', </p><p class="source-code">      output_path=s3_output)</p></li>
				<li>We set hyperparameters: how many topics we want to build (10), how many dimensions the problem has (the vocabulary size), and how many samples we're training on. Optionally, we can set a parameter named <strong class="source-inline">alpha0</strong>. According to the documentation: "<em class="italic">Small values are more likely to generate sparse topic mixtures and large values (greater than 1.0) produce more uniform mixtures."</em> Let's set it to 0.1 and hope that the algorithm can indeed build well-identified topics. Here's the code you'll need:<p class="source-code">lda.set_hyperparameters(num_topics=5,</p><p class="source-code">   feature_dim=len(dictionary),</p><p class="source-code">   mini_batch_size=num_lines,</p><p class="source-code">   alpha0=0.1)</p></li>
				<li>We <a id="_idIndexMarker628"/>launch the training. As RecordIO is the default format expected <a id="_idIndexMarker629"/>by the algorithm, we don't need to define channels. The code is illustrated in the following snippet:<p class="source-code">lda.fit(inputs={'train': s3_train_path})</p></li>
				<li>Once training is complete, we deploy to a small CPU instance, as follows:<p class="source-code">lda_predictor = lda.deploy(</p><p class="source-code">    initial_instance_count=1,    </p><p class="source-code">    instance_type='ml.t2.medium')</p></li>
				<li>Before we send samples for prediction, we need to process them just like we processed the training set. We write a function that takes care of this: building a sparse matrix, filling it with BoW, and saving to an in-memory protobuf buffer, as follows:<p class="source-code">def process_samples(samples, dictionary):</p><p class="source-code">    num_lines = len(samples)</p><p class="source-code">    num_columns = len(dictionary)</p><p class="source-code">    sample_matrix = lil_matrix((num_lines,  </p><p class="source-code">                    num_columns)).astype('float32')</p><p class="source-code">    for line in range(0, num_lines):</p><p class="source-code">        s = samples[line]</p><p class="source-code">        s = process_text(s)</p><p class="source-code">        s = dictionary.doc2bow(s)</p><p class="source-code">        for token_id, token_count in s:</p><p class="source-code">            sample_matrix[line, token_id] = token_count</p><p class="source-code">        line+=1</p><p class="source-code">    buf = io.BytesIO()</p><p class="source-code">    smac.write_spmatrix_to_sparse_tensor(</p><p class="source-code">        buf,</p><p class="source-code">        sample_matrix,</p><p class="source-code">        None)</p><p class="source-code">    buf.seek(0)</p><p class="source-code">    return buf</p><p>Please note that we need the dictionary here. This is why the corresponding SageMaker<a id="_idIndexMarker630"/> Processing job saved a pickled version of it, which we could later unpickle and use.</p></li>
				<li>Then, we<a id="_idIndexMarker631"/> define a Python array containing five headlines, named <strong class="source-inline">samples</strong>. These are real headlines I copied from the ABC news website at <a href="https://www.abc.net.au/news/">https://www.abc.net.au/news/</a>. The code is illustrated in the following snippet:<p class="source-code">samples = [ "Major tariffs expected to end Australian barley trade to China", "Satellite imagery sparks more speculation on North Korean leader Kim Jong-un", "Fifty trains out of service as fault forces Adelaide passengers to 'pack like sardines", "Germany's Bundesliga plans its return from lockdown as football world watches", "All AFL players to face COVID-19 testing before training resumes" ]</p></li>
				<li>Let's process and predict them, as follows:<p class="source-code">lda_predictor.serializer =  </p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">response = lda_predictor.predict(</p><p class="source-code">           process_samples(samples, dictionary))</p><p class="source-code">print(response)</p></li>
				<li>The response contains a score vector for each review (extra decimals have been removed for brevity). Each vector reflects a mix of topics, with a score per topic. All scores add up to 1. The code is illustrated in the following snippet:<p class="source-code"><strong class="bold">{'predictions': [</strong></p><p class="source-code"><strong class="bold">{'topic_mixture': [0,0.22,0.54,0.23,0,0,0,0,0,0]}, </strong></p><p class="source-code"><strong class="bold">{'topic_mixture': [0.51,0.49,0,0,0,0,0,0,0,0]}, {'topic_mixture': [0.38,0,0.22,0,0.40,0,0,0,0,0]}, {'topic_mixture': [0.38,0.62,0,0,0,0,0,0,0,0]}, {'topic_mixture': [0,0.75,0,0,0,0,0,0.25,0,0]}]}</strong></p></li>
				<li>This isn't <a id="_idIndexMarker632"/>easy to read. Let's print the top topic and its score, as<a id="_idIndexMarker633"/> follows:<p class="source-code">import numpy as np</p><p class="source-code">vecs = [r['topic_mixture'] for r in response['predictions']]</p><p class="source-code">for v in vecs:</p><p class="source-code">    top_topic = np.argmax(v)</p><p class="source-code">    print("topic %s, %2.2f"%(top_topic,v[top_topic]))</p><p>This prints out the following result:</p><p class="source-code"><strong class="bold">topic 2, 0.54</strong></p><p class="source-code"><strong class="bold">topic 0, 0.51</strong></p><p class="source-code"><strong class="bold">topic 4, 0.40</strong></p><p class="source-code"><strong class="bold">topic 1, 0.62</strong></p><p class="source-code"><strong class="bold">topic 1, 0.75 </strong></p></li>
				<li>As usual, we delete the endpoint once we're done, as follows:<p class="source-code">lda_predictor.delete_endpoint()</p></li>
			</ol>
			<p>Interpreting LDA results is not easy, so let's be careful here. No wishful thinking!</p>
			<ul>
				<li>We see that each headline has a definite topic, which is good news. Apparently, LDA was able to identify solid topics, maybe thanks to the low <strong class="source-inline">alpha0</strong> value.</li>
				<li>The top topics for unrelated headlines are different, which is promising.</li>
				<li>The last two headlines are both about sports and their top topic is the same, which is another good sign.</li>
				<li>All five reviews scored zero on topics 5, 6, 8, and 9. This probably means that other topics have been built, and we would need to run more examples to discover them.</li>
			</ul>
			<p>Is this a <a id="_idIndexMarker634"/>successful model? Probably. Can we be confident that topic 0 is about <a id="_idIndexMarker635"/>world affairs, topic 1 about sports, and topic 2 about sports? Not until we've predicted a few thousand more reviews and checked that related headlines are assigned to the same topic.</p>
			<p>As mentioned at the beginning of the chapter, LDA is not a classification algorithm. It has a mind of its own and it may build totally unexpected topics. Maybe it will group headlines according to sentiment or city names. It all depends on the distribution of these words inside the document collection.</p>
			<p>Wouldn't it be nice if we could see which words "weigh" more in a certain topic? That would certainly help us understand the topics a little better. Enter NTM!</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/>Modeling topics with NTM</h2>
			<p>This example<a id="_idIndexMarker636"/> is very similar to the previous one. We'll just highlight the<a id="_idIndexMarker637"/> differences, and you'll find a full example in the GitHub repository for the book. Let's get into it, as follows:</p>
			<ol>
				<li value="1">We upload the <strong class="bold">vocabulary file</strong> to S3, like this:<p class="source-code">s3_auxiliary_path =    </p><p class="source-code">    session.upload_data(path='vocab.txt',   </p><p class="source-code">    key_prefix=prefix + '/input/auxiliary')</p></li>
				<li>We select the NTM algorithm, as follows:<p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region_name = session.boto_session.region_name</p><p class="source-code">container = retrieve('ntm', region)</p></li>
				<li>Once we've configured the <strong class="source-inline">Estimator</strong> function, we set the hyperparameters, as follows:<p class="source-code">ntm.set_hyperparameters(num_topics=10, </p><p class="source-code">                        feature_dim=len(dictionary),</p><p class="source-code">                        optimizer='adam',</p><p class="source-code">                        mini_batch_size=256,</p><p class="source-code">                        num_patience_epochs=10)</p></li>
				<li>We launch <a id="_idIndexMarker638"/>training, passing the vocabulary file in the <strong class="source-inline">auxiliary</strong> channel, as follows:<p class="source-code">ntm.fit(inputs={'train': s3_training_path, </p><p class="source-code">                'auxiliary': s3_auxiliary_path})</p></li>
			</ol>
			<p>When training is <a id="_idIndexMarker639"/>complete, we see plenty of information in the training log. First, we see the average WETC and TU scores for the 10 topics, as follows:</p>
			<p class="source-code">(num_topics:10) [wetc 0.42, tu 0.86]</p>
			<p>These are decent results. Topic unicity is high, and the semantic distance between topic words is average.</p>
			<p>For each topic, we see its WETC and TU scores, as well as its top words—that is to say, the words that have the highest probability of appearing in documents associated with this topic. </p>
			<p>Let's look at each one in detail and try to put names to topics. </p>
			<p>Topic 0 is pretty obvious, I think. Almost all words are related to crime, so let's call it <strong class="source-inline">crime</strong>. You can see this topic here:</p>
			<p class="source-code"><strong class="bold">[0.51, 0.84] stabbing charged guilty pleads murder fatal man assault bail jailed alleged shooting arrested teen girl accused boy car found crash</strong></p>
			<p>The following topic 1 is a little fuzzier. How about <strong class="source-inline">legal</strong>? Have a look at it here:</p>
			<p class="source-code"><strong class="bold">[0.36, 0.85] seeker asylum climate live front hears change export carbon tax court wind challenge told accused rule legal face stand boat</strong></p>
			<p>Topic 2 is about accidents and fires. Let's call it <strong class="source-inline">disaster</strong>. You can see the topic here:</p>
			<p class="source-code"><strong class="bold">[0.39, 0.78] seeker crew hour asylum cause damage truck country firefighter blaze crash warning ta plane near highway accident one fire fatal</strong></p>
			<p>Topic 3 is obvious: <strong class="source-inline">sports</strong>. The TU score is the highest, showing that sports articles use a very specific <a id="_idIndexMarker640"/>vocabulary found nowhere else, as we can see here:</p>
			<p class="source-code"><strong class="bold">[0.54, 0.93] cup world v league one match win title final star live victory england day nrl miss beat team afl player</strong></p>
			<p>Topic 4 is a strange<a id="_idIndexMarker641"/> mix of weather information and natural resources. It has the lowest WETC and the lowest TU score too. Let's call it <strong class="source-inline">unknown1</strong>. Have a look at it here:</p>
			<p class="source-code"><strong class="bold">[0.35, 0.77] coast korea gold north east central pleads west south guilty queensland found qld rain beach cyclone northern nuclear crop mine</strong></p>
			<p>Topic 5 is about world affairs, it seems. Let's call it <strong class="source-inline">international</strong>. You can see the topic here:</p>
			<p class="source-code"><strong class="bold">[0.38, 0.88] iraq troop bomb trade korea nuclear kill soldier iraqi blast pm president china pakistan howard visit pacific u abc anti</strong></p>
			<p>Topic 6 feels like local news, as it contains abbreviations for Australian regions: <strong class="source-inline">qld</strong> is Queensland, <strong class="source-inline">ta</strong> is Tasmania, <strong class="source-inline">nsw</strong> is New South Wales, and so on. Let's call it <strong class="source-inline">local</strong>. The topic is shown here:</p>
			<p class="source-code"><strong class="bold">[0.25, 0.88] news hour country rural national abc ta sport vic abuse sa nsw weather nt club qld award business</strong></p>
			<p>Topic 7 is a no-brainer: <strong class="source-inline">finance</strong>. It has the highest WETC score, showing that its words are closely related from a semantic point of view. Topic unicity is also very high, and we would probably see the same for domain-specific topics on medicine or engineering. Have a look at the topic here:</p>
			<p class="source-code"><strong class="bold">[0.62, 0.90] share dollar rise rate market fall profit price interest toll record export bank despite drop loss post high strong trade</strong></p>
			<p>Topic 8 is about politics, with a bit of crime thrown in. Some people would say that's actually the same thing. As we already have a <strong class="source-inline">crime</strong> topic, we'll name this one <strong class="source-inline">politics</strong>. Have a look at the topic here:</p>
			<p class="source-code"><strong class="bold">[0.41, 0.90] issue election vote league hunt interest poll parliament gun investigate opposition raid arrest police candidate victoria house northern crime rate</strong></p>
			<p>Topic 9 is another <a id="_idIndexMarker642"/>mixed bag. It's hard to say whether it's about farming <a id="_idIndexMarker643"/>or missing people! Let's go with <strong class="source-inline">unknown2</strong>. You can see the topic here:</p>
			<p class="source-code"><strong class="bold">[0.37, 0.84] missing search crop body found wind rain continues speaks john drought farm farmer smith pacific crew river find mark tourist</strong></p>
			<p>All things considered, that's a pretty good model: 8 clear topics out of 10. </p>
			<p>Let's define our list of topics and run our sample headlines through the model after deploying it, as follows:</p>
			<p class="source-code">topics = ['crime','legal','disaster','sports','unknown1',</p>
			<p class="source-code">          'international','local','finance','politics', </p>
			<p class="source-code">          'unknown2']</p>
			<p class="source-code">samples = [ "Major tariffs expected to end Australian barley trade to China", "US woman wanted over fatal crash asks for release after coronavirus halts extradition", "Fifty trains out of service as fault forces Adelaide passengers to 'pack like sardines", "Germany's Bundesliga plans its return from lockdown as football world watches", "All AFL players to face COVID-19 testing before training resumes" ]</p>
			<p>We use the following function to print the top three topics and their score:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">for r in response['predictions']:</p>
			<p class="source-code">    sorted_indexes = np.argsort(r['topic_weights']).tolist()</p>
			<p class="source-code">    sorted_indexes.reverse()</p>
			<p class="source-code">    top_topics = [topics[i] for i in sorted_indexes]</p>
			<p class="source-code">    top_weights = [r['topic_weights'][i] </p>
			<p class="source-code">                   for i in sorted_indexes]</p>
			<p class="source-code">    pairs = list(zip(top_topics, top_weights))</p>
			<p class="source-code">    print(pairs[:3])</p>
			<p>Here's<a id="_idIndexMarker644"/> the <a id="_idIndexMarker645"/>output:</p>
			<p class="source-code"><strong class="bold">[('finance', 0.30),('international', 0.22),('sports', 0.09)]</strong></p>
			<p class="source-code"><strong class="bold">[('unknown1', 0.19),('legal', 0.15),('politics', 0.14)]</strong></p>
			<p class="source-code"><strong class="bold">[('crime', 0.32), ('legal', 0.18), ('international', 0.09)]</strong></p>
			<p class="source-code"><strong class="bold">[('sports', 0.28),('unknown1', 0.09),('unknown2', 0.08)]</strong></p>
			<p class="source-code"><strong class="bold">[('sports', 0.27),('disaster', 0.12),('crime', 0.11)]</strong></p>
			<p>Headlines 0, 2, 3, and 4 are right on target. That's not surprising given how strong these topics are.</p>
			<p>Headline 1 scores very high on the topic we called <strong class="source-inline">legal</strong>. Maybe Adelaide passengers should sue the train company? Seriously, we would need to find other matching headlines to get a better sense of what the topic is really about.</p>
			<p>As you can see, NTM makes it easier to understand what topics are about. We could improve the model by processing the vocabulary file, adding or removing specific words to influence topics, increasing the number of topics, fiddling with <strong class="source-inline">alpha0</strong>, and so on. My intuition tells me that we should really see a "weather" topic in there. Please experiment and see if you want to make it appear.</p>
			<p>If you'd like to run another example, you'll find interesting techniques in this notebook: </p>
			<p><a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb</a></p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor129"/>Summary</h1>
			<p>NLP is a very exciting topic. It's also a difficult one because of the complexity of language in general, and due to how much processing is required to build datasets. Having said that, the built-in algorithms in SageMaker will help you get good results out of the box. Training and deploying models are straightforward processes, which leaves you more time to explore, understand, and prepare data.</p>
			<p>In this chapter, you learned about the BlazingText, LDA, and NTM algorithms. You also learned how to process datasets using popular open source tools such as <strong class="source-inline">nltk</strong>, <strong class="source-inline">spaCy</strong>, and <strong class="source-inline">gensim</strong>, and how to save them in the appropriate format. Finally, you learned how to use the SageMaker SDK to train and deploy models with all three algorithms, as well as how to interpret results. This concludes our exploration of built-in algorithms. </p>
			<p>In the next chapter, you will learn how to use built-in ML frameworks such as <strong class="bold">scikit-learn</strong>, <strong class="bold">TensorFlow</strong>, <strong class="bold">PyTorch</strong>, and <strong class="bold">Apache MXNet</strong>.</p>
		</div>
	</div></body></html>
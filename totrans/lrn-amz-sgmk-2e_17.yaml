- en: 'Chapter 13: Optimizing Prediction Cost and Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to automate training and deployment
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, we'll focus on optimizing cost and performance for prediction
    infrastructure, which typically accounts for 90% of the machine learning spend
    by AWS customers. This number may come as a surprise, until we realize that a
    model built by a single training job may end on multiple endpoints running 24/7
    on a large scale.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, great care must be taken to optimize your prediction infrastructure to
    ensure that you get the most bang for your buck!
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter features the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling an endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a multi-model endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a model with Amazon Elastic Inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling models with Amazon SageMaker Neo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create it. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to install and configure the AWS **Command Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working Python 3.x environment. ­Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory but
    strongly encouraged, as it includes many projects that we will need (Jupyter,
    `pandas`, `numpy`, and more).
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling an endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoscaling has long been the most important technique in adjusting infrastructure
    size for incoming traffic, and it's available for SageMaker endpoints. However,
    it's based on **Application Auto Scaling** and not on **EC2 Auto Scaling** ([https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html](https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html)),
    although the concepts are extremely similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up autoscaling for the **XGBoost** model we trained on the Boston
    Housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create an **endpoint configuration**, and we use it to build the endpoint.
    Here, we use the m5 instance family; t2 and t3 are not recommended for autoscaling
    as their burstable behavior makes it harder to measure their real load:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the endpoint is in service, we define the target value that we want to
    scale on, namely the number of instances backing the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we apply a scaling policy for this target value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the only built-in metric available in SageMaker, `SageMakerVariantInvocationsPerInstance`.
    We could also define a custom metric if we wanted to. We set the metric threshold
    at 1,000 invocations per minute. This is a bit of an arbitrary value. In real
    life, we would run a load test on a single instance and monitor model latency
    in order to find the actual value that ought to trigger autoscaling. You can find
    more information at [https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html).
    We also define a 60-second cooldown for scaling in and out, a good practice for
    smoothing out transient traffic drops and peaks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As shown in the following screenshot, autoscaling is now configured on the endpoint:![Figure
    13.1 – Viewing autoscaling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_13_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.1 – Viewing autoscaling
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using an infinite loop, we send some traffic to the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at the **CloudWatch** metrics for the endpoints, as shown in the following
    screenshot, we see that invocations per instance exceed the threshold we defined:
    1.42k versus 1k:![Figure 13.2 – Viewing CloudWatch metrics'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_13_2.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.2 – Viewing CloudWatch metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Autoscaling quickly kicks in and decides to add another instance, as visible
    in the following screenshot. If the load was even higher, it could decide to add
    several instances at once:![Figure 13.3 – Viewing autoscaling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_13_3.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.3 – Viewing autoscaling
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A few minutes later, the extra instance is in service, and invocations per instance
    are now below the threshold (935 versus 1,000):![Figure 13.4 – Viewing CloudWatch
    metrics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_13_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.4 – Viewing CloudWatch metrics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A similar process takes place when traffic decreases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once we''re finished, we delete everything:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Setting up autoscaling is easy. It helps you automatically adapt your prediction
    infrastructure and the associated costs to changing business conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s study another technique that you''ll find extremely useful when
    you''re dealing with a very large number of models: **multi-model endpoints**.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a multi-model endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-model endpoints are useful when you're dealing with a large number of
    models where it wouldn't make sense to deploy to individual endpoints. For example,
    imagine a SaaS company building a regression model for each one of their 10,000
    customers. Surely, they wouldn't want to manage (and pay for) 10,000 endpoints!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multi-model endpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A multi-model endpoint can serve CPU-based predictions from an arbitrary number
    of models stored in S3 (GPUs are not supported at the time of writing). The path
    of the model artifact to use is passed in each prediction request. Models are
    loaded and unloaded dynamically, according to usage and the amount of memory available
    on the endpoint. Models can also be added to, or removed from, the endpoint by
    simply copying or deleting artifacts in S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to serve multiple models, your inference container must implement
    a specific set of APIs that the endpoint will invoke: LOAD MODEL, LIST MODEL,
    GET MODEL, UNLOAD MODEL, and INVOKE MODEL. You can find the details at [https://docs.aws.amazon.com/sagemaker/latest/dg/mms-container-apis.html](https://docs.aws.amazon.com/sagemaker/latest/dg/mms-container-apis.html).'
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the latest built-in containers for **scikit-learn**,
    **TensorFlow**, **Apache MXNet**, and **PyTorch** natively support these APIs.
    The **XGBoost**, **kNN**, **Linear Learner**, and **Random Cut Forest** built-in
    algorithms also support them.
  prefs: []
  type: TYPE_NORMAL
- en: For other algorithms and frameworks, your best option is to build a custom container
    that includes the **SageMaker Inference Toolkit**, as it already implements the
    required APIs ([https://github.com/aws/sagemaker-inference-toolkit](https://github.com/aws/sagemaker-inference-toolkit)).
  prefs: []
  type: TYPE_NORMAL
- en: This toolkit is based on the multi-model server ([https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server)),
    which you could also use directly from the CLI to serve predictions from multiple
    models. You can find more information at [https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html).
  prefs: []
  type: TYPE_NORMAL
- en: Building a multi-model endpoint with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s build a multi-model endpoint with **scikit-learn**, hosting models trained
    on the Boston Housing dataset. This is only supported on scikit-learn 0.23-1 and
    above:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We upload the dataset to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train three models with a different test size, storing their names in a
    dictionary. Here, we use the latest version of scikit-learn, the first one to
    support multi-model endpoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We find the S3 URI of the model artifact along with its prefix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We delete any previous model stored in S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We copy the three model artifacts to this location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This lists the model artifacts:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the name of the script and the S3 location where we''ll upload the
    code archive. Here, I''m passing the training script, which includes a `model_fn()`
    function to load the model. This is the only function that will be used to serve
    predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the code archive and we upload it to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the multi-model endpoint with the `create_model()` API and we set
    the `Mode` parameter accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the endpoint configuration as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the endpoint as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the endpoint is in service, we load samples from the dataset and convert
    them to a `numpy` array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We predict these samples with all three models, passing the name of the model
    to use for each prediction request, such as **sagemaker-scikit-learn-2021-09-01-08-05-33-229**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We could train more models, copy their artifacts to the same S3 location, and
    use them directly without recreating the endpoint. We could also delete those
    models we don't need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we''re finished, we delete the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, multi-model endpoints are a great way to serve as many models
    as you'd like from a single endpoint, and setting them up isn't difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we''re going to study another cost optimization technique
    that can help you save a lot of money on GPU prediction: **Amazon Elastic Inference**.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model with Amazon Elastic Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When deploying a model, you have to decide whether it should run on a CPU instance
    or on a GPU instance. In some cases, there isn't much of a debate. For example,
    some algorithms simply don't benefit from GPU acceleration, so they should be
    deployed to CPU instances. At the other end of the spectrum, complex deep learning
    models for computer vision or natural language processing run best on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the situation is not that clear-cut. First, you should know what
    the maximum predicted latency is for your application. If you're predicting a
    click-through rate for a real-time ad tech application, every millisecond counts;
    if you're predicting customer churn in a back-office application, not so much.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, even models that could benefit from GPU acceleration may not be
    large and complex enough to fully utilize the thousands of cores available on
    a modern GPU. In such scenarios, you''re stuck between a rock and a hard place:
    deploying on CPU would be a little slow for your needs, and deploying on GPU wouldn''t
    be cost-effective.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the problem that Amazon Elastic Inference aims to solve ([https://aws.amazon.com/machine-learning/elastic-inference/](https://aws.amazon.com/machine-learning/elastic-inference/)).
    It lets you attach fractional GPU acceleration to any EC2 instance, including
    notebook instances and endpoint instances. **Accelerators** come in three different
    sizes (medium, large, and extra large), which let you find the best cost-performance
    ratio for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Inference is available for **TensorFlow**, **PyTorch**, and **Apache
    MXNet**. You can use it in your own code running on EC2 instances, thanks to AWS
    extensions available in the **Deep Learning AMI**. You can also use it with **Deep
    Learning Containers**. More information is available at [https://docs.aws.amazon.com/elastic-inference/latest/developerguide/working-with-ei.html](https://docs.aws.amazon.com/elastic-inference/latest/developerguide/working-with-ei.html).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, **Elastic Inference** is available on SageMaker. You can attach an
    accelerator to a **Notebook Instance** at creation time and work with the built-in
    **conda** environments. You can also attach an accelerator to an endpoint, and
    we'll show you how to do this in the next example.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model with Amazon Elastic Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s reuse the **Image Classification** model we trained on dog and cat images
    in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091), *Training Computer
    Vision Models*. This is based on an 18-layer **ResNet** model, which is pretty
    small as far as convolution neural networks are concerned:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been trained, we deploy it as usual on two endpoints: one
    backed by an `ml.c5.large` instance and another one backed by an `ml.g4dn.xlarge`
    instance, the most cost-effective GPU instance available on SageMaker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then download a test image, predict it 1,000 times, and measure the total
    time it takes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The results are shown in the next table (us-east-1 prices):![](img/B17705_13_Table_1.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unsurprisingly, the GPU instance is about twice as fast. Yet, the CPU instance
    is more cost-effective, as it's over four times less expensive. Putting it another
    way, you could run your endpoint with four CPU instances instead of one GPU instance
    and get more throughput for the same cost. This shows why it's so important to
    understand the latency requirement of your application. "Fast" and "slow" are
    very relative concepts!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then deploy the same model on three more endpoints backed by `ml.c5.large`
    instances, accelerated by a medium, large, and extra-large `deploy()` API. Here''s
    the code for the medium endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see the results in the following table:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17705_13_Table_2.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We get up to 20% speed-up compared to the naked CPU endpoint, and the cost
    is lower than if we used a GPU instance. Let''s keep tweaking:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Attentive readers will have noticed that the previous tables include teraFLOP
    values for both 32-bit and 16-bit floating-point values. Indeed, either one of
    these data types may be used to store model parameters. Looking at the documentation
    for the image classification algorithm, we see that we can actually select a data
    type with the `precision_dtype` parameter and that the default value is `float32`.
    This begs the question: would the results differ if we trained our model in `float16`
    mode? There''s only one way to know, isn''t there?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Training again, we hit the same model accuracy as in `float32` mode. Deploying
    benchmarking again, we get the following results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17705_13_Table_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: No meaningful difference is visible on the naked instances. Predicting with
    an **FP-16** model on the large and extra-large accelerators helps us speed up
    predictions by about 10% compared to the **FP-32** model. Pretty good! This performance
    level is definitely a nice upgrade compared to a naked CPU instance, and it's
    cost-effective compared to a GPU instance.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, switching a single endpoint instance from `ml.g4dn.xlarge` to `ml.c5.large+ml.eia2.large`
    would save you ($0.736–$0.438) x 24 x 30 = $214 dollars per month. That's serious
    money!
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Amazon Elastic Inference is extremely easy to use, and it gives
    you additional deployment options. Once you've defined the prediction latency
    requirement for your application, you can quickly experiment and find the best
    cost-performance ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s talk about another SageMaker capability that lets you compile models
    for a specific hardware architecture: **Amazon Neo**.'
  prefs: []
  type: TYPE_NORMAL
- en: Compiling models with Amazon SageMaker Neo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embedded software developers have long learned how to write highly optimized
    code that both runs fast and uses hardware resources frugally. In theory, the
    same techniques could also be applied to optimize machine learning predictions.
    In practice, this is a daunting task given the complexity of machine learning
    libraries and models.
  prefs: []
  type: TYPE_NORMAL
- en: This is the problem that Amazon SageMaker Neo aims to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Amazon SageMaker Neo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon Neo has two components: a model compiler that optimizes models for the
    underlying hardware, and a small runtime named **Deep Learning Runtime** (**DLR**),
    used to load optimized models and run predictions ([https://aws.amazon.com/sagemaker/neo](https://aws.amazon.com/sagemaker/neo)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon SageMaker Neo can compile models trained with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Two built-in algorithms**: XGBoost and Image Classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Built-in frameworks**: TensorFlow, PyTorch, and Apache MXNet, as well as
    models in **ONNX** format. Many operators are supported, and you can find the
    full list at [https://aws.amazon.com/releasenotes/sagemaker-neo-supported-frameworks-and-operators](https://aws.amazon.com/releasenotes/sagemaker-neo-supported-frameworks-and-operators).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training takes place as usual, using your estimator of choice. Then, using
    the `compile_model()` API, we can easily compile the model for one of these hardware
    targets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon EC2 instances of the following families: `c4`, `c5`, `m4`, `m5`, `p2`,
    `p3`, and `inf1` (which we''ll discuss later in this chapter), as well as Lambda'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI-powered cameras: AWS DeepLens and Acer aiSage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA Jetson platforms: TX1, TX2, Nano, and Xavier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raspberry Pi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System-on-chip platforms from Rockchip, Qualcomm, Ambarella, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model compilation performs both architecture optimizations (such as fusing layers)
    and code optimizations (replacing machine learning operators with hardware-optimized
    versions). The resulting artifact is stored in S3 and contains both the original
    model and its optimized form.
  prefs: []
  type: TYPE_NORMAL
- en: The DLR is then used to load the model and predict with it. Of course, it can
    be used in a standalone fashion, such as on a Raspberry Pi. You can find installation
    instructions at [https://neo-ai-dlr.readthedocs.io](https://neo-ai-dlr.readthedocs.io).
    As the DLR is open source ([https://github.com/neo-ai/neo-ai-dlr](https://github.com/neo-ai/neo-ai-dlr)),
    you can also build it from source and – why not? – customize it for your own hardware
    platform!
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to using the DLR with SageMaker, things are much simpler. SageMaker
    provides built-in containers with Neo support, and these are the ones you should
    use to deploy models compiled with Neo (as already mentioned, the training container
    remains unchanged). You can find a list of Neo-enabled containers at [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-cli.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-cli.html).
  prefs: []
  type: TYPE_NORMAL
- en: Last, but not least, one of the benefits of the DLR is its small size. For example,
    the Python package for p2 and p3 instances is only 5.4 MB in size, orders of magnitude
    smaller than your typical deep learning library and its dependencies. This is
    obviously critical for embedded environments, and it's also welcome on SageMaker
    as containers will be smaller too.
  prefs: []
  type: TYPE_NORMAL
- en: Let's reuse our image classification example and see whether Neo can speed it
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling and deploying an image classification model on SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to give Neo a little more work, we train a 50-layer ResNet this time.
    Then, we''ll compile it, deploy it to an endpoint, and compare it with the vanilla
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `num_layers` to `50`, we train the model for 30 epochs. Then, we deploy
    it to an `ml.c5.4xlarge` instance as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We compile the model with Neo, targeting the EC2 c5 instance family. We also
    define the input shape of the model: one image, three channels (red, green, blue),
    and 224 x 224 pixels (the default value for the image classification algorithm).
    As built-in algorithms are implemented with Apache MXNet, we set the framework
    accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then deploy the compiled model as usual, explicitly setting the prediction
    container to the Neo-enabled version of image classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Downloading a test image, and using the same benchmarking function that we
    used for Amazon Elastic Inference, we measure the time required to predict 1,000
    images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Prediction with the vanilla model takes 87 seconds. Prediction with the Neo-optimized
    model takes 28.5 seconds, three times faster! That compilation step sure paid
    off. You'll also be happy to learn that compiling Neo models is free of charge,
    so there's really no reason not to try it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's take a look at these compiled models.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring models compiled with Neo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at the output location passed to the `compile_model()` API, we see
    the model artifact generated by Neo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Copying it locally and extracting it, we see that it contains both the original
    model and its compiled version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In particular, the `compiled.so` file is a native file containing hardware-optimized
    versions of the model operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We could look at the assembly code for these, but something tells me that most
    of you wouldn't particularly enjoy it. Joking aside, this is completely unnecessary.
    All we need to know is how to compile and deploy models with Neo.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how about we deploy our model on a **Raspberry Pi**?
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an image classification model on a Raspberry Pi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Raspberry Pi is a fantastic device, and despite its limited compute and
    memory capabilities, it's well capable of predicting images with complex deep
    learning models. Here, I'm using a Raspberry Pi 3 Model B, with a 1.2 GHz quad-core
    ARM processor and 1 GB of memory. That's definitely not much, yet it could run
    a vanilla Apache MXNet model.
  prefs: []
  type: TYPE_NORMAL
- en: Inexplicably, there is no pre-packaged version of MXNet for Raspberry Pi, and
    building it from source is a painstakingly long and unpredictable process. (I'm
    looking at you, OOM errors!) Fortunately, thanks to the DLR, we can do away with
    all of it!
  prefs: []
  type: TYPE_NORMAL
- en: 'In our SageMaker notebook, we compile the model for the Raspberry Pi:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On our local machine, we fetch the compiled model artifact from S3 and copy
    it to the Raspberry Pi:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Moving to the Raspberry Pi, we extract the compiled model to the `resnet50`
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Installing the DLR is very easy. We locate the appropriate package at [https://github.com/neo-ai/neo-ai-dlr/releases](https://github.com/neo-ai/neo-ai-dlr/releases),
    download it, and use `pip` to install it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We first write a function that loads an image from a file, resizes it to 224
    x 224 pixels, and shapes it as a (1, 3, 224, 224) `numpy` array, the correct input
    shape of our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we import the DLR and load the compiled model from the `resnet50` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we load a dog image… or an image of a cat. Your choice!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we predict the image 100 times, printing the prediction to defeat
    any lazy evaluation that MXNet could implement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following dog and cat images are respectively predicted as [2.554065e-09
    1.000000e+00] and [9.9967313e-01 3.2689856e-04], which is very nice given the
    validation accuracy of our model (about 84%):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Test images (source: Wikimedia)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_13_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.5 – Test images (source: Wikimedia)'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction time is about 1.2 seconds per image, which is slow but certainly
    good enough for plenty of embedded applications. Predicting with the vanilla model
    takes about 6–7 seconds, so the speed-up is very significant.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, compiling models is a very effective technique. In the next
    section, we're going to focus on one of Neo's targets, **AWS Inferentia**.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models on AWS Inferentia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS Inferentia is a custom chip designed specifically for high-throughput and
    low-cost prediction ([https://aws.amazon.com/machine-learning/inferentia](https://aws.amazon.com/machine-learning/inferentia)).
    Inferentia chips are hosted on **EC2 inf1** instances. These come in different
    sizes, with 1, 4, or 16 chips. Each chip contains four **NeuronCores**, implementing
    high-performance matrix multiply engines that speed up typical deep learning operations
    such as convolution. NeuronCores also contain large caches that save external
    memory accesses.
  prefs: []
  type: TYPE_NORMAL
- en: In order to run on Inferentia, models need to be compiled and deployed with
    the Neuron SDK ([https://github.com/aws/aws-neuron-sdk](https://github.com/aws/aws-neuron-sdk)).
    This SDK lets you work with TensorFlow, PyTorch, and Apache MXNet models.
  prefs: []
  type: TYPE_NORMAL
- en: You can work with the Neuron SDK on EC2 instances, compiling and deploying models
    yourself. Once again, SageMaker simplifies the whole process, as inf1 instances
    are part of the target architectures that Neo can compile models for.
  prefs: []
  type: TYPE_NORMAL
- en: You can find an example at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker_neo_compilation_jobs/deploy_tensorflow_model_on_Inf1_instance](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker_neo_compilation_jobs/deploy_tensorflow_model_on_Inf1_instance).
  prefs: []
  type: TYPE_NORMAL
- en: To close this chapter, let's sum up all the cost optimization techniques we
    discussed throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Building a cost optimization checklist
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should constantly pay attention to cost, even in the early stages of your
    machine learning project. Even if you're not paying the AWS bill, someone is,
    and I'm sure you'll quite quickly find out who that person is if you spend too
    much.
  prefs: []
  type: TYPE_NORMAL
- en: Regularly going through the following checklist will help you spend as little
    as possible, get the most machine learning-happy bang for your buck, and hopefully
    keep the finance team off your back!
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing costs for data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With so much focus on optimizing training and deployment, it's easy to overlook
    data preparation. Yet, this critical piece of the machine learning workflow can
    incur very significant costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #1'
  prefs: []
  type: TYPE_NORMAL
- en: Resist the urge to build ad hoc ETL tools running on instance-based services.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, your workflows will require data to be processed in a custom fashion,
    such as applying domain-specific feature engineering. Working with a managed service
    such as **Amazon Glue**, **Amazon Athena**, or **Amazon SageMaker Data Wrangler**,
    you will never have to provision any infrastructure, and you will only pay for
    what you use.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a second choice, **Amazon EMR** is a fine service, provided that you understand
    how to optimize its cost. As much as possible, you should avoid running long-lived,
    low-usage clusters. Instead, you should run transient clusters and rely massively
    on **Spot Instances** for task nodes. You can find more information at the following
    sites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same advice applies to **Amazon EC2** instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #2'
  prefs: []
  type: TYPE_NORMAL
- en: Use SageMaker Ground Truth and automatic labeling to cut down on data labeling
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to label large unstructured datasets, enabling automatic labeling
    in **SageMaker Ground Truth** can save you a significant amount of time and money
    compared to labeling everything manually. You can read about it at [https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html).
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing costs for experimentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimentation is another area that is often overlooked, and you should apply
    the following tips to minimize the related spend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #3'
  prefs: []
  type: TYPE_NORMAL
- en: You don't have to use SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: As explained in [*Chapter 1*](B17705_01_Final_JM_ePub.xhtml#_idTextAnchor013),
    *Introducing Amazon SageMaker*, you can easily work with SageMaker Python SDK
    on your local machine or on a local development server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #4'
  prefs: []
  type: TYPE_NORMAL
- en: Stop Studio instances when you don't need them.
  prefs: []
  type: TYPE_NORMAL
- en: This sounds like an obvious one, but are you really doing it? There's really
    no reason to run idle instances; commit your work, stop them, and then restart
    them when you need them again. Storage is persisted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #5'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment on a small scale and with instances of the correct size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you really need the full dataset to start visualizing data and evaluating
    algorithms? Probably not. By working on a small fraction of your dataset, you''ll
    be able to use smaller notebook instances. Here''s an example: imagine 5 developers
    working 10 hours a day on their own `ml.c5.2xlarge` notebook instance. The daily
    cost is 5 x 10 x $0.557 = $27.85\.'
  prefs: []
  type: TYPE_NORMAL
- en: Right-sizing to `ml.t3.xlarge` (less RAM, burstable behavior), the daily cost
    would be reduced to 5 x 10 x $0.233 = $11.65\. You would save $486 per month,
    which you could certainly spend on more experimentation, more training, and more
    **automatic model tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to perform large-scale cleaning and processing, please take the
    time to migrate that work to a managed service (see Tip #1) instead of working
    all day long with a humongous instance. Don''t say, "Me? Never!" I know you''re
    doing it!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #6'
  prefs: []
  type: TYPE_NORMAL
- en: Use local mode.
  prefs: []
  type: TYPE_NORMAL
- en: We saw in [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending
    Machine Learning Services with Built-In Frameworks*, how to use **local mode**
    to avoid firing up managed infrastructure in the AWS cloud. This is a great technique
    to quickly iterate at no cost in the experimentation phase!
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing costs for model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many techniques you can use, and we've already discussed most of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #7'
  prefs: []
  type: TYPE_NORMAL
- en: Don't train on Studio instances.
  prefs: []
  type: TYPE_NORMAL
- en: I'm going to repeat myself here, but it's an important point. Unfortunately,
    this antipattern seems to be pretty common. People pick a large instance (such
    as `ml.p3.2xlarge`), fire up a large job in their notebook, leave it running,
    forget about it, and end up paying good money for an instance sitting idle for
    hours once the job is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, please run your training jobs on **managed instances**. Thanks to **distributed**
    **training**, you'll get your results much quicker, and as instances are terminated
    as soon as training is complete, you will never overpay for training.
  prefs: []
  type: TYPE_NORMAL
- en: As a bonus, you won't be at the mercy of a clean-up script (or an overzealous
    admin) killing all notebook instances in the middle of the night ("because they're
    doing nothing, right?").
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #8'
  prefs: []
  type: TYPE_NORMAL
- en: Pack your dataset in RecordIO/TFRecord files.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it easier and faster to move your dataset around and distribute it
    to training instances. We discussed this at length in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091),
    *Training Computer Vision Models*, and [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108),
    *Training Natural Language Processing Models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #9'
  prefs: []
  type: TYPE_NORMAL
- en: Use pipe mode.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipe mode** streams your dataset directly from Amazon S3 to your training
    instances. No copying is involved, which saves on start-up time. We discussed
    this feature in detail in [*Chapter 9*](B17705_09_Final_JM_ePub.xhtml#_idTextAnchor168),
    *Scaling Your Training Jobs*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #10'
  prefs: []
  type: TYPE_NORMAL
- en: Right-size training instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw how to do this in [*Chapter 9*](B17705_09_Final_JM_ePub.xhtml#_idTextAnchor168),
    *Scaling Your Training Jobs*. One word: **CloudWatch** metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #11'
  prefs: []
  type: TYPE_NORMAL
- en: Use Managed Spot Training.
  prefs: []
  type: TYPE_NORMAL
- en: We covered this in great detail in [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Advanced Training Techniques*. If that didn't convince you, nothing will! Seriously,
    there are very few instances when **Managed Spot Training** should not be used,
    and it should be a default setting in your notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #12'
  prefs: []
  type: TYPE_NORMAL
- en: Use AWS-provided versions of TensorFlow, Apache MXNet, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have entire teams dedicated to extracting the last bit of performance from
    deep learning libraries on AWS. No offense, but if you think you can `pip install`
    and go faster, your time is probably better invested elsewhere. You can find more
    information at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/faster-training-with-optimized-tensorflow-1-6-on-amazon-ec2-c5-and-p3-instances/](https://aws.amazon.com/blogs/machine-learning/faster-training-with-optimized-tensorflow-1-6-on-amazon-ec2-c5-and-p3-instances/),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/about-aws/whats-new/2018/11/tensorflow-scalability-to-256-gpus/](https://aws.amazon.com/about-aws/whats-new/2018/11/tensorflow-scalability-to-256-gpus/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/amazon-web-services-aSchieves-fastest-training-times-for-bert-and-mask-r-cnn/](https://aws.amazon.com/blogs/machine-learning/amazon-web-services-aSchieves-fastest-training-times-for-bert-and-mask-r-cnn/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing costs for model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This very chapter was dedicated to several of these techniques. I'll add a few
    more ideas to cut costs even further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #13'
  prefs: []
  type: TYPE_NORMAL
- en: Use batch transform if you don't need online predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Some applications don't require a live endpoint. They are perfectly fine with
    **batch transform**, which we studied in [*Chapter 11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237),
    *Deploying Machine Learning Models*. The extra benefit is that the underlying
    instances are terminated automatically when the batch job is done, meaning that
    you will never overpay for prediction because you left an endpoint running for
    a week for no good reason.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #14'
  prefs: []
  type: TYPE_NORMAL
- en: Delete unnecessary endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: This requires no explanation, and I have written "Delete the endpoint when you're
    done" tens of times in this book already. Yet, this is still a common mistake.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #15'
  prefs: []
  type: TYPE_NORMAL
- en: Right-size endpoints and use autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #16'
  prefs: []
  type: TYPE_NORMAL
- en: Use a multi-model endpoint to consolidate models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #17'
  prefs: []
  type: TYPE_NORMAL
- en: Compile models with Amazon Neo to use fewer hardware resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #18'
  prefs: []
  type: TYPE_NORMAL
- en: At large scale, use AWS Inferentia instead of GPU instances.
  prefs: []
  type: TYPE_NORMAL
- en: And, of course, the mother of all tips for all things AWS, which is why we dedicated
    a full chapter to it ([*Chapter 12*](B17705_12_Final_JM_ePub.xhtml#_idTextAnchor260),
    *Automating Machine Learning Workflows*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #19'
  prefs: []
  type: TYPE_NORMAL
- en: Automate, automate, automate!
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip #20'
  prefs: []
  type: TYPE_NORMAL
- en: Purchase Savings Plans for Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: '**Savings Plans** is a flexible pricing model that offers low prices on AWS
    usage, in exchange for a commitment to a consistent amount of usage for a one-year
    or three-year term ([https://aws.amazon.com/savingsplans/](https://aws.amazon.com/savingsplans/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Savings Plans is now available for SageMaker, and you'll find it in the console
    at [https://console.aws.amazon.com/cost-management/home?/savings-plans/](https://console.aws.amazon.com/cost-management/home?/savings-plans/).
  prefs: []
  type: TYPE_NORMAL
- en: Built-in recommendations help you pick the right commitment and purchase a plan
    in minutes. Depending on the term and the commitment, you could save up to 72%
    (!) on all instance-based SageMaker costs. You can find a demo at [https://aws.amazon.com/blogs/aws/slash-your-machine-learning-costs-with-instance-price-reductions-and-savings-plans-for-amazon-sagemaker/](https://aws.amazon.com/blogs/aws/slash-your-machine-learning-costs-with-instance-price-reductions-and-savings-plans-for-amazon-sagemaker/).
  prefs: []
  type: TYPE_NORMAL
- en: Equipped with this checklist, not only will you slash your machine learning
    budget but you will also build more robust and more agile workflows. Rome wasn't
    built in a day, so please take your time, use common sense, apply the techniques
    that matter most right now, and iterate.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, you learned different techniques that help to reduce
    prediction costs with SageMaker. First, you saw how to use autoscaling to scale
    prediction infrastructure according to incoming traffic. Then, you learned how
    to deploy an arbitrary number of models on the same endpoint, thanks to multi-model
    endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: We also worked with Amazon Elastic Inference, which allows you to add fractional
    GPU acceleration to a CPU-based instance and find the right cost-performance ratio
    for your application. We then moved on to Amazon SageMaker Neo, an innovative
    capability that compiles models for a given hardware architecture, both for EC2
    instances and embedded devices. Finally, we built a cost optimization checklist
    that will come in handy for your upcoming SageMaker projects.
  prefs: []
  type: TYPE_NORMAL
- en: You've made it to the end. Congratulations! You now know a lot about SageMaker.
    Now, go grab a dataset, build cool stuff, and let me know about it!
  prefs: []
  type: TYPE_NORMAL

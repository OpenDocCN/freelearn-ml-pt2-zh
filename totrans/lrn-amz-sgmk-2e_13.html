<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer163">
			<h1 id="_idParaDest-207"><a id="_idTextAnchor206"/>Chapter 10: Advanced Training Techniques</h1>
			<p>In the previous chapter, you learned when and how to scale training jobs using features such as <strong class="bold">Pipe mode</strong> and <strong class="bold">distributed training</strong>, as well as alternatives to <strong class="bold">S3</strong> for dataset storage.</p>
			<p>In this chapter, we'll conclude our exploration of training techniques. In the first part of the chapter, you'll learn how to slash down your training costs with <strong class="bold">managed spot training</strong>, how to squeeze every drop of accuracy from your models with <strong class="bold">automatic model tuning</strong>, and how to crack models open with <strong class="bold">SageMaker Debugger</strong>.</p>
			<p>In the second part of the chapter, we'll introduce two new SageMaker capabilities that help you build more efficient workflows and higher quality models: <strong class="bold">SageMaker Feature Store</strong> and <strong class="bold">SageMaker Clarify</strong>. </p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Optimizing training costs with managed spot training</li>
				<li>Optimizing hyperparameters with automatic model tuning</li>
				<li>Exploring models with SageMaker Debugger</li>
				<li>Managing features and building datasets with SageMaker Feature Store</li>
				<li>Detecting bias and explaining predictions with SageMaker Clarify</li>
			</ul>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor207"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you haven't got one already, please point your browser at <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create it. You should also familiarize yourself with the AWS free tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need <a id="_idIndexMarker1069"/>to install and configure the AWS <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>) for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).  </p>
			<p>You will <a id="_idIndexMarker1070"/>need a working <strong class="bold">Python</strong> <strong class="bold">3.x</strong> environment. Installing the <strong class="bold">Anaconda</strong> distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory but strongly <a id="_idIndexMarker1071"/>encouraged as it includes many projects that we will need (<strong class="bold">Jupyter</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>Code examples included in this book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor208"/>Optimizing training costs with managed spot training</h1>
			<p>In the <a id="_idIndexMarker1072"/>previous chapter, we trained the <strong class="bold">image classification</strong> algorithm on the <strong class="bold">ImageNet</strong> dataset. The job ran for a <a id="_idIndexMarker1073"/>little less than 4 hours. At about $290 per hour, this job cost us roughly $1,160. That's a lot of money… but is it really?</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor209"/>Comparing costs</h2>
			<p>Before you throw your arms up the air yelling "<em class="italic">What is he thinking?</em>", please consider how much it would cost your organization to own and run this training cluster:</p>
			<ol>
				<li>A back-of-the-<a id="_idIndexMarker1074"/>envelope calculation for capital expenditure (servers, storage, GPUs, 100 Gbit/s networking equipment) says at least $1.5M. As far as operational expenditure is concerned, hosting costs won't be cheap, as each equivalent server will require 4-5 kW of power. That's enough to fill one rack at your typical hosting company, so even if high-density racks are available, you'll need several. Add bandwidth, cross connects, and so on, and my gut feeling says it would cost about $15K per month (much more in certain parts of the world). </li>
				<li>We would need to add hardware support contracts (say, 10% per year, so $150K). Depreciating this cluster over 5 years, total monthly costs would be ($1.5M + 60*$15K + 5*$150K)/60 = $52.5K. Let's round it to $55K to account for labor costs for server maintenance and so on.</li>
			</ol>
			<p>Using conservative estimates, this spend is equivalent to 190 hours of training with the large $290-an-hour cluster we've used for our ImageNet example. As we will see later in this chapter, managed spot training routinely delivers savings of 70%. So, now the spend would be equivalent to about 633 hours of ImageNet training per month. </p>
			<p>This amounts to 87% usage (633/720) month in, month out, and it's very unlikely you'd keep your training cluster that busy. Add downtime, accelerated depreciation caused by hardware innovation, hardware insurance costs, the opportunity cost of not investing $1.5M in other <a id="_idIndexMarker1075"/>ventures, and so on, and the business case for physical infrastructure gets worse by the minute.</p>
			<p>Financials matter, but the worst thing is that you'd only have one cluster. What if a potential business opportunity required another one? Would you spend another $1.5M? If not, would you have to time-share the existing cluster? Of course, only you could decide what's best for your organization. Just make sure that you look at the big picture.</p>
			<p>Now, let's see how you can easily enjoy that 70% cost reduction.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor210"/>Understanding Amazon EC2 Spot Instances</h2>
			<p>At any given time, <strong class="bold">Amazon</strong> <strong class="bold">EC2</strong> has more <a id="_idIndexMarker1076"/>capacity than needed. This allows customers to add on-demand <a id="_idIndexMarker1077"/>capacity to their platforms whenever they need to. On-demand instances may be created explicitly using an API call, or automatically if <strong class="bold">Auto Scaling</strong> is configured. Once a customer has acquired an on-demand instance, they will keep it until they decide to release it, either explicitly or automatically.</p>
			<p><strong class="bold">Spot Instances</strong> are a simple <a id="_idIndexMarker1078"/>way to tap into this unused capacity and to enjoy very significant discounts (50-70% are typical). You can request them in the same way, and they behave the same too. The only difference is that should AWS need the capacity to build on-demand instances, your Spot Instance may be reclaimed. It will receive an interruption notification two minutes before being forcefully terminated.</p>
			<p>This isn't as bad as it sounds. Depending on regions and instance families, Spot Instances may not be reclaimed very often, and customers routinely keep them for days, if not more. In addition, you can architecture your application for this requirement, for example, by running stateless workloads on Spot Instances and relying on managed services for data storage. The cost benefit is too good to pass!</p>
			<p>Going to the <strong class="bold">Spot Requests</strong> section in the EC2 console, you can view the price history per instance type in each region. For example, the following screenshot shows the spot price of <strong class="source-inline">p3dn.24xlarge</strong> for the last three months, where the spot price has been 60-70% cheaper than the on-demand price: </p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="Images/B17705_10_1.jpg" alt="Figure 10.1 – Viewing the spot price of p3dn.24xlarge&#13;&#10;" width="1273" height="759"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Viewing the spot price of p3dn.24xlarge</p>
			<p>These are EC2 prices, but the <a id="_idIndexMarker1079"/>same discount rates apply to SageMaker prices. Discounts vary across instance types, regions, and even availability zones. You can use the <strong class="source-inline">describe-spot-price-history</strong> API to collect this information programmatically and use it in your workflows:</p>
			<p>https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-spot-price-history.html</p>
			<p>Now, let's see what this means for SageMaker.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor211"/>Understanding managed spot training</h2>
			<p>Training with Spot <a id="_idIndexMarker1080"/>Instances is available in all SageMaker configurations: single-instance training, distributed training, built-in algorithms, frameworks, and your own algorithms.</p>
			<p>Setting a couple of estimator parameters is all it takes. You don't need to worry about handling notifications and interruptions. SageMaker automatically does it for you.</p>
			<p>If a training job is interrupted, SageMaker regains adequate spot capacity and relaunches the training job. If the algorithm uses checkpointing, training resumes from the latest checkpoint. If not, the job restarts from the beginning.</p>
			<p>How much work is required to implement checkpointing depends on the algorithm you're using:</p>
			<ul>
				<li>The three built-in algorithms for computer vision and <strong class="bold">XGBoost</strong> support checkpointing.</li>
				<li>All other built-in algorithms don't. You can still train them with Spot Instances. However, the maximum running time is limited to 60 minutes to minimize potential waste. If your <a id="_idIndexMarker1081"/>training job takes longer than 60 minutes, you should try scaling it. If that's not enough, you'll have to use on-demand instances.</li>
				<li>The <strong class="bold">deep learning containers</strong> for <strong class="bold">TensorFlow</strong>, <strong class="bold">PyTorch</strong>, <strong class="bold">Apache</strong> <strong class="bold">MXNet</strong>, and <strong class="bold">Hugging Face</strong> come with built-in checkpointing, and you don't need to modify your training script.</li>
				<li>If you use other frameworks or your own custom code, you need to implement checkpointing. </li>
			</ul>
			<p>During training, checkpoints are saved inside the training container. The default path is <strong class="source-inline">/opt/ml/checkpoints</strong>, and you can customize it with an estimator parameter. SageMaker also automatically persists these checkpoints to a user-defined S3 path. If your training job is interrupted and relaunched, checkpoints are automatically copied inside the container. Your code can check for their presence and load the appropriate one to resume training.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please note that checkpointing is available even when you train with on-demand instances. This may come in handy if you'd like to store checkpoints in S3 for further inspection or for incremental training. The only restriction is that checkpointing is not available with <strong class="bold">Local mode</strong>.</p>
			<p>Last but <a id="_idIndexMarker1082"/>not least, checkpointing does slow down jobs, especially for large models. However, this is a small price to pay to avoid restarting long-running jobs from scratch.</p>
			<p>Now, let's add managed spot training to the <strong class="bold">object detection</strong> job we ran in <a href="B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 5</em></a>, <em class="italic">Training Computer Vision Models</em>.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor212"/>Using managed spot training with object detection</h2>
			<p>Switching from on-demand training to managed spot training is very simple. We just have to set <a id="_idIndexMarker1083"/>the maximum duration <a id="_idIndexMarker1084"/>of the training job, including any time spent waiting for Spot Instances to be available. </p>
			<p>We set a maximum running time of 2 hours, plus 8 hours for any spot delay. If either one of these bounds is exceeded, the job will be terminated automatically. This is helpful in killing runaway jobs that last much longer than expected or jobs that are stuck waiting for spot instances:</p>
			<p class="source-code">od = sagemaker.estimator.Estimator(</p>
			<p class="source-code">     container,</p>
			<p class="source-code">     role,</p>
			<p class="source-code">     instance_count=2,                                 </p>
			<p class="source-code">     instance_type='ml.p3.2xlarge',                                 </p>
			<p class="source-code">     use_spot_instances=True,</p>
			<p class="source-code">     max_run=7200,                     # 2 hour</p>
			<p class="source-code">     max_wait=36000,                   # +8 hours</p>
			<p class="source-code">     output_path=s3_output_location)</p>
			<p>We train with the same configuration as before: Pipe mode and <strong class="source-inline">dist_sync</strong> mode. As the first epoch completes, the training log tells us that checkpointing is active. A new checkpoint is saved automatically each time the validation metric improves:</p>
			<p class="source-code"><strong class="bold">Updating the best model with validation-mAP=1.615789635726003e-05</strong></p>
			<p class="source-code"><strong class="bold">Saved checkpoint to "/opt/ml/model/model_algo_1-0000.params"</strong></p>
			<p>Once the training job is complete, the training log tells us how much we saved:</p>
			<p class="source-code"><strong class="bold">Training seconds: 7794</strong></p>
			<p class="source-code"><strong class="bold">Billable seconds: 2338</strong></p>
			<p class="source-code"><strong class="bold">Managed Spot Training savings: 70.0%</strong></p>
			<p>Not only <a id="_idIndexMarker1085"/>is this job 70% cheaper than its on-demand counterpart, but it's also less than half the price of our original single-instance job. This means that we could use more <a id="_idIndexMarker1086"/>instances and accelerate our training job for the same budget. Indeed, managed spot training lets you optimize the duration of a job and its cost. Instead of complex capacity planning, you can set a training budget that fits your business requirements, and then grab as much infrastructure as possible. </p>
			<p>Let's try another example where we implement checkpointing in <strong class="bold">Keras</strong>.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor213"/>Using managed spot training and checkpointing with Keras</h2>
			<p>In this example, we'll <a id="_idIndexMarker1087"/>build a simple <strong class="bold">CNN</strong> to classify the <strong class="bold">Fashion-MNIST</strong> dataset. We've already worked with it in <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services with Built-in Frameworks</em>, and we'll use <strong class="bold">Script mode</strong> again. This time, we build our model using the old-style <strong class="source-inline">Sequential</strong> API in TensorFlow 2.1.</p>
			<h3>Checkpointing with Keras</h3>
			<p>Let's first look at the Keras script itself. For the sake of brevity, only important steps are presented here. You <a id="_idIndexMarker1088"/>can find the full code in the GitHub repository for this book:</p>
			<ol>
				<li value="1">Using Script mode, we store dataset paths and hyperparameters. </li>
				<li>Then, we load the dataset and normalize pixel values to the [0,1] range. We also one-hot encode class labels.</li>
				<li>We build a <strong class="source-inline">Sequential</strong> model: two convolution blocks (<strong class="source-inline">Conv2D</strong> / <strong class="source-inline">BatchNormalization</strong> / <strong class="source-inline">ReLU</strong> / <strong class="source-inline">MaxPooling2D</strong> / <strong class="source-inline">Dropout</strong>), then two fully connected blocks (<strong class="source-inline">Dense</strong> / <strong class="source-inline">BatchNormalization</strong> / <strong class="source-inline">ReLU</strong> / <strong class="source-inline">Dropout</strong>), and finally, a <strong class="source-inline">softmax</strong> output layer for the 10 classes in the dataset.</li>
				<li>We compile <a id="_idIndexMarker1089"/>the model using the <strong class="bold">categorical cross-entropy</strong> loss function and the <strong class="bold">Adam</strong> optimizer:<p class="source-code">model.compile(</p><p class="source-code">    loss=tf.keras.losses.categorical_crossentropy,</p><p class="source-code">    optimizer=tf.keras.optimizers.Adam(),</p><p class="source-code">    metrics=['accuracy'])</p></li>
				<li>We define a Keras callback to checkpoint the model each time validation accuracy improves:<p class="source-code">from tensorflow.keras.callbacks import ModelCheckpoint</p><p class="source-code">chk_dir = '/opt/ml/checkpoints'</p><p class="source-code">chk_name = 'fmnist-cnn-{epoch:04d}'</p><p class="source-code">checkpointer = ModelCheckpoint(</p><p class="source-code">    filepath=os.path.join(chk_dir, chk_name),</p><p class="source-code">    monitor='val_accuracy')</p></li>
				<li>We train the model, adding the callback we just created:<p class="source-code">model.fit(x=x_train, y=y_train, </p><p class="source-code">          validation_data=(x_val, y_val),</p><p class="source-code">          batch_size=batch_size, epochs=epochs,</p><p class="source-code">          callbacks=[checkpointer],</p><p class="source-code">          verbose=1)</p></li>
				<li>When training is complete, we save the model in the <strong class="bold">TensorFlow Serving</strong> format, which is <a id="_idIndexMarker1090"/>required to deploy on SageMaker:<p class="source-code">from tensorflow.keras.models import save_model</p><p class="source-code">save_model(model, os.path.join(model_dir, '1'),  </p><p class="source-code">           save_format='tf')</p></li>
			</ol>
			<p>Now, let's look at our training notebook.</p>
			<h3>Training with managed spot training and checkpointing</h3>
			<p>We <a id="_idIndexMarker1091"/>use the same workflow as before:</p>
			<ol>
				<li value="1">We download the Fashion-MNIST dataset and save it to a local directory. We upload the dataset to S3, and we define the S3 location where SageMaker should copy the checkpoints.</li>
				<li>We configure a <strong class="source-inline">TensorFlow</strong> estimator, enabling <a id="_idIndexMarker1092"/>managed spot training and passing the S3 output location for checkpoints. This time, we use an <strong class="source-inline">ml.g4dn.xlarge</strong> instance. This very cost-effective GPU instance ($0.822 in <strong class="source-inline">eu-west-1</strong>) is more than enough for a small model:<p class="source-code">from sagemaker.tensorflow import TensorFlow</p><p class="source-code">tf_estimator = TensorFlow(</p><p class="source-code">    entry_point='fmnist-1.py',</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.g4dn.xlarge',     </p><p class="source-code">    framework_version='2.1.0',</p><p class="source-code">    py_version='py3',</p><p class="source-code">    hyperparameters={'epochs': 20},</p><p class="source-code">    output_path=output_path,</p><p class="source-code">    use_spot_instances=True,</p><p class="source-code">    max_run=3600,</p><p class="source-code">    max_wait=7200,</p><p class="source-code">    checkpoint_s3_uri=chk_path)</p></li>
				<li>We launch <a id="_idIndexMarker1093"/>training as usual, and the <a id="_idIndexMarker1094"/>job hits 93.11% accuracy. Training lasts 289 seconds, and we're only billed for 87 seconds, thanks to a 69.9% discount. The total cost is 1.98 cents! Who said GPU training had to be costly?</li>
				<li>In the training log, we see that a checkpoint is created every time validation accuracy improves:<p class="source-code"><strong class="bold">INFO:tensorflow:Assets written to /opt/ml/checkpoints/fmnist-cnn-0001/assets</strong></p><p>While the job is running, we also see that checkpoints are copied to S3: </p><p class="source-code"><strong class="bold">$ aws s3 ls s3://sagemaker-eu-west-1-123456789012/keras2</strong></p><p class="source-code"><strong class="bold">fashion-mnist/checkpoints/</strong></p><p class="source-code"><strong class="bold">PRE fmnist-cnn-0001/</strong></p><p class="source-code"><strong class="bold">PRE fmnist-cnn-0002/</strong></p><p class="source-code"><strong class="bold">PRE fmnist-cnn-0003/</strong></p><p class="source-code"><strong class="bold">PRE fmnist-cnn-0006/</strong></p><p class="source-code"><strong class="bold">. . .</strong></p></li>
			</ol>
			<p>If our spot job gets interrupted, SageMaker will copy checkpoints inside the container so that we can use them to resume training. This requires some logic in our Keras script to load the latest checkpoint. Let's see how to do this.</p>
			<h3>Resuming training from a checkpoint</h3>
			<p>This is a pretty <a id="_idIndexMarker1095"/>simple process—look for checkpoints, and resume training from the latest one:</p>
			<ol>
				<li value="1">We list the checkpoint directory:<p class="source-code">import glob</p><p class="source-code">checkpoints = sorted(</p><p class="source-code">    glob.glob(os.path.join(chk_dir,'fmnist-cnn-*')))</p></li>
				<li>If checkpoints are present, we find the most recent and its epoch number. Then, we load the model:<p class="source-code">from tensorflow.keras.models import load_model</p><p class="source-code">if checkpoints :</p><p class="source-code">    last_checkpoint = checkpoints[-1]</p><p class="source-code">    last_epoch = int(last_checkpoint.split('-')[-1])</p><p class="source-code">    model = load_model(last_checkpoint)</p><p class="source-code">    print('Loaded checkpoint for epoch ', last_epoch)</p></li>
				<li>If no checkpoint is present, we build the model as usual:<p class="source-code">else:</p><p class="source-code">    last_epoch = 0</p><p class="source-code">    model = Sequential()</p><p class="source-code">    . . .</p></li>
				<li>We compile the model, and we launch training, passing the number of the last epoch:<p class="source-code">model.fit(x=x_train, y=y_train, </p><p class="source-code">          validation_data=(x_val, y_val), </p><p class="source-code">          batch_size=batch_size,</p><p class="source-code">          epochs=epochs,</p><p class="source-code">          initial_epoch=last_epoch,</p><p class="source-code">          callbacks=[checkpointer],</p><p class="source-code">          verbose=1)</p></li>
			</ol>
			<p>How can we test this? There is no way to intentionally cause a spot interruption. </p>
			<p>Here's the trick: start a new training job with existing checkpoints in the <strong class="source-inline">checkpoint_s3_uri</strong> path, and <a id="_idIndexMarker1096"/>increase the number of epochs. This will simulate resuming an interrupted job.</p>
			<p>Setting the number of epochs to 25 and keeping the checkpoints in <strong class="source-inline">s3://sagemaker-eu-west-1-123456789012/keras2</strong></p>
			<p><strong class="source-inline">fashion-mnist/checkpoints</strong>, we launch the training job again.</p>
			<p>In the training log, we see that the latest checkpoint is loaded and that training resumes at epoch 21:</p>
			<p class="source-code"><strong class="bold">Loaded checkpoint for epoch 20</strong></p>
			<p class="source-code"><strong class="bold">. . .</strong></p>
			<p class="source-code"><strong class="bold">Epoch 21/25</strong></p>
			<p>We also see that new checkpoints are created as validation accuracy improves, and they're copied to S3:</p>
			<p class="source-code"><strong class="bold">INFO:tensorflow:Assets written to: /opt/ml/checkpoints/fmnist-cnn-0021/assets</strong></p>
			<p>As you can see, it's not difficult to set up checkpointing in SageMaker, and you should be able to do the same for other frameworks. Thanks to this, you can enjoy the deep discount provided by managed spot training without the risk of losing any work if an interruption occurs. Of course, you can use checkpointing on its own to inspect intermediate training results, or for incremental training.</p>
			<p>In the next section, we're going to introduce another important feature: automatic model tuning.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor214"/>Optimizing hyperparameters with automatic model tuning</h1>
			<p>Hyperparameters have a huge influence on the training outcome. Just like in <strong class="bold">chaos theory</strong>, tiny <a id="_idIndexMarker1097"/>variations of a single hyperparameter can <a id="_idIndexMarker1098"/>cause wild swings in accuracy. In most cases, the "why?" evades us, leaving us perplexed about what to try next.</p>
			<p>Over the years, several techniques have been devised to try to solve the problem of selecting optimal hyperparameters:</p>
			<ol>
				<li value="1"><strong class="bold">Manual search</strong>: This means using our best judgment and experience to select the "best" hyperparameters. Let's <a id="_idIndexMarker1099"/>face it: this doesn't really work, especially with deep learning and its horde of training and network architecture parameters.</li>
				<li><strong class="bold">Grid search</strong>: This entails systematically exploring the hyperparameter space, zooming in on hot spots, and <a id="_idIndexMarker1100"/>repeating the process. This is much better than a manual search. However, this usually requires training hundreds of jobs. Even with scalable infrastructure, the time and dollar budgets can be significant.</li>
				<li><strong class="bold">Random search</strong>: This refers to <a id="_idIndexMarker1101"/>selecting hyperparameters at random. Unintuitive as it sounds, James Bergstra and Yoshua Bengio (of Turing Award fame) proved in 2012 that this technique delivers better models than a grid search with the same compute budget</li>
				<li><a href="http://www.jmlr.org/papers/v13/bergstra12a.html">http://www.jmlr.org/papers/v13/bergstra12a.html</a></li>
				<li><strong class="bold">Hyperparameter optimization</strong> (HPO): This <a id="_idIndexMarker1102"/>means using optimization techniques to select hyperparameters, such as <strong class="bold">Bayesian optimization</strong> and <strong class="bold">Gaussian process regression</strong>. With the <a id="_idIndexMarker1103"/>same compute budget, HPO typically delivers results <a id="_idIndexMarker1104"/>with 10x fewer training epochs than other techniques. </li>
			</ol>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor215"/>Understanding automatic model tuning</h2>
			<p>SageMaker includes an <strong class="bold">automatic model tuning</strong> capability that lets you easily explore hyperparameter <a id="_idIndexMarker1105"/>ranges and quickly optimize any training metric with a limited number of jobs.</p>
			<p>Model tuning supports both random search and HPO. The former is an interesting baseline that helps you to check whether the latter is indeed overperforming. You can find a very detailed comparison in this excellent blog post:</p>
			<p>https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/<span class="hidden"> </span></p>
			<p>Model tuning is completely agnostic to the algorithm you're using. It works with built-in algorithms, and the documentation lists the hyperparameters that can be tuned. It also works with all frameworks and custom containers, and hyperparameters are passed in the same way.</p>
			<p>For each hyperparameter that we want to optimize, we have to define the following:</p>
			<ul>
				<li>A name</li>
				<li>A type (parameters can either be an integer, continuous, or categorical)</li>
				<li>A range of values to explore</li>
				<li>A scaling type (linear, logarithmic, or reverse logarithmic, or auto)—this lets us control how a specific parameter range will be explored</li>
			</ul>
			<p>We also define the metric we want to optimize for. It can be any numerical value as long as it's visible in the training log and you can pass a regular expression to extract it.</p>
			<p>Then, we launch the tuning jobs, passing all of these parameters as well as the number of training jobs to run and their degree of parallelism. With Bayesian optimization, you'll get the best results with sequential jobs (no parallelism), as optimization can be applied after each job. Having said that, running a small number of jobs in parallel is acceptable. Random search has no restrictions on parallelism as jobs are completely unrelated.</p>
			<p>Calling the <strong class="source-inline">deploy()</strong> API on the tuner object deploys the best model. If tuning is is still in progress, it will deploy the best <a id="_idIndexMarker1106"/>model so far, which can be useful for early testing.</p>
			<p>Let's run the first example with a built-in algorithm and learn about the model tuning API.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor216"/>Using automatic model tuning with object detection</h2>
			<p>We're <a id="_idIndexMarker1107"/>going to optimize our object detection job. Looking at the documentation, we can see the list of tunable hyperparameters:</p>
			<p>https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tuning.html</p>
			<p>Let's try to optimize <a id="_idIndexMarker1108"/>the learning rate, momentum, and weight decay:</p>
			<ol>
				<li value="1">We set up the input channels using Pipe mode. There's no change here.</li>
				<li>We also configure the estimator as usual, setting up managed spot training to minimize costs. We'll train on a single instance for maximum accuracy:<p class="source-code">od = sagemaker.estimator.Estimator(</p><p class="source-code">     container,</p><p class="source-code">     role,                                        </p><p class="source-code">     instance_count=1,                                        </p><p class="source-code">     instance_type='ml.p3.2xlarge',                                       </p><p class="source-code">     output_path=s3_output_location,                                        </p><p class="source-code">     use_spot_instances=True,</p><p class="source-code">     max_run=7200,</p><p class="source-code">     max_wait=36000,</p><p class="source-code">     volume_size=1)       </p></li>
				<li>We use the same hyperparameters as before:<p class="source-code">od.set_hyperparameters(base_network='resnet-50',</p><p class="source-code">                       use_pretrained_model=1,</p><p class="source-code">                       num_classes=20,</p><p class="source-code">                       epochs=30,</p><p class="source-code">                       num_training_samples=16551,</p><p class="source-code">                       mini_batch_size=90)</p></li>
				<li>We define the three extra hyperparameters we want to tune. We explicitly set logarithmic scaling for the learning rate, to make sure that different orders of magnitude are explored:<p class="source-code">from sagemaker.tuner import ContinuousParameter,</p><p class="source-code">hyperparameter_ranges = {</p><p class="source-code">    'learning_rate': ContinuousParameter(0.001, 0.1, </p><p class="source-code">                     scaling_type='Logarithmic'), </p><p class="source-code">    'momentum': ContinuousParameter(0.8, 0.999), </p><p class="source-code">    'weight_decay': ContinuousParameter(0.0001, 0.001)</p><p class="source-code">}</p></li>
				<li>We <a id="_idIndexMarker1109"/>set the metric to <a id="_idIndexMarker1110"/>optimize for:<p class="source-code">objective_metric_name = 'validation:mAP'</p><p class="source-code">objective_type = 'Maximize'</p></li>
				<li>We put everything together, using the <strong class="source-inline">HyperparameterTuner</strong> object. We decide to run 30 jobs, with two jobs in parallel. We also enable early stopping to weed out low performing jobs, saving us time and money:<p class="source-code">from sagemaker.tuner import HyperparameterTuner</p><p class="source-code">tuner = HyperparameterTuner(od,</p><p class="source-code">            objective_metric_name,</p><p class="source-code">            hyperparameter_ranges,</p><p class="source-code">            objective_type=objective_type,</p><p class="source-code">            max_jobs=30,</p><p class="source-code">            max_parallel_jobs=2,</p><p class="source-code">            early_stopping_type='Auto')</p></li>
				<li>We launch training on the tuner object (not on the estimator) without waiting for it to complete:<p class="source-code">tuner.fit(inputs=data_channels, wait=False)</p></li>
				<li>At the moment, <strong class="bold">SageMaker Studio</strong> doesn't provide a convenient view of tuning jobs. Instead, we can track progress in the <strong class="bold">Hyperparameter tuning jobs</strong> section of the SageMaker console, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="Images/B17705_10_2.jpg" alt="Figure 10.2 – Viewing tuning jobs in the SageMaker console&#13;&#10;" width="1622" height="807"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Viewing tuning jobs in the SageMaker console</p>
			<p>The job <a id="_idIndexMarker1111"/>runs for 17 hours (wall time). 22 jobs completed and 8 stopped early. The total training time is 30 hours and 15 <a id="_idIndexMarker1112"/>minutes. Applying the 70% spot discount, the total cost is 25.25 * $4.131 * 0.3 = $37.48.</p>
			<p>How well did this tuning job do? With default hyperparameters, our standalone training job reached a <strong class="bold">mAP</strong> accuracy of <strong class="source-inline">0.2453</strong>. Our tuning job hits <strong class="source-inline">0.6337</strong>, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="Images/B17705_10_3.jpg" alt="Figure 10.3 – Tuning job results&#13;&#10;" width="686" height="180"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Tuning job results</p>
			<p>The graph <a id="_idIndexMarker1113"/>for validation mAP is shown in the next image. It tells me that we could probably train a little longer and get extra accuracy:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="Images/B17705_10_4.jpg" alt="Figure 10.4 – Viewing the mAP metric&#13;&#10;" width="963" height="276"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Viewing the mAP metric</p>
			<p>One idea <a id="_idIndexMarker1114"/>would be to launch a single training job with the best hyperparameters and let it run for more epochs. We could also resume the tuning job using <strong class="bold">warm start</strong> and continue exploring the hyperparameter range. We also call <strong class="source-inline">deploy()</strong> on the tuner object and test our model just like any SageMaker model.</p>
			<p>As you can see, automatic model tuning is extremely powerful. By running a small number of jobs, we improved our metric by 158%! The cost is negligible compared to the time you would spend experimenting with other techniques.</p>
			<p>In fact, running the same tuning job using the random strategy delivers a top accuracy of 0.52. We would certainly need to run many more training jobs to even hope hitting 0.6315.</p>
			<p>Let's now try to optimize the Keras example we used earlier in this chapter.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor217"/>Using automatic model tuning with Keras</h2>
			<p>Automatic model tuning can easily be used any algorithm on SageMaker, which of course includes all <a id="_idIndexMarker1115"/>frameworks. Let's see how this works with Keras.</p>
			<p>Earlier <a id="_idIndexMarker1116"/>in this chapter, we trained our Keras CNN on the Fashion MNIST dataset for 20 epochs and reached a validation accuracy of 93.11%. Let's see if we can improve it with automatic model tuning. In the process, we'll also learn how to optimize for any metric present in the training log, not just metrics that are predefined in SageMaker.</p>
			<h3>Optimizing on a custom metric</h3>
			<p>Modifying <a id="_idIndexMarker1117"/>our training script, we install the <strong class="source-inline">keras-metrics</strong> package (<a href="https://github.com/netrack/keras-metrics">https://github.com/netrack/keras-metrics</a>) and add the <strong class="bold">precision</strong>, <strong class="bold">recall</strong>, and <strong class="bold">f1 score</strong> metrics to the training log:</p>
			<p class="source-code">import subprocess, sys</p>
			<p class="source-code">def install(package):</p>
			<p class="source-code">    subprocess.call([sys.executable, "-m", "pip",</p>
			<p class="source-code">                     "install", package])</p>
			<p class="source-code">install('keras-metrics')</p>
			<p class="source-code">import keras_metrics</p>
			<p class="source-code">. . . </p>
			<p class="source-code">model.compile(</p>
			<p class="source-code">    loss=tf.keras.losses.categorical_crossentropy,</p>
			<p class="source-code">    optimizer=tf.keras.optimizers.Adam(),</p>
			<p class="source-code">    metrics=['accuracy',</p>
			<p class="source-code">              keras_metrics.precision(),</p>
			<p class="source-code">              keras_metrics.recall(),</p>
			<p class="source-code">              keras_metrics.f1_score()])</p>
			<p>After 20 epochs, the metrics now look like this:</p>
			<p class="source-code"><strong class="bold">loss: 0.0869 - accuracy: 0.9678 - precision: 0.9072 - recall: 0.8908 - f1_score: 0.8989 - val_loss: 0.2301 - val_accuracy: 0.9310 - val_precision: 0.9078 - val_recall: 0.8915 - val_f1_score: 0.8996</strong></p>
			<p>If we wanted to <a id="_idIndexMarker1118"/>optimize on the f1 score, we would define the tuner metrics like this:</p>
			<p class="source-code">objective_metric_name = 'val_f1'</p>
			<p class="source-code">objective_type = 'Maximize'</p>
			<p class="source-code">metric_definitions = [</p>
			<p class="source-code">    {'Name': 'val_f1',</p>
			<p class="source-code">     'Regex': 'val_f1_score: ([0-9\\.]+)'</p>
			<p class="source-code">    }]</p>
			<p>That's all it takes. As long as a metric is printed in the training log, you can use it to tune models.</p>
			<h3>Optimizing our Keras model</h3>
			<p>Now, let's <a id="_idIndexMarker1119"/>run our tuning job:</p>
			<ol>
				<li value="1">We define the metrics for <strong class="source-inline">HyperparameterTuner</strong> like so, optimizing for accuracy and also displaying the f1 score:<p class="source-code">objective_metric_name = 'val_acc'</p><p class="source-code">objective_type = 'Maximize'</p><p class="source-code">metric_definitions = [</p><p class="source-code">    {'Name': 'val_f1', </p><p class="source-code">     'Regex': 'val_f1_score: ([0-9\\.]+)'},</p><p class="source-code">    {'Name': 'val_acc', </p><p class="source-code">     'Regex': 'val_accuracy: ([0-9\\.]+)'}</p><p class="source-code">]</p></li>
				<li>We define <a id="_idIndexMarker1120"/>the parameter ranges to explore:<p class="source-code">from sagemaker.tuner import ContinuousParameter, IntegerParameter</p><p class="source-code">hyperparameter_ranges = {</p><p class="source-code">    'learning_rate': ContinuousParameter(0.001, 0.2, </p><p class="source-code">                     scaling_type='Logarithmic'), </p><p class="source-code">    'batch-size': IntegerParameter(32,512)</p><p class="source-code">}</p></li>
				<li>We use the same estimator (20 epochs with spot instances) and we define the tuner:<p class="source-code">tuner = HyperparameterTuner(</p><p class="source-code">    tf_estimator,</p><p class="source-code">    objective_metric_name,</p><p class="source-code">    hyperparameter_ranges,                          </p><p class="source-code">    metric_definitions=metric_definitions,</p><p class="source-code">    objective_type=objective_type,</p><p class="source-code">    max_jobs=20,</p><p class="source-code">    max_parallel_jobs=2,</p><p class="source-code">    early_stopping_type='Auto')</p></li>
				<li>We launch the tuning job. While it's running, we can use the <strong class="bold">SageMaker SDK</strong> to display the list of training jobs and their properties: <p class="source-code">from sagemaker.analytics import HyperparameterTuningJobAnalytics</p><p class="source-code">exp = HyperparameterTuningJobAnalytics(</p><p class="source-code">   hyperparameter_tuning_job_name=</p><p class="source-code">   tuner.latest_tuning_job.name)</p><p class="source-code">jobs = exp.dataframe()</p><p class="source-code">jobs.sort_values('FinalObjectiveValue', ascending=0)</p><p>This prints out the table visible in the next screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="Images/B17705_10_5.jpg" alt="Figure 10.5 – Viewing information on a tuning job&#13;&#10;" width="1080" height="321"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Viewing information on a tuning job</p>
			<p>The tuning job runs for 2 hours and 8 minutes (wall time). Top validation accuracy is 93.46% – a decent improvement over our baseline. </p>
			<p>We could <a id="_idIndexMarker1121"/>certainly do better by training longer. However, the longer we train for, the more overfitting becomes a concern. We can alleviate it with early stopping, which can be implemented with a Keras callback. However, we should make sure that the job reports the metric for the best epoch, not for the last epoch. How can we display this in the training log? With another callback!</p>
			<h3>Adding callbacks for early stopping</h3>
			<p>Adding <a id="_idIndexMarker1122"/>a Keras callback for early stopping is very simple:</p>
			<ol>
				<li value="1">We add a built-in callback for early stopping, based on validation accuracy:<p class="source-code">from tensorflow.keras.callbacks import EarlyStopping</p><p class="source-code">early_stopping = EarlyStopping(</p><p class="source-code">    monitor='val_accuracy',</p><p class="source-code">    min_delta=0.0001,</p><p class="source-code">    patience=10,</p><p class="source-code">    verbose=1,</p><p class="source-code">    mode='auto')</p></li>
				<li>We add <a id="_idIndexMarker1123"/>a custom callback to store validation accuracy at the end of each epoch, and to display the best one at the end of training:<p class="source-code">from tensorflow.keras.callbacks import Callback</p><p class="source-code">class LogBestMetric(Callback):</p><p class="source-code">    def on_train_begin(self, logs={}):</p><p class="source-code">        self.val_accuracy = []</p><p class="source-code">    def on_train_end(self, logs={}):</p><p class="source-code">        print("Best val_accuracy:",</p><p class="source-code">              max(self.val_accuracy))</p><p class="source-code">    def on_epoch_end(self, batch, logs={}):</p><p class="source-code">        self.val_accuracy.append(</p><p class="source-code">            logs.get('val_accuracy'))</p><p class="source-code">        best_val_metric = LogBestMetric()</p></li>
				<li>We add these two callbacks to the training API:<p class="source-code">model.fit(. . . </p><p class="source-code">    callbacks=[checkpointer, early_stopping, </p><p class="source-code">               best_val_metric])</p><p>Testing with a few individual jobs, the last lines of the training log now look like this:</p><p class="source-code"><strong class="bold">Epoch 00048: early stopping</strong></p><p class="source-code"><strong class="bold">Best val_accuracy: 0.9259</strong></p></li>
				<li>In the notebook, we update our metric definition in order to extract the best validation accuracy:<p class="source-code">objective_metric_name = 'val_acc'</p><p class="source-code">objective_type = 'Maximize'</p><p class="source-code">metric_definitions = [</p><p class="source-code">    {'Name': 'val_acc', </p><p class="source-code">     'Regex': 'Best val_accuracy: ([0-9\\.]+)'}</p><p class="source-code">]</p></li>
			</ol>
			<p>Training <a id="_idIndexMarker1124"/>for 60 epochs this time (about 3 hours wall time), top validation accuracy is now at 93.78%. It looks like this is as good as it gets by tweaking the learning rate and the batch size.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor218"/>Using automatic model tuning for architecture search</h2>
			<p>Our neural <a id="_idIndexMarker1125"/>network has plenty <a id="_idIndexMarker1126"/>more hyperparameters: number of convolution filters, dropout, and so on. Let's try to optimize these as well:</p>
			<ol>
				<li value="1">We modify our training script to add command-line parameters for the following network parameters, which are used by Keras layers in our model:<p class="source-code">parser.add_argument(</p><p class="source-code">    '--filters1', type=int, default=64)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    '--filters2', type=int, default=64)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    '--dropout-conv', type=float, default=0.2)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    '--dropout-fc', type=float, default=0.2)</p><p>As you certainly guessed, the parameters let us set values for the number of convolution <a id="_idIndexMarker1127"/>filters in each <a id="_idIndexMarker1128"/>layer, the dropout value for convolution layers, and the dropout value for fully connected layers.</p></li>
				<li>Accordingly, in the notebook, we define these hyperparameters and their ranges. For the learning rate and the batch size, we use narrow ranges centered on the optimal values discovered by the previous tuning job:<p class="source-code">from sagemaker.tuner import ContinuousParameter, </p><p class="source-code">                            IntegerParameter</p><p class="source-code">hyperparameter_ranges = {</p><p class="source-code">    learning-rate': ContinuousParameter(0.01, 0.14), </p><p class="source-code">    'batch-size': IntegerParameter(130,160),</p><p class="source-code">    'filters1': IntegerParameter(16,256),</p><p class="source-code">    'filters2': IntegerParameter(16,256),</p><p class="source-code">    'dropout-conv': ContinuousParameter(0.001,0.5, </p><p class="source-code">                    scaling_type='Logarithmic'),</p><p class="source-code">    'dropout-fc': ContinuousParameter(0.001,0.5, </p><p class="source-code">                  scaling_type='Logarithmic')</p><p class="source-code">}</p></li>
				<li>We launch the tuning job, running 50 jobs two at a time for 100 epochs.</li>
			</ol>
			<p>The tuning job runs for about 12 hours, for a total cost of about $15. Top validation accuracy hits 94.09%. Compared to our baseline, automatic model tuning has improved the accuracy of our model by almost 1 percentage point – a very significant gain. If this model is used to predict 1 million samples a day, this translates to over 10,000 additional accurate predictions! </p>
			<p>In total, we've spent less about $50 on tuning our Keras model. Whatever business metric <a id="_idIndexMarker1129"/>would be improved by the <a id="_idIndexMarker1130"/>extra accuracy, it's fair to say that this spend would be recouped in no time. As many customers have told me, automatic model tuning pays for itself, and then some.</p>
			<p>This concludes our exploration of automatic model tuning, one of my favorite features in SageMaker. You can find more examples at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning</a>.</p>
			<p>Now, let's learn about SageMaker Debugger, and how it can help us to understand what's happening inside our models.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor219"/>Exploring models with SageMaker Debugger</h1>
			<p>SageMaker Debugger lets you configure <em class="italic">debugging rules</em> for your training job. These rules will <a id="_idIndexMarker1131"/>inspect its internal state and <a id="_idIndexMarker1132"/>check for specific unwanted conditions that could be developing during training. SageMaker Debugger includes a long list of built-in rules (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html">https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html</a>), and you can add your own written in Python.</p>
			<p>In addition, you can save and inspect the model state (gradients, weights, and so on) as well as the training state (metrics, optimizer parameters, and so on). At each training step, the <strong class="bold">tensors</strong> storing these values may be saved in near-real-time in an S3 bucket, making it possible to visualize them while the model is training. </p>
			<p>Of course, you can select the tensor <strong class="bold">collections</strong> that you'd like to save, how often, and so on. Depending on the framework you use, different collections are available. You can find more information at <a href="https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md">https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md</a>. Last but not least, you can save either raw tensor data or tensor reductions to limit the amount of data involved. Reductions include min, max, median, and more.</p>
			<p>If you are working with the built-in containers for supported versions of TensorFlow, PyTorch, Apache MXNet, or the built-in XGBoost algorithm, you can use SageMaker Debugger out of the box, without changing a line of code in your script. Yes, you read that right. All you have to do is add extra parameters to the estimator, as we will in the next examples. </p>
			<p>With other <a id="_idIndexMarker1133"/>versions, or with your own containers, minimal modifications are required. You can find the latest information and examples at <a href="https://github.com/awslabs/sagemaker-debugger">https://github.com/awslabs/sagemaker-debugger</a>.</p>
			<p>Debugging rules and saving tensors can be configured on the same training job. For clarity, we'll <a id="_idIndexMarker1134"/>run two separate examples. First, let's use the XGBoost and Boston Housing example from <a href="B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Machine Learning Models</em>.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor220"/>Debugging an XGBoost job</h2>
			<p>First, we will configure <a id="_idIndexMarker1135"/>several built-in rules, train our model, and <a id="_idIndexMarker1136"/>check the status of all rules:</p>
			<ol>
				<li value="1">Taking a look at the list of built-in rules, we decide to use <strong class="source-inline">overtraining</strong> and <strong class="source-inline">overfit</strong>. Each rule has extra parameters that we could tweak. We stick to defaults, and we configure the <strong class="source-inline">Estimator</strong> accordingly:<p class="source-code">from sagemaker.debugger import rule_configs, Rule</p><p class="source-code">xgb_estimator = Estimator(container,</p><p class="source-code">  role=sagemaker.get_execution_role(),</p><p class="source-code">  instance_count=1,</p><p class="source-code">  instance_type='ml.m5.large',</p><p class="source-code">  output_path='s3://{}/{}/output'.format(bucket, prefix),</p><p class="source-code">  rules=[</p><p class="source-code">    Rule.sagemaker(rule_configs.overtraining()),</p><p class="source-code">    Rule.sagemaker(rule_configs.overfit())</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
				<li>We set hyperparameters and launch training without waiting for the training job to complete. The training log won't be visible in the notebook, but it will still be available in <strong class="bold">CloudWatch Logs</strong>:<p class="source-code">xgb_estimator.set_hyperparameters(</p><p class="source-code">  objective='reg:linear', num_round=100)</p><p class="source-code">xgb_estimator.fit(xgb_data, wait=False)</p></li>
				<li>In addition to <a id="_idIndexMarker1137"/>the training job, one <a id="_idIndexMarker1138"/>debugging job per rule is running under the hood, and we can check their statuses:<p class="source-code">description = xgb_estimator.latest_training_job.rule_job_summary()</p><p class="source-code">for rule in description:</p><p class="source-code">  rule.pop('LastModifiedTime')</p><p class="source-code">  rule.pop('RuleEvaluationJobArn')</p><p class="source-code">  print(rule)</p><p>This tells us that the debugger jobs are running:</p><p class="source-code"><strong class="bold">{'RuleConfigurationName': 'Overtraining',  </strong></p><p class="source-code"><strong class="bold"> 'RuleEvaluationStatus': 'InProgress'}</strong></p><p class="source-code"><strong class="bold">{'RuleConfigurationName': 'Overfit', </strong></p><p class="source-code"><strong class="bold"> 'RuleEvaluationStatus': 'InProgress'}</strong></p></li>
				<li>Running the same cell once the training job is complete, we see that no rule was triggered:<p class="source-code"><strong class="bold">{'RuleConfigurationName': 'Overtraining',</strong></p><p class="source-code"><strong class="bold"> 'RuleEvaluationStatus': 'NoIssuesFound'}</strong></p><p class="source-code"><strong class="bold">{'RuleConfigurationName': 'Overfit', </strong></p><p class="source-code"><strong class="bold"> 'RuleEvaluationStatus': 'NoIssuesFound'}</strong></p></li>
			</ol>
			<p>Had a rule been triggered, we <a id="_idIndexMarker1139"/>would get an error message, and the training job would be stopped. Inspecting tensors stored in S3 would help us <a id="_idIndexMarker1140"/>understand what went wrong. </p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor221"/>Inspecting an XGBoost job</h2>
			<p>Let's configure a new <a id="_idIndexMarker1141"/>training job that saves all tensor collections available for XGBoost:</p>
			<ol>
				<li value="1">We configure the <a id="_idIndexMarker1142"/><strong class="source-inline">Estimator</strong>, passing a <strong class="source-inline">DebuggerHookConfig</strong> object. We save three tensor collections at each training step: metrics, feature importance, and average <strong class="bold">SHAP</strong> (<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>) values. These help us understand how each feature in a data sample contributes to increasing or decreasing the predicted value.<p>For larger models and datasets, this could generate a lot of data, which would take a long time to load and analyze. We would either increase the save interval or save tensor reductions instead of full tensors:</p><p class="source-code">from sagemaker.debugger import DebuggerHookConfig, CollectionConfig</p><p class="source-code">save_interval = '1'</p><p class="source-code">xgb_estimator = Estimator(container,</p><p class="source-code">    role=role,</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large',</p><p class="source-code">    output_path='s3://{}/{}/output'.format(bucket,  </p><p class="source-code">                                           prefix),</p><p class="source-code">    </p><p class="source-code">    debugger_hook_config=DebuggerHookConfig(                </p><p class="source-code">        s3_output_path=</p><p class="source-code">        's3://{}/{}/debug'.format(bucket,prefix),</p><p class="source-code">      collection_configs=[</p><p class="source-code">        CollectionConfig(name='metrics',</p><p class="source-code">          parameters={"save_interval": </p><p class="source-code">                      save_interval}),</p><p class="source-code">        CollectionConfig(name='average_shap',  </p><p class="source-code">          parameters={"save_interval": </p><p class="source-code">                      save_interval}),</p><p class="source-code">        CollectionConfig(name='feature_importance', </p><p class="source-code">          parameters={"save_interval": save_interval})</p><p class="source-code">      ]</p><p class="source-code">    )</p><p class="source-code">)</p></li>
				<li>Once the <a id="_idIndexMarker1143"/>training job has started, we <a id="_idIndexMarker1144"/>can create a trial and load data that has already been saved. As this job is very short, we see all data within a minute or so:<p class="source-code">from smdebug.trials import create_trial</p><p class="source-code">s3_output_path = xgb_estimator.latest_job_debugger_artifacts_path()</p><p class="source-code">trial = create_trial(s3_output_path)</p></li>
				<li>We can list the name of all tensors that were saved:<p class="source-code">trial.tensor_names()</p><p class="source-code"><strong class="bold">['average_shap/f0','average_shap/f1','average_shap/f10', … </strong></p><p class="source-code"><strong class="bold"> 'feature_importance/cover/f0','feature_importance/cover/f1',…</strong></p><p class="source-code"><strong class="bold"> 'train-rmse','validation-rmse']</strong></p></li>
				<li>We can also list the name of all tensors in a given collection:<p class="source-code">trial.tensor_names(collection="metrics")</p><p class="source-code"><strong class="bold">['train-rmse', 'validation-rmse']</strong></p></li>
				<li>For <a id="_idIndexMarker1145"/>each tensor, we can access training <a id="_idIndexMarker1146"/>steps and values. Let's plot feature information from the <strong class="source-inline">average_shap</strong> and <strong class="source-inline">feature_importance</strong> collections:<p class="source-code">def plot_features(tensor_prefix):</p><p class="source-code">    num_features = len(dataset.columns)-1</p><p class="source-code">    for i in range(0,num_features):</p><p class="source-code">    feature = tensor_prefix+'/f'+str(i)</p><p class="source-code">    steps = trial.tensor(feature).steps()</p><p class="source-code">    v = [trial.tensor(feature).value(s) for s in steps]</p><p class="source-code">    plt.plot(steps, v, label=dataset.columns[i+1])</p><p class="source-code">    plt.autoscale()</p><p class="source-code">    plt.title(tensor_prefix)</p><p class="source-code">    plt.legend(loc='upper left')</p><p class="source-code">    plt.show()</p></li>
				<li>We build the <strong class="source-inline">average_shap</strong> plot:<p class="source-code">plot_features('average_shap')</p></li>
				<li>You can see it in the <a id="_idIndexMarker1147"/>following screenshot – <strong class="bold">dis</strong>, <strong class="bold">crim</strong>, and <strong class="bold">nox</strong> have the largest <a id="_idIndexMarker1148"/>average values:<div id="_idContainer154" class="IMG---Figure"><img src="Images/B17705_10_6.jpg" alt="Figure 10.6 – Plotting average SHAP values over time&#13;&#10;" width="383" height="264"/></div><p class="figure-caption">Figure 10.6 – Plotting average SHAP values over time</p></li>
				<li>We build the <strong class="source-inline">feature_importance/weight</strong> plot:<p class="source-code">plot_features('feature_importance/weight')</p><p>You can see it in the following screenshot – <strong class="bold">crim</strong>, <strong class="bold">age</strong>, and <strong class="bold">dis</strong> have the largest weights:</p></li>
			</ol>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="Images/B17705_10_7.jpg" alt="Figure 10.7 – Plotting feature weights over time&#13;&#10;" width="377" height="264"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Plotting feature weights over time</p>
			<p>Now, let's use SageMaker Debugger on our Keras and Fashion-MNIST example.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor222"/>Debugging and inspecting a Keras job</h2>
			<p>We can <a id="_idIndexMarker1149"/>inspect and <a id="_idIndexMarker1150"/>debug a Keras job using the following steps:</p>
			<ol>
				<li value="1">The default <a id="_idIndexMarker1151"/>behavior in TensorFlow 2.x is eager mode, where gradients are not available. Hence, we <a id="_idIndexMarker1152"/>disable eager mode in our script, which is the only modification required:<p class="source-code">tf.compat.v1.disable_eager_execution()</p></li>
				<li>We start from the same estimator. The dataset has 70,000 samples (60,000 for training, plus 10,000 for validation). With 30 epochs and a batch size of 128, our training job will have about 16,400 steps (70,000 * 30 / 128). Saving tensors at each step feels like overkill. Let's save them every 100 steps instead:<p class="source-code">from sagemaker.tensorflow import TensorFlow</p><p class="source-code">from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig</p><p class="source-code">save_interval = '100'</p><p class="source-code">tf_estimator = TensorFlow(entry_point='fmnist-5.py',</p><p class="source-code">    role=role,</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.p3.2xlarge',</p><p class="source-code">    framework_version='2.1.0', </p><p class="source-code">    py_version='py3',</p><p class="source-code">    hyperparameters={'epochs': 30},</p><p class="source-code">    output_path=output_path,</p><p class="source-code">    use_spot_instances=True,</p><p class="source-code">    max_run=3600,</p><p class="source-code">    max_wait=7200,</p></li>
				<li>Looking <a id="_idIndexMarker1153"/>at the built-in rules available for TensorFlow, we decide to set up <strong class="source-inline">poor_weight_initialization</strong>, <strong class="source-inline">dead_relu,</strong> and <strong class="source-inline">check_input_images</strong>. We <a id="_idIndexMarker1154"/>need to <a id="_idIndexMarker1155"/>specify the index of channel information <a id="_idIndexMarker1156"/>in the input tensor. It's 4 for TensorFlow (batch size, height, width, and channels):<p class="source-code">    rules=[      </p><p class="source-code">Rule.sagemaker(</p><p class="source-code">    rule_configs.poor_weight_initialization()), </p><p class="source-code">Rule.sagemaker(</p><p class="source-code">    rule_configs.dead_relu()),</p><p class="source-code">Rule.sagemaker(</p><p class="source-code">    rule_configs.check_input_images(), </p><p class="source-code">    rule_parameters={"channel": '3'})</p><p class="source-code">    ],</p></li>
				<li>Looking <a id="_idIndexMarker1157"/>at the collections <a id="_idIndexMarker1158"/>available for <a id="_idIndexMarker1159"/>TensorFlow, we <a id="_idIndexMarker1160"/>decide to save metrics, losses, outputs, weights, and gradients:<p class="source-code">    debugger_hook_config=DebuggerHookConfig(                </p><p class="source-code">        s3_output_path='s3://{}/{}/debug'</p><p class="source-code">               .format(bucket, prefix),</p><p class="source-code">        collection_configs=[</p><p class="source-code">            CollectionConfig(name='metrics',  </p><p class="source-code">                parameters={"save_interval": </p><p class="source-code">                            save_interval}),</p><p class="source-code">            CollectionConfig(name='losses', </p><p class="source-code">                parameters={"save_interval": </p><p class="source-code">                            save_interval}),</p><p class="source-code">            CollectionConfig(name='outputs', </p><p class="source-code">                parameters={"save_interval": </p><p class="source-code">                            save_interval}),</p><p class="source-code">            CollectionConfig(name='weights', </p><p class="source-code">                parameters={"save_interval": </p><p class="source-code">                            save_interval}),</p><p class="source-code">            CollectionConfig(name='gradients', </p><p class="source-code">                parameters={"save_interval": </p><p class="source-code">                            save_interval})</p><p class="source-code">        ],</p><p class="source-code">    )</p><p class="source-code">)</p></li>
				<li>As <a id="_idIndexMarker1161"/>training starts, we <a id="_idIndexMarker1162"/>see the <a id="_idIndexMarker1163"/>rules being launched in <a id="_idIndexMarker1164"/>the training log:<p class="source-code"><strong class="bold">********* Debugger Rule Status *********</strong></p><p class="source-code"><strong class="bold">*</strong></p><p class="source-code"><strong class="bold">* PoorWeightInitialization: InProgress        </strong></p><p class="source-code"><strong class="bold">* DeadRelu: InProgress        </strong></p><p class="source-code"><strong class="bold">* CheckInputImages: InProgress        </strong></p><p class="source-code"><strong class="bold">*</strong></p><p class="source-code"><strong class="bold">****************************************</strong></p></li>
				<li>When training is complete, we check the status of the debugging rules:<p class="source-code">description = tf_estimator.latest_training_job.rule_job_summary()</p><p class="source-code">for rule in description:</p><p class="source-code">    rule.pop('LastModifiedTime')</p><p class="source-code">    rule.pop('RuleEvaluationJobArn')</p><p class="source-code">    print(rule)</p><p class="source-code"><strong class="bold">{'RuleConfigurationName': 'PoorWeightInitialization', </strong></p><p class="source-code"><strong class="bold"> 'RuleEvaluationStatus': 'NoIssuesFound'}</strong></p><p class="source-code"><strong class="bold">{'RuleConfigurationName': 'DeadRelu',</strong></p><p class="source-code"><strong class="bold"> 'RuleEvaluationStatus': 'NoIssuesFound'}</strong></p><p class="source-code"><strong class="bold">{'RuleConfigurationName': 'CheckInputImages', </strong></p><p class="source-code"><strong class="bold"> 'RuleEvaluationStatus': 'NoIssuesFound'}</strong></p></li>
				<li>We create a trial using the same tensors saved in S3:<p class="source-code">from smdebug.trials import create_trial</p><p class="source-code">s3_output_path = tf_estimator.latest_job_debugger_artifacts_path()</p><p class="source-code">trial = create_trial(s3_output_path)</p></li>
				<li>Let's inspect the filters in the first convolution layer:<p class="source-code">w = trial.tensor('conv2d/weights/conv2d/kernel:0')</p><p class="source-code">g = trial.tensor(</p><p class="source-code">'training/Adam/gradients/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter:0')</p><p class="source-code">print(w.value(0).shape)</p><p class="source-code">print(g.value(0).shape)</p><p class="source-code"><strong class="bold">(3, 3, 1, 64)</strong></p><p class="source-code"><strong class="bold">(3, 3, 1, 64)</strong></p><p>As defined in <a id="_idIndexMarker1165"/>our training <a id="_idIndexMarker1166"/>script, the first convolution layer <a id="_idIndexMarker1167"/>has 64 filters. Each <a id="_idIndexMarker1168"/>one is 3x3 pixels, with a single channel (2D). Accordingly, gradients have the same shape.</p></li>
				<li>We write a function to plot filter weights and gradients over time, and we plot weights in the last filter of the first convolution layer:<p class="source-code">plot_conv_filter('conv2d/weights/conv2d/kernel:0', 63)</p><p>You can see the graph in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="Images/B17705_10_8.jpg" alt="Figure 10.8 – Plotting the weights of a convolution filter over time&#13;&#10;" width="383" height="252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Plotting the weights of a convolution filter over time</p>
			<p>As you <a id="_idIndexMarker1169"/>can see, SageMaker Debugger makes it really easy to inspect <a id="_idIndexMarker1170"/>training jobs. If you work with the built-in <a id="_idIndexMarker1171"/>containers that support it, you <a id="_idIndexMarker1172"/>don't need to modify your code. All configuration takes place in the estimator.</p>
			<p>You can find additional examples at <a href="https://github.com/awslabs/amazon-sagemaker-examples">https://github.com/awslabs/amazon-sagemaker-examples</a>, including some advanced use cases such as real-time visualization and model pruning.</p>
			<p>This concludes the first part of the chapter, where we learned how to optimize the cost of training jobs with managed spot training, their accuracy with automatic model tuning, and how to inspect their internal state with SageMaker Debugger.</p>
			<p>In the second part, we're going to dive into two advanced capabilities that will help us build better training workflows – SageMaker Feature Store and SageMaker Clarify.</p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor223"/>Managing features and building datasets with SageMaker Feature Store</h1>
			<p>Until now, we've engineered our training and validation features in a notebook or in a SageMaker <a id="_idIndexMarker1173"/>Processing script, before <a id="_idIndexMarker1174"/>storing them as S3 objects. Then, we used these objects as-is to train and evaluate models. This is a perfectly <a id="_idIndexMarker1175"/>reasonable workflow. However, the following questions may arise as your machine learning <a id="_idIndexMarker1176"/>workflows grow and mature:</p>
			<ul>
				<li>How can we apply a well-defined schema to our features?</li>
				<li>How can we select a subset of our features to build different datasets?</li>
				<li>How can we store and manage different feature versions?</li>
				<li>How can we discover and reuse feature engineering by other teams?</li>
				<li>How can we access engineered features at prediction time?</li>
			</ul>
			<p>SageMaker Feature Store is designed to answer these questions. Let's add it to the classification training workflow we built with BlazingText and Amazon Reviews in <a href="B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 6</em></a>, <em class="italic">Training Natural Language Processing Models</em>.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor224"/>Engineering features with SageMaker Processing</h2>
			<p>We can reuse our previous SageMaker Processing job almost as-is. The only difference is the output <a id="_idIndexMarker1177"/>format of the engineered data. In the original job, we saved it as a plain text file according to the input format expected by BlazingText. This format is inconvenient for SageMaker Feature Store, as we need easy access to each column. CSV doesn't work either as reviews contain commas, so we decide to use TSV instead:</p>
			<ol>
				<li value="1">Accordingly, we add a few lines to our processing script:<p class="source-code">fs_output_dir = '/opt/ml/processing/output/fs/'</p><p class="source-code">os.makedirs(fs_output_dir, exist_ok=True)</p><p class="source-code">fs_output_path = os.path.join(fs_output_dir, 'fs_data.tsv')  </p><p class="source-code">data.to_csv(fs_output_path, index=False,header=True, sep='\t')</p></li>
				<li>Running our SageMaker Processing job as before, we now see two outputs: a plain text output for BlazingText (in case we wanted to train directly on the full dataset) and a TSV output that we'll ingest in SageMaker Feature Store:<p class="source-code"><strong class="bold">s3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2021-07-05-07-54-15-145/output/bt_data</strong></p><p class="source-code"><strong class="bold">s3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2021-07-05-07-54-15-145/output/fs_data</strong></p></li>
				<li>Let's load <a id="_idIndexMarker1178"/>the TSV file in a <strong class="source-inline">pandas</strong> dataframe and display the first few rows:<p class="source-code">fs_training_output_path = 's3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2021-07-05-07-54-15-145/output/fs_data/fs_data.tsv'</p><p class="source-code">data = pd.read_csv(fs_training_output_path, sep='\t',</p><p class="source-code">                   error_bad_lines=False, dtype='str')</p><p class="source-code">data.head()</p><p> This prints out the table visible in the next image:</p></li>
			</ol>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="Images/B17705_10_9.jpg" alt="Figure 10.9 – Viewing the first rows&#13;&#10;" width="700" height="163"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Viewing the first rows</p>
			<p>Now, let's create a feature group where we'll ingest this data.</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor225"/>Creating a feature group</h2>
			<p>A <strong class="bold">feature group</strong> is a resource that stores a collection of related features. Feature groups are <a id="_idIndexMarker1179"/>organized in rows, which have a unique identifier and a timestamp. Each row contains key-value pairs, where each pair represents a feature name and a feature value.</p>
			<ol>
				<li value="1">First, let's define the name of our feature group:<p class="source-code">from sagemaker.feature_store.feature_group import FeatureGroup</p><p class="source-code">feature_group_name = 'amazon-reviews-feature-group-' + strftime('%d-%H-%M-%S', gmtime())</p><p class="source-code">feature_group = FeatureGroup(</p><p class="source-code">    name=feature_group_name,    </p><p class="source-code">    sagemaker_session=feature_store_session)</p></li>
				<li>Next, we set the name of the feature that contains a unique identifier –  <strong class="source-inline">review_id</strong> works perfectly here, and you could use any unique value present in your data source, such as a primary key:<p class="source-code">record_identifier_feature_name = 'review_id'</p></li>
				<li>Then, we add a timestamp column to all rows in our <strong class="source-inline">pandas</strong> dataframe. If your data source already contains a timestamp, you can reuse that value, either in the <strong class="bold">float64</strong> format or in the <strong class="bold">UNIX</strong> date/time format:<p class="source-code">event_time_feature_name = 'event_time'</p><p class="source-code">current_time_sec = int(round(time.time()))</p><p class="source-code">data = data.assign(event_time=current_time_sec)</p><p>Our dataframe now looks like the following picture:</p><div id="_idContainer158" class="IMG---Figure"><img src="Images/B17705_10_10.jpg" alt="Figure 10.10 – Viewing timestamps&#13;&#10;" width="769" height="161"/></div><p class="figure-caption">Figure 10.10 – Viewing timestamps</p></li>
				<li>The next step is to define a schema for the feature group. We can either provide it explicitly <a id="_idIndexMarker1180"/>in a JSON document or let SageMaker pick it up from the pandas dataframe. We use the second option:<p class="source-code">data['review_id'] = data['review_id']</p><p class="source-code">    .astype('str').astype('string')</p><p class="source-code">data['product_id'] = data['product_id']</p><p class="source-code">    .astype('str').astype('string')</p><p class="source-code">data['review_body'] = data['review_body']</p><p class="source-code">    .astype('str').astype('string')</p><p class="source-code">data['label'] = data['label']</p><p class="source-code">    .astype('str').astype('string')</p><p class="source-code">data['star_rating'] = data['star_rating']</p><p class="source-code">    .astype('int64')</p><p class="source-code">data['event_time'] = data['event_time']</p><p class="source-code">    .astype('float64')</p><p>We then load feature definitions:</p><p class="source-code">feature_group.load_feature_definitions(</p><p class="source-code">    data_frame=data)</p></li>
				<li>Finally, we create the feature group, passing the S3 location where features will be stored. This is where we'll query them to build datasets. We enable the online store, which will give us low-latency access to features at prediction time. We also <a id="_idIndexMarker1181"/>add a description and tags which make it easier to discover the feature group:<p class="source-code">feature_group.create(</p><p class="source-code">  role_arn=role,</p><p class="source-code">  s3_uri='s3://{}/{}'.format(default_bucket, prefix),</p><p class="source-code">  enable_online_store=True,</p><p class="source-code">  record_identifier_name=</p><p class="source-code">      record_identifier_feature_name,</p><p class="source-code">  event_time_feature_name=</p><p class="source-code">      event_time_feature_name,</p><p class="source-code">  description="1.8M+ tokenized camera reviews from the   </p><p class="source-code">               Amazon Customer Reviews dataset",</p><p class="source-code">  tags=[</p><p class="source-code">      { 'Key': 'Dataset', </p><p class="source-code">        'Value': 'amazon customer reviews' },</p><p class="source-code">      { 'Key': 'Subset',</p><p class="source-code">        'Value': 'cameras' },</p><p class="source-code">      { 'Key': 'Owner',</p><p class="source-code">        'Value': 'Julien Simon' }</p><p class="source-code">  ])</p></li>
			</ol>
			<p>After a few seconds, the feature group is ready and visible in SageMaker Studio, under <strong class="bold">Components and registries</strong> / <strong class="bold">Feature Store</strong>, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="Images/B17705_10_11.jpg" alt="Figure 10.11 – Viewing a feature group&#13;&#10;" width="903" height="150"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – Viewing a feature group</p>
			<p>Now, let's ingest data.</p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor226"/>Ingesting features</h2>
			<p>SageMaker <a id="_idIndexMarker1182"/>Feature Store lets us ingest data in three ways:</p>
			<ul>
				<li>Call the <strong class="source-inline">PutRecord()</strong> API to ingest a single record.</li>
				<li>Call the <strong class="source-inline">ingest()</strong> API to upload the contents of a <strong class="source-inline">pandas</strong> dataframe.</li>
				<li>If we used <strong class="bold">SageMaker Data Wrangler</strong> for feature engineering, use an auto-generated <a id="_idIndexMarker1183"/>notebook to create a feature group and ingest data.</li>
			</ul>
			<p>We use the second option here, which is as simple as the following:</p>
			<p class="source-code">feature_group.ingest(data_frame=data, max_workers=10, </p>
			<p class="source-code">                     wait=True)</p>
			<p>Once ingestion is complete, features are stored at the S3 location we specified, as well as in a dedicated low-latency backend. Let's use the former to build a dataset.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor227"/>Querying features to build a dataset</h2>
			<p>When we <a id="_idIndexMarker1184"/>create the feature <a id="_idIndexMarker1185"/>group, SageMaker automatically adds a new table for it in the <strong class="bold">AWS Glue Data Catalog</strong>. This makes it easy to use <strong class="bold">Amazon Athena</strong> to query <a id="_idIndexMarker1186"/>data and build datasets on demand.</p>
			<p>Let's say that we'd like to build a dataset that contains best-selling cameras with at least 1,000 reviews: </p>
			<ol>
				<li value="1">First, we write an SQL query that computes the average rating for each camera, counts how many reviews each camera received, only keeps cameras with at least 1,000 reviews, and orders cameras by descending average rating:<p class="source-code">query_string = </p><p class="source-code">'SELECT label,review_body FROM </p><p class="source-code">"'+ feature_group_table +'"'</p><p class="source-code">+ ' INNER JOIN (</p><p class="source-code">      SELECT product_id FROM (</p><p class="source-code">          SELECT product_id, avg(star_rating) as  </p><p class="source-code">                 avg_rating, count(*) as review_count</p><p class="source-code">          FROM "'+ feature_group_table+ '"' + '</p><p class="source-code">          GROUP BY product_id) </p><p class="source-code">      WHERE review_count &gt; 1000) tmp </p><p class="source-code">ON "'+feature_group_table+'"'</p><p class="source-code">+ '.product_id=tmp.product_id;'</p></li>
				<li>Then, we use <a id="_idIndexMarker1187"/>Athena to query our feature group, store selected rows in a <strong class="source-inline">pandas</strong> dataframe, and display the first few rows:<p class="source-code">dataset = pd.DataFrame()</p><p class="source-code">feature_group_query.run(query_string=query_string, output_location='s3://'+default_bucket+'/query_results/')</p><p class="source-code">feature_group_query.wait()dataset = feature_group_query.as_dataframe()</p><p class="source-code">dataset.head()</p></li>
			</ol>
			<p>This prints out the table visible in the next image:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="Images/B17705_10_12.jpg" alt="Figure 10.12 – Viewing query results&#13;&#10;" width="435" height="161"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Viewing query results</p>
			<p>From then <a id="_idIndexMarker1188"/>on, it's business as usual. We can save this dataframe to a CSV file and use it to train models. You'll find an end-to-end example in the GitHub repository.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor228"/>Exploring other capabilities of SageMaker Feature Store</h2>
			<p>Over time, we could store different versions of the same feature – that is, several records with <a id="_idIndexMarker1189"/>the same identifier but with different timestamps. This would allow us to retrieve earlier versions of a dataset – "time traveling" in our data with a simple SQL query. </p>
			<p>Last but not least, features are also available in the online store. We can retrieve individual records with the <strong class="source-inline">GetRecord()</strong> API and use features at prediction time whenever needed.</p>
			<p>Again, you'll find code samples for both capabilities in the GitHub repository.</p>
			<p>To close this chapter, let's look at Amazon SageMaker Clarify, a capability that helps us build higher quality models by detecting potential bias present in datasets and models.</p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor229"/>Detecting bias in datasets and explaining predictions with SageMaker Clarify</h1>
			<p>A <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model is <a id="_idIndexMarker1190"/>only as good as the dataset <a id="_idIndexMarker1191"/>it was built from. If a dataset <a id="_idIndexMarker1192"/>is inaccurate or unfair in representing <a id="_idIndexMarker1193"/>the reality it's supposed to capture, a corresponding model is very likely to learn this biased representation and perpetuate it in its predictions. As ML practitioners, we need to be aware of these problems, understand how they impact predictions, and limit that impact whenever possible.</p>
			<p>In this <a id="_idIndexMarker1194"/>example, we'll work with the <strong class="bold">Adult Data Set</strong>, available at the <strong class="bold">UCI Machine Learning Repository</strong> (<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>, Dua, D. and Graff, C., 2019). This dataset describes a binary <a id="_idIndexMarker1195"/>classification task, where we try to <a id="_idIndexMarker1196"/>predict if an individual earns less <a id="_idIndexMarker1197"/>or more than $50,000 per year. Here, we'd like to check whether this dataset introduces gender bias or not. In other words, does it help us build models that predict equally well for men and women?</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset you'll find in the GitHub repository has been slightly processed. The label column has been moved to the front as per XGBoost requirements. Categorical variables have been one-hot encoded.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor230"/>Configuring a bias analysis with SageMaker Clarify</h2>
			<p>SageMaker Clarify computes pre-training and post-training metrics that help us understand how a model predicts.</p>
			<p>Post-training <a id="_idIndexMarker1198"/>metrics obviously require a trained model, so we first train a binary classification model with XGBoost. It's nothing we haven't seen many times already, and you'll find the code in the GitHub repository. This model hits a validation AuC of 92.75%. </p>
			<p>Once training is complete, we can proceed with the bias analysis:</p>
			<ol>
				<li value="1">Bias analyses run as SageMaker Processing jobs. Accordingly, we create a <strong class="source-inline">SageMakerClarifyProcessor</strong> object with our infrastructure requirements. As the job is small-scale, we use a single instance. For larger jobs, we could use an increased instance count, and the analysis would automatically run on <strong class="bold">Spark</strong>:<p class="source-code">from sagemaker import clarify</p><p class="source-code">clarify_processor = clarify.SageMakerClarifyProcessor(</p><p class="source-code">    role=role,</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large',</p><p class="source-code">    sagemaker_session=session)</p></li>
				<li>Then, we create a <strong class="source-inline">DataConfig</strong> object describing the dataset to analyze:<p class="source-code">bias_report_output_path = 's3://{}/{}/clarify-bias'.format(bucket, prefix)</p><p class="source-code">data_config = clarify.DataConfig(</p><p class="source-code">    s3_data_input_path=train_uri,</p><p class="source-code">    s3_output_path=bias_report_output_path,</p><p class="source-code">    label='Label',</p><p class="source-code">    headers=train_data.columns.to_list(),</p><p class="source-code">    dataset_type='text/csv')</p></li>
				<li>Likewise, we <a id="_idIndexMarker1199"/>create a <strong class="source-inline">ModelConfig</strong> object describing the model to analyze:<p class="source-code">model_config = clarify.ModelConfig(</p><p class="source-code">    model_name=xgb_predictor.endpoint_name,</p><p class="source-code">    instance_type='ml.t2.medium',</p><p class="source-code">    instance_count=1,</p><p class="source-code">    accept_type='text/csv')</p></li>
				<li>Finally, we create a <strong class="source-inline">BiasConfig</strong> object describing the metrics to compute. The <strong class="source-inline">label_values_or_threshold</strong> defines the label value for the positive outcome (1, indicating a revenue higher than $50K). The <strong class="source-inline">facet_name</strong> defines the feature on which we'd like to run the analysis (<strong class="source-inline">Sex_</strong>), and <strong class="source-inline">facet_values_or_threshold</strong> defines the feature value for the potentially disadvantaged group (1, indicating women).<p class="source-code">bias_config = clarify.BiasConfig(</p><p class="source-code">    label_values_or_threshold=[1],</p><p class="source-code">    facet_name='Sex_',</p><p class="source-code">    facet_values_or_threshold=[1])</p></li>
			</ol>
			<p>We're <a id="_idIndexMarker1200"/>now ready to run the analysis.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor231"/>Running a bias analysis</h2>
			<p>Putting <a id="_idIndexMarker1201"/>everything together, we launch the analysis with the following:</p>
			<p class="source-code">clarify_processor.run_bias(</p>
			<p class="source-code">    data_config=data_config,</p>
			<p class="source-code">    model_config=model_config,</p>
			<p class="source-code">    bias_config=bias_config)</p>
			<p>Once the analysis is complete, the results are visible in SageMaker Studio. A report is also generated and stored in S3 in HTML, PDF, and notebook format.</p>
			<p>In <strong class="bold">Experiments and trials</strong>, we locate our SageMaker Clarify job, and we right-click on <strong class="bold">Open trial details</strong>. Selecting <strong class="bold">Bias report</strong>, we see bias metrics, as shown in the next screenshot:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="Images/B17705_10_13.jpg" alt="Figure 10.13 – Viewing bias metrics&#13;&#10;" width="1030" height="715"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – Viewing bias metrics</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor232"/>Analyzing bias metrics</h2>
			<p>If you'd <a id="_idIndexMarker1202"/>like to learn more about bias metrics, what they mean, and how they're computed, I highly recommend these resources:</p>
			<ul>
				<li><a href="https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf">https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf</a></li>
				<li><a href="https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf">https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf</a></li>
				<li><a href="https://github.com/aws/amazon-sagemaker-clarify">https://github.com/aws/amazon-sagemaker-clarify</a></li>
			</ul>
			<p>Let's look at two pre-training metrics, <strong class="bold">Class Imbalance</strong> (<strong class="bold">CI</strong>) and <strong class="bold">Difference in Positive Proportions in Labels</strong> (<strong class="bold">DPL</strong>), and one post-training metric, <strong class="bold">Difference in Positive Proportions in Predicted Labels</strong> (<strong class="bold">DPPL</strong>).</p>
			<p>A non-zero <a id="_idIndexMarker1203"/>value of CI indicates that the dataset is imbalanced. Here, the difference between the men fraction and the women fraction is 0.35. Indeed, the men group is about two-thirds of the dataset, the women group is about <a id="_idIndexMarker1204"/>one-third. This isn't a very severe imbalance, but we should also look at the proportion of positive labels for each class.</p>
			<p>The DPL <a id="_idIndexMarker1205"/>measures if each class has the same proportion of positive labels. In other words, does the dataset contain the same ratio of men and women earning $50K? The DPL is non-zero (0.20), which tells us that men have a higher ratio of $50K earners.</p>
			<p>The DPPL is <a id="_idIndexMarker1206"/>a post-training metric similar to the DPL. Its value (0.18) shows that the model unfortunately picked up the bias present in the dataset, only lightly reducing it. Indeed, the model predicts a more favorable outcome for men (over-predicting $50K earners) and a less favorable outcome for women (under-predicting 50K earners). </p>
			<p>That's clearly a problem. Although the model has a rather nice validation AuC (92.75%), it doesn't predict both classes equally well.</p>
			<p>Before we dive into the data and try to mitigate this issue, let's run an explainability analysis.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor233"/>Running an explainability analysis</h2>
			<p>SageMaker <a id="_idIndexMarker1207"/>Clarify can compute local and global SHAP (<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>) values. They help us understand feature importance, and how individual feature values contribute to a positive or negative outcome.</p>
			<p>Bias analyses run as SageMaker Processing jobs, and the process is similar:</p>
			<ol>
				<li value="1">We create a <strong class="source-inline">DataConfig</strong> object describing the dataset to analyze:<p class="source-code">explainability_output_path = 's3://{}/{}/clarify-explainability.format(bucket, prefix)</p><p class="source-code">data_config = clarify.DataConfig(</p><p class="source-code">    s3_data_input_path=train_uri,</p><p class="source-code">    s3_output_path= explainability_output_path,</p><p class="source-code">    label='Label',</p><p class="source-code">    headers=train_data.columns.to_list(),</p><p class="source-code">    dataset_type='text/csv')</p></li>
				<li>We create a <strong class="source-inline">SHAPConfig</strong> object describing how we'd like to compute SHAP values – that is, which baseline to use (I use the test set where I removed labels), how <a id="_idIndexMarker1208"/>many samples to use (twice the number of features plus 2048, a common default), and how to aggregate values:<p class="source-code">shap_config = clarify.SHAPConfig(</p><p class="source-code">    baseline=test_no_labels_uri,</p><p class="source-code">    num_samples=2*86+2048,</p><p class="source-code">    agg_method='mean_abs',</p><p class="source-code">    save_local_shap_values=True</p><p class="source-code">)</p></li>
				<li>Finally, we run the analysis:<p class="source-code">clarify_processor.run_explainability(</p><p class="source-code">    data_config=explainability_data_config,</p><p class="source-code">    model_config=model_config,</p><p class="source-code">    explainability_config=shap_config</p><p class="source-code">)</p></li>
			</ol>
			<p>Results in available in SageMaker Studio, under <strong class="bold">Experiments and trials</strong> / <strong class="bold">Open trial details</strong> / <strong class="bold">Model explainability</strong>. As shown in the next image, the <strong class="source-inline">Sex</strong> feature is by far the most important, which confirms the bias analysis. Ethical considerations aside, this doesn't seem to make a lot of sense from a business perspective. Features such as education or capital gain should be more important.</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="Images/B17705_10_14.jpg" alt="Figure 10.14 – Viewing feature importance&#13;&#10;" width="1235" height="941"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – Viewing feature importance</p>
			<p>Local SHAP <a id="_idIndexMarker1209"/>values have also been computed and stored in S3. We could use them to understand how feature values impact the prediction of each individual sample.</p>
			<p>Now, let's see how we can try to mitigate the bias we detected in our dataset.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor234"/>Mitigating bias</h2>
			<p>This dataset combines two problems. First, it contains more men than women. Second, the men <a id="_idIndexMarker1210"/>group has a higher proportion of positive outcomes. The combination of these two problems leads to a situation where the dataset contains a disproportionately low number of women who earn more than $50K. This makes it harder for the model to learn in a fair way, and it tends to favor the majority class.</p>
			<p>Bias mitigation techniques include the following:</p>
			<ul>
				<li>Undersampling the majority class by removing majority samples to rebalance the dataset</li>
				<li>Oversampling the minority class by adding more samples through duplication of existing ones</li>
				<li>Adding synthetic samples to the minority class by generating new samples that have statistical properties similar to existing samples<p class="callout-heading">Note</p><p class="callout">Altering data shouldn't be done lightly, especially in organizations operating in regulated industries. This can have serious business, compliance, and legal consequences. Please make sure to get approval before doing this in production.</p></li>
			</ul>
			<p>Let's try <a id="_idIndexMarker1211"/>a combined approach based on the <strong class="bold">imbalanced-learn</strong> open source library (https://imbalanced-learn.org). First, we'll <a id="_idIndexMarker1212"/>add synthetic samples to the minority class with the <strong class="bold">Synthetic Minority Oversampling Technique</strong> (<strong class="bold">SMOTE</strong>) algorithm, in order to match the ratio of $50K earners present in the majority samples. Then, we'll undersample the majority class to match the number of samples <a id="_idIndexMarker1213"/>of the minority class. The result will be a perfectly balanced dataset, where both classes have the same size and the same ratio of $50K earners. Let's get started:</p>
			<ol>
				<li value="1">First, we need to compute the ratios for both classes:<p class="source-code">female_male_not_50k_count = train_data['Sex_'].where(</p><p class="source-code">    train_data['Label']==0).value_counts()</p><p class="source-code">female_male_50k_count = train_data['Sex_'].where(</p><p class="source-code">    train_data['Label']==1).value_counts()</p><p class="source-code">ratios = female_male_50k_count / </p><p class="source-code">         female_male_not_50k_count</p><p class="source-code">print(ratios)</p><p>This gives us the following result, showing that the majority class (class 0) has a much larger ratio of $50k earners:</p><p class="source-code"><strong class="bold">0.0    0.457002</strong></p><p class="source-code"><strong class="bold">1.0    0.128281</strong></p></li>
				<li>Then, we generate synthetic minority samples:<p class="source-code">from imblearn.over_sampling import SMOTE</p><p class="source-code">female_instances = train_data[train_data['Sex_']==1]</p><p class="source-code">female_X = female_instances.drop(['Label'], axis=1)</p><p class="source-code">female_Y = female_instances['Label']</p><p class="source-code">oversample = SMOTE(sampling_strategy=ratios[0])</p><p class="source-code">balanced_female_X, balanced_female_Y = oversample.fit_resample(female_X, female_Y)</p><p class="source-code">balanced_female=pd.concat([balanced_female_X, balanced_female_Y], axis=1)</p></li>
				<li>Next, we <a id="_idIndexMarker1214"/>rebuild the dataset with the original majority class and the rebalanced minority class:<p class="source-code">male_instances = train_data[train_data['Sex_']==0]</p><p class="source-code">balanced_train_data=pd.concat(</p><p class="source-code">    [male_instances, balanced_female], axis=0)</p></li>
				<li>Finally, we undersample the original majority class to rebalance ratios:<p class="source-code">from imblearn.under_sampling import RandomUnderSampler</p><p class="source-code">X = balanced_train_data.drop(['Sex_'], axis=1)</p><p class="source-code">Y = balanced_train_data['Sex_']</p><p class="source-code">undersample = RandomUnderSampler(</p><p class="source-code">    sampling_strategy='not minority')</p><p class="source-code">X,Y = undersample.fit_resample(X, Y)</p><p class="source-code">balanced_train_data=pd.concat([X, Y], axis=1)</p></li>
				<li>We count both classes and compute their ratios again:<p class="source-code">female_male_count= balanced_train_data['Sex_']    </p><p class="source-code">    .value_counts()</p><p class="source-code">female_male_50k_count = balanced_train_data['Sex_']</p><p class="source-code">    .where(balanced_train_data['Label']==1)</p><p class="source-code">    .value_counts()</p><p class="source-code">ratios = female_male_50k_count/female_male_count</p><p class="source-code">print(female_male_count)</p><p class="source-code">print(female_male_50k_count)</p><p class="source-code">print(ratios)</p><p>This displays the following results:</p><p class="source-code"><strong class="bold">1.0    0.313620</strong></p><p class="source-code"><strong class="bold">0.0    0.312039</strong></p></li>
			</ol>
			<p>Training with <a id="_idIndexMarker1215"/>this rebalanced dataset, and using the same test set, we get a validation AuC of 92.95%, versus 92.75% for the original model. Running a new bias analysis, CI is zero, and the DPL and DPPL are close to zero. </p>
			<p>Not only have we built a model that predicts more fairly, but it's also a little bit more accurate. For once, it looks like we got the best of both worlds!</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor235"/>Summary</h1>
			<p>This chapter concludes our exploration of training techniques. You learned about managed spot training, a simple way to slash training costs by 70% or more. You also saw how checkpointing helps to resume jobs that have been interrupted. Then, you learned about automatic model tuning, a great way to extract more accuracy from your models by exploring hyperparameter ranges. You learned about SageMaker Debugger, an advanced capability that automatically inspects training jobs for unwanted conditions and saves tensor collections to S3 for inspection and visualization. Finally, we discovered two capabilities that help you build higher quality workflows and models, SageMaker Feature Store and SageMaker Clarify.</p>
			<p>In the next chapter, we'll study model deployment in detail.</p>
		</div>
	</div></body></html>
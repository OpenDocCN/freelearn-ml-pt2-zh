["```py\nlines = [\n    'How to tokenize?\\nLike a boss.',\n    'Google is accessible via http://www.google.com',\n    '1000 new followers! #TwitterFamous',\n]\n```", "```py\nfor line in lines:\n    print(line.split())\n```", "```py\n['How', 'to', 'tokenize?', 'Like', 'a', 'boss.']\n['Google', 'is', 'accessible', 'via', 'http://www.google.com']\n['1000', 'new', 'followers!', '#TwitterFamous']\n```", "```py\nimport re\n_token_pattern = r\"\\w+\"\ntoken_pattern = re.compile(_token_pattern)\n\nfor line in lines:\n    print(token_pattern.findall(line))\n```", "```py\n['How', 'to', 'tokenize', 'Like', 'a', 'boss']\n['Google', 'is', 'accessible', 'via', 'http', 'www', 'google', 'com']\n['1000', 'new', 'followers', 'TwitterFamous']\n```", "```py\n_token_pattern = r\"\\w+\"\ntoken_pattern = re.compile(_token_pattern)\n\ndef tokenizer(line):\n    line = line.lower()\n    line = re.sub(r'http[s]?://[\\w\\/\\-\\.\\?]+','_url_', line)\n    line = re.sub(r'#\\w+', '_hashtag_', line)\n    line = re.sub(r'\\d+','_num_', line)\n    return token_pattern.findall(line)\n\nfor line in lines:\n    print(tokenizer(line))\n```", "```py\n['how', 'to', 'tokenize', 'like', 'a', 'boss']\n['google', 'is', 'accessible', 'via', '_url_']\n['_num_', 'new', 'followers', '_hashtag_']\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(lowercase=True, tokenizer=tokenizer)\nx = vec.fit_transform(lines)\n```", "```py\npd.DataFrame(\n    x.todense(), \n    columns=vec.get_feature_names()\n)\n```", "```py\nflight_delayed_lines = [\n    'Flight was delayed, I am not happy',\n    'Flight was not delayed, I am happy'\n]\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(ngram_range=(2,2))\nx = vec.fit_transform(flight_delayed_lines)\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(analyzer='char', ngram_range=(4,4))\nx = vec.fit_transform(flight_delayed_lines)\n```", "```py\nlines_fruits = [\n    'I like apples',\n    'I like oranges',\n    'I like pears',\n]\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(token_pattern=r'\\w+')\nx = vec.fit_transform(lines_fruits)\n```", "```py\n          pip install spacy\n\n          python -m spacy download en_core_web_lg\n\n```", "```py\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n\nterms = ['I', 'like', 'apples', 'oranges', 'pears']\nvectors = [\n    nlp(term).vector.tolist() for term in terms\n]\n```", "```py\n# pd.Series(vectors[terms.index('apples')]).rename('apples')\n\n0     -0.633400\n1      0.189810\n2     -0.535440\n3     -0.526580\n         ...   \n296   -0.238810\n297   -1.178400\n298    0.255040\n299    0.611710\nName: apples, Length: 300, dtype: float64\n```", "```py\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncm = sns.light_palette(\"Gray\", as_cmap=True)\n\npd.DataFrame(\n    cosine_similarity(vectors),\n    index=terms, columns=terms,\n).round(3).style.background_gradient(cmap=cm)\n```", "```py\nimport os\n\ndata_dir = f'{os.getcwd()}/data'\n\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)\n```", "```py\n          pip install requests\n\n```", "```py\nimport requests\n\nurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'\n\nresponse = requests.get(url)\n```", "```py\nimport zipfile\n\nfrom io import BytesIO\n\nwith zipfile.ZipFile(file=BytesIO(response.content), mode='r') as compressed_file:\n    compressed_file.extractall(data_dir)\n```", "```py\ndf_list = []\n\nfor csv_file in ['imdb_labelled.txt', 'yelp_labelled.txt', 'amazon_cells_labelled.txt']:\n\n    csv_file_with_path = f'{data_dir}/sentiment labelled sentences/{csv_file}'\n    temp_df = pd.read_csv(\n        csv_file_with_path, \n        sep=\"\\t\", header=0, \n        names=['text', 'sentiment']\n    ) \n    df_list.append(temp_df)\n\ndf = pd.concat(df_list)\n```", "```py\nexplode = [0.05, 0.05]\ncolors = ['#777777', '#111111']\ndf['sentiment'].value_counts().plot(\n    kind='pie', colors=colors, explode=explode\n)\n```", "```py\npd.options.display.max_colwidth = 90\ndf[['text', 'sentiment']].sample(5, random_state=42)\n```", "```py\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.4, random_state=42)\n```", "```py\ny_train = df_train['sentiment']\ny_test = df_test['sentiment']\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer(ngram_range=(1,3), min_df=3, strip_accents='ascii')\nx_train = vec.fit_transform(df_train['text'])\nx_test = vec.transform(df_test['text'])\n```", "```py\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB(fit_prior=True)\nclf.fit(x_train, y_train)\ny_test_pred = clf.predict(x_test)\n```", "```py\np, r, f, s = precision_recall_fscore_support(y_test, y_test_pred)\n```", "```py\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\npipe = Pipeline(steps=[\n    ('CountVectorizer', CountVectorizer()), \n    ('MultinomialNB', MultinomialNB())]\n)\n```", "```py\nparam_grid = {\n    'CountVectorizer__ngram_range': [(1,1), (1,2), (1,3)],\n    'MultinomialNB__alpha': [0.1, 1],\n    'MultinomialNB__fit_prior': [True, False],\n}\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\nsearch = GridSearchCV(pipe, param_grid, scoring='precision_macro', n_jobs=-1)\nsearch.fit(df_train['text'], y_train)\nprint(search.best_params_)\n```", "```py\nimport spacy\n\nclass WordEmbeddingVectorizer:\n\n    def __init__(self, language_model='en_core_web_md'):\n        self.nlp = spacy.load(language_model)\n\n    def fit(self):\n        pass\n\n    def transform(self, x, y=None):\n        return pd.Series(x).apply(\n            lambda doc: self.nlp(doc).vector.tolist()\n        ).values.tolist()\n\n    def fit_transform(self, x, y=None):\n        return self.transform(x)\n```", "```py\nvec = WordEmbeddingVectorizer()\nx_train_w2v = vec.transform(df_train['text'])\n```"]
<html><head></head><body>
    <section>
      <header class="header-title chapter-title">
                    Imbalanced Learning – Not Even 1% Win the Lottery
                </header>
      <article>
        <p>Cases where your classes are neatly balanced are more of an exception than the rule. In most of the interesting problems we'll come across, the classes are extremely imbalanced. Luckily, a small fraction of online payments are fraudulent, just like a small fraction of the population catch rare diseases. Conversely, few contestants win the lottery and fewer of your acquaintances become your close friends. That's why we are usually interested in capturing those rare cases.</p>
        <p>In this chapter, we will learn how to deal with imbalanced classes. We will start by giving different weights to our training samples to mitigate the class imbalance problem. Afterward, we will learn about other techniques, such as undersampling and oversampling. We will see the effect of these techniques in practice. We will also learn how to combine concepts such as ensemble learning with resampling, and also introduce new scores to validate if our learners are meeting our needs. </p>
        <p>The following topics will be covered in this chapter:</p>
        <ul>
          <li>Reweighting the training samples</li>
          <li>Random oversampling</li>
          <li>Random undersampling</li>
          <li>Combing sampling with ensembles</li>
          <li>Equal opportunity score</li>
        </ul>
        <p>Let's get started!</p>
        <h1 id="uuid-8795dfce-8be6-4ef3-a8e7-e779a02970f3" class="pull-left">Getting the click prediction dataset </h1>
        <p>Usually, a small percentage of people who see an advertisement click on it. In other words, the percentage of samples in a positive class in such an instance can be just 1% or even less. This makes it hard to predict the <strong>click-through rate</strong> (<strong>CTR</strong>) since the training data is highly imbalanced. In this section, we are going to use a highly imbalanced dataset from the <strong>Knowledge Discovery in Databases</strong> (<strong>KDD</strong>) Cup.</p>
        <p>The KDD Cup is an annual competition organized by the ACM Special Interest Group on Knowledge Discovery and Data Mining. In 2012, they released a dataset for the advertisements shown alongside the search results in a search engine. The aim of the competitors was to predict whether a user will click on each ad or not. A modified version of the data has been published on the OpenML platform (<a href="https://www.openml.org/d/1220">https://www.openml.org/d/1220</a>). The CTR in the modified dataset is 16.8%. This is our positive class. We can also call it the minority class since the majority of the cases did not lead to an ad being clicked on. </p>
        <p>Here, we are going to download the data and put it into a DataFrame, as follows:</p>
        <pre>from sklearn.datasets import fetch_openml<br/>data = fetch_openml(data_id=1220)<br/><br/>df = pd.DataFrame(<br/>    data['data'],<br/>    columns=data['feature_names']<br/>).astype(float)<br/><br/>df['target'] = pd.Series(data['target']).astype(int) </pre>
        <p>We can display <kbd>5</kbd> random rows of the dataset using the following line of code:</p>
        <pre>df.sample(n=5, random_state=42)</pre>
        <p class="mce-root">We can make sure we get the same random lines if we both set <kbd>random_state</kbd> to the same value. In <em>The Hitchhiker's Guide to the Galaxy</em> by <em>Douglas Adams</em>, the number <kbd>42</kbd> was deemed as the answer to the ultimate question of life, the universe, and everything. So, we will stick to setting <kbd>random_state</kbd>to<kbd>42</kbd>throughout this chapter. Here is our five-line sample: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/af9bb2e9-7f3c-4ff5-a9b7-72034f38b679.png" style="width:75.83em;"/>
        </p>
        <p>There are two things we need to keep in mind about this data:</p>
        <ul>
          <li> The classes are imbalanced, as mentioned earlier. You can check this by running <kbd>df['target'].mean()</kbd>, which will give you <kbd>16.8%</kbd>. </li>
          <li>Despite the fact that all the features are numerical, it is clear that all the features ending with the <kbd>id</kbd> suffix are supposed to be treated as categorical features. For example, the relationship between <kbd>ad_id</kbd> and the CTR is not expected to be linear, and thus when using a linear model, we may need to encode these features using a <em>one-hot encoder</em>. Nevertheless, due to their high cardinality, a one-hot encoding strategy will result in too many features for our classifier to deal with. Therefore, we need to come up with another scalable solution. For now, let's learn how to check the cardinality of each feature: </li>
        </ul>
        <pre style="padding-left: 60px">for feature in data['feature_names']:<br/>    print(<br/>       'Cardinality of {}: {:,}'.format(<br/>            feature, df[feature].value_counts().shape[0]<br/>        )<br/>    )</pre>
        <p style="padding-left: 60px">This will give us the following results:</p>
        <pre style="padding-left: 60px">Cardinality of impression: 99
Cardinality of ad_id: 19,228
Cardinality of advertiser_id: 6,064
Cardinality of depth: 3
Cardinality of position: 3
Cardinality of keyword_id: 19,803
Cardinality of title_id: 25,321
Cardinality of description_id: 22,381
Cardinality of user_id: 30,114</pre>
        <p style="padding-left: 60px" class="mce-root">Finally, we will convert our data into <kbd>x_train</kbd>, <kbd>x_test</kbd>, <kbd>y_train</kbd>, and <kbd>y_test</kbd> sets, as follows:</p>
        <pre style="padding-left: 60px" class="mce-root">from sklearn.model_selection import train_test_split<br/>x, y = df[data['feature_names']], df['target']<br/>x_train, x_test, y_train, y_test = train_test_split(<br/>    x, y, test_size=0.25, random_state=42<br/>)</pre>
        <p>In this section, we downloaded the necessary data and added it to a DataFrame. In the next section, we will install the <kbd>imbalanced-learn</kbd> library.</p>
        <h1 id="uuid-39bdfa64-84a0-491d-b56b-e904fb536123">Installing the imbalanced-learn library</h1>
        <p>Due to class imbalance, we will need to resample our training data or apply different techniques to get better classification results. Thus, we are going to rely on the<kbd>imbalanced-learn</kbd>library here. The project was started in 2014 by <em>Fernando Nogueira</em>. It now offers multiple resampling data techniques, as well as metrics for evaluating imbalanced classification problems. The library's interface is compatible with scikit-learn.</p>
        <p>You can download the library via <kbd>pip</kbd> by running the following command in your Terminal: </p>
        <pre>
          <strong>pip install -U imbalanced-learn</strong>
        </pre>
        <p>Now, you can import and use its different modules in your code, as we will see in the following sections. One of the metrics provided by the library is the <strong>g</strong><strong>eometric mean score</strong>. In<a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&amp;action=edit">Chapter 8</a><em>, Ensembles – When One Model is Not Enough</em>, we learned about the <strong>true positive rate</strong><strong/>(<strong>TPR</strong>),<strong/>or sensitivity, and the <strong>false positive rate</strong> (<strong>FPR</strong>), and we used them to draw the area under the curve. We also learned about the<strong>true negative rate</strong> (<strong>TNR</strong>), or specificity, which is basically 1 minus the FPR. The geometric mean score, for binary classification problems, is the square root of the product of the sensitivity (TPR) and specificity (TNR). By combining these two metrics, we try to maximize the accuracy of each of the classes while taking their imbalances into account. The interface for<kbd>geometric_mean_score</kbd>is similar to the other scikit-learn metrics. It takes the true and predicted values and returns the calculated score, as follows:</p>
        <pre>from imblearn.metrics import geometric_mean_score<br/>geometric_mean_score(y_true, y_pred)</pre>
        <p>We will be using this metric in addition to the precision and recall scores throughout this chapter. </p>
        <p>In the next section, we are going to alter the weights of our training samples and see if this helps us deal with our imbalanced classes.</p>
        <h1 id="uuid-dfa47e0b-4ea9-4de7-a77d-8d0a97ff5de6">Predicting the CTR</h1>
        <p>We have our data and installed the <kbd>imbalanced-learn</kbd> library. Now, we are ready to build our classifier. As we mentioned earlier, the one-hot encoding techniques we are familiar with will not scale well with the high cardinality of our categorical features. In <a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&amp;action=edit">Chapter 8</a>, <em>Ensembles – When One Model is Not Enough</em>, we briefly mentioned<strong>random trees embedding</strong> as a technique for transforming our features. It is an ensemble of totally random trees, where each sample of our data will be represented according to the leaves of each tree it ends upon. Here, we are going to build a pipeline where the data will be transformed into a random trees embedding and scaled. Finally, a <strong>logistic regression</strong> classifier will be used to predict whether a click has occurred or not: </p>
        <pre>from sklearn.preprocessing import MaxAbsScaler<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.ensemble import RandomTreesEmbedding<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.metrics import precision_score, recall_score<br/>from imblearn.metrics import geometric_mean_score<br/><br/><br/>def predict_and_evalutate(x_train, y_train, x_test, y_test, sample_weight=None, title='Unweighted'):<br/><br/><br/>    clf = Pipeline(<br/>        [<br/>            ('Embedder', RandomTreesEmbedding(n_estimators=10, max_leaf_nodes=20, random_state=42)), <br/>            ('Scaler', MaxAbsScaler()),<br/>            ('Classifier', LogisticRegression(solver='saga', max_iter=1000, random_state=42))<br/>        ]<br/>    )<br/>    clf.fit(x_train, y_train, Classifier__sample_weight=sample_weight)<br/>    y_test_pred = clf.predict(x_test)<br/><br/>    print(<br/>        'Precision: {:.02%}, Recall: {:.02%}; G-mean: {:.02%} @ {}'.format(<br/>            precision_score(y_test, y_test_pred),<br/>            recall_score(y_test, y_test_pred),<br/>            geometric_mean_score(y_test, y_test_pred),<br/>            title<br/>        )<br/>    )<br/><br/>    return clf</pre>
        <p>We wrapped the whole process into a function so that we can reuse it later in this chapter. The <kbd>predict_and_evalutate()</kbd> function takes the x's and the y's, as well as the sample weights. We are going to use the sample weights in a moment, but you can ignore them for now. Once you're done predicting, the function will also print the different scores and return an instance of the pipeline that was used. </p>
        <p>We can use the function we have just created as follows:</p>
        <pre>clf = predict_and_evalutate(x_train, y_train, x_test, y_test) </pre>
        <p>By default, the precision and recall that are calculated are for the positive class. The previous code gave us a recall of <kbd>0.3%</kbd>, a precision of <kbd>62.5%</kbd>, and a geometric mean score of <kbd>5.45%</kbd>. The recall is less than <kbd>1%</kbd>, which means that the classifier won't be able to capture the vast majority of the positive/minority class. This is an expected scenario when dealing with imbalanced data. One way to fix this is to give more weights to the samples in the minority class. This is like asking the classifier to give more attention to these samples since we care about capturing them, despite their rareness. In the next section, we are going to see the effect of sample weighting on our classifier. </p>
        <h2 id="uuid-52ee4ec4-84b6-4178-994b-672f1f4e11c9">Weighting the training samples differently</h2>
        <p>The number of samples in the majority class is about five times those in the minority class. You can double-check this by running the following line of code:</p>
        <pre>(1 - y_train.mean()) / y_train.mean() </pre>
        <p>Thus, it makes sense to give the samples in the minority class five times the weight of the other samples. We can use the same <kbd>predict_and_evalutate()</kbd> function from the previous section and change the sample weights, as follows:</p>
        <pre>sample_weight = (1 * (y_train == 0)) + (5 * (y_train == 1))<br/>clf = predict_and_evalutate(<br/>    x_train, y_train, x_test, y_test, <br/>    sample_weight=sample_weight<br/>)</pre>
        <p>Now, the recall jumps to <kbd>13.4%</kbd> at the expense of the precision, which went down to <kbd>24.8%</kbd>. The geometric mean score went down from<kbd>5.5%</kbd> to <kbd>34%</kbd>, thanks to the new weights. </p>
        <p>The <kbd>predict_and_evalutate()</kbd> function returns an instance of the pipeline that was used. We can get the last component of the pipeline, the logistic regression classifier, via<kbd>clf[-1]</kbd>. Then, we can access the coefficients of the classifier that were assigned to each feature as we intercept it. Due to the embedding step, we may end up with up to 200 features; 10 estimators x up to 20 leaf nodes. The following function prints the last nine features, as well as the intercept, along with their coefficients: </p>
        <pre>def calculate_feature_coeff(clf):<br/>    return pd.DataFrame(<br/>        {<br/>            'Features': [<br/>                f'EmbFeature{e}' <br/>                for e in range(len(clf[-1].coef_[0]))<br/>            ] + ['Intercept'],<br/>            'Coeff': list(<br/>                clf[-1].coef_[0]<br/>            ) + [clf[-1].intercept_[0]]<br/>        }<br/><br/>    ).set_index('Features').tail(10)</pre>
        <p>The output of <kbd>calculate_feature_coeff(clf).round(2)</kbd> can also be rounded to two decimal points so that it looks as follows:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/11752e5a-6f7a-4f3e-acc6-68b93b6678b5.png" style="width:15.67em;"/>
        </p>
        <p>Now, let's compare three weighting strategies side by side. With a weight of one, both the minority and the majority classes get the same weights. Then, we give the minority class double the weight of the majority class, as well as five times its weight, as follows:</p>
        <pre>df_coef_list = []<br/>weight_options = [1, 2, 5]<br/><br/>for w in weight_options:<br/><br/>    print(f'\nMinority Class (Positive Class) Weight = Weight x {w}')<br/>    sample_weight = (1 * (y_train == 0)) + (w * (y_train == 1))<br/>    clf = predict_and_evalutate(<br/>        x_train, y_train, x_test, y_test, <br/>        sample_weight=sample_weight<br/>    )<br/>    df_coef = calculate_feature_coeff(clf)<br/>    df_coef = df_coef.rename(columns={'Coeff': f'Coeff [w={w}]'})<br/>    df_coef_list.append(df_coef)</pre>
        <p>This gives us the following results:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/77a78a51-3757-49c9-bb16-f13565334100.png" style="width:36.92em;"/>
        </p>
        <p>It is easy to see how the weighting affects the precision and the recall. It is as if one of them always improves at the expense of the other. This behavior is the result of moving the classifier's boundaries. As we know, the class boundaries are defined by the coefficients of the different features, as well as the intercept. I bet you are tempted to see the coefficients of the three previous models side by side. Luckily, we have saved the coefficients in<kbd>df_coef_list</kbd> so that we can display them using the following code snippet:</p>
        <pre>pd.concat(df_coef_list, axis=1).round(2).style.bar(<br/>    subset=[f'Coeff [w={w}]' for w in weight_options], <br/>    color='#999',<br/>    align='zero'<br/>)</pre>
        <p class="CDPAlignLeft CDPAlign">This gives us the following visual comparison between the three classifiers:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/c341204c-5d34-4d3a-ac90-b307d4d5aa21.png" style="width:30.08em;"/>
        </p>
        <p>The coefficients of the features did change slightly, but the changes in the intercept are more noticeable. In summary, the weighting affects the intercept the most and moves the class boundaries as a result.</p>
        <p>A sample is classified as a member of the positive class if the predicted probability is above <kbd>50%</kbd>. The movement of the intercept, without any changes in the other coefficients, is equivalent to changing the probability threshold so that it's above or below that <kbd>50%</kbd>. If the weighting only affected the intercept, we might suggest that we should try different probability thresholds until we get the desired precision-recall tradeoff. To check whether the weighting offered any additional benefit on top of altering the intercept, we have to check the area under the <strong>Receiver Operating Characteristic</strong> (<strong>ROC</strong>) curve. </p>
        <h3 id="uuid-9911538b-6d06-4031-9fdc-733939e0a791">The effect of the weighting on the ROC</h3>
        <p>Did the weighting improve the area under the ROC curve? To answer this question, let's start by creating a function that will display the ROC curve and print the <strong>area under the curve </strong>(<strong>AUC</strong>):</p>
        <pre>from sklearn.metrics import roc_curve, auc<br/><br/>def plot_roc_curve(y, y_proba, ax, label):<br/>    fpr, tpr, thr = roc_curve(y, y_proba)<br/>    auc_value = auc(fpr, tpr)<br/>    pd.DataFrame(<br/>        {<br/>            'FPR': fpr,<br/>            'TPR': tpr<br/>        }<br/>    ).set_index('FPR')['TPR'].plot(<br/>        label=label + f'; AUC = {auc_value:.3f}',<br/>        kind='line',<br/>        xlim=(0,1),<br/>        ylim=(0,1),<br/>        color='k',<br/>        ax=ax<br/>    )<br/>    return (fpr, tpr, auc_value)</pre>
        <p>Now, we can loop over the three weighting options and render their corresponding curves, as follows: </p>
        <pre>from sklearn.metrics import roc_curve, auc<br/><br/>fig, ax = plt.subplots(1, 1, figsize=(15, 8), sharey=False)<br/><br/>ax.plot(<br/>    [0, 1], [0, 1], <br/>    linestyle='--', <br/>    lw=2, color='k',<br/>    label='Chance', alpha=.8<br/>)<br/><br/>for w in weight_options:<br/><br/>    sample_weight = (1 * (y_train == 0)) + (w * (y_train == 1))<br/><br/>    clf = Pipeline(<br/>        [<br/>            ('Embedder', RandomTreesEmbedding(n_estimators=20, max_leaf_nodes=20, random_state=42)), <br/>            ('Scaler', MaxAbsScaler()),<br/>            ('Classifier', LogisticRegression(solver='lbfgs', max_iter=2000, random_state=42))<br/>        ]<br/>    )<br/>    clf.fit(x_train, y_train, Classifier__sample_weight=sample_weight)<br/>    y_test_pred_proba = clf.predict_proba(x_test)[:,1]<br/><br/>    plot_roc_curve(<br/>        y_test, y_test_pred_proba, <br/>        label=f'\nMinority Class Weight = Weight x {w}',<br/>        ax=ax<br/>    ) <br/><br/>ax.set_title('Receiver Operating Characteristic (ROC)')<br/>ax.set_xlabel('False Positive Rate')<br/>ax.set_ylabel('True Positive Rate')<br/><br/>ax.legend(ncol=1, fontsize='large', shadow=True)<br/><br/>fig.show() </pre>
        <p> These three curves are displayed here: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/dff06623-8599-4f34-89d4-a0fcbeadf72c.png" style="width:50.83em;"/>
        </p>
        <p>The ROC curve is meant to show the tradeoff between the TPR and the FPR for the different probability thresholds. If the area under the ROC curve is more or less than the same for the three weighting strategies, then the weighting did not offer much value beyond altering the classifier's intercept. Thus, it is up to us if we want to increase the recall at the expense of the precision to either reweight our training samples or to try different probability thresholds for our classification decision.</p>
        <p>In addition to the sample weighting, we can resample the training data so that we train on a more balanced set. In the next section, we are going to see the different sampling techniques offered by the <kbd>imbalanced-learn</kbd> library.</p>
        <h1 id="uuid-bc24993c-01ea-4568-910f-b87ef04dc93c">Sampling the training data</h1>
        <div class="packt_quote">"It's not denial. I'm just selective about the reality I accept."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">                                                                                                                            - Bill Watterson</div>
        <p class="mce-root">If the machine learning models were humans, they would have believed that the end justifies the means. When 99% of their training data belongs to one class, and their aim is to optimize their objective function, we cannot blame them if they focus on getting that single class right since it contributes to 99% of the solution. In the previous section, we tried to change this behavior by giving more weights to the minority class, or classes. Another strategy might entail removing some samples from the majority class or adding new samples to the minority class until the two classes are balanced. </p>
        <h2 id="uuid-697b4fcb-7c78-42fa-9296-4e800e3326aa">Undersampling the majority class</h2>
        <div class="packt_quote CDPAlignLeft CDPAlign">"Truth, like gold, is to be obtained not by its growth, but by washing away from it all that is not gold."         </div>
        <div class="packt_quote CDPAlignRight CDPAlign">- Leo Tolstoy</div>
        <p>We can randomly remove samples from the majority class until it becomes the same size as the minority class. When dealing with non-binary classification tasks, we can remove samples from all the classes until they all become the same size as the minority class. This technique is known as <strong>Random Undersampling</strong>. The following code shows how <kbd>RandomUnderSampler()</kbd> can be used to downsample the majority class: </p>
        <pre>from imblearn.under_sampling import RandomUnderSampler<br/><br/>rus = RandomUnderSampler()<br/>x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train)</pre>
        <p>Rather than keeping the classes balanced, you can just reduce their imbalance by setting the<kbd>sampling_strategy</kbd>hyperparameter. Its value dictates the final ratio of the minority class versus the majority class. In the following example, we kept the final size of the majority class so that it's twice that of the minority class:</p>
        <pre>from imblearn.under_sampling import RandomUnderSampler<br/><br/>rus = RandomUnderSampler(sampling_strategy=0.5)<br/>x_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train)</pre>
        <p>The downsampling process doesn't have to be random. For example, we can use the nearest neighbors algorithm to remove the samples that do not agree with their neighbors. The<kbd>EditedNearestNeighbours</kbd>module allows you to set the number of neighbors to check via its <kbd>n_neighbors</kbd>hyperparameter, as follows:</p>
        <pre>from imblearn.under_sampling import EditedNearestNeighbours<br/><br/>enn = EditedNearestNeighbours(n_neighbors=5)<br/>x_train_resampled, y_train_resampled = enn.fit_resample(x_train, y_train)</pre>
        <p>The previous techniques belong to what is known as <strong>prototype selection</strong>. In this situation, we select samples from already existing ones. In contrast to Prototype Selection, the <strong>prototype generation</strong> approach generates new samples to summarize the existing ones. The <em>ClusterCentroids</em> algorithm puts the majority class samples into clusters and uses the cluster centroids instead of the original samples. More on clustering and cluster centroids will be provided in<a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=34&amp;action=edit">Chapter 11</a><em>, Clustering – Making Sense of Unlabeled Data</em>. </p>
        <p>To compare the aforementioned algorithms, let's create a function that takes the x's and y's, in addition to the sampler instance, and then trains them and returns the predicted values for the test set: </p>
        <pre>from sklearn.preprocessing import MaxAbsScaler<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.ensemble import RandomTreesEmbedding<br/>from sklearn.pipeline import Pipeline<br/><br/>def sample_and_predict(x_train, y_train, x_test, y_test, sampler=None):<br/><br/>    if sampler:<br/>        x_train, y_train = sampler.fit_resample(x_train, y_train)<br/><br/>    clf = Pipeline(<br/>        [<br/>            ('Embedder', RandomTreesEmbedding(n_estimators=10, max_leaf_nodes=20, random_state=42)), <br/>            ('Scaler', MaxAbsScaler()),<br/>            ('Classifier', LogisticRegression(solver='saga', max_iter=1000, random_state=42))<br/>        ]<br/>    )<br/>    clf.fit(x_train, y_train)<br/>    y_test_pred_proba = clf.predict_proba(x_test)[:,1]<br/><br/>    return y_test, y_test_pred_proba</pre>
        <p>Now, we can use the <kbd>sample_and_predict()</kbd> function we have just created and plot the resulting ROC curve for the following two sampling techniques: </p>
        <pre>from sklearn.metrics import roc_curve, auc<br/>from imblearn.under_sampling import RandomUnderSampler<br/>from imblearn.under_sampling import EditedNearestNeighbours<br/><br/>fig, ax = plt.subplots(1, 1, figsize=(15, 8), sharey=False)<br/><br/># Original Data<br/><br/>y_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=None)<br/>plot_roc_curve(<br/>    y_test, y_test_pred_proba, <br/>    label='Original Data',<br/>    ax=ax<br/>) <br/><br/><br/># RandomUnderSampler<br/><br/>rus = RandomUnderSampler(random_state=42)<br/>y_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=rus)<br/><br/>plot_roc_curve(<br/>    y_test, y_test_pred_proba, <br/>    label='RandomUnderSampler',<br/>    ax=ax<br/>) <br/><br/># EditedNearestNeighbours<br/><br/>nc = EditedNearestNeighbours(n_neighbors=5)<br/>y_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=nc)<br/><br/>plot_roc_curve(<br/>    y_test, y_test_pred_proba, <br/>    label='EditedNearestNeighbours',<br/>    ax=ax<br/>) <br/><br/>ax.legend(ncol=1, fontsize='large', shadow=True)<br/><br/>fig.show()</pre>
        <p>The resulting ROC curve will look as follows:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/c08eccba-8880-4140-8e4a-1b22976d5968.png" style="width:47.67em;"/>
        </p>
        <p>Here, we can see the value of the sampling techniques on the resulting area under the ROC curve in comparison to training on the original unsampled set. The three graphs may be too close for us to tell them apart, as is the case here, so it makes sense to check the resulting AUC number instead.</p>
        <h2 id="uuid-59790aac-9bfd-4ce0-aaef-9ce47cc52e44">Oversampling the minority class</h2>
        <p>Besides undersampling, we can also increase the data points of the minority class. <kbd>RandomOverSampler</kbd> naively clones random samples of the minority class until it becomes the same size as the majority class. <kbd>SMOTE</kbd> and <kbd>ADASYN</kbd>, on the other hand, generate new synthetic samples by interpolation. </p>
        <p>Here, we are comparing <kbd>RandomOverSampler</kbd> to the<kbd>SMOTE</kbd> oversampling algorithm:</p>
        <pre>from sklearn.metrics import roc_curve, auc<br/>from imblearn.over_sampling import RandomOverSampler<br/>from imblearn.over_sampling import SMOTE<br/><br/>fig, ax = plt.subplots(1, 1, figsize=(15, 8), sharey=False)<br/><br/># RandomOverSampler<br/><br/>ros = RandomOverSampler(random_state=42)<br/>y_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=ros)<br/>plot_roc_curve(<br/>    y_test, y_test_pred_proba, <br/>    label='RandomOverSampler',<br/>    ax=ax<br/>)<br/><br/># SMOTE <br/><br/>smote = SMOTE(random_state=42)<br/>y_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=smote)<br/>plot_roc_curve(<br/>    y_test, y_test_pred_proba, <br/>    label='SMOTE',<br/>    ax=ax<br/>) <br/><br/>ax.legend(ncol=1, fontsize='large', shadow=True)<br/><br/>fig.show()</pre>
        <p>The resulting ROC curve helps us compare the performance of the two techniques being used on the dataset at hand:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/b2af8b59-d845-4ad6-b36e-4940766ef48d.png" style="width:53.00em;"/>
        </p>
        <p>As we can see, the <kbd>SMOTE</kbd>algorithm did not perform on our current dataset, while<kbd>RandomOverSampler</kbd>pushed the curve upward. So far, the classifiers we've used have been agnostic to the sampling techniques we've applied. We can simply remove the logistic regression classifier and plug in any other classifier here without changing the data sampling code. In contrast to the algorithms we've used, the data sampling process is an integral part of some ensemble algorithms. In the next section, we'll learn how to make use of this fact to get the best of both worlds.</p>
        <h2 id="uuid-c77947a5-7cde-4749-a79e-3dd8f0087652">Combining data sampling with ensembles </h2>
        <p>In <a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&amp;action=edit">Chapter 8</a>,<em> Ensembles – When One Model is Not Enough,</em> we learned about bagging algorithms. They basically allow multiple estimators to learn from different subsets of the dataset, in the hope that these diverse training subsets will allow the different estimators to come to a better decision when combined. Now that we've undersampled the majority class to keep our training data balanced, it is natural that we combine the two ideas together; that is, the bagging and the under-sampling techniques.</p>
        <p><kbd>BalancedBaggingClassifier</kbd> builds several estimators on different randomly selected subsets of data, where the classes are balanced during the sampling process. Similarly,<kbd>BalancedRandomForestClassifier</kbd>builds its trees on balanced samples. In the following code, we're plotting the ROC curves for the two ensembles: </p>
        <pre>from imblearn.ensemble import BalancedRandomForestClassifier<br/>from imblearn.ensemble import BalancedBaggingClassifier<br/><br/>fig, ax = plt.subplots(1, 1, figsize=(15, 8), sharey=False)<br/><br/># BalancedBaggingClassifier<br/><br/>clf = BalancedBaggingClassifier(n_estimators=500, n_jobs=-1, random_state=42)<br/>clf.fit(x_train, y_train)<br/>y_test_pred_proba = clf.predict_proba(x_test)[:,1]<br/><br/>plot_roc_curve(<br/>    y_test, y_test_pred_proba, <br/>    label='Balanced Bagging Classifier',<br/>    ax=ax<br/>) <br/><br/># BalancedRandomForestClassifier<br/><br/>clf = BalancedRandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)<br/>clf.fit(x_train, y_train)<br/>y_test_pred_proba = clf.predict_proba(x_test)[:,1]<br/><br/>plot_roc_curve(<br/>    y_test, y_test_pred_proba, <br/>    label='Balanced Random Forest Classifier',<br/>    ax=ax<br/>) <br/><br/>fig.show()</pre>
        <p>Some formatting lines have been omitted for brevity. Running the previous code gives us the following graph:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/c0e7e068-c2ee-49c2-b736-f2aeaa10a8f7.png" style="width:51.58em;"/>
        </p>
        <p>From this, it's clear that the combination of undersampling and ensembles achieved better results than our earlier models. </p>
        <p>In addition to the bagging algorithms, <kbd>RUSBoostClassifier</kbd><span class="n"/>combines the random undersampling technique with the <kbd>adaBoost</kbd>classifier. </p>
        <h1 id="uuid-d11ba62c-5b69-4fec-961d-2b16943d56db">Equal opportunity score</h1>
        <p class="mce-root">So far, we've only focused on the imbalances in the class labels. In some situations, the imbalance in a particular feature may also be problematic. Say, historically, that the vast majority of the engineers in your company were men. Now, if you build an algorithm to filter the new applicants based on your existing data, would it discriminate against the female candidates?</p>
        <p>The <strong>equal opportunity score</strong> tries to evaluate how dependent a model is of a certain feature. Simply put, a model is considered to give an equal opportunity to the different value of a certain feature if the relationship between the model's predictions and the actual targets is the same, regardless of the value of this feature. Formally, this means that the conditional probability of the predicted target, which is conditional on the actual target, and the applicant's gender should be the same, regardless of gender. These conditional probabilities are shown in the following equation:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/800db37b-c415-4c0e-90db-8bb5858ee163.png" style="width:34.25em;"/>
        </p>
        <p>The previous equation only gives a binary outcome. Therefore, we can turn it into a ratio where we have a value between 0 and 1. Since we do not know which gender gets a better opportunity, we take the minimum value of the two possible fractions using the following equation:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/692b5a92-ae04-4609-ba2c-3b6a99f50088.png" style="width:38.83em;"/>
        </p>
        <p>To demonstrate this metric, let's assume we have a model trained on the applicant's <kbd>IQ</kbd> and <kbd>Gender</kbd>. This following code shows its predictions on the test set, where both the true label and the predictions are listed side by side:</p>
        <pre>df_engineers = pd.DataFrame(<br/>    {<br/>        'IQ': [110, 120, 124, 123, 112, 114],<br/>        'Gender': ['M', 'F', 'M', 'F', 'M', 'F'],<br/>        'Is Hired? (True Label)': [0, 1, 1, 1, 1, 0],<br/>        'Is Hired? (Predicted Label)': [1, 0, 1, 1, 1, 0],<br/>    }<br/>)</pre>
        <p>Now, we can create a function to calculate the equal opportunity score for us, as follows:</p>
        <pre>def equal_opportunity_score(df, true_label, predicted_label, feature_name, feature_value):<br/>    opportunity_to_value = df[<br/>        (df[true_label] == 1) &amp; (df[feature_name] == feature_value)<br/>    ][predicted_label].mean() / df[<br/>        (df[true_label] == 1) &amp; (df[feature_name] != feature_value)<br/>    ][predicted_label].mean()<br/>    opportunity_to_other_values = 1 / opportunity_to_value<br/>    better_opportunity_to_value = opportunity_to_value &gt; opportunity_to_other_values<br/>    return {<br/>        'Score': min(opportunity_to_value, opportunity_to_other_values),<br/>        f'Better Opportunity to {feature_value}': better_opportunity_to_value<br/>    }</pre>
        <p>When called with our <kbd>df_engineers</kbd> DataFrame, it will give us <kbd>0.5</kbd>. Having a value that's less than one tells us that the female applicants have less of an opportunity to get hired by our model:</p>
        <pre>equal_opportunity_score(<br/>    df=df_engineers, <br/>    true_label='Is Hired? (True Label)', <br/>    predicted_label='Is Hired? (Predicted Label)', <br/>    feature_name='Gender',<br/>    feature_value='F'<br/>)</pre>
        <p>Obviously, we can exclude the gender feature from this model altogether, yet this score is still useful if there are any remaining features that depend on the applicant's gender. Additionally, we need to alter this score when dealing with a non-binary classifier and/or a non-binary feature. You can read about this score in more detail in the original paper by <em>Moritz Hardt</em><em>et al</em>. </p>
        <h1 id="uuid-7938a3e1-2d02-45c1-942a-22b70083c772">Summary</h1>
        <p>In this chapter, we learned how to deal with class imbalances. This is a recurrent problem in machine learning, where most of the value lies in the minority class. This phenomenon is common enough that the <em>black swan</em> metaphor was coined to explain it. When the machine learning algorithms try to blindly optimize their out-of-the-box objective functions, they usually miss those black swans. Hence, we have to use techniques such as sample weighting, sample removal, and sample generation to force the algorithms to meet our own objectives. </p>
        <p>This was the last chapter in this book about supervised learning algorithms. There is a rough estimate that 80% of the machine learning problems in business setups and academia are supervised learning ones, which is why about 80% of this book focused on that paradigm. From the next chapter onward, we will start covering the other machine learning paradigms, which is where about 20% of the real-life value resides. We will start by looking at clustering algorithms, and then move on and look at other problems where the data is also unlabeled.</p>
      </article>
    </section>
  </body></html>
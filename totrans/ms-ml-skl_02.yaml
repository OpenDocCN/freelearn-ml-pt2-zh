- en: Chapter 2. Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter you will learn how to use linear models in regression problems.
    First, we will examine simple linear regression, which models the relationship
    between a response variable and single explanatory variable. Next, we will discuss
    multiple linear regression, a generalization of simple linear regression that
    can support more than one explanatory variable. Then, we will discuss polynomial
    regression, a special case of multiple linear regression that can effectively
    model nonlinear relationships. Finally, we will discuss how to train our models
    by finding the values of their parameters that minimize a cost function. We will
    work through a toy problem to learn how the models and learning algorithms work
    before discussing an application with a larger dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter you learned that training data is used to estimate the
    parameters of a model in supervised learning problems. Past observations of explanatory
    variables and their corresponding response variables comprise the training data.
    The model can be used to predict the value of the response variable for values
    of the explanatory variable that have not been previously observed. Recall that
    the goal in regression problems is to predict the value of a continuous response
    variable. In this chapter, we will examine several example linear regression models.
    We will discuss the training data, model, learning algorithm, and evaluation metrics
    for each approach. To start, let's consider **simple linear regression**. Simple
    linear regression can be used to model a linear relationship between one response
    variable and one explanatory variable. Linear regression has been applied to many
    important scientific and social problems; the example that we will consider is
    probably not one of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you wish to know the price of a pizza. You might simply look at a menu.
    This, however, is a machine learning book, so we will use simple linear regression
    instead to predict the price of a pizza based on an attribute of the pizza that
    we can observe. Let''s model the relationship between the size of a pizza and
    its price. First, we will write a program with scikit-learn that can predict the
    price of a pizza given its size. Then, we will discuss how simple linear regression
    works and how it can be generalized to work with other types of problems. Let''s
    assume that you have recorded the diameters and prices of pizzas that you have
    previously eaten in your pizza journal. These observations comprise our training
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training instance | Diameter (in inches) | Price (in dollars) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 10 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 14 | 17.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 18 | 18 |'
  prefs: []
  type: TYPE_TB
- en: 'We can visualize our training data by plotting it on a graph using `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script produces the following graph. The diameters of the pizzas
    are plotted on the *x* axis and the prices are plotted on the *y* axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple linear regression](img/8365OS_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see from the graph of the training data that there is a positive relationship
    between the diameter of a pizza and its price, which should be corroborated by
    our own pizza-eating experience. As the diameter of a pizza increases, its price
    generally increases too. The following pizza-price predictor program models this
    relationship using linear regression. Let''s review the following program and
    discuss how linear regression works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Simple linear regression assumes that a linear relationship exists between the
    response variable and explanatory variable; it models this relationship with a
    linear surface called a hyperplane. A hyperplane is a subspace that has one dimension
    less than the ambient space that contains it. In simple linear regression, there
    is one dimension for the response variable and another dimension for the explanatory
    variable, making a total of two dimensions. The regression hyperplane therefore,
    has one dimension; a hyperplane with one dimension is a line.
  prefs: []
  type: TYPE_NORMAL
- en: The `sklearn.linear_model.LinearRegression` class is an **estimator**. Estimators
    predict a value based on the observed data. In scikit-learn, all estimators implement
    the `fit()` and `predict()` methods. The former method is used to learn the parameters
    of a model, and the latter method is used to predict the value of a response variable
    for an explanatory variable using the learned parameters. It is easy to experiment
    with different models using scikit-learn because all estimators implement the
    `fit` and `predict` methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fit` method of `LinearRegression` learns the parameters of the following
    model for simple linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple linear regression](img/8365OS_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Simple linear regression](img/8365OS_02_31.jpg) is the predicted value of
    the response variable; in this example, it is the predicted price of the pizza.
    ![Simple linear regression](img/8365OS_02_32.jpg) is the explanatory variable.
    The intercept term ![Simple linear regression](img/8365OS_02_33.jpg) and coefficient
    ![Simple linear regression](img/8365OS_02_34.jpg) are parameters of the model
    that are learned by the learning algorithm. The line plotted in the following
    figure models the relationship between the size of a pizza and its price. Using
    this model, we would expect the price of an 8-inch pizza to be about $7.33, and
    the price of a 20-inch pizza to be $18.75.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple linear regression](img/8365OS_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using training data to learn the values of the parameters for simple linear
    regression that produce the best fitting model is called **ordinary least squares**
    or **linear least** **squares**. "In this chapter we will discuss methods for
    approximating the values of the model's parameters and for solving them analytically.
    First, however, we must define what it means for a model to fit the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the fitness of a model with a cost function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression lines produced by several sets of parameter values are plotted in
    the following figure. How can we assess which parameters produced the best-fitting
    regression line?
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the fitness of a model with a cost function](img/8365OS_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A **cost** **function**, also called a **loss function,** is used to define
    and measure the error of a model. The differences between the prices predicted
    by the model and the observed prices of the pizzas in the training set are called
    **residuals** or **training errors**. Later, we will evaluate a model on a separate
    set of test data; the differences between the predicted and observed values in
    the test data are called **prediction** **errors** or **test errors**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The residuals for our model are indicated by the vertical lines between the
    points for the training instances and regression hyperplane in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the fitness of a model with a cost function](img/8365OS_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can produce the best pizza-price predictor by minimizing the sum of the
    residuals. That is, our model fits if the values it predicts for the response
    variable are close to the observed values for all of the training examples. This
    measure of the model''s fitness is called the **residual** **sum of squares**
    cost function. Formally, this function assesses the fitness of a model by summing
    the squared residuals for all of our training examples. The residual sum of squares
    is calculated with the formula in the following equation, where ![Evaluating the
    fitness of a model with a cost function](img/8365OS_02_35.jpg) is the observed
    value and ![Evaluating the fitness of a model with a cost function](img/8365OS_02_36.jpg)
    is the predicted value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the fitness of a model with a cost function](img/8365OS_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s compute the residual sum of squares for our model by adding the following
    two lines to the previous script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a cost function, we can find the values of our model's parameters
    that minimize it.
  prefs: []
  type: TYPE_NORMAL
- en: Solving ordinary least squares for simple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will work through solving ordinary least squares for simple
    linear regression. Recall that simple linear regression is given by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Also, recall that our goal is to solve the values of ![Solving ordinary least
    squares for simple linear regression](img/8365OS_02_34.jpg) and ![Solving ordinary
    least squares for simple linear regression](img/8365OS_02_33.jpg) that minimize
    the cost function. We will solve ![Solving ordinary least squares for simple linear
    regression](img/8365OS_02_34.jpg) first. To do so, we will calculate the **variance**
    of ![Solving ordinary least squares for simple linear regression](img/8365OS_02_32.jpg)
    and **covariance** of ![Solving ordinary least squares for simple linear regression](img/8365OS_02_32.jpg)
    and ![Solving ordinary least squares for simple linear regression](img/8365OS_02_31.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Variance is a measure of how far a set of values is spread out. If all of the
    numbers in the set are equal, the variance of the set is zero. A small variance
    indicates that the numbers are near the mean of the set, while a set containing
    numbers that are far from the mean and each other will have a large variance.
    Variance can be calculated using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, ![Solving ordinary least squares for simple linear
    regression](img/8365OS_02_37.jpg) is the mean of ![Solving ordinary least squares
    for simple linear regression](img/8365OS_02_32.jpg), ![Solving ordinary least
    squares for simple linear regression](img/8365OS_02_38.jpg) is the value of ![Solving
    ordinary least squares for simple linear regression](img/8365OS_02_32.jpg) for
    the ![Solving ordinary least squares for simple linear regression](img/8365OS_02_39.jpg)
    training instance, and ![Solving ordinary least squares for simple linear regression](img/8365OS_02_40.jpg)
    is the number of training instances. Let''s calculate the variance of the pizza
    diameters in our training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy also provides the `var` method to calculate variance. The `ddof` keyword
    parameter can be used to set Bessel''s correction to calculate the sample variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Covariance is a measure of how much two variables change together. If the value
    of the variables increase together, their covariance is positive. If one variable
    tends to increase while the other decreases, their covariance is negative. If
    there is no linear relationship between the two variables, their covariance will
    be equal to zero; the variables are linearly uncorrelated but not necessarily
    independent. Covariance can be calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As with variance, ![Solving ordinary least squares for simple linear regression](img/8365OS_02_38.jpg)
    is the diameter of the ![Solving ordinary least squares for simple linear regression](img/8365OS_02_39.jpg)
    training instance, ![Solving ordinary least squares for simple linear regression](img/8365OS_02_37.jpg)
    is the mean of the diameters, ![Solving ordinary least squares for simple linear
    regression](img/8365OS_02_41.jpg) is the mean of the prices, ![Solving ordinary
    least squares for simple linear regression](img/8365OS_02_35.jpg) is the price
    of the ![Solving ordinary least squares for simple linear regression](img/8365OS_02_39.jpg)
    training instance, and ![Solving ordinary least squares for simple linear regression](img/8365OS_02_40.jpg)
    is the number of training instances. Let''s calculate the covariance of the diameters
    and prices of the pizzas in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have calculated the variance of our explanatory variable and the
    covariance of the response and explanatory variables, we can solve ![Solving ordinary
    least squares for simple linear regression](img/8365OS_02_34.jpg) using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_09.jpg)![Solving
    ordinary least squares for simple linear regression](img/8365OS_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Having solved ![Solving ordinary least squares for simple linear regression](img/8365OS_02_34.jpg),
    we can solve ![Solving ordinary least squares for simple linear regression](img/8365OS_02_33.jpg)
    using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding formula, ![Solving ordinary least squares for simple linear
    regression](img/8365OS_02_41.jpg) is the mean of ![Solving ordinary least squares
    for simple linear regression](img/8365OS_02_31.jpg) and ![Solving ordinary least
    squares for simple linear regression](img/8365OS_02_37.jpg) is the mean of ![Solving
    ordinary least squares for simple linear regression](img/8365OS_02_32.jpg). ![Solving
    ordinary least squares for simple linear regression](img/8365OS_02_42.jpg) are
    the coordinates of the centroid, a point that the model must pass through. We
    can use the centroid and the value of ![Solving ordinary least squares for simple
    linear regression](img/8365OS_02_34.jpg) to solve for ![Solving ordinary least
    squares for simple linear regression](img/8365OS_02_33.jpg) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have solved the values of the model's parameters that minimize the
    cost function, we can plug in the diameters of the pizzas and predict their prices.
    For instance, an 11-inch pizza is expected to cost around $12.70, and an 18-inch
    pizza is expected to cost around $19.54\. Congratulations! You used simple linear
    regression to predict the price of a pizza.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have used a learning algorithm to estimate a model''s parameters from the
    training data. How can we assess whether our model is a good representation of
    the real relationship? Let''s assume that you have found another page in your
    pizza journal. We will use the entries on this page as a test set to measure the
    performance of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Test Instance | Diameter (in inches) | Observed price (in dollars) | Predicted
    price (in dollars) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 8 | 11 | 9.7759 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 9 | 8.5 | 10.7522 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 11 | 15 | 12.7048 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 16 | 18 | 17.5863 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 12 | 11 | 13.6811 |'
  prefs: []
  type: TYPE_TB
- en: Several measures can be used to assess our model's predictive capabilities.
    We will evaluate our pizza-price predictor using **r-squared**. R-squared measures
    how well the observed values of the response variables are predicted by the model.
    More concretely, r-squared is the proportion of the variance in the response variable
    that is explained by the model. An r-squared score of one indicates that the response
    variable can be predicted without any error using the model. An r-squared score
    of one half indicates that half of the variance in the response variable can be
    predicted using the model. There are several methods to calculate r-squared. In
    the case of simple linear regression, r-squared is equal to the square of the
    Pearson product moment correlation coefficient, or Pearson's *r*.
  prefs: []
  type: TYPE_NORMAL
- en: Using this method, r-squared must be a positive number between zero and one.
    This method is intuitive; if r-squared describes the proportion of variance in
    the response variable explained by the model, it cannot be greater than one or
    less than zero. Other methods, including the method used by scikit-learn, do not
    calculate r-squared as the square of Pearson's *r*, and can return a negative
    r-squared if the model performs extremely poorly. We will follow the method used
    by scikit-learn to calculate r-squared for our pizza-price predictor.
  prefs: []
  type: TYPE_NORMAL
- en: First, we must measure the total sum of the squares. ![Evaluating the model](img/8365OS_02_35.jpg)
    is the observed value of the response variable for the ![Evaluating the model](img/8365OS_02_39.jpg)
    test instance, and ![Evaluating the model](img/8365OS_02_41.jpg) is the mean of
    the observed values of the response variable
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the model](img/8365OS_02_13.jpg)![Evaluating the model](img/8365OS_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Next, we must find the residual sum of the squares. Recall that this is also
    our cost function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the model](img/8365OS_02_06.jpg)![Evaluating the model](img/8365OS_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we can find r-squared using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the model](img/8365OS_02_16.jpg)![Evaluating the model](img/8365OS_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An r-squared score of **0.6620** indicates that a large proportion of the variance
    in the test instances'' prices is explained by the model. Now, let''s confirm
    our calculation using scikit-learn. The `score` method of `LinearRegression` returns
    the model''s r-squared value, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Multiple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have trained and evaluated a model to predict the price of a pizza. While
    you are eager to demonstrate the pizza-price predictor to your friends and co-workers,
    you are concerned by the model's imperfect r-squared score and the embarrassment
    its predictions could cause you. How can we improve the model?
  prefs: []
  type: TYPE_NORMAL
- en: 'Recalling your personal pizza-eating experience, you might have some intuitions
    about the other attributes of a pizza that are related to its price. For instance,
    the price often depends on the number of toppings on the pizza. Fortunately, your
    pizza journal describes toppings in detail; let''s add the number of toppings
    to our training data as a second explanatory variable. We cannot proceed with
    simple linear regression, but we can use a generalization of simple linear regression
    that can use multiple explanatory variables called multiple linear regression.
    Formally, multiple linear regression is the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/8365OS_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: this edit makes no sense. change to "Where simple linear regression uses a single
    explanatory variable with a single coefficient, multiple linear regression uses
    a coefficient for each of an arbitrary number of explanatory variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/8365OS_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For simple linear regression, this is equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/8365OS_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Multiple linear regression](img/8365OS_02_43.jpg) is a column vector of the
    values of the response variables for the training examples. ![Multiple linear
    regression](img/8365OS_02_34.jpg) is a column vector of the values of the model''s
    parameters. ![Multiple linear regression](img/8365OS_02_44.jpg), called the design
    matrix, is an ![Multiple linear regression](img/8365OS_02_45.jpg) dimensional
    matrix of the values of the explanatory variables for the training examples. ![Multiple
    linear regression](img/8365OS_02_46.jpg) is the number of training examples and
    ![Multiple linear regression](img/8365OS_02_40.jpg) is the number of explanatory
    variables. Let''s update our pizza training data to include the number of toppings
    with the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training Example | Diameter (in inches) | Number of toppings | Price (in
    dollars) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6 | 2 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8 | 1 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 10 | 0 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 14 | 2 | 17.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 18 | 0 | 18 |'
  prefs: []
  type: TYPE_TB
- en: 'We must also update our test data to include the second explanatory variable,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Test Instance | Diameter (in inches) | Number of toppings | Price (in dollars)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 8 | 2 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 9 | 0 | 8.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 11 | 2 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 16 | 2 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 12 | 0 | 11 |'
  prefs: []
  type: TYPE_TB
- en: 'Our learning algorithm must estimate the values of three parameters: the coefficients
    for the two features and the intercept term. While one might be tempted to solve
    ![Multiple linear regression](img/8365OS_02_34.jpg) by dividing each side of the
    equation by ![Multiple linear regression](img/8365OS_02_44.jpg), division by a
    matrix is impossible. Just as dividing a number by an integer is equivalent to
    multiplying by the inverse of the same integer, we can multiply ![Multiple linear
    regression](img/8365OS_02_34.jpg) by the inverse of ![Multiple linear regression](img/8365OS_02_44.jpg)
    to avoid matrix division. Matrix inversion is denoted with a superscript -1\.
    Only square matrices can be inverted. ![Multiple linear regression](img/8365OS_02_44.jpg)
    is not likely to be a square; the number of training instances will have to be
    equal to the number of features for it to be so. We will multiply ![Multiple linear
    regression](img/8365OS_02_44.jpg) by its transpose to yield a square matrix that
    can be inverted. Denoted with a superscript ![Multiple linear regression](img/8365OS_02_47.jpg),
    the transpose of a matrix is formed by turning the rows of the matrix into columns
    and vice versa, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/8365OS_02_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To recap, our model is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/8365OS_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We know the values of ![Multiple linear regression](img/8365OS_02_43.jpg) and
    ![Multiple linear regression](img/8365OS_02_44.jpg) from our training data. We
    must find the values of ![Multiple linear regression](img/8365OS_02_34.jpg), which
    minimize the cost function. We can solve ![Multiple linear regression](img/8365OS_02_34.jpg)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/8365OS_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can solve ![Multiple linear regression](img/8365OS_02_34.jpg) using NumPy,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy also provides a least squares function that can solve the values of the
    parameters more compactly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s update our pizza-price predictor program to use the second explanatory
    variable, and compare its performance on the test set to that of the simple linear
    regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It appears that adding the number of toppings as an explanatory variable has
    improved the performance of our model. In later sections, we will discuss why
    evaluating a model on a single test set can provide inaccurate estimates of the
    model's performance, and how we can estimate its performance more accurately by
    training and testing on many partitions of the data. For now, however, we can
    accept that the multiple linear regression model performs significantly better
    than the simple linear regression model. There may be other attributes of pizzas
    that can be used to explain their prices. What if the relationship between these
    explanatory variables and the response variable is not linear in the real world?
    In the next section, we will examine a special case of multiple linear regression
    that can be used to model nonlinear relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous examples, we assumed that the real relationship between the
    explanatory variables and the response variable is linear. This assumption is
    not always true. In this section, we will use **polynomial regression**, a special
    case of multiple linear regression that adds terms with degrees greater than one
    to the model. The real-world curvilinear relationship is captured when you transform
    the training data by adding polynomial terms, which are then fit in the same manner
    as in multiple linear regression. For ease of visualization, we will again use
    only one explanatory variable, the pizza''s diameter. Let''s compare linear regression
    with polynomial regression using the following datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training Instance | Diameter (in inches) | Price (in dollars) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 10 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 14 | 17.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 18 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| Testing Instance | Diameter (in inches) | Price (in dollars) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 10 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 14 | 17.5 |'
  prefs: []
  type: TYPE_TB
- en: '**Quadratic regression**, or regression with a second order polynomial, is
    given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/8365OS_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We are using only one explanatory variable, but the model now has three terms
    instead of two. The explanatory variable has been transformed and added as a third
    term to the model to capture the curvilinear relationship. Also, note that the
    equation for polynomial regression is the same as the equation for multiple linear
    regression in vector notation. The `PolynomialFeatures` transformer can be used
    to easily add polynomial features to a feature representation. Let''s fit a model
    to these features, and compare it to the simple linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The simple linear regression model is plotted with the solid line in the following
    figure. Plotted with a dashed line, the quadratic regression model visibly fits
    the training data better.
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/8365OS_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The r-squared score of the simple linear regression model is 0.81; the quadratic
    regression model''s r-squared score is an improvement at 0.87\. While quadratic
    and cubic regression models are the most common, we can add polynomials of any
    degree. The following figure plots the quadratic and cubic models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/8365OS_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s try an even higher-order polynomial. The plot in the following
    figure shows a regression curve created by a ninth-degree polynomial:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial regression](img/8365OS_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ninth-degree polynomial regression model fits the training data almost exactly!
    The model's r-squared score, however, is -0.09\. We created an extremely complex
    model that fits the training data exactly, but fails to approximate the real relationship.
    This problem is called **over-fitting**. The model should induce a general rule
    to map inputs to outputs; instead, it has memorized the inputs and outputs from
    the training data. As a result, the model performs poorly on test data. It predicts
    that a 16 inch pizza should cost less than $10, and an 18 inch pizza should cost
    more than $30\. This model exactly fits the training data, but fails to learn
    the real relationship between size and price.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Regularization** is a collection of techniques that can be used to prevent
    over-fitting. Regularization adds information to a problem, often in the form
    of a penalty against complexity, to a problem. Occam''s razor states that a hypothesis
    with the fewest assumptions is the best. Accordingly, regularization attempts
    to find the simplest model that explains the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn provides several regularized linear regression models. **Ridge
    regression**, also known as **Tikhonov regularization**, penalizes model parameters
    that become too large. Ridge regression modifies the residual sum of the squares
    cost function by adding the L2 norm of the coefficients, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization](img/8365OS_02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Regularization](img/8365OS_02_48.jpg) is a hyperparameter that controls the
    strength of the penalty. **Hyperparameters** are parameters of the model that
    are not learned automatically and must be set manually. As ![Regularization](img/8365OS_02_48.jpg)
    increases, the penalty increases, and the value of the cost function increases.
    When ![Regularization](img/8365OS_02_48.jpg) is equal to zero, ridge regression
    is equal to linear regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn also provides an implementation of the **Least Absolute Shrinkage
    and Selection Operator** (**LASSO**). LASSO penalizes the coefficients by adding
    their L1 norm to the cost function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization](img/8365OS_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The LASSO produces sparse parameters; most of the coefficients will become zero,
    and the model will depend on a small subset of the features. In contrast, ridge
    regression produces models in which most parameters are small but nonzero. When
    explanatory variables are correlated, the LASSO will shrink the coefficients of
    one variable toward zero. Ridge regression will shrink them more uniformly. Finally,
    scikit-learn provides an implementation of **elastic net** regularization, which
    linearly combines the L1 and L2 penalties used by the LASSO and ridge regression.
    That is, the LASSO and ridge regression are both special cases of the elastic
    net method in which the hyperparameter for either the L1 or L2 penalty is equal
    to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Applying linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have worked through a toy problem to learn how linear regression models relationships
    between explanatory and response variables. Now we'll use a real data set and
    apply linear regression to an important task. Assume that you are at a party,
    and that you wish to drink the best wine that is available. You could ask your
    friends for recommendations, but you suspect that they will drink any wine, regardless
    of its provenance. Fortunately, you have brought pH test strips and other tools
    to measure various physicochemical properties of wine—it is, after all, a party.
    We will use machine learning to predict the quality of the wine based on its physicochemical
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: The UCI Machine Learning Repository's Wine data set measures eleven physicochemical
    attributes, including the pH and alcohol content, of 1,599 different red wines.
    Each wine's quality has been scored by human judges. The scores range from zero
    to ten; zero is the worst quality and ten is the best quality. The data set can
    be downloaded from [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine).
    We will approach this problem as a regression task and regress the wine's quality
    onto one or more physicochemical attributes. The response variable in this problem
    takes only integer values between 0 and 10; we could view these as discrete values
    and approach the problem as a multiclass classification task. In this chapter,
    however, we will view the response variable as a continuous value.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Fixed acidity | Volatile acidity | Citric acidity | Residual sugar | Chlorides
    | Free sulfur dioxide | Total sulfur dioxide | Density | pH | Sulphates | Alcohol
    | Quality |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 7.4 | 0.7 | 0 | 1.9 | 0.076 | 11 | 34 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 7.8 | 0.88 | 0 | 2.6 | 0.098 | 25 | 67 | 0.9968 | 3.2 | 0.68 | 9.8 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15 | 54 | 0.997 | 3.26 | 0.65 | 9.8 | 5
    |'
  prefs: []
  type: TYPE_TB
- en: '| 11.2 | 0.28 | 0.56 | 1.9 | 0.075 | 17 | 60 | 0.998 | 3.16 | 0.58 | 9.8 |
    6 |'
  prefs: []
  type: TYPE_TB
- en: scikit-learn is intended to be a tool to build machine learning systems; its
    capabilities to explore data are impoverished compared to those of packages such
    as SPSS Statistics or the R language. We will use pandas, an open source data
    analysis library for Python, to generate descriptive statistics from the data;
    we will use these statistics to inform some of the design decisions of our model.
    pandas introduces Python to some concepts from R such as the dataframe, a two-dimensional,
    tabular, and heterogeneous data structure. Using pandas for data analysis is the
    topic of several books; we will use only a few basic methods in the following
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load the data set and review some basic summary statistics for
    the variables. The data is provided as a `.csv` file. Note that the fields are
    separated by semicolons rather than commas):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pd.read_csv()` function is a convenience utility that loads the `.csv`
    file into a dataframe. The `Dataframe.describe()` method calculates summary statistics
    for each column of the dataframe. The preceding code sample shows the summary
    statistics for only the last four columns of the dataframe. Note the summary for
    the quality variable; most of the wines scored five or six. Visualizing the data
    can help indicate if relationships exist between the response variable and the
    explanatory variables. Let''s use `matplotlib` to create some scatter plots. Consider
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the data](img/8365OS_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A weak positive relationship between the alcohol content and quality is visible
    in the scatter plot in the preceding figure; wines that have high alcohol content
    are often high in quality. The following figure reveals a negative relationship
    between volatile acidity and quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the data](img/8365OS_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These plots suggest that the response variable depends on multiple explanatory
    variables; let's model the relationship with multiple linear regression. How can
    we decide which explanatory variables to include in the model? `Dataframe.corr()`
    calculates a pairwise correlation matrix. The correlation matrix confirms that
    the strongest positive correlation is between the alcohol and quality, and that
    quality is negatively correlated with volatile acidity, an attribute that can
    cause wine to taste like vinegar. To summarize, we have hypothesized that good
    wines have high alcohol content and do not taste like vinegar. This hypothesis
    seems sensible, though it suggests that wine aficionados may have less sophisticated
    palates than they claim.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting and evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will split the data into training and testing sets, train the regressor,
    and evaluate its predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: First, we loaded the data using pandas and separated the response variable from
    the explanatory variables. Next, we used the `train_test_split` function to randomly
    partition the data into training and test sets. The proportions of the data for
    both partitions can be specified using keyword arguments. By default, 25 percent
    of the data is assigned to the test set. Finally, we trained the model and evaluated
    it on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The r-squared score of 0.35 indicates that 35 percent of the variance in the
    test set is explained by the model. The performance might change if a different
    75 percent of the data is partitioned to the training set. We can use cross-validation
    to produce a better estimate of the estimator''s performance. Recall from chapter
    one that each cross-validation round trains and tests different partitions of
    the data to reduce variability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `cross_val_score` helper function allows us to easily perform cross-validation
    using the provided data and estimator. We specified a five-fold cross validation
    using the `cv` keyword argument, that is, each instance will be randomly assigned
    to one of the five partitions. Each partition will be used to train and test the
    model. `cross_val_score` returns the value of the estimator's `score` method for
    each round. The r-squared scores range from 0.13 to 0.36! The mean of the scores,
    0.29, is a better estimate of the estimator's predictive power than the r-squared
    score produced from a single train / test split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s inspect some of the model''s predictions and plot the true quality scores
    against the predicted scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fitting and evaluating the model](img/8365OS_02_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, few predictions exactly match the true values of the response variable.
    The model is also better at predicting the qualities of average wines, since most
    of the training data is for average wines.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting models with gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the examples in this chapter, we analytically solved the values of the model''s
    parameters that minimize the cost function with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fitting models with gradient descent](img/8365OS_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Recall that ![Fitting models with gradient descent](img/8365OS_02_44.jpg) is
    the matrix of the values of the explanatory variables for each training example.
    The dot product of ![Fitting models with gradient descent](img/8365OS_02_49.jpg)
    results in a square matrix with dimensions ![Fitting models with gradient descent](img/8365OS_02_50.jpg),
    where ![Fitting models with gradient descent](img/8365OS_02_40.jpg) is equal to
    the number of explanatory variables. The computational complexity of inverting
    this square matrix is nearly cubic in the number of explanatory variables. While
    the number of explanatory variables has been small in this chapter's examples,
    this inversion can be prohibitively costly for problems with tens of thousands
    of explanatory variables, which we will encounter in the following chapters. Furthermore,
    ![Fitting models with gradient descent](img/8365OS_02_49.jpg) cannot be inverted
    if its determinant is equal to zero. In this section, we will discuss another
    method to efficiently estimate the optimal values of the model's parameters called
    **gradient descent**. Note that our definition of a good fit has not changed;
    we will still use gradient descent to estimate the values of the model's parameters
    that minimize the value of the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is sometimes described by the analogy of a blindfolded man
    who is trying to find his way from somewhere on a mountainside to the lowest point
    of the valley. He cannot see the topography, so he takes a step in the direction
    with the steepest decline. He then takes another step, again in the direction
    with the steepest decline. The sizes of his steps are proportional to the steepness
    of the terrain at his current position. He takes big steps when the terrain is
    steep, as he is confident that he is still near the peak and that he will not
    overshoot the valley's lowest point. The man takes smaller steps as the terrain
    becomes less steep. If he were to continue taking large steps, he may accidentally
    step over the valley's lowest point. He would then need to change direction and
    step toward the lowest point of the valley again. By taking decreasingly large
    steps, he can avoid stepping back and forth over the valley's lowest point. The
    blindfolded man continues to walk until he cannot take a step that will decrease
    his altitude; at this point, he has found the bottom of the valley.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, gradient descent is an optimization algorithm that can be used to
    estimate the local minimum of a function. Recall that we are using the residual
    sum of squares cost function, which is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fitting models with gradient descent](img/8365OS_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can use gradient descent to find the values of the model's parameters that
    minimize the value of the cost function. Gradient descent iteratively updates
    the values of the model's parameters by calculating the partial derivative of
    the cost function at each step. The calculus required to compute the partial derivative
    of the cost function is beyond the scope of this book, and is also not required
    to work with scikit-learn. However, having an intuition for how gradient descent
    works can help you use it effectively.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that gradient descent estimates the local minimum of
    a function. A three-dimensional plot of the values of a convex cost function for
    all possible values of the parameters looks like a bowl. The bottom of the bowl
    is the sole local minimum. Non-convex cost functions can have many local minima,
    that is, the plots of the values of their cost functions can have many peaks and
    valleys. Gradient descent is only guaranteed to find the local minimum; it will
    find a valley, but will not necessarily find the lowest valley. Fortunately, the
    residual sum of the squares cost function is convex.
  prefs: []
  type: TYPE_NORMAL
- en: An important hyperparameter of gradient descent is the learning rate, which
    controls the size of the blindfolded man's steps. If the learning rate is small
    enough, the cost function will decrease with each iteration until gradient descent
    has converged on the optimal parameters. As the learning rate decreases, however,
    the time required for gradient descent to converge increases; the blindfolded
    man will take longer to reach the valley if he takes small steps than if he takes
    large steps. If the learning rate is too large, the man may repeatedly overstep
    the bottom of the valley, that is, gradient descent could oscillate around the
    optimal values of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: There are two varieties of gradient descent that are distinguished by the number
    of training instances that are used to update the model parameters in each training
    iteration. **Batch gradient descent**, which is sometimes called only gradient
    descent, uses all of the training instances to update the model parameters in
    each iteration. **Stochastic Gradient Descent** (**SGD**), in contrast, updates
    the parameters using only a single training instance in each iteration. The training
    instance is usually selected randomly. Stochastic gradient descent is often preferred
    to optimize cost functions when there are hundreds of thousands of training instances
    or more, as it will converge more quickly than batch gradient descent. Batch gradient
    descent is a deterministic algorithm, and will produce the same parameter values
    given the same training set. As a stochastic algorithm, SGD can produce different
    parameter estimates each time it is run. SGD may not minimize the cost function
    as well as gradient descent because it uses only single training instances to
    update the weights. Its approximation is often close enough, particularly for
    convex cost functions such as residual sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use stochastic gradient descent to estimate the parameters of a model
    with scikit-learn. `SGDRegressor` is an implementation of SGD that can be used
    even for regression problems with hundreds of thousands or more features. It can
    be used to optimize different cost functions to fit different linear models; by
    default, it will optimize the residual sum of squares. In this example, we will
    predict the prices of houses in the Boston Housing data set from 13 explanatory
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'scikit-learn provides a convenience function for loading the data set. First,
    we split the data into training and testing sets using `train_test_split`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we scaled the features using `StandardScaler`, which we will describe
    in detail in the next chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we trained the estimator, and evaluated it using cross validation
    and the test set. The following is the output of the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we discussed three cases of linear regression. We worked through
    an example of simple linear regression, which models the relationship between
    a single explanatory variable and a response variable using a line. We then discussed
    multiple linear regression, which generalizes simple linear regression to model
    the relationship between multiple explanatory variables and a response variable.
    Finally, we described polynomial regression, a special case of multiple linear
    regression that models non-linear relationships between explanatory variables
    and a response variable. These three models can be viewed as special cases of
    the generalized linear model, a framework for model linear relationships, which
    we will discuss in more detail in [Chapter 4](ch04.html "Chapter 4. From Linear
    Regression to Logistic Regression"), *From Linear Regression to Logistic Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: We assessed the fitness of models using the residual sum of squares cost function
    and discussed two methods to learn the values of a model's parameters that minimize
    the cost function. First, we solved the values of the model's parameters analytically.
    We then discussed gradient descent, a method that can efficiently estimate the
    optimal values of the model's parameters even when the model has a large number
    of features. The features in this chapter's examples were simple measurements
    of their explanatory variables; it was easy to use them in our models. In the
    next chapter, you will learn to create features for different types of explanatory
    variables, including categorical variables, text, and images.
  prefs: []
  type: TYPE_NORMAL

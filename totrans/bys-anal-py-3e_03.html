<html><head></head><body>
<section id="chapter-4-modeling-with-lines" class="level2 chapterHead" data-number="1.8">&#13;
<h1 class="chapterHead" data-number="1.8">Chapter 4<br/>&#13;
<span id="x1-760004"/>Modeling with Lines</h1>&#13;
<blockquote>&#13;
<p>In more than three centuries of science everything has changed except perhaps one thing: the love for the simple. – Jorge Wagensberg</p>&#13;
</blockquote>&#13;
<p>Music—from classical compositions to <em>Sheena is a Punk Rocker</em> by The Ramones, passing through unrecognized hits from garage bands and Piazzolla’s Libertango—is made of recurring patterns. The same scales, combinations of chords, riffs, motifs, and so on appear over and over again, giving rise to a wonderful sonic landscape capable of eliciting and modulating the entire range of emotions that humans can experience. Similarly, the universe of statistics is built upon recurring patterns, small motifs that appear now and again. In this chapter, we are going to look at one of the most popular and useful of them, the <strong>linear</strong> <strong>model</strong> (or motif, if you want). This is a very useful model on its own and also the building block of many other models. If you’ve ever taken a statistics course, you may have heard of simple and multiple linear regression, logistic regression, ANOVA, ANCOVA, and so on. All these methods are variations of the same underlying motif, the linear regression model.</p>&#13;
<p>In this chapter, we will cover the following topics:</p>&#13;
<ul>&#13;
<li><p>Simple linear regression</p></li>&#13;
<li><p>NegativeBinomial regression</p></li>&#13;
<li><p>Robust regression</p></li>&#13;
<li><p>Logistic regression</p></li>&#13;
<li><p>Variable variance</p></li>&#13;
<li><p>Hierarchical linear regression</p></li>&#13;
<li><p>Multiple linear regression</p></li>&#13;
</ul>&#13;
<p><span id="x1-76001r165"/></p>&#13;
<section id="simple-linear-regression" class="level3 sectionHead" data-number="1.8.1">&#13;
<h2 class="sectionHead" data-number="1.8.1">4.1 <span id="x1-770001"/>Simple linear regression</h2>&#13;
<p><span id="dx1-77001"/></p>&#13;
<p>Many problems we find in science, engineering, and business are of the following form. We have a variable <em>X</em> and we want to model or predict a variable <em>Y</em> . Importantly, these variables are paired like <span class="cmsy-10x-x-109">{</span>(<em>x</em><sub>1</sub><em>,y</em><sub>1</sub>)<em>,</em>(<em>x</em><sub>2</sub><em>,y</em><sub>2</sub>)<em>,</em><img src="../media/file96.jpg" class="@cdots" alt="⋅⋅⋅"/><em>,</em>(<em>x</em><sub><em>n</em></sub><em>,y</em><sub><em>n</em></sub>)<span class="cmsy-10x-x-109">}</span>. In the most simple scenario, known as simple linear regression, both <em>X</em> and <em>Y</em> are uni-dimensional continuous random variables. By continuous, we mean a variable represented using real numbers. Using NumPy, you will represent these variables as one-dimensional arrays of floats. Usually, people call <em>Y</em> the dependent, predicted, or outcome variable, and <em>X</em> the independent, predictor, or input variable.</p>&#13;
<p>Some typical situations where linear <span id="dx1-77002"/>regression models can be used are the following:</p>&#13;
<ul>&#13;
<li><p>Model the relationship between soil salinity and crop productivity. Then, answer questions such as: is the relationship linear? How strong is this relationship?</p></li>&#13;
<li><p>Find a relationship between average chocolate consumption by country and the number of Nobel laureates in that country, and then understand why this relationship could be spurious.</p></li>&#13;
<li><p>Predict the gas bill (used for heating and cooking) of your house by using the solar radiation from the local weather report. How accurate is this prediction?</p></li>&#13;
</ul>&#13;
<p>In <em>Chapter <a href="CH02.xhtml#x1-440002">2</a></em>, we saw the Normal model, which we define as:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file97.jpg" class="math-display" alt=" μ ∼ some prior σ ∼ some other prior Y ∼ 𝒩 (μ,σ) "/>&#13;
</div>&#13;
<p>The main idea of linear regression is to extend this model by adding a predictor variable <em>X</em> to the estimation of the mean <em>μ</em>:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file98.jpg" class="math-display" alt=" 𝛼 ∼ a prior 𝛽 ∼ another prior σ ∼ some other prior μ = 𝛼 + 𝛽X Y ∼ 𝒩 (μ,σ) "/>&#13;
</div>&#13;
<p>This model says that there is a linear relation between the variable <em>X</em> and the variable <em>Y</em> . But that relationship is not deterministic, because of the noise term <em>σ</em>. Additionally, the model says that the mean of <em>Y</em> is a linear function of <em>X</em>, with <strong>intercept</strong> <em>α</em> and <strong>slope</strong> <em>β</em>. The intercept tells us the value of <em>Y</em> when <em>X</em> = 0 and the slope tells us the change in <em>Y</em> per unit change in <em>X</em>. Because we don’t know the values of <em>α</em>, <em>β</em>, or <em>σ</em> we set priors distribution over them.</p>&#13;
<p>When setting priors for linear models we typically assume that they are independent. This assumption greatly simplifies setting priors because we then need to set three priors instead of one joint prior. At least in principle, <em>α</em> and <em>β</em> can take any value on the real line, thus it is common to use Normal priors for them. And because <em>σ</em> is a positive number, it is common to use a HalfNormal or Exponential prior for it.</p>&#13;
<p>The values the intercept can take can vary a lot from one problem to another and for different domain knowledge. For many problems I have worked on, <em>α</em> is usually centered around 0 and with a standard deviation no larger than 1, but this is just my experience (almost anecdotal) with a small subset of problems and not something easy to transfer to other problems. Usually, it may be easier to have an informed guess for the slope (<em>β</em>). For instance, we may know the sign of the slope a priori; for example, we expect the variable weight to increase, on average, with the variable height. For <em>σ</em>, we can set it to a large value on the scale of the variable <em>Y</em> , for example, two times the value for its standard deviation. We should be careful of using the observed data to guesstimate priors; usually, it is fine if the data is used to avoid using very restrictive priors. If we don’t have too much knowledge of the parameter, it makes sense to ensure our prior is vague. If we instead want more informative priors, then we should not get that information from the observed data; instead, we should get it from our domain knowledge.</p>&#13;
<div id="tcolobox-9" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Extending the Normal Model</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>A linear regression model is an extension of the Normal model where the mean is computed as a linear function of a predictor variable.</p>&#13;
</div>&#13;
</div>&#13;
<p><span id="x1-77003r168"/></p>&#13;
</section>&#13;
<section id="linear-bikes" class="level3 sectionHead" data-number="1.8.2">&#13;
<h2 class="sectionHead" data-number="1.8.2">4.2 <span id="x1-780002"/>Linear bikes</h2>&#13;
<p><span id="dx1-78001"/></p>&#13;
<p>We now have a general idea of what Bayesian linear models look like. Let’s try to cement that idea with an example. We are going to start very simply; we have a record of temperatures and the number of bikes rented in a city. We want to model the relationship between the temperature and the number of bikes rented. <em>Figure <a href="#x1-78002r1">4.1</a></em> shows a scatter plot of these two variables from the bike-sharing dataset from the UCI Machine Learning Repository ( <a href="https://archive.ics.uci.edu/ml/index.php" class="url">https://archive.ics.uci.edu/ml/index.php</a>).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file99.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-78002r1"/><strong>Figure 4.1</strong>: Bike-sharing dataset. Scatter plot of temperature in Celcius vs. number of rented bikes</p>&#13;
<p>The original dataset contains 17,379 records, and each record has 17 variables. We will only use 359 records and two variables, <code>temperature</code> (Celcius) <code>rented </code>(number of rented bikes). We are going to use<code>temperature</code> as our independent variable (our X) and the number of bikes rented as our dependent variable (our Y). We are going to use the following model:</p>&#13;
<p><span id="x1-78003r1"/> <span id="x1-78004"/><strong>Code 4.1</strong></p>&#13;
<pre id="listing-43" class="source-code"><code>with pm.Model() as model_lb: </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", mu=0, sigma=100) </code>&#13;
<code>    <em>β</em> = pm.Normal("<em>β</em>", mu=0, sigma=10) </code>&#13;
<code>    σ = pm.HalfCauchy("σ", 10) </code>&#13;
<code>    μ = pm.Deterministic("μ", <em>α</em> + <em>β</em> * bikes.temperature) </code>&#13;
<code>    y_pred = pm.Normal("y_pred", mu=μ, sigma=σ, observed=bikes.rented) </code>&#13;
<code>    idata_lb = pm.sample()</code></pre>&#13;
<p>Take a moment to read the code line by line and be sure to understand what is going on. Also check <em>Figure <a href="#x1-78012r2">4.2</a></em> for a visual representation of this model.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file100.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-78012r2"/><strong>Figure 4.2</strong>: Bayesian linear model for the bike-sharing dataset</p>&#13;
<p>As we have previously said, this is like a Normal model, but now the mean is modeled as a linear function of the temperature. The intercept is <em>α</em> and the slope is <em>β</em>. The noise term is <img src="../media/e.png" style="width:0.75em; vertical-align: -0.10em;"/> and the mean is <em>μ</em>. The only new thing here is the <code>Deterministic </code>variable <em>μ</em>. This variable is not a random variable, it is a deterministic variable, and it is computed from the intercept, the slope, and the temperature. We need to specify this variable because we want to save it in InferenceData for later use. We could have just written <em>μ</em> <code>= </code><em>α</em> <code>+ </code><em>β</em> <code>* bikes.temperature </code>or even <code>_ = pm.Normal(’y_pred’, mu=</code><em>α</em> <code>+ </code><em>β</em> <code>* bikes.temperature, ... </code>and the model will be the same, but we would not have been able to save <em>μ</em> in InferenceData. Notice that <em>μ</em> is a vector with the same length as <code>bikes.temperature</code>, which is the same as the number of records in the dataset. <span id="x1-78013r143"/></p>&#13;
<section id="interpreting-the-posterior-mean" class="level4 subsectionHead" data-number="1.8.2.1">&#13;
<h3 class="subsectionHead" data-number="1.8.2.1">4.2.1 <span id="x1-790001"/>Interpreting the posterior mean</h3>&#13;
<p><span id="dx1-79001"/></p>&#13;
<p>To explore the results of our inference, we are going to generate a posterior plot but omit the deterministic variable <em>μ</em>. We commit it because otherwise, we would get a lot of plots, one for each value of <code>temperature</code>. We can do this by passing the names of the variables we want to include in the plot as a list to the <code>var_names </code>argument or we can negate the variable that we want to exclude as in the following block of code:</p>&#13;
<p><span id="x1-79002r2"/> <span id="x1-79003"/><strong>Code 4.2</strong></p>&#13;
<pre id="listing-44" class="source-code"><code>az.plot_posterior(idata_lb, var_names=['∼μ'])</code></pre>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file101.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-79005r3"/><strong>Figure 4.3</strong>: Posterior plot for the bike linear model</p>&#13;
<p>From <em>Figure <a href="#x1-79005r3">4.3</a></em>, we can see the marginal posterior distribution for <em>α</em>, <em>β</em>, and <em>σ</em>. If we only read the means of each distribution, say <em>μ</em> = 69 + 7<em>.</em>9<em>X</em>, with this information we can say that the expected value of rented bikes when the temperature is 0 is 69, and for each degree of temperature the number of rented bikes increases by 7.9. So for a temperature of 28 degrees, we expect to rent 69 + 7<em>.</em>9 <span class="cmsy-10x-x-109">∗ </span>28 <span class="cmsy-10x-x-109">≈ </span>278 bikes. This is our expectation, but the posterior also informs us about the uncertainty around this estimate. For instance, the 94% HDI for <em>β</em> is (6.1, 9.7), so for each degree of temperature the number of rented bikes could increase from 6 to about 10. Also even if we omit the posterior uncertainty and we only pay attention to the means, we still have uncertainty about the number of rented bikes because we have a value of <em>σ</em> of 170. So if we say that for a temperature of 28 degrees, we expect to rent 278 bikes, we should not be <span id="dx1-79006"/>surprised if the actual number turns out to be somewhere between 100 and 500 bikes.</p>&#13;
<p>Now let’s create a few plots that will help us visualize the combined uncertainty of these parameters. Let’s start with two plots for the mean (see <em>Figure <a href="#x1-79007r4">4.4</a></em>). Both are plots of the mean number of rented bikes as a function of the temperature. The difference is how we represent the uncertainty. We show two popular ways of doing it. In the left subpanel, we take 50 samples from the posterior and plot them as individual lines. In the right subpanel, we instead take all the available posterior samples for <em>μ</em> and use them to compute the 94% HDI.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file102.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-79007r4"/><strong>Figure 4.4</strong>: Posterior plot for the bike linear model</p>&#13;
<p>The plots in <em>Figure <a href="#x1-79007r4">4.4</a></em> convey essentially the same information, but one represents uncertainty as a set of lines and the other as a shaded area. Notice that if you repeat the code to generate the plot, you will get different lines, because we are sampling from the posterior. The shaded area, however, will be the same, because we are using all the available posterior samples. If we go further and refit the model, we will not only get different lines but the shaded area could also change, and probably the difference between runs is going to be very small; if not, you probably need to increase the number of draws, or there is something funny about your model and sampling (see <em>Chapter <a href="CH10.xhtml#x1-18900010">10</a></em> for guidance).</p>&#13;
<p>Anyway, why are we showing two slightly different plots if they convey the same information? Well, to highlight that there are different ways to <span id="dx1-79008"/>represent uncertainty. Which one is better? As usual, that is context-dependent. The shaded area is a good option; it is very common, and it is simple to compute and interpret. Unless there are specific reasons to show individual posterior samples, the shaded area may be your preferred choice. But we may want to show individual posterior samples. For instance, most of the lines might span a certain region, but we get a few with very high slopes. A shaded area could hide that information. When showing individual samples from the posterior it may be a good idea to animate them if you are showing them in a presentation or a video (see <a href="Bibliography.xhtml#Xkale_2018">Kale et al.</a> [<a href="Bibliography.xhtml#Xkale_2018">2019</a>] for more on this).</p>&#13;
<p>Another reason to show you the two plots in <em>Figure <a href="#x1-79007r4">4.4</a></em> is that you can learn different ways of extracting information from the posterior. Please pay attention to the next block of code. For clarity, we have omitted the code for plotting and we only show the core computations:</p>&#13;
<p><span id="x1-79009r3"/> <span id="x1-79010"/><strong>Code 4.3</strong></p>&#13;
<pre id="listing-45" class="source-code"><code>posterior = az.extract(idata_lb, num_samples=50) </code>&#13;
<code>x_plot = xr.DataArray( </code>&#13;
<code>    np.linspace(bikes.temperature.min(), bikes.temperature.max(), 50), </code>&#13;
<code>    dims="plot_id" </code>&#13;
<code>) </code>&#13;
<code>mean_line = posterior["<em>α</em>"].mean() + posterior["<em>β</em>"].mean() * x_plot </code>&#13;
<code>lines = posterior["<em>α</em>"] + posterior["<em>β</em>"] * x_plot </code>&#13;
<code>hdi_lines = az.hdi(idata_lb.posterior["μ"]) </code>&#13;
<code>...</code></pre>&#13;
<p>You can see that in the first line, we used <code>az.extract</code>. This function takes the <code>chain </code>and <code>draw </code>dimensions and stacks them in a single <code>sample </code>dimension, which can be useful for later processing. Additionally, we use the <code>num_samples</code> argument to ask for a subsample from the posterior. By default, <code>az.extract </code>will operate on the posterior group. If you want to extract information from another group, you can use the <code>group </code>argument. On the second line, we define a DataArray called <code>x_plot</code>, with equally spaced values ranging from the minimum to the maximum observed temperatures. The reason to create a DataArray is to be able to use Xarray’s automatic alignment capabilities in the next two lines. If we use a NumPy array, we will need to add extra dimensions, which is usually confusing. The best way to fully understand what I mean is to define <code>x_plot = np.linspace(bikes.temperature.min(), bikes.temperature.max()) </code>and try to redo the plot. In the third line of code, we compute the mean of the posterior for <em>μ</em> for <span id="dx1-79020"/>each value of <code>x_plot</code>, and in the fourth line, we compute individual values for <em>μ</em>. In these two lines we could have used <code>posterior[’</code><em>μ</em><code>’]</code>, but instead, we explicitly rewrite the linear model. We do this with the hope that it will help you to gain more intuition about linear models. <span id="x1-79021r172"/></p>&#13;
</section>&#13;
<section id="interpreting-the-posterior-predictions" class="level4 subsectionHead" data-number="1.8.2.2">&#13;
<h3 class="subsectionHead" data-number="1.8.2.2">4.2.2 <span id="x1-800002"/>Interpreting the posterior predictions</h3>&#13;
<p>What if we are not just interested in the <span id="dx1-80001"/>expected (mean) value, but we want to think in terms of predictions, that is, in terms of rented bikes? Well, for that, we can do posterior predictive sampling. After executing the next line of code, <code>idata_lb </code>will be populated with a new group, <code>posterior_predictive</code>, with a variable, <code>y_pred</code>, representing the posterior predictive distribution for the number of rented bikes.</p>&#13;
<p><span id="x1-80002r4"/> <span id="x1-80003"/><strong>Code 4.4</strong></p>&#13;
<pre id="listing-46" class="source-code"><code>pm.sample_posterior_predictive(idata_lb, model=model_lb, extend_inferencedata=True)</code></pre>&#13;
<p>The black line in <em>Figure <a href="#x1-80006r5">4.5</a></em> is the mean of the number of rented bikes. This is the same as in <em>Figure <a href="#x1-79007r4">4.4</a></em>. The new elements are the dark gray band representing the central 50% (quantiles 0.25 and 0.75) for the rented bikes and the light gray band, representing the central 94% (quantiles 0.03 and 0.97). You may notice that our model is predicting a negative number of bikes, which does not make sense. But upon reflection, this should be expected as we use a Normal distribution for the likelihood in <code>model_lb</code>. A very dirty <em>fix</em> could be to clip the predictions at values lower than 0, but that’s ugly. In the next section, we will see that we can easily improve this <span id="dx1-80005"/>model to avoid nonsensical predictions.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file103.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-80006r5"/><strong>Figure 4.5</strong>: Posterior predictive plot for the bike linear model</p>&#13;
<p><span id="x1-80007r169"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="generalizing-the-linear-model" class="level3 sectionHead" data-number="1.8.3">&#13;
<h2 class="sectionHead" data-number="1.8.3">4.3 <span id="x1-810003"/>Generalizing the linear model</h2>&#13;
<p>The linear model we have been using is a <span id="dx1-81001"/>special case of a more general model, the <strong>Generalized Linear Model</strong> (<strong>GLM</strong>). The GLM is a generalization of the <span id="dx1-81002"/>linear model that allows us to use different distributions for the likelihood. At a high level, we can write a Bayesian GLM like:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file104.jpg" class="math-display" alt="𝛼 ∼ a prior 𝛽 ∼ another prior θ ∼ some prior μ = 𝛼 + 𝛽X Y ∼ ϕ (f (μ ),θ) "/>&#13;
</div>&#13;
<p><img src="../media/phi.png" style="width: 0.8em; vertical-align: -0.30em;"/> is an arbitrary distribution; some common cases are Normal, Student’s t, Gamma, and NegativeBinomial. <em>θ</em> represents any <em>auxiliary</em> parameter the distribution may have, like <em>σ</em> for the Normal. We also have <em>f</em>, usually <span id="dx1-81003"/>called the inverse link function. When <img src="../media/phi.png" style="width: 0.8em; vertical-align: -0.30em;"/> is Normal, then <em>f</em> is the identity function. For distributions like Gamma and NegativeBinomial, <em>f</em> is usually the exponential function. Why do we need <em>f</em>? Because the linear model will generally be on the real line, but the <em>μ</em> parameter (or its equivalent) may be defined on a different domain. For instance, <em>μ</em> for the NegativeBinomial is defined for positive values, so we need to transform <em>μ</em>. The exponential function is a good candidate for this transformation. We are going to explore a few GLMs in this book. A good exercise for you, while reading the book, is to <span id="dx1-81004"/>create a table, and every time you see a new GLM, you add one line indicating what <em>phi</em>, <em>theta</em>, and <em>f</em> are and maybe some notes about when this GLM is used. OK, let’s start with our first concrete example of a GLM. <span id="x1-81005r180"/></p>&#13;
</section>&#13;
<section id="counting-bikes" class="level3 sectionHead" data-number="1.8.4">&#13;
<h2 class="sectionHead" data-number="1.8.4">4.4 <span id="x1-820004"/>Counting bikes</h2>&#13;
<p>How can we change <code>model_lb </code>to better accommodate the bike data? There are two things to note: the number of rented <span id="dx1-82001"/>bikes is discrete and it is bounded at 0. This is usually known as count data, which is data that is the result of <span id="dx1-82002"/>counting something. Count data is sometimes modeled using a continuous distribution like a Normal, especially when the number of counts is large. But it is often a good idea to use a discrete distribution. Two common choices are the Poisson and NegativeBinomial distributions. The main difference is that for Poisson, the mean and the variance are the same, but if this is not true or even approximately true, then NegativeBinomial may be a better choice as it allows the mean and variance to be different. When in doubt, you can fit both Poisson and NegativeBinomial and see which one provides a better model. We are going to do that in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>. But for now, we are going to use NegativeBinomial.</p>&#13;
<p><span id="x1-82003r5"/> <span id="x1-82004"/><strong>Code 4.5</strong></p>&#13;
<pre id="listing-47" class="source-code"><code>with pm.Model() as model_neg: </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", mu=0, sigma=1) </code>&#13;
<code>    <em>β</em> = pm.Normal("<em>β</em>", mu=0, sigma=10) </code>&#13;
<code>    σ = pm.HalfNormal("σ", 10) </code>&#13;
<code>    μ = pm.Deterministic("μ", pm.math.exp(<em>α</em> + <em>β</em> * bikes.temperature)) </code>&#13;
<code>    y_pred = pm.NegativeBinomial("y_pred", mu=μ, alpha=σ, observed=bikes.rented) </code>&#13;
<code>    idata_neg = pm.sample() </code>&#13;
<code>    idata_neg.extend(pm.sample_posterior_predictive(idata_neg))</code></pre>&#13;
<p>The PyMC model is very similar to the previous one but with two main differences. First, we use <code>pm.NegativeBinomial </code>instead of <code>pm.Normal </code>for the likelihood. The NegativeBinomial distribution has two parameters, the mean <em>μ</em> and a dispersion parameter <em>α</em>. The variance of NegativeBinomial is <em>μ</em> + <img src="../media/file105.jpg" class="frac" data-align="middle" alt="μ2 𝛼-"/>, so the larger the value of <em>α</em> the larger the variance. The second difference is that <em>μ</em> is <code>pm.math.exp(</code><em>α</em> <code>+ </code><em>β</em> <code>* bikes.temperature) </code>instead of just <em>α</em> <code>+ </code><em>β</em> <code>* bikes.temperature </code>and, as we already explained, this is needed to transform the real line into the positive interval.</p>&#13;
<p>The posterior predictive distribution for <code>model_neg </code>is shown in <em>Figure <a href="#x1-82014r6">4.6</a></em>. The posterior predictive distribution is also very <span id="dx1-82013"/>similar to the one we obtained with the linear model (<em>Figure <a href="#x1-80006r5">4.5</a></em>). The main difference is that now we are not predicting a negative number of rented bikes! We can also see that the variance of the predictions increases with the mean. This is expected because the variance of NegativeBinomial is <em>μ</em> + <img src="../media/file106.jpg" class="frac" data-align="middle" alt="μ2 𝛼"/>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file107.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-82014r6"/><strong>Figure 4.6</strong>: Posterior predictive plot for the bike NegativeBinomial linear model</p>&#13;
<p><em>Figure <a href="#x1-82017r7">4.7</a></em> shows the posterior predictive check for <code>model_lb </code>on the left and <code>model_neg </code>on the right. We can see that when using a Normal, the largest mismatch is that the model predicts a negative number of rented bikes, but even on the positive side we see that the fit is not that good. On the other hand, the NegativeBinomial model seems to be a better fit, although it’s not perfect. Look at the right tail: it’s heavier for the predictions than observations. But also notice that the <span id="dx1-82015"/>probability of this very high demand is low. So, overall we can restate that the NegativeBinomial model is better <span id="dx1-82016"/>than the Normal one.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file108.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-82017r7"/><strong>Figure 4.7</strong>: Posterior predictive check for the bike linear model</p>&#13;
<p><span id="x1-82018r181"/></p>&#13;
</section>&#13;
<section id="robust-regression" class="level3 sectionHead" data-number="1.8.5">&#13;
<h2 class="sectionHead" data-number="1.8.5">4.5 <span id="x1-830005"/>Robust regression</h2>&#13;
<p>I once ran a complex simulation of a molecular system. At each step of the simulation, I needed it to fit a linear regression as an intermediate step. I had theoretical and empirical reasons to think that my Y was conditionally Normal given my Xs, so I decided simple linear regression should do the trick. But from time to time the simulation generated a few values of Y that were way above or <span id="dx1-83001"/>below the bulk of the data. This completely ruined my simulation and I had to restart it.</p>&#13;
<p>Usually, these values that are very different from the bulk of the data are called outliers. The reason for the <span id="dx1-83002"/>failure of my simulations was that the outliers were <em>pulling</em> the regression line away from the bulk of the data and when I passed this estimate to the next step in the simulation, the thing just halted. I solved this with the help of our good friend the Student’s t-distribution, which, as we saw in <em>Chapter <a href="CH02.xhtml#x1-440002">2</a></em>, has heavier tails than the Normal distribution. This means that the outliers have less influence on the regression line. This is an example of a robust regression.</p>&#13;
<p>To exemplify the robustness that a Student’s T distribution brings to linear regression, we are going to use a very simple and nice dataset: the third data group from <span id="dx1-83003"/>Anscombe’s quartet. If you do not know what Anscombe’s quartet is, check it out on Wikipedia ( <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" class="url">https://en.wikipedia.org/wiki/Anscombe%27s_quartet</a>).</p>&#13;
<p>In the following model, <code>model_t</code>, we are using a shifted exponential to avoid values close to 0. The non-shifted Exponential puts too much weight on values close to 0. In my experience, this is fine for data with none to moderate outliers, but for data with extreme outliers (or data with a few bulk points), like in Anscombe’s third dataset, it is better to avoid such low values. Take this, as well as other prior recommendations, with a pinch of salt. The defaults are good starting points, but there’s no need to stick to them. Other common priors are Gamma(2, 0.1) and Gamma(mu=20, sigma=15), which are somewhat similar to Exponential(1/30) but with less values closer to 0:</p>&#13;
<p><span id="x1-83004r6"/> <span id="x1-83005"/><strong>Code 4.6</strong></p>&#13;
<pre id="listing-48" class="source-code"><code>with pm.Model() as model_t: </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", mu=ans.y.mean(), sigma=1) </code>&#13;
<code>    <em>β</em> = pm.Normal("<em>β</em>", mu=0, sigma=1) </code>&#13;
<code>    σ = pm.HalfNormal("σ", 5) </code>&#13;
<code>    ν_ = pm.Exponential("ν_", 1 / 29) </code>&#13;
<code>    ν = pm.Deterministic("ν", ν_ + 1) </code>&#13;
<code>    μ = pm.Deterministic("μ", <em>α</em> + <em>β</em> * ans.x) </code>&#13;
<code>    _ = pm.StudentT("y_pred", mu=μ, sigma=σ, nu=ν, observed=ans.y) </code>&#13;
<code>    idata_t = pm.sample(2000)</code></pre>&#13;
<p>In <em>Figure <a href="#x1-83015r8">4.8</a></em>, we can see the robust fit, according to <code>model_t</code>, and the non-robust fit, according to SciPy’s <code>linregress </code>(this function is doing least-squares regression).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file109.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-83015r8"/><strong>Figure 4.8</strong>: Robust regression according to <code>model_t</code></p>&#13;
<p>While the non-robust fit tries to <em>compromise</em> and include all points, the robust Bayesian model, <code>model_t</code>, automatically <em>discards</em> one point and fits a line that passes closer through all the remaining points. I know this is a very peculiar dataset, but the message remains the same as for other datasets; a Student’s t-distribution, due to its heavier tails, gives less importance to points that are far away from the bulk of the data.</p>&#13;
<p>From <em>Figure <a href="#x1-83017r9">4.9</a></em>, we can see that for the <span id="dx1-83016"/>bulk of the data, we get a very good match. Also, notice that our model predicts values away from the bulk to both sides and not just above the bulk (as in the observed data). For our current purposes, this model is performing just fine and it does not need further changes. Nevertheless, notice that for some problems, we may want to avoid this. In such a case, we should probably go back and change the model to restrict the possible values of <code>y_pred </code>to positive values using a truncated Student’s t-distribution. This is left as an exercise for the reader.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file110.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-83017r9"/><strong>Figure 4.9</strong>: Posterior predictive check for <code>model_t</code></p>&#13;
<p><span id="x1-83018r184"/></p>&#13;
</section>&#13;
<section id="logistic-regression" class="level3 sectionHead" data-number="1.8.6">&#13;
<h2 class="sectionHead" data-number="1.8.6">4.6 <span id="x1-840006"/>Logistic regression</h2>&#13;
<p>The logistic regression model is a generalization of the linear regression model, which we can use when the response variable is binary. This model uses the logistic function as an inverse link function. Let’s get familiar with this function before we move on to the model:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file111.jpg" class="math-display" alt="logistic(z) = ---1--- 1+ e−z "/>&#13;
</div>&#13;
<p>For our purpose, the key property of the logistic function is that irrespective of the values of its argument <em>z</em>, the result will always be a <span id="dx1-84001"/>number in the [0-1] interval. Thus, we can see this function as a convenient way to compress the values computed from a linear model into values that we can feed into a Bernoulli distribution. This logistic function is also known as the sigmoid function <span id="dx1-84002"/>because of its characteristic S-shaped aspect, as we can see from <em>Figure <a href="#x1-84003r10">4.10</a></em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file112.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-84003r10"/><strong>Figure 4.10</strong>: Logistic function</p>&#13;
<p><span id="x1-84004r177"/></p>&#13;
<section id="the-logistic-model" class="level4 subsectionHead" data-number="1.8.6.1">&#13;
<h3 class="subsectionHead" data-number="1.8.6.1">4.6.1 <span id="x1-850001"/>The logistic model</h3>&#13;
<p>We have almost all the elements to turn a simple <span id="dx1-85001"/>linear regression into a simple logistic regression. Let’s begin with the case of only two classes, for example, ham/spam, safe/unsafe, cloudy/sunny, healthy/ill, or hotdog/not hotdog. First, we codify these classes by saying that the predicted variable <em>y</em> can only take two values, 0 or 1, that is <em>y</em> <span class="cmsy-10x-x-109">∈{</span>0<em>,</em>1<span class="cmsy-10x-x-109">}</span>.</p>&#13;
<p>Stated this way, the problem sounds very similar to the coin-flipping one we used in previous chapters. We may remember we used the Bernoulli distribution as the likelihood. The difference with the coin-flipping problem is that now <em>θ</em> is not going to be generated from a beta distribution; instead, <em>θ</em> is going to be <span id="dx1-85002"/>defined by a linear model with the logistic as the inverse link function. Omitting the priors, we have:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file113.jpg" class="math-display" alt="θ = logistic(𝛼 + 𝛽x) y ∼ Bernoulli(θ) "/>&#13;
</div>&#13;
<p>We are going to apply logistic regression to the classic iris dataset which has measurements from flowers from three closely related species: setosa, virginica, and versicolor. These measurements are the petal length, petal width, sepal length, and sepal width. In case you are wondering, sepals are modified leaves whose function is generally related to protecting the flowers in a bud.</p>&#13;
<p>We are going to begin with a simple case. Let’s assume we only have two classes, setosa, and versicolor, and just one independent variable or feature, <code>sepal_length</code>. We want to predict the probability of a flower being setosa given its sepal length.</p>&#13;
<p>As is usually done, we are going to encode the <code>setosa </code>and <code>versicolor </code>categories with the numbers <code>0 </code>and <code>1</code>. Using pandas, we can do the following:</p>&#13;
<p><span id="x1-85003r7"/> <span id="x1-85004"/><strong>Code 4.7</strong></p>&#13;
<pre id="listing-49" class="source-code"><code>df = iris.query("species == ('setosa', 'versicolor')") </code>&#13;
<code>y_0 = pd.Categorical(df["species"]).codes </code>&#13;
<code>x_n = "sepal_length" </code>&#13;
<code>x_0 = df[x_n].values </code>&#13;
<code>x_c = x_0 - x_0.mean()</code></pre>&#13;
<p>As with other linear models, centering the data can help with the sampling. Now that we have the data in the right format, we can <span id="dx1-85010"/>finally build the model with PyMC:</p>&#13;
<p><span id="x1-85011r8"/> <span id="x1-85012"/><strong>Code 4.8</strong></p>&#13;
<pre id="listing-50" class="source-code"><code>with pm.Model() as model_lrs: </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", mu=0, sigma=1) </code>&#13;
<code>    <em>β</em> = pm.Normal("<em>β</em>", mu=0, sigma=5) </code>&#13;
<code>    μ = <em>α</em> + x_c * <em>β</em> </code>&#13;
<code>    <em>θ</em> = pm.Deterministic("<em>θ</em>", pm.math.sigmoid(μ)) </code>&#13;
<code>    bd = pm.Deterministic("bd", -<em>α</em> / <em>β</em>) </code>&#13;
<code>    yl = pm.Bernoulli("yl", p=<em>θ</em>, observed=y_0) </code>&#13;
<code> </code>&#13;
<code>    idata_lrs = pm.sample()</code></pre>&#13;
<p><code>model_lrs </code>has two deterministic variables: <em>θ</em> and <code>bd</code>. <em>θ</em> is the result of applying the logistic function to variable <em>μ</em>. <code>bd </code>is the boundary decision, which is the value we use to separate classes. We will discuss this later in detail. Another point worth mentioning is that <span id="dx1-85022"/>instead of writing the logistic function ourselves, we are using the one provided by PyMC, <code>pm.math.sigmoid</code>.</p>&#13;
<p><em>Figure <a href="#x1-85023r11">4.11</a></em> shows the result of <code>model_lrs</code>:</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file114.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-85023r11"/><strong>Figure 4.11</strong>: Logistic regression, result of <code>model_lrs</code></p>&#13;
<p><em>Figure <a href="#x1-85023r11">4.11</a></em> shows the sepal length versus the probability of being versicolor <em>θ</em> (and if you want, also the probability of being setosa, 1 <span class="cmsy-10x-x-109">− </span><em>θ</em>). We have added some jitter (noise) to the binary response so the point does not overlap. An S-shaped (black) line is the mean value of <em>θ</em>. This line can be interpreted as the probability of a flower being versicolor, given that we know the value of the sepal length. The semitransparent S-shaped band is the 94% HDI. What about the <span id="dx1-85024"/>vertical line? That’s the topic of the next section. <span id="x1-85025r188"/></p>&#13;
</section>&#13;
<section id="classification-with-logistic-regression" class="level4 subsectionHead" data-number="1.8.6.2">&#13;
<h3 class="subsectionHead" data-number="1.8.6.2">4.6.2 <span id="x1-860002"/>Classification with logistic regression</h3>&#13;
<p><span id="dx1-86001"/></p>&#13;
<p>My mother prepares a delicious dish called sopa seca, which is basically a spaghetti-based recipe and translates literally to ”dry soup.” While it may sound like a misnomer or even an oxymoron, the name of the dish makes total sense when you learn how it is cooked (you may check out the recipe in the GitHub repo for this book at <a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>). Something similar happens with logistic regression, a model that, despite its name, is generally framed as a method for solving classification problems. Let’s see the source of this duality.</p>&#13;
<p>Regression problems are about predicting a continuous value for an output variable given the values of one or more input variables. We have seen many examples of regression that include logistic regression. However, logistic regression is usually discussed in terms of classification. Classification involves assigning discrete values (representing a class, like versicolor) to an output variable given some input variables, for instance, stating that a flower is versicolor or setosa given its sepal length.</p>&#13;
<p>So, is logistic regression a regression or a classification method? The answer is that it is a regression method; we are regressing the probability of belonging to some class, but it can be used for classification too. The only thing we need is a decision rule: for example, we assign the class <code>versicolor </code>if <em>θ</em> <span class="cmsy-10x-x-109">≥ </span>0<em>.</em>5 and assign <code>setosa </code>otherwise. The vertical line in <em>Figure <a href="#x1-85023r11">4.11</a></em> is the boundary decision, and it is defined as the value of the independent variable that makes the probability of being versicolor equal to 0.5. We can calculate this value analytically, and it is equal to <span class="cmsy-10x-x-109">−</span><img src="../media/file115.jpg" class="frac" data-align="middle" alt="𝛼- 𝛽"/>. This calculation is based on the definition of the model:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file116.jpg" class="math-display" alt="θ = logistic(𝛼 + 𝛽x) "/>&#13;
</div>&#13;
<p>And from the definition of the logistic function, we have that <em>θ</em> = 0<em>.</em>5 when <em>α</em> + <em>β</em><em>x</em> = 0.</p>&#13;
<div class="math-display">&#13;
<img src="../media/file117.jpg" class="math-display" alt="0.5 = logistic(𝛼 + 𝛽x ) ⇐ ⇒ 0 = 𝛼 + 𝛽x "/>&#13;
</div>&#13;
<p>Reordering, we find that the value of <em>x</em> that makes <em>θ</em> = 0<em>.</em>5 is <span class="cmsy-10x-x-109">−</span><img src="../media/file118.jpg" class="frac" data-align="middle" alt="𝛼𝛽-"/>.</p>&#13;
<p>Because we have uncertainty in the value of <em>α</em> and <em>β</em>, we also have uncertainty about the value of the boundary decision. This uncertainty is represented as the vertical (gray) band in <em>Figure <a href="#x1-85023r11">4.11</a></em>, which goes from <span class="cmsy-10x-x-109">≈ </span>5<em>.</em>3 to <span class="cmsy-10x-x-109">≈ </span>5<em>.</em>6. If we were doing automatic classification of flowers based on their sepal length (or any similar problem that could be framed within this model), we could assign setosa to flowers with a sepal length below 5.3 and versicolor to flowers with sepal length above 5.6. For flowers with a sepal lengths between 5.3 and 5.6, we would be uncertain about their class, so we could either assign them randomly or use some other information to make a decision, including asking a human to check the flower.</p>&#13;
<p>To summarize this section:</p>&#13;
<ul>&#13;
<li><p>The value of <em>θ</em> is, generally speaking, <em>P</em>(<em>Y</em> = 1<span class="cmsy-10x-x-109">|</span><em>X</em>). In this sense, logistic regression is a true regression; the key detail is that we are regressing the probability that a data point belongs to class 1, given a linear combination of features.</p></li>&#13;
<li><p>We are modeling the mean of a dichotomous variable, which is a number in the [0-1] interval. Thus, if we want to use logistic regression for classification, we need to introduce a rule to turn this probability into a two-class assignment. For example, if <em>P</em>(<em>Y</em> = 1) <em>&gt;</em> 0<em>.</em>5, we assign that observation to class 1, otherwise we assign it to class 0.</p></li>&#13;
<li><p>There is nothing special about the value of 0.5, other than that it is the number in the middle of 0 and 1. This boundary can be justified when we are OK with misclassifying a data point in either direction. But this is not always the case, because the cost associated with the misclassification does not need to be symmetrical. For example, if we are trying to predict whether a patient has a disease or not, we may want to use a boundary that minimizes the number of false negatives (patients that have the disease but we predict they don’t) or false positives (patients that don’t have the disease but we predict they do). We will discuss this in more detail in the next section.</p></li>&#13;
</ul>&#13;
<p><span id="x1-86002r193"/></p>&#13;
</section>&#13;
<section id="interpreting-the-coefficients-of-logistic-regression" class="level4 subsectionHead" data-number="1.8.6.3">&#13;
<h3 class="subsectionHead" data-number="1.8.6.3">4.6.3 <span id="x1-870003"/>Interpreting the coefficients of logistic regression</h3>&#13;
<p><span id="dx1-87001"/></p>&#13;
<p>We must be careful when interpreting the coefficients of logistic regression. Interpretation is not as straightforward as with simple linear models. Using the logistic inverse link function introduces a non-linearity that we have to take into account. If <em>β</em> is positive, increasing <em>x</em> will increase <em>p</em>(<em>y</em> = 1) by some amount, but the amount is not a linear function of <em>x</em>. Instead, the dependency is non-linear on the value of <em>x</em>, meaning that the effect of <em>x</em> on <em>p</em>(<em>y</em> = 1) depends on the value of <em>x</em>. We can visualize this fact in <em>Figure <a href="#x1-85023r11">4.11</a></em>. Instead of a line with a constant slope, we have an S-shaped line with a slope that changes as a function of <em>x</em>.</p>&#13;
<p>A little bit of algebra can give us some further insight into how much <em>p</em>(<em>y</em> = 1) changes with <em>x</em>. The basic logistic model is:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file119.jpg" class="math-display" alt="θ = logistic(𝛼 + 𝛽x) "/>&#13;
</div>&#13;
<p>The inverse of the logistic is the logit function, which is:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file120.jpg" class="math-display" alt=" --z-- logit(z) = log 1 − z "/>&#13;
</div>&#13;
<p>Combining these two expressions, we get:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file121.jpg" class="math-display" alt=" θ logit(θ) = log1-−-θ = 𝛼 + 𝛽x "/>&#13;
</div>&#13;
<p>Remember that <em>θ</em> in our model is <em>p</em>(<em>y</em> = 1), so we can rewrite the previous expression as:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file122.jpg" class="math-display" alt=" ( ) log --p(y-=-1)-- = 𝛼 + 𝛽x 1 − p(y = 1) "/>&#13;
</div>&#13;
<p>The <img src="../media/file123.jpg" class="frac" data-align="middle" alt="-p(y=1)-- 1−p(y=1)"/> quantity is known as the <strong>odds</strong> of <em>y</em> = 1. If we call <em>y</em> = 1 a <em>success</em>, then the odds of success is the ratio of the probability of success over the probability of failure. For example, while the probability of getting a 2 by rolling a fair die is <img src="../media/file124.jpg" class="frac" data-align="middle" alt="1 6"/>, the odds of getting a 2 are <img src="../media/file125.jpg" class="frac" data-align="middle" alt="1∕6- 5∕6"/> = <img src="../media/file126.jpg" class="frac" data-align="middle" alt="1 5"/> = 0<em>.</em>2. In other words, there is one favorable event for every five unfavorable events. Odds are often used by gamblers because they provide a more intuitive tool to think about bets than raw probabilities. <em>Figure <a href="#x1-87002r12">4.12</a></em> shows how probabilities are related to odds and log-odds.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file127.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-87002r12"/><strong>Figure 4.12</strong>: Relationship between probability, odds, and log-odds</p>&#13;
<div id="tcolobox-10" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Interpreting Logistic Regression</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>In logistic regression, the <em>β</em> coefficient (the <em>slope</em>) encodes the increase in log-odds units by a unit increase of the <em>x</em> variable.</p>&#13;
</div>&#13;
</div>&#13;
<p>The transformation from probability to odds is a monotonic transformation, meaning the odds increase as the probability increases, and the other way around. While probabilities are restricted to the [0<em>,</em>1] interval, odds live in the [0<em>,</em><span class="cmsy-10x-x-109">∞</span>) interval. The logarithm is another monotonic transformation and log-odds are in the (<span class="cmsy-10x-x-109">−∞</span><em>,</em><span class="cmsy-10x-x-109">∞</span>) interval. <span id="x1-87003r187"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="variable-variance" class="level3 sectionHead" data-number="1.8.7">&#13;
<h2 class="sectionHead" data-number="1.8.7">4.7 <span id="x1-880007"/>Variable variance</h2>&#13;
<p><span id="dx1-88001"/> <span id="dx1-88002"/></p>&#13;
<p>We have been using the linear motif to model the mean of a distribution and, in the previous section, we used it to model interactions. In statistics, it is said that a linear regression model presents heteroskedasticity when the variance of the errors is not constant in all the observations made. For those cases, we may want to consider the variance (or standard deviation) as a (linear) function of the dependent variable.</p>&#13;
<p>The World Health Organization and other health institutions around the world collect data for newborns and toddlers and design growth chart standards. These charts are an essential component of the pediatric toolkit and also a measure of the general well-being of populations to formulate health-related policies, plan interventions, and monitor their effectiveness. An example of such data is the lengths (heights) of newborn/toddler girls as a function of their age (in months):</p>&#13;
<p><span id="x1-88003r9"/> <span id="x1-88004"/><strong>Code 4.9</strong></p>&#13;
<pre id="listing-51" class="source-code"><code>data = pd.read_csv("data/babies.csv") </code>&#13;
<code>data.plot.scatter("month", "length")</code></pre>&#13;
<p>To model this data, we are going to introduce three elements we have not seen before:</p>&#13;
<ul>&#13;
<li><p><em>σ</em> is now a linear function of the predictor variable. Thus, we add two new parameters, <em>γ</em> and <em>δ</em>. These are direct analogs of <em>α</em> and <em>β</em> in the linear model for the mean.</p></li>&#13;
<li><p>The linear model for the mean is a function of <img src="../media/file128.png" style="width: 2em; vertical-align: -0.10em;"/>. This is just a simple trick to fit a linear model to a curve.</p></li>&#13;
<li><p>We define a <code>MutableData </code>variable, <code>x_shared</code>. Why we want to do this will become clear soon.</p></li>&#13;
</ul>&#13;
<p>Our full model is:</p>&#13;
<p><span id="x1-88007r10"/> <span id="x1-88008"/><strong>Code 4.10</strong></p>&#13;
<pre id="listing-52" class="source-code"><code>with pm.Model() as model_vv: </code>&#13;
<code>    x_shared = pm.MutableData("x_shared", data.month.values.astype(float)) </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", sigma=10) </code>&#13;
<code>    <em>β</em> = pm.Normal("<em>β</em>", sigma=10) </code>&#13;
<code>    γ = pm.HalfNormal("γ", sigma=10) </code>&#13;
<code>    δ = pm.HalfNormal("δ", sigma=10) </code>&#13;
<code> </code>&#13;
<code>    μ = pm.Deterministic("μ", 𝛼 + <em>β</em> * x_shared**0.5) </code>&#13;
<code>    σ = pm.Deterministic("σ", γ + δ * x_shared) </code>&#13;
<code> </code>&#13;
<code>    y_pred = pm.Normal("y_pred", mu=μ, sigma=σ, observed=data.length) </code>&#13;
<code> </code>&#13;
<code>    idata_vv = pm.sample()</code></pre>&#13;
<p>On the left panel of <em>Figure <a href="#x1-88022r13">4.13</a></em>, we can see the mean of <em>μ</em> represented by a black curve, and the two semi-transparent gray bands represent one and two standard deviations. On the right panel, we have the estimated variance as a function of the length. As you can see, the variance increases with the length, which is what we expected.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file129.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-88022r13"/><strong>Figure 4.13</strong>: Posterior fit for <code>model_vv </code>on the left panel. On the right is the mean estimated variance as a function of the length</p>&#13;
<p>Now that we have fitted the model, we might want to use the model to find out how the length of a particular girl compares to the distribution. One way to answer this question is to ask the model for the distribution of the variable <code>length </code>for babies of, say, 0.5 months. We can answer this question by sampling from the posterior predictive distribution conditional on a length of 0.5. Using PyMC, we can get the answer by sampling <code>pm.sample_posterior_predictive</code>; the only problem is that by default, this function will return values of <em>ỹ</em> for the already observed values of <em>x</em>, i.e., the values used to fit the model. The easiest way to get predictions for unobserved values is to define a <code>MutableData </code>variable (<code>x_shared </code>in the example) and then update the value of this variable right before sampling the posterior predictive distribution, as shown in the following code block:</p>&#13;
<p><span id="x1-88023r11"/> <span id="x1-88024"/><strong>Code 4.11</strong></p>&#13;
<pre id="listing-53" class="source-code"><code>with model_vv: </code>&#13;
<code>    pm.set_data({"x_shared": [0.5]}) </code>&#13;
<code>    ppc = pm.sample_posterior_predictive(idata_vv) </code>&#13;
<code>    y_ppc = ppc.posterior_predictive["y_pred"].stack(sample=("chain", "draw"))</code></pre>&#13;
<p>Now we can plot the expected distribution of lengths for 2-week-old girls and calculate other quantities, like the percentile for a girl of that length (see <em>Figure <a href="#x1-88029r14">4.14</a></em>).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file130.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-88029r14"/><strong>Figure 4.14</strong>: Expected distribution of length at 0.5 months. The shaded area represents 32% of the accumulated mass</p>&#13;
<p><span id="x1-88030r195"/></p>&#13;
</section>&#13;
<section id="hierarchical-linear-regression" class="level3 sectionHead" data-number="1.8.8">&#13;
<h2 class="sectionHead" data-number="1.8.8">4.8 <span id="x1-890008"/>Hierarchical linear regression</h2>&#13;
<p><span id="dx1-89001"/></p>&#13;
<p>In <em>Chapter <a href="CH03.xhtml#x1-670003">3</a></em>, we learned the rudiments of hierarchical models, a very powerful concept that allows us to model complex data structures. Hierarchical models allow us to deal with inferences at the group level and estimations above the group level. As we have already seen, this is done by including hyperpriors. We also showed that groups can share information by using a common hyperprior and this provides shrinkage, which can help us to regularize the estimates.</p>&#13;
<p>We can apply these very same concepts to linear regression to obtain hierarchical linear regression models. In this section, we are going to walk through two examples to elucidate the application of these concepts in practical scenarios. The first one uses a synthetic dataset, and the second one uses the <code>pigs</code> dataset.</p>&#13;
<p>For the first example, I have created eight related groups, including one group with just one data point. We can see what the data looks like from <em>Figure <a href="#x1-89002r15">4.15</a></em>. If you want to learn more how this data was generated please check the GitHub repository <a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file131.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-89002r15"/><strong>Figure 4.15</strong>: Synthetic data for the hierarchical linear regression example</p>&#13;
<p>First, we are going to fit a non-hierarchical model:</p>&#13;
<p><span id="x1-89003r12"/> <span id="x1-89004"/><strong>Code 4.12</strong></p>&#13;
<pre id="listing-54" class="source-code"><code>coords = {"group": ["A", "B", "C", "D", "E", "F", "G", "H"]} </code>&#13;
<code> </code>&#13;
<code>with pm.Model(coords=coords) as unpooled_model: </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", mu=0, sigma=10, dims="group") </code>&#13;
<code>    <em>β</em> = pm.Normal("<em>β</em>", mu=0, sigma=10, dims="group") </code>&#13;
<code>    σ = pm.HalfNormal("σ", 5) </code>&#13;
<code>    _ = pm.Normal("y_pred", mu=<em>α</em>[idx] + <em>β</em>[idx] * x_m, sigma=σ, observed=y_m) </code>&#13;
<code> </code>&#13;
<code>    idata_up = pm.sample()</code></pre>&#13;
<p><em>Figure <a href="#x1-89014r16">4.16</a></em> shows the posterior estimated values for the parameters <em>α</em> and <em>β</em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file132.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-89014r16"/><strong>Figure 4.16</strong>: Posterior distribution for <em>α</em> and <em>β</em> for <code>unpooled_model</code></p>&#13;
<p>As you can see from <em>Figure <a href="#x1-89014r16">4.16</a></em> the estimates for group <code>H </code>are very different from the ones for the other groups. This is expected as for group <code>H</code>, we only have one data point, that is we do not have enough information to fit a line. We need at least two points; otherwise, the model will be over-parametrized, meaning we have more parameters than the ones we can determine from the data alone.</p>&#13;
<p>To overcome this situation we can provide some more information; we can do this by using priors or by adding more structure to the model. Let’s add more structure by building a hierarchical model.</p>&#13;
<p>This is the PyMC model for the hierarchical model:</p>&#13;
<p><span id="x1-89015r13"/> <span id="x1-89016"/><strong>Code 4.13</strong></p>&#13;
<pre id="listing-55" class="source-code"><code>with pm.Model(coords=coords) as hierarchical_centered: </code>&#13;
<code>    # hyperpriors </code>&#13;
<code>    <em>α</em>_μ = pm.Normal("<em>α</em>_μ", mu=y_m.mean(), sigma=1) </code>&#13;
<code>    <em>α</em>_σ = pm.HalfNormal("<em>α</em>_σ", 5) </code>&#13;
<code>    <em>β</em>_μ = pm.Normal("<em>β</em>_μ", mu=0, sigma=1) </code>&#13;
<code>    <em>β</em>_σ = pm.HalfNormal("<em>β</em>_σ", sigma=5) </code>&#13;
<code> </code>&#13;
<code>    # priors </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", mu=<em>α</em>_μ, sigma=<em>α</em>_σ, dims="group") </code>&#13;
<code>    <em>β</em> = pm.Normal("<em>β</em>", mu=<em>β</em>_μ, sigma=<em>β</em>_σ, dims="group") </code>&#13;
<code>    σ = pm.HalfNormal("σ", 5) </code>&#13;
<code>    _ = pm.Normal("y_pred", mu=<em>α</em>[idx] + <em>β</em>[idx] * x_m, sigma=σ, observed=y_m) </code>&#13;
<code> </code>&#13;
<code>    idata_cen = pm.sample()</code></pre>&#13;
<p>If you run <code>hierarchical_centered</code>, you will see a message from PyMC saying something like <code>There were 149 divergences after tuning. Increase target_accept or reparameterize. </code>This message means that samples generated from PyMC may not be trustworthy. So far, we have assumed that PyMC always returns samples that we can use without issues, but that’s not always the case. In <em>Chapter <a href="CH10.xhtml#x1-18900010">10</a></em>, we further discuss why this is, along with diagnostic methods to help you identify those situations and recommendations to fix the potential issues. In that section, we also explain what divergences are. For now, we will only say that when <span id="dx1-89031"/>working with hierarchical linear models, we will usually get a lot of divergences.</p>&#13;
<p>The easy way to solve them is to increase <code>target_accept</code>, as PyMC kindly suggests. This is an argument of <code>pm.sample() </code>that defaults to 0.8 and can take a maximum value of 1. If you see divergences, setting this argument to values like 0.85, 0.9, or even higher can help. But if you reach values like 0.99 and still have divergences, you are probably out of luck with this simple trick and you need to do something else. And that’s reparametrization. What is this? Reparametrization is writing a model in a different way, but that is mathematically equivalent to your original model: you are not changing the model, just writing it in another way. Many models, if not all, can be written in alternative ways. Sometimes, reparametrization can have a positive effect on the efficiency of the sampler or on the model’s interpretability. For instance, you can remove divergences by doing a reparametrization. Let’s see how to do that in the next section. <span id="x1-89032r194"/></p>&#13;
<section id="centered-vs.-noncentered-hierarchical-models" class="level4 subsectionHead" data-number="1.8.8.1">&#13;
<h3 class="subsectionHead" data-number="1.8.8.1">4.8.1 <span id="x1-900001"/>Centered vs. noncentered hierarchical models</h3>&#13;
<p>There are two common <span id="dx1-90001"/>parametrizations for hierarchical linear models, centered and non-centered. The <code>hierarchical_centered </code>model uses the centered one. The hallmark of this parametrization is that we are directly estimating parameters for individual groups; for instance, we are explicitly estimating the slope of each group. On the contrary, for the non-centered parametrization, we estimate the common slope for all groups and then a deflection for each group. It is <span id="dx1-90002"/>important to notice that we are still modeling the slope of each group, but relative to the common slope, the information we are getting is the same, just represented differently. Since a model is worth a thousand words, let’s check <code>hierarchical_non_centered</code>:</p>&#13;
<p><span id="x1-90003r14"/> <span id="x1-90004"/><strong>Code 4.14</strong></p>&#13;
<pre id="listing-56" class="source-code"><code>with pm.Model(coords=coords) as hierarchical_non_centered: </code>&#13;
<code>    # hyperpriors </code>&#13;
<code>    <em>α</em>_μ = pm.Normal("<em>α</em>_μ", mu=y_m.mean(), sigma=1) </code>&#13;
<code>    <em>α</em>_σ = pm.HalfNormal("<em>α</em>_σ", 5) </code>&#13;
<code>    <em>β</em>_μ = pm.Normal("<em>β</em>_μ", mu=0, sigma=1) </code>&#13;
<code>    <em>β</em>_σ = pm.HalfNormal("<em>β</em>_σ", sigma=5) </code>&#13;
<code> </code>&#13;
<code>    # priors </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", mu=<em>α</em>_μ, sigma=<em>α</em>_σ, dims="group") </code>&#13;
<code> </code>&#13;
<code>    <em>β</em>_offset = pm.Normal("<em>β</em>_offset", mu=0, sigma=1, dims="group") </code>&#13;
<code>    <em>β</em> = pm.Deterministic("<em>β</em>", <em>β</em>_μ + <em>β</em>_offset * <em>β</em>_σ, dims="group") </code>&#13;
<code> </code>&#13;
<code>    σ = pm.HalfNormal("σ", 5) </code>&#13;
<code>    _ = pm.Normal("y_pred", mu=<em>α</em>[idx] + <em>β</em>[idx] * x_m, sigma=σ, observed=y_m) </code>&#13;
<code> </code>&#13;
<code>    idata_ncen = pm.sample(target_accept=0.85)</code></pre>&#13;
<p>The difference is that for the model <code>hierarchical_centered</code>, we defined <em>β</em> <span class="cmsy-10x-x-109">∼ <img src="../media/N.PNG" style="width: 1.2em; vertical-align: -0.10em;"/></span>(<em>β</em><sub><em>μ</em></sub><em>,<em>β</em></em><sub><em>σ</em></sub>), and for <code>hierarchical_non_centered </code>we did <em>β</em> = <em>β</em><sub><em>μ</em></sub> + <em>β</em><sub>offset</sub> * <em>β</em><sub><em>σ</em></sub>. The non-centered parametrization is more efficient: when I run the model I only get 2 divergences instead of 148 as before. To remove these remaining divergences, we may still need to increase <code>target_accept</code>. For this particular case, changing it from 0.8 to 0.85 worked like magic. To fully understand why this reparametrization works, you need to understand the geometry of the posterior distribution, but that’s <span id="dx1-90022"/>beyond the scope of this section. Don’t worry, we will discuss this in <em>Chapter <a href="CH10.xhtml#x1-18900010">10</a></em>.</p>&#13;
<p>Now that our samples are divergence-free, we can go back to analyze the posterior. <em>Figure <a href="#x1-90023r17">4.17</a></em> shows the estimated values for <em>α</em> and <em>β</em> for <code>hierarchical_model</code>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file133.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-90023r17"/><strong>Figure 4.17</strong>: Posterior distribution for <em>α</em> and <em>β</em> for <code>hierarchical_non_centered</code></p>&#13;
<p>The estimates for group <code>H </code>are still the ones with higher uncertainty. But the results look less crazy than those in <em>Figure <a href="#x1-89014r16">4.16</a></em>; the reason is that groups are sharing information. Hence, even when we don’t have enough information to fit a line to a single point, group <code>H </code><em>is being informed</em> by the other groups. Actually, all groups are informing all groups. This is the power of hierarchical models.</p>&#13;
<p><em>Figure <a href="#x1-90024r18">4.18</a></em> shows the fitted lines for each of the eight groups. We can see that we managed to fit a line to a single point. At first, this may sound weird or even fishy, but this is just a consequence of the structure of the hierarchical model. Each line is informed by the lines of the other groups, thus we are not truly adjusting a line to a single point. Instead, we are adjusting a line that’s been informed by the points in the other groups to a single point.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file134.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-90024r18"/><strong>Figure 4.18</strong>: Fitted lines for <code>hierarchical_non_centered</code></p>&#13;
<p><span id="x1-90025r202"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="multiple-linear-regression" class="level3 sectionHead" data-number="1.8.9">&#13;
<h2 class="sectionHead" data-number="1.8.9">4.9 <span id="x1-910009"/>Multiple linear regression</h2>&#13;
<p><span id="dx1-91001"/></p>&#13;
<p>So far, we have been working with one dependent variable and one independent variable. Nevertheless, it is not unusual to have several independent variables that we want to include in our model. Some examples could be:</p>&#13;
<ul>&#13;
<li><p>Perceived quality of wine (dependent) and acidity, density, alcohol level, residual sugar, and sulfates content (independent variables)</p></li>&#13;
<li><p>A student’s average grades (dependent) and family income, distance from home to school, and mother’s education level (categorical variable)</p></li>&#13;
</ul>&#13;
<p>We can easily extend the simple linear regression model to deal with more than one independent variable. We call this model multiple linear regression or, less often, multivariable linear regression (not to be confused with multivariate linear regression, the case where we have multiple dependent variables).</p>&#13;
<p>In a multiple linear regression model, we model the mean of the dependent variable as follows:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file135.jpg" class="math-display" alt="μ = 𝛼 + 𝛽1X1 + 𝛽2X2 + ⋅⋅⋅+ 𝛽kXk "/>&#13;
</div>&#13;
<p>Using linear algebra notation, we can write a shorter version:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file136.jpg" class="math-display" alt="μ = 𝛼+ X𝛽 "/>&#13;
</div>&#13;
<p><strong>X</strong> is a matrix of size <em>n</em> <span class="cmsy-10x-x-109">× </span><em>k</em> with the values of the independent variables, <em>β</em> is a vector of size <em>k</em> with the coefficients of the independent variables, and <em>n</em> is the number of observations.</p>&#13;
<p>If you are a little rusty with your linear algebra, you may want to check the Wikipedia article about the dot product between two vectors and its generalization to matrix multiplication: <a href="https://en.wikipedia.org/wiki/Dot_product" class="url">https://en.wikipedia.org/wiki/Dot_product</a>. Basically, what you need to know is that we are <span id="dx1-91002"/>just using a shorter and more convenient way to write our model:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file137.jpg" class="math-display" alt=" ∑n X 𝛽 = 𝛽iXi = 𝛽1X1 + 𝛽2X2 + ⋅⋅⋅+ 𝛽kXk i "/>&#13;
</div>&#13;
<p>Using the simple linear regression model, we find a straight line that (hopefully) explains our data. Under the multiple linear regression model, we find, instead, a hyperplane of dimension <em>k</em>. Thus, the multiple linear regression model is essentially the same as the simple linear regression model, the only difference being that now <em>β</em> is a vector and <strong>X</strong> is a matrix.</p>&#13;
<p>To see an example of a</p>&#13;
<p><span id="dx1-91003"/>multiple linear regression model, let’s go back to the bikes dataset. We will use the temperature and the humidity of the day to predict the number of rented bikes:</p>&#13;
<p><span id="x1-91004r15"/> <span id="x1-91005"/><strong>Code 4.15</strong></p>&#13;
<pre id="listing-57" class="source-code"><code>with pm.Model() as model_mlb: </code>&#13;
<code>    <em>α</em> = pm.Normal("<em>α</em>", mu=0, sigma=1) </code>&#13;
<code>    <em>β</em>0 = pm.Normal("<em>β</em>0", mu=0, sigma=10) </code>&#13;
<code>    <em>β</em>1 = pm.Normal("<em>β</em>1", mu=0, sigma=10) </code>&#13;
<code>    σ = pm.HalfNormal("σ", 10) </code>&#13;
<code>    μ = pm.Deterministic("μ", pm.math.exp(<em>α</em> + <em>β</em>0 * bikes.temperature + </code>&#13;
<code>                                              <em>β</em>1 * bikes.hour)) </code>&#13;
<code>    _ = pm.NegativeBinomial("y_pred", mu=μ, alpha=σ, observed=bikes.rented) </code>&#13;
<code> </code>&#13;
<code>    idata_mlb = pm.sample()</code></pre>&#13;
<p>Please take a moment to compare <code>model_mlb</code>, which has two independent variables, <code>temperature </code>and <code>hour</code>, with <code>model_neg</code>, which only has one independent variable, <code>temperature</code>. The only difference is that now we have two <em>β</em> coefficients, one for each independent variable. The rest of the model is the same. Notice that we could have written <em>β</em> <code>= pm.Normal("</code><em>β</em><code>1", mu=0, sigma=10, shape=2) </code>and then used <em>β</em><code>1[0] </code>and <em>β</em><code>1[1] </code>in the definition of <em>μ</em>. I usually do that.</p>&#13;
<p>As you can see, writing a multiple regression model is not that different from writing a simple regression model. Interpreting the results can be more challenging, though. For instance, the coefficient of <code>temperature </code>is now <em>β</em><sub>0</sub> and the coefficient of <code>hour </code>is <em>β</em><sub>1</sub>. We can still interpret the coefficients as the change in the dependent variable for a unit change in the independent variable. But now we have to be careful to specify which independent variable we are talking about. For instance, we can say that for a unit increase in the temperature, the number of rented bikes increases by <em>β</em><sub>0</sub> units, while holding the value of <code>hour </code>constant. Or we can say that for a unit increase in the hour, the number of rented bikes increases by <em>β</em><sub>1</sub> units, while holding the value of <code>temperature </code>constant. Also, the value of a coefficient for a given variable is dependent on what other variables we are including in the model. For instance, the <span id="dx1-91016"/>coefficient of <code>temperature </code>will vary depending on whether we incorporate the variable <code>hour </code>into the model or not.</p>&#13;
<p><em>Figure <a href="#x1-91017r19">4.19</a></em> shows the <em>β</em> coefficients for models <code>model_neg </code>(only <code>temperature</code>) and for model <code>model_mld </code>(<code>temperature </code>and <code>hour</code>).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file138.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-91017r19"/><strong>Figure 4.19</strong>: Scaled <em>β</em> coefficients for <code>model_neg </code>and <code>model_mlb</code></p>&#13;
<p>We can see that the coefficient of <code>temperature </code>is different in both models. This is because the effect of <code>temperature </code>on the number of rented bikes depends on the hour of the day. Even more, the values of the <em>β</em> coefficients have been scaled by the standard deviation of their corresponding independent variable, so we can make them comparable. We can see that once we include <code>hour </code>in the model, the effect of <code>temperature </code>on the number of rented <span id="dx1-91018"/>bikes gets smaller. This is because the effect of <code>hour </code>is already explaining some of the variations in the number of rented bikes that were previously explained by <code>temperature</code>. In extreme cases, the addition of a new variable can make the coefficient go to 0 or even change the sign. We will discuss more of this in the next chapter. <span id="x1-91019r210"/></p>&#13;
</section>&#13;
<section id="summary-3" class="level3 sectionHead" data-number="1.8.10">&#13;
<h2 class="sectionHead" data-number="1.8.10">4.10 <span id="x1-9200010"/>Summary</h2>&#13;
<p>In this chapter, we have learned about linear regression, which aims to model the relationship between a dependent variable and an independent variable. We have seen how to use PyMC to fit a linear regression model and how to interpret the results and make plots that we can share with different audiences.</p>&#13;
<p>Our first example was a model with a Gaussian response. But then we saw that this is just one assumption and we can easily change it to deal with non-Gaussian responses, such as count data, using a NegativeBinomial regression model or a logistic regression model for binary data. We saw that when doing so we also need to set an inverse link function to map the linear predictor to the response variable. Using a Student’s t-distribution as the likelihood can be useful for dealing with outliers. We spent most of the chapter modeling the mean as a linear function of the independent variable, but we learned that we can also model other parameters, like the variance. This is useful when we have heteroscedastic data. We learned how to apply the concept of partial pooling to create hierarchical linear regression models. Finally, we briefly discussed multiple linear regression models.</p>&#13;
<p>PyMC makes it very easy to implement all these different flavors of Bayesian linear regression by changing one or a few lines of code. In the next chapter, we will learn more about linear regression and we will learn about Bambi, a tool built on top of PyMC that makes it even easier to build and analyze linear regression models. <span id="x1-92001r213"/></p>&#13;
</section>&#13;
<section id="exercises-3" class="level3 sectionHead" data-number="1.8.11">&#13;
<h2 class="sectionHead" data-number="1.8.11">4.11 <span id="x1-9300011"/>Exercises</h2>&#13;
<ol>&#13;
<li><div id="x1-93002x1">&#13;
<p>Using the howell dataset (available at <a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>), create a linear model of the weight (x) against the height (y). Exclude subjects that are younger than 18. Explain the results.</p>&#13;
</div></li>&#13;
<li><div id="x1-93004x2">&#13;
<p>For four subjects, we get the weights (45.73, 65.8, 54.2, 32.59), but not their heights. Using the model from the previous exercise, predict the height for each subject, together with their 50% and 94% HDIs. Tip: Use <code>pm.MutableData</code>.</p>&#13;
</div></li>&#13;
<li><div id="x1-93006x3">&#13;
<p>Repeat exercise 1, this time including those below 18 years old. Explain the results.</p>&#13;
</div></li>&#13;
<li><div id="x1-93008x4">&#13;
<p>It is known for many species that weight does not scale with height, but with the logarithm of the weight. Use this information to fit the howell data (including subjects from all ages).</p>&#13;
</div></li>&#13;
<li><div id="x1-93010x5">&#13;
<p>See the accompanying code <code>model_t2 </code>(and the data associated with it). Experiment with priors for <em>ν</em>, like the non-shifted Exponential and Gamma priors (they are commented on in the code). Plot the prior distribution to ensure that you understand them. An easy way to do this is to call the <code>pm.sample_prior_predictive() </code>function instead of <code>pm.sample()</code>. You can also use PreliZ.</p>&#13;
</div></li>&#13;
<li><div id="x1-93012x6">&#13;
<p>Rerun <code>model_lrs </code>using the <code>petal_length </code>variable and then the <code>petal_width </code>variable. What are the main differences in the results? How wide or narrow is the 94% HDI in each case?</p>&#13;
</div></li>&#13;
<li><div id="x1-93014x7">&#13;
<p>Repeat the previous exercise, this time using a Student’s t-distribution as a weakly informative prior. Try different values of <em>ν</em>.</p>&#13;
</div></li>&#13;
<li><div id="x1-93016x8">&#13;
<p>Choose a dataset that you find interesting and use it with the simple linear regression model. Be sure to explore the results using ArviZ functions. If you do not have an interesting dataset, try searching online, for example, at <a href="http://data.worldbank.org" class="url">http://data.worldbank.org</a> or <a href="http://www.stat.ufl.edu/~winner/datasets.html" class="url">http://www.stat.ufl.edu/~winner/datasets.html</a>.</p>&#13;
</div></li>&#13;
</ol>&#13;
</section>&#13;
<section id="join-our-community-discord-space-4" class="level3 likesectionHead" data-number="1.8.12">&#13;
<h2 class="likesectionHead" data-number="1.8.12"><span id="x1-9400011"/>Join our community Discord space</h2>&#13;
<p>Join our Discord community to meet like-minded people and learn alongside more than 5000 members at: <a href="https://packt.link/bayesian">https://packt.link/bayesian</a></p>&#13;
<p><img src="../media/file1.png" alt="PIC"/> <span id="x1-94001r167"/></p>&#13;
</section>&#13;
</section>&#13;
</body></html>
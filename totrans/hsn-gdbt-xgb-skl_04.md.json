["```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import cross_val_score\n    import warnings\n    warnings.filterwarnings('ignore')\n    ```", "```py\n    df_census = pd.read_csv('census_cleaned.csv')\n    X_census = df_census.iloc[:,:-1]\n    y_census = df_census.iloc[:,-1]\n    ```", "```py\n    rf = RandomForestClassifier(n_estimators=10, random_state=2, n_jobs=-1)\n    ```", "```py\n    scores = cross_val_score(rf, X_census, y_census, cv=5)\n    ```", "```py\n    print('Accuracy:', np.round(scores, 3))\n    print('Accuracy mean: %0.3f' % (scores.mean()))\n    Accuracy: [0.851 0.844 0.851 0.852 0.851]\n    Accuracy mean: 0.850\n    ```", "```py\n    df_bikes = pd.read_csv('bike_rentals_cleaned.csv')\n    df_bikes.head()\n    ```", "```py\n    X_bikes = df_bikes.iloc[:,:-1]\n    y_bikes = df_bikes.iloc[:,-1]\n    ```", "```py\n    from sklearn.ensemble import RandomForestRegressor\n    rf = RandomForestRegressor(n_estimators=10, random_state=2, n_jobs=-1)\n    ```", "```py\n    scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)\n    ```", "```py\n    rmse = np.sqrt(-scores)\n    print('RMSE:', np.round(rmse, 3))\n    print('RMSE mean: %0.3f' % (rmse.mean()))\n    ```", "```py\n    RMSE: [ 801.486  579.987  551.347  846.698  895.05  1097.522   893.738  809.284  833.488 2145.046]\n    RMSE mean: 945.365\n    ```", "```py\nrf = RandomForestClassifier(oob_score=True, n_estimators=10, random_state=2, n_jobs=-1)\n```", "```py\nrf.fit(X_census, y_census)\n```", "```py\nrf.oob_score_\n```", "```py\n0.8343109855348423\n```", "```py\nrf = RandomForestClassifier(n_estimators=50, oob_score=True, random_state=2, n_jobs=-1)\nrf.fit(X_census, y_census)\nrf.oob_score_\n```", "```py\n0.8518780135745216\n```", "```py\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=2, n_jobs=-1)\nrf.fit(X_census, y_census)\nrf.oob_score_\n```", "```py\n0.8551334418476091\n```", "```py\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    sns.set()\n    ```", "```py\n    oob_scores = []\n    rf = RandomForestClassifier(n_estimators=50, warm_start=True, oob_score=True, n_jobs=-1, random_state=2)\n    ```", "```py\n    rf.fit(X_census, y_census)\n    oob_scores.append(rf.oob_score_)\n    ```", "```py\n    est = 50\n    estimators=[est]\n    ```", "```py\n    for i in range(9):\n        est += 50\n        estimators.append(est)\n        rf.set_params(n_estimators=est)\n        rf.fit(X_census, y_census)\n        oob_scores.append(rf.oob_score_)\n    ```", "```py\n    plt.figure(figsize=(15,7))\n    plt.plot(estimators, oob_scores)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('oob_score_')\n    plt.title('Random Forest Warm Start', fontsize=15)\n    plt.savefig('Random_Forest_Warm_Start', dpi=325)\n    plt.show()\n    ```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)\n```", "```py\nrf = RandomForestRegressor(n_estimators=50, warm_start=True, n_jobs=-1, random_state=2)\nscores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)\nrmse = np.sqrt(-scores)\nprint('RMSE:', np.round(rmse, 3))\nprint('RMSE mean: %0.3f' % (rmse.mean()))\n```", "```py\nRMSE: [ 836.482  541.898  533.086  812.782  894.877  881.117   794.103  828.968  772.517 2128.148]\nRMSE mean: 902.398\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\ndef randomized_search_reg(params, runs=16, reg=RandomForestRegressor(random_state=2, n_jobs=-1)):\n    rand_reg = RandomizedSearchCV(reg, params, n_iter=runs, scoring='neg_mean_squared_error', cv=10, n_jobs=-1, random_state=2)\n    rand_reg.fit(X_train, y_train)\n    best_model = rand_reg.best_estimator_\n    best_params = rand_reg.best_params_\n    print(\"Best params:\", best_params)\n    best_score = np.sqrt(-rand_reg.best_score_)\n    print(\"Training score: {:.3f}\".format(best_score))\n    y_pred = best_model.predict(X_test)\n    from sklearn.metrics import mean_squared_error as MSE\n    rmse_test = MSE(y_test, y_pred)**0.5\n    print('Test set score: {:.3f}'.format(rmse_test))\n```", "```py\nrandomized_search_reg(params={'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05],'min_samples_split':[2, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.1],'min_samples_leaf':[1,2,4,6,8,10,20,30],'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,2,4,6,8,10,20]})\n```", "```py\nBest params: {'min_weight_fraction_leaf': 0.0, 'min_samples_split': 0.03, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.05, 'max_leaf_nodes': 25, 'max_features': 0.7, 'max_depth': None}\nTraining score: 759.076\nTest set score: 701.802\n```", "```py\nrandomized_search_reg(params={'min_samples_leaf': [1,2,4,6,8,10,20,30], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4], 'max_depth':[None,2,4,6,8,10,20]})\n```", "```py\nBest params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 10}\nTraining score: 679.052\nTest set score: 626.541\n```", "```py\nrandomized_search_reg(params={'min_samples_leaf':[1,2,4,6,8,10,20,30],'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,4,6,8,10,12,15,20]}, runs=20)\n```", "```py\nBest params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 12}\nTraining score: 675.128\nTest set score: 619.014\n```", "```py\nrandomized_search_reg(params={'min_samples_leaf':[1,2,3,4,5,6], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.08, 0.10, 0.12, 0.15], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,8,10,12,14,16,18,20]})\n```", "```py\nBest params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.05, 'max_features': 0.7, 'max_depth': 18}\nTraining score: 679.595\nTest set score: 630.954\n```", "```py\nrandomized_search_reg(params={'min_samples_leaf':[1,2,4,6,8,10,20,30], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,4,6,8,10,12,15,20],'n_estimators':[100]}, runs=20)\n```", "```py\nBest params: {'n_estimators': 100, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 12}\nTraining score: 675.128\nTest set score: 619.014\n```", "```py\nrf = RandomForestRegressor(n_estimators=100,  min_impurity_decrease=0.1, max_features=0.6, max_depth=12, warm_start=True, n_jobs=-1, random_state=2)\nscores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)\nrmse = np.sqrt(-scores)\nprint('RMSE:', np.round(rmse, 3))\nprint('RMSE mean: %0.3f' % (rmse.mean()))\n```", "```py\nRMSE: [ 818.354  514.173  547.392  814.059  769.54   730.025  831.376  794.634  756.83  1595.237]\nRMSE mean: 817.162\n```", "```py\nfrom sklearn.utils import shuffle\n```", "```py\ndf_shuffle_bikes = shuffle(df_bikes, random_state=2)\n```", "```py\nX_shuffle_bikes = df_shuffle_bikes.iloc[:,:-1]\ny_shuffle_bikes = df_shuffle_bikes.iloc[:,-1]\nrf = RandomForestRegressor(n_estimators=100,  min_impurity_decrease=0.1, max_features=0.6, max_depth=12, n_jobs=-1, random_state=2)\nscores = cross_val_score(rf, X_shuffle_bikes, y_shuffle_bikes, scoring='neg_mean_squared_error', cv=10)\nrmse = np.sqrt(-scores)\nprint('RMSE:', np.round(rmse, 3))\nprint('RMSE mean: %0.3f' % (rmse.mean()))\n```", "```py\nRMSE: [630.093 686.673 468.159 526.676 593.033 724.575 774.402 672.63  760.253  616.797]\nRMSE mean: 645.329\n```"]
["```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor, XGBClassifier, XGBRFRegressor, XGBRFClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as MSE\n```", "```py\n    X, y = load_diabetes(return_X_y=True)\n    ```", "```py\n    kfold = KFold(n_splits=5, shuffle=True, random_state=2)  \n    ```", "```py\n    def regression_model(model):\n        scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kfold)\n        rmse = (-scores)**0.5\n        return rmse.mean()\n    ```", "```py\n    regression_model(XGBRegressor(booster='gblinear'))\n    ```", "```py\n    55.4968907398679\n    ```", "```py\n    regression_model(LinearRegression())\n    ```", "```py\n    55.50927267834351\n    ```", "```py\n    regression_model(Lasso())\n    ```", "```py\n    62.64900771743497\n    ```", "```py\n    regression_model(Ridge())\n    ```", "```py\n    58.83525077919004\n    ```", "```py\n    regression_model(XGBRegressor(booster='gbtree'))\n    ```", "```py\n    65.96608419624594\n    ```", "```py\n    def grid_search(params, reg=XGBRegressor(booster='gblinear')):\n        grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=kfold)\n        grid_reg.fit(X, y)\n        best_params = grid_reg.best_params_\n        print(\"Best params:\", best_params)\n        best_score = np.sqrt(-grid_reg.best_score_)\n        print(\"Best score:\", best_score)\n    ```", "```py\n    grid_search(params={'reg_alpha':[0.001, 0.01, 0.1, 0.5, 1, 5]})\n    ```", "```py\n    Best params: {'reg_alpha': 0.01}\n    Best score: 55.485310447306425\n    ```", "```py\n    grid_search(params={'reg_lambda':[0.001, 0.01, 0.1, 0.5, 1, 5]})\n    ```", "```py\n    Best params: {'reg_lambda': 0.001}\n    Best score: 56.17163554152289\n    ```", "```py\n    grid_search(params={'feature_selector':['shuffle']})\n    ```", "```py\n    Best params: {'feature_selector': 'shuffle'}\n    Best score: 55.531684115240594\n    ```", "```py\n    grid_search(params={'feature_selector':['random', 'greedy', 'thrifty'], 'updater':['coord_descent'] })\n    ```", "```py\n    Best params: {'feature_selector': 'thrifty', 'updater': 'coord_descent'}\n    Best score: 55.48798105805444\n    This is a slight improvement from the base score.\n    ```", "```py\n    grid_search(params={'feature_selector':['greedy', 'thrifty'], 'updater':['coord_descent'], 'top_k':[3, 5, 7, 9]})\n    ```", "```py\n    Best params: {'feature_selector': 'thrifty', 'top_k': 3, 'updater': 'coord_descent'}\n    Best score: 55.478623763746256\n    ```", "```py\n    X = np.arange(1,100)\n    ```", "```py\n    np.random.seed(2) \n    ```", "```py\n    y = []\n    ```", "```py\n    for i in X:\n           y.append(i * np.random.uniform(-0.2, 0.2))\n    ```", "```py\n    y = np.array(y)\n    ```", "```py\n    X = X.reshape(X.shape[0], 1)\n    y = y.reshape(y.shape[0], 1)\n    ```", "```py\nregression_model(XGBRegressor(booster='gblinear', objective='reg:squarederror'))\n```", "```py\n6.214946302686011\n```", "```py\nregression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror'))\n```", "```py\n9.37235946501318\n```", "```py\nregression_model(LinearRegression())\n```", "```py\n6.214962315808842\n```", "```py\n    X, y = load_diabetes(return_X_y=True)\n    ```", "```py\n    regression_model(XGBRegressor(booster='dart', objective='reg:squarederror'))\n    ```", "```py\n    65.96444746130739\n    ```", "```py\n    df_census = pd.read_csv('census_cleaned.csv')\n    X_census = df_census.iloc[:, :-1]\n    y_census = df_census.iloc[:, -1]\n    ```", "```py\n    def classification_model(model):\n        scores = cross_val_score(model, X_census, y_census, scoring='accuracy', cv=kfold)\n        return scores.mean()\n    ```", "```py\n    classification_model(XGBClassifier(booster='gbtree'))\n    ```", "```py\n    0.8701208195968675\n    ```", "```py\n    classification_model(XGBClassifier(booster='dart')\n    ```", "```py\n    0.8701208195968675\n    ```", "```py\n    classification_model(XGBClassifier(booster='gblinear'))\n    ```", "```py\n    0.8501275704120015\n    ```", "```py\n    classification_model(LogisticRegression(max_iter=1000))\n    ```", "```py\n    0.8008968643699182\n    ```", "```py\nclassification_model(XGBClassifier(booster='dart', one_drop=1))\n```", "```py\n0.8718714338474818\n```", "```py\n    regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', sample_type='weighted'))\n    ```", "```py\n    65.96444746130739\n    ```", "```py\n    regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', normalize_type='forest'))\n    ```", "```py\n    65.96444746130739\n    ```", "```py\n    regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))\n    ```", "```py\n    61.81275131335009\n    ```", "```py\ngrid_search(params={'rate_drop':[0.01, 0.1, 0.2, 0.4]}, reg=XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))\n```", "```py\nBest params: {'rate_drop': 0.2}\nBest score: 61.07249602732062\n```", "```py\ngrid_search(params={'skip_drop':[0.01, 0.1, 0.2, 0.4]}, reg=XGBRegressor(booster='dart', objective='reg:squarederror'))\n```", "```py\nBest params: {'skip_drop': 0.1}\nBest score: 62.879753748627635\n```", "```py\n    regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror', num_parallel_tree=25))\n    ```", "```py\n    65.96604877151103\n    ```", "```py\n    regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror', num_parallel_tree=5))\n    ```", "```py\n    65.96445649315855\n    ```", "```py\n    regression_model(XGBRFRegressor(objective='reg:squarederror'))\n    ```", "```py\n    59.447250741400595\n    ```", "```py\n    regression_model(RandomForestRegressor())\n    ```", "```py\n    59.46563031802505\n    ```", "```py\n    classification_model(XGBRFClassifier())\n    ```", "```py\n    0.856085650471878\n    ```", "```py\n    classification_model(RandomForestClassifier())\n    ```", "```py\n    0.8555328202034789\n    ```"]
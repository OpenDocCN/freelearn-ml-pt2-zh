["```py\njupyter notebook\n```", "```py\nfrom urllib.request import urlretrieve, ProxyHandler, build_opener, install_opener\nimport requests\nimport os\npfx = \"https://archive.ics.uci.edu/ml/machine-learning databases/spambase/\"\ndata_dir = \"data\"\n# We might need to set a proxy handler...\ntry:\n    proxies = {\"http\": os.environ['http_proxy'],\n               \"https\": os.environ['https_proxy']}\n    print(\"Found proxy settings\")\n    #create the proxy object, assign it to a variable\n    proxy = ProxyHandler(proxies)\n    # construct a new opener using your proxy settings\n    opener = build_opener(proxy)\n    # install the opener on the module-level\n    install_opener(opener)\n\nexcept KeyError:\n    pass\n# The following will download the data if you don't already have it...\ndef get_data(link, where):\n    # Append the prefix\n    link = pfx + link\n```", "```py\nimport pandas as pd\nnames = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \n         \"word_freq_3d\", \"word_freq_our\", \"word_freq_over\", \n         \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\",\n         \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\", \n         \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \n         \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \n         \"word_freq_you\", \"word_freq_credit\", \"word_freq_your\", \n         \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \n         \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \n         \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \n         \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \n         \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\", \n         \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \n         \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \n         \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \n         \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \n         \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \n         \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\", \n         \"capital_run_length_longest\", \"capital_run_length_total\",\n         \"is_spam\"]\ndf = pd.read_csv(os.path.join(\"data\", \"spam.csv\"), header=None, names=names)\n# pop off the target\ny = df.pop(\"is_spam\")\ndf.head()\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42, stratify=y)\nprint(\"Num training samples: %i\" % X_train.shape[0])\nprint(\"Num test samples: %i\" % X_test.shape[0])\n```", "```py\nNum training samples: 3680\nNum test samples: 921\n```", "```py\nfrom packtml.utils.plotting import plot_learning_curve\nfrom packtml.decision_tree import CARTClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# very basic decision tree\nplot_learning_curve(\n        CARTClassifier, metric=accuracy_score,\n        X=X_train, y=y_train, n_folds=3, seed=21, trace=True,\n        train_sizes=(np.linspace(.25, .75, 4) * X_train.shape[0]).astype(int),\n        max_depth=8, random_state=42)\\\n    .show()\n```", "```py\ndecision_tree = CARTClassifier(X_train, y_train, random_state=42, max_depth=8)\n```", "```py\nfrom packtml.regression import SimpleLogisticRegression\n# simple logistic regression classifier\nplot_learning_curve(\n        SimpleLogisticRegression, metric=accuracy_score,\n        X=X_train, y=y_train, n_folds=3, seed=21, trace=True,\n        train_sizes=(np.linspace(.25, .8, 4) *     X_train.shape[0]).astype(int),\n        n_steps=250, learning_rate=0.0025, loglik_interval=100)\\\n    .show()\n```", "```py\nspam_email = \"\"\"\nDear small business owner,\n\nThis email is to inform you that for $0 down, you can receive a \nFREE CREDIT REPORT!!! Your money is important; PROTECT YOUR CREDIT and \nreply direct to us for assistance!\n\"\"\"\n\nprint(spam_email)\n```", "```py\nDear small business owner,\n\nThis email is to inform you that for $0 down, you can receive a \nFREE CREDIT REPORT!!! Your money is important; PROTECT YOUR CREDIT and \nreply direct to us for assistance!\n```", "```py\nfrom collections import Counter\nimport numpy as np\ndef encode_email(email):\n    # tokenize the email\n    tokens = email.split()\n\n    # easiest way to count characters will be to join everything\n    # up and split them into chars, then use a counter to count them\n    # all ONE time.\n    chars = list(\"\".join(tokens))\n    char_counts = Counter(chars)\n    n_chars = len(chars)\n\n    # we can do the same thing with \"tokens\" to get counts of words\n    # (but we want them to be lowercase!)\n    word_counts = Counter([t.lower() for t in tokens])\n\n    # Of the names above, the ones that start with \"word\" are\n    # percentages of frequencies of words. Let's get the words\n    # in question\n    freq_words = [ \n        name.split(\"_\")[-1]\n        for name in names \n        if name.startswith(\"word\")\n    ]\n\n    # compile the first 48 values using the words in question\n    word_freq_encodings = [100\\. * (word_counts.get(t, 0) / len(tokens))\n                           for t in freq_words]\n```", "```py\n # make a np array to compute the next few stats quickly\ncapital_runs = np.asarray(capital_runs)\n    capital_stats = [capital_runs.mean(), \n                     capital_runs.max(), \n                     capital_runs.sum()]\n\n```", "```py\n# get the email vectors\nfake_email = encode_email(spam_email)\nreal_email = encode_email(not_spam)\n# this is what they look like:\nprint(\"Spam email:\")\nprint(fake_email)\nprint(\"\\nReal email:\")\nprint(real_email)\n```", "```py\npredict = (lambda rec, mod: \"SPAM!\" if mod.predict([rec])[0] == 1 else \"Not spam\")\n\nprint(\"Decision tree predictions:\")\nprint(\"Spam email prediction: %r\" % predict(fake_email, decision_tree))\nprint(\"Real email prediction: %r\" % predict(real_email, decision_tree))\n\nprint(\"\\nLogistic regression predictions:\")\nprint(\"Spam email prediction: %r\" % predict(fake_email, logistic_regression))\nprint(\"Real email prediction: %r\" % predict(real_email, logistic_regression))\n```", "```py\ncat environment.yml conda env create -f environment.yml\n```", "```py\n source activate packt-sml\n```", "```py\ncat setup.py\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\nrs = np.random.RandomState(42)\nX,y = make_classification(n_samples=10, random_state=rs)\n```", "```py\ndef make_life_alterning_decision(X):\n    \"\"\"Determine whether something big happens\"\"\"\n    row_sums = X.sum(axis=1)\n    return (row_sums > 0).astype(int)\nmake_life_alterning_decision(X)\n```", "```py\narray([0, 1, 0, 0, 1, 1, 1, 0, 1, 0])\n```", "```py\ndef make_more_complex_life_alterning_decision(X):\n    \"\"\"Make a more complicated decision about something big\"\"\"   \n    row_sums = X.sum(axis=1)\n      return ((X.min(axis=1) < -1.5) &\n              ((row_sums >= 0.) |\n               (row_sums % 0.5 == 0) |\n               (X.max(axis=1) > 1.5))).astype(int)\n\nmake_more_complex_life_alterning_decision(X) \n```", "```py\narray([0, 1, 1, 1, 1, 1, 0, 1, 1, 0])\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\ndef learn_life_lession(X, y):\n    \"\"\"Learn a lesson abd apply it in a future situation\"\"\"\n    model = LogisticRegression().fit(X, y)\n    return (lambda X: model.predict(X))\neducated_decision = learn_life_lession(X, y)(X)\neducated_decision\n```", "```py\narray([1, 1, 0, 0, 0, 1, 1, 0, 1, 0])\n```", "```py\nimport numpy as np\n\nseed = (42)\n\nX = np.random.RandomState(seed).rand(5, 3).round(4)\n\ny = np.array([1, 1, 0, 1, 0])\n\nh = (lambda X: 1\\. / (1\\. + np.exp(-X)))\n\ntheta = np.zeros(3)\n\nlam = 0.05\n\ndef iteration(theta):\n\n    y_hat = h(X.dot(theta))\n\n    residuals = y - y_hat\n\n    gradient = X.T.dot(residuals)\n    theta += gradient * lam\n    print(\"y hat: %r\" % y_hat.round(3).tolist())\n    print(\"Gradient: %r\" % gradient.round(3).tolist())\n    print(\"New theta: %r\\n\" % theta.round(3).tolist())\n\niteration(theta)\niteration(theta)\n```", "```py\ny hat: [0.5, 0.5, 0.5, 0.5, 0.5]\nGradient: [0.395, 0.024, 0.538]\nNew theta: [0.02, 0.001, 0.027]\n\ny hat: [0.507, 0.504, 0.505, 0.51, 0.505]\nGradient: [0.378, 0.012, 0.518]\nNew theta: [0.039, 0.002, 0.053]\n\n```", "```py\nfrom sklearn.datasets import load_boston\n\nfrom sklearn.model_selection import train_test_split\n\nboston_housing = load_boston() # load data\n\nX, y = boston_housing.data, boston_housing.target # get X, y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n\n                                                                                                              random_state=42)\n\n# show num samples (there are no duplicates in either set!)\nprint(\"Num train samples: %i\" % X_train.shape[0])\n\nprint(\"Num test samples: %i\" % X_test.shape[0])\n```", "```py\nNum train samples: 404\nNum test samples: 102\n```"]
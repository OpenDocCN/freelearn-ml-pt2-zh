<html><head></head><body>
		<div>
			<div id="_idContainer323" class="Content">
			</div>
		</div>
		<div id="_idContainer324" class="Content">
			<h1 id="_idParaDest-192"><a id="_idTextAnchor195"/>9. Hotspot Analysis</h1>
		</div>
		<div id="_idContainer354" class="Content">
			<p class="callout-heading">Overview<a id="_idTextAnchor196"/></p>
			<p class="callout">In this chapter, we will perform hotspot analysis. We will also visualize the results of hotspot analysis. We will use kernel density estimation, which is the most popular algorithm for building distributions using a collection of observations. We will build kernel density estimation models. We will describe the fundamentals behind probability density functions. By the end of the chapter, you should be able to leverage Python libraries to build multi-dimensional density estimation models and work with geo-spatial data.</p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor197"/>Introduction</h1>
			<p>In the preceding chapter, we explored market basket analysis. Market basket analysis, as you hopefully recall, is an algorithm that seeks to understand the relationships between all the items and groups of items in transaction data. These relationships are then leveraged to help retailers optimize store layouts, more accurately order inventory, and adjust prices without shrinking the number of items in each transaction. We now change directions to explore hotspot modeling.</p>
			<p>Let's consider an imaginary scenario: a new disease has begun spreading through numerous communities in the country that you live in and the government is trying to figure out how to confront this health emergency. Critical to any plan to confront this health emergency is epidemiological knowledge, including where the patients are located and how the disease is moving. The ability to locate and quantify problem areas (which are classically referred to as hotspots) can help health professionals, policy makers, and emergency response teams craft the most effective and efficient strategies for combating the disease. This scenario highlights one of the many applications of hotspot modeling.</p>
			<p><strong class="bold">Hotspot modeling</strong> is an approach that is used to identify how a population is distributed across a geographical area; for example, how the population of individuals infected with the previously mentioned disease is spread across the country. The creation of this distribution relies on the availability of representative sample data. Note that the population can be anything definable in geographical terms, which includes, but is not limited to, crime, disease-infected individuals, people with certain demographic characteristics, or hurricanes:</p>
			<div>
				<div id="_idContainer325" class="IMG---Figure">
					<img src="image/B15923_09_01.jpg" alt="Figure 9.1: A fabricated example of fire location data showing some potential hotspots&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1: A fabricated example of fire location data showing some potential hotspots</p>
			<p>Hotspot analysis is incredibly popular, and this is mainly because of how easy it is to visualize and interpret the results. Newspapers, websites, blogs, and TV shows all leverage hotspot analysis to support the arguments, chapters, and topics included in or on them. While it might not be as well-known as the most popular machine learning models, the main hotspot analysis algorithm, known as <strong class="bold">kernel density estimation</strong>, is arguably one of the most widely used analytical techniques. People even perform kernel density estimation mentally on a daily basis without knowing it. Kernel density estimation is a hotspot analysis technique that is used to estimate the true population distribution of specific geographical events. Before getting into the algorithm itself, we need to briefly review spatial statistics and probability density functions.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor198"/>Spatial Statistics</h1>
			<p><strong class="bold">Spatial statistics</strong> is the branch of statistics that focuses on the analysis of data that has spatial properties, including geographic or topological coordinates. It is similar to time series analysis in that the goal is to analyze data that changes across some dimension. In the case of time series analysis, the dimension across which the data changes is time, whereas in the spatial statistics case, the data changes across the spatial dimension. There are a number of techniques that are included under the spatial statistics umbrella, but the technique we are concerned with here is kernel density estimation. As is the goal of most statistical analyses, in spatial statistics, we are trying to take samples of geographic data and use them to generate insights and make predictions. The analysis of earthquakes is one arena in which spatial statistical analyses are commonly deployed. By collecting earthquake location data, maps that identify areas of high and low earthquake likelihood can be generated, which can help scientists determine both where future earthquakes are likely to occur and what to expect in terms of intensity.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor199"/>Probability Density Functions</h2>
			<p>Kernel density estimation uses the idea of the <strong class="bold">Probability Density Function</strong> (<strong class="bold">PDF</strong>), which is one of the foundational concepts in statistics. The probability density function is a function that describes the behavior of a continuous <strong class="bold">random variable</strong>. That is, it expresses the likelihood, or probability, that the random variable takes on some range of values. Consider the heights of males in the United States as an example. Using the probability density function of the heights of males in the United States, we could calculate the probability that some United States-based male is between 1.9 and 1.95 meters tall.</p>
			<div>
				<div id="_idContainer326" class="IMG---Figure">
					<img src="image/B15923_09_02.jpg" alt="Figure 9.2: The standard normal distribution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2: The standard normal distribution</p>
			<p>Possibly the most popular density function in statistics is the standard normal distribution, which is simply the normal distribution centered at zero with a standard deviation equal to one.</p>
			<p>Instead of the density function, what is typically available to statisticians or data scientists are randomly collected sample values coming from a population distribution that is unknown. This is where kernel density estimation comes in; it is a technique that is used for estimating the unknown probability density function of a random variable using sample data. The following figure represents a simple, but somewhat more reasonable example of a distribution that we would want to estimate with kernel density estimation. We would take some number of observations (sample data points) and use those observations to create a smooth distribution that mimics the true underlying distribution that is unknown to us.</p>
			<div>
				<div id="_idContainer327" class="IMG---Figure">
					<img src="image/B15923_09_03.jpg" alt="Figure 9.3: A mixture of three normal distributions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3: A mixture of three normal distributions</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor200"/>Using Hotspot Analysis in Business</h2>
			<p>We have already mentioned some of the ways in which hotspot modeling can be leveraged to meaningfully impact industry. When reporting on infectious diseases, health organizations and media companies typically use hotspot analysis to communicate where the diseases are located and the likelihood of contracting the disease based on geographic location. Using hotspot analysis, this information could be reliably computed and disseminated. Hotspot analysis is great for dealing with health data because the visualizations are very straightforward. This means that the chances of data being misinterpreted either intentionally or unintentionally are relatively low.</p>
			<p>Hotspot analysis can also be used to predict where certain events are likely to occur geographically. One research area that is leveraging the predictive capabilities of hotspot analysis more and more are the environmental sciences, which includes the study of natural disasters and extreme weather events. Earthquakes, for example, are notorious for being difficult to predict, because the time between significant earthquakes can be large, and the machinery needed to track and measure earthquakes to the degree required to make these predictions is relatively new.</p>
			<p>In terms of public policy and resource deployment, hotspot analysis can be very impactful when dealing with the analysis of population demographics. Determining where resources, both monetary and personnel, should be deployed can be challenging; however, given that resources are often demographic-specific, hotspot analysis is a useful technique since it can be used to determine the distribution of certain demographic characteristics. By demographic characteristics we mean that we could find the geographic distribution of high school graduates, immigrants from a specific global region, or individuals making $100,000 or more annually.</p>
			<p>The number of applications of hotspot modeling are virtually endless. We have here only discussed three of the major ones.</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor201"/>Kernel Density Estimation</h1>
			<p>One of the main methodological approaches to hotspot analysis is kernel density estimation. Kernel density estimation builds an estimated density using sample data and two parameters known as the <strong class="bold">kernel function</strong> and the <strong class="bold">bandwidth value</strong>. The estimated density is, like any distribution, essentially a guideline for the behavior of a random variable. Here, we mean how frequently the random variable takes on any specific value, {x<span class="subscript">1</span>, ….., x<span class="subscript">n</span>}. When dealing with hotspot analysis where the data is typically geographic, the estimated density answers the question <em class="italic">How frequently do specific longitude and latitude pairs appear for a given event?</em> If a specific longitude and latitude pair {x<span class="subscript">longitude</span>, x<span class="subscript">latitude</span>} and other nearby pairs occur with high frequency, then the estimated density built using the sample data will show that the area around the aforementioned longitude and latitude pair occurs with high likelihood.</p>
			<p>Kernel density estimation is referred to as a smoothing algorithm because a smooth curve is drawn over the sample data, which, if the data is a representative sample, can be a good estimate of the true population density function. Stated another way, when it is done correctly, kernel density estimation aims to remove the noise that is inherent in sampled data, but is not a feature of the total population. The only assumption of the model is that the data truly belongs to some interpretable and meaningful density from which insights can be derived and acted upon. That is, there exists a true underlying distribution. We assume that the sample data contains clusters of data points and that these clusters align to regions of high likelihood in the true population. A benefit of creating a quality estimate of the true population density is that the estimated density can then be used to sample more data from the population.</p>
			<p>Following this brief introduction, you probably have the following two questions:</p>
			<ul>
				<li>What is the bandwidth value?</li>
				<li>What is the kernel function?</li>
			</ul>
			<p>We answer both of these questions next.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor202"/>The Bandwidth Value</h2>
			<p>The most crucial parameter in kernel density estimation is called the <strong class="bold">bandwidth value</strong> and its impact on the quality of the estimate cannot be overestimated. A high-level definition of the bandwidth value is that it is a value that determines the degree of smoothing. If the bandwidth value is low, then the estimated density will feature limited smoothing, which means that the density will capture all the noise in the sample data. If the bandwidth value is high, then the estimated density will be very smooth. An overly smooth density will remove characteristics of the true density from the estimated density, which are legitimate and not noise.</p>
			<p>In more statistical parlance, the bandwidth parameter controls the bias-variance trade-off. That is, high variance is the result of low bandwidth values because the density is sensitive to the variance of the sample data. Low bandwidth values limit any ability the model may have had to adapt to and work around gaps in the sample data that are not present in the population. A density estimated using a low bandwidth value will tend to overfit the data (this is also known as an under-smoothed density). When high bandwidth values are used, then the resulting density is underfit and the estimated density has a high bias (this is also known as an over-smoothed density).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In all the subsequent exercises and activities, the output could vary slightly from what is shown below. This is because of the following: differences in sampled data can lead to slightly different output and the <strong class="source-inline">sklearn</strong> and <strong class="source-inline">seaborn</strong> libraries have some non-deterministic elements that could cause the results to change from run to run.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor203"/>Exercise 9.01: The Effect of the Bandwidth Value</h2>
			<p>In this exercise, we will fit nine different models with nine different bandwidth values to sample data created in the exercise. The goal here is to solidify our understanding of the impact the bandwidth parameter can have and make clear that if an accurate estimated density is sought, then the bandwidth value needs to be selected with care. Note that finding an optimal bandwidth value will be the topic of the next section. All exercises will be done in a Jupyter notebook utilizing Python 3; ensure that all package installation is done using <strong class="source-inline">pip</strong>. The easiest way to install the <strong class="source-inline">basemap</strong> module from <strong class="source-inline">mpl_toolkits</strong> is by using <em class="italic">Anaconda</em>. Instructions for downloading and installing <em class="italic">Anaconda</em> can be found at the beginning of this book:</p>
			<ol>
				<li>Load all of the libraries that are needed for the exercises in this chapter. The <strong class="source-inline">basemap</strong> library is used to create graphics involving location data. All the other libraries have been used previously in this title.<p class="source-code">get_ipython().run_line_magic('matplotlib', 'inline')</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import mpl_toolkits.basemap</p><p class="source-code">import numpy</p><p class="source-code">import pandas</p><p class="source-code">import scipy.stats</p><p class="source-code">import seaborn</p><p class="source-code">import sklearn.model_selection</p><p class="source-code">import sklearn.neighbors</p><p class="source-code">seaborn.set()</p></li>
				<li>Create some sample data (<strong class="source-inline">vals</strong>) by mixing three normal distributions. In addition to the sample data, define the true density curve (<strong class="source-inline">true_density</strong>) and the range over which the data will be plotted (<strong class="source-inline">x_vec</strong>):<p class="source-code">x_vec = numpy.linspace(-30, 30, 10000)[:, numpy.newaxis]</p><p class="source-code">numpy.random.seed(42)</p><p class="source-code">vals = numpy.concatenate(( \</p><p class="source-code">       numpy.random.normal(loc=1, scale=2.5, size=500), \</p><p class="source-code">       numpy.random.normal(loc=10, scale=4, size=500), \</p><p class="source-code">       numpy.random.normal(loc=-12, scale=5, size=500) \</p><p class="source-code">))[:, numpy.newaxis]</p><p class="source-code">true_density = ((1 / 3) * scipy.stats.norm(1, 2.5)\</p><p class="source-code">                          .pdf(x_vec[:, 0]) \</p><p class="source-code">                + (1 / 3) * scipy.stats.norm(10, 4)\</p><p class="source-code">                            .pdf(x_vec[:, 0]) \</p><p class="source-code">                + (1 / 3) * scipy.stats.norm(-12, 5)\</p><p class="source-code">                            .pdf(x_vec[:, 0]))</p></li>
				<li>Define a list of tuples that will guide the creation of the multiplot graphic. Each tuple contains the row and column indices of the specific subplot, and the bandwidth value used to create the estimated density in that particular subplot. Note that, for the sake of this exercise, the bandwidth values are picked randomly, but there is some strategy that goes into picking optimal bandwidth values. We will dig more into this in the next section.<p class="source-code">position_bandwidth_vec = [(0, 0, 0.1), (0, 1, 0.4), (0, 2, 0.7), \</p><p class="source-code">                          (1, 0, 1.0), (1, 1, 1.3), (1, 2, 1.6), \</p><p class="source-code">                          (2, 0, 1.9), (2, 1, 2.5), (2, 2, 5.0)]</p></li>
				<li>Create nine plots each using a different bandwidth value. The first plot, with the index of (0, 0), will have the lowest bandwidth value and the last plot, with the index of (2, 2), will have the highest. These values are not the absolute lowest or absolute highest bandwidth values, rather they are only the minimum and maximum of the list defined in the previous step:<p class="source-code">fig, ax = plt.subplots(3, 3, sharex=True, \</p><p class="source-code">                       sharey=True, figsize=(12, 9))</p><p class="source-code">fig.suptitle('The Effect of the Bandwidth Value', fontsize=16)</p><p class="source-code">for r, c, b in position_bandwidth_vec:</p><p class="source-code">    kde = sklearn.neighbors.KernelDensity(bandwidth=b).fit(vals)</p><p class="source-code">    log_density = kde.score_samples(x_vec)</p><p class="source-code">    ax[r, c].hist(vals, bins=50, density=True, alpha=0.5)</p><p class="source-code">    ax[r, c].plot(x_vec[:, 0], numpy.exp(log_density), \</p><p class="source-code">                  '-', linewidth=2)</p><p class="source-code">    ax[r, c].set_title('Bandwidth = {}'.format(b))</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer328" class="IMG---Figure"><img src="image/B15923_09_04.jpg" alt="Figure 9.4: A 3 x 3 matrix of subplots&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.4: A 3 x 3 matrix of subplots</p>
			<p>Notice that the estimated density curve in the ninth subplot (where the bandwidth is 5) clearly underfits the data. As the bandwidth values increase, the estimated density becomes smoother until it noticeably underfits the data. Visually, it looks like the optimal bandwidth may be around <strong class="source-inline">1.6</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2UOHbTZ">https://packt.live/2UOHbTZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38DbmTo">https://packt.live/38DbmTo</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<p>The next step is to design an algorithm to identify the optimal bandwidth value, so that the estimated density is the most reasonable and, therefore, the most reliable and actionable.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor204"/>Selecting the Optimal Bandwidth</h2>
			<p>As mentioned in the preceding exercise, we can come quite close to selecting the optimal bandwidth by simply comparing several densities visually. However, this is neither the most efficient method of selecting parameter values nor the most reliable. </p>
			<p>There are two standard approaches to optimizing the bandwidth value, and both of these will appear in future exercises and activities. The first approach is a plug-in method (or a formulaic approach) that is deterministic and not optimized on the sample data. Plug-in methods are generally much faster to implement, simpler to code, and easier to explain. However, these methods have one big downside, which is that their accuracy tends to suffer compared to approaches that are optimized on the sample data. These methods also have distributional assumptions. The most popular plug-in methods are Silverman's Rule and Scott's Rule. Explaining these rules in detail is beyond the scope of this text, not necessary for fully understanding kernel density estimation, and would require some tricky mathematical work, so we will skip any further exploration here. That being said, if interested, there are a number of great sources publicly available that explain these rules at various levels of detail. By default, the <strong class="source-inline">seaborn</strong> package (which will be used in future exercises) uses Scott's Rule as the method to determine the bandwidth value. </p>
			<p>The second, and arguably the more robust, approach to finding an optimal bandwidth value is by searching a predefined grid of bandwidth values. Grid search is an empirical approach that is used frequently in machine learning and predictive modeling to optimize model hyperparameters. The process starts by defining the bandwidth grid, which is simply the collection of bandwidth values to be evaluated. The bandwidth grid is chosen at random. Use each bandwidth value in the grid to create an estimated density; then, score the estimated density using the pseudo-log-likelihood value. The optimal bandwidth value is that which has the maximum pseudo-log-likelihood value. Think of the pseudo-log-likelihood value as the probability of getting data points where we got data points and the probability of not getting points where we did not get any data points. Ideally, both of these probabilities would be large. Consider the case where the probability of getting data points where we did get points is low. In this situation, the implication would be that the data points in the sample were anomalous because, under the true distribution, getting points where we did would not be expected with a high likelihood value. The pseudo-log-likelihood value is an evaluation metric that plays the same role as the accuracy score in classification problems and root mean squared error in regression problems.</p>
			<p>Let's now implement the grid search approach to optimize the bandwidth value.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor205"/>Exercise 9.02: Selecting the Optimal Bandwidth Using Grid Search</h2>
			<p>In this exercise, we will create an estimated density for the sample data created in <em class="italic">Exercise 9.01</em>, <em class="italic">The Effect of the Bandwidth Value</em> with an optimal bandwidth value identified using grid search and cross-validation. To run the grid search with cross-validation, we will leverage <strong class="source-inline">sklearn</strong>, which we have used throughout this book. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This exercise is a continuation of <em class="italic">Exercise 9.01</em>, <em class="italic">The Effect of the Bandwidth Value</em> as we are using the same sample data and continuing our exploration of the bandwidth value.</p>
			<ol>
				<li value="1">Define a grid of bandwidth values and the grid search cross-validation model. Ideally, the leave-one-out approach to cross-validation should be used, but for the sake of having the model run in a reasonable amount of time, we will do a 10-fold cross-validation. Fit the model on the sample data, as follows:<p class="source-code"># define a grid of 100 possible bandwidth values</p><p class="source-code">bandwidths = 10 ** numpy.linspace(-1, 1, 100)</p><p class="source-code"># define the grid search cross validation model</p><p class="source-code">grid = sklearn.model_selection.GridSearchCV\</p><p class="source-code">       (estimator=sklearn.neighbors.KernelDensity(),\</p><p class="source-code">        param_grid={"bandwidth": bandwidths},\</p><p class="source-code">        cv=10)</p><p class="source-code"># run the model on the previously defined data</p><p class="source-code">grid.fit(vals)</p><p>The output is as follows:</p><div id="_idContainer329" class="IMG---Figure"><img src="image/B15923_09_05.jpg" alt="Figure 9.5: Output of cross-validation model&#13;&#10;"/></div><p class="figure-caption">Figure 9.5: Output of cross-validation model</p></li>
				<li>Extract the optimal bandwidth value from the model. The <strong class="source-inline">best_params_</strong> function extracts from the model object the best performing parameters in the grid.<p class="source-code">best_bandwidth = grid.best_params_["bandwidth"]</p><p class="source-code">print("Best Bandwidth Value: {}" \</p><p class="source-code">      .format(best_bandwidth))</p><p>The optimal bandwidth value should be approximately <strong class="source-inline">1.6</strong>. We can interpret the optimal bandwidth value as the bandwidth value producing the maximum pseudo-log-likelihood value. Note that depending on the values included in the grid, the optimal bandwidth value can change.</p></li>
				<li>Plot the histogram of the sample data overlaid by both the true and estimated densities. In this case, the estimated density will be the optimal estimated density:<p class="source-code">fig, ax = plt.subplots(figsize=(14, 10))</p><p class="source-code">ax.hist(vals, bins=50, density=True, alpha=0.5, \</p><p class="source-code">        label='Sampled Values')</p><p class="source-code">ax.fill(x_vec[:, 0], true_density,\</p><p class="source-code">        fc='black', alpha=0.3, label='True Distribution')</p><p class="source-code">log_density = numpy.exp(grid.best_estimator_\</p><p class="source-code">                        .score_samples(x_vec))</p><p class="source-code">ax.plot(x_vec[:, 0], log_density,\</p><p class="source-code">        '-', linewidth=2, label='Kernel = Gaussian')</p><p class="source-code">ax.legend(loc='upper right')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer330" class="IMG---Figure"><img src="image/B15923_09_06.jpg" alt="Figure 9.6: A histogram of the random sample &#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.6: A histogram of the random sample </p>
			<p>In this histogram, the true density and the optimal estimated density are overlaid. The estimated density is neither overfit or underfit to any noticeable degree and it definitely captures the three clusters. Arguably, it could map to the true density better, but this is just an estimated density generated by a model that is limited by the dataset provided.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2UOHbTZ">https://packt.live/2UOHbTZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38DbmTo">https://packt.live/38DbmTo</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<p>Let's now move onto the second question: what is the kernel function and what role does it play?</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor206"/>Kernel Functions</h2>
			<p>The other parameter to be set is the kernel function. The kernel is a non-negative function that controls the shape of the density. Like topic models, we are working in a non-negative environment because it does not make sense to have negative likelihoods or probabilities. The kernel function controls the shape of the estimated density by weighting the points in a systematic way. This systematic methodology for weighting is fairly simple; data points that are in close proximity to many other data points are up-weighted, whereas data points that are alone or far away from any other data points are down-weighted. Up-weighted data points will correspond to points of higher likelihood in the final estimated density.</p>
			<p>Many functions can be used as kernels, but six frequent choices are Gaussian, Tophat, Epanechnikov, Exponential, Linear, and Cosine. Each of these functions represents a unique distributional shape. Note that in each of the formulas the parameter, <em class="italic">h</em>, represents the bandwidth value:</p>
			<ul>
				<li>Gaussian: Each observation has a bell-shaped weight.<div id="_idContainer331" class="IMG---Figure"><img src="image/B15923_09_07.jpg" alt="Figure 9.7: The formula for the Gaussian kernel&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 9.7: The formula for the Gaussian kernel</p>
			<ul>
				<li>Tophat: Each observation has a rectangular-shaped weight.<div id="_idContainer332" class="IMG---Figure"><img src="image/B15923_09_08.jpg" alt="Figure 9.8: The formula for the Tophat kernel&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 9.8: The formula for the Tophat kernel</p>
			<ul>
				<li>Epanechnikov: Each observation has a mound-shaped weight.</li>
			</ul>
			<div>
				<div id="_idContainer333" class="IMG---Figure">
					<img src="image/B15923_09_09.jpg" alt="Figure 9.9: The formula for the Epanechnikov kernel&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9: The formula for the Epanechnikov kernel</p>
			<ul>
				<li>Exponential: Each observation has a triangular-shaped weight. The sides of the triangle are concave.</li>
			</ul>
			<div>
				<div id="_idContainer334" class="IMG---Figure">
					<img src="image/B15923_09_10.jpg" alt="Figure 9.10: The formula for the Exponential kernel&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10: The formula for the Exponential kernel</p>
			<ul>
				<li>Linear: Each observation has a triangular-shaped weight.</li>
			</ul>
			<div>
				<div id="_idContainer335" class="IMG---Figure">
					<img src="image/B15923_09_11.jpg" alt="Figure 9.11: The formula for the Linear kernel&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11: The formula for the Linear kernel</p>
			<ul>
				<li>Cosine: Each observation has a mound-shaped weight. This mound-shape is narrower at the top than the Epanechnikov kernel.</li>
			</ul>
			<div>
				<div id="_idContainer336" class="IMG---Figure">
					<img src="image/B15923_09_12.jpg" alt="Figure 9.12: The formula for the Cosine kernel&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12: The formula for the Cosine kernel</p>
			<p>Here are the distributional shapes of the six kernel functions:</p>
			<div>
				<div id="_idContainer337" class="IMG---Figure">
					<img src="image/B15923_09_13.jpg" alt="Figure 9.13: The general shapes of the six kernel functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13: The general shapes of the six kernel functions</p>
			<p>The choice of kernel function is not completely insignificant, but it is definitely not nearly as important as the choice of bandwidth value. A reasonable course of action would be to use the gaussian kernel for all density estimation problems, which is what we will do in the following exercises and activities.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor207"/>Exercise 9.03: The Effect of the Kernel Function</h2>
			<p>We will demonstrate how the choice of kernel function affects the quality of the density estimate. Like we did when exploring the bandwidth value effect, we will hold all other parameters constant, use the same data generated in the first two exercises, and run six different kernel density estimation models using the six kernel functions previously specified. Clear differences should be noticeable between the six estimated densities, but these differences should be slightly less dramatic than the differences between the densities estimated using the different bandwidth values. Note that this exercise should be executed in the same notebook as the previous exercises.</p>
			<ol>
				<li value="1">Define a list of tuples along the same lines as the one defined previously. Each tuple includes the row and column indices of the subplot, and the kernel function to be used to create the density estimation:<p class="source-code">position_kernel_vec = [(0, 0, 'gaussian'), (0, 1, 'tophat'), \</p><p class="source-code">                       (1, 0, 'epanechnikov'), \</p><p class="source-code">                       (1, 1, 'exponential'), \</p><p class="source-code">                       (2, 0, 'linear'), (2, 1, 'cosine'),]</p></li>
				<li>Fit and plot six kernel density estimation models using a different kernel function for each. To truly understand the differences between the kernel functions, we will set the bandwidth value to the optimal bandwidth value found in <em class="italic">Exercise 9.02</em>, <em class="italic">Selecting the Optimal Bandwidth Using Grid Search</em> and not adjust it:<p class="source-code">fig, ax = plt.subplots(3, 2, sharex=True, \</p><p class="source-code">                       sharey=True, figsize=(12, 9))</p><p class="source-code">fig.suptitle('The Effect of Different Kernels', fontsize=16)</p><p class="source-code">for r, c, k in position_kernel_vec:</p><p class="source-code">    kde = sklearn.neighbors.KernelDensity(\</p><p class="source-code">          kernel=k, bandwidth=best_bandwidth).fit(vals)</p><p class="source-code">    log_density = kde.score_samples(x_vec)</p><p class="source-code">    ax[r, c].hist(vals, bins=50, density=True, alpha=0.5)</p><p class="source-code">    ax[r, c].plot(x_vec[:, 0], numpy.exp(log_density), \</p><p class="source-code">                  '-', linewidth=2)</p><p class="source-code">    ax[r, c].set_title('Kernel = {}'.format(k.capitalize()))</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer338" class="IMG---Figure"><img src="image/B15923_09_14.jpg" alt="Figure 9.14: A 3 x 2 matrix of subplots&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.14: A 3 x 2 matrix of subplots</p>
			<p>Out of the six kernel functions, the gaussian kernel produced the most reasonable estimated density. Beyond that, notice that the difference between the estimated densities with different kernels is less than the difference between the estimated densities with different bandwidth values. This goes to the previously made claim that the bandwidth value is the more important parameter and should be the focus during the model building process.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2UOHbTZ">https://packt.live/2UOHbTZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38DbmTo">https://packt.live/38DbmTo</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<p>With our understanding mostly formed, let's discuss the derivation of kernel density estimation at a high-level.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor208"/>Kernel Density Estimation Derivation</h2>
			<p>Let's skip the formal mathematical derivation in favor of the popular derivation by intuition. Kernel density estimation turns each data point in the sample into its own distribution whose width is controlled by the bandwidth value. The individual distributions are then summed to create the desired density estimate. This concept is fairly easy to demonstrate; however, before doing that in the next exercise, let's try to think through it in an abstract way. For geographic regions containing many sample data points, the individual densities will overlap and, through the process of summing those densities, will create points of higher likelihood in the estimated density. Similarly, for geographic regions containing few to no sample data points, the individual densities will not overlap and, therefore, will correspond to points of lower likelihood in the estimated density.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor209"/>Exercise 9.04: Simulating the Derivation of Kernel Density Estimation</h2>
			<p>The goal here is to demonstrate the concept of summing individual distributions to create an overall estimated density for a random variable. We will establish the concept incrementally by starting with one sample data point and then work up too many sample data points. Additionally, different bandwidth values will be applied, so our understanding of the effect of the bandwidth value on these individual densities will further solidify. Note that this exercise should be done in the same notebook as all the other exercises.</p>
			<ol>
				<li value="1">D Hotspot Analysis efine a function that will evaluate the normal distribution. The input values are the grid representing the range of the random variable, <strong class="source-inline">X</strong>, the sampled data point, <strong class="source-inline">m</strong>, and the bandwidth, <strong class="source-inline">b</strong>:<p class="source-code">def eval_gaussian(x, m, b):</p><p class="source-code">    numerator = numpy.exp(-numpy.power(x - m, 2) \</p><p class="source-code">                          / (2 * numpy.power(b, 2)))</p><p class="source-code">    denominator = b * numpy.sqrt(2 * numpy.pi)</p><p class="source-code">    return numerator / denominator</p></li>
				<li>Plot a single sample data point as a histogram and as an individual density with varying bandwidth values:<p class="source-code">m = numpy.array([5.1])</p><p class="source-code">b_vec = [0.1, 0.35, 0.8]</p><p class="source-code">x_vec = numpy.linspace(1, 10, 100)[:, None]</p><p class="source-code">figOne, ax = plt.subplots(2, 3, sharex=True, \</p><p class="source-code">                          sharey=True, figsize=(15, 10))</p><p class="source-code">for i, b in enumerate(b_vec):</p><p class="source-code">    ax[0, i].hist(m[:], bins=1, fc='#AAAAFF', density=True)</p><p class="source-code">    ax[0, i].set_title("Histogram: Normed")</p><p class="source-code">    evaluation = eval_gaussian(x_vec, m=m[0], b=b)</p><p class="source-code">    ax[1, i].fill(x_vec, evaluation, '-k', fc='#AAAAFF')</p><p class="source-code">    ax[1, i].set_title("Gaussian Dist: b={}".format(b))</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer339" class="IMG---Figure"><img src="image/B15923_09_15.jpg" alt="Figure 9.15: Showing one data point and its individual density at various bandwidth values&#13;&#10;"/></div><p class="figure-caption">Figure 9.15: Showing one data point and its individual density at various bandwidth values</p><p>Here, we see what has already been established, which is that lower bandwidth values produce very narrow densities that tend to overfit the data.</p></li>
				<li>Reproduce the work done in <em class="italic">Step 2</em>, but now scale up to 16 data points:<p class="source-code">m = numpy.random.normal(4.7, 0.88, 16)</p><p class="source-code">n = len(m)</p><p class="source-code">b_vec = [0.1, 0.35, 1.1]</p><p class="source-code">x_vec = numpy.linspace(-1, 11, 100)[:, None]</p><p class="source-code">figMulti, ax = plt.subplots(2, 3, sharex=True, \</p><p class="source-code">                            sharey=True, figsize=(15, 10))</p><p class="source-code">for i, b in enumerate(b_vec):</p><p class="source-code">    ax[0, i].hist(m[:], bins=n, fc='#AAAAFF', density=True)</p><p class="source-code">    ax[0, i].set_title("Histogram: Normed")</p><p class="source-code">    sum_evaluation = numpy.zeros(len(x_vec))</p><p class="source-code">    for j in range(n):</p><p class="source-code">        evaluation = eval_gaussian(x_vec, m=m[j], b=b) / n</p><p class="source-code">        sum_evaluation += evaluation[:, 0]</p><p class="source-code">        ax[1, i].plot(x_vec, evaluation, \</p><p class="source-code">                     '-k', linestyle="dashed")</p><p class="source-code">    ax[1, i].fill(x_vec, sum_evaluation, '-k', fc='#AAAAFF')</p><p class="source-code">    ax[1, i].set_title("Gaussian Dist: b={}".format(b))</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer340" class="IMG---Figure"><img src="image/B15923_09_16.jpg" alt="Figure 9.16: Plotting the data points&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.16: Plotting the data points</p>
			<p>The preceding image shows 16 data points, their individual densities at various bandwidth values, and the sum of their individual densities.</p>
			<p>Again, unsurprisingly, the plot utilizing the smallest bandwidth value features a wildly overfit estimated density. That is, the estimated density captures all the noise in the sample data. Of these three densities, the second one, where the bandwidth value was set to <strong class="source-inline">0.35</strong>, is the most reasonable.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2UOHbTZ">https://packt.live/2UOHbTZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38DbmTo">https://packt.live/38DbmTo</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor210"/>Activity 9.01: Estimating Density in One Dimension</h2>
			<p>In this activity, we will be generating some fake sample data and estimating the density function using kernel density estimation. The bandwidth value will be optimized using grid search cross-validation. The goal is to solidify our understanding of this useful methodology by running the model in a simple one-dimensional case. We will once again leverage Jupyter notebooks to do our work.</p>
			<p>Imagine that the sample data we will be creating describes the price of homes in a state in the United States. Momentarily ignore the values in the following sample data. The question is, <em class="italic">what does the distribution of home prices look like, and can we extract the probability of a house having a price that falls in some specific range?</em> These questions and more are answerable using kernel density estimation.</p>
			<p>Here are the steps to complete the activity:</p>
			<ol>
				<li value="1">Open a new notebook and install all the necessary libraries.</li>
				<li>Sample 1,000 data points from the standard normal distribution. Add 3.5 to each of the last 625 values of the sample (that is, the indices between 375 and 1,000). Set a random state of 100 using <strong class="source-inline">numpy.random.RandomState</strong> to guarantee the same sampled values, and then randomly generate the data points using the <strong class="source-inline">rand.randn(1000)</strong> call.</li>
				<li>Plot the 1,000-point sample data as a histogram and add a scatterplot below it.</li>
				<li>Define a grid of bandwidth values. Then, define and fit a grid search cross-validation algorithm.</li>
				<li>Extract the optimal bandwidth value.</li>
				<li>Replot the histogram from <em class="italic">Step 3</em> and overlay the estimated density.</li>
			</ol>
			<p>The output will be as follows:</p>
			<div>
				<div id="_idContainer341" class="IMG---Figure">
					<img src="image/B15923_09_17.jpg" alt="Figure 9.17: A histogram of the random sample with the optimal estimated density overlaid&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17: A histogram of the random sample with the optimal estimated density overlaid</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 501.</p>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor211"/>Hotspot Analysis</h1>
			<p>To start, hotspots are areas of higher concentrations of data points, such as particular neighborhoods where the crime rate is abnormally high or swaths of the country that are impacted by an above-average number of tornadoes. Hotspot analysis is the process of finding these hotspots, should any exist, in a population using sampled data. This process is generally done by leveraging kernel density estimation.</p>
			<p>Hotspot analysis can be described in four high-level steps: </p>
			<ol>
				<li value="1"><strong class="bold">Collect the data</strong>: The data should include the locations of the objects or events. As we have briefly mentioned, the amount of data needed to run and achieve actionable results is relatively flexible. The optimal state is to have a sample dataset that is representative of the population.</li>
				<li><strong class="bold">Identify the base map</strong>: The next step is to identify which base map would best suit the analytical and presentational needs of the project. On this base map, the results of the model will be overlaid, so that the locations of the hotspots can be easily articulated in much more digestible terms, such as city, neighborhood, or region. </li>
				<li><strong class="bold">Execute the model</strong>: In this step, you select and execute one or multiple methodologies of extracting spatial patterns to identify hotspots. For us, this method will be – no surprise – kernel density estimation. </li>
				<li><strong class="bold">Create the visualization</strong>: The hotspot maps are generated by overlaying the model results on the base map to support whatever business questions are outstanding.</li>
			</ol>
			<p>One of the principal issues with hotspot analysis from a usability standpoint is that the statistical significance of a hotspot is not particularly easy to ascertain. Most questions about statistical significance revolve around the existence of the hotspots. That is, do the fluctuations in likelihood of occurrence actually amount to statistically significant fluctuations? It is important to note that statistical significance is not required to perform kernel density estimation and that we will not be dealing with significance at all going forward.</p>
			<p>While the term hotspot is traditionally reserved to describe a cluster of location data points, it is not limited to location data. Any data type can have hotspots regardless of whether or not they are referred to as hotspots. In one of the following exercises, we will model some non-location data to find hotspots, which will be regions of the feature space having a high or low likelihood of occurrence.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor212"/>Exercise 9.05: Loading Data and Modeling with Seaborn</h2>
			<p>In this exercise, we will work with the <strong class="source-inline">seaborn</strong> library to fit and visualize kernel density estimation models. This is done on both location and non-location data. Before getting into the modeling, we load the data, which is the California housing dataset that comes with <strong class="source-inline">sklearn</strong>, that has been provided in csv form. The file needs to be downloaded from the GitHub repository and saved on your local machine. Taken from the United States census in 1990, this dataset describes the housing situation in California during that time. One row of data describes one census block group. The definition of a census block group is irrelevant to this exercise, so we will bypass the definition here in favor of more hands-on coding and modeling. It is important to mention that all the variables are aggregated to the census block. For example, <strong class="source-inline">MedInc</strong> is the median income of households in each census block. Additional information on this dataset is available at <a href="https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset">https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset</a>.</p>
			<ol>
				<li value="1">Load the California housing dataset using <strong class="source-inline">california_housing.csv</strong>. Print the first five rows of the data frame:<p class="source-code">df = pandas.read_csv('./california_housing.csv', header=0)</p><p class="source-code">df.head()</p><p class="callout-heading">Note</p><p class="callout">The path of the file depends on the location of the file on your system.</p><p>The output is as follows:</p><div id="_idContainer342" class="IMG---Figure"><img src="image/B15923_09_18.jpg" alt="Figure 9.18: The first five rows of the California housing dataset from sklearn&#13;&#10;"/></div><p class="figure-caption">Figure 9.18: The first five rows of the California housing dataset from sklearn</p></li>
				<li>Filter the data frame on the <strong class="source-inline">HouseAge</strong> feature, which is the median home age of each census block. Keep only the rows with <strong class="source-inline">HouseAge</strong> less than or equal to 15 and name the data frame <strong class="source-inline">dfLess15</strong>. Print out the first five rows of the data frame, then reduce the data frame down to just the longitude and latitude features:<p class="source-code">dfLess15 = df[df['HouseAge'] &lt;= 15.0]</p><p class="source-code">dfLess15 = dfLess15[['Latitude', 'Longitude']]</p><p class="source-code">dfLess15.head()</p><p>The output is as follows:</p><div id="_idContainer343" class="IMG---Figure"><img src="image/B15923_09_19.jpg" alt="Figure 9.19: The first five rows of the filtered dataset&#13;&#10;"/></div><p class="figure-caption">Figure 9.19: The first five rows of the filtered dataset</p></li>
				<li>Use <strong class="source-inline">seaborn</strong> to fit and visualize the kernel density estimation model built on the longitude and latitude data points. There are four inputs to the model, which are the names of the two columns over which the estimated density is sought (that is, the longitude and latitude), the data frame to which those columns belong, and the method of density estimation (that is, the <strong class="source-inline">kde</strong> or kernel density estimation):<p class="source-code">seaborn.jointplot("Longitude", "Latitude", dfLess15, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer344" class="IMG---Figure"><img src="image/B15923_09_20.jpg" alt="Figure 9.20: A joint plot&#13;&#10;"/></div><p class="figure-caption">Figure 9.20: A joint plot</p><p class="callout-heading">Note</p><p class="callout">The graph might differ as the estimation is not exactly the same every time.</p><p>This joint plot contains both the two-dimensional estimated density plus the marginal densities for the dfLess15 dataset.</p><p>If we overlay these results on a map of California, we will see that the hotspots are southern California, including Los Angeles and San Diego, the bay area, including San Francisco, and to a small degree the area known as the central valley. A benefit of this <strong class="source-inline">seaborn</strong> graphic is that we get the two-dimensional estimated density and the marginal densities for both longitude and latitude.</p></li>
				<li>Create another filtered data frame based on the <strong class="source-inline">HouseAge</strong> feature; this time keep only the rows with <strong class="source-inline">HouseAge</strong> greater than 40 and name the data frame <strong class="source-inline">dfMore40</strong>. Additionally, remove all the columns other than longitude and latitude. Then, print the first five rows of the data frame:<p class="source-code">dfMore40 = df[df['HouseAge'] &gt; 40.0]</p><p class="source-code">dfMore40 = dfMore40[['Latitude', 'Longitude']]</p><p class="source-code">dfMore40.head()</p><p>The output is as follows:</p><div id="_idContainer345" class="IMG---Figure"><img src="image/B15923_09_21.jpg" alt="Figure 9.21: The top of the dataset filtered&#13;&#10;"/></div><p class="figure-caption">Figure 9.21: The top of the dataset filtered</p></li>
				<li>Repeat the process from <em class="italic">Step 3</em>, but using this new filtered data frame:<p class="source-code">seaborn.jointplot("Longitude", "Latitude", dfMore40, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer346" class="IMG---Figure"><img src="image/B15923_09_22.jpg" alt="Figure 9.22: The joint plot &#13;&#10;"/></div><p class="figure-caption">Figure 9.22: The joint plot </p><p>This joint plot contains both the two-dimensional estimated density plus the marginal densities for the dfMore40 dataset.</p><p>This estimated density is much more compact in that the data is clustered almost entirely in two areas. Those areas are Los Angeles and the bay area. Comparing this to the plot in <em class="italic">Step 3</em>, we notice that housing development has spread out across the state. Additionally, newer housing developments occur with much higher frequencies in a larger number of census blocks.</p></li>
				<li>Let's again create another filtered data frame. This time only keeping rows where <strong class="source-inline">HouseAge</strong> is less than or equal to five and name the data frame <strong class="source-inline">dfLess5</strong>. Plot <strong class="source-inline">Population</strong> and <strong class="source-inline">MedInc</strong> as a scatterplot, as follows:<p class="source-code">dfLess5 = df[df['HouseAge'] &lt;= 5]</p><p class="source-code">x_vals = dfLess5.Population.values</p><p class="source-code">y_vals = dfLess5.MedInc.values</p><p class="source-code">fig = plt.figure(figsize=(10, 10))</p><p class="source-code">plt.scatter(x_vals, y_vals, c='black')</p><p class="source-code">plt.xlabel('Population', fontsize=18)</p><p class="source-code">plt.ylabel('Median Income', fontsize=16)</p><p>The output is as follows:</p><div id="_idContainer347" class="IMG---Figure"><img src="image/B15923_09_23.jpg" alt="Figure 9.23: Scatterplot of the median income against population&#13;&#10;"/></div><p class="figure-caption">Figure 9.23: Scatterplot of the median income against population</p><p>This is the scatterplot of the median income against population for values of five or less in the <strong class="source-inline">HouseAge</strong> column.</p></li>
				<li>Use yet another <strong class="source-inline">seaborn</strong> function to fit a kernel density estimation model. The optimal bandwidth is found using Scott's Rule. Replot the histogram and overlay the estimated density, as follows:<p class="source-code">fig = plt.figure(figsize=(10, 10))</p><p class="source-code">ax = seaborn.kdeplot(x_vals, \</p><p class="source-code">                     y_vals,\</p><p class="source-code">                     kernel='gau',\</p><p class="source-code">                     cmap='Blues', \</p><p class="source-code">                     shade=True, \</p><p class="source-code">                     shade_lowest=False)</p><p class="source-code">plt.scatter(x_vals, y_vals, c='black', alpha=0.05)</p><p class="source-code">plt.xlabel('Population', fontsize=18)</p><p class="source-code">plt.ylabel('Median Income', fontsize=18)</p><p class="source-code">plt.title('Density Estimation With Scatterplot Overlay', size=18)</p><p>The output is as follows:</p><div id="_idContainer348" class="IMG---Figure"><img src="image/B15923_09_24.jpg" alt="Figure 9.24: The same scatterplot as created in step 6 with the estimated density overlaid&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.24: The same scatterplot as created in step 6 with the estimated density overlaid</p>
			<p>Here, the estimated density shows that census blocks with smaller populations have lower median incomes at higher likelihoods than they have high median incomes. The point of this step is to showcase how kernel density estimation can be used on non-location data. A plot like this is typically referred to as a contour plot.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2UOHbTZ">https://packt.live/2UOHbTZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38DbmTo">https://packt.live/38DbmTo</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<p>When presenting the results of hotspot analysis, some type of map should be involved since hotspot analysis is generally done on location data. Acquiring maps on which estimated densities can be overlaid is not an easy process. Due to copyright issues, we will use very basic maps, called basemaps, on which we can overlay our estimated densities. It will be left to you to extend the knowledge you acquire in this chapter to fancier and more detailed maps. Mapping environments can also be complicated and time-consuming to download and install.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor213"/>Exercise 9.06: Working with Basemaps</h2>
			<p>This exercise leverages the <strong class="source-inline">basemap</strong> module of <strong class="source-inline">mpl_toolkits</strong>. <strong class="source-inline">basemap</strong> is a mapping library, which can be used to create basic maps or outlines of geographic regions. These maps can have the results of kernel density estimation overlaid, so that we can clearly see where the hotspots are located.</p>
			<p>First, check whether <strong class="source-inline">basemap</strong> is installed by running <strong class="source-inline">import mpl_toolkits.basemap</strong> in a Jupyter notebook. If it loads without error, then you are ready and need to take no further action. If the call fails, then install <strong class="source-inline">basemap</strong> using <strong class="source-inline">pip</strong> by running <strong class="source-inline">python3 -m pip install basemap</strong>. You should be good to go after restarting any already-open notebooks. Note that the <strong class="source-inline">pip</strong> installation will only work if Anaconda is installed.</p>
			<p>The goal of this exercise is to remodel and replot the location data from <em class="italic">Exercise 9.05</em>, <em class="italic">Loading Data and Modeling with Seaborn</em>, using the kernel density estimation functions of <strong class="source-inline">sklearn</strong> and the mapping capabilities of <strong class="source-inline">basemap</strong>. Extract the longitude and latitude values from the filtered data frame called <strong class="source-inline">dfLess15</strong> to work through the steps. Note that this exercise should be done in the same notebook as all the other exercises. </p>
			<ol>
				<li value="1">Form the grid of locations over which the estimated density will be laid. The grid of locations is the two-dimensional location equivalent of the one-dimensional vector defining the range of the random variable in <em class="italic">Exercise 9.01</em>, <em class="italic">The Effect of the Bandwidth Value</em>:<p class="source-code">xgrid15 = numpy.sort(list(dfLess15['Longitude']))</p><p class="source-code">ygrid15 = numpy.sort(list(dfLess15['Latitude']))</p><p class="source-code">x15, y15 = numpy.meshgrid(xgrid15, ygrid15)</p><p class="source-code">print("X Grid Component:\n{}\n".format(x15))</p><p class="source-code">print("Y Grid Component:\n{}\n".format(y15))</p><p class="source-code">xy15 = numpy.vstack([y15.ravel(), x15.ravel()]).T </p><p>The output is as follows:</p><div id="_idContainer349" class="IMG---Figure"><img src="image/B15923_09_25.jpg" alt="Figure 9.25: The x and y components of the grid representing the dfLess15 dataset&#13;&#10;"/></div><p class="figure-caption">Figure 9.25: The x and y components of the grid representing the dfLess15 dataset</p></li>
				<li>Define and fit a kernel density estimation model. Set the bandwidth value to 0.05. Then create likelihood values for each point on the location grid:<p class="source-code">kde15 = sklearn.neighbors.KernelDensity(bandwidth=0.05, \</p><p class="source-code">                                        metric='minkowski', \</p><p class="source-code">                                        kernel='gaussian', \</p><p class="source-code">                                        algorithm='ball_tree')</p><p class="source-code">kde15.fit(dfLess15.values)</p><p>The output of kernel density estimation model is as follows: </p><p class="source-code">KernelDensity(algorithm='ball_tree', atol=0, bandwidth=0.05, \</p><p class="source-code">              breadth_first=True, kernel='gaussian', \</p><p class="source-code">              leaf_size=40, metric='minkowski', \</p><p class="source-code">              metric_params=None, rtol=0)</p></li>
				<li>Fit the trained model on the <strong class="source-inline">xy</strong> grid and print the shape as follows:<p class="source-code">log_density = kde15.score_samples(xy15)</p><p class="source-code">density = numpy.exp(log_density)</p><p class="source-code">density = density.reshape(x15.shape)</p><p class="source-code">print("Shape of Density Values:\n{}\n".format(density.shape))</p><p>The output is as follows:</p><p class="source-code">Shape of Density Values:</p><p class="source-code">(3287, 3287)</p><p>Notice that if you print out the shape of the likelihood values, it is 3,287 rows by 3,287 columns, which is 10,804,369 likelihood values. This is the same number of values in the preestablished longitude and latitude grid, called <strong class="source-inline">xy15</strong>.</p></li>
				<li>Create an outline of California and overlay the estimated density computed in <em class="italic">Step 2</em>:<p class="source-code-heading">Exercise9.01-Exercise9.06.ipynb</p><p class="source-code">fig15 = plt.figure(figsize=(10, 10))</p><p class="source-code">fig15.suptitle(\</p><p class="source-code">    """</p><p class="source-code">    Density Estimation:</p><p class="source-code">    Location of Housing Blocks</p><p class="source-code">    Where the Median Home Age &lt;= 15 Years</p><p class="source-code">    """,\</p><p class="source-code">    fontsize=16)</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/38DbmTo">https://packt.live/38DbmTo</a>.</p><p>The output is as follows:</p><div id="_idContainer350" class="IMG---Figure"><img src="image/B15923_09_26.jpg" alt="Figure 9.26: The estimated density of dfLess15 overlaid onto an outline of California&#13;&#10;"/></div><p class="figure-caption">Figure 9.26: The estimated density of dfLess15 overlaid onto an outline of California</p><p>The <strong class="source-inline">0.05</strong> value was set to purposefully overfit the data slightly. You'll notice that instead of the larger clusters that made up the density in <em class="italic">Exercise 9.05,</em> <em class="italic">Loading Data and Modeling with Seaborn</em> the estimated density here is made up of much smaller clusters. This slightly overfit density might be a bit more helpful than the previous version of the density because it gives you a clearer view of where the high likelihood census blocks are truly located. One of the high-likelihood areas in the previous density was southern California, but southern California is a huge area with an enormous population and many municipalities. Bear in mind that when using the results for business decisions, certain levels of specificity might be required and should be provided if the sample data can support results with that level of specificity or granularity.</p></li>
				<li>Repeat <em class="italic">Step 1</em>, but with the <strong class="source-inline">dfMore40</strong> data frame:<p class="source-code">xgrid40 = numpy.sort(list(dfMore40['Longitude']))</p><p class="source-code">ygrid40 = numpy.sort(list(dfMore40['Latitude']))</p><p class="source-code">x40, y40 = numpy.meshgrid(xgrid40, ygrid40)</p><p class="source-code">print("X Grid Component:\n{}\n".format(x40))</p><p class="source-code">print("Y Grid Component:\n{}\n".format(y40))</p><p class="source-code">xy40 = numpy.vstack([y40.ravel(), x40.ravel()]).T </p><p>The output is as follows:</p><div id="_idContainer351" class="IMG---Figure"><img src="image/B15923_09_27.jpg" alt="Figure 9.27: The x and y components of the grid representing the dfMore40 dataset&#13;&#10;"/></div><p class="figure-caption">Figure 9.27: The x and y components of the grid representing the dfMore40 dataset</p></li>
				<li>Repeat <em class="italic">Step 2 </em>using the grid established in <em class="italic">Step 4</em>:<p class="source-code">kde40 = sklearn.neighbors.KernelDensity(bandwidth=0.05, \</p><p class="source-code">                                        metric='minkowski', \</p><p class="source-code">                                        kernel='gaussian', \</p><p class="source-code">                                        algorithm='ball_tree')</p><p class="source-code">kde40.fit(dfMore40.values)</p><p class="source-code">log_density = kde40.score_samples(xy40)</p><p class="source-code">density = numpy.exp(log_density)</p><p class="source-code">density = density.reshape(x40.shape)</p><p class="source-code">print("Shape of Density Values:\n{}\n".format(density.shape))</p></li>
				<li>Repeat <em class="italic">Step 3</em> using the estimated density computed in <em class="italic">Step 5</em>:</li>
			</ol>
			<p class="source-code-heading">Exercise9.01-Exercise9.06.ipynb</p>
			<p class="source-code">fig40 = plt.figure(figsize=(10, 10))</p>
			<p class="source-code">fig40.suptitle(\</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Density Estimation:</p>
			<p class="source-code">    Location of Housing Blocks</p>
			<p class="source-code">    Where the Median Home Age &gt; 40 Years</p>
			<p class="source-code">    """, \</p>
			<p class="source-code">    fontsize=16)</p>
			<p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/38DbmTo">https://packt.live/38DbmTo</a>.</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer352" class="IMG---Figure">
					<img src="image/B15923_09_28.jpg" alt="Figure 9.28: The estimated density of dfMore40 overlaid onto an outline of California&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.28: The estimated density of dfMore40 overlaid onto an outline of California</p>
			<p class="callout-heading">Note </p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2UOHbTZ">https://packt.live/2UOHbTZ</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38DbmTo">https://packt.live/38DbmTo</a>. You must execute the entire Notebook in order to get the desired result. </p>
			<p>This estimated density is again a redo of the one that we did in <em class="italic">Exercise 9.05</em>, <em class="italic">Loading Data and Modeling with Seaborn</em>. While the density from <em class="italic">Step 3</em> will provide more detail for a person interested in real estate or the census, this density does not actually look that different from its corollary density in <em class="italic">Exercise 9.05</em>, <em class="italic">Loading Data and Modeling with Seaborn</em>. The clusters are primarily around Los Angeles and San Francisco with almost no points anywhere else.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor214"/>Activity 9.02: Analyzing Crime in London</h2>
			<p>In this activity, we will perform hotspot analysis with kernel density estimation on London crime data from <a href="https://data.police.uk/data/">https://data.police.uk/data/</a>. Due to the difficulties of working with map data, we will visualize the results of the analysis using <strong class="source-inline">seaborn</strong>. However, if you feel brave and were able to run all the plots in <em class="italic">Exercise 9.06</em>, <em class="italic">Working with Basemaps</em> you are encouraged to try using maps.</p>
			<p>The motivation for performing hotspot analysis on this crime data is two-fold. We are asked first to determine where certain types of crimes are occurring in high likelihood, so that police resources can be allocated for maximum impact. Then, as a follow up, we are asked to ascertain whether the hotspots for certain types of crime are changing over time. Both of these questions are answerable using kernel density estimation.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://data.police.uk/data/">https://data.police.uk/data/</a>. It contains public sector information licensed under the Open Government License v3.0.</p>
			<p class="callout">You can also download it from the Packt GitHub at <a href="https://packt.live/2JIWs2z">https://packt.live/2JIWs2z</a>.</p>
			<p class="callout">Alternatively, to download the data directly from the source, go to the preceding police website, check the box for <strong class="source-inline">Metropolitan Police Service</strong>, and then set the date range to <strong class="source-inline">July 2018</strong> to <strong class="source-inline">Dec 2018</strong>. Next, click <strong class="source-inline">Generate file</strong> followed by <strong class="source-inline">Download now</strong> and name the downloaded file <strong class="source-inline">metro-jul18-dec18</strong>. Make sure that you know how or can retrieve the path to the downloaded directory.</p>
			<p class="callout">This dataset contains public sector information licensed under the Open Government License v3.0.</p>
			<p>Here are the steps to complete the activity:</p>
			<ol>
				<li value="1">Load the crime data. Use the path where you saved the downloaded directory, create a list of the year-month tags, use the <strong class="source-inline">read_csv</strong> command to load the individual files iteratively, and then concatenate these files together.</li>
				<li>Print diagnostics of the complete (six months) and concatenated dataset.</li>
				<li>Subset the data frame down to four variables (<strong class="source-inline">Longitude</strong>, <strong class="source-inline">Latitude</strong>, <strong class="source-inline">Month</strong>, and <strong class="source-inline">Crime type</strong>).</li>
				<li>Using the <strong class="source-inline">jointplot</strong> function from <strong class="source-inline">seaborn</strong>, fit and visualize three kernel density estimation models for bicycle theft in July, September, and December 2018.</li>
				<li>Repeat <em class="italic">Step 4</em>; this time, use shoplifting crimes for the months of August, October, and November 2018.</li>
				<li>Repeat <em class="italic">Step 5</em>; this time, use burglary crimes for the months of July, October, and December 2018.</li>
			</ol>
			<p>The output from the last part of <em class="italic">Step 6</em> will be as follows:</p>
			<div>
				<div id="_idContainer353" class="IMG---Figure">
					<img src="image/B15923_09_29.jpg" alt="Figure 9.29: The estimated joint and marginal densities for burglaries in December 2018&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.29: The estimated joint and marginal densities for burglaries in December 2018</p>
			<p>To clarify one more time, the densities found in this activity should have been overlaid on maps so that we could see exactly what areas these densities cover. Attempting to overlay the results on maps on your own would be encouraged if you have the appropriate mapping platforms at your disposal. If not, you could go to the mapping services available online and use the longitude and latitude pairs to gain insight into the specific locations.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 505.</p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor215"/>Summary</h1>
			<p>Kernel density estimation is a classic statistical technique that is in the same family of techniques as the histogram. It allows the user to extrapolate out from sample data to make insights and predictions about the population of particular objects or events. This extrapolation comes in the form of a probability density function, which is nice because the results read as likelihoods or probabilities. The quality of this model is dependent on two parameters: the bandwidth value and the kernel function. As discussed, the most crucial component of leveraging kernel density estimation successfully is the setting of an optimal bandwidth. Optimal bandwidths are most frequently identified using grid search cross-validation with pseudo-log-likelihood as the scoring metric. What makes kernel density estimation great is both its simplicity and its applicability to so many fields.</p>
			<p>It is routine to find kernel density estimation models in criminology, epidemiology, meteorology, and real estate to only name a few. Regardless of your area of business, kernel density estimation should be applicable.</p>
			<p>Between supervised and unsupervised learning, unsupervised learning is undoubtedly the least used and appreciated learning category. But this should not be the case. Supervised learning techniques are limited and most of the available data does not fit well with regression and classification. Expanding your skill set to include unsupervised learning techniques means that you will be able to leverage different datasets, answer business problems in more creative ways, and even enhance your existing supervised learning models. This text by no means exhausts all the unsupervised learning algorithms, but serves as a good start to pique your interest and drive continued learnings.</p>
		</div>
	</body></html>
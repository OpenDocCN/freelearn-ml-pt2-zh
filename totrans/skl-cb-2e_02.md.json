["```py\nfrom sklearn import datasets\ndatasets.make_*?\n\ndatasets.make_biclusters\ndatasets.make_blobs\ndatasets.make_checkerboard\ndatasets.make_circles\ndatasets.make_classification\n...\n```", "```py\nimport sklearn.datasets as d\nimport numpy as np\n```", "```py\nreg_data = d.make_regression()\n```", "```py\ncomplex_reg_data = d.make_regression(1000, 10, 5, 2, 1.0)\ncomplex_reg_data[0].shape\n\n(1000L, 10L)\n```", "```py\nclassification_set = d.make_classification(weights=[0.1])\nnp.bincount(classification_set[1])\n\narray([10, 90], dtype=int64)\n```", "```py\nblobs_data, blobs_target = d.make_blobs()\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline \n#Within an Ipython notebook\nplt.scatter(blobs_data[:,0],blobs_data[:,1],c = blobs_target) \n```", "```py\nX = np.random.randn(n_samples, n_features)\n```", "```py\nground_truth = np.zeros((np_samples, n_target))\nground_truth[:n_informative, :] = 100*np.random.rand(n_informative, n_targets)\n```", "```py\ny = np.dot(X, ground_truth) + bias\n```", "```py\nfrom sklearn import preprocessing\nimport numpy as np # we'll need it later\n```", "```py\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nX,y = boston.data, boston.target\n```", "```py\nX[:, :3].mean(axis=0) #mean of the first 3 features\n\narray([  3.59376071,  11.36363636,  11.13677866])\n\nX[:, :3].std(axis=0)\n\narray([  8.58828355,  23.29939569,   6.85357058])\n```", "```py\nX_2 = preprocessing.scale(X[:, :3])\nX_2.mean(axis=0)\n\narray([  6.34099712e-17,  -6.34319123e-16,  -2.68291099e-15])\n\nX_2.std(axis=0)\n\narray([ 1., 1., 1.])\n```", "```py\npd.Series(X[:,2]).hist(bins=50)\n```", "```py\npd.Series(preprocessing.scale(X[:, 2])).hist(bins=50)\n```", "```py\nmy_scaler = preprocessing.StandardScaler()\nmy_scaler.fit(X[:, :3])\nmy_scaler.transform(X[:, :3]).mean(axis=0)\n\narray([  6.34099712e-17,  -6.34319123e-16,  -2.68291099e-15])\n```", "```py\nmy_minmax_scaler = preprocessing.MinMaxScaler()\nmy_minmax_scaler.fit(X[:, :3])\nmy_minmax_scaler.transform(X[:, :3]).max(axis=0)\n\narray([ 1., 1., 1.])\n\nmy_minmax_scaler.transform(X[:, :3]).min(axis=0)\n\narray([ 0., 0., 0.])\n```", "```py\n\nmy_odd_scaler = preprocessing.MinMaxScaler(feature_range=(-3.14, 3.14))\n```", "```py\nnormalized_X = preprocessing.normalize(X[:, :3]) \n```", "```py\n(normalized_X * normalized_X).sum(axis = 1)\n\narray([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n        1.,  1\\. \n ...]\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nX, y = boston.data, boston.target.reshape(-1, 1)\n```", "```py\nfrom sklearn import preprocessing\nnew_target = preprocessing.binarize(y,threshold=boston.target.mean())\nnew_target[:5]\n\n array([[ 1.],\n [ 0.],\n [ 1.],\n [ 1.],\n [ 1.]])\n```", "```py\n(y[:5] > y.mean()).astype(int)\n\narray([[1],\n       [0],\n       [1],\n       [1],\n       [1]])\n```", "```py\nbinar = preprocessing.Binarizer(y.mean())\nnew_target = binar.fit_transform(y)\nnew_target[:5]\n\narray([[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 1.],\n       [ 1.]])\n```", "```py\nfrom scipy.sparse import coo\nspar = coo.coo_matrix(np.random.binomial(1, .25, 100))\npreprocessing.binarize(spar, threshold=-1)\n\nValueError: Cannot binarize a sparse matrix with threshold &lt; 0\n```", "```py\nfrom sklearn import datasets\nimport numpy as np\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\n```", "```py\nfrom sklearn import preprocessing\ncat_encoder = preprocessing.OneHotEncoder()\ncat_encoder.fit_transform(y.reshape(-1,1)).toarray()[:5]\n\narray([[ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.]])\n```", "```py\ncat_encoder.transform(np.ones((3, 1))).toarray() \n\narray([[ 0.,  1.,  0.],\n [ 0.,  1.,  0.],\n [ 0.,  1.,  0.]])\n```", "```py\nfrom sklearn.linear_model import Ridge\nridge_inst = Ridge()\n```", "```py\nfrom sklearn.multioutput import MultiOutputRegressor\nmulti_ridge = MultiOutputRegressor(ridge_inst, n_jobs=-1)\n```", "```py\nfrom sklearn import preprocessing\ncat_encoder = preprocessing.OneHotEncoder()\ny_multi = cat_encoder.fit_transform(y.reshape(-1,1)).toarray()\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_multi, stratify=y, random_state= 7)\n```", "```py\nmulti_ridge.fit(X_train, y_train)\n\nMultiOutputRegressor(estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n   normalize=False, random_state=None, solver='auto', tol=0.001),\n           n_jobs=-1)\n```", "```py\ny_multi_pre = multi_ridge.predict(X_test)\ny_multi_pre[:5]\n\narray([[ 0.81689644,  0.36563058, -0.18252702],\n [ 0.95554968,  0.17211249, -0.12766217],\n [-0.01674023,  0.36661987,  0.65012036],\n [ 0.17872673,  0.474319  ,  0.34695427],\n [ 0.8792691 ,  0.14446485, -0.02373395]])\n```", "```py\nfrom sklearn import preprocessing\ny_multi_pred = preprocessing.binarize(y_multi_pre,threshold=0.5)\ny_multi_pred[:5]\n\narray([[ 1.,  0.,  0.],\n [ 1.,  0.,  0.],\n [ 0.,  0.,  1.],\n [ 0.,  0.,  0.],\n [ 1.,  0.,  0.]])\n```", "```py\nfrom sklearn.metrics import roc_auc_score\n\n roc_auc_score(y_test, y_multi_pre)\n\n0.91987179487179482\n```", "```py\nfrom sklearn.metrics import accuracy_score\n\nprint (\"Multi-Output Scores for the Iris Flowers: \")\nfor column_number in range(0,3):\n print (\"Accuracy score of flower \" + str(column_number),accuracy_score(y_test[:,column_number], y_multi_pred[:,column_number]))\n print (\"AUC score of flower \" + str(column_number),roc_auc_score(y_test[:,column_number], y_multi_pre[:,column_number]))\n print (\"\")\n\n Multi-Output Scores for the Iris Flowers:\n ('Accuracy score of flower 0', 1.0)\n ('AUC score of flower 0', 1.0)\n\n ('Accuracy score of flower 1', 0.73684210526315785)\n ('AUC score of flower 1', 0.76923076923076927)\n\n ('Accuracy score of flower 2', 0.97368421052631582)\n ('AUC score of flower 2', 0.99038461538461542)\n```", "```py\nfrom sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer()\nmy_dict = [{'species': iris.target_names[i]} for i in y]\ndv.fit_transform(my_dict).toarray()[:5]\n\narray([[ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 1.,  0.,  0.]])\n```", "```py\nfrom sklearn import datasets\nimport numpy as np\n\niris = datasets.load_iris()\niris_X = iris.data\nmasking_array = np.random.binomial(1, .25,iris_X.shape).astype(bool)\niris_X[masking_array] = np.nan\n```", "```py\nmasking_array[:5]\n\narray([[ True, False, False,  True],\n       [False, False, False, False],\n       [False, False, False, False],\n       [ True, False, False, False],\n       [False, False, False,  True]], dtype=bool)\n\niris_X [:5]\n\narray([[ nan,  3.5,  1.4,  nan],\n       [ 4.9,  3\\. ,  1.4,  0.2],\n       [ 4.7,  3.2,  1.3,  0.2],\n       [ nan,  3.1,  1.5,  0.2],\n       [ 5\\. ,  3.6,  1.4,  nan]])\n```", "```py\nfrom sklearn import preprocessing\nimpute = preprocessing.Imputer()\niris_X_prime = impute.fit_transform(iris_X)\niris_X_prime[:5]\n\narray([[ 5.82616822,  3.5       ,  1.4       ,  1.22589286],\n       [ 4.9       ,  3\\.        ,  1.4       ,  0.2       ],\n       [ 4.7       ,  3.2       ,  1.3       ,  0.2       ],\n       [ 5.82616822,  3.1       ,  1.5       ,  0.2       ],\n       [ 5\\.        ,  3.6       ,  1.4       ,  1.22589286]])\n```", "```py\niris_X_prime[0, 0]\n\n5.8261682242990664\n\niris_X[0, 0] \n\nnan\n```", "```py\nimpute = preprocessing.Imputer(strategy='median')\niris_X_prime = impute.fit_transform(iris_X)\niris_X_prime[:5]\n\narray([[ 5.8,  3.5,  1.4,  1.3],\n       [ 4.9,  3\\. ,  1.4,  0.2],\n       [ 4.7,  3.2,  1.3,  0.2],\n       [ 5.8,  3.1,  1.5,  0.2],\n       [ 5\\. ,  3.6,  1.4,  1.3]])\n```", "```py\niris_X[np.isnan(iris_X)] = -1\niris_X[:5]\n```", "```py\nimpute = preprocessing.Imputer(missing_values=-1)\niris_X_prime = impute.fit_transform(iris_X)\niris_X_prime[:5]\n\narray([[ 5.1 , 3.5 , 1.4 , 0.2 ],\n [ 4.9 , 3\\. , 1.4 , 0.2 ],\n [ 4.7 , 3.2 , 1.3 , 0.2 ],\n [ 5.87923077, 3.1 , 1.5 , 0.2 ],\n [ 5\\. , 3.6 , 1.4 , 0.2 ]])\n```", "```py\nimport pandas as pd\niris_X_prime = np.where(pd.DataFrame(iris_X).isnull(),-1,iris_X)\niris_X_prime[:5]\n\narray([[-1\\. ,  3.5,  1.4, -1\\. ],\n       [ 4.9,  3\\. ,  1.4,  0.2],\n       [ 4.7,  3.2,  1.3,  0.2],\n       [-1\\. ,  3.1,  1.5,  0.2],\n       [ 5\\. ,  3.6,  1.4, -1\\. ]])\n```", "```py\npd.DataFrame(iris_X).fillna(-1)[:5].values\n\narray([[-1\\. ,  3.5,  1.4, -1\\. ],\n       [ 4.9,  3\\. ,  1.4,  0.2],\n       [ 4.7,  3.2,  1.3,  0.2],\n       [-1\\. ,  3.1,  1.5,  0.2],\n       [ 5\\. ,  3.6,  1.4, -1\\. ]])\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnum_points = 100\nx_vals = np.arange(num_points)\ny_truth = 2 * x_vals\nplt.plot(x_vals, y_truth)\n```", "```py\ny_noisy = y_truth.copy()\n#Change y-values of some points in the line\ny_noisy[20:40] = y_noisy[20:40] * (-4 * x_vals[20:40]) - 100\n\nplt.title(\"Noise in y-direction\")\nplt.xlim([0,100])\nplt.scatter(x_vals, y_noisy,marker='x')\n```", "```py\nfrom sklearn.linear_model import LinearRegression, TheilSenRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nnamed_estimators = [('OLS ', LinearRegression()), ('TSR ', TheilSenRegressor())]\n\nfor num_index, est in enumerate(named_estimators):\n y_pred = est[1].fit(x_vals.reshape(-1, 1),y_noisy).predict(x_vals.reshape(-1, 1))\n print (est[0], \"R-squared: \", r2_score(y_truth, y_pred), \"Mean Absolute Error\", mean_absolute_error(y_truth, y_pred))\n plt.plot(x_vals, y_pred, label=est[0])\n\n('OLS   ', 'R-squared: ', 0.17285546630270587, 'Mean Absolute Error', 44.099173357335729)\n('TSR   ', 'R-squared: ', 0.99999999928066519, 'Mean Absolute Error', 0.0013976236426276058)\n```", "```py\nplt.plot(x_vals, y_truth, label='True line')\nplt.legend(loc='upper left')\n```", "```py\nfor num_index, est in enumerate(named_estimators):\n y_pred = est[1].fit(x_vals.reshape(-1, 1),y_noisy).predict(x_vals.reshape(-1, 1))\n plt.plot(x_vals, y_pred, label=est[0])\nplt.legend(loc='upper left')\nplt.title(\"Noise in y-direction\")\nplt.xlim([0,100])\nplt.scatter(x_vals, y_noisy,marker='x', color='red')\n```", "```py\nfrom sklearn.linear_model import Ridge, LinearRegression, TheilSenRegressor, RANSACRegressor, ElasticNet, HuberRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error\nnamed_estimators = [('OLS ', LinearRegression()),\n('Ridge ', Ridge()),('TSR ', TheilSenRegressor()),('RANSAC', RANSACRegressor()),('ENet ',ElasticNet()),('Huber ',HuberRegressor())]\n\nfor num_index, est in enumerate(named_estimators):\n y_pred = est[1].fit(x_vals.reshape(-1, 1),y_noisy).predict(x_vals.reshape(-1, 1))\n print (est[0], \"R-squared: \", r2_score(y_truth, y_pred), \"Mean Absolute Error\", mean_absolute_error(y_truth, y_pred))\n\n('OLS   ', 'R-squared: ', 0.17285546630270587, 'Mean Absolute Error', 44.099173357335729)\n('Ridge ', 'R-squared: ', 0.17287378039132695, 'Mean Absolute Error', 44.098937961740631)\n('TSR   ', 'R-squared: ', 0.99999999928066519, 'Mean Absolute Error', 0.0013976236426276058)\n('RANSAC', 'R-squared: ', 1.0, 'Mean Absolute Error', 1.0236256287043944e-14)\n('ENet  ', 'R-squared: ', 0.17407294649885618, 'Mean Absolute Error', 44.083506446776603)\n('Huber ', 'R-squared: ', 0.99999999999404421, 'Mean Absolute Error', 0.00011755074198335526)\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_iris\nimport numpy as np\niris = load_iris()\niris_data = iris.data\nmask = np.random.binomial(1, .25, iris_data.shape).astype(bool)\niris_data[mask] = np.nan\niris_data[:5]\n\narray([[ nan,  3.5,  1.4,  0.2],\n       [ 4.9,  3\\. ,  1.4,  nan],\n       [ nan,  3.2,  nan,  nan],\n       [ nan,  nan,  1.5,  0.2],\n       [ nan,  3.6,  1.4,  0.2]])\n```", "```py\nfrom sklearn import pipeline, preprocessing, decomposition\n```", "```py\npca = decomposition.PCA()\nimputer = preprocessing.Imputer()\n```", "```py\npipe = pipeline.Pipeline([('imputer', imputer), ('pca', pca)])\niris_data_transformed = pipe.fit_transform(iris_data)\niris_data_transformed[:5]\n\narray([[-2.35980262,  0.6490648 ,  0.54014471,  0.00958185],\n       [-2.29755917, -0.00726168, -0.72879348, -0.16408532],\n       [-0.00991161,  0.03354407,  0.01597068,  0.12242202],\n       [-2.23626369,  0.50244737,  0.50725722, -0.38490096],\n       [-2.36752684,  0.67520604,  0.55259083,  0.1049866 ]])\n```", "```py\npipe2 = pipeline.make_pipeline(imputer, pca)\npipe2.steps\n\n[('imputer',\n Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)),\n ('pca',\n PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n svd_solver='auto', tol=0.0, whiten=False))]\n```", "```py\niris_data_transformed2 = pipe2.fit_transform(iris_data)\niris_data_transformed2[:5]\n\narray([[-2.35980262,  0.6490648 ,  0.54014471,  0.00958185],\n       [-2.29755917, -0.00726168, -0.72879348, -0.16408532],\n       [-0.00991161,  0.03354407,  0.01597068,  0.12242202],\n       [-2.23626369,  0.50244737,  0.50725722, -0.38490096],\n       [-2.36752684,  0.67520604,  0.55259083,  0.1049866 ]])\n```", "```py\npipe2.set_params(pca__n_components=2)\n\nPipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False))])\n```", "```py\niris_data_transformed3 = pipe2.fit_transform(iris_data)\niris_data_transformed3[:5]\n\narray([[-2.35980262,  0.6490648 ],\n [-2.29755917, -0.00726168],\n [-0.00991161,  0.03354407],\n [-2.23626369,  0.50244737],\n [-2.36752684,  0.67520604]])\n```", "```py\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nboston_X = boston.data\nboston_y = boston.target\ntrain_set = np.random.choice([True, False], len(boston_y),p=[.75, .25])\n```", "```py\nsklearn.gaussian_process import GaussianProcessRegressor\ngpr = GaussianProcessRegressor()\ngpr\n\nGaussianProcessRegressor(alpha=1e-10, copy_X_train=True, kernel=None,\n             n_restarts_optimizer=0, normalize_y=False,\n             optimizer='fmin_l_bfgs_b', random_state=None)\n```", "```py\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as CK\n\nmixed_kernel = kernel = CK(1.0, (1e-4, 1e4)) * RBF(10, (1e-4, 1e4))\n```", "```py\ngpr = GaussianProcessRegressor(alpha=5,\n n_restarts_optimizer=20,\n kernel = mixed_kernel)\n\ngpr.fit(boston_X[train_set],boston_y[train_set])\n```", "```py\ntest_preds = gpr.predict(boston_X[~train_set])\n```", "```py\n>from sklearn.model_selection import cross_val_predict\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nf, ax = plt.subplots(figsize=(10, 7), nrows=3)\nf.tight_layout()\n\nax[0].plot(range(len(test_preds)), test_preds,label='Predicted Values');\nax[0].plot(range(len(test_preds)), boston_y[~train_set],label='Actual Values');\nax[0].set_title(\"Predicted vs Actuals\")\nax[0].legend(loc='best')\n\nax[1].plot(range(len(test_preds)),test_preds - boston_y[~train_set]);\nax[1].set_title(\"Plotted Residuals\")\nax[2].hist(test_preds - boston_y[~train_set]);\nax[2].set_title(\"Histogram of Residuals\")\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\ngpr5 = GaussianProcessRegressor(alpha=5,\n n_restarts_optimizer=20,\n kernel = mixed_kernel)\n\nscores_5 = (cross_val_score(gpr5,\n boston_X[train_set],\n boston_y[train_set],\n cv = 4,\n scoring = 'neg_mean_absolute_error'))\n```", "```py\ndef score_mini_report(scores_list):\n print \"List of scores: \", scores_list\n print \"Mean of scores: \", scores_list.mean()\n print \"Std of scores: \", scores_list.std()\n\n score_mini_report(scores_5)\n\nList of scores:  [ -4.10973995  -4.93446898  -3.78162    -13.94513686]\nMean of scores:  -6.69274144767\nStd of scores:  4.20818506589\n```", "```py\n\ngpr7 = GaussianProcessRegressor(alpha=7,\n n_restarts_optimizer=20,\n kernel = mixed_kernel)\n\nscores_7 = (cross_val_score(gpr7,\n boston_X[train_set],\n boston_y[train_set],\n cv = 4,\n scoring = 'neg_mean_absolute_error'))\n\nscore_mini_report(scores_7)\n\nList of scores:  [ -3.70606009  -4.92211642  -3.63887969 -14.20478333]\nMean of scores:  -6.61795988295\nStd of scores:  4.40992783912\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\n gpr7n = GaussianProcessRegressor(alpha=7,\n n_restarts_optimizer=20,\n kernel = mixed_kernel,\n normalize_y=True)\n\n scores_7n = (cross_val_score(gpr7n,\n boston_X[train_set],\n boston_y[train_set],\n cv = 4,\n scoring = 'neg_mean_absolute_error'))\nscore_mini_report(scores_7n)\n\nList of scores:  [-4.0547601  -4.91077385 -3.65226736 -9.05596047]\nMean of scores:  -5.41844044809\nStd of scores:  2.1487361839\n```", "```py\ngpr7n.fit(boston_X[train_set],boston_y[train_set])\n```", "```py\ntest_preds = gpr7n.predict(boston_X[~train_set])\n```", "```py\ngpr_new = GaussianProcessRegressor(alpha=boston_y[train_set]/4,\n n_restarts_optimizer=20,\n kernel = mixed_kernel)\n```", "```py\ntest_preds, MSE = gpr7n.predict(boston_X[~train_set], return_std=True)\nMSE[:5]\n\narray([ 1.20337425,  1.43876578,  1.19910262,  1.35212445,  1.32769539])\n```", "```py\nf, ax = plt.subplots(figsize=(7, 5))\nn = 133\nrng = range(n)\nax.scatter(rng, test_preds[:n])\nax.errorbar(rng, test_preds[:n], yerr=1.96*MSE[:n])\nax.set_title(\"Predictions with Error Bars\")\nax.set_xlim((-1, n));\n```", "```py\nfrom sklearn.datasets import make_regression\nX, y = make_regression(int(1e6))  #1,000,000 rows\n```", "```py\nprint \"{:,}\".format(X.nbytes)\n\n800,000,000\n```", "```py\nX.nbytes / 1e6\n\n800\n```", "```py\nX.nbytes / (X.shape[0]*X.shape[1])\n\n8\n```", "```py\n\nfrom sklearn.linear_model import SGDRegressor\nsgd = SGDRegressor()\ntrain = np.random.choice([True, False], size=len(y), p=[.75, .25])\nsgd.fit(X[train], y[train])\n\nSGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n       random_state=None, shuffle=True, verbose=0, warm_start=False)\n```", "```py\ny_pred = sgd.predict(X[~train])\n\n%matplotlib inline\nimport pandas as pd\n\npd.Series(y[~train] - y_pred).hist(bins=50)\n```", "```py\nwhile not converged:\n w = w – learning_rate*gradient(cost(w))\n```"]
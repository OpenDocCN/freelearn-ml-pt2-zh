<html><head></head><body>
		<div>
			<div id="_idContainer067" class="Content">
			</div>
		</div>
		<div id="_idContainer068" class="Content">
			<h1 id="_idParaDest-107"><a id="_idTextAnchor109"/>4. Supervised Learning Algorithms: Predicting Annual Income</h1>
		</div>
		<div id="_idContainer082" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will take a look at three different supervised learning algorithms used for classification. We will also solve a supervised learning classification problem using these algorithms and perform error analysis by comparing the results of the three different algorithms.</p>
			<p class="callout">By the end of this chapter, you will be able to identify the algorithm with the best performance.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor110"/>Introduction</h1>
			<p>In the previous chapter, we covered the key steps involved in working with a supervised learning data problem. Those steps aim to create high-performing algorithms, as<a id="_idTextAnchor111"/> explained in the previous chapter. </p>
			<p>This chapter focuses on applying different algorithms to a real-life dataset, with the underlying objective of applying the steps that we learned previously to choose the best-performing algorithm for the case study. Considering this, you will pre-process and analyze a dataset, and then create three models using different algorithms. These models will be compared to one another in order to measure their performance.</p>
			<p>The Census Income dataset that we'll be using contains demographical and financial information, which can be used to try and predict the level of income of an individual. By creating a model capable of predicting this outcome for new observations, it will be possible to determine whether a person can be pre-approved to receive a loan.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor112"/>Exploring the Dataset</h1>
			<p>Real-life applications are crucial for cementing knowledge. Therefore, this chapter consists of a real-life case study involving a classification task, where the key steps that you learned about in the previous chapter will be applied in order to select the best performing model. </p>
			<p>To accomplish this, the Census Income dataset will be used, which is available at the UC Irvine Machine Learning Repository.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset that will be used in the following section, as well as in this chapter's activities, can be found in this book's GitHub repository at <a href="https://packt.live/2xUGShx">https://packt.live/2xUGShx</a>.</p>
			<p class="callout">Citation: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<p>You can download the dataset from this book's GitHub repository. Alternatively, toÂ download the dataset from the original source, follow these steps:</p>
			<ol>
				<li>Visit the following link: <a href="http://archive.ics.uci.edu/ml/datasets/Census+Income">http://archive.ics.uci.edu/ml/datasets/Census+Income</a>.</li>
				<li>First, click the <strong class="source-inline">Data Folder</strong> link. </li>
				<li>For this chapter, the data available under <strong class="source-inline">adult.data</strong> will be used. Once you click this link, the download will be triggered. Save it as a <strong class="source-inline">.csv</strong> file.<p class="callout-heading"><a id="_idTextAnchor113"/><a id="_idTextAnchor114"/>Note</p><p class="callout">Open the file and add header names over each column to make pre-processing easier. For instance, the first column should have the header <strong class="source-inline">Age</strong>, as per the features available in the dataset. These can be seen in the preceding link, under <strong class="source-inline">Attribute Information</strong>.</p></li>
			</ol>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor115"/>Understanding the Dataset</h2>
			<p>To build a model that fits the data accurately, it is important to understand the different details of the dataset, as mentioned in previous chapters. </p>
			<p>First, the data that's available is revised to understand the size of the dataset and the type of supervised learning task to be developed: classification or regression. Next, the purpose of the study should be clearly defined, even if it is obvious. For supervised learning, the purpose is closely linked to the class labels. Finally, each feature is analyzed so that we can be aware of their types for pre-processing purposes. </p>
			<p>The Census Income dataset is a collection of demographical data on adults, which is an extract from the 1994 Census Database from the United States. For this chapter, only the data available under the <strong class="source-inline">adult.data</strong> link will be used. The dataset consists of 32,561 instances, 14 features, and 1 binary class label. Considering that the class label is discrete, our task is to achieve the classification of the different observations.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The following exploration of the dataset does not require coding of any sort, but rather a simple evaluation by opening the dataset in Excel or a similar program.</p>
			<p>Through a quick evaluation of the data, it is possible to observe that some features present missing values in the form of a question mark. This is common when dealing with datasets that are available online and should be handled by replacing the symbol with an empty value (not a space). Other common forms of missing values are the <strong class="source-inline">NULL</strong> value and a dash.</p>
			<p>To edit missing value symbols in Excel, use the <strong class="bold">Replace</strong> functionality, as follows:</p>
			<ul>
				<li><strong class="bold">Find what</strong>: Input the symbol that is being used to signify a missing value (for example, <strong class="source-inline">?</strong>).</li>
				<li><strong class="bold">Replace with</strong>: Leave it blank (do not enter a space).</li>
			</ul>
			<p>This way, once we import the dataset into the code, NumPy will be able to find the missing values so that it can handle them.</p>
			<p>The prediction task for this dataset involves determining whether a person earns over 50K dollars a year. According to this, the two possible outcome labels are <strong class="source-inline">&gt;50K</strong> (greater than 50K) or <strong class="source-inline">&lt;=50K</strong> (less than, or equal to 50K).</p>
			<p>A brief explanation of each of the features in the dataset is shown in the following table:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B15781_04_01.jpg" alt="Figure 4.1: Dataset feature analysis&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1: Dataset feature analysis</p>
			<p class="callout-heading">Note</p>
			<p class="callout">*Publisher's Note: Gender and race would have impacted the earning potential of an individual at the date this study was conducted. However, for the purpose of this chapter, we have decided to exclude these categories from our exercises and activities. </p>
			<p class="callout">We recognize that due to biases and discriminatory practices, it is impossible to separate issues such as gender, race, and educational and vocational opportunities. The removal of certain features from our dataset in the pre-processing stage of these exercises is not intended to ignore the issues, nor the valuable work undertaken by organizations and individuals working in the civil rights sphere. </p>
			<p class="callout">We strongly recommend that you consider the sociopolitical impacts of data and the way it is used, and also consider how past prejudices can be perpetuated by using historical data to introduce bias into new algorithms.</p>
			<p>From the preceding table, it is possible to conclude the following:</p>
			<ul>
				<li>Five features are not relevant to the study: <strong class="source-inline">fnlwgt</strong>, <strong class="source-inline">education</strong>, <strong class="source-inline">relationship</strong>, <strong class="source-inline">race</strong>, and <strong class="source-inline">sex</strong>. These features must be deleted from the dataset before we proceed with pre-processing and training the model.</li>
				<li>Out of the remaining features, four are presented as qualitative values. Considering that many algorithms do not take qualitative features into account, the values should be represented in numerical form.</li>
			</ul>
			<p>Using the concepts that we learned about in the previous chapters, the preceding statements, as well as the pre-processing process for handling outliers and missing values, can be taken care of. The following steps explain the logic of this process:</p>
			<ol>
				<li value="1">You need to import the dataset and drop the features that are irrelevant to the study.</li>
				<li>You should check for missing values. Considering the feature with the most missing values (<strong class="source-inline">occupation</strong>, with 1,843 missing values), there will be no need to delete or replace the missing values as they represent only 5% or less of the entire dataset.</li>
				<li>You must convert the qualitative values into their numeric representations. </li>
				<li>You should check for outliers. Upon using three standard deviations to detect outliers, the feature with the maximum number of outliers is <strong class="source-inline">capital-loss</strong>, which contains 1,470 outliers. Again, the outliers represent less than 5% of the entire dataset, meaning they can be left untouched without impacting the result of the model. </li>
			</ol>
			<p>The preceding process will convert the original dataset into a new dataset with 32,561 instances (since no instances were deleted), but with 9 features and a class label. All values should be in their numerical forms. Save the pre-processed dataset into a file using pandas' <strong class="source-inline">to_csv</strong> function, as per the following code snippet:</p>
			<p class="source-code">preprocessed_data.to_csv("census_income_dataset_preprocessed.csv")</p>
			<p>The preceding code snippet takes the pre-processed data stored in a Pandas DataFrame and saves it into a CSV file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure that you perform the preceding pre-processing steps, as this is the dataset that will be used for training the models in the different activities of this chapter.</p>
			<p class="callout">To review these steps, visit the GitHub repository of this book, under the folder named <strong class="source-inline">Chapter04</strong>, in the file named <strong class="source-inline">Census income dataset preprocessing</strong>. </p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor116"/>The NaÃ¯ve Bayes Algorithm</h1>
			<p><strong class="bold">NaÃ¯ve Bayes</strong> is a classification algorithm based on <strong class="bold">Bayes' theorem</strong> that <em class="italic">naÃ¯vely</em> assumes independence between features and assigns the same weight (degree of importance) to all features. This means that the algorithm assumes that no single feature correlates to or affects another. For example, although weight and height are somehow correlated when predicting a person's age, the algorithm assumes that each feature is independent. Additionally, the algorithm considers all features equally important. For instance, even though an education degree may influence the earnings of a person to a greater degree than the number of children the person has, the algorithm still considers both features equally important.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Bayes' theorem is a mathematical formula that calculates conditional probabilities. To learn more about this theorem, visit the following URL: <a href="https://plato.stanford.edu/entries/bayes-theorem/">https://plato.stanford.edu/entries/bayes-theorem/</a>.</p>
			<p>Although real-life datasets contain features that are not equally important, nor independent, this algorithm is popular among scientists as it performs surprisingly well on large datasets. Also, due to the simplistic approach of the algorithm, it runs quickly, thus allowing it to be applied to problems that require predictions in real-time. Moreover, it is frequently used for text classification as it commonly outperforms more complex algorithms.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor117"/>How Does the NaÃ¯ve Bayes Algorithm Work?</h2>
			<p>The algorithm converts the input data into a summary of occurrences of each class label against each feature, which is then used to calculate the likelihood of one event (a class label), given a combination of features. Finally, this likelihood is normalized against the likelihood of the other class labels. The result is the probability of an instance belonging to each class label. The sum of the probabilities must be one, and the class label with a higher probability is the one that the algorithm chooses as the prediction.</p>
			<p>Let's take, for example, the data presented in the following tables:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B15781_04_02.jpg" alt="Figure 4.2: Table A - Input data and Table B - Occurrence count"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2: Table A - Input data and Table B - Occurrence count</p>
			<p>Table A represents the data that is fed to the algorithm to build the model. Table B refer to the occurrence count that the algorithm uses implicitly to calculate the probabilities.</p>
			<p>To calculate the likelihood of an event occurring when given a set of features, the algorithm multiplies the probability of the event occurring, given each individual feature, by the probability of the occurrence of the event, independent of the rest of the features, as follows:</p>
			<p class="source-code">Likelihood [A<span class="subscript">1</span>|E] = P[A<span class="subscript">1</span>|E<span class="subscript">1</span>] * P[A<span class="subscript">1</span>|E<span class="subscript">2</span>] * â¦ * P[A<span class="subscript">1</span>|E<span class="subscript">n</span>] * P[A<span class="subscript">1</span>]</p>
			<p>Here, <em class="italic">A</em><span class="subscript">1</span> refers to an event (one of the class labels) and <em class="italic">E</em> represents the set of features, where <em class="italic">E</em><span class="subscript">1</span> is the first feature and <em class="italic">E</em><span class="subscript">n</span> is the last feature in the dataset.</p>
			<p class="callout-heading">Note </p>
			<p class="callout">The multiplication of these probabilities can only be made by assuming independence between features.</p>
			<p>The preceding equation is calculated for all possible outcomes (all class labels), and then the normalized probability of each outcome is calculated as follows:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B15781_04_03.jpg" alt="Figure 4.3: Formula to calculate normalized probability&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3: Formula to calculate normalized probability</p>
			<p>For the example in <em class="italic">Figure 4.2</em>, given a new instance with weather equal to <em class="italic">sunny</em> and temperature equal to <em class="italic">cool</em>, the calculation of probabilities is as follows:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B15781_04_04.jpg" alt="Figure 4.4: Calculation of the likelihood and probabilities for the example dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4: Calculation of the likelihood and probabilities for the example dataset</p>
			<p>By looking at the preceding equations, it is possible to conclude that the prediction should be <em class="italic">yes</em>.</p>
			<p>It is important to mention that for continuous features, the summary of occurrences is done by creating ranges. For instance, for a feature of price, the algorithm may count the number of instances with prices below 100K, as well as the instances with prices above 100K.</p>
			<p>Moreover, the algorithm may encounter some issues if one value of a feature is never associated with one of the outcomes. This is an issue mainly because the probability of the outcome given that feature will be zero, which influences the entire calculation. In the preceding example, for predicting the outcome of an instance with weather equal to <em class="italic">mild</em> and temperature equal to <em class="italic">cool</em>, the probability of <em class="italic">no</em>, given the set of features will be equal to zero, considering that the probability of <em class="italic">no</em>, given <em class="italic">mild</em> weather, computes to zero, since there are no occurrences of <em class="italic">mild</em> weather when the outcome is <em class="italic">no</em>.</p>
			<p>To avoid this, the <strong class="bold">Laplace estimator</strong> technique should be used. Here, the fractions representing the probability of the occurrence of an event given a feature, <em class="italic">P[A|E</em><span class="subscript">1</span><em class="italic">]</em>, are modified by adding 1 to the numerator while also adding the number of possible values of that feature to the denominator.</p>
			<p>For this example, to perform a prediction for a new instance with weather equal to <em class="italic">mild</em> and temperature equal to <em class="italic">cool</em> using the Laplace estimator, this would be done as follows:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B15781_04_05.jpg" alt="Figure 4.5: Calculation of the likelihood and probability using the Laplace estimator for the example dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5: Calculation of the likelihood and probability using the Laplace estimator for the example dataset</p>
			<p>Here, the fraction that calculates the occurrences of <em class="italic">yes</em>, given <em class="italic">mild</em> weather, goes from 2/7 to 3/10, as a result of the addition of 1 to the numerator and 3 (for <em class="italic">sunny</em>, <em class="italic">mild</em>, and <em class="italic">rainy</em>) to the denominator. The same goes for the other fractions that calculate the probability of the event, given a feature. Note that the fraction that calculates the probability of the event occurring independently of any feature is left unaltered.</p>
			<p>Nevertheless, as you have learned so far, the scikit-learn library allows you to train models and then use them for predictions, without needing to hardcode the math.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor118"/>Exercise 4.01: Applying the NaÃ¯ve Bayes Algorithm </h2>
			<p>Now, let's apply the NaÃ¯ve Bayes algorithm to a Fertility dataset, which aims to determine whether the fertility level of an individual has been affected by their demographics, their environmental conditions, and their previous medical conditions. Follow these steps to complete this exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For the exercises and activities within this chapter, you will need to have Python 3.7, NumPy, Jupyter, Pandas, and scikit-learn installed on your system.</p>
			<ol>
				<li value="1">Download the Fertility dataset from <a href="http://archive.ics.uci.edu/ml/datasets/Fertility">http://archive.ics.uci.edu/ml/datasets/Fertility</a>. Go to the link and click on <strong class="source-inline">Data Folder</strong>. Click on <strong class="source-inline">fertility_Diagnosis.txt</strong>, which will trigger the download. Save it as a <strong class="source-inline">.csv</strong> file.<p class="callout-heading">Note</p><p class="callout">The dataset is also available in this book's GitHub repository at <a href="https://packt.live/39SsSSN">https://packt.live/39SsSSN</a>.</p><p class="callout">It was downloaded from the UC Irvine Machine Learning Repository: David Gil, Jose Luis Girela, Joaquin De Juan, M. Jose Gomez-Torres, and Magnus Johnsson. <em class="italic">Predicting seminal quality with artificial intelligence methods</em>. Expert Systems with Applications.</p></li>
				<li>Open a Jupyter Notebook to implement this exercise. Import pandas, as well as the <strong class="source-inline">GaussianNB</strong> class from scikit-learn's <strong class="source-inline">naive_bayes</strong> module:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.naive_bayes import GaussianNB</p></li>
				<li>Read the <strong class="source-inline">.csv</strong> file that you downloaded in the first step. Make sure that you add the <strong class="source-inline">header</strong> argument equal to <strong class="source-inline">None</strong> to the <strong class="source-inline">read_csv</strong> function, considering that the dataset does not contain a header row:<p class="source-code">data = pd.read_csv("fertility_Diagnosis.csv", header=None)</p></li>
				<li>Split the data into <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>, considering that the class label is found under the column with an index equal to 9. Use the following code to do so:<p class="source-code">X = data.iloc[:,:9]</p><p class="source-code">Y = data.iloc[:,9]</p></li>
				<li>Instantiate the <strong class="source-inline">GaussianNB</strong> class that we imported previously. Next, use the <strong class="source-inline">fit</strong> method to train the model using <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>:<p class="source-code">model = GaussianNB()</p><p class="source-code">model.fit(X, Y)</p><p>The output from running this script is as follows:</p><p class="source-code">GaussianNB(priors=None, var_smoothing=1e-09)</p><p>This states that the instantiation of the class was successful. The information inside the parentheses represents the values used for the arguments that the class accepts, which are the hyperparameters. </p><p>For instance, for the <strong class="source-inline">GaussianNB</strong> class, it is possible to set the prior probabilities to consider for each class label and a smoothing argument that stabilizes variance. Nonetheless, the model was initialized without setting any arguments, which means that it will use the default values for each argument, which is <strong class="source-inline">None</strong> for the case of <strong class="source-inline">priors</strong> and <strong class="source-inline">1e-09</strong> for the smoothing hyperparameter.</p></li>
				<li>Finally, perform a prediction using the model that you trained before, for a new instance with the following values for each feature: <strong class="source-inline">â0.33</strong>, <strong class="source-inline">0.69</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">0.8</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">0.88</strong>. Use the following code to do so:<p class="source-code">pred = model.predict([[-0.33,0.69,0,1,1,0,0.8,0,0.88]])</p><p class="source-code">print(pred)</p><p>Note that we feed the values inside of double square brackets, considering that the <strong class="source-inline">predict</strong> function takes in the values for prediction as an array of arrays, where the first set of arrays corresponds to the list of new instances to predict and the second array refers to the list of features for each instance.</p><p>The output from the preceding code snippet is as follows:</p><p class="source-code">['N']</p><p>The predicted class for that subject is equal to <strong class="source-inline">N</strong>, which means that the fertility of the subject has not been affected.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Y2wW0c">https://packt.live/2Y2wW0c</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3e40LTt">https://packt.live/3e40LTt</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>You have successfully trained a NaÃ¯ve Bayes model and performed prediction on a new observation.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor119"/>Activity 4.01: Training a NaÃ¯ve Bayes Model for Our Census Income Dataset</h2>
			<p>To test different classification algorithms on a real-life dataset, consider the following scenario: you work for a bank, and they have decided to implement a model that is able to predict a person's annual income and use that information to decide whether to approve a loan. You are given a dataset with 32,561 suitable observations, which you have already pre-processed. Your job is to train three different models on the dataset and determine which one best suits the case study. The first model to be built is a Gaussian NaÃ¯ve Bayes model. Use the following steps to complete this activity:</p>
			<ol>
				<li value="1">In a Jupyter Notebook, import all the required elements to load and split the dataset, as well as to train a NaÃ¯ve Bayes algorithm.</li>
				<li>Load the pre-processed Census Income dataset. Next, separate the features from the target by creating two variables, <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>.<p class="callout-heading">Note</p><p class="callout">The pre-processed Census Income dataset can be found in this book's GitHub repository at <a href="https://packt.live/2JMhsFB">https://packt.live/2JMhsFB</a>. It consists of the transformed Census Income dataset that was pre-processed at the beginning of this chapter.</p></li>
				<li>Divide the dataset into training, validation, and testing sets, using a split ratio of 10%.<p class="callout-heading">Note</p><p class="callout">When all three sets are created from the same dataset, it is not required to create an additional train/dev set to measure data mismatch. Moreover, note that it is OK to try a different split ratio, considering that the percentages explained in the previous chapter are not set in stone. Even though they tend to work well, it is important that you embrace experimentation at different levels when building machine learning models. </p></li>
				<li>Use the <strong class="source-inline">fit</strong> method to train a NaÃ¯ve Bayes model on the training sets (<strong class="source-inline">X_train</strong> and <strong class="source-inline">Y_train</strong>).</li>
				<li>Finally, perform a prediction using the model that you trained previously, for a new instance with the following values for each feature: <strong class="source-inline">39</strong>, <strong class="source-inline">6</strong>, <strong class="source-inline">13</strong>, <strong class="source-inline">4</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">2174</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">40</strong>, <strong class="source-inline">38</strong>.<p>The prediction for the individual should be equal to zero, meaning that the individual most likely has an income less than or equal to 50K.</p><p class="callout-heading">Note</p><p class="callout">Use the same Jupyter Notebook for all the activities within this chapter so that you can perform a comparison of different models on the same dataset.</p><p class="callout">The solution for this activity can be found on page 236.</p></li>
			</ol>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor120"/>The Decision Tree Algorithm</h1>
			<p>The <strong class="bold">decision tree algorithm</strong> performs classification based on a sequence that resembles a tree-like structure. It works by dividing the dataset into small subsets that serve as guides to develop the decision tree nodes. The nodes can be either decision nodes or leaf nodes, where the former represent a question or decision, andÂ the latter represent the decisions made or the final outcome.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor121"/>How Does the Decision Tree Algorithm Work?</h2>
			<p>Considering what we just mentioned, decision trees continually split the dataset according to the parameters defined in the decision nodes. Decision nodes have branches coming out of them, where each decision node can have two or more branches. The branches represent the different possible answers that define the way in which the data is split.</p>
			<p>For instance, consider the following table, which shows whether a person has a pending student loan based on their age, highest education, and current income:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B15781_04_06.jpg" alt="Figure 4.6: Dataset for student loans&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6: Dataset for student loans</p>
			<p>One possible configuration for a decision tree built based on the preceding data is shown in the following diagram, where the light boxes represent the decision nodes, the arrows are the branches representing each answer to the decision node, and the dark boxes refer to the outcome for instances that follow the sequence:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B15781_04_07.jpg" alt="Figure 4.7: Data represented in a decision tree&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7: Data represented in a decision tree</p>
			<p>To perform the prediction, once the decision tree has been built, the model takes each instance and follows the sequence that matches the instance's features until it reaches a leaf, that is, the outcome. According to this, the classification process starts at the root node (the one on top) and continues along the branch that describes the instance. This process continues until a leaf node is reached, which represents the prediction for that instance.</p>
			<p>For instance, a person <em class="italic">over 40 years old</em> with an income <em class="italic">below $150,000</em> and an education level of <em class="italic">bachelor</em> is likely to not have a student loan; hence, the class label assigned to it would be <em class="italic">No</em>.</p>
			<p>Decision trees can handle both quantitative and qualitative features, considering that continuous features will be handled in ranges. Additionally, leaf nodes can handle categorical or continuous class labels; for categorical class labels, a classification is made, while for continuous class labels, the task to be handled is regression.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor122"/>Exercise 4.02: Applying the Decision Tree Algorithm </h2>
			<p>In this exercise, we will apply the decision tree algorithm to the Fertility Dataset, with the objective of determining whether the fertility level of an individual is affected by their demographics, their environmental conditions, and their previous medical conditions. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook to implement this exercise and import <strong class="source-inline">pandas</strong>, as well as the <strong class="source-inline">DecisionTreeClassifier</strong> class from scikit-learn's <strong class="source-inline">tree</strong> module:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.tree import DecisionTreeClassifier</p></li>
				<li>Load the <strong class="source-inline">fertility_Diagnosis</strong> dataset that you downloaded in <em class="italic">Exercise 4.01</em>, <em class="italic">Applying the NaÃ¯ve Bayes Algorithm</em>. Make sure that you add the <strong class="source-inline">header</strong> argument equal to <strong class="source-inline">None</strong> to the <strong class="source-inline">read_csv</strong> function, considering that the dataset does not contain a header row:<p class="source-code">data = pd.read_csv("fertility_Diagnosis.csv", header=None)</p></li>
				<li>Split the data into <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>, considering that the class label is found under the column with the index equal to <strong class="source-inline">9</strong>. Use the following code:<p class="source-code">X = data.iloc[:,:9]</p><p class="source-code">Y = data.iloc[:,9]</p></li>
				<li>Instantiate the <strong class="source-inline">DecisionTreeClassifier</strong> class. Next, use the <strong class="source-inline">fit</strong> function to train the model using <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>:<p class="source-code">model = DecisionTreeClassifier()</p><p class="source-code">model.fit(X, Y)</p><p>Again, the output from running the preceding code snippet will appear. This output summarizes the conditions that define your model by printing the values that are used for every hyperparameter that the model uses, as follows:</p><p class="source-code">DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â criterion='gini', max_depth=None,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â max_features=None, max_leaf_nodes=None,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â min_impurity_decrease=0.0,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â min_impurity_split=None,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â min_samples_leaf=1, min_samples_split=2,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â min_weight_fraction_leaf=0.0,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â presort='deprecated',</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â random_state=None, splitter='best')</p><p>Since the model has been instantiated without setting any hyperparameters, the summary will show the default values that were used for each.</p></li>
				<li>Finally, perform a prediction by using the model that you trained before, for the same instances that we used in <em class="italic">Exercise 4.01</em>, <em class="italic">Applying the NaÃ¯ve Bayes Algorithm</em>: <strong class="source-inline">â0.33</strong>, <strong class="source-inline">0.69</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">0.8</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">0.88</strong>.<p>Use the following code to do so:</p><p class="source-code">pred = model.predict([[-0.33,0.69,0,1,1,0,0.8,0,0.88]])</p><p class="source-code">print(pred)</p><p>The output from the prediction is as follows:</p><p class="source-code">['N']</p><p>Again, the model predicted that the fertility of the subject has not been affected.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hDlvns">https://packt.live/3hDlvns</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3fsVw07">https://packt.live/3fsVw07</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>You have successfully trained a decision tree model and performed a prediction on new data.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor123"/>Activity 4.02: Training a Decision Tree Model for Our Census Income Dataset</h2>
			<p>You continue to work on building a model that's able to predict a person's annual income. Using the pre-processed Census Income dataset, you have chosen to build a decision tree model:</p>
			<ol>
				<li value="1">Open the Jupyter Notebook that you used for the previous activity and import the decision tree algorithm from scikit-learn.</li>
				<li>Train the model using the <strong class="source-inline">fit</strong> method on the <strong class="source-inline">DecisionTreeClassifier</strong> class from scikit-learn. To train the model, use the training set data from the previous activity (<strong class="source-inline">X_train</strong> and <strong class="source-inline">Y_train</strong>).</li>
				<li>Finally, perform a prediction by using the model that you trained for a new instance with the following values for each feature: <strong class="source-inline">39</strong>, <strong class="source-inline">6</strong>, <strong class="source-inline">13</strong>, <strong class="source-inline">4</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">2174</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">40</strong>, <strong class="source-inline">38</strong>.<p>The prediction for the individual should be equal to zero, meaning that the individual most likely has an income less than or equal to 50K.</p><p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found on page 237.</p></li>
			</ol>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor124"/>The Support Vector Machine Algorithm</h1>
			<p>The <strong class="bold">Support Vector Machine</strong> (<strong class="bold">SVM</strong>) algorithm is a classifier that finds the hyperplane that effectively separates the observations into their class labels. It starts by positioning each instance into a data space with <em class="italic">n</em> dimensions, where <em class="italic">n</em> represents the number of features. Next, it traces an imaginary line that clearly separates the instances belonging to a class label from the instances belonging to others.</p>
			<p>A support vector refers to the coordinates of a given instance. According to this, the support vector machine is the boundary that effectively segregates the different support vectors in a data space. </p>
			<p>For a two-dimensional data space, the hyperplane is a line that splits the data space into two sections, each one representing a class label.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor125"/>How Does the SVM Algorithm Work?</h2>
			<p>The following diagram shows a simple example of an SVM model. Both the triangles and circular data points represent the instances from the input dataset, where the shapes define the class label that each instance belongs to. The dashed line signifies the hyperplane that clearly segregates the data points, which is defined based on the data points' location in the data space. This line is used to classify unseen data, as represented by the square. This way, new instances that are located to the left of the line will be classified as triangles, while the ones to the right will be circles.</p>
			<p>The larger the number of features, the more dimensions the data space will have, which will make visually representing the model impossible:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B15781_04_08.jpg" alt="Figure 4.8: Graphical example of an SVM model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8: Graphical example of an SVM model</p>
			<p>Although the algorithm seems to be quite simple, its complexity is evident in the algorithm's methodology for drawing the appropriate hyperplane. This is because the model generalizes to hundreds of observations with multiple features.</p>
			<p>To choose the right hyperplane, the algorithm follows the following rules, wherein <em class="italic">Rule 1</em> is more important than <em class="italic">Rule 2</em>:</p>
			<ul>
				<li><strong class="bold">Rule 1</strong>: The hyperplane must maximize the correct classification of instances. This basically means that the best line is the one that effectively separates data points belonging to different class labels while keeping those that belong to the same one together.<p>For instance, in the following diagram, although both lines are able to separate most instances into their correct class labels, line A would be selected by the model as the one that segregates the classes better than line B, which fails to classify two data points:</p><div id="_idContainer077" class="IMG---Figure"><img src="image/B15781_04_09.jpg" alt="Figure 4.9: Sample of hyperplanes that explain Rule 1&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 4.9: Sample of hyperplanes that explain Rule 1</p>
			<ul>
				<li><strong class="bold">Rule 2</strong>: The hyperplane must maximize its distance to the nearest data point of either of the class labels, which is also known as the <strong class="bold">margin</strong>. This rule helps the model become more robust, which means that the model is able to generalize the input data so that it works efficiently on unseen data. This rule is especially important in preventing new instances from being mislabeled.<p>For example, by looking at the following diagram, it is possible to conclude that both hyperplanes comply with <em class="italic">Rule 1</em>. Nevertheless, line A is selected, since it maximizes its distance to the nearest data points for both classes in comparison to the distance of line B to its nearest data point:</p><div id="_idContainer078" class="IMG---Figure"><img src="image/B15781_04_10.jpg" alt="Figure 4.10: Sample of hyperplanes that explain Rule 2&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 4.10: Sample of hyperplanes that explain Rule 2</p>
			<p>By default, the SVM algorithm uses a linear function to split the data points of the input data. However, this configuration can be modified by changing the kernel type of the algorithm. For example, consider the following diagram:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For scikit-learn's SVM algorithm, the kernel refers to the mathematical function to be used to split the data points, which can be linear, polynomial, or sigmoidal, among others. To learn more about the parameters for this algorithm, visit the following URL: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC</a>.</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B15781_04_11.jpg" alt="Figure 4.11: Sample observations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11: Sample observations</p>
			<p>To segregate these observations, the model would have to draw a circle or another similar shape. The algorithm handles this by using kernels (mathematical functions) that can introduce additional features to the dataset in order to modify the distribution of data points into a form that allows a line to segregate them. There are several kernels available for this, and the selection of one should be done by trial and error so that you can find the one that best classifies the data that's available.</p>
			<p>However, the default kernel for the SVM algorithm in scikit-learn is the <strong class="bold">Radial Basis Function</strong> (<strong class="bold">RBF</strong>) kernel. This is mainly because, based on several studies, this kernel has proved to work great for most data problems.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor126"/>Exercise 4.03: Applying the SVM Algorithm</h2>
			<p>In this exercise, we will apply the SVM algorithm to the Fertility dataset. The idea, which is the same as in previous exercises, is to determine whether the fertility level of an individual is affected by their demographics, their environmental conditions, and their previous medical conditions. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook to implement this exercise. Import pandas as well as the <strong class="source-inline">SVC</strong> class from scikit-learn's <strong class="source-inline">svm</strong> module:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.svm import SVC</p></li>
				<li>Load the <strong class="source-inline">fertility_Diagnosis</strong> dataset that you downloaded in <em class="italic">Exercise 4.01</em>, <em class="italic">Applying the NaÃ¯ve Bayes Algorithm</em>. Make sure to add the <strong class="source-inline">header = None</strong> argument to the <strong class="source-inline">read_csv</strong> function, considering that the dataset does not contain a header row:<p class="source-code">data = pd.read_csv("fertility_Diagnosis.csv", header=None)</p></li>
				<li>Split the data into <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>, considering that the class label is found under the column with the index equal to <strong class="source-inline">9</strong>. Use the following code to do so:<p class="source-code">X = data.iloc[:,:9]</p><p class="source-code">Y = data.iloc[:,9]</p></li>
				<li>Instantiate scikit-learn's <strong class="source-inline">SVC</strong> class and use the <strong class="source-inline">fit</strong> function to train the model using <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>:<p class="source-code">model = SVC()</p><p class="source-code">model.fit(X, Y)</p><p>Again, the output from running this code represents the summary of the model, along with its default hyperparameters, as follows:</p><p class="source-code">SVC(C=1.0, break_ties=False, cache_size=200,</p><p class="source-code">Â Â Â Â class_weight=None, coef0=0.0,</p><p class="source-code">Â Â Â Â decision_function_shape='ovr', degree=3,</p><p class="source-code">Â Â Â Â gamma='scale', kernel='rbf', max_iter=-1,</p><p class="source-code">Â Â Â Â probability=False, random_state=None, shrinking=True,</p><p class="source-code">Â Â Â Â tol=0.001, verbose=False)</p></li>
				<li>Finally, perform a prediction using the model that you trained previously, for the same instances that we used in <em class="italic">Exercise 4.01</em>, <em class="italic">Applying the NaÃ¯ve Bayes Algorithm</em>: â<strong class="source-inline">0.33</strong>, <strong class="source-inline">0.69</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">0.8</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">0.88</strong>.<p>Use the following code to do so:</p><p class="source-code">pred = model.predict([[-0.33,0.69,0,1,1,0,0.8,0,0.88]])</p><p class="source-code">print(pred)</p><p>The output is as follows:</p><p class="source-code">['N']</p><p>Again, the model predicts the instance's class label as <strong class="source-inline">N</strong>, meaning that the fertility of the subject has not been affected.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YyEMNX">https://packt.live/2YyEMNX</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2Y3nIR2">https://packt.live/2Y3nIR2</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>You have successfully trained an SVM model and performed a prediction.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor127"/>Activity 4.03: Training an SVM Model for Our Census Income Dataset</h2>
			<p>Continuing with your task of building a model that is capable of predicting a person's annual income, the final algorithm that you want to train is the Support Vector Machine. Follow these steps to implement this activity:</p>
			<ol>
				<li value="1">Open the Jupyter Notebook that you used for the previous activity and import the SVM algorithm from scikit-learn. </li>
				<li>Train the model using the <strong class="source-inline">fit</strong> method on the <strong class="source-inline">SVC</strong> class from scikit-learn. To train the model, use the training set data from the previous activity (<strong class="source-inline">X_train</strong> and <strong class="source-inline">Y_train</strong>).<p class="callout-heading">Note</p><p class="callout">The process of training the SVC class using the <strong class="source-inline">fit</strong> method may take a while. </p></li>
				<li>Finally, perform a prediction using the model that you trained previously, for a new instance with the following values for each feature: <strong class="source-inline">39</strong>, <strong class="source-inline">6</strong>, <strong class="source-inline">13</strong>, <strong class="source-inline">4</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">2174</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">40</strong>, <strong class="source-inline">38</strong>.<p>The prediction for the individual should be equal to zero, that is, the individual most likely has an income less than or equal to 50K.</p><p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found on page 238.</p></li>
			</ol>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor128"/>Error Analysis</h1>
			<p>In the previous chapter, we explained the importance of error analysis. In this section, the different evaluation metrics will be calculated for all three models that were created in the previous activities so that we can compare them.</p>
			<p>For learning purposes, we will compare the models using accuracy, precision, and recall metrics. This way, it will be possible to see that even though a model might be better in terms of one metric, it could be worse when measuring a different metric, which helps to emphasize the importance of choosing the right metric to measure your model according to the goal you wish to achieve.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor129"/>Accuracy, Precision, and Recall</h2>
			<p>As a quick reminder, in order to measure performance and perform error analysis, it is required that you use the <strong class="source-inline">predict</strong> method for the different sets of data (training, validation, and testing). The following code snippets present a clean way of measuring all three metrics on our three sets at once:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The following steps are to be performed after solving the activities of this chapter. This is mainly because the steps in this section correspond to a continuation of this chapter's activities. </p>
			<ol>
				<li value="1">First, the three metrics to be used are imported:<p class="source-code">from sklearn.metrics import accuracy_score, \</p><p class="source-code">precision_score, recall_score</p></li>
				<li>Next, we create two lists containing the different sets of data that will be used inside a <strong class="source-inline">for</strong> loop to perform the performance calculation on all sets of data for all models:<p class="source-code">X_sets = [X_train, X_dev, X_test]</p><p class="source-code">Y_sets = [Y_train, Y_dev, Y_test]</p></li>
				<li>A dictionary will be created, which will hold the value of each evaluation metric for each set of data for each model:<p class="source-code">metrics = {"NB":{"Acc":[],"Pre":[],"Rec":[]},</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â "DT":{"Acc":[],"Pre":[],"Rec":[]},</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â "SVM":{"Acc":[],"Pre":[],"Rec":[]}}</p></li>
				<li>A <strong class="source-inline">for</strong> loop is used to go through the different sets of data:<p class="source-code">for i in range(0,len(X_sets)):</p><p class="source-code">Â Â Â Â pred_NB = model_NB.predict(X_sets[i])</p><p class="source-code">Â Â Â Â metrics["NB"]["Acc"].append(accuracy_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_NB))</p><p class="source-code">Â Â Â Â metrics["NB"]["Pre"].append(precision_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_NB))</p><p class="source-code">Â Â Â Â metrics["NB"]["Rec"].append(recall_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_NB))</p><p class="source-code">Â Â Â Â pred_tree = model_tree.predict(X_sets[i])</p><p class="source-code">Â Â Â Â metrics["DT"]["Acc"].append(accuracy_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_tree))</p><p class="source-code">Â Â Â Â metrics["DT"]["Pre"].append(precision_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_tree))</p><p class="source-code">Â Â Â Â metrics["DT"]["Rec"].append(recall_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_tree))</p><p class="source-code">Â Â Â Â pred_svm = model_svm.predict(X_sets[i])</p><p class="source-code">Â Â Â Â metrics["SVM"]["Acc"].append(accuracy_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_svm))</p><p class="source-code">Â Â Â Â metrics["SVM"]["Pre"].append(precision_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_svm))</p><p class="source-code">Â Â Â Â metrics["SVM"]["Rec"].append(recall_score(Y_sets[i], \</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pred_svm))</p></li>
				<li>Print the metrics, as follows:<p class="source-code">print(metrics)</p><p>The output is as follows:</p><div id="_idContainer080" class="IMG---Figure"><img src="image/B15781_04_12.jpg" alt="Figure 4.12: Printing the metrics&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.12: Printing the metrics</p>
			<p>Inside the <strong class="source-inline">for</strong> loop, there are three blocks of code, one for each model we created in the previous activities. Each block of code performs the following actions.</p>
			<p>First, a prediction is made. The prediction is achieved by calling the <strong class="source-inline">predict</strong> method on the model and inputting a set of data. As this operation occurs inside a <strong class="source-inline">for</strong> loop, the prediction will occur for all sets of data.</p>
			<p>Next, the calculation of all three metrics is done by comparing the ground truth data with the prediction that we calculated previously. The calculation is appended to the dictionary that was created previously.</p>
			<p>From the preceding snippets, the following results are obtained:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B15781_04_13.jpg" alt="Figure 4.13: Performance results of all three models&#13;&#10;"/>
				</div>
			</div>
			<p> </p>
			<p class="figure-caption">Figure 4.13: Performance results of all three models</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Review the code to arrive at these results, which can be found in this book's GitHub repository, under the folder named <strong class="source-inline">Chapter04</strong>, by opening the file named <strong class="source-inline">Error analysis</strong>.</p>
			<p>Initially, the following inferences, in relation to selecting the best-fitted model, as well as with regard to the conditions that each model suffers from, will be done while considering only the values from the accuracy metric, assuming a Bayes error of close to 0 (meaning that the model could reach a maximum success rate of close to 1):</p>
			<ul>
				<li>Upon comparing the three accuracy scores of the NaÃ¯ve Bayes and the SVM models, it is possible to conclude that the models behave almost the same way for all three sets of data. This basically means that the models are generalizing the data from the training set, which allows them to perform well on unseen data. Nevertheless, the overall performance of the models is around 0.8, which is far from the maximum success rate. This means that the models may be suffering from high bias.</li>
				<li>Moreover, the performance of the decision tree model, in terms of the accuracy of the training set, is closer to the maximum success rate. However, the model is suffering from a case of overfitting, considering that the accuracy level of the model on the validation set is much lower than its performance on the training set. According to this, it would be possible to address the overfitting issue by adding more data to the training set or by fine-tuning the hyperparameters of the model, which would help to bring up the accuracy level of the validation and testing sets. Pruning the tree can help an overfitted model.</li>
			</ul>
			<p>Considering this, the researcher now has the required information to select a model and work on improving the results to achieve the maximum possible performance of the model.</p>
			<p>Next, for learning purposes, let's compare the results of all the metrics for the decision tree model. Although the values for all three metrics prove the existence of overfitting, it is possible to observe that the degree of overfitting is much larger for the precision and recall metrics. Also, it is possible to conclude that the performance of the model on the training set measured by the recall metric is much lower, which means that the model is not as good at classifying positive labels. This means that if the purpose of the case study was to maximize the number of positive classifications, regardless of the classification of negative labels, the model would also need to improve its performance on the training set. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding comparison is done to show that the performance of the same model can vary if measured with a different metric. According to this, it is crucial to choose the metric of relevance for the case study. </p>
			<p>Using the knowledge that you have gained from previous chapters, feel free to keep exploring the results shown in the preceding table.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor130"/>Summary</h1>
			<p>Using the knowledge from previous chapters, we started this chapter by performing an analysis of the Census Income dataset, with the objective of understanding the data that's available and making decisions about the pre-processing process. Three supervised learning classification algorithmsâthe NaÃ¯ve Bayes algorithm, the Decision Tree algorithm, and the SVM algorithmâwere explained, and were applied to the previously pre-processed dataset to create models that generalized to the training data. Finally, we compared the performance of the three models on the Census Income dataset by calculating the accuracy, precision, and recall on the different sets of data (training, validation, and testing).</p>
			<p>In the next chapter, we will look at <strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>), their different types, and their advantages and disadvantages. We will also use an ANN to solve the same data problem that was discussed in this chapter, as well as to compare its performance with that of the other supervised learning algorithms.</p>
		</div>
	</body></html>
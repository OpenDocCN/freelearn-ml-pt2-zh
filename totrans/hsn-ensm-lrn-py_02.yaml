- en: A Machine Learning Refresher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a sub field of **artificial intelligence** (**AI**) focused
    on the aim of developing algorithms and techniques that enable computers to learn
    from massive amounts of data. Given the increasing rate at which data is produced,
    machine learning has played a critical role in solving difficult problems in recent
    years. This success was the main driving force behind the funding and development
    of many great machine learning libraries that make use of data in order to build
    predictive models. Furthermore, businesses have started to realize the potential
    of machine learning, driving the demand for data scientists and machine learning
    engineers to new heights, in order to design better-performing predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter serves as a refresher on the main concepts and terminology, as
    well as an introduction to the frameworks that will be used throughout the book,
    in order to approach ensemble learning with a solid foundation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics covered in this chapter are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The various machine learning problems and datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate the performance of a predictive model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python environment setup and the required libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter01)'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/30u8sv8](http://bit.ly/30u8sv8).
  prefs: []
  type: TYPE_NORMAL
- en: Learning from data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is the raw ingredient of machine learning. Processing data can produce
    information; for example, measuring the height of a portion of a school's students
    (data) and calculating their average (processing) can give us an idea of the whole
    school's height (information). If we process the data further, for example, by
    grouping males and females and calculating two averages – one for each group,
    we will gain more information, as we will have an idea about the average height
    of the school's males and females. Machine learning strives to produce the most
    information possible from any given data. In this example, we produced a very
    basic predictive model. By calculating the two averages, we can predict the average
    height of any student just by knowing whether the student is male or female.
  prefs: []
  type: TYPE_NORMAL
- en: The set of data that a machine learning algorithm is tasked with processing
    is called the problem's dataset. In our example, the dataset consists of height
    measurements (in centimeters) and the child's sex (male/female). In machine learning,
    input variables are called features and output variables are called targets. In
    this dataset, the features of our predictive model consist solely of the students'
    sex, while our target is the students' height in centimeters. The predictive model
    that is produced and maps features to targets will be referred to as simply the model from
    now on, unless otherwise specified. Each data point is called an instance. In
    this problem, each student is an instance of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: When the target is a continuous variable (a number), it presents a regression
    problem, as the aim is to regress the target on the features. When the target
    is a set of categories, it presents a classification problem, as we try to assign
    each instance to a category or class.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in classification problems, the target class can be represented by
    a number; this does not mean that it is a regression problem. The most useful
    way to determine whether it is a regression problem is to think about whether
    the instances can be ordered by their targets. In our example, the target is height,
    so we can order the students from tallest to shortest, as 100 cm is less than
    110 cm. As a counter example, if the target was their favorite color, we could
    represent each color by a number, but we could not order them. Even if we represented
    red as one and blue as two, we could not say that red is "before" or "less than"
    blue. Thus, this counter example is a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Popular machine learning datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning relies on data in order to produce high-performing models.
    Without data, it's not even possible to create models. In this section, we'll
    present some popular machine learning datasets, which we will utilize throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Diabetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The diabetes dataset concerns 442 individual diabetes patients and the progression
    of the disease one year after a baseline measurement. The dataset consists of
    10 features, which are the patient's age, sex, **body mass index** (**bmi**),
    average **blood pressure** (**bp**), and six measurements of their blood serum.
    The dataset target is the progression of the disease one year after the baseline
    measurement. This is a regression dataset, as the target is a number.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, the dataset features are mean-centered and scaled such that the
    dataset sum of squares for each feature equals one. The following table depicts
    a sample of the diabetes dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **age** | **sex** | **bmi** | **bp** | **s1** | **s2** | **s3** | **s4**
    | **s5** | **s6** | **target** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.04 | 0.05 | 0.06 | 0.02 | -0.04 | -0.03 | -0.04 | 0.00 | 0.02 | -0.02 |
    151 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.00 | -0.04 | -0.05 | -0.03 | -0.01 | -0.02 | 0.07 | -0.04 | -0.07 | -0.09
    | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.09 | 0.05 | 0.04 | -0.01 | -0.05 | -0.03 | -0.03 | 0.00 | 0.00 | -0.03
    | 141 |'
  prefs: []
  type: TYPE_TB
- en: '| -0.09 | -0.04 | -0.01 | -0.04 | 0.01 | 0.02 | -0.04 | 0.03 | 0.02 | -0.01
    | 206 |'
  prefs: []
  type: TYPE_TB
- en: Breast cancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The breast cancer dataset concerns 569 biopsies of malignant and benign tumors.
    The dataset provides 30 features extracted from images of fine-needle aspiration
    biopsies that describe cell nuclei. The images provide information about the shape,
    size, and texture of each cell nucleus. Furthermore, for each characteristic,
    three distinct values are provided. The mean, the standard error, and the worst
    or largest value. This ensures that, for each image, the cell population is adequately
    described.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset target concerns the diagnosis, that is, whether a tumor is malignant
    or benign. Thus, this is a classification dataset. The available features are
    listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean radius
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean texture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean perimeter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean smoothness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean compactness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean concavity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean concave points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean symmetry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean fractal dimension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radius error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Texture error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perimeter error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoothness error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compactness error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concavity error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concave points error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Symmetry error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fractal dimension error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst radius
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst texture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst perimeter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst smoothness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst compactness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst concavity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst concave points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst symmetry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst fractal dimension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MNIST handwritten digit dataset is one of the most famous image recognition
    datasets. It consists of square images, 8 x 8 pixels, each containing a single
    handwritten digit. Thus, the dataset features are an 8 by 8 matrix, containing
    each pixel''s color in grayscale. The target consists of 10 classes, one for each
    digit from 0 to 9\. This is a classification dataset. The following figure is
    a sample from the handwritten digit dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67c53ed8-f82c-4835-9794-93a597511b7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample of the handwritten digit dataset
  prefs: []
  type: TYPE_NORMAL
- en: Supervised and unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning can be divided into many subcategories; two broad categories
    are supervised and unsupervised learning. These categories contain some of the
    most popular and widely used machine learning methods. In this section, we present
    them, as well as some toy example uses of supervised and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In examples such as those in the previous section, the data consisted of some
    features and a target; no matter whether the target was quantitative (regression)
    or categorical (classification). Under these circumstances, we call the dataset
    a labeled dataset. When we try to produce a model from a labeled dataset in order
    to make predictions about unseen or future data (for example, to diagnose a new
    tumor case), we make use of supervised learning. In simple cases, supervised learning
    models can be visualized as a line. This line's purpose is to either separate
    the data based on the target (in classification) or to closely follow the data
    (in regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates a simple regression example. Here, *y* is
    the target and *x* is the dataset feature. Our model consists of the simple equation
    *y*=2*x*-5\. As is evident, the line closely follows the data. In order to estimate
    the *y* value of a new unseen point, we calculate its value using the preceding
    formula. The following figure shows a simple regression with *y*=2*x*-5 as the
    predictive model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1680c76-c8ad-4c1f-92f7-7091d33a1870.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple regression with y=2x-5 as the predictive model
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, a simple classification problem is depicted. Here,
    the dataset features are *x* and *y*, while the target is the instance color.
    Again, the dotted line is *y*=2*x*-5, but this time we test whether the point
    is above or below the line. If the point''s *y* value is lower than expected (smaller),
    then we expect it to be orange. If it is higher (greater), we expect it to be
    blue. The following figure is a simple classification with *y*=2*x*-5 as the boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/210bb1bf-db70-4be2-ba9c-a344b16a1bf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple classification with y=2x-5 as boundary
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In both regression and classification, we have a clear understanding of how
    the data is structured or how it behaves. Our goal is to simply model that structure
    or behavior. In some cases, we do not know how the data is structured. In those
    cases, we can utilize unsupervised learning in order to discover the structure,
    and thus information, within the data. The simplest form of unsupervised learning
    is clustering. As the name implies, clustering techniques attempt to group (or
    cluster) data instances. Thus, instances that belong to the same cluster share
    many similarities in their features, while they are dissimilar to instances that
    belong in separate clusters. A simple example with three clusters is depicted
    in the following figure. Here, the dataset features are *x* and *y*, while there
    is no target.
  prefs: []
  type: TYPE_NORMAL
- en: 'The clustering algorithm discovered three distinct groups, centered around
    the points (0, 0), (1, 1), and (2, 2):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ab1932-b0fd-4959-b37a-583d2257a464.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering with three distinct groups
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another form of unsupervised learning is dimensionality reduction. The number
    of features present in a dataset equals the dataset's dimensions. Often, many
    features can be correlated, noisy, or simply not provide much information. Nonetheless,
    the cost of storing and processing data is correlated with a dataset's dimensionality.
    Thus, by reducing the dataset's dimensions, we can help the algorithms to better
    model the data.
  prefs: []
  type: TYPE_NORMAL
- en: Another use of dimensionality reduction is for the visualization of high-dimensional
    datasets. For example, using the t-distributed Stochastic Neighbor Embedding (t-SNE)
    algorithm, we can reduce the breast cancer dataset to two dimensions or components.
    Although it is not easy to visualize 30 dimensions, it is quite easy to visualize
    two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we can visually test whether the information contained within
    the dataset can be utilized to separate the dataset''s classes or not. The next
    figure depicts the two components on the *y* and *x* axis, while the color represents
    the instance''s class. Although we cannot plot all of the dimensions, by plotting
    the two components, we can conclude that a degree of separability between the
    classes exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a007fc1c-de3b-4a3f-9f7f-afeea4c6546e.png)'
  prefs: []
  type: TYPE_IMG
- en: Using t-SNE to reduce the dimensionality of the breast cancer dataset
  prefs: []
  type: TYPE_NORMAL
- en: Performance measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a highly quantitative field. Although we can gauge the performance
    of a model by plotting how it separates classes and how closely it follows data,
    more quantitative performance measures are needed in order to evaluate models.
    In this section, we present cost functions and metrics. Both of them are used
    in order to assess a model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Cost functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A machine learning model's objective is to model our dataset. In order to assess
    each model's performance, we define an objective function. These functions usually
    express a cost, or how far from perfect a model is. These cost functions usually
    utilize a loss function to assess how well the model performed on each individual
    dataset instance.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the most widely used cost functions are described in the following sections,
    assuming that the dataset has *n* instances, the target's true value for instance *i* is *t[i]* and
    the model's output is *y[i .]*
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**) or L1 loss is the mean absolute distance
    between the target''s real values and the model''s outputs. It is calculated as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e08a7ef6-713b-408c-af9f-ae0a55486bc5.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**) or L2 loss is the mean squared distance between
    the target''s real values and the model''s output. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3233ee2b-4852-4c38-baeb-1c1482ac59b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross entropy loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cross entropy loss is used in models that output probabilities between 0 and
    1, usually to express the probability that an instance is a member of a specific
    class. As the output probability diverges from the actual label, the loss increases.
    For a simple case where the dataset consists of two classes, it is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca72266c-12e1-4d41-828d-049d20b0a341.png)'
  prefs: []
  type: TYPE_IMG
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cost functions are useful when we try to numerically optimize our models. But
    as humans, we need metrics that are useful and intuitive to understand and report.
    As such, there are a number of metrics available that give insight into a model's
    performance. The most common metrics are presented in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Classification accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest and easiest to grasp of all, classification accuracy refers to
    the percentage of correct predictions. In order to calculate accuracy, we divide
    the number of correct predictions by the total number of instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be71cf1e-4842-4b5c-888f-7226840eb631.png)'
  prefs: []
  type: TYPE_IMG
- en: In order for accuracy to hold any substantial value, the dataset must contain
    an equal number of instances belonging to each class. If the dataset is unbalanced,
    accuracy will be affected. For example, if a dataset consists of 90% class A and
    10% class B, a model that predicts each instance as class A will have 90% accuracy,
    although it will hold zero predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to tackle the preceding problem, it is possible to utilize a confusion
    matrix. Confusion matrices present the number of instances correctly or incorrectly
    predicted as each possible class. In a dataset with only two classes (Yes and
    No), a confusion matrix has the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **n = 200** | **Predicted: Yes** | **Predicted: No** |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Yes** | 80 | 70 |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: No** | 20 | 30 |'
  prefs: []
  type: TYPE_TB
- en: 'There are four cells, each corresponding to one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives** (**TP**): When the target belongs to the Yes class and the
    model predicted Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negatives** (**TN**): When the target belongs to the No class and the
    model predicted No'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives** (**FP**): When the target belongs to the No class and the
    model predicted Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives** (**FN**): When the target belongs to the Yes class and
    the model predicted No'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Confusion matrices provide information about the balance of the true and predicted
    classes. In order to calculate the accuracy from a confusion matrix, we divide
    the sum of TP and TN by the total number of instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56ea3bcb-3940-47ff-bae2-6699fa045bf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Sensitivity, specificity, and area under the curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Area under the curve (AUC) concerns binary classification datasets, and it
    depicts the probability that the model will rank any given instance correctly.
    In order to define it, we must first define sensitivity and specificity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity** (**True Positive Rate**): Sensitivity is the percentage of
    positive instances correctly predicted as positive, relative to all positive instances.
    It is calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/44038c3e-a8c4-466f-8c94-001702317038.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Specificity** (**False Positive Rate**): Specificity is the percentage of
    negative instances incorrectly predicted as positive, relative to all negative
    instances. It is calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f1bd2469-3805-4932-86c7-e075947d82ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By iteratively computing (1-specificity) and sensitivity at specific intervals
    (for example, in 0.05 increments), we can see how the model behaves. The intervals
    concern the model''s output probability for each instance; for example, we first
    compute them for all instances with an estimated probability of belonging to the
    Yes class of less than 0.05\. Then, we re-compute for all instances with an estimated
    probability of less than 0.1 and so on. The result is depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/289e210c-25ec-47c6-a2be-a45556c5018e.png)'
  prefs: []
  type: TYPE_IMG
- en: Receiver operator characteristic curve
  prefs: []
  type: TYPE_NORMAL
- en: 'The straight line represents an equal probability of ranking an instance correctly
    or incorrectly: a random model. The orange line (ROC curve) depicts the model''s
    probability. If the ROC curve is below the straight line, it means that the model
    performs worse than a random, uninformed model.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision, recall, and the F1 score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Precision gauges how a model behaves by quantifying the percentage of instances
    correctly classified as a specific class, relative to all instances predicted
    as the same class. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/996f726a-e333-4ba7-b981-460abc3ff77a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall is another name for sensitivity. The harmonic mean of precision and
    recall is called the F1 score and is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89a6f1fc-1249-429a-802a-e77de45ad214.png)'
  prefs: []
  type: TYPE_IMG
- en: The reason to use the harmonic mean instead of a simple average is that the
    harmonic mean is greatly affected by imbalances between the two values (precision
    and recall). Thus, if either precision or recall is significantly smaller than
    the other, the F1 score will reflect this imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there are various metrics that indicate a model's performance, it is
    important to carefully set the testing environment. One of the most important
    things is to split the dataset into two parts. One part of the dataset will be
    utilized by the algorithm in order to generate a model; the second part will be
    utilized to assess the model. These are usually called the train and test set.
  prefs: []
  type: TYPE_NORMAL
- en: The train set is available to the algorithm to generate and optimize a model,
    using any cost function. After the algorithm is finished, the produced model is
    tested on the test set, in order to assess its predictive ability on unseen data.
    While the algorithm may produce a model that performs well on the train set (in-sample
    performance), it may not be able to generalize and perform as well on the test
    set (out-of-sample performance). This can be attributed to many factors – covered
    in the next chapter. Some of the problems that arise can be tackled with the use
    of ensembles. Nonetheless, if the algorithm is presented with low-quality data,
    there is little that can be done to improve out-of-sample performance.
  prefs: []
  type: TYPE_NORMAL
- en: In order to obtain a fair estimate, we sometimes iteratively split different
    parts of a dataset into fixed-size train and test sets, say, 90% train and 10%
    test, until we have tested the whole dataset. This is called K-fold cross validation.
    In the case of a 90% to 10% split, it is called 10-fold cross validation, because
    we need to perform it 10 times in order to get an estimate for the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of machine learning algorithms, for both supervised and unsupervised
    learning. In this book, we will cover some of the most popular algorithms that
    can be utilized within ensembles. In this chapter, we will go over the key concepts
    behind each algorithm, the basic algorithms, and the libraries that implement
    them in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Python packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to leverage the power of any programming language, libraries are essential.
    They provide a convenient and tested implementation of many algorithms. In this
    book, we will be using Python 3.6 along with the following libraries: NumPy, for
    its excellent implementation of numerical operators and matrices; Pandas, for
    its convenient data manipulation methods; Matplotlib, to visualize our data; scikit-learn,
    for its excellent implementations of various machine learning algorithms, and
    Keras to build neural networks, utilizing its Pythonic, intuitive interface. Keras
    is an interface for other frameworks, such as TensorFlow, PyTorch, and Theano.
    The specific versions of each library used in this book are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: numpy==1.15.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas==0.23.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn==0.19.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: matplotlib==2.2.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras==2.2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common class of machine learning algorithm is supervised learning algorithms.
    These concern problems where data has a known structure. This means that each
    data point has a specific value related to it that we wish to model or predict.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is one of the simplest machine learning algorithms. The **Ordinary
    Least Squares** (**OLS**) regression of the form *y=ax+b *attempts to optimize
    the *a* and *b* parameters in order to fit the data. It uses MSE as its cost function.
    As the name implies, it is able to solve regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the scikit-learn implementation of OLS to try and model the diabetes
    dataset (the dataset is provided with the library):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first section deals with importing libraries and loading data. We use the
    `LinearRegression` implementation that exists in the `linear_model` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The second section splits the data into a train and a test set. For this example,
    we used the first 400 instances as the train set and the other 42 as the test
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next section instantiates a linear regression object with `ols = LinearRegression()`.
    It then optimizes the parameters, or fits the model with our training instances,
    using `ols.fit(train_x, train_y)`. Finally, by using the `metrics` package, we
    calculate the MSE and *R²* of our model, using the test data in Section 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The code''s output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Another form of regression, logistic regression, attempts to model the probability
    that an instance belongs to one of two classes. Again, it attempts to optimize
    the *a* and *b* parameters in order to model *p=1/(1+e^(-(ax+b)))* . Once again,
    using scikit-learn and the breast cancer dataset, we can create and evaluate a
    simple logistic regression. The following code sections are similar to the preceding
    ones, but this time we''ll use classification accuracy and a confusion matrix
    rather than *R²* as a metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The test classification accuracy achieved with this model is 95%, which is
    quite good. Furthermore, the confusion matrix that follows here indicates that
    the model does not try to take advantage of class imbalances. Later in this book,
    we will learn how to further increase the classification accuracy with the use
    of ensemble methods. The following table shows the logit model confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Malignant** | 38 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Benign** | 8 | 122 |'
  prefs: []
  type: TYPE_TB
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Support vector machines or SVMs use a subset of training data, specifically
    data points near the edge of each class, in order to define a separating hyperplane
    (in two dimensions, a line). These edge cases are called support vectors. The
    goal of an SVM is to find the hyperplane that maximizes the margin (distance)
    between the support vectors (depicted in the following figure). In order to classify
    nonlinear separable classes, SVMs use the kernel trick to map data in a higher
    dimensional space, where it can become linearly separable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9689d3f-38d0-478f-abdb-313232de0725.png)'
  prefs: []
  type: TYPE_IMG
- en: SVM margins and support vectors
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about the kernel trick, this is a good starting point: [https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick).
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, an SVM is implemented under `sklearn.svm`, both for regression
    with `sklearn.svm.SVR` and classification with `sklearn.svm.SVC`. Once again,
    we''ll test the algorithm''s potential using scikit-learn and the code utilized
    in the regression examples. Using an SVM with a linear kernel on the breast cancer
    dataset results in 95% accuracy and the following confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Malignant** | 39 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Benign** | 9 | 121 |'
  prefs: []
  type: TYPE_TB
- en: On the diabetes dataset, by fine-tuning the *C* parameter to 1,000 during the `(svr
    = SVR(kernel='linear', C=1e3))` object instantiation, we are able to achieve an R2
    of 0.71 and an MSE of 1622.36, marginally better than the logit model.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks, inspired by the way biological brains are connected, consist
    of many neurons, or computational modules, organized in layers. Data is provided
    at the input layer and predictions are produced at the output layer. All intermediate
    layers are called hidden layers. Neurons that belong to the same layer are not
    connected to each other, only to neurons that belong in other layers. Each neuron
    can have multiple inputs, where each input is multiplied by a specific weight
    and the sum of multiplied inputs is passed to an activation function that defines
    the neuron''s output. Common activation functions include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sigmoid** | **Tanh** | **ReLU** | **Linear** |'
  prefs: []
  type: TYPE_TB
- en: '|     ![](img/15d85d5f-1a56-4f98-a267-9bec857704fa.png) |        ![](img/3b36b9f5-f2db-4490-b19a-977905bd2c45.png)
    |        ![](img/263f0f9f-6ebc-4155-a347-2f46288986d6.png) |           ![](img/1c60ae33-f144-43ff-890a-0739f7e46fc1.png)
    |'
  prefs: []
  type: TYPE_TB
- en: The network's goal is to optimize each neuron's weights, such that the cost
    function is minimized. Neural networks can be either used for regression, where
    the output layer consists of a single neuron, or classification, where it consists
    of many neurons, usually equal to the number of classes. There are a number of
    optimizing algorithms or optimizers available for neural networks. The most common
    is stochastic gradient descent or SGD. The main idea is that the weights are updated
    based on the direction and magnitude (first derivative) of the error's gradient,
    multiplied by a factor called the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Variations and extensions have been proposed that take into account the second
    derivative, adapt the learning rate, or use the momentum of previous weight changes
    to update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Although the concept of neural networks has existed for a long time, recently
    their popularity has greatly increased with the advent of deep learning. Modern
    architectures consist of convolutional layers, where each layer's weights consist
    of matrices, and the output is calculated by sliding the weight matrix onto the
    input. Another type of layers, max pooling layers, calculates the output as the
    maximum input element again by sliding a fixed-size window onto the input. Recurrent
    layers retain information about their previous
  prefs: []
  type: TYPE_NORMAL
- en: states. Finally, fully connected layers are traditional neurons, as described
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn implements traditional neural networks, under the `sklearn.neural_network`
    package. Once again, using the preceding examples, we''ll try to model the diabetes
    and breast cancer datasets. On the diabetes dataset, we''ll use `MLPRegressor`
    with **Stochastic Gradient Descent** (**SGD**) as the optimizer, with `mlpr =
    MLPRegressor(solver=''sgd'')`. Without any further fine-tuning, we achieve an
    R² of 0.64 and an MSE of 1977\. On the breast cancer dataset, using the **Limited-memory
    Broyden–Fletcher–Goldfarb–Shanno** (**LBFGS**) optimizer, with `mlpc = MLPClassifier(solver=''lbfgs'')`,
    we get a classification accuracy of 93% and a competent confusion matrix. The
    following table shows the neural network confusion matrix for the breast cancer
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Malignant** | 35 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Benign** | 8 | 122 |'
  prefs: []
  type: TYPE_TB
- en: 'A very important note on neural networks: the initial weights of a network
    are randomly initialized. Thus, the same code can perform differently if it is
    executed several times. In order to ensure non-random (non-stochastic) execution,
    the initial random state of the network must be fixed. The two scikit-learn classes
    implement this feature through the `random_state` parameter in the object constructor.
    In order to set the random state to a specific seed value, the constructor must
    be called as follows: `mlpc = MLPClassifier(solver=''lbfgs'', random_state=12418)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are less of a black box than other machine learning algorithms.
    They can easily explain how they produce a prediction, which is called **interpretability**.
    The main concept is that they produce rules by splitting the training set using
    the provided features. By iteratively splitting the data, a tree form is produced,
    thus this is where their name derives from. Let's consider a dataset where the
    instances are individual persons deciding on their vacations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset features consist of the person''s age and available money, while
    the target is their preferred destination, one of either **Summer Camp**, **Lake**,
    or **Bahamas**. A possible decision tree model is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f80c05fa-627e-4833-b9ae-d3ae34c8aa1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree model for the vacation destination problem
  prefs: []
  type: TYPE_NORMAL
- en: As is evident, the model can explain how it produces any predictions. The way
    that the model itself is built is by trying to select the feature and threshold
    that maximize the information produced. Roughly, this means that the model will
    try to iteratively split the dataset in a way that separates the greatest number
    of remaining instances.
  prefs: []
  type: TYPE_NORMAL
- en: Although intuitive to understand, decision trees can produce unreasonable models,
    with the extreme being the generation of so many rules that, eventually, each
    rule combination leads to a single instance. In order to avoid such models, we
    can restrict the model by requiring that it does not exceed a specific depth (maximum
    number of consecutive rules), or that each node has at least a minimum number
    of instances before it can be further split.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, decision trees are implemented under the `sklearn.tree` package,
    with `DecisionTreeClassifier` and `DecisionTreeRegressor`. In our examples, using
    `DecisionTreeRegressor` with `dtr = DecisionTreeRegressor(max_depth=2)`, we achieve
    an R² of 0.52 and an MSE of 2655\. On the breast cancer dataset, using `dtc =
    DecisionTreeClassifier(max_depth=2)`, we achieve 89% accuracy and the following
    confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Malignant** | 37 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Benign** | 17 | 113 |'
  prefs: []
  type: TYPE_TB
- en: 'Although not the best-performing algorithm so far, we can clearly see how each
    individual was classified, by exporting the tree to the `graphviz` format with
    `export_graphviz(dtc, feature_names=bc.feature_names, class_names=bc.target_names,
    impurity=False)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9da5bb06-3901-4bb1-9903-c3e921d77f90.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision tree generated for the breast cancer dataset
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**k-Nearest Neighbors** (**k-NN**) is a relatively simple machine learning
    algorithm. Each instance is classified by comparing it to its K-nearest examples
    as the majority class. In regression, the average value of neighbors is used.
    Scikit-learn''s implementation lies within the `sklearn.neighbors` package of
    the library. As it is the naming convention of the library, `KNeighborsClassifier`
    implements the classification and `KNeighborsRegressor` implements the regression
    version of the algorithm. Using them in our examples, the regressor generates
    an R² of 0.58 with an MSE of 2342, while the classifier achieves 93% accuracy.
    The following table shows the k-NN confusion matrix for the breast cancer dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Malignant** | 37 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Target: Benign** | 9 | 121 |'
  prefs: []
  type: TYPE_TB
- en: K-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'K-means is a clustering algorithm that presents similarities to k-NN. A number
    of cluster centers are produced, and each instance is assigned to its nearest
    cluster. After all instances are assigned to a cluster, the centroid of the cluster
    becomes the new center, until the algorithm converges to a stable solution. In
    scikit-learn, this algorithm is implemented in `sklearn.cluster.KMeans`. We can
    try to cluster the first two features of the breast cancer dataset: the mean radius
    and the texture of the FNA imaging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the required data and libraries, while retaining only the first
    two features of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we fit the cluster on the data. Note that we don''t have to split the
    data into train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Following that, we create a two-dimensional mesh and cluster every point, in
    order to plot the cluster areas and boundaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the actual data, color-mapped to its respective clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a two-dimensional image with color-coded boundaries of each cluster,
    as well as the instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5590354e-8202-4dda-9414-be4d73bfa4ec.png)'
  prefs: []
  type: TYPE_IMG
- en: K-means clustering of the first two features of the breast cancer dataset
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the basic datasets, algorithms, and metrics that
    we will use throughout the book. We talked about regression and classification
    problems, where datasets have not only features but also targets. We called these
    labeled datasets. We also talked about unsupervised learning, in the form of clustering
    and dimensionality reduction. We introduced cost functions and model metrics that
    we will use to evaluate the models that we generate. Furthermore, we presented
    the basic learning algorithms and Python libraries that we will utilize in the
    majority of our examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will introduce the concepts of bias and variance, as
    well as the concept of ensemble learning. Some key points to remember are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We try to solve a regression problem when the target variable is a continuous
    number and its values have a meaning in terms of magnitude, such as speed, cost,
    blood pressure, and so on. Classification problems can have their targets coded
    as numbers, but we cannot treat them as such. There is no meaning in trying to
    sort colors or foods based on the number they are assigned during a problem's
    encoding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost functions are a way to quantify how far away a predictive model is from
    modelling data perfectly. Metricsprovide information that is easier for humans
    to understand and report.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the algorithms presented in this chapter have implementations for both
    classification and regression problems in scikit-learn. Some are better suited
    to particular tasks, at least without tuning their hyper parameters. Decision
    trees produce models that are easily interpreted by humans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL

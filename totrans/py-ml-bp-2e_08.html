<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Classifying Images with Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to explore the vast and awesome world of computer vision.</p>
<p>If you've ever wanted to construct a predictive machine learning model using image data, this chapter will serve as an easily-digestible and practical resource. We'll go step by step through building an image-classification model, cross-validating it, and then building it in a better way. At the end of this chapter, we'll have a <em>darn good</em> model and discuss some paths for future enhancement.</p>
<p>Of course, some background in the fundamentals of predictive modeling will help this to go smoothly. As you'll soon see, the process of converting images into usable features for our model might might feel new, but once our features are extracted, the model-building and cross-validation processes are exactly the same.</p>
<p>In this chapter, we're going to build a convolutional neural network to classify images of articles of clothing from the Zalando Research dataset—a dataset of 70,000 images, each depicting 1 of 10 possible articles of clothing such as T-shirt/top, a pair of pants, a sweater, a dress, a coat, a sandal, a shirt, a sneaker, a bag, or an ankle boot. But first, we'll explore some of the fundamentals together, starting with image-feature extraction and walking through how convolutional neural networks work.  </p>
<p>So, let's get started. Seriously!.</p>
<p>Here's what we'll cover in this chapter:</p>
<ul>
<li style="font-weight: 400">Image-feature extraction</li>
<li style="font-weight: 400">Convolutional neural networks:
<ul>
<li style="font-weight: 400">Network topology</li>
<li style="font-weight: 400">Convolutional layers and filters</li>
<li style="font-weight: 400">Max pooling layers</li>
<li style="font-weight: 400">Flattening</li>
<li style="font-weight: 400">Fully-connected layers and output</li>
</ul>
</li>
<li style="font-weight: 400">Building a convolutional neural network to classify images in the Zalando Research dataset, using Keras</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image-feature extraction</h1>
                </header>
            
            <article>
                
<p>When dealing with unstructured data, be it text or images, we must first convert the data into a numerical representation that's usable by our machine learning model. The process of converting data that is non-numeric into a numerical representation is called <strong>feature extraction</strong>. For image data, our features are the pixel values of the image.</p>
<p>First, let's imagine a 1,150 x 1,150 pixel grayscale image. A 1,150 x 1,150 pixel image will return a 1,150 x 1,150 matrix of pixel intensities. For grayscale images, the pixel values can range from 0 to 255, with 0 being a completely black pixel, and 255 being a completely white pixel, and shades of gray in between.  </p>
<p>To demonstrate what this looks like in code, let's extract the features from our grayscale cat burrito. The image is available on GitHub at <a href="https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08" target="_blank">https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08</a> as <kbd>grayscale_cat_burrito.jpg</kbd>. <a href="https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08" target="_blank"/></p>
<div class="packt_infobox">I've made the image assets used throughout this chapter available to you at <a href="https://github.com/mroman09/packt-image-assets">https://github.com/mroman09/packt-image-assets</a>. You can find our cat burritos there!</div>
<p>Let's now take a look at a sample of this in the following code:</p>
<pre><strong>import matplotlib.image as mpimg</strong><br/><strong>import matplotlib.pyplot as plt</strong><br/><strong>import pandas as pd</strong><br/><strong>%matplotlib inline</strong><br/><br/><strong>cat_burrito = mpimg.imread('images/grayscale_cat_burrito.jpg')</strong><br/><strong>cat_burrito</strong></pre>
<div class="packt_tip">If you're unable to read a <kbd>.jpg</kbd> by running the preceding code, just install <kbd>PIL</kbd> by running <kbd>pip install pillow</kbd>. </div>
<p>In the preceding code, we imported <kbd>pandas</kbd> and two submodules: <kbd><span>image</span></kbd><span> and </span><kbd><span>pyplot</span></kbd>, from <kbd>matplotlib</kbd>. We used the <kbd>imread</kbd> method from <kbd>matplotlib.image</kbd> to read-in the image.</p>
<p>Running the preceding code gives us the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1089 image-border" src="assets/cd46ced0-2f96-410d-9801-0f7aa0f1fb90.png" style="width:31.42em;height:9.25em;"/></p>
<p>The output is a two-dimensional <kbd>numpy</kbd> ndarray that contains the features of our model. As with most applied machine learning applications, there are several preprocessing steps you'll want to perform on these extracted features, some of which we'll explore together on the Zalando fashion dataset later in this chapter, but these are the raw extracted features of the image!</p>
<p>The shape of the extracted features for our grayscale image is <kbd>image_height</kbd> rows x <kbd>image_width</kbd> columns. We can check the shape easily by running the following:</p>
<pre><strong>cat_burrito.shape</strong></pre>
<p>The preceding code returns this output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1090 image-border" src="assets/69f79aee-22b1-481c-b230-2035e84f9d23.png" style="width:7.92em;height:1.83em;"/></p>
<p>We can check the maximum and minimum pixel values in our ndarray easily, too:</p>
<pre><strong>print(cat_burrito.max())</strong><br/><strong>print(cat_burrito.min())</strong></pre>
<p>This returns the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1091 image-border" src="assets/0eb77b3e-f4b9-483c-9426-4fa4498031ae.png" style="width:3.83em;height:3.33em;"/></p>
<p class="mce-root"/>
<p>Finally, we can display our grayscale image from our ndarray by running this code:</p>
<pre><strong>plt.axis('off')</strong><br/><strong>plt.imshow(cat_burrito, cmap='gray');</strong></pre>
<p>The preceding code returns our image, which is available at <a href="https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08" target="_blank">https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08</a> as <kbd>output_grayscale_cat_burrito.png</kbd>.</p>
<p>The feature-extraction process for color images is identical; however, with color images, the shape of our ndarray output will be three-dimensional—a <strong>tensor</strong>—representing the <strong>red, green, and blue</strong> (<strong>RGB</strong>) pixel values of our image. Here, we'll carry out the same process as before, this time on a color version of the cat burrito. <span>The image is available on GitHub at </span><a href="https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08" target="_blank">https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08</a><span> as <kbd>color_cat_burrito.jpg</kbd>.</span></p>
<p><span>Let's extract the features from our color version of the cat burrito by using the following code:</span></p>
<pre><strong>color_cat_burrito = mpimg.imread('images/color_cat_burrito.jpg')</strong><br/><strong>color_cat_burrito.shape</strong></pre>
<p>Running this code returns the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1094 image-border" src="assets/642c0bf0-c57c-491e-92e5-b0923f98f55f.png" style="width:9.33em;height:1.83em;"/></p>
<p>Again, here we see that this image contains three channels. Our <kbd>color_cat_burrito</kbd> variable is a tensor that contains three matrices that tell us what the RGB values are for each pixel in our image.</p>
<p>We can display the color image from our ndarray by running the following:</p>
<pre><strong>plt.axis('off')</strong><br/><strong>plt.imshow(color_cat_burrito);</strong></pre>
<p>This returns our color image. <span>The image is available on GitHub at </span><a href="https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08" target="_blank">https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints-Second-Edition/tree/master/Chapter08</a><span> as <kbd>output_color_cat_burrito.png</kbd>.</span></p>
<p>This is the first step of our image-feature extraction. We've taken a single image at a time and converted those images into numeric values using just a few lines of code. In doing so, we saw that extracting features from grayscale images produces a two-dimensional ndarray and extracting features from color images produces a tensor of pixel-intensity values.<br/>
However, there's a slight problem. Remember, this is just a single image, a single training sample, a single <em>row</em> of our data. In the instance of our grayscale image, if we were to flatten this matrix into a single row, we would have <kbd>image_height</kbd> x <kbd>image_width</kbd> columns, or in our case, 1,322,500 columns. We can confirm that in code by running the following snippet:</p>
<pre><strong># flattening our grayscale cat_burrito and checking the length</strong><br/><strong>len(cat_burrito.flatten())</strong></pre>
<p>This is an issue! As with other machine learning modeling tasks, high dimensionality leads to model-performance issues. At this magnitude of dimensionality, any model we build will likely overfit, and training times will be slow.</p>
<p>This dimensionality problem is endemic to computer-vision tasks of this sort. Even a dataset of a lower resolution, 400 x 400 pixel grayscale cat burritos, would leave us with 160,000 features per image.</p>
<p>There is, however, a known solution to this problem: convolutional neural networks. In the next section, we'll continue our feature-extraction process using convolutional neural networks to build lower-dimensional representations of these raw image pixels. We'll go over the mechanics of how they work and continue to build an idea of why they're so performant in image-classification tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p>Convolutional neural networks are a class of neural network that resolve the high-dimensionality problem we alluded to in the previous section, and, as a result, excel at image-classification tasks. It turns out that image pixels in a given image region are highly correlated—they tell us similar information about that specific image region. Accordingly, using convolutional neural networks, we can scan regions of an image and summarize that region in lower-dimensional space. As we'll see, these lower-dimensional representations, called <strong>feature maps</strong>, tell us many interesting things about the presence of all sorts of shapes—from the simplest lines, shadows, loops, and swirls, to very abstract, complex forms specific to our data, in our case, cat ears, cat faces, or tortillas—and do this in fewer dimensions than the original image.</p>
<p>After using convolutional neural networks to extract these lower-dimensional features from our images, we'll pass the output of the convolutional neural network into a network suitable for the classification or regression task we want to perform. In our case, when modeling the Zalando research dataset, the output of our convolutional neural network will be passed into a fully-connected neural network for multi-class classification.</p>
<p>But how does this work? There are several key components we'll discuss with respect to convolutional neural networks on grayscale images, and these are all important for building our understanding.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network topology</h1>
                </header>
            
            <article>
                
<p>You may have encountered a diagram similar to the aforementioned that depicts a convolutional neural network to a feedforward neural network architecture. We'll be building something such as this very soon! But what's being depicted here? Check it out:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1186 image-border" src="assets/82007989-4f27-47f2-bd42-69f1e2c7a49c.png" style="width:52.58em;height:19.33em;"/></p>
<p><span><span>In the preceding diagram, on the very left, we have </span>our input. These are the extracted features of our image, the matrix (as was the case with the grayscale cat burrito) of values ranging from 0 to 255 that describe the pixel intensities present in the image.</span></p>
<p>Next, we pass the data through alternating convolutional and max-pooling layers. These layers define the convolutional neural network component of the architecture depicted. We'll describe what each of these layer types do in the following two sections.</p>
<p>After this, we pass the data to a fully-connected layer before arriving at the output layer. These two layers describe a fully-connected neural network. You're free to use any multi-class classification algorithm you like here, instead of a fully-connected neural network—a <strong>logistic regression</strong> or <strong>random forest classifier</strong>, perhaps—but for our dataset, we'll be using a fully-connected neural network.</p>
<p>The output layer depicted is the same as for any other multi-class classifier. Sticking with our cat burrito example, let's suppose we were building a model to predict what kind of cat burrito an image was from five distinct classes: chicken cat burrito, steak cat burrito, cat burrito al pastor, vegetarian cat burrito, or fish cat burrito (I'll let you use your imagination to visualize what our training data might look like). The output layer would be the predicted probability that the image belonged to one of the five classes, with <kbd>max(probability)</kbd> indicating what our model believes to be the most likely class.</p>
<p>At a high level, we've walked through the architecture, or <strong>topology</strong> of the preceding network. We've discussed our input versus the convolutional neural network component versus the fully-connected neural network component of the preceding topology. Let's dig just a bit deeper now and add some concepts that allow us to describe the topology in more detail:</p>
<ul>
<li>How many convolutional layers does the network have? Two.</li>
<li>And in each convolutional layer, how many feature maps are there? There are seven in convolutional layer 1 and 12 in convolutional layer 2.</li>
<li>How many pooling layers does the network have? Two.</li>
<li>How many fully-connected layers are there? One.</li>
<li>How many <strong>neurons</strong> are in the fully-connected layer? 10.</li>
<li>What is the output? Five.</li>
</ul>
<p>The modeler's decision to use two convolutional layers versus any other number or just a single fully-connected layer versus any other number should be thought of as the <strong>hyperparameters</strong> of the model. That is, it's something that we, as the modelers, should experiment with and cross-validate but not a parameter our model is explicitly learning and optimizing.</p>
<p>There are other useful things you can infer about the problem you're solving just by looking at the network's topology. As we discussed, the fact that our network's output layer contains five nodes lets us know that this neural network was designed to solve a multi-class classification task for which there are five classes. If it were a regression or a binary classification problem, our network's architecture would (in most cases) have a single output node. We also know that the modeler used seven filters in the first convolutional layer and 12 kernels in the second convolutional layer because of the number of feature maps resulting from each layer (we'll discuss what these kernels are in some more detail in the next section).</p>
<p>Great! We learned some useful jargon that will help us describe our networks and build our conceptual understanding of how they work. Now let's explore the convolutional layers of our architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional layers and filters</h1>
                </header>
            
            <article>
                
<p>Convolutional layers and filters are at the heart of convolutional neural networks. In these layers, we slide a filter (also referred to in this text as a <strong>window</strong> or <strong>kernel</strong>) over our ndarray feature and take the inner product at each step. Convolving our ndarray and kernel in this way results in a lower-dimensional image representation. Let's explore how this works on this grayscale image (available in image-assets repository):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1096 image-border" src="assets/cfd2a688-480d-4f87-82e0-cd686e4bdf3c.png" style="width:20.33em;height:20.50em;"/></p>
<p>The preceding image is a 5 x 5 pixel grayscale image shows a black diagonal line against a white background.</p>
<p>Extracting the features from the following diagram, we get the following matrix of pixel intensities:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1097 image-border" src="assets/f0fe030e-ccaa-4d16-9edb-32c7012b9689.png" style="width:14.67em;height:13.67em;"/></p>
<p>Next, let's assume we (or Keras) instantiate the following kernel:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1098 image-border" src="assets/d0f28169-8372-4341-954f-3867f5b576af.png" style="width:15.67em;height:16.92em;"/></p>
<p>We'll now visualize the convolution process. The movement of the window starts from the top, left of our image matrix. We'll slide the window right by a predetermined stride size. In this case, our stride size will be 1, but in general the stride size should be considered another hyperparameter of your model. Once the window reaches the rightmost edge of the image, we'll slide our window down by 1 (our stride size), move the window back to the leftmost edge of the image, and start the process of taking the inner product again.</p>
<p>Now let's do this step by step:</p>
<ol>
<li class="mce-root">Slide the kernel over the top-left part of the matrix and calculate the inner product:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1099 image-border" src="assets/16616ed8-ac7b-4479-84a4-013bb46bb2c5.png" style="width:46.17em;height:20.67em;"/></p>
<p style="padding-left: 60px">I'll explicitly map out the inner product for this first step so that you can easily follow along:  </p>
<p style="padding-left: 60px"><kbd>(0x0)+(255x0)+(255x0)+(255x0)+(0x1)+(255x0)+(255x0)+(255x0)+(0x0) = 0</kbd></p>
<p style="padding-left: 60px">We write the result to our feature map and continue!</p>
<ol start="2">
<li class="mce-root"><span>Take the inner product and write the result to our feature map:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1100 image-border" src="assets/6a00c530-8e36-4547-9038-4cfadbd29e61.png" style="width:42.17em;height:19.50em;"/></p>
<ol start="3">
<li class="mce-root">Step 3:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1101 image-border" src="assets/65e5b537-dd45-4afd-a1e5-6f1679b47ddb.png" style="width:42.42em;height:19.00em;"/></p>
<ol start="4">
<li class="mce-root">We've reached the rightmost edge of the image. Slide the window down by 1, our stride size, and start the process again at the leftmost edge of the image:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1102 image-border" src="assets/a19790b1-455f-4a34-96f0-aeb86d966c6a.png" style="width:43.92em;height:21.00em;"/></p>
<ol start="5">
<li class="mce-root">Step 5:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1103 image-border" src="assets/1d16c2c3-909d-4a32-b221-8111f72cb5c9.png" style="width:45.08em;height:20.83em;"/></p>
<ol start="6">
<li class="mce-root">Step 6:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1104 image-border" src="assets/bb8ce393-a48b-40c5-86cf-e7d5cac709a1.png" style="width:44.08em;height:20.25em;"/></p>
<ol start="7">
<li>Step 7:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1105 image-border" src="assets/6c75d988-2688-4c58-a16e-ba6a3b23da98.png" style="width:42.50em;height:19.75em;"/></p>
<ol start="8">
<li class="mce-root">Step 8:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1107 image-border" src="assets/b403905a-5501-4067-92ab-e41a221b564d.png" style="width:43.08em;height:19.92em;"/></p>
<ol start="9">
<li class="mce-root">Step 9:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1108 image-border" src="assets/71dd0a5d-47a9-4e7d-804e-f1e833273591.png" style="width:45.92em;height:21.33em;"/></p>
<p>Voila! We've now represented our original 5 x 5 image in a 3 x 3 matrix (our feature map). In this toy example, we've been able to reduce the dimensionality from 25 features down to just 9. Let's take a look at the image that results from this operation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1109 image-border" src="assets/9dac7706-ca00-4455-8cb1-6fa21d61e9f2.png" style="width:17.17em;height:17.17em;"/></p>
<p>If you're thinking that this looks exactly like our original black diagonal line but smaller, you're right. The values the kernel takes determine what's being identified, and in this specific example, we used what's called an <strong>identity kernel</strong>. Kernels taking other values will return other properties of the image—detecting the presence of lines, edges, outlines, areas of high contrast, and more.</p>
<p>We'll apply multiple kernels to the image, simultaneously, at each convolutional layer. The number of kernels used is up to the modeler—another hyperparameter. Ideally, you want to use as few as possible while still achieving acceptable cross-validation results. The simpler the better! However, depending on the complexity of the task, we may see performance gains by using more. The same thinking can can be applied when tuning the other hyperparameters of the model, such as the number of layers in the network or the number of neurons per layer. We're trading simplicity for complexity, and generalizability and speed for detail and precision.</p>
<p>While the number of kernels is our choice, the values that each kernel takes is a parameter of our model, which is learned from our training data and optimized during training in a manner that reduces the cost function.</p>
<p>We've seen the step-by-step process of how to convolve a filter with our image features to create a single feature map. But what happens when we apply multiple kernels simultaneously? And how do these feature maps pass through each layer of the network? Lets have a look at the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1110 image-border" src="assets/5550bd45-05ec-4fa9-9611-e5e383a9564c.png" style="width:19.25em;height:40.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Image source: Lee et al., Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations, via stack exchange. Source text here: https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The preceding screenshot visualizes the feature maps generated at each convolutional layer of a network trained on images of faces. In the early layers of the network (at the very bottom), we detect the presence of simple visual structures—simple lines and edges. We did this with our identity kernel! The output of this first layer gets passed on to the next layer (the middle row), which combines these simple shapes into abstract forms. We see here that the combination of edges build the components of a face—eyes, noses, ears, mouths, and eyebrows. The output of this middle layer, in turn, gets passed to a final layer, which combines the combination of edges into complete objects—in this case, different people's faces.</p>
<p>One particularly powerful property of this entire process is that all of these features and representations are learned from the data. At no point do we explicitly tell our model: <em>Model, for this task, I'd like to use an identity kernel and a bottom sobel kernel in the first convolutional layer because I think these two kernels will extract the most signal-rich feature maps</em>. Once we've set the hyperparameter for the number of kernels we want to use, the model learns through optimization what lines, edges, shadows, and complex combinations thereof are best suited to determine what a face is or isn't. The model performs this optimization with no domain-specific, hardcoded rules about what faces, cat burritos, or clothes are.</p>
<p>There are many other fascinating properties of convolutional neural networks, which we won't cover in this chapter. However, we did explore the fundamentals, and hopefully you have a sense of the importance of using convolutional neural networks to extract highly expressive, signal-rich, low-dimensional features.</p>
<p>Next, we'll discuss <em>Max pooling layers</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Max pooling layers</h1>
                </header>
            
            <article>
                
<p>We've discussed the importance of reducing our dimensional space and how we use convolutional layers to achieve this. We use max pooling layers for the same reason—to further reduce dimensionality. Quite intuitively, as the name suggests, with max pooling, we slide a window over our feature map and take the max value for the window. Let's return to the feature map from our diagonal-line example to illustrate, this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1111 image-border" src="assets/8c08fee6-0ca0-4ec3-85ef-5a8894ca291d.png" style="width:11.83em;height:11.25em;"/></p>
<p>Let's see what happens when we max pool the preceding feature map using a 2 x 2 window. Again, all we're doing here is returning <kbd>max(values in window)</kbd>:</p>
<ol>
<li>Return <kbd>max(0,255,255,0)</kbd><span>, which gets us 255:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1112 image-border" src="assets/6c75d908-819d-450b-ad34-eb16754aca77.png" style="width:24.33em;height:11.08em;"/></p>
<p class="mce-root"/>
<ol start="2">
<li class="mce-root">Step 2:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1113 image-border" src="assets/cfa2a066-bab6-4b25-bc96-c98f85bda1e5.png" style="width:24.67em;height:11.33em;"/></p>
<ol start="3">
<li class="mce-root">Step 3:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1114 image-border" src="assets/38d64d16-026a-4c48-9afd-413a09c42d45.png" style="width:23.42em;height:10.67em;"/></p>
<ol start="4">
<li class="mce-root">Step 4:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1115 image-border" src="assets/32f83e69-0ce7-41c5-971b-06e015cd1e3a.png" style="width:23.42em;height:10.75em;"/></p>
<p>By max pooling our feature map with a 2 x 2 window, we've knocked a column and a row off, getting us from a 3 x 3 representation to a 2 x 2—Not bad!</p>
<p class="mce-root"/>
<p>There are other forms of pooling as well—average pooling and min pooling, for example; however, you'll see max pooling used most often.</p>
<p>Next, we'll discuss flattening, a step we'll perform to turn our max-pooled feature map into the right shape for modeling.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flattening</h1>
                </header>
            
            <article>
                
<p>So far, we've focused on building as condensed and expressive a representation of our features as possible and used convolutional neural networks and max pooling layers to do this. The last step of our transformation is to flatten our convolved and max-pooled ndarray, in our example a 2 x 2 matrix, into a single row of training data.</p>
<p>Our max-pooled diagonal black line example would look something like the following, in code:</p>
<pre><strong>import numpy as np<br/>max_pooled = np.array([[255,255],[255,255]])</strong><br/><strong>max_pooled</strong></pre>
<p>Running this code returns the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1116 image-border" src="assets/43d89914-dd37-48cd-af87-1bc40ec8cce9.png" style="width:16.92em;height:3.83em;"/></p>
<p>We can check the shape here by running the following:</p>
<pre><strong>max_pooled.shape</strong></pre>
<p>This returns this output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1117 image-border" src="assets/9325c607-8ce4-4e88-bdcf-e6355d65483f.png" style="width:5.25em;height:2.17em;"/></p>
<p>To turn this matrix into a single training sample, we just run <kbd>flatten()</kbd>. Let's do this and look at the shape of our flattened matrix:</p>
<pre><strong>flattened = max_pooled.flatten()</strong><br/><strong>flattened.shape</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1118 image-border" src="assets/e63af8b4-430b-47df-a61b-c6fc09bce587.png" style="width:3.67em;height:2.33em;"/></p>
<p>What started as a 5 x 5 matrix of pixel intensities is now a single row with four features. We can now pass this into a fully-connected neural network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully-connected layers and output</h1>
                </header>
            
            <article>
                
<p>The fully-connected layers are where we map our input—the rows resulting from us convolving, max-pooling, and flattening our original extracted features—to our target class or classes. Here, each input is connected to every <strong>neuron</strong> or <strong>node</strong> in the following layer. The strength of these connections, or <strong>weights</strong>, and a <strong>bias</strong> term present in each node of the network are parameters of the model, optimized throughout the training process to minimize an objective function.</p>
<p>The final layer of our model will be our output layer, which gives us our model predictions. The number of neurons in our output layer and the <strong>activation function</strong> we apply to it are determined by the kind of problem we're trying to solve: regression, binary classification, or multi-class classification. We'll see exactly how to set up the fully-connected and output layers for a multi-class classification task when we start working with the Zalando Research fashion dataset in the next section.</p>
<div class="packt_infobox"><span>The fully-connected layers and output—that is, the feedforward neural network component of our architecture—belong to a distinct neural network type from the convolutional neural networks we discussed in this section. We briefly described how feedforward networks work in this section only to provide color on how the classifier component of our architecture works. You can always substitute this portion of the architecture for a classifier you are more familiar with, such as a <strong>logit</strong>!</span></div>
<p>With this fundamental knowledge, you're now ready to build your network!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a convolutional neural network to classify images in the Zalando Research dataset, using Keras</h1>
                </header>
            
            <article>
                
<p>In this section, we'll be building our convolutional neural network to classify images of clothing, using Zalando Research's fashion dataset. The repository for this dataset is available at <a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a>.</p>
<p>This dataset contains 70,000 grayscale images—each depicting an article of clothing—from 10 possible clothing articles. Specifically, the target classes are as follows: T-shirt/top, pants, sweater, dress, coat, sandal, shirt, sneaker, bag, and ankle boot.</p>
<p>Zalando, a Germany-based e-commerce company, released this dataset to provide researchers with an alternative to the classic MNIST dataset of handwritten digits. Additionally, this dataset, which they call <strong>Fashion MNIST</strong>, is a bit more challenging to predict excellently—the MNIST handwritten-digits dataset can be predicted with 99.7% accuracy without the need for extensive preprocessing or particularly deep neural networks.</p>
<p>So, let's get started! Follow these steps:</p>
<ol>
<li>Clone the repository to our desktop. From the terminal, run the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd ~/Desktop/</strong><br/><strong>git clone git@github.com:zalandoresearch/fashion-mnist.git</strong></pre>
<div class="packt_infobox">If you haven't done so already, please install Keras by running <kbd>pip install keras</kbd> from the command line. We'll also need to install TensorFlow. To do this, run <kbd>pip install tensorflow</kbd> from the command line. </div>
<div>
<ol start="2" style="font-size: 16px">
<li>Import the libraries we'll be using:</li>
</ol>
</div>
<pre style="padding-left: 60px"><strong>import sys</strong><br/><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><strong>from keras.models import Sequential</strong><br/><strong>from keras.layers import Dense, Dropout, Flatten</strong><br/><strong>from keras.layers import Conv2D, MaxPool2D</strong><br/><strong>from keras.utils import np_utils, plot_model</strong><br/><strong>from PIL import Image</strong><br/><strong>import matplotlib.pyplot as plt</strong></pre>
<p class="mce-root"/>
<p style="padding-left: 60px">Many of these libraries should look familiar by now. However, for some of you, this may be your first time using Keras. Keras is a popular Python deep learning library. It's a wrapper that can run on top of machine learning frameworks such as TensorFlow, CNTK, or Theano.</p>
<p style="padding-left: 60px">For our project, Keras will be running TensorFlow under the hood. Using TensorFlow directly would allow us more explicit control of the behavior of our networks; however, because TensorFlow uses dataflow graphs to represent its operations, this can take some getting used to. Luckily for us, Keras abstracts a lot of this away and its API is a breeze to learn for those comfortable with <kbd>sklearn</kbd>.</p>
<p style="padding-left: 60px">The only other library that may be new to some of you here will be the <strong><span>Python Imaging Library</span></strong> (<strong>PIL</strong>). PIL provides certain image-manipulation functionalities. We'll use it to visualize our Keras network's topology.</p>
<ol start="3">
<li>Load in the data. Zalando has provided us with a helper script that does the loading in for us. We just have to make sure that <kbd>fashion-mnist/utils/</kbd> is in our path:</li>
</ol>
<pre style="padding-left: 60px"><strong>sys.path.append('/Users/Mike/Desktop/fashion-mnist/utils/')</strong><br/><strong>import mnist_reader</strong></pre>
<ol start="4">
<li>Load in the data using the helper script:</li>
</ol>
<pre style="padding-left: 60px"><strong>X_train, y_train = mnist_reader.load_mnist('<span class="s1">/Users/Mike/Desktop/fashion-mnist/data/fashion</span>', kind='train')</strong><br/><strong>X_test, y_test = mnist_reader.load_mnist('<span class="s1">/Users/Mike/Desktop/fashion-mnist/data/fashion</span>', kind='t10k')</strong></pre>
<ol start="5">
<li>Take a look at the shapes of <kbd>X_train</kbd>, <kbd>X_test</kbd>, <kbd>y_train</kbd>, and <kbd>y_test</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>print(X_train.shape, y_train.shape)</strong><br/><strong>print(X_test.shape, y_test.shape)</strong></pre>
<p style="padding-left: 60px">Running that code gives us the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1119 image-border" src="assets/a8500b89-ac19-4d4c-b8b6-8506b8303741.png" style="width:15.42em;height:3.58em;"/></p>
<p class="mce-root"/>
<p>Here, we can see our training set contains 60,000 images and our test contains 10,000 images. Each image is currently a vector of values 784 that are elements long. Let's now check the data types:</p>
<pre><strong>print(type(X_train))</strong><br/><strong>print(type(y_train))</strong><br/><strong>print(type(X_test))</strong><br/><strong>print(type(y_test))</strong></pre>
<p>This returns the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1120 image-border" src="assets/ec3a5302-7c97-4748-8bae-72a155ad21db.png" style="width:18.50em;height:6.58em;"/></p>
<p>Next, let's see what the data looks like. Remember, in its current form, each image is a vector of values. We know the images are grayscale, so to visualize each image, we'll have to reshape these vectors into a 28 x 28 matrix. Let's do this and peek at the first image:</p>
<pre><strong>image_1 = X_train[0].reshape(28,28)</strong><br/><strong>plt.axis('off')</strong><br/><strong>plt.imshow(image_1, cmap='gray');</strong></pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1121 image-border" src="assets/02c86512-feb3-4b76-bf86-1b412698621e.png" style="width:20.83em;height:20.83em;"/></p>
<p>Awesome! We can check to see the class this image belongs to by running the following:</p>
<pre><strong>y_train[0]</strong></pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1122 image-border" src="assets/03c7a2b5-8520-4a03-9418-10238801bc15.png" style="width:2.42em;height:2.58em;"/></p>
<p>The classes are encoded from 0-9. In the README, Zalando provides us with the mapping:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1123 image-border" src="assets/e7cf2657-275b-4cfa-9f61-c0dc75214a1b.png" style="width:10.25em;height:20.92em;"/></p>
<p>Given this, we now know our first image is of an ankle boot. Sweet! Let's create an explicit mapping of these encoded values to their class names. This will come in handy momentarily:</p>
<pre><strong>mapping = {0: "T-shirt/top", 1:"Trouser", 2:"Pullover", 3:"Dress", <br/> 4:"Coat", 5:"Sandal", 6:"Shirt", 7:"Sneaker", 8:"Bag", 9:"Ankle Boot"}</strong></pre>
<p>Great. We've seen a single image, but we still need to get a feel for what's in our data. What do the images look like? Getting a grasp of this will tell us certain things. As an example, I'm interested to see how visually distinct the classes are. Classes that look similar to other classes will be harder for a classifier to differentiate than classes that are more unique.</p>
<p class="mce-root"/>
<p>Here, we define a helper function to help us through our visualization journey:</p>
<pre><strong>def show_fashion_mnist(plot_rows, plot_columns, feature_array, target_array, cmap='gray', random_seed=None):</strong><br/><strong>    '''Generates a plot_rows * plot_columns grid of randomly selected images from a feature         array. Sets</strong><strong> the title of each subplot equal to the associated index in the target array and     unencodes (i.e. title is</strong><strong> in plain English, not numeric). Takes as optional args a color map     and a random seed. Meant for EDA.</strong><strong>'''</strong><br/><strong>    <br/>    # Grabs plot_rows*plot_columns indices at random from X_train. </strong><br/><strong>    if random_seed is not None:</strong><br/><strong>        np.random.seed(random_seed)</strong><br/><strong>        </strong><br/><strong>    feature_array_indices = np.random.randint(0,feature_array.shape[0], size = plot_rows*plot_columns)</strong><br/><strong>    </strong><br/><strong>    # Creates our plots</strong><br/><strong>    fig, ax = plt.subplots(plot_rows, plot_columns, figsize=(18,18))</strong><br/><strong>    </strong><br/><strong>    reshaped_images_list = []</strong><br/><br/><strong>    for feature_array_index in feature_array_indices:</strong><br/><strong>        # Reshapes our images, appends tuple with reshaped image and class to a reshaped_images_list.</strong><br/><strong>        reshaped_image = feature_array[feature_array_index].reshape((28,28))</strong><br/><strong>        image_class = mapping[target_array[feature_array_index]]</strong><br/><strong>        reshaped_images_list.append((reshaped_image, image_class))</strong><br/><strong>    </strong><br/><strong>    # Plots each image in reshaped_images_list to its own subplot</strong><br/><strong>    counter = 0</strong><br/><strong>    for row in range(plot_rows):</strong><br/><strong>        for col in range(plot_columns):</strong><br/><strong>            ax[row,col].axis('off')</strong><br/><strong>            ax[row, col].imshow(reshaped_images_list[counter][0], <br/>                                cmap=cmap)</strong><br/><strong>            ax[row, col].set_title(reshaped_images_list[counter][1])</strong><br/><strong>            counter +=1</strong></pre>
<p>What does this function do? It creates a grid of images selected at random from the data so that we can view multiple images simultaneously.</p>
<p>It takes as arguments the desired number of image rows (<kbd>plot_rows</kbd>), image columns (<kbd>plot_columns</kbd>), our <kbd>X_train</kbd> (<kbd>feature_array</kbd>), and <kbd>y_train</kbd> (<kbd>target_array</kbd>) and generates a matrix of images that's <kbd>plot_rows</kbd> x <kbd>plot_columns</kbd> large. As optional arguments, you can specify a <kbd>cmap</kbd>, or colormap (the default is <kbd>‘gray'</kbd> because these are grayscale images), and a <kbd>random_seed</kbd>, if replicating the visualization is important.</p>
<p>Let's see how to run this, as follows:</p>
<pre><strong>show_fashion_mnist(4,4, X_train, y_train, random_seed=72)</strong></pre>
<p>This returns the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1124 image-border" src="assets/21924c3b-e0af-4a11-a139-27f612ad96c5.png" style="width:44.58em;height:44.17em;"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Visualization output</div>
<p class="mce-root"/>
<p>Remove the <kbd>random_seed</kbd> argument and rerun this function several times. Specifically, run the following code:</p>
<pre><strong>show_fashion_mnist(4,4, X_train, y_train)</strong></pre>
<p>You may have noticed that at this resolution some classes look quite similar and others quite distinct. For example, samples of the <span class="packt_screen">t-shirt/top</span> target class can look very similar to samples from the <span class="packt_screen">shirt</span> and <span class="packt_screen">coat</span> target classes, whereas the <span class="packt_screen">sandal</span> target class seems to be quite different than the rest. This is food for thought when thinking about where our model may be weak versus where it's likely to be strong.</p>
<p>Now let's take a peek at the distribution of target classes in our dataset. Will we have to do any upsampling or downsampling? Let's check:</p>
<pre><strong>y = pd.Series(np.concatenate((y_train, y_test)))</strong><br/><strong>plt.figure(figsize=(10,6))</strong><br/><strong>plt.bar(x=[mapping[x] for x in y.value_counts().index], height = y.value_counts());</strong><br/><strong>plt.xlabel("Class")</strong><br/><strong>plt.ylabel("Number of Images per Class")</strong><br/><strong>plt.title("Distribution of Target Classes");</strong></pre>
<p>Running the preceding code generates the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1125 image-border" src="assets/6bc6b269-edaa-4f0c-a68a-980ae4c01d42.png" style="width:42.33em;height:26.83em;"/></p>
<p>Awesome! No class-balancing to do here.</p>
<p>Next, let's start preprocessing our data to get it ready for modeling.</p>
<p>As we discussed in our <em>Image-feature extraction</em> section, these grayscale images contain pixel values ranging from 0 to 255. We confirm this by running the following code:</p>
<pre><strong>print(X_train.max())</strong><br/><strong>print(X_train.min())</strong><br/><strong>print(X_test.max())</strong><br/><strong>print(X_test.min())</strong></pre>
<p>This returns the following values:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1126 image-border" src="assets/0782a868-cbe5-4b34-a5ed-33ae7d0d6511.png" style="width:2.83em;height:5.50em;"/></p>
<p>For the purposes of modeling, we're going to want to normalize these values on a 0–1 scale. This is a common preprocessing step when preparing image data for modeling. Keeping our values in this range will allow our neural network to converge more quickly. We can normalize the data by running the following:</p>
<pre><strong># First we cast as float</strong><br/><strong>X_train = X_train.astype('float32')</strong><br/><strong>X_test = X_test.astype('float32')</strong><br/><strong># Then normalize</strong><br/><strong>X_train /= 255</strong><br/><strong>X_test /= 255</strong></pre>
<p>Our data is now scaled from 0.0 to 1.0. We can confirm this by running the following code:</p>
<pre><strong>print(X_train.max())</strong><br/><strong>print(X_train.min())</strong><br/><strong>print(X_test.max())</strong><br/><strong>print(X_test.min())</strong></pre>
<p>This returns the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1127 image-border" src="assets/5fca7448-9f2a-4680-be20-ec3b55f749de.png" style="width:2.50em;height:5.50em;"/></p>
<p class="mce-root"/>
<p>The next preprocessing step we'll need to perform before running our first Keras network will be to reshape our data. Remember, the shapes of our <kbd>X_train</kbd> and <kbd>X_test</kbd> are currently (60,000, 784) and (10,000,784), respectively. Our images are still vectors. For us to convolve these lovely kernels all over the image, we'll need need to reshape them into their 28 x 28 matrix form. Additionally, Keras requires that we explicitly declare the number of channels for our data. Accordingly, when we reshape these grayscale images for modeling, we'll declare <kbd>1</kbd>:</p>
<pre><strong>X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)</strong><br/><strong>X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)</strong></pre>
<p>Lastly, we'll one-hot encode our <kbd>y</kbd> vectors to conform with the target shape requirements of Keras:</p>
<pre><strong>y_train = np_utils.to_categorical(y_train, 10)</strong><br/><strong>y_test = np_utils.to_categorical(y_test, 10)</strong></pre>
<p>We're now ready for modeling. Our first network will have eight hidden layers. The first six hidden layers will consist of alternating convolutional and max pooling layers. We'll then flatten the output of this network and feed that into a two-layer feedforward neural network before generating our predictions. Here's what this looks like, in code:</p>
<pre><strong>model = Sequential()</strong><br/><strong>model.add(Conv2D(filters = 35, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))</strong><br/><strong>model.add(MaxPool2D(pool_size=(2,2)))</strong><br/><strong>model.add(Conv2D(filters = 35, kernel_size=(3,3), activation='relu'))</strong><br/><strong>model.add(MaxPool2D(pool_size=(2,2)))</strong><br/><strong>model.add(Conv2D(filters = 45, kernel_size=(3,3), activation='relu'))</strong><br/><strong>model.add(MaxPool2D(pool_size=(2,2)))</strong><br/><strong>model.add(Flatten())</strong><br/><strong>model.add(Dense(64, activation='relu'))</strong><br/><strong>model.add(Dense(32, activation='relu'))</strong><br/><strong>model.add(Dense(10, activation='softmax'))</strong><br/><strong>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</strong></pre>
<p>Let's describe what's happening on each line in some depth:</p>
<ul>
<li><strong>Line 1</strong>: Here, we just instantiate our model object. We'll further define the architecture—<span>that is,</span> the number of layers—sequentially with a series of <kbd>.add()</kbd> method calls that follow. This is the beauty of the Keras API.</li>
<li><strong>Line 2</strong>: Here, we add our first convolutional layer. We specify <kbd>35</kbd> kernels, each 3 x 3 in size. After this, we specify the image input shape, 28 x 28 x 1. We only have to specify the input shape in the first <kbd>.add()</kbd> call of our network. Lastly, we specify our activation function as <kbd>relu</kbd><span>. Activation functions transform the output of a layer before it's passed into the next layer. We'll apply activation functions to our <kbd>Conv2D</kbd> and <kbd>Dense</kbd> layers. These transformations have many important properties. Using</span> <kbd>relu</kbd> <span>here speeds up the convergence of our network</span><span>, <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank">http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf</a>http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf and</span> <kbd>relu</kbd><span>, relative to alternative activation functions, isn't expensive to compute—we're just transforming negative values to 0, and otherwise keeping all positive values. Mathematically, the</span> <kbd>relu</kbd> <span>function is given by</span> <kbd>max(0, value)</kbd><span>. For the purpose of this chapter, we'll stick to the </span><kbd>relu</kbd> <span>activation for every layer but the output layer.</span></li>
<li><strong>Line 3</strong>: Here, we add our first max pooling layer. We specify that the window size of this layer will be  2 x 2.</li>
<li><strong>Line 4</strong>: This is our second convolutional layer. We set it up just as we set up the first convolutional layer.</li>
<li><strong>Line 5</strong>: This is the second max pooling layer. We set this layer up just as we set up the first max pooling layer.</li>
<li><strong>Line 6</strong>: This is our third and final convolutional layer. This time, we add additional filters (<kbd>45</kbd> versus the <kbd>35</kbd> in previous layers). This is just a hyperparameter, and I encourage you to try multiple variations of this.</li>
<li><strong>Line 7</strong>: This is the third and final max pooling layer. It's configured the same as all max pooling layers that came before it.</li>
<li><strong>Line 8</strong>: Here's where we flatten the output of our convolutional neural network.</li>
<li><strong>Line 9</strong>: Here's the first layer of our fully-connected network. We specify <kbd>64</kbd> neurons in this layer and a <kbd>relu</kbd> activation function.</li>
<li><strong>Line 10</strong>: Here's the second layer of our fully-connected network. We specify <kbd>32</kbd> neurons for this layer and a <kbd>relu</kbd> activation function.</li>
<li><strong>Line 11</strong>: This is our output layer. We specify <kbd>10</kbd> neurons, equal to the number of target classes in our data. Since this is a multi-class classification problem, we specify a <kbd>softmax</kbd> activation function. The output will represent the predicted probability of the image belonging to classes 0–9. These probabilities will sum to <kbd>1</kbd>. The highest predicted probability of the <kbd>10</kbd> will represent the class our model believes to be the most likely class.</li>
<li><strong>Line 12</strong>: Here's where we compile our Keras model. In the compile step, we specify our optimizer, <kbd>Adam</kbd>, a <strong>gradient-descent</strong> algorithm that automatically adapts its learning rate. We specify our <strong>loss function</strong>—in this case, <kbd>categorical cross entropy</kbd> because we're performing a multi-class classification problem. Lastly, for the metrics argument, we specify <kbd>accuracy</kbd>. By specifying this, Keras will inform us of our train and validation accuracy for each epoch that our model runs.</li>
</ul>
<p>We can get a summary of our model by running the following:</p>
<pre><strong>model.summary()</strong></pre>
<p>This outputs the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1128 image-border" src="assets/485b7f6f-0585-468a-b359-a5b0cd770863.png" style="width:31.17em;height:24.75em;"/></p>
<p>Notice how the output shapes change as the data passes through the model. Specifically, look at the shape of our output after the flattening occurs—just 45 features. The raw data in <kbd>X_train</kbd> and <kbd>X_test</kbd> consisted of 784 features per row, so this is fantastic!</p>
<div class="packt_tip">You'll need to install <kbd>pydot</kbd> to render the visualization. To install it, run <kbd>pip install pydot</kbd> from the terminal. You may need to restart your kernel for the install to take effect. </div>
<p>Using the <kbd>plot_model</kbd> function in Keras, we can visualize the topology of our network differently. To do this, run the following code:</p>
<pre><strong>plot_model(model, to_file='Conv_model1.png', show_shapes=True)</strong><br/><strong>Image.open('Conv_model1.png')</strong></pre>
<p>Running the preceding code saves the topology to <kbd>Conv_model1.png</kbd> and generates the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1129 image-border" src="assets/b88953fb-ec6c-4b7a-bf4a-62caa3e1e736.png" style="width:20.58em;height:43.58em;"/></p>
<div class="packt_infobox"><span>This model will take several minutes to fit. If you have concerns about your system's hardware specs, you can easily reduce the training time by reducing the number of epochs to <kbd>10</kbd>.</span></div>
<p>Running the following code block will fit the model:</p>
<pre><strong>my_fit_model = model.fit(X_train, y_train, epochs=25, validation_data=<br/>                        (X_test, y_test))</strong></pre>
<p>In the fit step, we specify our <kbd>X_train</kbd> and <kbd>y_train</kbd>. We then specify the number of epochs we'd like to train the model. Then we plug in the validation data—<kbd>X_test</kbd> and <kbd>y_test</kbd>—to observe our model's out-of-sample performance. I like to save the <kbd>model.fit</kbd> step as a variable, <kbd>my_fit_model</kbd>, so we can later easily visualize the training and validation losses over epochs.</p>
<p>As the code runs, you'll see the model's train and validation loss, and accuracy after each epoch. Let's plot our model's train loss and validation loss using the following code:</p>
<pre><strong>plt.plot(my_fit_model.history['val_loss'], label="Validation")</strong><br/><strong>plt.plot(my_fit_model.history['loss'], label = "Train")</strong><br/><strong>plt.xlabel("Epoch", size=15)</strong><br/><strong>plt.ylabel("Cat. Crossentropy Loss", size=15)</strong><br/><strong>plt.title("Conv Net Train and Validation loss over epochs", size=18)</strong><br/><strong>plt.legend();</strong></pre>
<p>Running the preceding code generates the following plot. Your plot won't be identical—there are several stochastic processes taking place here—but it should look roughly the same:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1130 image-border" src="assets/d0a689ed-6b00-4d32-baf9-733f49b1804e.png" style="width:27.50em;height:18.25em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>A quick glance at this plot shows us that our model is overfitting. We see our train loss continue to fall in every epoch, but the validation loss doesn't move in lockstep. Let's glance at our accuracy scores to grasp how well this model did at the classification task. We can do this by running the following code:</p>
<pre><strong>plt.plot(my_fit_model.history['val_acc'], label="Validation")</strong><br/><strong>plt.plot(my_fit_model.history['acc'], label = "Train")</strong><br/><strong>plt.xlabel("Epoch", size=15)</strong><br/><strong>plt.ylabel("Accuracy", size=15)</strong><br/><strong>plt.title("Conv Net Train and Validation accuracy over epochs", <br/>           size=18)</strong><br/><strong>plt.legend();</strong></pre>
<p>This generates the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1131 image-border" src="assets/cfd56e4a-0ee6-4060-ad8b-04cc3521cc0b.png" style="width:33.92em;height:20.42em;"/></p>
<p>This plot, too, tells us we've overfit. But it appears as though our validation accuracy is in the high 80s, which is great! To get the max accuracy our model achieved and the epoch in which it occurred, we can run the following code:</p>
<pre><strong>print(max(my_fit_model.history['val_acc']))</strong><br/><strong>print(my_fit_model.history['val_acc'].index(max(my_fit_model.history['v<br/>      al_acc'])))</strong></pre>
<p>Your specific results will differ from mine, but here's my output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1132 image-border" src="assets/7e6eb9cd-38e2-4a6b-a1f6-fd4b19c2adac.png" style="width:4.50em;height:2.75em;"/></p>
<p>Using our convolutional neural network, we achieved a max classification accuracy of 89.48% in the 21st epoch. This is amazing! But we've still got to address that overfitting problem. Next, we'll rebuild our model using <strong>dropout regularization</strong>.</p>
<p>Dropout regularization is a form of regularization we can apply to the fully-connected layers of our neural network. Using dropout regularization, we randomly drop neurons and their connections from the network during training. By doing this, the network doesn't become too reliant on the weights or biases associated with any specific node, allowing it to generalize better out of sample.</p>
<p>Here, we add dropout regularization, specifying that we'd like to drop <kbd>35%</kbd> of the neurons at each <kbd>Dense</kbd> layer:</p>
<pre><strong>model = Sequential()</strong><br/><strong>model.add(Conv2D(filters = 35, kernel_size=(3,3), input_shape=<br/>         (28,28,1), activation='relu'))</strong><br/><strong>model.add(MaxPool2D(pool_size=(2,2)))</strong><br/><strong>model.add(Conv2D(filters = 35, kernel_size=(3,3), activation='relu'))</strong><br/><strong>model.add(MaxPool2D(pool_size=(2,2)))</strong><br/><strong>model.add(Conv2D(filters = 45, kernel_size=(3,3), activation='relu'))</strong><br/><strong>model.add(MaxPool2D(pool_size=(2,2)))</strong><br/><strong>model.add(Flatten())</strong><br/><strong>model.add(Dense(64, activation='relu'))</strong><br/><strong>model.add(Dropout(0.35))</strong><br/><strong>model.add(Dense(32, activation='relu'))</strong><br/><strong>model.add(Dropout(0.35))</strong><br/><strong>model.add(Dense(10, activation='softmax'))</strong><br/><strong>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</strong></pre>
<p>Running the preceding code will compile our new model. Let's have another look at the summary by rerunning the following:</p>
<pre><strong>model.summary()</strong></pre>
<p>Running the preceding code returns the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/819a4e32-36b5-4fe9-91b8-2ceb69e1f8aa.png" style="width:36.33em;height:31.83em;"/></p>
<p>Let's refit our model by rerunning the following:</p>
<pre><strong>my_fit_model = model.fit(X_train, y_train, epochs=25, validation_data=<br/>                        (X_test, y_test))</strong></pre>
<p>Once your model has refit, rerun the plot code to visualize loss. Here's mine:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1189 image-border" src="assets/ad4333d4-48bf-43f0-962c-725832e0a4d3.png" style="width:33.17em;height:21.58em;"/></p>
<p>This looks better! The difference between our training and validation losses has shrunk, which was the intended purpose, though there does appear to be some room for improvement.</p>
<p>Next, re-plot your accuracy curves. Here are mine for this run:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1190 image-border" src="assets/97fd4ea0-c7dc-412b-bd43-8da26dc2cf95.png" style="width:32.92em;height:19.33em;"/></p>
<p>This also looks better from an overfitting perspective. Fantastic! What was the best classification accuracy we achieved after applying regularization? Let's run the following code:</p>
<pre><strong>print(max(my_fit_model.history['val_acc']))</strong><br/><strong>print(my_fit_model.history['val_acc'].index(max(my_fit_model.history['v<br/>      al_acc'])))</strong></pre>
<p>My output from this run of the model was as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1134 image-border" src="assets/f8b9853f-45f0-440c-89fd-fa95f6b5b572.png" style="width:4.92em;height:3.08em;"/></p>
<p>Interesting! The best validation accuracy we achieved was lower than that in our unregularized model, but not by much. And it's still quite good! Our model is telling us that we predict the correct type of clothing article 88.85% of the time.</p>
<p>One way to think about how well we've done here is to compare our model's accuracy with the <strong>baseline accuracy</strong> for our dataset. The baseline accuracy is simply the score we would get by naïvely selecting the most-commonly occurring class in the dataset. For this specific dataset, because the classes are perfectly balanced and there are 10 classes, the baseline accuracy is 10%. Our model handily beats this baseline accuracy. It's clearly learned something about the data!</p>
<p>There are so many different places you can go from here! Try building deeper models or grid-searching over the many hyperparameters we used in our models. Assess your classifier's performance as you would with any other model—try building a confusion matrix to understand what classes we predicted well and what classes we weren't as strong in!</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We certainly covered a lot of ground here! We talked about how to extract features from images, how convolutional neural networks work, and then we built a convolutional neural network to a fully-connected network architecture. Along the way, we picked up lots of new jargon and concepts, too!</p>
<p>Hopefully, after reading this chapter, you feel that these image-classification techniques—knowledge of which you may have once considered the province of sorcerers—is actually just a series of mathematical optimizations carried out for intuitive reasons! And hopefully this content can help move you forward in tackling an image-processing project that interests you!</p>


            </article>

            
        </section>
    </body></html>
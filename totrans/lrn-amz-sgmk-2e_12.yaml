- en: 'Chapter 9: Scaling Your Training Jobs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章：扩展你的训练任务
- en: In the four previous chapters, you learned how to train models with built-in
    algorithms, frameworks, or your own code.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前四章中，你学习了如何使用内置算法、框架或你自己的代码来训练模型。
- en: 'In this chapter, you''ll learn how to scale training jobs, allowing them to
    train on larger datasets while keeping training time and cost under control. We''ll
    start by discussing when and how to take scaling decisions, thanks to monitoring
    information and simple guidelines. You''ll also see how to collect profiling information
    with **Amazon** **SageMaker Debugger**, in order to understand how efficient your
    training jobs are. Then, we''ll look at several key techniques for scaling: **pipe
    mode**, **distributed training**, **data parallelism**, and **model parallelism**.
    After that, we''ll launch a large training job on the large **ImageNet** dataset
    and see how to scale it. Finally, we''ll discuss storage alternatives to **S3**
    for large-scale training, namely **Amazon** **EFS** and **Amazon** **FSx for Lustre**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何扩展训练任务，使其能够在更大的数据集上训练，同时控制训练时间和成本。我们将首先讨论如何根据监控信息和简单的指南来做出扩展决策。你还将看到如何使用**Amazon**
    **SageMaker Debugger**收集分析信息，以了解训练任务的效率。接着，我们将探讨几种扩展的关键技术：**管道模式**、**分布式训练**、**数据并行性**和**模型并行性**。之后，我们将在大型**ImageNet**数据集上启动一个大型训练任务，并观察如何进行扩展。最后，我们将讨论用于大规模训练的存储替代方案，即**Amazon**
    **EFS**和**Amazon** **FSx for Lustre**。
- en: 'We''ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下主题：
- en: Understanding when and how to scale
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解何时以及如何进行扩展
- en: Monitoring and profiling training jobs with Amazon SageMaker Debugger
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon SageMaker Debugger监控和分析训练任务
- en: Streaming datasets with pipe mode
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道模式流式传输数据集
- en: Distributing training jobs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分发训练任务
- en: Scaling an image classification model on ImageNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet上扩展图像分类模型
- en: Training with data and model parallelism
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据并行性和模型并行性进行训练
- en: Using other storage services
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其他存储服务
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create it. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个AWS账户才能运行本章中包含的示例。如果你还没有账户，请访问[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)创建一个。你还应熟悉AWS免费套餐([https://aws.amazon.com/free/](https://aws.amazon.com/free/))，它允许你在一定的使用限制内免费使用许多AWS服务。
- en: You will need to install and configure the AWS **Command Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为你的账户安装并配置AWS **命令行界面** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))。
- en: You will need a working `pandas`, `numpy`, and more).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装并配置`pandas`、`numpy`等工具。
- en: Code examples included in this book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例可以在GitHub上找到，链接为[https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装Git客户端来访问这些示例([https://git-scm.com/](https://git-scm.com/))。
- en: Understanding when and how to scale
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解何时以及如何进行扩展
- en: Before we dive into scaling techniques, let's first discuss the monitoring information
    that we should consider when deciding whether we need to scale, and how we should
    do it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解扩展技术之前，让我们首先讨论在决定是否需要扩展以及如何扩展时应考虑的监控信息。
- en: Understanding what scaling means
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解扩展意味着什么
- en: The training log tells us how long the job lasted. In itself, this isn't really
    useful. How long is *too long*? This feels very subjective, doesn't it? Furthermore,
    even when training on the same dataset and infrastructure, changing a single hyperparameter
    can significantly impact training time. Batch size is one example of this, and
    there are many more.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练日志告诉我们任务持续了多长时间。但单凭这一点，其实并不太有用。多长时间是*太长*了呢？这个问题似乎非常主观，不是吗？此外，即使在相同的数据集和基础设施上，改变一个超参数也可能显著影响训练时间。批次大小就是一个例子，且这种情况还有很多。
- en: 'When we''re concerned about training time, I think we''re really trying to
    answer three questions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们关注训练时间时，我认为我们实际上是在尝试回答三个问题：
- en: Is the training time compatible with our business requirements?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练时间是否与我们的业务需求兼容？
- en: Are we making good use of the infrastructure we're paying for? Did we underprovision
    or overprovision?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否在充分利用我们所支付的基础设施？我们是否进行了资源不足或过度配置？
- en: Could we train faster without spending more money?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能在不花更多钱的情况下训练得更快吗？
- en: Adapting training time to business requirements
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使训练时间适应业务需求
- en: Ask yourself this question—what would be the direct impact on your business
    if your training job ran twice as fast? In many cases, the honest answer should
    be *none*. There is no clear business metric that would be improved.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 问问自己这个问题——如果您的训练任务运行速度加倍，直接对您的业务产生什么影响？在很多情况下，诚实的答案应该是*没有*。没有任何明确的业务指标会因此得到改善。
- en: Sure, some companies run training jobs that last days, even weeks—think autonomous
    driving or life sciences. For them, any significant reduction in training time
    means that they get results much faster, analyze them, and launch the next iteration.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一些公司运行的训练任务可能需要几天甚至几周——比如自动驾驶或生命科学领域。对他们来说，训练时间的任何显著减少意味着他们能够更快获得结果，进行分析，并启动下一个迭代。
- en: Some other companies want the freshest models possible, and they retrain every
    hour. Of course, training time needs to be kept under control to make the deadline.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些公司希望拥有最新的模型，并且每小时都会重新训练。当然，训练时间需要控制在一定范围内，以确保能按时完成。
- en: In both types of companies, scaling is vital. For everyone else, things are
    not so clear. If your company trains a production model every week or every month,
    does it really matter whether training reaches the same level of accuracy 30 minutes
    sooner? Probably not.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种类型的公司中，扩展能力至关重要。对于其他公司而言，情况就不那么明确。如果您的公司每周或每月训练一次生产模型，训练时间提前 30 分钟能否带来明显的效果？可能不会。
- en: Some people would certainly object that they need to train a lot of models all
    of the time. I'm afraid this is a fallacy. As SageMaker lets you create on-demand
    infrastructure whenever you need it, training activities will not be capacity-bound.
    This is the case when you work with physical infrastructure, but not with cloud
    infrastructure. Even if you need to train 1,000 **XGBoost** jobs every day, does
    it really matter whether each individual job takes 5 minutes instead of 6? Probably
    not.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人肯定会反对，说他们需要一直训练很多模型。我恐怕这是一种误解。由于 SageMaker 允许您根据需要随时创建按需基础设施，因此训练活动不会受到容量的限制。这种情况适用于物理基础设施，但不适用于云基础设施。即使您每天需要训练
    1,000 个 **XGBoost** 任务，是否真的在乎每个任务需要 5 分钟而不是 6 分钟？可能不。
- en: Some would retort that "the faster you train, the less it costs." Again, this
    is a fallacy. The cost of a SageMaker training job is the training time in seconds
    multiplied by the cost of the instance type and by the number of instances. If
    you pick a larger instance type, training time will most probably decrease. Will
    it decrease enough to offset the increased instance cost? Maybe, maybe not. Some
    training workloads will make good use of the extra infrastructure, and some won't.
    The only way to know is to run tests and make data-driven decisions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会反驳，“训练越快，成本越低。”再次提醒，这是一种误解。SageMaker 训练任务的成本是训练时间（以秒为单位）乘以实例类型的成本，再乘以实例数量。如果您选择更大的实例类型，训练时间很可能会缩短。它是否会缩短足够多，以抵消增加的实例成本？也许会，也许不会。某些训练工作负载能充分利用额外的基础设施，而某些则不能。唯一的办法就是进行测试并做出数据驱动的决策。
- en: Right-sizing training infrastructure
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确配置训练基础设施
- en: SageMaker supports a long list of instance types, which looks like a very nice
    candy store ([https://aws.amazon.com/sagemaker/pricing/instance-types](https://aws.amazon.com/sagemaker/pricing/instance-types)).
    All you have to do is call an API to fire up an 8 GPU EC2 instance – more powerful
    than any server your company would have allowed you to buy. Caveat emptor – don't
    forget the "pricing" part of the URL!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 支持一长串实例类型，看起来像一个非常诱人的糖果店 ([https://aws.amazon.com/sagemaker/pricing/instance-types](https://aws.amazon.com/sagemaker/pricing/instance-types))。您所需要做的，就是调用一个
    API 启动一个 8 GPU 的 EC2 实例——它比您公司允许您购买的任何服务器都要强大。提醒注意——不要忘记 URL 中的“定价”部分！
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the words "EC2 instance" don't mean much to you, I would definitely recommend
    reading a bit about **Amazon** **EC2** at [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“EC2 实例”这几个词对您来说没有太大意义，我强烈建议您阅读一下关于 **Amazon** **EC2** 的资料，[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)。
- en: Granted, cloud infrastructure doesn't require you to pay a lot of money upfront
    to buy and host servers. Still, the AWS bill will come at the end of the month.
    Hence, even using cost optimization techniques such as **Managed Spot Training**
    (which we'll discuss in the next chapter), it's critical that you right-size your
    training infrastructure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，云基础设施不要求你提前支付大量资金购买和托管服务器。然而，AWS 的账单还是会在月底到来。因此，即使使用了诸如 **Managed Spot Training**（我们将在下一章讨论）等成本优化技术，正确配置你的训练基础设施仍然至关重要。
- en: 'My advice is always the same:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议始终是一样的：
- en: Identify business requirements that depend on training time.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定依赖于训练时间的业务需求。
- en: Start with the smallest reasonable amount of infrastructure.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从最小合理的基础设施开始。
- en: Measure technical metrics and cost.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量技术指标和成本。
- en: 'If business requirements are met, did you overprovision? There are two possible
    answers:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果业务需求已满足，你是否过度配置了？有两种可能的答案：
- en: 'a) **Yes**: Scale down and repeat.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) **是**：缩小规模并重复。
- en: 'b) **No**: You''re done.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) **否**：你完成了。
- en: If business requirements are not met, identify bottlenecks.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果业务需求未能满足，识别瓶颈。
- en: Run some tests on scaling up (larger instance type) and scaling out (more instances).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行一些扩展测试（更大的实例类型）和横向扩展（更多实例）。
- en: Measure technical metrics and costs.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量技术指标和成本。
- en: Implement the best solution for your business context.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施最适合你业务环境的解决方案。
- en: Repeat.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复。
- en: Of course, this process is as good as the people who take part in it. Be critical!
    "Too slow" is not a data point—it's an opinion.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个过程的效果取决于参与者的水平。要保持批判性思维！“太慢”不是一个数据点——它只是一个意见。
- en: Deciding when to scale
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定何时扩展
- en: 'When it comes to monitoring information, you can rely on three sources: the
    training log, **Amazon** **CloudWatch** metrics, and the profiling capability
    in **Amazon** **SageMaker Debugger**.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控信息方面，你可以依赖三个来源：训练日志、**Amazon** **CloudWatch** 指标，以及 **Amazon** **SageMaker
    Debugger** 中的性能分析功能。
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If "CloudWatch" doesn't mean much to you, I would definitely recommend reading
    a bit about it at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“CloudWatch”对你来说意义不大，我强烈推荐你阅读一下 [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/)。
- en: The training log shows you the total training time and the number of samples
    per second. As discussed in the previous section, total training time is not a
    very useful metric. Unless you have very strict deadlines, it's best to ignore
    it. The number of samples per second is more interesting. You can use it to compare
    your training job to benchmarks available in research papers or blog posts. If
    someone has managed to train the same model twice as fast on the same GPU, you
    should be able to do the same. When you get close to that number, you'll also
    know that there's not a lot of room for improvement and that other scaling techniques
    should be considered.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练日志展示了总训练时间和每秒样本数。正如前一部分讨论的，总训练时间不是一个很有用的指标。除非你有非常严格的截止日期，否则最好忽略它。每秒样本数更为有趣。你可以用它来将你的训练任务与研究论文或博客文章中提供的基准进行比较。如果某人在相同的
    GPU 上将同一个模型训练速度提高了一倍，你也应该能做到。当你接近那个数字时，你也能知道改进的空间不大，需要考虑其他扩展技术。
- en: CloudWatch gives you coarse-grained infrastructure metrics with a 1-minute resolution.
    For simple training jobs, these metrics are all you need to check if your training
    makes efficient use of the underlying infrastructure and identify potential bottlenecks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch 提供了 1 分钟分辨率的粗粒度基础设施指标。对于简单的训练任务，这些指标足以检查你的训练是否高效利用了底层基础设施，并帮助你识别潜在的瓶颈。
- en: For more complex jobs (distributed training, custom code, and so on), SageMaker
    Debugger gives you fine-grained, near real-time infrastructure and Python metrics,
    with a resolution as low as 100 milliseconds. This information will let you drill
    down and identify complex performance and scaling problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的任务（分布式训练、自定义代码等），SageMaker Debugger 提供了细粒度、近实时的基础设施和 Python 指标，分辨率低至 100
    毫秒。这些信息能帮助你深入分析并识别复杂的性能和扩展问题。
- en: Deciding how to scale
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定如何扩展
- en: As mentioned earlier, you can either scale up (move to a bigger instance) or
    scale out (use several instances for distributed training). Let's look at the
    pros and cons.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以选择扩展（迁移到更大的实例）或横向扩展（使用多个实例进行分布式训练）。我们来看看各自的优缺点。
- en: Scaling up
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展
- en: Scaling up is simple. You just need to change the instance type. Monitoring
    stays the same, and there's only one training log to read. Last but not least,
    training on a single instance is predictable and very often delivers the best
    accuracy, as there's only one set of model parameters to learn and update.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 向上扩展很简单。你只需要更改实例类型。监控保持不变，而且只需要读取一个训练日志。最后但同样重要的是，单实例训练具有可预测性，并且通常能提供最佳的准确性，因为只有一组模型参数需要学习和更新。
- en: On the downside, your algorithm may not be compute-intensive and parallel enough
    to benefit from the extra computing power. Extra vCPUs and GPUs are only useful
    if they're put to work. Your network and storage layers must also be fast enough
    to keep them busy at all times, which may require using alternatives to S3, generating
    some extra engineering work. Even if you don't hit any of these problems, there
    comes a point where there simply isn't a bigger instance you can use!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你的算法计算密集度不高或并行性不足，那么额外的计算能力可能不会带来好处。额外的 vCPU 和 GPU 只有在被充分利用时才有用。你的网络和存储层也必须足够快速，以确保它们始终保持繁忙，这可能需要使用
    S3 以外的替代方案，从而增加一些额外的工程工作。即使你没有遇到这些问题，也有一个时刻，你会发现再没有更大的实例可以使用了！
- en: Scaling up with multi-GPU instances
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过多 GPU 实例进行扩展
- en: As tempting as multi-GPU instances are, they create specific challenges. An
    `ml.g4dn.16xlarge` and `ml.p3dn.24xlarge` support 100-Gbit networking and ultra-fast
    SSD NVMe local storage. Still, that level of performance comes at a price, and
    you need to make sure it's really worth it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多 GPU 实例非常诱人，但它们也带来了特定的挑战。`ml.g4dn.16xlarge` 和 `ml.p3dn.24xlarge` 支持 100 Gbit
    的网络和超快速的 SSD NVMe 本地存储。但这种性能水平是有代价的，你需要确保它真的是值得的。
- en: You should keep in mind that bigger isn't always better. Inter-GPU communication,
    no matter how fast, introduces some overhead that could kill the performance of
    smaller training jobs. Here too, you should experiment and find the sweetest spot.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该记住，更大并不总是更好。无论 GPU 之间的通信多么快速，它都会引入一些开销，这可能会影响较小训练任务的性能。这里，你也需要进行实验，找到最合适的平衡点。
- en: In my experience, getting great performance with multi-GPU instances takes some
    work. Unless the model is too large to fit on a single GPU or the algorithm doesn't
    support distributed training, I'd recommend trying first to scale out on single-GPU
    instances.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，想要在多 GPU 实例上获得出色的性能需要一些努力。除非模型太大无法放入单个 GPU，或者算法不支持分布式训练，否则我建议首先尝试在单 GPU
    实例上进行扩展。
- en: Scaling out
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展
- en: Scaling out lets you distribute large datasets to a cluster of training instances.
    Even if your training job doesn't scale linearly, you'll get a noticeable speedup
    compared to single-instance training. You can use plenty of smaller instances
    that only process a subset of your dataset, which helps to keep costs under control.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展使你能够将大数据集分发到训练实例的集群中。即使你的训练任务无法线性扩展，与你的单实例训练相比，你仍然会看到明显的加速效果。你可以使用大量较小的实例，这些实例只处理数据集的一个子集，有助于控制成本。
- en: On the downside, datasets need to be prepared in a format that can be efficiently
    distributed across training clusters. As distributed training is pretty chatty,
    network I/O can also become a bottleneck. Still, the main problem is usually accuracy,
    which is often lower than for single-instance training, as each instance works
    with its own set of model parameters. This can be alleviated by asking training
    instances to synchronize their work periodically, but this is a costly operation
    that impacts training time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据集需要以能够高效分发到训练集群的格式进行准备。由于分布式训练需要频繁的通信，网络 I/O 也可能成为瓶颈。不过，通常最大的问题是准确性，通常低于单实例训练的准确性，因为每个实例都使用自己的一组模型参数。这可以通过定期同步训练实例的工作来缓解，但这是一项成本高昂的操作，会影响训练时间。
- en: If you think that scaling is harder than it seems, you're right. Let's try to
    put all of these notions into practice with a first simple example.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得扩展比看起来的要困难，你是对的。让我们尝试用第一个简单的例子将这些概念付诸实践。
- en: Scaling a BlazingText training job
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展 BlazingText 训练任务
- en: 'In [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108), *Training
    Natural Language Processing Models*, we used **BlazingText** and the Amazon reviews
    dataset to train a sentiment analysis model. At the time, we only trained it on
    100,000 reviews. This time, we''ll train it on the full dataset: 1.8 million reviews
    (151 million words).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第六章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)，*训练自然语言处理模型* 中，我们使用
    **BlazingText** 和 Amazon 评论数据集训练了情感分析模型。当时，我们只训练了 100,000 条评论。这一次，我们将使用完整的数据集进行训练：180
    万条评论（1.51 亿个单词）。
- en: Reusing our SageMaker Processing notebook, we process the full dataset on an
    `ml.c5.9xlarge` instance, store results in S3, and feed them to our training job.
    The size of the training set has grown to a respectable 720 MB.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重新利用我们的 SageMaker Processing 笔记本，我们在 `ml.c5.9xlarge` 实例上处理完整的数据集，将结果存储在 S3 中，并将其馈送到训练作业中。训练集的大小已增长到
    720 MB。
- en: 'To give BlazingText extra work, we apply the following hyperparameters to increase
    the complexity of the word vectors the job will learn:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给 BlazingText 增加额外工作量，我们应用以下超参数来提高任务将学习的词向量的复杂度：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We train on a single `ml.c5.2xlarge` instance. It has 8 vCPU and 16 GB of RAM
    and uses `gp2` class, which is SSD-based).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在单个 `ml.c5.2xlarge` 实例上进行训练。它具有 8 个 vCPU 和 16 GB 的 RAM，并使用 `gp2` 类型（基于 SSD）。
- en: 'The job runs for 2,109 seconds (a little more than 35 minutes), peaking at
    4.84 million words per second. Let''s take a look at the CloudWatch metrics:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务运行了 2,109 秒（略超过 35 分钟），最大速度为每秒 484 万个词。我们来看一下 CloudWatch 指标：
- en: Starting from the **Experiments and trials** panel in **SageMaker Studio**,
    we locate the training job and right-click on **Open in trial details**.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 **SageMaker Studio** 中的 **实验与试验** 面板开始，我们定位到训练作业，并右键点击 **在试验详情中打开**。
- en: Then, we select the **AWS settings** tab. Scrolling down, we see a link named
    **View instance metrics**. Clicking on it takes us directly to the CloudWatch
    metrics for our training job.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们选择 **AWS 设置** 标签。向下滚动，我们看到一个名为 **查看实例指标** 的链接。点击它会直接带我们到训练作业的 CloudWatch
    指标页面。
- en: 'Let''s select `CPUUtilization` and `MemoryUtilization` in **All metrics** and
    visualize them as shown in the next screenshot:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在 **所有指标** 中选择 `CPUUtilization` 和 `MemoryUtilization`，并按下图所示进行可视化：
- en: '![Figure 9.1 – Viewing CloudWatch metrics'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – 查看 CloudWatch 指标'
- en: '](img/B17705_09_1.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_1.jpg)'
- en: Figure 9.1 – Viewing CloudWatch metrics
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 查看 CloudWatch 指标
- en: On the right-hand Y-axis, memory utilization is stable at 20%, so we definitely
    don't need more RAM.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧的 Y 轴上，内存利用率稳定在 20%，所以我们肯定不需要更多的 RAM。
- en: Still on the right-hand Y-axis, disk utilization is about 3% during the training,
    going up to 12% when the model is saved. We allocated way too much storage to
    this instance. By default, SageMaker instances get 30 GB of Amazon EBS storage,
    so how much money did we waste here? The EBS cost for SageMaker in `eu-west-1`
    is $0.154 per GB-month, so 30 GB for 2,117 seconds costs 0.154*30*(2109/(24*30*3600))
    = $0.00376\. That's a silly low amount, but if you train thousands of jobs per
    month, it will add up. Even if this saves us $10 a year, we should save that!
    This can easily be done by setting the `volume_size` parameter in all estimators.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在右侧的 Y 轴上，磁盘利用率在训练过程中约为 3%，当模型被保存时，磁盘利用率会上升到 12%。我们为这个实例分配了太多的存储。默认情况下，SageMaker
    实例会获得 30 GB 的 Amazon EBS 存储，那么我们浪费了多少钱呢？在 `eu-west-1` 区域，SageMaker 的 EBS 成本为每
    GB 每月 0.154 美元，因此 30 GB 在 2,117 秒内的费用为 0.154*30*(2109/(24*30*3600)) = 0.00376
    美元。虽然这个金额非常低，但如果每月训练数千个任务，这些费用会累积起来。即使每年节省 10 美元，我们也应该节省！这可以通过在所有估算器中设置 `volume_size`
    参数来轻松实现。
- en: On the left-hand Y-axis, we see that the CPU utilization plateaus around 790%,
    very close to the maximum value of 800% (8 vCPUs at 100% usage). This job is obviously
    compute-bound.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧的 Y 轴上，我们看到 CPU 利用率稳定在 790% 左右，非常接近最大值 800%（8 个 vCPU 在 100% 使用率下）。这个任务显然是计算密集型的。
- en: 'So, what are our options? If BlazingText supported distributed training in
    supervised mode (it doesn''t), we could have considered scaling out with smaller
    `ml.c5.xlarge` instances (4 vCPUs and 8 GB of RAM). That''s more than enough RAM,
    and adding capacity in small chunks is good practice. This is what right-sizing
    is all about: not too much, not too little—it should be just right.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有哪些选择呢？如果 BlazingText 支持监督模式下的分布式训练（但实际上不支持），我们可以考虑使用更小的 `ml.c5.xlarge`
    实例（4 个 vCPU 和 8 GB 的 RAM）进行扩展。那样的 RAM 已经足够了，按小步增长容量是一种良好的实践。这就是右尺寸的核心：不多不少，正好合适。
- en: Anyway, our only choice here is to scale up. Looking at the list of available
    instances, we could try `ml.c5.4xlarge`. As BlazingText supports single-GPU acceleration,
    `ml.p3.2xlarge` (1 NVIDIA V100 GPU) is also an option.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们此时唯一的选择是扩展实例。查看可用实例列表，我们可以尝试 `ml.c5.4xlarge`。由于 BlazingText 支持单个 GPU
    加速，`ml.p3.2xlarge`（1 个 NVIDIA V100 GPU）也是一个选项。
- en: Note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, the cost-effective `ml.g4dn.xlarge` is unfortunately
    not supported by BlazingText.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 写作时，成本效益较高的 `ml.g4dn.xlarge` 不幸未被 BlazingText 支持。
- en: Let's try both and compare training times and costs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试两者并比较训练时间和成本。
- en: '![](img/011.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/011.jpg)'
- en: The `ml.c5.4xlarge` instance provides a nice speedup for a moderate price increase.
    Interestingly, the job is still compute-bound, so I decided to try the even larger
    `ml.c5.9xlarge` instance (36 vCPUs) for good measure, but the speedup was large
    enough to offset the increased cost.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`ml.c5.4xlarge` 实例在适度价格增加的情况下提供了不错的加速效果。有趣的是，工作负载仍然是计算密集型的，因此我决定尝试更大的 `ml.c5.9xlarge`
    实例（36 个 vCPU），以进一步测试，但加速效果足以抵消成本增加。'
- en: The GPU instance is almost 3x faster, as BlazingText has been optimized to utilize
    thousands of cores. It's also about 3x more expensive, which could be acceptable
    if minimizing training time was very important.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 实例速度几乎是原来的 3 倍，因为 BlazingText 已经过优化，能够利用数千个核心。它的费用大约是原来的 3 倍，如果最小化训练时间非常重要，这可能是可以接受的。
- en: This simple example shows you that right-sizing your training infrastructure
    is not black magic. By following simple rules, looking at a few metrics, and using
    common sense, you can find the right instance size for your project.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的示例告诉我们，调整训练基础设施并非神秘难懂。通过遵循简单的规则，查看一些指标并运用常识，你可以为你的项目找到合适的实例大小。
- en: Now, let's introduce the monitoring and profiling capability in Amazon SageMaker
    Debugger, which will give us even more information on the performance of our training
    jobs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们介绍 Amazon SageMaker Debugger 中的监控和分析功能，它将为我们提供更多关于训练任务性能的信息。
- en: Monitoring and profiling training jobs with Amazon SageMaker Debugger
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker Debugger 监控和分析训练任务
- en: SageMaker Debugger includes a monitoring and profiling capability that lets
    us collect infrastructure and code performance information at much lower time
    resolution than CloudWatch (as often as every 100 milliseconds). It also allows
    us to configure and trigger built-in or custom rules that watch for unwanted conditions
    in our training jobs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger 包含监控和分析功能，使我们能够以比 CloudWatch 更低的时间分辨率收集基础设施和代码性能信息（通常每 100
    毫秒一次）。它还允许我们配置和触发内置或自定义规则，监控训练任务中的不良条件。
- en: 'Profiling is very easy to use, and in fact, it''s on by default! You may have
    noticed a line such as this one in your training log:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 分析功能非常容易使用，事实上，它默认是开启的！你可能在训练日志中注意到类似下面这样的行：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This tells us that SageMaker is automatically running a profiling job, in parallel
    with our training job. The role of the profiling job is to collect data points
    that we can then display in SageMaker Studio, in order to visualize metrics and
    understand potential performance issues.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，SageMaker 正在自动运行一个分析任务，并与我们的训练任务并行进行。分析任务的作用是收集数据点，我们可以在 SageMaker Studio
    中显示这些数据点，以便可视化指标并了解潜在的性能问题。
- en: Viewing monitoring and profiling information in SageMaker Studio
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SageMaker Studio 中查看监控和分析信息
- en: 'Let''s go back to the `ml.p3.2xlarge` instance. We right-click on it and select
    **Open Debugger for insights** this time. This opens a new tab, visible in the
    next screenshot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到 `ml.p3.2xlarge` 实例。我们右键点击它，然后选择 **打开调试器以获取洞察**。这会打开一个新标签，见下图所示：
- en: '![Figure 9.2 – Viewing monitoring and profiling information'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – 查看监控和分析信息'
- en: '](img/B17705_09_2.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_2.jpg)'
- en: Figure 9.2 – Viewing monitoring and profiling information
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 查看监控和分析信息
- en: 'At the top, we can see that monitoring is indeed on by default and that profiling
    isn''t. Expanding the **Resource utilization summary** item in the **Overview**
    tab, we see a summary of infrastructure metrics, as shown in the next screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，我们可以看到监控默认是开启的，而分析功能没有开启。在 **概览** 标签页中展开 **资源使用情况总结** 项目，我们可以看到基础设施指标的总结，如下一个截图所示：
- en: '![Figure 9.3 – Viewing utilization summary'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3 – 查看使用情况总结'
- en: '](img/B17705_09_3.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_3.jpg)'
- en: Figure 9.3 – Viewing utilization summary
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 查看使用情况总结
- en: Note
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: P50, p95, and p99 are percentiles. If you're not familiar with this concept,
    you can find more information at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Percentiles](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Percentiles).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: P50、p95 和 p99 是百分位数。如果你不熟悉这个概念，可以访问 [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Percentiles](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Percentiles)
    了解更多信息。
- en: 'Moving on to the `algo-1`. For example, you can see its GPU utilization in
    the next screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 继续讨论 `algo-1`。例如，您可以在下一个截图中看到其 GPU 使用情况：
- en: '![Figure 9.4 – Viewing GPU utilization over time'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4 – 查看 GPU 使用情况随时间变化'
- en: '](img/B17705_09_4.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_4.jpg)'
- en: Figure 9.4 – Viewing GPU utilization over time
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 查看 GPU 使用情况随时间变化
- en: 'We also get a very nice view of system utilization over time, with one line
    per vCPU and GPU, as shown in the next screenshot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到一个非常好的系统使用情况视图，按 vCPU 和 GPU 每个一条线，如下图所示：
- en: '![Figure 9.5 – Viewing system utilization over time'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5 – 查看系统使用情况随时间变化'
- en: '](img/B17705_09_5.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_5.jpg)'
- en: Figure 9.5 – Viewing system utilization over time
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 查看系统使用情况随时间变化
- en: All this information is updated in near-real-time while your training job is
    running. Just launch a training job, open this view, and, after a few minutes,
    the graphs will show up and get updated.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些信息会在训练任务运行时近乎实时更新。只需启动一个训练任务，打开此视图，几分钟后，图表就会显示并更新。
- en: Now, let's see how we can enable detailed profiling information in our training
    jobs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在训练任务中启用详细的分析信息。
- en: Enabling profiling in SageMaker Debugger
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SageMaker Debugger 中启用分析
- en: Profiling collects framework metrics (**TensorFlow**, **PyTorch**, **Apache**
    **MXNet**, and XGBoost), data loader metrics, and Python metrics. For the latter,
    we can use **CProfile** or **Pyinstrument**.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 分析会收集框架指标（**TensorFlow**、**PyTorch**、**Apache** **MXNet** 和 XGBoost）、数据加载器指标和
    Python 指标。对于后者，我们可以使用 **CProfile** 或 **Pyinstrument**。
- en: Profiling can be configured in the estimator (which is the option we'll use).
    You can also enable it manually in SageMaker Studio on a running job (see the
    slider in *Figure 9.2*).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 分析可以在估算器中配置（这是我们将使用的选项）。你也可以在 SageMaker Studio 中的运行任务上手动启用它（见 *图 9.2* 中的滑块）。
- en: 'Let''s reuse our TensorFlow/Keras example from [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108),
    *Training Computer Vision Models*, and collect all profiling information every
    100 milliseconds:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重用我们在 [*第 6 章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108) 中的 TensorFlow/Keras
    示例，*训练计算机视觉模型*，并每 100 毫秒收集一次所有分析信息：
- en: 'First, we create a `FrameworkProfile` object containing default settings for
    the profiling, data loading, and Python configurations. For each one of these,
    we could specify precise time ranges or step ranges for data collection:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个 `FrameworkProfile` 对象，包含分析、数据加载和 Python 配置的默认设置。对于每一项配置，我们都可以指定精确的时间范围或步骤范围来进行数据收集：
- en: '[PRE2]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we create a `ProfilerConfig` object that sets framework parameters and
    the time interval for data collection:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个 `ProfilerConfig` 对象，设置框架参数和数据收集的时间间隔：
- en: '[PRE3]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we pass this configuration to our estimator, and train as usual:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将这个配置传递给估算器，然后像往常一样进行训练：
- en: '[PRE4]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As the training job runs, profiling data is automatically collected and saved
    in a default location in S3 (you can define a custom path with the `s3_output_path`
    parameter in `ProfilingConfig`). We could also use the `smdebug` **SDK** ([https://github.com/awslabs/sagemaker-debugger](https://github.com/awslabs/sagemaker-debugger))
    to load and inspect profiling data.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当训练任务运行时，分析数据会自动收集并保存在 S3 的默认位置（你可以通过 `ProfilingConfig` 中的 `s3_output_path`
    参数定义自定义路径）。我们还可以使用 `smdebug` **SDK**（[https://github.com/awslabs/sagemaker-debugger](https://github.com/awslabs/sagemaker-debugger)）来加载并检查分析数据。
- en: Shortly after the training job completes, we see summary information in the
    **Overview** tab, as shown in the next screenshot:![Figure 9.6 – Viewing profiling
    information
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练任务完成后不久，我们会在 **概览** 标签中看到汇总信息，如下图所示：![图 9.6 – 查看分析信息
- en: '](img/B17705_09_6.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_09_6.jpg)'
- en: Figure 9.6 – Viewing profiling information
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.6 – 查看分析信息
- en: 'We can also download a detailed report in HTML format (see the button in *Figure
    9.2*). For example, it tells us which are the most expensive GPU operators. Unsurprisingly,
    we see our `fmnist_model` function and the TensorFlow operator for 2D convolution,
    as visible in the next screenshot:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以下载详细的 HTML 格式报告（见 *图 9.2* 中的按钮）。例如，它会告诉我们哪些是最昂贵的 GPU 操作。毫无意外地，我们看到我们的 `fmnist_model`
    函数和用于二维卷积的 TensorFlow 操作，如下图所示：
- en: '![Figure 9.7 – Viewing the profiling report'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7 – 查看分析报告'
- en: '](img/B17705_09_7.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_7.jpg)'
- en: Figure 9.7 – Viewing the profiling report
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 查看分析报告
- en: The report also contains information on built-in rules that have been triggered
    during training, warning us about conditions such as low GPU usage, CPU bottlenecks,
    and more. These rules have default settings that can be customized if needed.
    We'll cover rules in more details in the next chapter when we'll discuss how to
    use SageMaker Debugger to debug training jobs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 报告还包含训练过程中触发的内置规则信息，提醒我们如 GPU 使用率低、CPU 瓶颈等情况。这些规则有默认设置，如果需要，可以进行自定义。我们将在下一章详细讨论规则，当时我们将讨论如何使用
    SageMaker Debugger 调试训练任务。
- en: For now, let's look at some common scaling issues for training jobs, and how
    we could address them. In the process, we'll mention several SageMaker features
    that will be covered in the rest of this chapter.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看训练任务中的一些常见扩展问题，以及我们如何解决它们。在此过程中，我们将提到本章后面将介绍的几个 SageMaker 特性。
- en: Solving training challenges
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决训练中的挑战
- en: 'We will dive into the challenges, and their solutions, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨挑战及其解决方案，如下所示：
- en: '*I need lots of storage on training instances.*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在训练实例上需要大量存储。*'
- en: As discussed in the previous example, most SageMaker training instances use
    EBS volumes, and you can set their size in the estimator. The maximum size of
    an EBS volume is 16 TB, so you should have more than enough. If your algorithm
    needs lots of temporary storage for intermediate results, this is the way to go.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的例子所述，大多数 SageMaker 训练实例使用 EBS 卷，你可以在估算器中设置它们的大小。EBS 卷的最大大小为 16 TB，所以应该绰绰有余。如果你的算法需要大量临时存储来存放中间结果，这就是解决方案。
- en: '*My dataset is very large, and it takes a long time to copy it to training
    instances.*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的数据集非常大，复制到训练实例上需要很长时间。*'
- en: Define "long"! If you're looking for a quick fix, you can use instance types
    with high network performance. For example, `ml.g4dn` and `ml.p3dn` instances
    support the **Elastic Fabric Adapter** (https://aws.amazon.com/hpc/efa), and can
    go all the way to 100 Gbit/s.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一下“长时间”！如果你在寻找快速解决方案，你可以使用具有高网络性能的实例类型。例如，`ml.g4dn` 和 `ml.p3dn` 实例支持 **弹性结构适配器**（[https://aws.amazon.com/hpc/efa](https://aws.amazon.com/hpc/efa)），可以达到最高
    100 Gbit/s 的速度。
- en: If that's not enough, and if you're training on a single instance, you should
    use pipe mode, which streams data from S3 instead of copying it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够，而且如果你正在对单个实例进行训练，你应该使用管道模式，它从 S3 流式传输数据，而不是复制数据。
- en: If training is distributed, you can switch the `FullyReplicated` to `ShardedbyS3Key`,
    which will only distribute a fraction of the dataset to each instance. This can
    be combined with pipe mode for extra performance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练是分布式的，你可以将 `FullyReplicated` 更改为 `ShardedbyS3Key`，这将仅将数据集的一部分分发到每个实例。这可以与管道模式结合使用，以提高性能。
- en: '*My dataset is very large, and it doesn''t fit in RAM.*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的数据集非常大，无法完全装入内存。*'
- en: If you want to stick to a single instance, a quick way to solve the problem
    is to scale up. The `ml.r5d.24xlarge` and `ml.p3dn.24xlarge` instances have 768
    GB of RAM! If distributed training is an option, then you should configure it
    and apply data parallelism.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想坚持使用单个实例，解决这个问题的快速方法是进行扩展。`ml.r5d.24xlarge` 和 `ml.p3dn.24xlarge` 实例具有 768
    GB 的内存！如果分布式训练是一个选项，那么你应该配置它并应用数据并行处理。
- en: '*CPU utilization is low.*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*CPU 使用率很低。*'
- en: Assuming you haven't overprovisioned, the most likely cause is I/O latency (network
    or storage). The CPU is stalled because it's waiting for data to be fetched from
    wherever it's stored.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你没有过度配置，最可能的原因是 I/O 延迟（网络或存储）。CPU 被阻塞，因为它在等待从存储位置获取数据。
- en: The first thing you should review is the data format. As discussed in previous
    chapters, there's no escaping **RecordIO** or **TFRecord** files. If you're using
    other formats (CSV, individual images, and so on), you should start there before
    tweaking the infrastructure.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先你应该检查数据格式。如前几章所述，**RecordIO** 或 **TFRecord** 文件是不可避免的。如果你使用的是其他格式（如 CSV、单独的图片等），你应该在调整基础设施之前从这些地方入手。
- en: 'If data is copied from S3 to an EBS volume, you can try using an instance with
    more EBS bandwidth. Numbers are available at the following location:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是从 S3 复制到 EBS 卷的，你可以尝试使用具有更多 EBS 带宽的实例。有关详细信息，请访问以下位置：
- en: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html)'
- en: You can also switch to an instance type with local NVMe storage (g4dn and p3dn).
    If the problem persists, you should review the code that reads data and passes
    it to the training algorithm. It probably needs more parallelism.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以切换到具有本地 NVMe 存储的实例类型（g4dn 和 p3dn）。如果问题仍然存在，你应该检查读取数据并将其传递给训练算法的代码。它可能需要更多的并行处理。
- en: If data is streamed from S3 with pipe mode, it's unlikely that you've hit the
    maximum transfer speed of 25 GB/s, but it's worth checking the instance metric
    in CloudWatch. If you're sure that nothing else could be the cause, you should
    move to other file storage services, such as **Amazon** **EFS** and **Amazon**
    **FSx for Lustre**.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '*GPU memory utilization is low.*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'The GPU doesn''t receive enough data from the CPU. You need to increase batch
    size until memory utilization is close to 100%. If you increase it too much, you''ll
    get an angry `out of memory` error message, such as this one:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When working with a multi-GPU instance in a data-parallel configuration, you
    should multiply the batch size passed to the estimator by the number of GPUs present
    in an instance.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: When increasing batch size, you have to factor in the number of training samples
    available. For example, the **Pascal** VOC dataset that we used for Semantic Segmentation
    in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091), *Training Computer
    Vision Models*, only has 1,464 samples, so it would probably not make sense to
    increase batch size above 64 or 128.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Finally, batch size has an important effect on job convergence. Very large batches
    may slow it down, so you may want to increase the learning rate accordingly.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, you'll simply have to accept that GPU memory utilization is low!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '*GPU utilization is low.*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Maybe your model is simply not large enough to keep the GPU really busy. You
    should try scaling down on a smaller GPU.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: If you're working with a large model, the GPU is probably stalled because the
    CPU can't feed it fast enough. If you're in control of the data loading code,
    you should try to add more parallelism, such as additional threads for data loading
    and preprocessing. If you're not, you should try a larger instance type with more
    vCPUs. Hopefully, they can be put to good use by the data-loading code.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: If there's enough parallelism in the data loading code, then slow I/O is likely
    to be responsible. You should look for a faster alternative (NVMe, EFS, or FSx
    for Lustre).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '*GPU utilization is high*.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: That's a good place to be! You're efficiently using the infrastructure that
    you're paying for. As discussed in the previous example, you can try scaling up
    (more vCPUs or more GPUs), or scaling out (more instances). Combining both can
    work for highly parallel workloads such as deep learning.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Now we know a little more about scaling jobs, let's learn about more SageMaker
    features, starting with pipe mode.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Streaming datasets with pipe mode
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The default setting of estimators is to copy the dataset to training instances,
    which is known as **file mode**. Instead, **pipe mode** streams it directly from
    S3\. The name of the feature comes from its use of **Unix** **named pipes** (also
    known as **FIFOs**): at the beginning of each epoch, one pipe is created per input
    channel.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Pipe mode removes the need to copy any data to training instances. Obviously,
    training jobs start quicker. They generally run faster too, as pipe mode is highly
    optimized. Another benefit is that you won't have to provision any storage for
    the dataset on training instances.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Cutting on training time and storage means that you''ll save money. The larger
    the dataset, the more you''ll save. You can find benchmarks at the following link:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you can start experimenting with pipe mode for datasets in the
    hundreds of megabytes and beyond. In fact, this feature enables you to work with
    infinitely large datasets. As storage and RAM requirements are no longer coupled
    to the size of the dataset, there's no practical limit on the amount of data that
    your algorithm can crunch. Training on petabyte-scale datasets becomes possible.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Using pipe mode with built-in algorithms
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prime candidates for pipe mode are built-in algorithms, as most of them
    support it natively:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Learner**, **k-Means**, **k-Nearest Neighbors**, **Principal Component
    Analysis**, **Random Cut Forest**, and **Neural Topic Modeling**: RecordIO-wrapped
    protobuf or CSV data'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factorization Machines**, **Latent Dirichlet Allocation**: RecordIO-wrapped
    protobuf data'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BlazingText** (supervised mode): Augmented manifest'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image Classification** or **Object Detection**: RecordIO-wrapped protobuf
    data or augmented manifest'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic segmentation**: Augmented manifest.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should already be familiar with `im2rec` tool has an option to generate
    multiple list files (`--chunks`). If you have existing list files, you can of
    course split them yourself.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'We looked at the **augmented manifest** format when we discussed datasets annotated
    by **SageMaker** **Ground Truth** in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091),
    *Training Computer Vision Models*. For computer vision algorithms, this **JSON
    Lines** file contains the location of images in S3 and their labeling information.
    You can learn more at the following link:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html](https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Using pipe mode with other algorithms and frameworks
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TensorFlow supports pipe mode thanks to the `PipeModeDataset` class implemented
    by AWS. Here are some useful resources:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/aws/sagemaker-tensorflow-extensions](https://github.com/aws/sagemaker-tensorflow-extensions)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_script_mode_pipe_mode](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_script_mode_pipe_mode)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233](mailto:https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For other frameworks and for your own custom code, it''s still possible to
    implement pipe mode inside the training container. A Python example is available
    at the following link:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/pipe_bring_your_own](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/pipe_bring_your_own)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying data loading with MLIO
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**MLIO** (https://github.com/awslabs/ml-io) is an AWS open source project that
    lets you load data stored in memory, on local storage, or in S3 with pipe mode.
    The data can then be converted into different popular formats.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the high-level features:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '**Input formats**: **CSV**, **Parquet**, RecordIO-protobuf, **JPEG**, **PNG**'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversion formats**: NumPy arrays, SciPy matrices, **Pandas** **DataFrames**,
    TensorFlow tensors, PyTorch tensors, Apache MXNet arrays, and **Apache** **Arrow**'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API available in Python and **C++**
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's run some examples with pipe mode.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Training factorization machines with pipe mode
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''re going to revisit the example we used in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*, where we trained a recommendation model on
    the **MovieLens** dataset. At the time, we used a small version of the dataset,
    limited to 100,000 reviews. This time, we''ll go for the largest version:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'We download and extract the dataset:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This dataset includes 25,000,095 reviews, from 162,541 users, on 62,423 movies.
    Unlike the 100k version, movies are not numbered sequentially. The last movie
    ID is 209,171, which needlessly increases the number of features. The alternative
    would be to renumber movies, but let''s not do that here:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Just like in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)*,
    Training Machine Learning Models* we load the dataset into a sparse matrix (`lil_matrix`
    from SciPy), split it for training and testing, and convert both datasets into
    RecordIO-wrapped protobuf. Given the size of the dataset, this could take 45 minutes
    on a small Studio instance. Then, we upload the datasets to S3.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we configure the two input channels, and we set their input mode to pipe
    mode instead of file mode:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We then configure the estimator, and train as usual on an `ml.c5.xlarge` instance
    (4 vCPUs, 8 GB RAM, $0.23 per hour in `eu-west-1`).
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Looking at the training log, we see the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As expected, no time was spent copying the dataset. The same step in file mode
    takes 66 seconds. Even with a modest 1.5 GB dataset, pipe mode already makes sense.
    As datasets get bigger, this advantage will only increase!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to distributed training.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Distributing training jobs
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Distributed training lets you scale training jobs by running them on a cluster
    of CPU or GPU instances. It can be used to solve two different problems: very
    large datasets, and very large models.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data parallelism and model parallelism
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some datasets are too large to be trained in a reasonable amount of time on
    a single CPU or GPU. Using a technique called *data parallelism*, we can distribute
    data across the training cluster. The full model is still loaded on each CPU/GPU,
    which only receive an equal share of the dataset, not the full dataset. In theory,
    this should speed up training linearly according to the number of CPU/GPUs involved,
    and as you can guess, the reality is often different.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Believe it or not, some state-of-the-art-deep learning models are too large
    to fit on a single GPU. Using a technique called *model parallelism*, we can split
    it, and distribute the layers across a cluster of GPUs. Hence, training batches
    will flow across several GPUs to be processed by all layers.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see where we can use distributed training in SageMaker.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Distributing training for built-in algorithms
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data parallelism is available for almost all built-in algorithms (semantic segmentation
    and LDA are notable exceptions). As they are implemented with Apache MXNet, they
    automatically use its native distributed training mechanism.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Distributing training for built-in frameworks
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow, PyTorch, Apache MXNet, and **Hugging Face** have native data parallelism
    mechanisms, and they're supported on SageMaker. **Horovod** ([https://github.com/horovod/horovod](https://github.com/horovod/horovod))
    is available too.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: For TensorFlow, PyTorch, and Hugging Face, you can also use the newer **SageMaker
    Distributed Data Parallel Library** and **SageMaker Model Parallel Library**.
    Both will be covered later in this chapter.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed training often requires framework-specific changes to your training
    code. You can find more information in the framework documentation (for example
    [https://www.tensorflow.org/guide/distributed_training](https://www.tensorflow.org/guide/distributed_training)),
    and in sample notebooks hosted at [https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '`sagemaker-python-sdk/tensorflow_script_mode_horovod`'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b) `advanced_functionality/distributed_tensorflow_mask_rcnn`
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sagemaker-python-sdk/keras_script_mode_pipe_mode_horovod`'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sagemaker-python-sdk/pytorch_horovod_mnist`'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each framework has its peculiarities, yet everything we discussed in the previous
    sections stands true. If you want to make the most of your infrastructure, you
    need to pay attention to batch size, synchronization, and so on. Experiment, monitor,
    analyze, and iterate!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Distributing training for custom containers
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you're training with your own custom container, you have to implement your
    own distributed training mechanism. Let's face it, this is going to be a lot of
    work. SageMaker only helps to provide the name of cluster instances and the name
    of the container network interface. They are available inside the container in
    the `/opt/ml/input/config/resourceconfig.json` file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information at the following link:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: It's time for a distributed training example!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Scaling an image classification model on ImageNet
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091), *Training
    Computer Vision Models*, we trained the image classification algorithm on a small
    dataset with dog and cat images (25,000 training images). This time, let's go
    for something a little bigger.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: We're going to train a ResNet-50 network from scratch on the **ImageNet** dataset
    – the reference dataset for many computer vision applications ([http://www.image-net.org](http://www.image-net.org)).
    The 2012 version contains 1,281,167 training images (140 GB) and 50,000 validation
    images (6.4 GB) from 1,000 classes.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: If you want to experiment at a smaller scale, you can work with 5-10% of the
    dataset. Final accuracy won't be as good, but it doesn't matter for our purposes.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the ImageNet dataset
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This requires a lot of storage – the dataset is 150 GB, so please make sure
    you have at least 500 GB available to store it in ZIP and processed formats. You're
    also going to need a lot of bandwidth and a lot of patience to download it. I
    used an EC2 instance running `us-east-1` region, and my download took *five days*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Visit the ImageNet website, register to download the dataset, and accept the
    conditions. You'll get a username and an access key allowing you to download the
    dataset.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One of the TensorFlow repositories includes a great script that will download
    the dataset and extract it. Using `nohup` is essential so that the process continues
    running even if your session is terminated:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once this is over (again, downloading will take days), the `imagenet/train`
    directory contains the training dataset (one folder per class). The `imagenet/validation`
    directory contains 50,000 images in the same folder. We can use a simple script
    to organize it with one folder per class:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''re going to build RecordIO files with the `im2rec` tool present in the
    Apache MXNet repository. Let''s install dependencies, and fetch `im2rec`:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the `imagenet` directory, we run `im2rec` twice – once to build the list
    files, and once to build the RecordIO files. We create RecordIO files that are
    approximately 1 GB each (we''ll see why that matters in a second). We also resize
    the smaller dimension of images to `224` so that the algorithm won''t have to
    do it:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we sync the dataset to S3:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The dataset is now ready for training.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Defining our training job
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the dataset is ready, we need to think about the configuration of
    our training job. Specifically, we need to come up with the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: An input configuration, defining the location and the properties of the dataset
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure requirements to run the training job
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameters to configure the algorithm
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at each one of these items in detail.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Defining the input configuration
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the size of the dataset, pipe mode sounds like a great idea. Out of curiosity,
    I tried training in file mode. Even with a 100 Gbit/s network interface, it took
    almost 25 minutes to copy the dataset from S3 to local storage. Pipe mode it is!
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'You may wonder why we took care of splitting the dataset into multiple files.
    Here''s why:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: In general, multiple files create opportunities for more parallelism, making
    it easier to write fast data loading and processing code.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can shuffle the files at the beginning of each epoch, removing any potential
    bias caused by the order of samples.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It makes it very easy to work with a fraction of the dataset.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we've defined the input configuration, what about infrastructure requirements?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Defining infrastructure requirements
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ImageNet is a large and complex dataset that requires a lot of training to reach
    good accuracy.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: A quick test shows that a single `ml.p3.2xlarge` instance with the batch size
    set to 128 will crunch through the dataset at about 335 images per second. As
    we have about 1,281,167 images, we can expect one epoch to last about 3,824 seconds
    (about 1 hour and 4 minutes).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that we need to train for 150 epochs to get decent accuracy, we're
    looking at a job that should last (3,824/3,600)*150 = 158 hours (about 6.5 days).
    This is probably not acceptable from a business perspective. For the record, at
    $3.825 per instance per hour in `us-east-1`, that job would cost about $573.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to speed up our job with `ml.p3dn.24xlarge` instances. Each one hosts
    eight NVIDIA V100s with 32 GB of GPU memory (twice the amount available on other
    `p3` instances). They also have 96 **Intel** **Skylake** cores, 768 GB of RAM,
    and 1.8 TB of local NVMe storage. Although we're not going to use it here, the
    latter is a fantastic storage option for long-running, large-scale jobs. Last
    but not least, this instance type has 100 Gbit/s networking, a great feature for
    streaming data from S3 and for inter-instance communication.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: At $35.894 per hour per instance in `us-east-1`, you may not want to try this
    at home or even at work without getting permission. Your service quotas probably
    don't let you run that much infrastructure anyway, and you would have to get in
    touch with AWS Support first.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to talk about *managed spot training* – a great
    way to slash training costs. We'll revisit the ImageNet example once we've covered
    this topic, so you definitely should refrain from training right now!
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Training on ImageNet
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s configure the training job:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'We configure pipe mode on both input channels. The files of the training channel
    are shuffled for extra randomness:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To begin with, we configure the `Estimator` module with a single `ml.p3dn.24xlarge`
    instance:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We set hyperparameters, starting with a reasonable batch size of 1,024, and
    we launch training:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Updating batch size
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time per epochs is 727 seconds. For 150 epochs, this translates into 30.3 hours
    of training (1.25 days), and a cost of $1,087\. The good news is that we're going
    5x faster. The bad news is that cost has gone up 2x. Let's start scaling this.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at total GPU utilization in CloudWatch, we see that it doesn''t exceed
    300%. That is, 37.5% on each GPU. This probably means that our batch size is too
    low to keep the GPUs fully busy. Let''s bump it to (1,024/0.375)=2730, rounded
    up to 2,736 to be divisible by 8:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Depending on algorithm versions, `out of memory` errors.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Training again, an epoch now lasts 758 seconds. It looks like maxing out GPU
    memory usage didn't make a big difference this time. Maybe it's offset by the
    cost of synchronizing gradients? Anyway, keeping GPU cores as busy as possible
    is good practice.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Adding more instances
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s add a second instance to scale out the training job:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Time for epoch is now 378 seconds! For 150 epochs, this translates to 15.75
    hours of training, and a cost of $1,221\. Compared to our initial job, this is
    2x faster and 3x cheaper!
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'How about four instances? Let''s see if we can we keep scaling:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Time for epoch is now 198 seconds! For 150 epochs, this translates to 8.25 hours
    of training, and a cost of $1,279\. We sped up 2x again, with a marginal cost
    increase.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, shall we train eight instances? Of course! Who wouldn''t want to train
    on 64 GPUs, 327K CUDA cores, and 2 TB (!) of GPU RAM:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Time for epoch is now 99 seconds. For 150 epochs, this translates into 4.12
    hours of training, and a cost of $1,277\. We sped up 2x *again*, at no cost increase.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Summing things up
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For 2x the initial cost, we've accelerated our training job 38x, thanks to pipe
    mode, distributed training, and state-of-the-art GPU instances.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 9.8 Outcome of the training jobs'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Fig 9.8 Outcome of the training jobs
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Not bad at all! Saving days on your training jobs helps you iterate faster,
    get to a high-quality model quicker, and get to production sooner. I'm pretty
    sure this would easily offset the extra cost. Still, in the next chapter, we'll
    see how we can slash training costs massively with managed spot training.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Now that we're familiar with distributed training, let's take a look at two
    new SageMaker libraries for data parallelism and model parallelism.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Training with the SageMaker data and model parallel libraries
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These two libraries were introduced in late 2020, and significantly improve
    the performance of large-scale training jobs.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'The **SageMaker** **Distributed Data Parallel** (**DDP**) library implements
    a very efficient distribution of computation on GPU clusters. It optimizes network
    communication by eliminating inter-GPU communication, maximizing the amount of
    time and resources they spend on training. You can learn more at the following
    link:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/](https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: DDP is available for TensorFlow, PyTorch, and Hugging Face. The first two require
    minor modifications to the training code, but the last one doesn't. As DDP only
    makes sense for large, long-running training jobs, available instance sizes are
    `ml.p3.16xlarge`, `ml.p3dn24dnxlarge`, and `ml.p4d.24xlarge`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'The **SageMaker** **Distributed Model Parallel** (**DMP**) library solves a
    different problem. Some large deep learning models are simply too bulky to fit
    inside the memory of a single GPU. Others barely fit, forcing you to work with
    very small batch sizes, and slowing down your training jobs. DMP solves this problem
    by automatically partitioning models across a cluster of GPUs and orchestrating
    the flow of data through these different partitions. You can learn more at the
    following link:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/aws/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/](https://aws.amazon.com/blogs/aws/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: DMP is available for TensorFlow, PyTorch, and Hugging Face. Again, the first
    two require small modifications to the training code, and the last one doesn't,
    as the Hugging Face `Trainer` API fully supports DMP.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Let's give both a try by revisiting our TensorFlow and Hugging Face examples
    from [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending
    Machine Learning Services Using Built-In Frameworks*.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Training on TensorFlow with SageMaker DDP
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our initial code used the high-level Keras API: `compile()`, `fit()`, and so
    on. In order to implement DDP, we need to rewrite this code to use `tf.GradientTape()`,
    and to implement a custom training loop. It''s not as difficult as it sounds,
    so let''s get to work:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import and initialize DDP:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we retrieve the list of GPUs present on an instance, and we assign them
    a local DDP rank, which is just an integer identifier. We also allow memory growth,
    a TensorFlow feature required by DDP:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As recommended by the documentation, we increase the batch size and the learning
    rate according to the number of GPUs present in the training cluster. This is
    very important for job accuracy:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We then create a loss function and an optimizer. Labels have been one-hot encoded
    during preprocessing, so we use `CategoricalCrossentropy`, not `SparseCategoricalCrossentropy`.
    We also initialize model and optimizer variables on all GPUs:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we need to write a `training_step()` function, and decorate it with `@tf.function`
    so that DDP recognizes it. As its name implies, this function is responsible for
    running a training step on each GPU in the training cluster: predict a batch,
    compute loss, compute gradients, and apply them. It''s based on the `tf.GradientTape()`
    API, which we simply wrap with `sdp.DistributedGradientTape()`. At the end of
    each training step, we use `sdp.oob_allreduce()` to compute the average loss,
    using values coming from all GPUs:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we write the training loop. There''s nothing particular about it. To
    avoid log pollution, we only print out messages from the master GPU (rank 0):'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we save the model on GPU #0 only:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Moving to our notebook, we configure this job with two `ml.p3.16xlarge` instances,
    and we enable data parallelism with an additional parameter in the estimator:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We train as usual, and we see steps going by in the training log:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, it's not really difficult to scale training jobs with SageMaker
    DDP, especially if your training code already uses low-level APIs. We used TensorFlow
    here, and the process for PyTorch is very similar.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we can train large Hugging Face models with both libraries.
    Indeed, state-of-the-art NLP models are getting larger and more complex all the
    time, and they're good candidates for data parallelism and model parallelism.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Training on Hugging Face with SageMaker DDP
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the Hugging Face `Trainer` API fully supports DDP, we don''t need to change
    anything in our training script. Woohoo. All it takes is an extra parameter in
    the estimator. Set the instance type and instance count, and you''re good to go:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Training on Hugging Face with SageMaker DMP
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adding DMP is not difficult either. Our Hugging Face example uses a **DistilBERT**
    model that is about 250 MB. That''s small enough to fit on a single GPU, but let''s
    try to train with DMP anyway:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to configure `processes_per_host` to a value lower or equal
    to the number of GPUs on a training instance. Here, I''ll use an `ml.p3dn.24xlarge`
    instance with 8 NVIDIA V100 GPUs:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we configure DMP options. Here, I set the most important ones – the number
    of model partitions that we want (`partitions`), and how many times they should
    be replicated for increased parallelism (`microbatches`). In other words, our
    model will be split in four, each split will be duplicated, and these eight splits
    will each run on a different GPU. You can find more information on all parameters
    at the following link:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we configure our estimator and train as usual:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can find additional examples here:'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: TensorFlow and PyTorch
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training](https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training)'
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face: [https://github.com/huggingface/notebooks/tree/master/sagemaker](https://github.com/huggingface/notebooks/tree/master/sagemaker)'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To close this chapter, let's now look at storage options you should consider
    for very large-scale, high-performance training jobs.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Using other storage services
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve used S3 to store training data. At a large scale, throughput
    and latency can become a bottleneck, making it necessary to consider other storage
    services:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon** **Elastic File System** (**EFS**): [https://aws.amazon.com/efs](https://aws.amazon.com/efs)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon** **FSx for Lustre**: [https://aws.amazon.com/fsx/lustre](https://aws.amazon.com/fsx/lustre).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This section requires a little bit of AWS knowledge on VPCs, subnets, and security
    groups. If you''re not familiar at all with these, I''d recommend reading the
    following:'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html)'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Working with SageMaker and Amazon EFS
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: EFS is a managed storage service compatible with **NFS** v4\. It lets you create
    volumes that can be attached to EC2 instances and SageMaker instances. This is
    a convenient way to share data, and you can use it to scale I/O for large training
    jobs.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: By default, files are stored in the **Standard** class. You can enable a life
    cycle policy that automatically moves files that haven't been accessed for a certain
    time to the **Infrequent Access**, which is slower but more cost-effective.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pick one of two throughput modes:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '**Bursting throughput**: Burst credits are accumulated over time, and burst
    capacity depends on the size of the filesystem: 100 MB/s, plus an extra 100 MB/s
    for each TB of storage.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provisioned throughput**: You set the expected throughput, from 1 to 1,024
    MB/s.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also pick one of two performance modes:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '**General purpose**: This is fine for most applications.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max I/O**: This is the one to use if tens or hundreds of instances are accessing
    the volume. Throughput will be maximized at the expense of latency.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's create an 8 GB EFS volume. Then, we'll mount it on an EC2 instance to
    copy the **Pascal VOC** dataset that we previously prepared, and we'll train an
    object detection job. To keep costs reasonable, we won't scale the job, but the
    overall process would be exactly the same at any scale.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning an EFS volume
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The EFS console makes it extremely simple to create a volume. You can find
    detailed instructions at [https://docs.aws.amazon.com/efs/latest/ug/getting-started.html](https://docs.aws.amazon.com/efs/latest/ug/getting-started.html):'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: We set the volume name to `sagemaker-demo`.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We select our default VPC, and use **Regional** availability.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We create the volume. Once it''s ready, you should see something similar to
    the following screenshot:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.9– Creating an EFS volume'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_09_8.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9– Creating an EFS volume
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: The EFS volume is ready to receive data. We're now going to create a new EC2
    instance, mount the EFS volume, and copy the dataset.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Creating an EC2 instance
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As EFS volumes live inside a VPC, they can only be accessed by instances located
    in the same VPC. These instances must also have a *security group* that allows
    inbound NFS traffic:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: In the VPC console ([https://console.aws.amazon.com/vpc/#vpcs:sort=VpcId](https://console.aws.amazon.com/vpc/#vpcs:sort=VpcId)),
    we write down the ID of our default VPC. For me, it's `vpc-def884bb`.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Still in the VPC console, we move to the **Subnets** section ([https://console.aws.amazon.com/vpc/#subnets:sort=SubnetId](https://console.aws.amazon.com/vpc/#subnets:sort=SubnetId)).
    We write down the subnet IDs and the availability zone for all subnets hosted
    in the default VPC.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For me, they look like what''s shown in the next screenshot:'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Viewing subnets for the default VPC'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_09_9.jpg)'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.10 – Viewing subnets for the default VPC
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moving to the EC2 console, we create an EC2 instance. We select the Amazon Linux
    2 image and a `t2.micro` instance size.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we set `eu-west-1a` **Availability Zone**. We also assign it the security
    group we just created, **IAM role** to a role with appropriate S3 permissions,
    and **File Systems** to the EFS filesystem that we just created. We also make
    sure to tick the box that automatically creates and attaches the required security
    groups.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next screens, we leave storage and tags as they are, and we attach a
    security group that allows incoming `ssh`. Finally, we launch instance creation.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accessing an EFS volume
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the instance is ready, we can `ssh` to it:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'We see that the EFS volume has been automatically mounted:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We move to that location, and sync our PascalVOC dataset from S3\. As the filesystem
    is mounted as `root`, we need to use `sudo`.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Job done. We can log out and shut down or terminate the instance, as we won't
    need it anymore.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's train with this dataset.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Training an object detection model with EFS
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training process is identical, except for the location of the input data:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using the `TrainingInput` object to define input channels, we use
    the `FileSystemInput` object, passing the identifier of our EFS volume and the
    absolute data path inside the volume:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We configure the `Estimator` module, passing the list of subnets for the VPC
    hosting the EFS volume. SageMaker will launch training instances there so that
    they may mount the EFS volume. We also need to pass a security group allowing
    NFS traffic. We can reuse the one that was automatically created for our EC2 instance
    (not the one allowing ssh access) – it's visible in the **Security** tab in the
    instance details, as shown in the next screenshot:![Figure 9.11 – Viewing security
    groups
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_09_10.jpg)'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: For testing purposes, we only train for one epoch. Business as usual, although,
    this time, data is loaded from our EFS volume.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once training is complete, you may delete the EFS volume in the EFS console
    to avoid unnecessary costs.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we can use another storage service – Amazon FSx for Lustre.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Working with SageMaker and Amazon FSx for Lustre
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Very large-scale workloads require high throughput and low latency storage –
    two qualities that Amazon FSx for Lustre possesses. As the name implies, this
    service is based on the Lustre filesystem ([http://lustre.org](http://lustre.org)),
    a popular open source choice for **HPC** applications.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'The smallest filesystem you can create is 1.2 TB (like I said, "very large-scale").
    We can pick one of two deployment options for FSx filesystems:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '**Persistent**: This should be used for long-term storage that requires high
    availability.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scratch**: Data is not replicated, and it won''t persist if a file server
    fails. In exchange, we get high burst throughput, making this is a good choice
    for spiky, short-term jobs.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, a filesystem can be backed by an S3 bucket. Objects are automatically
    copied from S3 to FSx when they're first accessed.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like for EFS, a filesystem lives inside a VPC, and we''ll need a security
    group allowing inbound Lustre traffic (ports 988 and 1,021-2,023). You can create
    this in the EC2 console, and it should be similar to the following screenshot:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Creating a security group for FSx for Lustre'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_09_11.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 – Creating a security group for FSx for Lustre
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the filesystem:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: In the FSx console, we create a filesystem named `sagemaker-demo`, and we select
    the **Scratch** deployment type.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set storage capacity to 1.2 TB.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `eu-west-1a` subnet of the default VPC, and we assign it to the security
    group we just created.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `s3://sagemaker-eu-west-1-123456789012`) and the prefix (`pascalvoc`).
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, we review our choices, as shown in the following screenshot,
    and we create the filesystem.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After a few minutes, the filesystem is in service, as shown in the following
    screenshot:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Creating an FSx volume'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_09_12.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.13 – Creating an FSx volume
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: As the filesystem is backed by an S3 bucket, we don't need to populate it. We
    can proceed directly to training.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Training an object detection model with FSx for Lustre
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will train the model using FSx as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to what we just did with EFS, we define input channels with `FileSystemInput`.
    One difference is that the directory path must start with the name of the filesystem
    mount point. You can find it as **Mount name** in the FSx console:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: All other steps are identical. Don't forget to update the name of the security
    group passed to the `Estimator` module.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we're done training, we delete the FSx filesystem in the console.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This concludes our exploration of storage options for SageMaker. Summing things
    up, here are my recommendations:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: First, you should use RecordIO or TFRecord data as much as possible. They're
    convenient to move around, faster to train on, and they work with both file mode
    and pipe mode.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For development and small-scale production, file mode is completely fine. Your
    primary focus should always be your machine learning problem, not useless optimization.
    Even at a small scale, EFS can be an interesting option for collaboration, as
    it makes it easy to share datasets and notebooks.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you train with built-in algorithms, pipe mode is a no-brainer, and you should
    use it at every opportunity. If you train with frameworks or your own code, implementing
    pipe mode will take some work, and is probably not worth the engineering effort
    unless you're working at a significant scale (hundreds of gigabytes or more).
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have large, distributed workloads with tens of instances or more, EFS
    in Performance Mode is worth trying. Don't go near the mind-blowing FSx for Lustre
    unless you have insane workloads.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how and when to scale training jobs. You saw that
    it definitely takes some careful analysis and experimentation to find the best
    setup: scaling up versus scaling out, CPU versus GPU versus multi-GPU, and so
    on. This should help you to make the right decisions for your own workloads and
    avoid costly mistakes.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to achieve significant speedup with techniques such as
    distributed training, data parallelism, model parallelism, RecordIO, and pipe
    mode. Finally, you learned how to set Amazon EFS and Amazon FSx for Lustre for
    large-scale training jobs.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll cover advanced features for hyperparameter optimization,
    cost optimization, model debugging, and more.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL

# 神经网络

本章我们将涵盖以下配方：

+   感知机分类器

+   神经网络 – 多层感知机

+   与神经网络堆叠

# 介绍

最近，神经网络和深度学习非常流行，因为它们解决了很多难题，并且可能已经成为人工智能公众面貌的重要组成部分。让我们探索scikit-learn中可用的前馈神经网络。

# 感知机分类器

使用scikit-learn，你可以探索感知机分类器，并将其与scikit-learn中的其他分类方法进行比较。此外，感知机是神经网络的构建模块，神经网络是机器学习中的一个重要部分，特别是在计算机视觉领域。

# 准备开始

让我们开始吧。过程如下：

1.  加载UCI糖尿病分类数据集。

1.  将数据集划分为训练集和测试集。

1.  导入感知机。

1.  实例化感知机。

1.  然后训练感知机。

1.  尝试在测试集上使用感知机，或者更好地计算`cross_val_score`。

加载UCI糖尿病数据集：

[PRE0]

你已经加载了`X`，输入特征集，以及`y`，我们希望预测的变量。将`X`和`y`划分为测试集和训练集。通过分层目标集来完成这一步，确保在训练集和测试集中类的比例平衡：

[PRE1]

# 如何实现……

1.  对特征集进行缩放。仅在训练集上执行缩放操作，然后继续进行测试集：

[PRE2]

1.  实例化并在训练集上训练感知机：

[PRE3]

1.  测量交叉验证得分。将`roc_auc`作为交叉验证评分机制。此外，通过设置`cv=skf`，使用分层K折交叉验证：

[PRE4]

1.  在测试集上评估性能。导入`sklearn.metrics`模块中的两个指标，`accuracy_score`和`roc_auc_score`：

[PRE5]

测试很快就完成了。结果表现还行，比逻辑回归稍差，逻辑回归的准确率为75%（这是一个估计值；我们不能将本章的逻辑回归与任何之前章节中的逻辑回归做比较，因为训练集和测试集的划分不同）。

# 它是如何工作的……

感知机是大脑中神经元的简化模型。在下面的图示中，感知机从左边接收输入 *x[1]* 和 *x[2]*。计算偏置项 *w[0]* 和权重 *w[1]*、*w[2]*。*x[i]* 和 *w[i]* 组成一个线性函数。然后将这个线性函数传递给激活函数。

在以下激活函数中，如果权重与输入向量的点积之和小于零，则将单独的行分类为0；否则分类为1：

![](img/56833da2-8df6-4a37-a6cb-5133ddeda3c3.png)

这发生在单次迭代或遍历感知机时。该过程会在多个迭代中重复，每次都会重新调整权重，从而最小化损失函数。

关于感知器和当前神经网络的状态，它们表现良好，因为研究人员已经尝试了许多方法。实际上，基于目前的计算能力，它们的表现非常好。

随着计算能力的不断提升，神经网络和感知器能够解决越来越复杂的问题，且训练时间持续缩短。

# 还有更多内容...

尝试通过调整感知器的超参数来运行网格搜索。几个显著的参数包括正则化参数`penalty`和`alpha`、`class_weight`以及`max_iter`。`class_weight`参数通过赋予代表性不足的类别更多的权重，能够很好地处理类别不平衡问题。`max_iter`参数表示感知器的最大迭代次数。一般来说，其值越大越好，因此我们将其设置为50。（请注意，这是针对scikit-learn 0.19.0的代码。在scikit-learn 0.18.1版本中，请使用`n_iter`参数代替`max_iter`参数。）

尝试以下网格搜索：

[PRE6]

查看最佳参数和最佳得分：

[PRE7]

使用交叉验证调整超参数已改善结果。现在尝试使用一组感知器进行集成学习，如下所示。首先注意并选择网格搜索中表现最好的感知器：

[PRE8]

执行网格搜索：

[PRE9]

查看新的交叉验证得分和最佳参数：

[PRE10]

因此，对于这个数据集，一组感知器的表现优于单一感知器。

# 神经网络 – 多层感知器

在 scikit-learn 中使用神经网络非常简单，步骤如下：

1.  加载数据。

1.  使用标准缩放器对数据进行缩放。

1.  进行超参数搜索。首先调整alpha参数。

# 准备就绪

加载我们在[第9章](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml)中使用的中等规模的加州住房数据集，*决策树算法与集成方法*：

[PRE11]

将目标变量进行分箱，使得目标训练集和目标测试集更为相似。然后使用分层的训练/测试拆分：

[PRE12]

# 如何做...

1.  首先对输入变量进行缩放。仅在训练数据上训练缩放器：

[PRE13]

1.  然后，在测试集上执行缩放：

[PRE14]

1.  最后，执行随机搜索（或如果你喜欢，也可以进行网格搜索）来找到`alpha`的合理值，确保其得分较高：

[PRE15]

# 它是如何工作的...

在神经网络的背景下，单一感知器的结构如下所示：

![](img/a494cfb8-d5cd-4eea-8df9-c8190c0558f0.png)

输出是权重和输入的点积之和的函数。该函数*f*是激活函数，可以是sigmoid曲线。例如，在神经网络中，超参数激活指的就是这个函数。在 scikit-learn 中，有identity、logistic、tanh和relu的选项，其中logistic即为sigmoid曲线。

整个网络是这样的（以下是来自scikit文档的图示，链接：[http://scikit-learn.org/stable/modules/neural_networks_supervised.html](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)）：

![](img/1eefe899-179d-46b0-81eb-c8adbda3965c.png)

使用我们熟悉的数据集——加利福尼亚住房数据集来训练神经网络是很有教育意义的。加利福尼亚住房数据集似乎更适合非线性算法，特别是树算法和树的集成。树算法在这个数据集上表现得很好，并为算法在该数据集上的表现建立了基准。

最终，神经网络表现还不错，但远不如梯度提升机好。此外，它们在计算上非常昂贵。

# 关于神经网络的哲学思考

神经网络是数学上通用的函数逼近器，可以学习任何函数。此外，隐藏层通常被解释为网络学习过程的中间步骤，而无需人工编写这些中间步骤。这可以来自计算机视觉中的卷积神经网络，在那里很容易看到神经网络如何推断出每一层。

这些事实构成了有趣的心智图像，并且可以应用于其他估计器。许多人往往不会把随机森林看作是树在逐步推理的过程，或者说是树与树之间的推理（也许是因为它们的结构不如有组织，而随机森林不会让人联想到生物大脑的可视化）。在更实际的细节上，如果你想组织随机森林，你可以限制它们的深度，或者使用梯度提升机。

无论神经网络是否真正智能这一事实如何，随着领域的进展和机器变得越来越聪明，携带这样的心智图像是有帮助的。携带这个想法，但要专注于结果；这就是现在机器学习的意义。

# 使用神经网络进行堆叠

两种最常见的元学习方法是袋装法和提升法。堆叠法使用得较少；然而，它非常强大，因为可以将不同类型的模型结合起来。这三种方法都通过一组较弱的估计器创建了一个更强的估计器。我们在[第九章](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml)，*树算法与集成方法*中尝试了堆叠过程。在这里，我们尝试将神经网络与其他模型结合。

堆叠的过程如下：

1.  将数据集划分为训练集和测试集。

1.  将训练集划分为两部分。

1.  在训练集的第一部分上训练基础学习器。

1.  使用基础学习器对训练集第二部分进行预测。存储这些预测向量。

1.  将存储的预测向量作为输入，目标变量作为输出。训练一个更高层次的学习器（注意我们仍然处于训练集的第二部分）。

之后，你可以查看在测试集上整体过程的结果（注意，不能通过查看测试集上的结果来选择模型）。

# 准备就绪

导入加利福尼亚州住房数据集以及我们一直使用的库：`numpy`、`pandas`和`matplotlib`。这是一个中等大小的数据集，但相对于其他scikit-learn数据集来说，它还是比较大的：

[PRE16]

将目标变量进行分箱，以提高数据集在目标变量上的平衡性：

[PRE17]

将数据集`X`和`y`划分为三个集合。`X_1`和`X_stack`分别表示第一个和第二个训练集的输入变量，`y_1`和`y_stack`分别表示第一个和第二个训练集的输出目标变量。测试集由`X_test_prin`和`y_test_prin`组成：

[PRE18]

另一个选择是使用来自scikit-learn的`model_selection`模块中的`StratifiedShuffleSplit`。

# 如何实现...

我们将使用三个基础回归器：一个神经网络，一个单一的梯度提升机，和一个梯度提升机的袋装集成。

# 第一个基础模型 - 神经网络

1.  通过对第一个训练集进行交叉验证的网格搜索，添加一个神经网络：`X_1`为输入，`y_1`为目标集合。这将找到该数据集的最佳神经网络参数。在此示例中，我们仅调整`alpha`参数。不要忘记对输入进行标准化，否则网络将无法很好地运行：

[PRE19]

1.  查看网格搜索的最佳参数和最佳得分：

[PRE20]

1.  持久化在网格搜索中表现最好的神经网络。这将保存我们已经完成的训练，以免我们不得不反复进行训练：

[PRE21]

# 第二个基础模型 - 梯度提升集成

1.  对梯度增强树进行随机网格搜索：

[PRE22]

1.  查看最佳得分和参数：

[PRE23]

1.  增加估算器的数量并训练：

[PRE24]

1.  对估算器进行持久化。为了方便和可重用性，持久化的代码被封装成一个函数：

[PRE25]

# 第三个基础模型 - 梯度提升集成的袋装回归器

1.  现在，进行一个小规模的网格搜索，尝试一组梯度增强树的袋装。理论上很难判断这种集成方法是否会表现良好。对于堆叠而言，只要它与其他基础估算器的相关性不太高，它就足够好：

[PRE26]

1.  查看最佳参数和评分：

[PRE27]

1.  持久化最佳估算器：

[PRE28]

# 堆叠器的一些函数

1.  使用类似于[第9章](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml)的函数，*树算法与集成方法*。`handle_X_set`函数在`X_stack`集合上创建预测向量的数据框。概念上，它指的是对训练集第二部分进行预测的第四步：

[PRE29]

1.  如果你之前已经持久化了文件，并希望从这一步开始，解持久化文件。以下文件使用正确的文件名和变量名加载，以执行`handle_X_set`函数：

[PRE30]

1.  使用`handle_X_set`函数创建预测数据框。打印预测向量之间的皮尔逊相关性：

[PRE31]

# 元学习器 – 额外树回归器

1.  类似于[第9章](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml)，*树算法与集成方法*，在预测数据框上训练一个额外树回归器。使用`y_stack`作为目标向量：

[PRE32]

1.  查看最佳参数：

[PRE33]

1.  训练额外树回归器，但增加估计器的数量：

[PRE34]

1.  查看`final_etr`估计器的交叉验证性能：

[PRE35]

1.  查看测试集上的性能：

[PRE36]

1.  或许我们可以进一步提高结果。将训练列与预测向量并排放置。首先修改我们一直在使用的函数：

[PRE37]

1.  按照之前的方式继续训练额外树回归器：

[PRE38]

1.  我们继续按之前的方式进行。查看最佳参数，并使用更多的估计器训练模型：

[PRE39]

1.  查看交叉验证性能：

[PRE40]

1.  我们在堆叠器的高级学习器训练中包含了原始输入列。交叉验证性能从0.8221提高到0.8297。因此，我们得出结论，包含输入列的模型是最优模型。现在，在我们选择这个模型作为最终最佳模型后，我们查看估计器在测试集上的表现：

[PRE41]

# 还有更多...

在尝试过scikit-learn中的神经网络后，你可以尝试像`skflow`这样的包，它借用了scikit-learn的语法，但利用了谷歌强大的开源TensorFlow。

关于堆叠，你可以尝试在整个训练集`X_train_prin`上进行交叉验证性能评估和预测，而不是将其分割成两部分`X_1`和`X_stack`。

数据科学中的许多包都大量借鉴了scikit-learn或R的语法。

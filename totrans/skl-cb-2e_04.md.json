["```py\nfrom sklearn import datasets boston = datasets.load_boston()\n```", "```py\nfrom sklearn.linear_model import LinearRegression lr = LinearRegression()\n```", "```py\nlr.fit(boston.data, boston.target)\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n```", "```py\npredictions = lr.predict(boston.data)\n```", "```py\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt  #within an Ipython notebook: \n%matplotlib inline \n\npd.Series(boston.target - predictions).hist(bins=50)\n```", "```py\nlr.coef_\n\narray([ -1.07170557e-01,   4.63952195e-02,   2.08602395e-02,\n         2.68856140e+00,  -1.77957587e+01,   3.80475246e+00,\n         7.51061703e-04,  -1.47575880e+00,   3.05655038e-01,\n        -1.23293463e-02,  -9.53463555e-01,   9.39251272e-03,\n        -5.25466633e-01])\n```", "```py\nlr.intercept_\n\n36.491103280361955\n```", "```py\n[... #partial output due to length\n 'coef_',\n 'copy_X',\n 'decision_function',\n 'fit',\n 'fit_intercept',\n 'get_params',\n 'intercept_',\n 'n_jobs',\n 'normalize',\n 'predict',\n 'rank_',\n 'residues_',\n 'score',\n 'set_params',\n 'singular_']\n```", "```py\nlr2 = LinearRegression(normalize=True)\nlr2.fit(boston.data, boston.target)\nLinearRegression(copy_X=True, fit_intercept=True, normalize=True) \npredictions2 = lr2.predict(boston.data)\n```", "```py\nfrom sklearn import datasets\nboston = datasets.load_boston()\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n```", "```py\nlr.fit(boston.data, boston.target)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n```", "```py\nfrom sklearn.model_selection import cross_val_predict\n\npredictions_cv = cross_val_predict(lr, boston.data, boston.target, cv=10)\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#within Ipython\n%matplotlib inline \n\npd.Series(boston.target - predictions_cv).hist(bins=50)\n\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n```", "```py\nfrom scipy.stats import probplot\nf = plt.figure(figsize=(7, 5))\nax = f.add_subplot(111)\ntuple_out = probplot(boston.target - predictions_cv, plot=ax)\n```", "```py\n(4.4568597454452306, -2.9208080837569337e-15, 0.94762914118318298)\n```", "```py\ndef MSE(target, predictions):\n squared_deviation = np.power(target - predictions, 2)\n return np.mean(squared_deviation)\n\nMSE(boston.target, predictions)\n\n21.897779217687503\n\ndef MAD(target, predictions):\n absolute_deviation = np.abs(target - predictions)\n return np.mean(absolute_deviation)\n\nMAD(boston.target, predictions)\n\n3.2729446379969205\n```", "```py\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nprint 'MAE: ', mean_absolute_error(boston.target, predictions)\nprint 'MSE: ', mean_squared_error(boston.target, predictions)\n\n 'MAE: ', 3.2729446379969205\n 'MSE: ', 21.897779217687503\n```", "```py\nn_bootstraps = 1000\nlen_boston = len(boston.target)\nsubsample_size = np.int(0.5*len_boston)\n\nsubsample = lambda: np.random.choice(np.arange(0, len_boston),size=subsample_size)\ncoefs = np.ones(n_bootstraps) #pre-allocate the space for the coefs\nfor i in range(n_bootstraps):\n subsample_idx = subsample()\n subsample_X = boston.data[subsample_idx]\n subsample_y = boston.target[subsample_idx]\n lr.fit(subsample_X, subsample_y)\n coefs[i] = lr.coef_[0]\n```", "```py\nimport matplotlib.pyplot as plt\nf = plt.figure(figsize=(7, 5))\nax = f.add_subplot(111)\nax.hist(coefs, bins=50)\nax.set_title(\"Histogram of the lr.coef_[0].\")\n```", "```py\nnp.percentile(coefs, [2.5, 97.5])\n\narray([-0.18497204,  0.03231267])\n```", "```py\nfrom sklearn.datasets import make_regression\nreg_data, reg_target = make_regression(n_samples=2000,n_features=3, effective_rank=2, noise=10)\n```", "```py\nimport numpy as np\nn_bootstraps = 1000\nlen_data = len(reg_data)\nsubsample_size = np.int(0.5*len_data)\nsubsample = lambda: np.random.choice(np.arange(0, len_data),size=subsample_size)\n\ncoefs = np.ones((n_bootstraps, 3))\nfor i in range(n_bootstraps):\n subsample_idx = subsample()\n subsample_X = reg_data[subsample_idx]\n subsample_y = reg_target[subsample_idx]\n lr.fit(subsample_X, subsample_y)\n coefs[i][0] = lr.coef_[0]\n coefs[i][1] = lr.coef_[1]\n coefs[i][2] = lr.coef_[2]\n```", "```py\nfrom sklearn.linear_model import Ridge\nr = Ridge()\nn_bootstraps = 1000\nlen_data = len(reg_data)\nsubsample_size = np.int(0.5*len_data)\nsubsample = lambda: np.random.choice(np.arange(0, len_data),size=subsample_size)\n\ncoefs_r = np.ones((n_bootstraps, 3))\nfor i in range(n_bootstraps):\n subsample_idx = subsample()\n subsample_X = reg_data[subsample_idx]\n subsample_y = reg_target[subsample_idx]\n r.fit(subsample_X, subsample_y)\n coefs_r[i][0] = r.coef_[0]\n coefs_r[i][1] = r.coef_[1]\n coefs_r[i][2] = r.coef_[2]\n```", "```py\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 5))\n\nax1 = plt.subplot(311, title ='Coef 0')\nax1.hist(coefs_r[:,0])\n\nax2 = plt.subplot(312,sharex=ax1, title ='Coef 1')\nax2.hist(coefs_r[:,1])\n\nax3 = plt.subplot(313,sharex=ax1, title ='Coef 2')\nax3.hist(coefs_r[:,2])\nplt.tight_layout()\n```", "```py\nnp.var(coefs, axis=0)\n\narray([ 228.91620444,  380.43369673,  297.21196544])\n```", "```py\nnp.var(coefs_r, axis=0) \n\narray([ 19.28079241,  15.53491973,  21.54126386])\n```", "```py\nfrom sklearn.datasets import make_regression\nreg_data, reg_target = make_regression(n_samples=100, n_features=2, effective_rank=1, noise=10)\n```", "```py\nfrom sklearn.linear_model import RidgeCV\nrcv = RidgeCV(alphas=np.array([.1, .2, .3, .4]))\nrcv.fit(reg_data, reg_target)\n```", "```py\nrcv.alpha_\n\n0.10000000000000001\n```", "```py\nrcv2 = RidgeCV(alphas=np.array([.08, .09, .1, .11, .12]))\nrcv2.fit(reg_data, reg_target)\n\nrcv2.alpha_\n\n0.080000000000000002\n```", "```py\nalphas_to_test = np.linspace(0.01, 1)\nrcv3 = RidgeCV(alphas=alphas_to_test, store_cv_values=True)\nrcv3.fit(reg_data, reg_target)\n```", "```py\nrcv3.cv_values_.shape\n\n(100L, 50L)\n```", "```py\nsmallest_idx = rcv3.cv_values_.mean(axis=0).argmin()\nalphas_to_test[smallest_idx]\n\n0.030204081632653063\n```", "```py\nrcv3.alpha_\n\n0.030204081632653063\n```", "```py\nplt.plot(alphas_to_test, rcv3.cv_values_.mean(axis=0))\n```", "```py\nfrom sklearn.metrics import mean_absolute_error\n```", "```py\nfrom sklearn.metrics import make_scorer\nMAD_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n```", "```py\nrcv4 = RidgeCV(alphas=alphas_to_test, store_cv_values=True, scoring=MAD_scorer)\nrcv4.fit(reg_data, reg_target)\nsmallest_idx = rcv4.cv_values_.mean(axis=0).argmin()\n```", "```py\nrcv4.cv_values_.mean(axis=0)[smallest_idx]\n\n-0.021805192650070034\n```", "```py\nalphas_to_test[smallest_idx]\n\n0.01\n```", "```py\nX = np.linspace(0, 5)\ny_truth = 3 * X\ny_noise = np.random.normal(0, 0.5, len(y_truth)) #normally distributed noise with mean 0 and spread 0.1\ny_noisy = (y_truth + y_noise)\n```", "```py\nfrom sklearn.linear_model import BayesianRidge\nbr_inst = BayesianRidge().fit(X.reshape(-1, 1), y_noisy)\n```", "```py\ny_pred, y_err = br_inst.predict(X.reshape(-1, 1), return_std=True)\n```", "```py\nplt.figure(figsize=(7, 5))\nplt.scatter(X, y_noisy)\nplt.title(\"Bayesian Ridge Line With Error Bars\")\nplt.errorbar(X, y_pred, y_err, color='green')\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import make_regression\nreg_data, reg_target = make_regression(n_samples=200, n_features=500, n_informative=5, noise=5)\n```", "```py\nfrom sklearn.linear_model import Lasso\nlasso = Lasso()\n```", "```py\nlasso.fit(reg_data, reg_target)\n```", "```py\nnp.sum(lasso.coef_ != 0)\n\n7\n\nlasso_0 = Lasso(0)\nlasso_0.fit(reg_data, reg_target)\nnp.sum(lasso_0.coef_ != 0)\n\n500\n```", "```py\nfrom sklearn.linear_model import LassoCV\nlassocv = LassoCV()\nlassocv.fit(reg_data, reg_target)\n```", "```py\n lassocv.alpha_\n\n0.75182924196508782\n```", "```py\nlassocv.coef_[:5]\n\narray([-0., -0.,  0.,  0., -0.])\n```", "```py\nnp.sum(lassocv.coef_ != 0)\n\n15\n```", "```py\nmask = lassocv.coef_ != 0\nnew_reg_data = reg_data[:, mask]\nnew_reg_data.shape\n\n(200L, 15L)\n```", "```py\nfrom sklearn.datasets import make_regression\nreg_data, reg_target = make_regression(n_samples=200, n_features=500, n_informative=10, noise=2)\n```", "```py\nfrom sklearn.linear_model import Lars\nlars = Lars(n_nonzero_coefs=10)\nlars.fit(reg_data, reg_target)\n```", "```py\nnp.sum(lars.coef_ != 0)\n\n 10\n```", "```py\ntrain_n = 100\nlars_12 = Lars(n_nonzero_coefs=12)\nlars_12.fit(reg_data[:train_n], reg_target[:train_n])\nlars_500 = Lars() # it's 500 by default\nlars_500.fit(reg_data[:train_n], reg_target[:train_n]);\n\nnp.mean(np.power(reg_target[train_n:] - lars_12.predict(reg_data[train_n:]), 2))\n```", "```py\n87.115080975821513\n\nnp.mean(np.power(reg_target[train_n:] - lars_500.predict(reg_data[train_n:]), 2))\n\n2.1212501492030518e+41\n```", "```py\nfrom sklearn.linear_model import LarsCV\nlcv = LarsCV()\nlcv.fit(reg_data, reg_target)\n```", "```py\nnp.sum(lcv.coef_ != 0)\n\n23\n```"]
["```py\n>>> import pandas as pd\n>>> df = pd.read_csv('data/SMSSpamCollection', delimiter='\\t', header=None)\n>>> print df.head()\n\n      0                                                  1\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...\n[5 rows x 2 columns]\n\n>>> print 'Number of spam messages:', df[df[0] == 'spam'][0].count()\n>>> print 'Number of ham messages:', df[df[0] == 'ham'][0].count()\n\nNumber of spam messages: 747\nNumber of ham messages: 4825\n```", "```py\nSpam: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005\\. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\nSpam: WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461\\. Claim code KL341\\. Valid 12 hours only.\nHam: Sorry my roommates took forever, it ok if I come by now?\nHam: Finished class where are you.\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> from sklearn.linear_model.logistic import LogisticRegression\n>>> from sklearn.cross_validation import train_test_split, cross_val_score\n```", "```py\n>>> df = pd.read_csv('data/SMSSpamCollection', delimiter='\\t', header=None)\n>>> X_train_raw, X_test_raw, y_train, y_test = train_test_split(df[1], df[0])\n```", "```py\n>>> vectorizer = TfidfVectorizer()\n>>> X_train = vectorizer.fit_transform(X_train_raw)\n>>> X_test = vectorizer.transform(X_test_raw)\n```", "```py\n>>> classifier = LogisticRegression()\n>>> classifier.fit(X_train, y_train)\n>>> predictions = classifier.predict(X_test)\n>>> for i, prediction in enumerate(predictions[:5]):\n>>>     print 'Prediction: %s. Message: %s' % (prediction, X_test_raw[i])\n```", "```py\nPrediction: ham. Message: If you don't respond imma assume you're still asleep and imma start calling n shit\nPrediction: spam. Message: HOT LIVE FANTASIES call now 08707500020 Just 20p per min NTT Ltd, PO Box 1327 Croydon CR9 5WB 0870 is a national rate call\nPrediction: ham. Message: Yup... I havent been there before... You want to go for the yoga? I can call up to book \nPrediction: ham. Message: Hi, can i please get a  &lt;#&gt;  dollar loan from you. I.ll pay you back by mid february. Pls.\nPrediction: ham. Message: Where do you need to go to get it?\n```", "```py\n>>> from sklearn.metrics import confusion_matrix\n>>> import matplotlib.pyplot as plt\n\n>>> y_test = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n>>> y_pred = [0, 1, 0, 0, 0, 0, 0, 1, 1, 1]\n>>> confusion_matrix = confusion_matrix(y_test, y_pred)\n>>> print(confusion_matrix)\n>>> plt.matshow(confusion_matrix)\n>>> plt.title('Confusion matrix')\n>>> plt.colorbar()\n>>> plt.ylabel('True label')\n>>> plt.xlabel('Predicted label')\n>>> plt.show()\n\n [[4 1]\n [2 3]]\n```", "```py\n>>> from sklearn.metrics import accuracy_score\n>>> y_pred, y_true = [0, 1, 1, 0], [1, 1, 1, 1]\n>>> print 'Accuracy:', accuracy_score(y_true, y_pred)\n\nAccuracy: 0.5\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> from sklearn.linear_model.logistic import LogisticRegression\n>>> from sklearn.cross_validation import train_test_split, cross_val_score\n>>> df = pd.read_csv('data/sms.csv')\n>>> X_train_raw, X_test_raw, y_train, y_test = train_test_split(df['message'], df['label'])\n>>> vectorizer = TfidfVectorizer()\n>>> X_train = vectorizer.fit_transform(X_train_raw)\n>>> X_test = vectorizer.transform(X_test_raw)\n>>> classifier = LogisticRegression()\n>>> classifier.fit(X_train, y_train)\n>>> scores = cross_val_score(classifier, X_train, y_train, cv=5)\n>>> print np.mean(scores), scores\n\nAccuracy 0.956217208018 [ 0.96057348  0.95334928  0.96411483  0.95454545  0.94850299]\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> from sklearn.linear_model.logistic import LogisticRegression\n>>> from sklearn.cross_validation import train_test_split, cross_val_score\n>>> df = pd.read_csv('data/sms.csv')\n>>> X_train_raw, X_test_raw, y_train, y_test = train_test_split(df['message'], df['label'])\n>>> vectorizer = TfidfVectorizer()\n>>> X_train = vectorizer.fit_transform(X_train_raw)\n>>> X_test = vectorizer.transform(X_test_raw)\n>>> classifier = LogisticRegression()\n>>> classifier.fit(X_train, y_train)\n>>> precisions = cross_val_score(classifier, X_train, y_train, cv=5, scoring='precision')\n>>> print 'Precision', np.mean(precisions), precisions\n>>> recalls = cross_val_score(classifier, X_train, y_train, cv=5, scoring='recall')\n>>> print 'Recalls', np.mean(recalls), recalls\n\nPrecision 0.992137651822 [ 0.98717949  0.98666667  1\\.          0.98684211  1\\.        ]\nRecall 0.677114261885 [ 0.7         0.67272727  0.6         0.68807339  0.72477064]\n```", "```py\n>>> f1s = cross_val_score(classifier, X_train, y_train, cv=5, scoring='f1')\n>>> print 'F1', np.mean(f1s), f1s\n\nF1 0.80261302628 [ 0.82539683  0.8         0.77348066  0.83157895  0.7826087 ]\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> from sklearn.linear_model.logistic import LogisticRegression\n>>> from sklearn.cross_validation import train_test_split, cross_val_score\n>>> from sklearn.metrics import roc_curve, auc\n>>> df = pd.read_csv('data/sms.csv')\n>>> X_train_raw, X_test_raw, y_train, y_test = train_test_split(df['message'], df['label'])\n>>> vectorizer = TfidfVectorizer()\n>>> X_train = vectorizer.fit_transform(X_train_raw)\n>>> X_test = vectorizer.transform(X_test_raw)\n>>> classifier = LogisticRegression()\n>>> classifier.fit(X_train, y_train)\n>>> predictions = classifier.predict_proba(X_test)\n>>> false_positive_rate, recall, thresholds = roc_curve(y_test, predictions[:, 1])\n>>> roc_auc = auc(false_positive_rate, recall)\n>>> plt.title('Receiver Operating Characteristic')\n>>> plt.plot(false_positive_rate, recall, 'b', label='AUC = %0.2f' % roc_auc)\n>>> plt.legend(loc='lower right')\n>>> plt.plot([0, 1], [0, 1], 'r--')\n>>> plt.xlim([0.0, 1.0])\n>>> plt.ylim([0.0, 1.0])\n>>> plt.ylabel('Recall')\n>>> plt.xlabel('Fall-out')\n>>> plt.show()\n```", "```py\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model.logistic import LogisticRegression\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\npipeline = Pipeline([\n    ('vect', TfidfVectorizer(stop_words='english')),\n    ('clf', LogisticRegression())\n])\nparameters = {\n    'vect__max_df': (0.25, 0.5, 0.75),\n    'vect__stop_words': ('english', None),\n    'vect__max_features': (2500, 5000, 10000, None),\n    'vect__ngram_range': ((1, 1), (1, 2)),\n    'vect__use_idf': (True, False),\n    'vect__norm': ('l1', 'l2'),\n    'clf__penalty': ('l1', 'l2'),\n    'clf__C': (0.01, 0.1, 1, 10),\n}\n```", "```py\nif __name__ == \"__main__\":\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=3)\n    df = pd.read_csv('data/sms.csv')\n    X, y, = df['message'], df['label']\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    grid_search.fit(X_train, y_train)\n    print 'Best score: %0.3f' % grid_search.best_score_\n    print 'Best parameters set:'\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print '\\t%s: %r' % (param_name, best_parameters[param_name])\n    predictions = grid_search.predict(X_test)\n    print 'Accuracy:', accuracy_score(y_test, predictions)\n    print 'Precision:', precision_score(y_test, predictions)\n    print 'Recall:', recall_score(y_test, predictions)\n```", "```py\nFitting 3 folds for each of 1536 candidates, totalling 4608 fits\n[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.2s\n[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    4.0s\n[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   16.9s\n[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:   36.7s\n[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  1.1min\n[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  1.7min\n[Parallel(n_jobs=-1)]: Done 1800 jobs       | elapsed:  2.5min\n[Parallel(n_jobs=-1)]: Done 2450 jobs       | elapsed:  3.4min\n[Parallel(n_jobs=-1)]: Done 3200 jobs       | elapsed:  4.4min\n[Parallel(n_jobs=-1)]: Done 4050 jobs       | elapsed:  7.7min\n[Parallel(n_jobs=-1)]: Done 4608 out of 4608 | elapsed:  8.5min finished\nBest score: 0.983\nBest parameters set:\n  clf__C: 10\n  clf__penalty: 'l2'\n  vect__max_df: 0.5\n  vect__max_features: None\n  vect__ngram_range: (1, 2)\n  vect__norm: 'l2'\n  vect__stop_words: None\n  vect__use_idf: True\nAccuracy: 0.989956958393\nPrecision: 0.988095238095\nRecall: 0.932584269663\n```", "```py\n>>> import pandas as pd\n>>> df = pd.read_csv('movie-reviews/train.tsv', header=0, delimiter='\\t')\n>>> print df.count()\n\nPhraseId      156060\nSentenceId    156060\nPhrase        156060\nSentiment     156060\ndtype: int64\n```", "```py\n>>> print df.head()\n\n   PhraseId  SentenceId                                             Phrase  \\\n0         1           1  A series of escapades demonstrating the adage ...\n1         2           1  A series of escapades demonstrating the adage ...\n2         3           1                                           A series\n3         4           1                                                  A\n4         5           1                                             series\n\n   Sentiment\n0          1\n1          2\n2          2\n3          2\n4          2\n\n[5 rows x 4 columns]\n```", "```py\n>>> print df['Phrase'].head(10)\n\n0    A series of escapades demonstrating the adage ...\n1    A series of escapades demonstrating the adage ...\n2                                             A series\n3                                                    A\n4                                               series\n5    of escapades demonstrating the adage that what...\n6                                                   of\n7    escapades demonstrating the adage that what is...\n8                                            escapades\n9    demonstrating the adage that what is good for ...\nName: Phrase, dtype: object\n```", "```py\n>>> print df['Sentiment'].describe()\n\ncount    156060.000000\nmean          2.063578\nstd           0.893832\nmin           0.000000\n25%           2.000000\n50%           2.000000\n75%           3.000000\nmax           4.000000\nName: Sentiment, dtype: float64\n\n>>> print df['Sentiment'].value_counts()\n\n2    79582\n3    32927\n1    27273\n4     9206\n0     7072\ndtype: int64\n\n>>> print df['Sentiment'].value_counts()/df['Sentiment'].count()\n\n2    0.509945\n3    0.210989\n1    0.174760\n4    0.058990\n0    0.045316\ndtype: float64\n```", "```py\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model.logistic import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\n\ndef main():\n    pipeline = Pipeline([\n        ('vect', TfidfVectorizer(stop_words='english')),\n        ('clf', LogisticRegression())\n    ])\n    parameters = {\n        'vect__max_df': (0.25, 0.5),\n        'vect__ngram_range': ((1, 1), (1, 2)),\n        'vect__use_idf': (True, False),\n        'clf__C': (0.1, 1, 10),\n    }\n    df = pd.read_csv('data/train.tsv', header=0, delimiter='\\t')\n    X, y = df['Phrase'], df['Sentiment'].as_matrix()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=3, verbose=1, scoring='accuracy')\n    grid_search.fit(X_train, y_train)\n    print 'Best score: %0.3f' % grid_search.best_score_\n    print 'Best parameters set:'\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print '\\t%s: %r' % (param_name, best_parameters[param_name])\n\nif __name__ == '__main__':\n    main()\n```", "```py\nFitting 3 folds for each of 24 candidates, totalling 72 fits\n[Parallel(n_jobs=3)]: Done   1 jobs       | elapsed:    3.3s\n[Parallel(n_jobs=3)]: Done  50 jobs       | elapsed:  1.1min\n[Parallel(n_jobs=3)]: Done  68 out of  72 | elapsed:  1.9min remaining:    6.8s\n[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:  2.1min finished\nBest score: 0.620\nBest parameters set:\n  clf__C: 10\n  vect__max_df: 0.25\n  vect__ngram_range: (1, 2)\n  vect__use_idf: False\n```", "```py\n    predictions = grid_search.predict(X_test)\n    print 'Accuracy:', accuracy_score(y_test, predictions)\n    print 'Confusion Matrix:', confusion_matrix(y_test, predictions)\n    print 'Classification Report:', classification_report(y_test, predictions)\n```", "```py\nAccuracy: 0.636370626682\nConfusion Matrix: [[ 1129  1679   634    64     9]\n [  917  6121  6084   505    35]\n [  229  3091 32688  3614   166]\n [   34   408  6734  8068  1299]\n [    5    35   494  2338  1650]]\nClassification Report:              precision    recall  f1-score   support\n\n          0       0.49      0.32      0.39      3515\n          1       0.54      0.45      0.49     13662\n          2       0.70      0.82      0.76     39788\n          3       0.55      0.49      0.52     16543\n          4       0.52      0.36      0.43      4522\n\navg / total       0.62      0.64      0.62     78030\n```", "```py\n>>> import numpy as np\n>>> from sklearn.metrics import hamming_loss\n>>> print hamming_loss(np.array([[0.0, 1.0], [1.0, 1.0]]), np.array([[0.0, 1.0], [1.0, 1.0]]))\n0.0\n>>> print hamming_loss(np.array([[0.0, 1.0], [1.0, 1.0]]), np.array([[1.0, 1.0], [1.0, 1.0]]))\n0.25\n>>> print hamming_loss(np.array([[0.0, 1.0], [1.0, 1.0]]), np.array([[1.0, 1.0], [0.0, 1.0]]))\n0.5\n>>> print jaccard_similarity_score(np.array([[0.0, 1.0], [1.0, 1.0]]), np.array([[0.0, 1.0], [1.0, 1.0]]))\n1.0\n>>> print jaccard_similarity_score(np.array([[0.0, 1.0], [1.0, 1.0]]), np.array([[1.0, 1.0], [1.0, 1.0]]))\n0.75\n>>> print jaccard_similarity_score(np.array([[0.0, 1.0], [1.0, 1.0]]), np.array([[1.0, 1.0], [0.0, 1.0]]))\n0.5\n```"]
<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer246">
    <h1 class="chapterNumber">6</h1>
    <h1 class="chapterTitle" id="_idParaDest-133">Predicting Stock Prices with Artificial Neural Networks</h1>
    <p class="normal">Continuing the same project of stock price prediction from the last chapter, in this chapter, we will introduce and explain neural network models in depth. We will start by building the simplest neural network and go deeper by adding more computational units to it. We will cover neural network building blocks and other important concepts, including activation functions, feedforward, and backpropagation. We will also implement neural networks from scratch with scikit-learn, TensorFlow, and PyTorch. We will pay attention to how to learn with neural networks efficiently without overfitting, utilizing dropout and early stopping techniques. Finally, we will train a neural network to predict stock prices and see whether it can beat what we achieved with the three regression algorithms in the previous chapter.</p>
    <p class="normal">We will cover the following topics in this chapter:</p>
    <ul>
      <li class="bulletList">Demystifying neural networks</li>
      <li class="bulletList">Building neural networks</li>
      <li class="bulletList">Picking the right activation functions</li>
      <li class="bulletList">Preventing overfitting in neural networks</li>
      <li class="bulletList">Predicting stock prices with neural networks</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-134">Demystifying neural networks</h1>
    <p class="normal">Here comes <a id="_idIndexMarker558"/>probably the most frequently mentioned model in the media, <strong class="keyWord">Artificial Neural Networks</strong> (<strong class="keyWord">ANNs</strong>); more<a id="_idIndexMarker559"/> often, we just call them <strong class="keyWord">neural networks</strong>. Interestingly, the neural network has been (falsely) considered equivalent to machine learning or artificial intelligence by the general public.</p>
    <div class="note">
      <p class="normal">An ANN is just one type of algorithm among many in machine learning, and machine learning is a branch of artificial intelligence. It is one of the ways we achieve <strong class="keyWord">Artificial General Intelligence</strong> (<strong class="keyWord">AGI</strong>), which<a id="_idIndexMarker560"/> is a hypothetical type of AI that can think, learn, and solve problems like a human.</p>
    </div>
    <p class="normal">Regardless, it is one of the most important machine learning models and has been rapidly evolving <a id="_idIndexMarker561"/>along with the <a id="_idIndexMarker562"/>revolution of <strong class="keyWord">Deep Learning</strong> (<strong class="keyWord">DL</strong>).</p>
    <p class="normal">Let’s first understand how neural networks work.</p>
    <h2 class="heading-2" id="_idParaDest-135">Starting with a single-layer neural network</h2>
    <p class="normal">We start by <a id="_idIndexMarker563"/>explaining different layers in a network, then move on to <a id="_idIndexMarker564"/>the activation function, and finally, training a network with backpropagation.</p>
    <h3 class="heading-3" id="_idParaDest-136">Layers in neural networks</h3>
    <p class="normal">A simple neural <a id="_idIndexMarker565"/>network is composed of three layers—the <strong class="keyWord">input layer</strong>, <strong class="keyWord">hidden layer</strong>, and <strong class="keyWord">output layer— a</strong>s shown in the following diagram:</p>
    <figure class="mediaobject"><img alt="A diagram of a network  Description automatically generated with medium confidence" src="../Images/B21047_06_01.png"/></figure>
    <p class="packt_figref">Figure 6.1: A simple shallow neural network</p>
    <p class="normal">A <strong class="keyWord">layer</strong> is a<a id="_idIndexMarker566"/> conceptual<a id="_idIndexMarker567"/> collection of <strong class="keyWord">nodes</strong> (also called <strong class="keyWord">units</strong>), which<a id="_idIndexMarker568"/> simulate <a id="_idIndexMarker569"/>neurons in a biological brain. The input layer represents the input features, <strong class="keyWord">x</strong>, and each node is a predictive feature, <em class="italic">x</em>. The output layer represents the target variable(s).</p>
    <p class="normal">In binary classification, the output layer contains only one node, whose value is the probability of the positive class. In multiclass classification, the output layer consists of <em class="italic">n</em> nodes, where <em class="italic">n</em> is the number of possible classes and the value of each node is the probability of predicting that class. In regression, the output layer contains only one node, the value of which is the prediction result.</p>
    <p class="normal">The hidden layer can be considered a composition of latent information extracted from the previous layer. There can be more than one hidden layer. Learning with a neural network with two or more hidden layers is<a id="_idIndexMarker570"/> called <strong class="keyWord">deep learning</strong>. In this chapter, we will focus on one hidden layer to begin with.</p>
    <p class="normal">Two adjacent layers are connected by conceptual edges (sort of like the synapses in a biological brain), which transmit signals from one neuron in a layer to another neuron in the next layer. The <strong class="keyWord">edges</strong> are<a id="_idIndexMarker571"/> parameterized by the weights, <em class="italic">W</em>, of the model. For example, <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">(1)</sup> in the preceding diagram connects the input and hidden layers and <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">(2)</sup> connects the hidden and output layers.</p>
    <p class="normal">In a standard neural network, data is conveyed only from the input layer to the output layer, through a hidden layer(s). Hence, this kind of network is called<a id="_idIndexMarker572"/> a <strong class="keyWord">feedforward</strong> neural network. Basically, logistic regression is a feedforward neural network with no hidden layer where the<a id="_idIndexMarker573"/> output layer connects directly with the input layer. Adding hidden layers between the input and output layers introduces non-linearity. This allows the neural networks to learn more about the underlying relationship between the input data and the target.</p>
    <h2 class="heading-2" id="_idParaDest-137">Activation functions</h2>
    <p class="normal">An <strong class="keyWord">activation function</strong> is<a id="_idIndexMarker574"/> a mathematical operation <a id="_idIndexMarker575"/>applied to the output of each neuron in a neural network. It determines whether the neuron should be activated (i.e., its output value should be propagated forward to the next layer) based on the input it receives.</p>
    <p class="normal">Suppose the input, <em class="italic">x</em>, is of <em class="italic">n</em> dimensions, and the hidden layer is composed of <em class="italic">H</em> hidden units. The weight matrix, <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">(1)</sup>, connecting the input and hidden layers is of size <em class="italic">n</em> by <em class="italic">H</em>, where each column, <img alt="" role="presentation" src="../Images/B21047_06_001.png"/>, represents the coefficients associating the input with the <em class="italic">h</em>-th hidden unit. The output (also<a id="_idIndexMarker576"/> called <strong class="keyWord">activation</strong>) of the hidden layer can be expressed mathematically as follows:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_002.png"/></p>
    <p class="normal">Here, <em class="italic">f(z)</em> is an activation function. As its name implies, the activation function checks how activated each neuron is, simulating the way our brains work. Their primary purpose is to introduce non-linearity into the output of a neuron, allowing the network to learn and perform complex mappings between inputs and outputs. Typical activation functions include the logistic function (more often <a id="_idIndexMarker577"/>called the <strong class="keyWord">sigmoid</strong> function in neural networks) and<a id="_idIndexMarker578"/> the <strong class="keyWord">tanh</strong> function, which is considered a rescaled version of the logistic function, as well as <strong class="keyWord">ReLU</strong> (short for <strong class="keyWord">Rectified Linear Unit</strong>), which<a id="_idIndexMarker579"/> is often used in DL:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_003.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_004.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_005.png"/></p>
    <p class="normal">We plot these three activation functions as follows:</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">logistic</strong> (<strong class="keyWord">sigmoid</strong>) function <a id="_idIndexMarker580"/>where the<a id="_idIndexMarker581"/> output <a id="_idIndexMarker582"/>value is in the range of (<code class="inlineCode">0, 1</code>):</li>
    </ul>
    <figure class="mediaobject"><img alt="A graph with a line  Description automatically generated" src="../Images/B21047_06_02.png"/></figure>
    <p class="packt_figref">Figure 6.2: The logistic function</p>
    <p class="normal-one">The visualization is produced by the following code:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> numpy </span><span class="hljs-con-keyword">as</span><span class="language-python"> np</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> matplotlib.pyplot </span><span class="hljs-con-keyword">as</span><span class="language-python"> plt</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">sigmoid</span><span class="language-python">(</span><span class="hljs-con-params">z</span><span class="language-python">):</span>
        return 1.0 / (1 + np.exp(-z))
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">z = np.linspace(-</span><span class="hljs-con-number">8</span><span class="language-python">, </span><span class="hljs-con-number">8</span><span class="language-python">, </span><span class="hljs-con-number">1000</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y = sigmoid(z)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.plot(z, y)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.xlabel(</span><span class="hljs-con-string">'z'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.ylabel(</span><span class="hljs-con-string">'y(z)'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.title(</span><span class="hljs-con-string">'logistic'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.grid()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <ul>
      <li class="bulletList">The <strong class="keyWord">tanh</strong> function <a id="_idIndexMarker583"/>plot where the<a id="_idIndexMarker584"/> output <a id="_idIndexMarker585"/>value is in the range of <code class="inlineCode">(-1, 1)</code>:</li>
    </ul>
    <figure class="mediaobject"><img alt="A graph with a line  Description automatically generated" src="../Images/B21047_06_03.png"/></figure>
    <p class="packt_figref">Figure 6.3: The tanh function</p>
    <p class="normal-one">The visualization is produced by the following code:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">tanh</span><span class="language-python">(</span><span class="hljs-con-params">z</span><span class="language-python">):</span>
        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">z = np.linspace(-</span><span class="hljs-con-number">8</span><span class="language-python">, </span><span class="hljs-con-number">8</span><span class="language-python">, </span><span class="hljs-con-number">1000</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y = tanh(z)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.plot(z, y)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.xlabel(</span><span class="hljs-con-string">'z'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.ylabel(</span><span class="hljs-con-string">'y(z)'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.title(</span><span class="hljs-con-string">'tanh'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.grid()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <ul>
      <li class="bulletList">The <strong class="keyWord">ReLU</strong> function <a id="_idIndexMarker586"/>plot where the<a id="_idIndexMarker587"/> output<a id="_idIndexMarker588"/> value is in the range of <code class="inlineCode">(0, +inf)</code>:</li>
    </ul>
    <figure class="mediaobject"><img alt="A graph with a line  Description automatically generated" src="../Images/B21047_06_04.png"/></figure>
    <p class="packt_figref">Figure 6.4: The ReLU function</p>
    <p class="normal-one">The visualization is produced by the following code:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">relu(z):</span>
        return np.maximum(np.zeros_like(z), z)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">z = np.linspace(-</span><span class="hljs-con-number">8</span><span class="language-python">, </span><span class="hljs-con-number">8</span><span class="language-python">, </span><span class="hljs-con-number">1000</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y = relu(z)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.plot(z, y)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.xlabel(</span><span class="hljs-con-string">'z'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.ylabel(</span><span class="hljs-con-string">'y(z)'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.title(</span><span class="hljs-con-string">'relu'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.grid()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">As for the output layer, let’s assume that there is one output unit (regression or binary classification) and that the weight matrix, <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">(2)</sup>, connecting the hidden layer to the output layer is of size <em class="italic">H</em> by <em class="italic">1</em>. In regression, the output can be expressed mathematically as follows (for consistency, I here denote it as <em class="italic">a</em><sup class="superscript-italic" style="font-style: italic;">(3)</sup> instead of <em class="italic">y</em>):</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_006.png"/></p>
    <p class="normal">The <strong class="keyWord">Universal Approximation Theorem</strong> is a key<a id="_idIndexMarker589"/> concept in understanding how neural networks enable learning. It states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function to arbitrary precision, given a sufficiently large number of neurons in the hidden layer. During the training process, the neural network learns to approximate the target function by adjusting its parameters (weights). This is typically done using optimization <a id="_idIndexMarker590"/>algorithms, such as gradient descent, which <a id="_idIndexMarker591"/>iteratively update the parameters to minimize the difference between the predicted outputs and the true targets. Let’s see this process in detail in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-138">Backpropagation</h2>
    <p class="normal">So, how can<a id="_idIndexMarker592"/> we<a id="_idIndexMarker593"/> obtain the optimal weights, <em class="italic">W = {W(1), W(2)}</em>, of the model? Similar to logistic regression, we can learn all weights using gradient descent with the goal of <a id="_idIndexMarker594"/>minimizing the <strong class="keyWord">mean squared error</strong> (<strong class="keyWord">MSE</strong>) cost or other loss function, <em class="italic">J(W)</em>. The difference is that the gradients, <img alt="" role="presentation" src="../Images/B21047_06_007.png"/>, are computed through <strong class="keyWord">backpropagation</strong>. After each forward pass through a network, a backward pass is performed to adjust the model’s parameters.</p>
    <p class="normal">As the word <em class="italic">back</em> in the name implies, the computation of the gradient proceeds backward: the gradient of the final layer is computed first and the gradient of the first layer is computed last. As for <em class="italic">propagation</em>, it means that partial computations of the gradient on one layer are reused in the computation of the gradient on the previous layer. Error information is propagated layer by layer, instead of being calculated separately.</p>
    <p class="normal">In a single-layer network, the detailed steps of backpropagation are as follows:</p>
    <ol>
      <li class="numberedList" value="1">We travel through the network from the input to the output and compute the output values, <em class="italic">a</em><sup class="superscript-italic" style="font-style: italic;">(2)</sup>, of the hidden layer as well as the output layer, <em class="italic">a</em><sup class="superscript-italic" style="font-style: italic;">(3)</sup>. This is the feedforward step.</li>
      <li class="numberedList">For the last layer, we calculate the derivative of the cost function with regard to <a id="_idIndexMarker595"/>the <a id="_idIndexMarker596"/>input to the output layer:</li>
    </ol>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_008.png"/></p>
    <ol>
      <li class="numberedList" value="3">For the hidden layer, we compute the derivative of the cost function with regard to the input to the hidden layer:</li>
    </ol>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_009.png"/></p>
    <ol>
      <li class="numberedList" value="4">We compute the gradients by applying the <strong class="keyWord">chain rule</strong>:</li>
    </ol>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_010.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_011.png"/></p>
    <ol>
      <li class="numberedList" value="1">We update the weights with the computed gradients and learning rate <img alt="" role="presentation" src="../Images/B21047_06_012.png"/>:</li>
    </ol>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_013.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_06_014.png"/></p>
    <p class="normal-one">Here, <em class="italic">m</em> is the number of samples.</p>
    <p class="normal">We repeatedly update all the weights by taking these steps with the latest weights until the cost function converges or the model goes through enough iterations.</p>
    <div class="note">
      <p class="normal">The chain rule is a fundamental concept in calculus. It allows you to find the derivative of a composite function. You can read more in the mathematics course from Stanford University (<a href="https://mathematics.stanford.edu/events/chain-rule-calculus"><span class="url">https://mathematics.stanford.edu/events/chain-rule-calculus</span></a>), or the differential calculus course, <em class="italic">Module 6, Applications of Differentiation</em>, from MIT (<a href="https://ocw.mit.edu/courses/18-03sc-differential-equations-fall-2011/"><span class="url">https://ocw.mit.edu/courses/18-03sc-differential-equations-fall-2011/</span></a>).</p>
    </div>
    <p class="normal">This might not <a id="_idIndexMarker597"/>be easy to digest at first glance, so right after the <a id="_idIndexMarker598"/>next section, we will implement it from scratch, which will help you understand neural networks better.</p>
    <h2 class="heading-2" id="_idParaDest-139">Adding more layers to a neural network: DL</h2>
    <p class="normal">In real applications, a neural network <a id="_idIndexMarker599"/>usually comes with multiple hidden layers. That is how DL got its name—learning using neural networks with “stacked” hidden layers. An example of a DL model is as follows:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B21047_06_05.png"/></figure>
    <p class="packt_figref">Figure 6.5: A deep neural network</p>
    <p class="normal">In a stack of multiple hidden layers, the input of one hidden layer is the output of its previous layer, as you can see from <em class="italic">Figure 6.5</em>. Features (signals) are extracted from each hidden layer. Features <a id="_idIndexMarker600"/>from different layers represent patterns from different levels. Going beyond shallow neural networks (usually with only one hidden layer), a DL model (usually with two or more hidden layers) with the right network architectures and parameters can better learn complex non-linear relationships from data.</p>
    <p class="normal">Let’s see some typical applications of DL so that you will be more motivated to get started with upcoming DL projects.</p>
    <p class="normal"><strong class="keyWord">Computer vision</strong> is<a id="_idIndexMarker601"/> widely considered the area with massive breakthroughs in DL. You will learn more about this in <em class="chapterRef">Chapter 11</em>, <em class="italic">Categorizing Images of Clothing with Convolutional Neural Networks</em>, and <em class="chapterRef">Chapter 14</em>, <em class="italic">Building an Image Search Engine Using CLIP: A Multimodal Approach</em>. For now, here is a list of common applications in computer vision:</p>
    <ul>
      <li class="bulletList">Image recognition, such as face recognition and handwritten digit recognition. Handwritten digit recognition, along with the common evaluation dataset MNIST, has become a “Hello, World!” project in DL.</li>
      <li class="bulletList">Image-based search engines heavily utilize DL techniques in their image classification and image similarity encoding components.</li>
      <li class="bulletList">Machine vision, which is a critical part of autonomous vehicles, perceives camera views to make real-time decisions.</li>
      <li class="bulletList">Color restoration from black and white photos and art transfer that ingeniously blends two images of different styles. The artificial masterpieces in Google Arts &amp; Culture (<a href="https://artsandculture.google.com/"><span class="url">https://artsandculture.google.com/</span></a>) are impressive.</li>
      <li class="bulletList">Realistic image generation based on textual descriptions. This has applications in creating visual storytelling content and assisting in content creation for marketing and advertising.</li>
    </ul>
    <p class="normal"><strong class="keyWord">Natural Language Processing</strong> (<strong class="keyWord">NLP</strong>) is<a id="_idIndexMarker602"/> another field where you can see the dominant use of DL in its modern<a id="_idIndexMarker603"/> solutions. You will learn more about this in <em class="chapterRef">Chapter 12</em>, <em class="italic">Making Predictions with Sequences Using Recurrent Neural Networks, </em>and<em class="italic"> </em><em class="chapterRef">Chapter 13</em>, <em class="italic">Advancing Language Understanding and Generation with the Transformer Models</em>. But let’s quickly look at some examples now:</p>
    <ul>
      <li class="bulletList">Machine translation, where DL has dramatically improved accuracy and fluency, for example, the<a id="_idIndexMarker604"/> sentence-based <strong class="keyWord">Google Neural Machine Translation</strong> (<strong class="keyWord">GNMT</strong>) system.</li>
      <li class="bulletList">Text generation reproduces text by learning the intricate relationships between words in sentences and paragraphs with deep neural networks. You can become a virtual J. K. Rowling or Shakespeare if you train a model well on their works.</li>
      <li class="bulletList">Image captioning, also known as image-to-text, leverages deep neural networks to detect and recognize objects in images and “describe” those objects in a comprehensible sentence. It couples recent breakthroughs in computer vision and NLP. Examples can be found at <a href="https://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/"><span class="url">https://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/</span></a> (developed by Andrej Karpathy from Stanford University).</li>
      <li class="bulletList">In other common NLP tasks such as sentiment analysis and information retrieval and extraction, DL models have achieved state-of-the-art performance.</li>
      <li class="bulletList"><strong class="keyWord">Artificial Intelligence-Generated Content</strong> (<strong class="keyWord">AIGC</strong>) is one of the recent breakthroughs. It <a id="_idIndexMarker605"/>uses DL technologies to create or assist in creating various types of content, such as articles, product descriptions, music, images, and videos.</li>
    </ul>
    <p class="normal">Similar to shallow networks, we learn all the weights in a deep neural network using gradient descent with the goal of minimizing the MSE cost, <em class="italic">J(W)</em>. And gradients, <img alt="" role="presentation" src="../Images/B21047_06_007.png"/>, are computed through backpropagation. The difference is that we backpropagate more than one hidden layer. In the next section, we will implement neural networks by starting with <a id="_idIndexMarker606"/>shallow networks and then moving on to deep ones.</p>
    <h1 class="heading-1" id="_idParaDest-140">Building neural networks</h1>
    <p class="normal">This practical section <a id="_idIndexMarker607"/>will start with implementing a shallow network from scratch, followed by a deep network with two layers using scikit-learn. We will then implement a deep network with TensorFlow and PyTorch.</p>
    <h2 class="heading-2" id="_idParaDest-141">Implementing neural networks from scratch</h2>
    <p class="normal">To demonstrate<a id="_idIndexMarker608"/> how activation functions work, we will use sigmoid as the activation function in this example.</p>
    <p class="normal">We first define the <code class="inlineCode">sigmoid</code> function and its derivative function:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">sigmoid_derivative</span><span class="language-python">(</span><span class="hljs-con-params">z</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">return</span><span class="language-python"> sigmoid(z) * (</span><span class="hljs-con-number">1.0</span><span class="language-python"> - sigmoid(z))</span>
</code></pre>
    <p class="normal">You can derive the derivative yourself if you are interested in verifying it.</p>
    <p class="normal">We then define the training function, which takes in the training dataset, the number of units in the hidden layer (we will only use one hidden layer as an example), and the number of iterations:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">train</span><span class="language-python">(</span><span class="hljs-con-params">X, y, n_hidden, learning_rate, n_iter</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    m, n_input = X.shape</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    W1 = np.random.randn(n_input, n_hidden)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    b1 = np.zeros((</span><span class="hljs-con-number">1</span><span class="language-python">, n_hidden))</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    W2 = np.random.randn(n_hidden, </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    b2 = np.zeros((</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">))</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">1</span><span class="language-python">, n_iter+</span><span class="hljs-con-number">1</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        Z2 = np.matmul(X, W1) + b1</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        A2 = sigmoid(Z2)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        Z3 = np.matmul(A2, W2) + b2</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        A3 = Z3</span>
<span class="hljs-con-meta">...</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        dZ3 = A3 - y</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        dW2 = np.matmul(A2.T, dZ3)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        db2 = np.</span><span class="hljs-con-built_in">sum</span><span class="language-python">(dZ3, axis=</span><span class="hljs-con-number">0</span><span class="language-python">, keepdims=</span><span class="hljs-con-literal">True</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        dZ2 = np.matmul(dZ3, W2.T) * sigmoid_derivative(Z2)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        dW1 = np.matmul(X.T, dZ2)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        db1 = np.</span><span class="hljs-con-built_in">sum</span><span class="language-python">(dZ2, axis=</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        W2 = W2 - learning_rate * dW2 / m</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        b2 = b2 - learning_rate * db2 / m</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        W1 = W1 - learning_rate * dW1 / m</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        b1 = b1 - learning_rate * db1 / m</span>
<span class="hljs-con-meta">...</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        </span><span class="hljs-con-keyword">if</span><span class="language-python"> i % </span><span class="hljs-con-number">100</span><span class="language-python"> == </span><span class="hljs-con-number">0</span><span class="language-python">:</span>
<span class="hljs-con-meta">...</span> <span class="language-python">            cost = np.mean((y - A3) ** </span><span class="hljs-con-number">2</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">            </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">'Iteration %i, training loss: %f'</span><span class="language-python"> %</span>
                                                  (i, cost))
<span class="hljs-con-meta">...</span> <span class="language-python">    model = {</span><span class="hljs-con-string">'W1'</span><span class="language-python">: W1, </span><span class="hljs-con-string">'b1'</span><span class="language-python">: b1, </span><span class="hljs-con-string">'W2'</span><span class="language-python">: W2, </span><span class="hljs-con-string">'</span><span class="hljs-con-string">b2'</span><span class="language-python">: b2}</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">return</span><span class="language-python"> model</span>
</code></pre>
    <p class="normal">Note that besides weights, <em class="italic">W</em>, we also employ bias, <em class="italic">b</em>. Before training, we first randomly initialize weights and<a id="_idIndexMarker609"/> biases. In each iteration, we feed all layers of the network with the latest weights and biases, then calculate the gradients using the backpropagation algorithm, and finally, update the weights and biases with the resulting gradients. For training performance inspection, we print out the loss and the MSE for every 100 iterations.</p>
    <p class="normal">To test the model, we will use California house prices as the example dataset again. As a reminder, data normalization is usually recommended whenever gradient descent is used. Hence, we will standardize the input data by removing the mean and scaling to unit variance:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn </span><span class="hljs-con-keyword">import</span><span class="language-python"> datasets</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">housing = datasets.fetch_california_housing()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">num_test = </span><span class="hljs-con-number">10</span><span class="language-python"> </span><span class="hljs-con-comment"># the last 10 samples as testing set</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn </span><span class="hljs-con-keyword">import</span><span class="language-python"> preprocessing</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">scaler = preprocessing.StandardScaler()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_train = housing.data[:-num_test, :]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_train = scaler.fit_transform(X_train)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_train = housing.target[:-num_test].reshape(-</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_test = housing.data[-num_test:, :]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_test = scaler.transform(X_test)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_test = housing.target[-num_test:]</span>
</code></pre>
    <p class="normal">With the scaled<a id="_idIndexMarker610"/> dataset, we can now train a one-layer neural network with <code class="inlineCode">20</code> hidden units, a <code class="inlineCode">0.1</code> learning rate, and <code class="inlineCode">2000</code> iterations:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">n_hidden = </span><span class="hljs-con-number">20</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">learning_rate = </span><span class="hljs-con-number">0.1</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">n_iter = </span><span class="hljs-con-number">2000</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = train(X_train, y_train, n_hidden, learning_rate, n_iter)</span>
Iteration 100, training loss: 0.557636
Iteration 200, training loss: 0.519375
Iteration 300, training loss: 0.501025
Iteration 400, training loss: 0.487536
Iteration 500, training loss: 0.476553
Iteration 600, training loss: 0.467207
Iteration 700, training loss: 0.459076
Iteration 800, training loss: 0.451934
Iteration 900, training loss: 0.445621
Iteration 1000, training loss: 0.440013
Iteration 1100, training loss: 0.435024
Iteration 1200, training loss: 0.430558
Iteration 1300, training loss: 0.426541
Iteration 1400, training loss: 0.422920
Iteration 1500, training loss: 0.419653
Iteration 1600, training loss: 0.416706
Iteration 1700, training loss: 0.414049
Iteration 1800, training loss: 0.411657
Iteration 1900, training loss: 0.409502
Iteration 2000, training loss: 0.407555
</code></pre>
    <p class="normal">Then, we define a prediction function, which will take in a model and produce the regression results:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">predict</span><span class="language-python">(</span><span class="hljs-con-params">x, model</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    W1 = model[</span><span class="hljs-con-string">'W1'</span><span class="language-python">]</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    b1 = model[</span><span class="hljs-con-string">'b1'</span><span class="language-python">]</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    W2 = model[</span><span class="hljs-con-string">'W2'</span><span class="language-python">]</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    b2 = model[</span><span class="hljs-con-string">'b2'</span><span class="language-python">]</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    A2 = sigmoid(np.matmul(x, W1) + b1)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    A3 = np.matmul(A2, W2) + b2</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">return</span><span class="language-python"> A3</span>
</code></pre>
    <p class="normal">Finally, we apply the trained model on the testing set:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = predict(X_test, model)</span>
</code></pre>
    <p class="normal">Print out the predictions and their ground truths to compare them:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(predictions[:, </span><span class="hljs-con-number">0</span><span class="language-python">])</span>
[1.11805681 1.1387508  1.06071523 0.81930286 1.21311999 0.6199933 0.92885765 0.81967297 0.90882797 0.87857088]
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(y_test)</span>
[1.12  1.072 1.156 0.983 1.168 0.781 0.771 0.923 0.847 0.894]
</code></pre>
    <p class="normal">After successfully<a id="_idIndexMarker611"/> building a neural network model from scratch, we will move on to the implementation with scikit-learn.</p>
    <h2 class="heading-2" id="_idParaDest-142">Implementing neural networks with scikit-learn</h2>
    <p class="normal">We <a id="_idIndexMarker612"/>will<a id="_idIndexMarker613"/> utilize the <code class="inlineCode">MLPRegressor</code> class (<strong class="keyWord">MLP</strong> stands for <strong class="keyWord">multi-layer perceptron</strong>, a<a id="_idIndexMarker614"/> nickname for neural networks) to implement neural networks:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.neural_network </span><span class="hljs-con-keyword">import</span><span class="language-python"> MLPRegressor</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nn_scikit = MLPRegressor(hidden_layer_sizes=(</span><span class="hljs-con-number">16</span><span class="language-python">, </span><span class="hljs-con-number">8</span><span class="language-python">),</span>
                         activation='relu',
                         solver='adam',
                         learning_rate_init=0.001,
                         random_state=42,
                         max_iter=2000)
</code></pre>
    <p class="normal">The <code class="inlineCode">hidden_layer_sizes</code> hyperparameter represents the number of hidden neurons. In this example, the network contains two hidden layers with <code class="inlineCode">16</code> and <code class="inlineCode">8</code> nodes, respectively. ReLU activation is used.</p>
    <div class="note">
      <p class="normal">The Adam optimizer is a replacement for the stochastic gradient descent algorithm. It updates the gradients adaptively based on training data. For more information about Adam, check out the paper at <a href="https://arxiv.org/abs/1412.6980"><span class="url">https://arxiv.org/abs/1412.6980</span></a>.</p>
    </div>
    <p class="normal">We fit the neural network model on the training set and predict on the testing data:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nn_scikit.fit(X_train, y_train.ravel())</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = nn_scikit.predict(X_test)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(predictions)</span>
[1.19968791 1.2725324  1.30448323 0.88688675 1.18623612 0.72605956 0.87409406 0.85671201 0.93423154 0.94196305]
</code></pre>
    <p class="normal">And we calculate the MSE on the prediction:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.metrics </span><span class="hljs-con-keyword">import</span><span class="language-python"> mean_squared_error</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(mean_squared_error(y_test, predictions))</span>
0.010613171947751738
</code></pre>
    <p class="normal">We’ve implemented<a id="_idIndexMarker615"/> a neural network with<a id="_idIndexMarker616"/> scikit-learn. Let’s do so with TensorFlow in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-143">Implementing neural networks with TensorFlow</h2>
    <p class="normal">In TensorFlow 2.x, it is <a id="_idIndexMarker617"/>simple to initiate a deep<a id="_idIndexMarker618"/> neural network model using<a id="_idIndexMarker619"/> the Keras (<a href="https://keras.io/"><span class="url">https://keras.io/</span></a>) module. Let’s implement neural networks with TensorFlow by following these steps:</p>
    <ol>
      <li class="numberedList" value="1">First, we import the necessary modules and set a random seed, which is recommended for reproducible modeling:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> tensorflow </span><span class="hljs-con-keyword">as</span><span class="language-python"> tf</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> tensorflow </span><span class="hljs-con-keyword">import</span><span class="language-python"> keras</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tf.random.set_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Next, we create a Keras Sequential model by passing a list of layer instances to the constructor, including two fully connected hidden layers with <code class="inlineCode">16</code> nodes and <code class="inlineCode">8</code> nodes, respectively. And again, ReLU activation is used:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = keras.Sequential([</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    keras.layers.Dense(units=</span><span class="hljs-con-number">16</span><span class="language-python">, activation=</span><span class="hljs-con-string">'relu'</span><span class="language-python">),</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    keras.layers.Dense(units=</span><span class="hljs-con-number">8</span><span class="language-python">, activation=</span><span class="hljs-con-string">'relu'</span><span class="language-python">),</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    keras.layers.Dense(units=</span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">])</span>
</code></pre>
      </li>
      <li class="numberedList">We compile the model by using Adam as the optimizer with a learning rate of <code class="inlineCode">0.01</code> and MSE as the learning goal:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.</span><span class="hljs-con-built_in">compile</span><span class="language-python">(loss=</span><span class="hljs-con-string">'mean_squared_error'</span><span class="language-python">,</span>
<span class="hljs-con-meta">...</span> <span class="language-python">              optimizer=tf.keras.optimizers.Adam(</span><span class="hljs-con-number">0.01</span><span class="language-python">))</span>
</code></pre>
      </li>
      <li class="numberedList">After defining<a id="_idIndexMarker620"/> the model, we <a id="_idIndexMarker621"/>now train it against the training set:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.fit(X_train, y_train, epochs=</span><span class="hljs-con-number">300</span><span class="language-python">)</span>
Train on 496 samples
Epoch 1/300
645/645 [==============================] - 1s 1ms/step - loss: 0.6494
Epoch 2/300
645/645 [==============================] - 1s 1ms/step - loss: 0.3827
Epoch 3/300
645/645 [==============================] - 1s 1ms/step - loss: 0.3700
……
……
Epoch 298/300
645/645 [==============================] - 1s 1ms/step - loss: 0.2724
Epoch 299/300
645/645 [==============================] - 1s 1ms/step - loss: 0.2735
Epoch 300/300
645/645 [==============================] - 1s 1ms/step - loss: 0.2730
1/1 [==============================] - 0s 82ms/step
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We fit the model with <code class="inlineCode">300</code> iterations. In each iteration, the training loss (MSE) is displayed.</p>
    <ol>
      <li class="numberedList" value="5">Finally, we use the trained model to predict the testing cases and print out the predictions and their MSE:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = model.predict(X_test)[:, </span><span class="hljs-con-number">0</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(predictions)</span>
[1.2387774  1.2480505  1.229521   0.8988129  1.1932802  0.75052583 0.75052583 0.88086814 0.9921638  0.9107932 ]
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(mean_squared_error(y_test, predictions))</span>
0.008271122735361234
</code></pre>
      </li>
    </ol>
    <p class="normal">As you can see, we add layer by layer to the neural network model in the TensorFlow Keras API. We start from the first hidden layer (with 16 nodes), then the second hidden layer (with 8 nodes), and finally, the output layer (with 1 unit, the target variable). It is quite similar to building with LEGO.</p>
    <p class="normal">In the<a id="_idIndexMarker622"/> industry, neural networks are often <a id="_idIndexMarker623"/>implemented with PyTorch. Let’s see how to do it in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-144">Implementing neural networks with PyTorch</h2>
    <p class="normal">We will now implement<a id="_idIndexMarker624"/> neural networks with <a id="_idIndexMarker625"/>PyTorch by following these steps:</p>
    <ol>
      <li class="numberedList" value="1">First, we import the necessary modules and set a random seed, which is recommended for reproducible modeling:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch.nn </span><span class="hljs-con-keyword">as</span><span class="language-python"> nn</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Next, we create a <code class="inlineCode">torch.nn</code> Sequential model by passing a list of layer instances to the constructor, including two fully connected hidden layers with <code class="inlineCode">16</code> nodes and <code class="inlineCode">8</code> nodes, respectively. Again, ReLU activation is used in each fully connected layer:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = nn.Sequential(nn.Linear(X_train.shape[</span><span class="hljs-con-number">1</span><span class="language-python">], </span><span class="hljs-con-number">16</span><span class="language-python">),</span>
                      nn.ReLU(),
                      nn.Linear(16, 8),
                      nn.ReLU(),
                      nn.Linear(8, 1))
</code></pre>
      </li>
      <li class="numberedList">We initialize an Adam optimizer with a learning rate of <code class="inlineCode">0.01</code> and MSE as the learning goal:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">loss_function = nn.MSELoss()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">0.01</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">After defining the model, we need to create tensor objects from the input NumPy arrays before using them to train the PyTorch model:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_train_torch = torch.from_numpy(X_train.astype(np.float32))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_train_torch = torch.from_numpy(y_train.astype(np.float32))</span>
</code></pre>
      </li>
      <li class="numberedList">Now we can train the model against the PyTorch-compatible training set. We first define a training function that will be called in each epoch as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">train_step</span><span class="language-python">(</span><span class="hljs-con-params">model, X_train, y_train, loss_function, optimizer</span><span class="language-python">):</span>
        pred_train = model(X_train)
        loss = loss_function(pred_train, y_train)
      
        model.zero_grad()
        loss.backward()
        optimizer.step()
        return loss.item()
</code></pre>
      </li>
      <li class="numberedList">We fit the <a id="_idIndexMarker626"/>model with <code class="inlineCode">500</code> iterations. In<a id="_idIndexMarker627"/> every 100 iterations, the training loss (MSE) is displayed as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">500</span><span class="language-python">):</span>
        loss = train_step(model, X_train_torch, y_train_torch,
                          loss_function, optimizer)      
        if epoch % 100 == 0:
            print(f"Epoch {epoch} - loss: {loss}")
Epoch 0 - loss: 4.908532619476318
Epoch 100 - loss: 0.5002815127372742
Epoch 200 - loss: 0.40820521116256714
Epoch 300 - loss: 0.3870624303817749
Epoch 400 - loss: 0.3720889091491699
</code></pre>
      </li>
      <li class="numberedList">Finally, we use the trained model to predict the testing cases and print out the predictions and their MSE:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_test_torch = torch.from_numpy(X_test.astype(np.float32))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = model(X_test_torch).detach().numpy()[:, </span><span class="hljs-con-number">0</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(predictions)</span>
[1.171479  1.130001  1.1055213 0.8627995 1.0910968 0.6725116 0.8869568 0.8009699 0.8529027 0.8760005]
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(mean_squared_error(y_test, predictions))</span>
0.006939044434639928
</code></pre>
      </li>
    </ol>
    <p class="normal">It turns out that developing a neural network model with PyTorch is as simple as building with LEGO too.</p>
    <div class="note">
      <p class="normal">Both PyTorch and TensorFlow are popular deep learning frameworks, and their popularity can vary depending on different factors such as application domains, research communities, industry adoption, and personal preferences. However, as of 2023, PyTorch has been more widely adopted and has a larger user base overall, according to Papers With Code (<a href="https://paperswithcode.com/trends"><span class="url">https://paperswithcode.com/trends</span></a>) and Google Trends (<a href="https://trends.google.com/trends/explore?geo=US&amp;q=tensorflow,pytorch&amp;hl=en"><span class="url">https://trends.google.com/trends/explore?geo=US&amp;q=tensorflow,pytorch&amp;hl=en</span></a>). Hence, we will focus on PyTorch implementations for DL throughout the rest of the book.</p>
    </div>
    <p class="normal">Next, we<a id="_idIndexMarker628"/> will<a id="_idIndexMarker629"/> look at how to choose the right activation functions.</p>
    <h1 class="heading-1" id="_idParaDest-145">Picking the right activation functions</h1>
    <p class="normal">So far, we have used the<a id="_idIndexMarker630"/> ReLU and sigmoid activation functions in our implementations. You may wonder how to pick the right activation function for your neural networks. Detailed advice on when to choose a particular activation function is given next:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Linear</strong>: <em class="italic">f(z) = z</em>. You can interpret this as no activation function. We usually use it in the output layer in regression networks as we don’t need any transformation to the outputs.</li>
      <li class="bulletList"><strong class="keyWord">Sigmoid</strong> (logistic) transforms the output of a layer to a range between 0 and 1. You can interpret it as the probability of an output prediction. Therefore, we usually use it in the output layer<a id="_idIndexMarker631"/> in <strong class="keyWord">binary classification</strong> networks. Besides that, we sometimes use it in hidden layers. However, it should be noted that the sigmoid function is monotonic but its derivative is not. Hence, the neural network may get stuck at a suboptimal solution.</li>
      <li class="bulletList"><strong class="keyWord">Softmax</strong>: As was mentioned in <em class="chapterRef">Chapter 4</em>, <em class="italic">Predicting Online Ad Click-Through with Logistic Regression</em>, softmax is a generalized logistic function used for multiclass classification. Hence, we use it in the output<a id="_idIndexMarker632"/> layer in <strong class="keyWord">multiclass classification</strong> networks.</li>
      <li class="bulletList"><strong class="keyWord">Tanh</strong> is a better version of the sigmoid function with stronger gradients. As you can see in the plots earlier in the chapter, the derivatives in the tanh function are steeper than those for the sigmoid function. It has a range of <code class="inlineCode">-1</code> to <code class="inlineCode">1</code>. It is common to use the <code class="inlineCode">tanh</code> function in hidden layers.</li>
      <li class="bulletList"><strong class="keyWord">ReLU</strong> is probably the most frequently used activation function nowadays. It is the “default” one in hidden layers in feedforward networks. Its range is from <code class="inlineCode">0</code> to infinity, and<a id="_idIndexMarker633"/> both the function itself and its derivative are monotonic. It has several benefits over <code class="inlineCode">tanh</code>. One is sparsity, meaning that only a subset of neurons are activated at any given time. This can help reduce the computational cost of training and inference, as fewer neurons need to be computed. ReLU also mitigates the vanishing gradient problem, which occurs when gradients become very small during backpropagation, leading to slow or stalled learning. ReLU does not saturate for positive inputs, allowing gradients to flow more freely during training. One drawback of the ReLU function is the inability to appropriately map the negative part of the input where all negative inputs are transformed to 0. To fix the “dying negative” problem in ReLU, <strong class="keyWord">Leaky ReLU</strong> was <a id="_idIndexMarker634"/>invented to introduce a small slope in the negative part. When <em class="italic">z &lt; 0</em>, <em class="italic">f(z) = az</em>, where <em class="italic">a</em> is usually a small value, such as <code class="inlineCode">0.01</code>.</li>
    </ul>
    <p class="normal">To recap, ReLU is usually in hidden layer activation. You can try Leaky ReLU if ReLU doesn’t work well. Sigmoid and tanh can be used in hidden layers but are not recommended in deep networks with many layers. For the output layer, linear activation (or no activation) is used in the regression network; sigmoid is for the binary classification network and softmax is for the multiple classification case.</p>
    <p class="normal">Picking the right<a id="_idIndexMarker635"/> activation is important, and so is avoiding overfitting in neural networks. Let’s see how to do this in the next section.</p>
    <h1 class="heading-1" id="_idParaDest-146">Preventing overfitting in neural networks</h1>
    <p class="normal">A neural network<a id="_idIndexMarker636"/> is powerful as it can derive hierarchical features from data with the right architecture (the right number of hidden layers and hidden nodes). It offers a great deal of flexibility and can fit a complex dataset. However, this advantage will become a weakness if the network is not given enough control over the learning process. Specifically, it may lead to overfitting if a network is only good at fitting to the training set but is not able to generalize to unseen data. Hence, preventing overfitting is essential to the success of a neural network model.</p>
    <p class="normal">There are mainly three ways to impose restrictions on our neural networks: L1/L2 regularization, dropout, and early stopping. We practiced the first method in <em class="chapterRef">Chapter 4</em>, <em class="italic">Predicting Online Ad Click-Through with Logistic Regression</em>, and will discuss the other two in this section.</p>
    <h2 class="heading-2" id="_idParaDest-147">Dropout</h2>
    <p class="normal"><strong class="keyWord">Dropout</strong> means <a id="_idIndexMarker637"/>ignoring a certain set of hidden nodes during the learning phase of a neural network. And those hidden nodes are chosen randomly given a specified probability. In the forward pass during a training iteration, the randomly selected nodes are temporarily not used in calculating the loss; in the backward pass, the randomly selected nodes are not updated temporarily.</p>
    <p class="normal">In the following diagram, we choose three nodes in the network to ignore during training:</p>
    <figure class="mediaobject"><img alt="A diagram of a network  Description automatically generated with medium confidence" src="../Images/B21047_06_06.png"/></figure>
    <p class="packt_figref">Figure 6.6: Three nodes to ignore in a neural network</p>
    <p class="normal">Recall that a regular layer has nodes fully connected to nodes from the previous layer and the following layer. It will lead to overfitting if a large network develops and memorizes the <a id="_idIndexMarker638"/>co-dependency between individual pairs of nodes. Dropout breaks this co-dependency by temporarily deactivating certain nodes in each iteration. Therefore, it effectively reduces overfitting and won’t disrupt learning at the same time.</p>
    <p class="normal">The fraction of nodes being randomly chosen in each iteration is also called the dropout rate. In practice, we usually set a dropout rate no greater than 50%. If the dropout rate is too high, it can excessively hinder the model’s learning capacity, slowing down training and reducing the model’s ability to extract useful patterns from the data.</p>
    <div class="note">
      <p class="normal"> <strong class="keyWord">Best practice</strong></p>
      <p class="normal">Determining the dropout rate empirically involves experimenting with different dropout rates and evaluating their effects on the model’s performance. Here’s a typical approach:</p>
      <ol>
        <li class="numberedList" value="1">Start with a low rate (e.g., <code class="inlineCode">0.1</code> or <code class="inlineCode">0.2</code>) and train the model on your dataset. Monitor the model’s performance metrics on a validation set.</li>
        <li class="numberedList">Gradually increase the dropout rate in small increments (e.g., by <code class="inlineCode">0.1</code>) and retrain the model each time. Monitor the performance metrics after each training run.</li>
        <li class="numberedList">Evaluate performance obtained with different dropout rates. Be mindful of overfitting, as too high of a dropout rate can hinder model performance; if the dropout rate is too low, the model may not effectively prevent overfitting.</li>
      </ol>
    </div>
    <p class="normal">In PyTorch, we use the <code class="inlineCode">torch.nn.Dropout</code> object to add dropout to a layer. An example is as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model_with_dropout = nn.Sequential(nn.Linear(X_train.shape[</span><span class="hljs-con-number">1</span><span class="language-python">], </span><span class="hljs-con-number">16</span><span class="language-python">),</span>
                                   nn.ReLU(),
                                   nn.Dropout(0.1),
                                   nn.Linear(16, 8),
                                   nn.ReLU(),
                                   nn.Linear(8, 1))
</code></pre>
    <p class="normal">In the preceding example, 10% of nodes randomly picked from the first hidden layer are ignored in an iteration during training.</p>
    <p class="normal">Keep in mind that <a id="_idIndexMarker639"/>dropout should only occur in the training phase. In the prediction phase, all nodes should be fully connected again. Hence, we have to switch the model to evaluation mode with the <code class="inlineCode">.eval()</code> method to disable dropout before we evaluate the model or make predictions with the trained model. Let’s see it in the following California housing example:</p>
    <ol>
      <li class="numberedList" value="1">First, we compile the model (with dropout) by using Adam as the optimizer with a learning rate of <code class="inlineCode">0.01</code> and MSE as the learning goal:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model_with_dropout.parameters(), lr=</span><span class="hljs-con-number">0.01</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Next, we can train the model (with dropout) for <code class="inlineCode">1,000</code> iterations:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">1000</span><span class="language-python">):</span>
        loss = train_step(model_with_dropout, X_train_torch, y_train_torch,
                          loss_function, optimizer)
        if epoch % 100 == 0:
            print(f"Epoch {epoch} - loss: {loss}")
Epoch 0 - loss: 4.921249866485596
Epoch 100 - loss: 0.5313398838043213
Epoch 200 - loss: 0.4458008408546448
Epoch 300 - loss: 0.4264270067214966
Epoch 400 - loss: 0.4085545539855957
Epoch 500 - loss: 0.3640516400337219
Epoch 600 - loss: 0.35677382349967957
Epoch 700 - loss: 0.35208994150161743
Epoch 800 - loss: 0.34980857372283936
Epoch 900 - loss: 0.3431631028652191
</code></pre>
      </li>
    </ol>
    <p class="normal-one">In every 100 iterations, the training loss (MSE) is displayed.</p>
    <ol>
      <li class="numberedList" value="3">Finally, we use the trained model (with dropout) to predict the testing cases and print out the MSE:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model_with_dropout.</span><span class="hljs-con-built_in">eval</span><span class="language-python">()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = model_with_dropout (X_test_torch).detach().numpy()[:, </span><span class="hljs-con-number">0</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(mean_squared_error(y_test, predictions))</span>
 0.005699420832357341
</code></pre>
      </li>
    </ol>
    <p class="normal-one">As mentioned earlier, don’t forget to run <code class="inlineCode">model_with_dropout.eval()</code> before evaluating<a id="_idIndexMarker640"/> the model with dropout. Otherwise, the dropout layers will continue to randomly deactivate neurons, leading to inconsistent results between different model evaluations on the same data.</p>
    <h2 class="heading-2" id="_idParaDest-148">Early stopping</h2>
    <p class="normal">As the name <a id="_idIndexMarker641"/>implies, training a network with <strong class="keyWord">early stopping</strong> will end if the model performance doesn’t improve for a certain number of iterations. The model performance is measured on a validation set that is different from the training set, in order to assess how well it generalizes. During training, if the performance degrades after several (let’s say 50) iterations, it means the model is overfitting and not able to generalize well anymore. Hence, stopping the learning early in this case helps prevent overfitting. Usually, we evaluate the model against a validation set. If the metric on the validation set is not improving for more than <em class="italic">n</em> epochs, we stop the training process.</p>
    <p class="normal">We will demonstrate how to apply early stopping in PyTorch using the California housing example as well:</p>
    <ol>
      <li class="numberedList" value="1">First, we recreate the model and optimizer as we did previously:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = nn.Sequential(nn.Linear(X_train.shape[</span><span class="hljs-con-number">1</span><span class="language-python">], </span><span class="hljs-con-number">16</span><span class="language-python">),</span>
                      nn.ReLU(),
                      nn.Linear(16, 8),
                      nn.ReLU(),
                      nn.Linear(8, 1))
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">0.01</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Next, we define the early stopping criterion as the test loss doesn’t improve for more than <code class="inlineCode">100</code> epochs:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">patience = </span><span class="hljs-con-number">100</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">epochs_no_improve = </span><span class="hljs-con-number">0</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">best_test_loss = </span><span class="hljs-con-built_in">float</span><span class="language-python">(</span><span class="hljs-con-string">'inf'</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Now we adopt <a id="_idIndexMarker642"/>early stopping and train the model for, at most, <code class="inlineCode">500</code> iterations:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> copy</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">best_model = model</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">500</span><span class="language-python">):</span>
        loss = train_step(model, X_train_torch, y_train_torch,
                          loss_function, optimizer)      
        predictions = model(X_test_torch).detach().numpy()[:, 0]
        test_loss = mean_squared_error(y_test, predictions)
        if test_loss &gt; best_test_loss:
            epochs_no_improve += 1
            if epochs_no_improve &gt; patience:
                print(f"Early stopped at epoch {epoch}")
                break
        else:
            epochs_no_improve = 0
            best_test_loss = test_loss
            best_model = copy.deepcopy(model)
Early stopped at epoch 224
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Following every training step, we compute the test loss and compare it to the previously recorded best one. If it shows improvement, we save the current model using the <code class="inlineCode">copy</code> module and reset the <code class="inlineCode">epochs_no_improve</code> counter. However, if there is no improvement in the test loss for up to 100 consecutive iterations, we stop the training process as we have reached the tolerance threshold (<code class="inlineCode">patience</code>). In our example, training stopped after epoch <code class="inlineCode">224</code>.</p>
    <ol>
      <li class="numberedList" value="4">Finally, we use the previously recorded best model to predict the testing cases and print out the predictions and their MSE:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = best_model(X_test_torch).detach().numpy()[:, </span><span class="hljs-con-number">0</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(mean_squared_error(y_test, predictions))</span>
0.005459465255681108
</code></pre>
      </li>
    </ol>
    <p class="normal-one">This is better than <code class="inlineCode">0.0069</code>, which we obtained in the vanilla approach, and <code class="inlineCode">0.0057</code>, which we achieved using dropout for overfitting prevention.</p>
    <p class="normal">While the Universal Approximation Theorem guarantees that neural networks can represent any function, it doesn’t guarantee good generalization performance. Overfitting can occur if the model has too much capacity relative to the complexity of the underlying data distribution. Therefore, controlling the capacity of the model through techniques<a id="_idIndexMarker643"/> like regularization and early stopping is essential to ensure that the learned function generalizes well to unseen data.</p>
    <p class="normal">Now that you’ve learned about neural networks and their implementation, let’s utilize them to solve our stock price prediction problem.</p>
    <h1 class="heading-1" id="_idParaDest-149">Predicting stock prices with neural networks</h1>
    <p class="normal">We will build the stock<a id="_idIndexMarker644"/> predictor with<a id="_idIndexMarker645"/> PyTorch in this section. We will start with feature generation and data preparation, followed by network building and training. After that, we will fine-tune the network to boost the stock predictor.</p>
    <h2 class="heading-2" id="_idParaDest-150">Training a simple neural network</h2>
    <p class="normal">We prepare <a id="_idIndexMarker646"/>data and train a simple neural work with the following steps:</p>
    <ol>
      <li class="numberedList" value="1">We load the stock data, generate features, and label the <code class="inlineCode">generate_features</code> function we developed in <em class="chapterRef">Chapter 5</em>, <em class="italic">Predicting Stock Prices with Regression Algorithms</em>:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_raw = pd.read_csv(</span><span class="hljs-con-string">'19900101_20230630.csv'</span><span class="language-python">, index_col=</span><span class="hljs-con-string">'</span><span class="hljs-con-string">Date'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data = generate_features(data_raw)</span>
</code></pre>
      </li>
      <li class="numberedList">We construct the training set using data from 1990 to 2022 and the testing set using data from the first half of 2023:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">start_train = </span><span class="hljs-con-string">'1990-01-01'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">end_train = </span><span class="hljs-con-string">'2022-12-31'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">start_test = </span><span class="hljs-con-string">'2023-01-01'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">end_test = </span><span class="hljs-con-string">'2023-06-30'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_train = data.loc[start_train:end_train]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_train = data_train.drop(</span><span class="hljs-con-string">'close'</span><span class="language-python">, axis=</span><span class="hljs-con-number">1</span><span class="language-python">).values</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_train = data_train[</span><span class="hljs-con-string">'close'</span><span class="language-python">].values</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_test = data.loc[start_test:end_test]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_test = data_test.drop(</span><span class="hljs-con-string">'close'</span><span class="language-python">, axis=</span><span class="hljs-con-number">1</span><span class="language-python">).values</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_test = data_test[</span><span class="hljs-con-string">'close'</span><span class="language-python">].values</span>
</code></pre>
      </li>
      <li class="numberedList">We need to normalize features into the same or a comparable scale. We do so by removing the mean and rescaling to unit variance:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.preprocessing </span><span class="hljs-con-keyword">import</span><span class="language-python"> StandardScaler</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">scaler = StandardScaler()</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We rescale both sets with the scaler taught by the training set:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_scaled_train = scaler.fit_transform(X_train)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_scaled_test = scaler.transform(X_test)</span>
</code></pre>
    <ol>
      <li class="numberedList" value="4">Next, we need to create tensor objects from the input NumPy arrays before using them to train the PyTorch model:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_train_torch = torch.from_numpy(X_scaled_train.astype(np.float32))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_test_torch = torch.from_numpy(X_scaled_test.astype(np.float32))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_train = y_train.reshape(y_train.shape[</span><span class="hljs-con-number">0</span><span class="language-python">], </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_train_torch = torch.from_numpy(y_train.astype(np.float32))</span>
</code></pre>
      </li>
      <li class="numberedList">We now build a neural network using the <code class="inlineCode">torch.nn</code> module:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> torch.manual_seed(<span class="hljs-con-number">42</span>)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = nn.Sequential(nn.Linear(X_train.shape[</span><span class="hljs-con-number">1</span><span class="language-python">], </span><span class="hljs-con-number">32</span><span class="language-python">),</span>
                          nn.ReLU(),
                          nn.Linear(32, 1))
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The network we begin with has one hidden layer with <code class="inlineCode">32</code> nodes followed by a ReLU function.</p>
    <ol>
      <li class="numberedList" value="6">We compile the model by using Adam as the optimizer with a learning rate of <code class="inlineCode">0.3</code> and MSE as the learning goal:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">loss_function = nn.MSELoss()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">0.3</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">After <a id="_idIndexMarker647"/>defining the model, we perform training for <code class="inlineCode">1,000</code> iterations:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">1000</span><span class="language-python">):</span>
        loss = train_step(model, X_train_torch, y_train_torch,
                          loss_function, optimizer)
        if epoch % 100 == 0:
            print(f"Epoch {epoch} - loss: {loss}")
Epoch 0 - loss: 24823446.0
Epoch 100 - loss: 189974.171875
Epoch 200 - loss: 52102.01171875
Epoch 300 - loss: 17849.333984375
Epoch 400 - loss: 8928.6689453125
Epoch 500 - loss: 6497.75927734375
Epoch 600 - loss: 5670.634765625
Epoch 700 - loss: 5265.48828125
Epoch 800 - loss: 5017.7021484375
Epoch 900 - loss: 4834.28466796875
</code></pre>
      </li>
      <li class="numberedList">Finally, we use the trained model to predict the testing data and display metrics:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = model(X_test_torch).detach().numpy()[:, </span><span class="hljs-con-number">0</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.metrics </span><span class="hljs-con-keyword">import</span><span class="language-python"> mean_squared_error, mean_absolute_error, r2_score</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'MSE: </span><span class="hljs-con-subst">{mean_squared_error(y_test, predictions):</span><span class="hljs-con-number">.3</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
MSE: 30051.643
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'MAE: </span><span class="hljs-con-subst">{mean_absolute_error(y_test, predictions):</span><span class="hljs-con-number">.3</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
MAE: 137.096
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'R^2: </span><span class="hljs-con-subst">{r2_score(y_test, predictions):</span><span class="hljs-con-number">.3</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
R^2: 0.954
</code></pre>
      </li>
    </ol>
    <p class="normal">We achieve<a id="_idIndexMarker648"/> an <em class="italic">R</em><sup class="superscript-italic" style="font-style: italic;">2</sup> of <code class="inlineCode">0.954</code> with a simple neural network model. </p>
    <h2 class="heading-2" id="_idParaDest-151">Fine-tuning the neural network</h2>
    <p class="normal">Can we do better? Of <a id="_idIndexMarker649"/>course, we haven’t fine-tuned the hyperparameters yet. We perform model fine-tuning in PyTorch with the following steps:</p>
    <ol>
      <li class="numberedList" value="1">TensorBoard provides functionality for logging various metrics and visualizations during model training and evaluation. You can use TensorBoard with PyTorch to track and visualize metrics such as loss, accuracy, gradients, and model architectures, among others. We rely on the <code class="inlineCode">tensorboard</code> module in PyTorch <code class="inlineCode">utils</code>, so we import it first:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torch.utils.tensorboard </span><span class="hljs-con-keyword">import</span><span class="language-python"> SummaryWriter</span>
</code></pre>
      </li>
      <li class="numberedList">We want to tweak the number of hidden nodes in the hidden layer (again, we are using one hidden layer for this example), the number of training iterations, and the learning rate. We pick the following values of hyperparameters to experiment on:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">hparams_config = {</span>
        "hidden_size": [16, 32],
        "epochs": [1000, 3000],
        "lr": [0.1, 0.3],
    }
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, we experiment with two options for the number of hidden nodes, <code class="inlineCode">16</code> and <code class="inlineCode">32</code>; we use two options for the number of iterations, <code class="inlineCode">300</code> and <code class="inlineCode">1000</code>; and we use two options, <code class="inlineCode">0.1</code> and <code class="inlineCode">0.3</code>, for the learning rate.</p>
    <ol>
      <li class="numberedList" value="3">After initializing the hyperparameters to optimize, we will iterate each hyperparameter combination, and train and validate the model using a given set of <a id="_idIndexMarker650"/>hyperparameters by calling the helper function <code class="inlineCode">train_validate_model</code> as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">train_validate_model</span><span class="language-python">(</span><span class="hljs-con-params">hidden_size, epochs, lr</span><span class="language-python">):</span>
        model = nn.Sequential(nn.Linear(X_train.shape[1], hidden_size),
                                  nn.ReLU(),
                                  nn.Linear(hidden_size, 1))
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        # Create the TensorBoard writer
        writer_path = f"runs/{experiment_num}/{hidden_size}/{epochs}/{lr}"
        writer = SummaryWriter(log_dir=writer_path)
        for epoch in range(epochs):
            loss = train_step(model, X_train_torch, y_train_torch, loss_function, 
                              optimizer)
            predictions = model(X_test_torch).detach().numpy()[:, 0]
            test_mse = mean_squared_error(y_test, predictions)
            writer.add_scalar(
                tag="train loss",
                scalar_value=loss,
                global_step=epoch,
            )
            writer.add_scalar(
                tag="test loss",
                scalar_value=test_mse,
                global_step=epoch,
            )
        test_r2 = r2_score(y_test, predictions)
        print(f'R^2: {test_r2:.3f}\n')
        # Add the hyperparameters and metrics to TensorBoard
        writer.add_hparams(
            {
                "hidden_size": hidden_size,
                "epochs": epochs,
                "lr": lr,
            },
            {
                "test MSE": test_mse,
                "test R^2": test_r2,
            },
        )
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, in each hyperparameter combination, we build and fit a neural network model based on the given hyperparameters, including the number of hidden nodes, the learning rate, and the number of training iterations. There’s nothing much different here from what we did before. But when we train the model, we also update TensorBoard by logging the hyperparameters and metrics including the train loss and test loss with the <code class="inlineCode">add_scalar</code> method.</p>
    <p class="normal-one">The TensorBoard writer object is straightforward. It provides visualization for the model <a id="_idIndexMarker651"/>graph and metrics during training and validation.</p>
    <p class="normal-one">At the end, we compute and display the <em class="italic">R</em><sup class="superscript">2</sup> of the prediction on the test set. We also log the test MSE and <em class="italic">R</em><sup class="superscript">2</sup> using the <code class="inlineCode">add_hparams</code> method along with the given hyperparameter combination.</p>
    <ol>
      <li class="numberedList" value="4">Now we fine-tune the neural network by iterating eight hyperparameter combinations:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> torch.manual_seed(<span class="hljs-con-number">42</span>)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">experiment_num = </span><span class="hljs-con-number">0</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> hidden_size </span><span class="hljs-con-keyword">in</span><span class="language-python"> hparams_config[</span><span class="hljs-con-string">"hidden_size"</span><span class="language-python">]:</span>
        for epochs in hparams_config["epochs"]:
            for lr in hparams_config["lr"]:
                experiment_num += 1
                print(f"Experiment {experiment_num}: hidden_size = {hidden_size}, 
                        epochs = {epochs}, lr = {lr}")
                train_validate_model(hidden_size, epochs, lr)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">You will see the following output:</p>
    <pre class="programlisting con-one"><code class="hljs-con">Experiment 1: hidden_size = 16, epochs = 1000, lr = 0.1
R^2: 0.771
Experiment 2: hidden_size = 16, epochs = 1000, lr = 0.3
R^2: 0.952
Experiment 3: hidden_size = 16, epochs = 3000, lr = 0.1
R^2: 0.969
Experiment 4: hidden_size = 16, epochs = 3000, lr = 0.3
R^2: 0.977
Experiment 5: hidden_size = 32, epochs = 1000, lr = 0.1
R^2: 0.877
Experiment 6: hidden_size = 32, epochs = 1000, lr = 0.3
R^2: 0.957
Experiment 7: hidden_size = 32, epochs = 3000, lr = 0.1
R^2: 0.970
Experiment 8: hidden_size = 32, epochs = 3000, lr = 0.3
R^2: 0.959
</code></pre>
    <p class="normal-one">Experiment <a id="_idIndexMarker652"/>4 with the combination of <code class="inlineCode">(hidden_size=16, epochs=3000, learning_rate=0.3)</code> is the best performing one, where we achieve an <em class="italic">R</em><sup class="superscript">2</sup> of <code class="inlineCode">0.977</code>.</p>
    <div class="note-one">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">Hyperparameter tuning for neural networks can significantly impact model performance. Here are some <a id="_idIndexMarker653"/>best practices for hyperparameter tuning in neural networks:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Define a search space</strong>: Determine which hyperparameters to tune and their ranges. Common hyperparameters include learning rate, batch size, number of hidden layers, number of neurons per layer, activation functions, dropout rates, and so on.</li>
        <li class="bulletList"><strong class="keyWord">Use cross-validation</strong>: This helps prevent overfitting and provides a more robust estimate of model performance.</li>
        <li class="bulletList"><strong class="keyWord">Monitor performance metrics</strong>: Track relevant metrics such as loss, accuracy, precision, recall, MSE, <em class="italic">R</em><sup class="superscript">2</sup>, and so on during training and validation.</li>
        <li class="bulletList"><strong class="keyWord">Early stopping</strong>: Monitor the validation loss during training, and stop training when it starts to increase consistently while the training loss decreases.</li>
        <li class="bulletList"><strong class="keyWord">Regularization</strong>: Use regularization techniques such as L1 and L2 regularization and dropout to prevent overfitting and improve generalization performance.</li>
        <li class="bulletList"><strong class="keyWord">Experiment with different architectures</strong>: Try different network architectures, including the number of layers, the number of neurons per layer, and the activation functions. Experiment with deep vs. shallow networks and wide vs. narrow networks.</li>
        <li class="bulletList"><strong class="keyWord">Use parallelism</strong>: If computational resources allow, parallelize the hyperparameter search process to speed up experimentation. Tools like TensorFlow’s <code class="inlineCode">tf.distribute.Strategy</code> or PyTorch’s <code class="inlineCode">torch.nn.DataParallel</code> can be used <a id="_idIndexMarker654"/>to distribute training across multiple GPUs or machines.</li>
      </ul>
    </div>
    <ol>
      <li class="numberedList" value="5">You will <a id="_idIndexMarker655"/>notice that a new folder, <code class="inlineCode">runs</code>, is created after these experiments start. It contains the training and validation performance for each experiment. After 8 experiments finish, it’s time to launch TensorBoard. We use the following command:
        <pre class="programlisting con-one"><code class="hljs-con">tensorboard --host 0.0.0.0 --logdir runs
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Once it is launched, you will see the beautiful dashboard at <code class="inlineCode">http://localhost:6006/</code>. You can see a screenshot of the expected result here:</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21047_06_07.png"/></figure>
    <p class="packt_figref">Figure 6.7: Screenshot of TensorBoard</p>
    <div class="note-one">
      <p class="normal">The time series of train and test loss provide valuable insights. They allow us to assess the progress of training and identify signs of overfitting or underfitting. Overfitting can be identified when the train loss decreases over time while the test loss remains stagnant or increases. On the other hand, underfitting is indicated by relatively high train and test loss values, indicating that the model fails to adequately fit the training data.</p>
    </div>
    <ol>
      <li class="numberedList" value="6">Next, we click on<a id="_idIndexMarker656"/> the <strong class="keyWord">HPARAMS</strong> tab to see the hyperparameter logs. You can see all the <a id="_idIndexMarker657"/>hyperparameter combinations and the respective metrics (MSE and <em class="italic">R</em><sup class="superscript-italic" style="font-style: italic;">2</sup>) displayed in a table, as shown here:</li>
    </ol>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21047_06_08.png"/></figure>
    <p class="packt_figref">Figure 6.8: Screenshot of TensorBoard for hyperparameter tuning</p>
    <p class="normal-one">Again, you can see experiment 4 yields the best performance.</p>
    <ol>
      <li class="numberedList" value="7">Finally, we use the optimal model to make predictions:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">hidden_size = </span><span class="hljs-con-number">16</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">epochs = </span><span class="hljs-con-number">3000</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">lr = </span><span class="hljs-con-number">0.3</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">best_model = nn.Sequential(nn.Linear(X_train.shape[</span><span class="hljs-con-number">1</span><span class="language-python">], hidden_size),</span>
                           nn.ReLU(),
                           nn.Linear(hidden_size, 1))
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(best_model.parameters(), lr=lr)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(epochs):</span>
    train_step(best_model, X_train_torch, y_train_torch, loss_function,
               optimizer
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = best_model(X_test_torch).detach().numpy()[:, </span><span class="hljs-con-number">0</span><span class="language-python">]</span>
</code></pre>
      </li>
      <li class="numberedList">Plot the <a id="_idIndexMarker658"/>prediction along with the ground truth as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> matplotlib.pyplot </span><span class="hljs-con-keyword">as</span><span class="language-python"> plt</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.rc(</span><span class="hljs-con-string">'xtick'</span><span class="language-python">, labelsize=</span><span class="hljs-con-number">10</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.rc(</span><span class="hljs-con-string">'ytick'</span><span class="language-python">, labelsize=</span><span class="hljs-con-number">10</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.plot(data_test.index, y_test, c=</span><span class="hljs-con-string">'k'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.plot(data_test.index, predictions, c=</span><span class="hljs-con-string">'b'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.xticks(</span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">0</span><span class="language-python">, </span><span class="hljs-con-number">130</span><span class="language-python">, </span><span class="hljs-con-number">10</span><span class="language-python">), rotation=</span><span class="hljs-con-number">60</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.xlabel(</span><span class="hljs-con-string">'Date'</span><span class="language-python"> , fontsize=</span><span class="hljs-con-number">10</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.ylabel(</span><span class="hljs-con-string">'Close price'</span><span class="language-python"> , fontsize=</span><span class="hljs-con-number">10</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.legend([</span><span class="hljs-con-string">'Truth'</span><span class="language-python">, </span><span class="hljs-con-string">'Neural network'</span><span class="language-python">] , fontsize=</span><span class="hljs-con-number">10</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Refer to the following screenshot for the result:</p>
    <figure class="mediaobject"><img alt="A graph with blue lines and numbers  Description automatically generated" src="../Images/B21047_06_09.png"/></figure>
    <p class="packt_figref">Figure 6.9: Prediction and ground truth of stock prices</p>
    <p class="normal">The fine-tuned<a id="_idIndexMarker659"/> neural network does a good job of predicting stock prices.</p>
    <p class="normal">In this section, we further improved the neural network stock predictor by fine-tuning the hyperparameters. Feel free to use more hidden layers, or apply dropout or early stopping to see whether you can get a better result.</p>
    <h1 class="heading-1" id="_idParaDest-152">Summary</h1>
    <p class="normal">In this chapter, we worked on the stock prediction project again, but with neural networks this time. We started with a detailed explanation of neural networks, including the essential components (layers, activations, feedforward, and backpropagation), and transitioned to DL. We moved on to implementations from scratch with scikit-learn, TensorFlow, and PyTorch. We also learned about ways to avoid overfitting, such as dropout and early stopping. Finally, we applied what we covered in this chapter to solve our stock price prediction problem.</p>
    <p class="normal">In the next chapter, we will explore NLP techniques and unsupervised learning.</p>
    <h1 class="heading-1" id="_idParaDest-153">Exercises</h1>
    <ol>
      <li class="numberedList" value="1">As mentioned, can you use more hidden layers in the neural network stock predictor and rerun the model fine-tuning? Can you get a better result?</li>
      <li class="numberedList">Following the first exercise, can you apply dropout and/or early stopping and see if you can beat the current best <em class="italic">R</em><sup class="superscript-italic" style="font-style: italic;">2</sup> of <code class="inlineCode">0.977</code>?</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-154">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/yuxi"><span class="url">https://packt.link/yuxi</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code187846872178698968.png"/></p>
  </div>
</body></html>
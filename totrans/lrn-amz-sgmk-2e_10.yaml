- en: 'Chapter 8: Using Your Algorithms and Code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to train and deploy models with built-in
    frameworks such as **scikit-learn** and **TensorFlow**. Thanks to **script mode**,
    these frameworks make it easy to use your own code, without having to manage any
    training or inference containers.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, your business or technical environment could make it difficult
    or even impossible to use these containers. Maybe you need to be in full control
    of how containers are built. Maybe you'd like to implement your own prediction
    logic. Maybe you're working with a framework or language that's not natively supported
    by SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you'll learn how to tailor training and inference containers
    to your own needs. You'll also learn how to train and deploy your own custom code,
    using either the SageMaker SDK directly or command-line open source tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how SageMaker invokes your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing built-in framework containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building custom training containers with the SageMaker Training Toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building fully custom containers for training and inference with Python and
    R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and deploying with your custom Python code on MLflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building fully custom containers for SageMaker Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create it. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to install and configure the AWS **Command-Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory but
    strongly encouraged as it includes many projects that we will need (Jupyter, `pandas`,
    `numpy`, and more).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working Docker installation. You'll find installation instructions
    and documentation at [https://docs.docker.com](https://docs.docker.com).
  prefs: []
  type: TYPE_NORMAL
- en: The code examples included in this book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how SageMaker invokes your code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we worked with built-in algorithms and frameworks, we didn''t pay much
    attention to how SageMaker actually invoked the training and deployment code.
    After all, that''s what "built-in" means: grab what you need off the shelf and
    get to work.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, things are different if we want to use our own custom code and containers.
    We need to understand how they interface with SageMaker so that we implement them
    exactly right.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll discuss this interface in detail. Let's start with the
    file layout.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the file layout inside a SageMaker container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make our life simpler, SageMaker estimators automatically copy hyperparameters
    and input data inside training containers. Likewise, they automatically copy the
    trained model (and any checkpoints) from the container to S3\. At deployment time,
    they do the reverse operation, copying the model from S3 into the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can imagine, this requires a file layout convention:'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters are stored as a JSON dictionary in `/opt/ml/input/config/hyperparameters.json`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input channels are stored in `/opt/ml/input/data/CHANNEL_NAME`. We saw in the
    previous chapter that the channel names match the ones passed to the `fit()` API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model should be saved in and loaded from `/opt/ml/model`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, we'll need to use these paths in our custom code. Now, let's see how
    the training and deployment code is invoked.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the options for custom training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending
    Machine Learning Services Using Built-In Frameworks*, we studied script mode and
    how SageMaker uses it to invoke our training script. This feature is enabled by
    additional Python code present in the framework containers, namely, the SageMaker
    Training Toolkit ([https://github.com/aws/sagemaker-training-toolkit](https://github.com/aws/sagemaker-training-toolkit)).
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, this training toolkit copies the entry point script, its hyperparameters,
    and its dependencies inside the container. It also copies data from the input
    channels inside the container. Then, it invokes the entry point script. Curious
    minds can read the code at `src/sagemaker_training/entry_point.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to customizing your training code, you have the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: Customize an existing framework container, adding only your extra dependencies
    and code. Script mode and the framework estimator will be available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a custom container based solely on the SageMaker Training Toolkit. Script
    mode and the generic `Estimator` module will be available, but you'll have to
    install everything else.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a fully custom container. If you want to start from a blank page or don't
    want any extra code inside your container, this is the way to go. You'll train
    with the generic `Estimator` module, and script mode won't be available. Your
    training code will be invoked directly (more on this later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the options for custom deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Framework containers include additional Python code for deployment. Here are
    the repositories for the most popular frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow**: [https://github.com/aws/sagemaker-tensorflow-serving-container](https://github.com/aws/sagemaker-tensorflow-serving-container).
    Models are served with **TensorFlow Serving** ([https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyTorch**: [https://github.com/aws/sagemaker-pytorch-inference-toolkit](https://github.com/aws/sagemaker-pytorch-inference-toolkit).
    Models are served with **TorchServe** ([https://pytorch.org/serve](https://pytorch.org/serve)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache MXNet**: [https://github.com/aws/sagemaker-mxnet-inference-toolkit](https://github.com/aws/sagemaker-mxnet-inference-toolkit).
    Models are served with the **Multi-Model Server** ([https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server)),
    integrated into the **SageMaker Inference Toolkit** ([https://github.com/aws/sagemaker-inference-toolkit](https://github.com/aws/sagemaker-inference-toolkit)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scikit-learn**: [https://github.com/aws/sagemaker-scikit-learn-container](https://github.com/aws/sagemaker-scikit-learn-container).
    Models are served with the Multi-Model Server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGBoost**: [https://github.com/aws/sagemaker-xgboost-container](https://github.com/aws/sagemaker-xgboost-container).
    Models are served with the Multi-Model Server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Just like for training, you have three options:'
  prefs: []
  type: TYPE_NORMAL
- en: Customize an existing framework container. Models will be served using the existing
    inference logic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a custom container based solely on the SageMaker Inference Toolkit. Models
    will be served by the Multi-Model Server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a fully custom container, doing away with any inference logic and implementing
    your own instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whether you use a single container for training and deployment or two different
    containers is up to you. A lot of different factors come into play: who builds
    the containers, who runs them, and so on. Only you can decide what the best option
    for your particular setup is.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's run some examples!
  prefs: []
  type: TYPE_NORMAL
- en: Customizing an existing framework container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of course, we could simply write a Dockerfile referencing one of the Deep Learning
    Containers images ([https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md))
    and add our own commands. See the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Instead, let's customize and rebuild the **PyTorch** training and inference
    containers on our local machine. The process is similar to other frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Build environment
  prefs: []
  type: TYPE_NORMAL
- en: Docker needs to be installed and running. To avoid throttling when pulling base
    images, I recommend that you create a `docker login` or **Docker Desktop**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid bizarre dependency issues (I''m looking at you, macOS), I also recommend
    that you build images on an `m5.large` should suffice), but please make sure to
    provision more storage than the default 8 GB. I recommend 64 GB. You also need
    to make sure that the **IAM** role for the instance allows you to push and pull
    EC2 images. If you''re unsure how to create and connect to an EC2 instance, this
    tutorial will get you started: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your build environment on EC2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will get started using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your EC2 instance is up, we connect to it with `ssh`. We first install
    Docker and add the `ec2-user` to the `docker` group. This will allow us to run
    Docker commands as a non-root user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In order to apply this permission change, we log out and log in again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We make sure that `docker` is running and we log in to Docker Hub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We install `git`, Python 3, and `pip`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our EC2 instance is now ready, and we can move on to building containers.
  prefs: []
  type: TYPE_NORMAL
- en: Building training and inference containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This can be done using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We clone the `deep-learning-containers` repository, which centralizes all training
    and inference code for TensorFlow, PyTorch, Apache MXNet, and Hugging Face, and
    adds convenient scripts to build their containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set environment variables for our account ID, the region we''re running
    in, and the name of a new repository we''re going to create in Amazon ECR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the repository in Amazon ECR, and we log in. Please refer to the
    documentation for details ([https://docs.aws.amazon.com/ecr/index.html](https://docs.aws.amazon.com/ecr/index.html)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a virtual environment, and we install the Python requirements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we''d like to build the training and inference containers for PyTorch
    1.8, on both the CPU and GPU. We can find the corresponding Docker files in `pytorch/training/docker/1.8/py3/`
    and customize them to our needs. For example, we could pin Deep Graph Library
    to version 0.6.1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we''ve edited the Docker files, we take a look at the build configuration
    file for the latest PyTorch version (`pytorch/buildspec.yml`). We decide to customize
    image tags to make sure each image is clearly identifiable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we run the setup script and launch the build process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a little while, all four images are built (plus an example image), and
    we can see them in our local Docker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can also see them in our ECR repository, as shown in the following screenshot:![Figure
    8.1 – Viewing images in ECR
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_08_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.1 – Viewing images in ECR
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The images are now available with the SageMaker SDK. Let''s train with our
    new CPU image. All we have to do is pass its name in the `image_uri` parameter
    of the `PyTorch` estimator. Please note that we can remove `py_version` and `framework_version`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, it's pretty easy to customize Deep Learning Containers. Now,
    let's go one level deeper and work only with the training toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Using the SageMaker Training Toolkit with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we're going to build a custom Python container with the SageMaker
    Training Toolkit. We'll use it to train a scikit-learn model on the Boston Housing
    dataset, using script mode and the `SKLearn` estimator.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need three building blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: The training script. Since script mode will be available, we can use exactly
    the same code as in the scikit-learn example from [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130),
    *Extending Machine Learning Services Using Built-In Frameworks*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need a Dockerfile and Docker commands to build our custom container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also need an `SKLearn` estimator configured to use our custom container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take care of the container:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Dockerfile can get quite complicated. No need for that here! We start from
    the official Python 3.7 image available on Docker Hub ([https://hub.docker.com/_/python](https://hub.docker.com/_/python)).
    We install scikit-learn, `numpy`, `pandas`, `joblib`, and the SageMaker Training
    Toolkit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We build the image with the `docker build` command, tagging it as `sklearn-customer:sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the image is built, we find its identifier:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the AWS CLI, we create a repository in Amazon ECR to host this image,
    and we log in to the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the image identifier, we tag the image with the repository identifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We push the image to the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The image is now ready for training with a SageMaker estimator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We define an `SKLearn` estimator, setting the `image_uri` parameter to the
    name of the container we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set the location of the training channel and launch the training as usual.
    In the training log, we see that our code is indeed invoked with script mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, it's easy to customize training containers. Thanks to the SageMaker
    Training Toolkit, you can work just as with a built-in framework container. We
    used scikit-learn here, and you can do the same with all other frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: However, we cannot use this container for deployment, as it doesn't contain
    any model-serving code. We should add bespoke code to launch a web app, which
    is exactly what we're going to do in the next example.
  prefs: []
  type: TYPE_NORMAL
- en: Building a fully custom container for scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we're going to build a fully custom container without any AWS
    code. We'll use it to train a scikit-learn model on the Boston Housing dataset,
    using a generic `Estimator` module. With the same container, we'll deploy the
    model thanks to a Flask web application.
  prefs: []
  type: TYPE_NORMAL
- en: We'll proceed in a logical way, first taking care of the training, and then
    updating the code to handle deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Training with a fully custom container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we can''t rely on script mode anymore, the training code needs to be
    modified. This is what it looks like, and you''ll easily figure out what''s happening
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Using the standard file layout for SageMaker containers, we read hyperparameters
    from their JSON file. Then, we load the dataset, train the model, and save it
    at the correct location.
  prefs: []
  type: TYPE_NORMAL
- en: There's another very important difference, and we have to dive a bit into Docker
    to explain it. SageMaker will run the training container as `docker run <IMAGE_ID>
    train`, passing the `train` argument to the entry point of the container.
  prefs: []
  type: TYPE_NORMAL
- en: If your container has a predefined entry point, the `train` argument will be
    passed to it, say, `/usr/bin/python train`. If your container doesn't have a predefined
    entry point, `train` is the actual command that will be run.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid annoying issues, I recommend that your training code ticks the following
    boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: Name it `train`—no extension, just `train`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make it executable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure it's in the `PATH` value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first line of the script should define the path to the interpreter, for
    example, `#!/usr/bin/env python`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This should guarantee that your training code is invoked correctly whether your
    container has a predefined entry point or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll take care of this in the Dockerfile, starting from an official Python
    image. Note that we''re not installing the SageMaker Training Toolkit any longer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The name of the script is correct. It's executable, and `/usr/bin` is in `PATH`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should be all set—let''s create our custom container and launch a training
    job with it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We build and push the image, using a different tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We update our notebook code to use the generic `Estimator` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We train as usual.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let's add code to deploy this model.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a fully custom container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flask is a highly popular web framework for Python ([https://palletsprojects.com/p/flask](https://palletsprojects.com/p/flask)).
    It's simple and well documented. We're going to use it to build a simple prediction
    API hosted in our container.
  prefs: []
  type: TYPE_NORMAL
- en: Just like for our training code, SageMaker requires that the deployment script
    is copied inside the container. The image will be run as `docker run <IMAGE_ID>
    serve`.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP requests will be sent to port `8080`. The container must provide a `/ping`
    URL for health checks and an`/invocations` URL for prediction requests. We'll
    use CSV as the input format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, your deployment code needs to tick the following boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: Name it `serve`—no extension, just `serve`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make it executable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure it's in `PATH`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure port `8080` is exposed by the container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide code to handle the `/ping` and `/invocations` URLs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s the updated Dockerfile. We install Flask, copy the deployment code,
    and open port `8080`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how we could implement a simple prediction service with Flask:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the required modules. We load the model from `/opt/ml/model` and
    initialize the Flask application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We implement the `/ping` URL for health checks, by simply returning HTTP code
    200 (OK):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We implement the `/invocations` URL. If the content type is not `text/csv`,
    we return HTTP code 415 (Unsupported Media Type). If it is, we decode the request
    body and store it in a file-like memory buffer. Then, we read the CSV samples,
    predict them, and send the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At startup, the script launches the Flask app on port `8080`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's not too difficult, even if you're not yet familiar with Flask.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We rebuild and push the image, and then we train again with the same estimator.
    No change is required here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We deploy the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Reminder
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you see some weird behavior here (the endpoint not deploying, cryptic error
    messages, and so on), Docker is probably hosed. `sudo service docker restart`
    should fix most problems. Cleaning `tmp*` cruft in `/tmp` may also help.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We prepare a couple of test samples, set the content type to `text/csv`, and
    invoke the prediction API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see something similar to this. The API has been successfully invoked:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we''re done, we delete the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next example, we're going to train and deploy a model using the R environment.
    This will give us an opportunity to step out of the Python world for a bit. As
    you will see, things are not really different.
  prefs: []
  type: TYPE_NORMAL
- en: Building a fully custom container for R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R is a popular language for data exploration and analysis. In this example,
    we're going to build a custom container to train and deploy a linear regression
    model on the Boston Housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The overall process is similar to building a custom container for Python. Instead
    of using Flask to build our prediction API, we'll use `plumber` ([https://www.rplumber.io](https://www.rplumber.io)).
  prefs: []
  type: TYPE_NORMAL
- en: Coding with R and plumber
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Don''t worry if you''re not familiar with R. This is a really simple example,
    and I''m sure you''ll be able to follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We write a function to train our model. It loads the hyperparameters and the
    dataset from the conventional paths. It normalizes the dataset if we requested
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It trains a linear regression model, taking all features into account to predict
    the median house price (the `medv` column). Finally, it saves the model in the
    right place:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We write a function to serve predictions. Using `plumber` annotations, we define
    a `/ping` URL for health checks and an`/invocations` URL for predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Putting these two pieces together, we write a main function that will serve
    as the entry point for our script. SageMaker will pass either a `train` or `serve`
    command-line argument, and we''ll call the corresponding function in our code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is all of the R code that we need. Now, let's take care of the container.
  prefs: []
  type: TYPE_NORMAL
- en: Building a custom container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to build a custom container storing the R runtime, as well as our script.
    The Dockerfile is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start from an official R image in **Docker Hub** and add the dependencies
    we need (these are the ones I needed on my machine; your mileage may vary):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we copy our code inside the container and define the main function as
    its explicit entry point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a new repository in ECR. Then, we build the image (this could take
    a while and involve compilation steps) and push it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We're all set, so let's train and deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Training and deploying a custom container on SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Jumping to a Jupyter notebook, we use the SageMaker SDK to train and deploy
    our container:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We configure an `Estimator` module with our custom container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the training job is complete, we deploy the model as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we read the full dataset (why not?) and send it to the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we''re done, we delete the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Whether you're using Python, R, or something else, it's reasonably easy to build
    and deploy your own custom container. Still, you need to build your own little
    web application, which is something you may neither know how to do nor enjoy doing.
    Wouldn't it be nice if we had a tool that took care of all of that pesky container
    and web stuff?
  prefs: []
  type: TYPE_NORMAL
- en: 'As a matter of fact, there is one: **MLflow**.'
  prefs: []
  type: TYPE_NORMAL
- en: Training and deploying with your own code on MLflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLflow is an open source platform for machine learning ([https://mlflow.org](https://mlflow.org)).
    It was initiated by Databricks ([https://databricks.com](https://databricks.com)),
    who also brought us **Spark**. MLflow has lots of features, including the ability
    to deploy Python-trained models on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: This section is not intended to be an MLflow tutorial. You can find documentation
    and examples at [https://www.mlflow.org/docs/latest/index.html](https://www.mlflow.org/docs/latest/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Installing MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On our local machine, let''s set up a virtual environment for MLflow and install
    the required libraries. The following example was tested with MLflow 1.17:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first initialize a new virtual environment named `mlflow-example`. Then,
    we activate it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We install MLflow and the libraries required by our training script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we download the Direct Marketing dataset we already used with XGBoost
    in [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending Machine
    Learning Services Using Built-In Frameworks*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The setup is complete. Let's train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model with MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training script sets the MLflow experiment for this run so that we may
    log metadata (hyperparameters, metrics, and so on). Then, it loads the dataset,
    trains an XGBoost classifier, and logs the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The `load_dataset()` function does what its name implies and logs several parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s train the model and visualize its results in the MLflow web application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the virtual environment we just created on our local machine, we run
    the training script just like any Python program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We launch the MLflow web application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pointing our browser at [http://localhost:5000](http://localhost:5000), we
    see information on our run, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Viewing our job in MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_08_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Viewing our job in MLflow
  prefs: []
  type: TYPE_NORMAL
- en: The training was successful. Before we can deploy the model on SageMaker, we
    must build a SageMaker container. As it turns out, it's the simplest thing.
  prefs: []
  type: TYPE_NORMAL
- en: Building a SageMaker container with MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All it takes is a single command on our local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: MLflow will automatically build a Docker container compatible with SageMaker,
    with all required dependencies. Then, it creates a repository in Amazon ECR named
    `mlflow-pyfunc` and pushes the image to it. Obviously, this requires your AWS
    credentials to be properly set up. MLflow will use the default region configured
    by the AWS CLI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this command completes, you should see the image in ECR, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Viewing our container in ECR'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_08_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Viewing our container in ECR
  prefs: []
  type: TYPE_NORMAL
- en: Our container is now ready for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model locally with MLflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will deploy our model using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can deploy our model locally with a single command, passing its run identifier
    (visible in the MLflow URL for the run) and the HTTP port to use. This fires up
    a local web application based on `gunicorn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see something similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our prediction code is quite straightforward. We load CSV samples from the
    dataset, convert them into JSON format, and send them to the endpoint using the
    `requests` library, a popular Python library for HTTP ([https://requests.readthedocs.io](https://requests.readthedocs.io)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code in another shell invokes the local model and prints out predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we're done, we terminate the local server with *Ctrl* + *C*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we're confident that our model works locally, we can deploy it on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model on SageMaker with MLflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is a one-liner again:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to pass an application name, the model path, and the name of the SageMaker
    role. You can use the same role you''ve used in previous chapters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a few minutes, the endpoint is in service. We invoke it with the following
    code. It loads the test dataset and sends the first 10 samples in JSON format
    to the endpoint named after our application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Wait a minute! We are not using the SageMaker SDK. What's going on here?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this example, we're dealing with an existing endpoint, not an endpoint that
    we created by fitting an estimator and deploying a predictor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We could still rebuild a predictor using the SageMaker SDK, as we'll see in
    [*Chapter 11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237), *Deploying Machine
    Learning Models*. Instead, we use our good old friend `boto3`, the AWS SDK for
    Python. We first invoke the `describe_endpoint()` API to check that the endpoint
    is in service. Then, we use the `invoke_endpoint()` API to…invoke the endpoint!
    For now, we don't need to know more.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We run the prediction code on our local machine, and it produces the following
    output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we''re done, we delete the endpoint with the MLflow CLI. This cleans up
    all resources created for deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The development experience with MLflow is pretty simple. It also has plenty
    of other features you may want to explore.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've run examples for training and prediction. There's another area
    of SageMaker that lets us use custom containers, **SageMaker Processing**, which
    we studied in [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030), *Handling
    Data Preparation Techniques*. To close this chapter, let's build a custom Python
    container for SageMaker Processing.
  prefs: []
  type: TYPE_NORMAL
- en: Building a fully custom container for SageMaker Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll reuse the news headlines example from [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108),
    *Training Natural Processing Models*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with a Dockerfile based on a minimal Python image. We install dependencies,
    add our processing script, and define it as our entry point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We build the image and tag it as `sm-processing-custom:latest`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the AWS CLI, we create a repository in Amazon ECR to host this image,
    and we log in to the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the image identifier, we tag the image with the repository identifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We push the image to the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Moving to a Jupyter notebook, we configure a generic `Processor` object with
    our new container, which is the equivalent of the generic `Estimator` module we
    used for training. Accordingly, no `framework_version` parameter is required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the same `ProcessingInput` and `ProcessingOutput` objects, we run the
    processing job. As our processing code is now stored inside the container, we
    don''t need to pass a `code` parameter as we did with `SKLearnProcessor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the training job is complete, we can fetch its outputs in S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This concludes our exploration of custom containers in SageMaker. As you can
    see, you can pretty much run anything as long as it fits inside a Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Built-in frameworks are extremely useful, but sometimes you need something a
    little—or very—different. Whether starting from built-in containers or from scratch,
    SageMaker lets you build your training and deployment containers exactly the way
    you want them. Freedom for all!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how to customize Python and R containers for data
    processing, training, and deployment. You saw how you could use them with the
    SageMaker SDK and its usual workflow. You also learned about MLflow, a nice open
    source tool that lets you train and deploy models using a CLI.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our extensive coverage of modeling options in SageMaker: built-in
    algorithms, built-in frameworks, and custom code. In the next chapter, you''ll
    learn about SageMaker features that help you to scale your training jobs.'
  prefs: []
  type: TYPE_NORMAL

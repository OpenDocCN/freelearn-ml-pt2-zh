- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting Stock Prices with Regression Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we predicted ad clicks using logistic regression. In
    this chapter, we will solve a problem that interests everyone—predicting stock
    prices. Getting wealthy by means of smart investment—who isn’t interested?! Stock
    market movements and stock price predictions have been actively researched by
    a large number of financial, trading, and even technology corporations. A variety
    of methods have been developed to predict stock prices using machine learning
    techniques. Herein, we will focus on learning several popular regression algorithms,
    including linear regression, regression trees and regression forests, and support
    vector regression, utilizing them to tackle this billion (or trillion)-dollar
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is regression?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mining stock price data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating with linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating with decision tree regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a regression forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating regression performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting stock prices with the three regression algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is regression?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Regression** is one of the main types of supervised learning in machine learning.
    In regression, the training set contains observations (also called features) and
    their associated **continuous** target values. The process of regression has two
    phases:'
  prefs: []
  type: TYPE_NORMAL
- en: The first phase is exploring the relationships between the observations and
    the targets. This is the training phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second phase is using the patterns from the first phase to generate the
    target for a future observation. This is the prediction phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The overall process is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, diagram  Description automatically
    generated](img/B21047_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Training and prediction phase in regression'
  prefs: []
  type: TYPE_NORMAL
- en: The major difference between regression and classification is that the output
    values in regression are continuous, while in classification they are discrete.
    This leads to different application areas for these two supervised learning methods.
    Classification is basically used to determine desired memberships or characteristics,
    as you’ve seen in previous chapters, such as email being spam or not, newsgroup
    topics, or ad click-through. Conversely, regression mainly involves estimating
    an outcome or forecasting a response.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of estimating continuous targets with linear regression is depicted
    as follows, where we try to fit a line against a set of two-dimensional data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, line, rectangle  Description automatically
    generated](img/B21047_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Linear regression example'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical machine learning regression problems include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting house prices based on location, square footage, and the number of
    bedrooms and bathrooms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating power consumption based on information about a system’s processes
    and memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting demand in retail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting stock prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ve talked about regression in this section and will briefly introduce its
    use in the stock market and trading in the next one.
  prefs: []
  type: TYPE_NORMAL
- en: Mining stock price data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll work as a stock quantitative analyst/researcher, exploring
    how to predict stock prices with several typical machine learning regression algorithms.
    Let’s start with a brief overview of the stock market and stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of the stock market and stock prices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The stock of a corporation signifies ownership in the corporation. A single
    share of the stock represents a claim on the fractional assets and the earnings
    of the corporation in proportion to the total number of shares. Stocks can be
    traded between shareholders and other parties via stock exchanges and organizations.
    Major stock exchanges include the New York Stock Exchange, the NASDAQ, London
    Stock Exchange Group, and the Hong Kong Stock Exchange. The prices that a stock
    is traded at fluctuate essentially due to the law of supply and demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, investors want to buy low and sell high. This sounds simple enough,
    but it’s very challenging to implement, as it’s monumentally difficult to say
    whether a stock price will go up or down. There are two main streams of studies
    that attempt to understand factors and conditions that lead to price changes,
    or even forecast future stock prices, **fundamental analysis** and **technical
    analysis**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fundamental analysis**: This stream focuses on underlying factors that influence
    a company’s value and business, including overall economy and industry conditions
    from macro perspectives, the company’s financial conditions, management, and competitors
    from micro perspectives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical analysis**: Conversely, this stream predicts future price movements
    through the statistical study of past trading activity, including price movement,
    volume, and market data. Predicting prices via machine learning techniques is
    an important topic in technical analysis nowadays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many quantitative, or quant, trading firms use machine learning to empower automated
    and algorithmic trading.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, we can apply regression techniques to predict the prices of a particular
    stock. However, it’s difficult to ensure the stock we pick is suitable for learning
    purposes—its price should follow some learnable patterns, and it can’t have been
    affected by unprecedented instances or irregular events. Hence, herein we’ll focus
    on one of the most popular **stock indexes** to better illustrate and generalize
    our price regression approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first cover what an index is. A stock index is a statistical measure of
    the value of a portion of the overall stock market. An index includes several
    stocks that are diverse enough to represent a section of the whole market. Also,
    the price of an index is typically computed as the weighted average of the prices
    of selected stocks.
  prefs: []
  type: TYPE_NORMAL
- en: The **NASDAQ Composite** is one of the longest-established and most commonly
    watched indexes in the world. It includes all the stocks listed on the NASDAQ
    exchange, covering a wide range of sectors. NASDAQ primarily lists stocks of technology
    companies, including established giants like Apple, Amazon, Microsoft, and Google
    (Alphabet), as well as emerging growth companies.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view its daily prices and performance on Yahoo Finance at [https://finance.yahoo.com/quote/%5EIXIC/history?p=%5EIXIC](https://finance.yahoo.com/quote/%5EIXIC/history?p=%5EIXIC).
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a graph  Description automatically generated](img/B21047_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Screenshot of daily prices and performance in Yahoo Finance'
  prefs: []
  type: TYPE_NORMAL
- en: 'On each trading day, the price of stock changes and is recorded in real time.
    Five values illustrating the movements in price over one unit of time (usually
    one day, but it can also be one week or one month) are key trading indicators.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open**: The starting price for a given trading day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Close**: The final price on that day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High**: The highest prices at which the stock traded on that day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low**: The lowest prices at which the stock traded on that day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume**: The total number of shares traded before the market closed on that
    day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will focus on NASDAQ and use its historical prices and performance to predict
    future prices. In the following sections, we will explore how to develop price
    prediction models, specifically regression models, and what can be used as indicators
    or predictive features.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to a machine learning algorithm, the first question to ask is
    usually what features are available or what the predictive variables are.
  prefs: []
  type: TYPE_NORMAL
- en: The driving factors that are used to predict future prices of NASDAQ, the **close**
    prices, include historical and current **open** prices as well as historical performance
    (**high**, **low**, and **volume**). Note that current or same-day performance
    (**high**, **low**, and **volume**) shouldn’t be included because we simply can’t
    foresee the highest and lowest prices at which the stock traded, or the total
    number of shares traded before the market closed on that day.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the close price with only those preceding four indicators doesn’t
    seem promising and might lead to underfitting. So, we need to think of ways to
    generate more features in order to increase predictive power. In machine learning,
    **feature engineering** is the process of creating features in order to improve
    the performance of a machine learning algorithm. Feature engineering is essential
    in machine learning and is usually where we spend the most effort in solving a
    practical problem.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering usually requires sufficient domain knowledge and can be
    very difficult and time-consuming. In reality, features used to solve a machine
    learning problem are not usually directly available and need to be specifically
    designed and constructed.
  prefs: []
  type: TYPE_NORMAL
- en: When making an investment decision, investors usually look at historical prices
    over a period of time, not just the price the day before. Therefore, in our stock
    price prediction case, we can compute the average close price over the past week
    (five trading days), over the past month, and over the past year as three new
    features. We can also customize the time window to the size we want, such as the
    past quarter or the past six months. On top of these three averaged price features,
    we can generate new features associated with the price trend by computing the
    ratios between each pair of average prices in the three different time frames,
    for instance, the ratio between the average price over the past week and the past
    year.
  prefs: []
  type: TYPE_NORMAL
- en: Besides prices, volume is another important factor that investors analyze. Similarly,
    we can generate new volume-based features by computing the average volumes in
    several different time frames and the ratios between each pair of averaged values.
  prefs: []
  type: TYPE_NORMAL
- en: Besides historical averaged values in a time window, investors also greatly
    consider stock volatility. Volatility describes the degree of variation of prices
    for a given stock or index over time. In statistical terms, it’s basically the
    standard deviation of the close prices. We can easily generate new sets of features
    by computing the standard deviation of close prices in a particular time frame,
    as well as the standard deviation of volumes traded. Similarly, ratios between
    each pair of standard deviation values can be included in our engineered feature
    pool.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, return is a significant financial metric that investors
    closely watch for. Return is the gain or loss percentage of a close price for
    a stock/index in a particular period. For example, daily return and annual return
    are financial terms we frequently hear.
  prefs: []
  type: TYPE_NORMAL
- en: 'They are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_05_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *price*[i] is the price on the *i*^(th) day and *price*[i][-1] is the
    price on the day before. Weekly and monthly returns can be computed similarly.
    Based on daily returns, we can produce a moving average over a particular number
    of days.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, given the daily returns of the past week, *return*[i:i-1], *return*[i-1:i-2],
    *return*[i-2:i-3], *return*[i-3:i-4], and *return*[i-4:i-5], we can calculate
    the moving average over that week as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In summary, we can generate the following predictive variables by applying
    feature engineering techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Generated features (1)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Generated features (2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually, we are able to generate, in total, 31 sets of features, along with
    the following six original features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenPrice[i]: This feature represents the open price'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenPrice[i-1]: This feature represents the open price on the past day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ClosePrice[i-1]: This feature represents the close price on the past day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HighPrice[i-1]: This feature represents the highest price on the past day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LowPrice[i-1]: This feature represents the lowest price on the past day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Volume[i-1]: This feature represents the volume on the past day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acquiring data and generating features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For easier reference, we will implement the code to generate features here rather
    than in later sections. We will start by obtaining the dataset we need for our
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the project, we will acquire stock index price and performance data
    from Yahoo Finance. For example, on the Historical Data [https://finance.yahoo.com/quote/%5EIXIC/history?p=%5EIXIC](https://finance.yahoo.com/quote/%5EIXIC/history?p=%5EIXIC),
    we can change the `Time Period` to `Dec 01, 2005 – Dec10, 2005`, select `Historical
    Prices` in `Show` and `Daily` in `Frequency` (or open this link directly: [https://finance.yahoo.com/quote/%5EIXIC/history?period1=1133395200&period2=1134172800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true](https://finance.yahoo.com/quote/%5EIXIC/history?period1=1133395200&period2=1134172800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true)),
    and then click on the **Apply** button. Click the **Download data** button to
    download the data and name the file `20051201_20051210.csv`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load the data we just downloaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that the output is a pandas DataFrame object. The `Date` column is the
    index column, and the rest of the columns are the corresponding financial variables.
    In the following lines of code, you will see how powerful pandas is at simplifying
    data analysis and transformation on **relational** (or table-like) data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we implement feature generation by starting with a sub-function that
    directly creates features from the original six features, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we develop a sub-function that generates six features related to average
    close prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, a sub-function that generates six features related to average volumes
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the standard deviation, we develop the following sub-function for the
    price-related features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, a sub-function that generates six volume-based standard deviation
    features is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Seven return-based features are generated using the following sub-function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we put together the main feature generation function that calls all
    the preceding sub-functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that the window sizes here are `5`, `21`, and `252`, instead of `7`, `30`,
    and `365`, representing the weekly, monthly, and yearly windows respectively.
    This is because there are 252 (rounded) trading days in a year, 21 trading days
    in a month, and 5 in a week.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply this feature engineering strategy on the NASDAQ Composite data
    queried from 1990 to the first half of 2023, as follows (or directly download
    it from this page: [https://finance.yahoo.com/quote/%5EIXIC/history?period1=631152000&period2=1688083200&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true](https://finance.yahoo.com/quote/%5EIXIC/history?period1=631152000&period2=1688083200&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at what the data with the new features looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command line generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21047_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Printout of the first five rows of the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: Since all the features and driving factors are ready, we will now focus on regression
    algorithms that estimate the continuous target variables based on these predictive
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating with linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first regression model that comes to mind is **linear regression**. Does
    this mean fitting data points using a linear function, as its name implies? Let’s
    explore it.
  prefs: []
  type: TYPE_NORMAL
- en: How does linear regression work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In simple terms, linear regression tries to fit as many of the data points
    as possible, with a straight line in two-dimensional space or a plane in three-dimensional
    space. It explores the linear relationship between observations and targets, and
    the relationship is represented in a linear equation or weighted sum function.
    Given a data sample *x* with *n* features, *x*[1], *x*[2], …, *x*[n] (*x* represents
    a feature vector and *x = (x*[1]*, x*[2]*, …, x*[n]*)*), and weights (also called
    **coefficients**) of the linear regression model *w* (*w* represents a vector
    (*w*[1], *w*[2], …, *w*[n])), the target *y* is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, sometimes the linear regression model comes with an intercept (also called
    bias), *w*[0], so the preceding linear relationship becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Does it look familiar? The **logistic regression** algorithm you learned in
    *Chapter 4*, *Predicting Online Ad Click-Through with Logistic Regression*, is
    just an addition of logistic transformation on top of the linear regression, which
    maps the continuous weighted sum to the *0* (negative) or *1* (positive) class.
    Similarly, a linear regression model, or specifically its weight vector, *w*,
    is learned from the training data, with the goal of minimizing the estimation
    error defined as the **mean squared error** (**MSE**), which measures the average
    of squares of difference between the truth and prediction. Given *m* training
    samples, (*x*^((1)), *y*^((1))), (*x*^((2)), *y*^((2))), … (*x*^((i)), *y*^((i)))…,
    (*x*^((m)), *y*^((m))), the loss function *J(w)* regarding the weights to be optimized
    is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B21047_05_007.png) is the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we can obtain the optimal *w* so that *J*(*w*) is minimized using gradient
    descent. The first-order derivative, the gradient ![](img/B21047_04_027.png),
    is derived as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Combined with the gradient and learning rate ![](img/B21047_05_010.png), the
    weight vector *w* can be updated in each step as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After a substantial number of iterations, the learned *w* is then used to predict
    a new sample *x’*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_012.png)'
  prefs: []
  type: TYPE_IMG
- en: After learning about the mathematical theory behind linear regression, let’s
    implement it from scratch in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a thorough understanding of gradient-descent-based linear
    regression, we’ll implement it from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the function computing the prediction,![](img/B21047_05_013.png),
    with the current weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we continue with the function updating the weight, *w*, with one step
    in a gradient descent manner, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add the function that calculates the loss *J(w)* as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, put all functions together with a model training function by performing
    the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Update the weight vector in each iteration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print out the current cost for every 500 (or it can be any number) iterations
    to ensure cost is decreasing and things are on the right track
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s see how it’s done by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, predict the results of new input values using the trained model as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Implementing linear regression is very similar to logistic regression, as you
    just saw. Let’s examine it with a small example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a linear regression model with `100` iterations, at a learning rate of
    `0.01`, based on intercept-included weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the model’s performance on new samples as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, display, rectangle, square  Description
    automatically generated](img/B21047_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Linear regression on a toy dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The model we trained correctly predicts new samples (depicted by the stars).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it on another dataset, the diabetes dataset from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Train a linear regression model with `5000` iterations, at a learning rate
    of `1`, based on intercept-included weights (the loss is displayed every `500`
    iterations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The estimate is pretty close to the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s utilize scikit-learn to implement linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have used gradient descent in weight optimization, but like with
    logistic regression, linear regression is also open to **Stochastic Gradient Descent**
    (**SGD**). To use it, we can simply replace the `update_weights_gd` function with
    the `update_weights_sgd` function we created in *Chapter 4*, *Predicting Online
    Ad Click-Through with Logistic Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also directly use the SGD-based regression algorithm, `SGDRegressor`,
    from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `''squared_error''` for the `loss` parameter indicates that the cost
    function is MSE; `penalty` is the regularization term, and it can be `None`, `l1`,
    or `l2`, which is similar to `SGDClassifier` in *Chapter 4*, *Predicting Online
    Ad Click-Through with Logistic Regression*, in order to reduce overfitting; `max_iter`
    is the number of iterations; and the remaining two parameters mean the learning
    rate is `0.2` and unchanged during the course of training over, at most, `100`
    iterations. Train the model and output predictions on the testing set of the diabetes
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You can also implement linear regression with TensorFlow. Let’s see this in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression with TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we import TensorFlow and construct the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: It uses a linear layer (or you can think of it as a linear function) to connect
    the input in the `X_train.shape[1]` dimension and the output in the `1` dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we specify the loss function, the MSE, and a gradient descent optimizer,
    `Adam`, with a learning rate of `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we train the model on the diabetes dataset for 100 iterations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This also prints out the loss for every iteration. Finally, we make predictions
    using the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The next regression algorithm you will learn about is decision tree regression.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating with decision tree regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Decision tree regression** is also called a **regression tree**. It is easy
    to understand a regression tree by comparing it with its sibling, the classification
    tree, which you are already familiar with. In this section, we will delve into
    employing decision tree algorithms for regression tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from classification trees to regression trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In classification, a decision tree is constructed by recursive binary splitting
    and growing each node into left and right children. In each partition, it greedily
    searches for the most significant combination of features and its value as the
    optimal splitting point. The quality of separation is measured by the weighted
    purity of the labels of the two resulting children, specifically via Gini Impurity
    or Information Gain.
  prefs: []
  type: TYPE_NORMAL
- en: 'In regression, the tree construction process is almost identical to the classification
    one, with only two differences because the target becomes continuous:'
  prefs: []
  type: TYPE_NORMAL
- en: The quality of the splitting point is now measured by the weighted MSE of two
    children; the MSE of a child is equivalent to the variance of all target values,
    and the smaller the weighted MSE, the better the split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **average** value of targets in a terminal node becomes the leaf value,
    instead of the majority of labels in the classification tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make sure you understand regression trees, let’s work on a small house price
    estimation example using the **house type** and **number of bedrooms**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, number, font  Description automatically
    generated](img/B21047_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Toy dataset of house prices'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define the MSE and weighted MSE computation functions that will be
    used in our calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the weighted MSE after a split in a node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Test things out by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To build the house price regression tree, we first exhaust all possible feature
    and value pairs, and we compute the corresponding MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The lowest MSE is achieved with the `type, semi` pair, and the root node is
    then formed by this splitting point. The result of this partition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Splitting using (type=semi)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are satisfied with a one-level regression tree, we can stop here by assigning
    both branches as leaf nodes, with the value as the average of the targets of the
    samples included. Alternatively, we can go further down the road by constructing
    the second level from the right branch (the left branch can’t be split further):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'With the second splitting point specified by the `bedroom, 3` pair (whether
    it has at least three bedrooms or not) with the lowest MSE, our tree becomes as
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Splitting using (bedroom>=3)'
  prefs: []
  type: TYPE_NORMAL
- en: We can finish up the tree by assigning average values to both leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing decision tree regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’re clear about the regression tree construction process, it’s time
    for coding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The node splitting utility function we will define in this section is identical
    to what we used in *Chapter 3*, *Predicting Online Ad Click-Through with Tree-Based
    Algorithms*, which separates samples in a node into left and right branches, based
    on a feature and value pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the greedy search function, trying out all the possible splits
    and returning the one with the least weighted MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding selection and splitting process occurs recursively in each of
    the subsequent children. When a stopping criterion is met, the process at a node
    stops, and the mean value of the sample, `targets`, will be assigned to this terminal
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, here is the recursive function, `split`, that links it all together.
    It checks whether any stopping criteria are met and assigns the leaf node if so,
    proceeding with further separation otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The entry point of the regression tree construction is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s test it with a hand-calculated example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that the trained tree is identical to what we constructed by hand,
    we write a function displaying the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have a better understanding of the regression tree after implementing
    it from scratch, we can directly use the `DecisionTreeRegressor` package ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html))
    from scikit-learn. Let’s apply it to an example of predicting California house
    prices. The dataset contains a median house value as the target variable, median
    income, housing median age, total rooms, total bedrooms, population, households,
    latitude, and longitude as features. It was obtained from the StatLib repository
    ([https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html))
    and can be directly loaded using the `sklearn.datasets.fetch_california_housing`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We take the last 10 samples for testing and the rest to train a `DecisionTreeRegressor`
    decision tree, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We then apply the trained decision tree to the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare predictions with the ground truth, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We see the predictions are quite accurate.
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented a regression tree in this section. Is there an ensemble
    version of the regression tree? Let’s see next.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a regression forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Predicting Online Ad Click-Through with Tree-Based Algorithms*,
    we explored **random forests** as an ensemble learning method, by combining multiple
    decision trees that are separately trained and randomly subsampling training features
    in each node of a tree. In classification, a random forest makes a final decision
    by a majority vote of all tree decisions. Applied to regression, a random forest
    regression model (also called a **regression forest**) assigns the average of
    regression results from all decision trees to the final decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will use the regression forest package, `RandomForestRegressor`, from
    scikit-learn and deploy it in our California house price prediction example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You’ve learned about three regression algorithms. So, how should we evaluate
    regression performance? Let’s find out in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating regression performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve covered three popular regression algorithms in depth and implemented
    them from scratch by using several prominent libraries. Instead of judging how
    well a model works on testing sets by printing out the prediction, we need to
    evaluate its performance with the following metrics, which give us better insights:'
  prefs: []
  type: TYPE_NORMAL
- en: The MSE, as I mentioned, measures the squared loss corresponding to the expected
    value. Sometimes, the square root is taken on top of the MSE in order to convert
    the value back into the original scale of the target variable being estimated.
    This yields the **Root Mean Squared Error** (**RMSE**). Also, the RMSE has the
    benefit of penalizing large errors more, since we first calculate the square of
    an error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversely, the **Mean Absolute Error** (**MAE**) measures the absolute loss.
    It uses the same scale as the target variable and gives us an idea of how close
    the predictions are to the actual values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For both the MSE and MAE, the smaller the value, the better the regression model.
  prefs: []
  type: TYPE_NORMAL
- en: R² (pronounced **r squared**) indicates the goodness of the fit of a regression
    model. It is the fraction of the dependent variable variation that a regression
    model is able to explain. It ranges from `0` to `1`, representing from no fit
    to a perfect prediction. There is a variant of R² called **adjusted** R². It adjusts
    for the number of features in a model relative to the number of data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s compute these three measurements on a linear regression model, using
    corresponding functions from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will work on the diabetes dataset again and fine-tune the parameters of
    the linear regression model, using the grid search technique:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We obtain the optimal set of parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We predict the testing set with the optimal model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We evaluate the performance on testing sets based on the MSE, MAE, and R² metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that you’ve learned about three (or four, you could say) commonly used and
    powerful regression algorithms and performance evaluation metrics, let’s utilize
    each of them to solve our stock price prediction problem.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting stock prices with the three regression algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the steps to predict the stock price:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, we generated features based on data from 1990 to the first half of
    2023, and we will now continue to construct the training set with data from 1990
    to 2022 and the testing set with data from the first half of 2023:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All fields in the `dataframe` data except `''close''` are feature columns,
    and `''close''` is the target column. We have `8,061` training samples and each
    sample is `37`-dimensional. We also have `124` testing samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: Time series data often exhibits temporal dependencies, where values at one time
    point are influenced by previous values. Ignoring these dependencies can lead
    to poor model performance. We need to use a train-test split to evaluate models,
    ensuring that the test set contains data from a later time period than the training
    set to simulate real-world forecasting scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first experiment with SGD-based linear regression. Before we train
    the model, you should realize that SGD-based algorithms are sensitive to data
    with features at very different scales; for example, in our case, the average
    value of the `open` feature is around 3,777, while that of the `moving_avg_365`
    feature is 0.00052 or so. Hence, we need to normalize features into the same or
    a comparable scale. We do so by removing the mean and rescaling to unit variance
    with `StandardScaler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We rescale both sets with `scaler`, taught by the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can search for the SGD-based linear regression with the optimal set
    of parameters. We specify `l2` regularization and `5000` maximal iterations and
    we tune the regularization term multiplier, `alpha`, and initial learning rate,
    `eta0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For cross-validation, we need to ensure that the training data in each split
    comes before the corresponding test data, preserving the temporal order of the
    time series. Here, we use the `TimeSeriesSplit` method from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we create a 3-fold time series-specific cross-validator and employ it
    in grid search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the best linear regression model and make predictions of the testing
    samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Measure the prediction performance via R²:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We achieve an R² of `0.959` with a fine-tuned linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: With time series data, there is a risk of overfitting due to the potential complexity
    of temporal patterns. Models may capture noise instead of genuine patterns if
    not regularized properly. We need to apply regularization techniques like L1 or
    L2 regularization to prevent overfitting. Also, when you perform cross-validation
    for hyperparameter tuning, consider using time series-specific cross-validation
    methods to assess model performance while preserving temporal order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, let’s experiment with a decision tree. We tune the maximum depth
    of the tree, `max_depth`; the minimum number of samples required to further split
    a node, `min_samples_split`; and the minimum number of samples required to form
    a leaf node, `min_samples_leaf`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note this may take a while; hence, we use all available CPU cores for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the best regression forest model and make predictions of the testing
    samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Measure the prediction performance as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: An R² of `0.912` is obtained with a tweaked decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we experiment with a random forest. We specify 30 decision trees to
    ensemble and tune the same set of hyperparameters used in each tree, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note this may take a while; hence, we use all available CPU cores for training
    (indicated by `n_jobs=-1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the best regression forest model and make predictions of the testing
    samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Measure the prediction performance as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: An R² of `0.937` is obtained with a tweaked forest regressor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also plot the prediction generated by each of the three algorithms, along
    with the ground truth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A graph with numbers and lines  Description automatically generated](img/B21047_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Predictions using the three algorithms versus the ground truth'
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization is produced by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We’ve built a stock predictor using three regression algorithms individually
    in this section. Overall, linear regression outperforms the other two algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Stock markets are known for their wild swings. Unlike more stable systems or
    a well-defined project in this chapter, stock prices are volatile and influenced
    by complex factors that are hard to quantify. Also, their behavior is not easily
    captured by even the most sophisticated models. Hence, it is notoriously difficult
    to accurately predict the stock market in the real world. This makes it a fascinating
    challenge to explore the capabilities of different machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked on the project of predicting stock (specifically
    stock index) prices using machine learning regression techniques. Regression estimates
    a continuous target variable, as opposed to discrete output in classification
  prefs: []
  type: TYPE_NORMAL
- en: We started with a short introduction to the stock market and the factors that
    influence trading prices. We followed this with an in-depth discussion of three
    popular regression algorithms, linear regression, regression trees, and regression
    forests. We covered their definitions, mechanics, and implementations from scratch
    with several popular frameworks, including scikit-learn and TensorFlow, along
    with applications on toy datasets. You also learned the metrics used to evaluate
    a regression model. Finally, we applied what was covered in this chapter to solve
    our stock price prediction problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue working on the stock price prediction
    project, but with powerful **neural networks**. We will see whether they can beat
    what we have achieved with the three regression models in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, can you add more signals to our stock prediction system, such
    as the performance of other major indexes? Does this improve prediction?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to ensemble those three regression models, for example, by averaging the
    predictions, and see whether you can perform better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code187846872178698968.png)'
  prefs: []
  type: TYPE_IMG

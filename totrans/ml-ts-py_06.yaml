- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Models for¬†Time-Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning has come a long way in recent years, and this is reflected
    in the methods available to time-series predictions. We've introduced a few state-of-the-art
    machine learning methods for time-series in *Chapter 4*, *Introduction to Machine
    Learning for Time-Series*. In the current chapter, we'll introduce several more
    machine learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: We'll go through methods that are commonly used as baseline methods, or that
    stand out in terms of either performance, ease of use, or their applicability.
    I'll introduce k-nearest neighbors with dynamic time warping and gradient boosting
    for time-series as a baseline and we'll go over other methods, such as Silverkite
    and gradient boosting. Finally, we'll go through an applied exercise with some
    of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: More machine learning methods for time-series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-nearest neighbors with dynamic time warping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silverkite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python exercise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are looking for a discussion of state-of-the-art machine learning algorithms,
    please refer to *Chapter 4*, *Introduction to Machine Learning for Time-Series*.
    The discussion of algorithms will assume some of the information of that chapter.
    The algorithms that we'll cover in the next sections are all highly competitive
    for forecasting and prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss algorithms here in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: More machine learning methods for time-series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The algorithms that we'll cover in this section are all highly competitive for
    forecasting and prediction tasks. If you are looking for a discussion of state-of-the-art
    machine learning algorithms, please refer to *Chapter 4*, *Introduction to Machine
    Learning for Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: In the aforementioned chapter, we've briefly discussed a few of these algorithms,
    but¬†we'll discuss them here in more detail and we will also introduce other algorithms
    that we haven't discussed before, such as Silverkite, gradient boosting, and k-nearest
    neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: We'll dedicate a separate practice section to a library that was released in
    2021, which¬†is facebook's Kats. Kats provides many advanced features, including
    hyperparameter tuning and ensemble learning. On top of these features, they implement
    feature extraction based on the TSFresh library and include many models, including
    Prophet, SARIMA, and others. They claim that their hyperparameter tuning for time-series
    is about 6-20 times faster in benchmarks compared with other¬†hyperparameter tuning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This graph provides an overview of the popularity of selected time-series machine
    learning libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![more_machine_learning-star_history.png](img/B17577_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Popularity of time-series machine learning libraries'
  prefs: []
  type: TYPE_NORMAL
- en: As of mid-2021, Kats and GreyKite have been released very recently, and although
    they have been garnering stars on GitHub, they haven't accumulated enough to rival
    TSFresh's popularity. I've included TSFresh even though it is a library for feature
    generation, and not prediction. I found it interesting to see how important it
    is in relation to other libraries that we use in this chapter. After TSFresh,
    SKTime is second, and it has been attracting a lot of stars over a relatively
    short time period.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use a few of these libraries in the practical examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Another important issue is validation, and it's worth covering this separately.
  prefs: []
  type: TYPE_NORMAL
- en: Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've discussed validation before in *Chapter 4*, *Introduction to Machine Learning
    for Time-Series*. Often, in machine learning tasks, we use k-fold cross-validation,
    where splits of the data are performed pseudo-randomly, so the training and the
    test/validation datasets can come from any part of the data as long as it hasn't
    been used for training (**out-of-sample data**).
  prefs: []
  type: TYPE_NORMAL
- en: With time-series data, this way of validation can lead to an overconfidence
    in the model's performance because, realistically, time-series tend to change
    over time according to trend, seasonality, and changes to the time-series characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, with time-series, validation is often performed in a so-called **walk-forward
    validation**. This means that we train the model on past data, and we'll test
    it on the newest slice of data. This will take out the optimistic bias and give
    us a more realistic estimate of performance once the model is deployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of training, validation, and test datasets, this means that we''ll
    adjust model parameters entirely on training and validation datasets, and we''ll
    benchmark our test based on a set of data that''s more advanced in time, as illustrated
    in the following diagram (source: Greykite library''s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 20)/Screenshot 2021-06-06 at 21.39.47.png](img/B17577_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Walk-forward validation'
  prefs: []
  type: TYPE_NORMAL
- en: In walk-forward validation, we train on an initial segment of the data and then
    test on a period after the training set. Next, we roll forward and repeat the
    process. This way, we have multiple out-of-sample periods and can combine the
    results over these periods. With walk-forward, we are less likely to suffer from
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbors with dynamic time warping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-nearest neighbors is a well-known machine learning method (sometimes also
    going under the guise of case-based reasoning). In kNN, we can use a distance
    measure to find similar data points. We can then take the known labels of these
    nearest neighbors as the output and integrate them in some way using a function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.3* illustrates the basic idea of kNN for classification (source ‚Äì
    WikiMedia Commons: [https://commons.wikimedia.org/wiki/File:KnnClassification.svg](https://commons.wikimedia.org/wiki/File:KnnClassification.svg)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![/Users/ben/Downloads/Machine-Learning for Time-Series with Python/knn.png](img/B17577_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: K-nearest neighbor for classification'
  prefs: []
  type: TYPE_NORMAL
- en: We know a few data points already. In the preceding illustration, these points
    are indicated as squares and triangles, and they represent data points of two
    different classes, respectively. Given a new data point, indicated by a circle,
    we find the closest known data points to it. In this example, we find that the
    new point is similar to triangles, so we might assume that the new point is of
    the triangle class as well.
  prefs: []
  type: TYPE_NORMAL
- en: While this method is conceptually very simple, it often serves as a strong baseline
    method, or is sometimes even competitive with more sophisticated machine learning
    algorithms, even when we only compare the closest neighbor *(**ùëò**=1)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important hyperparameters in this algorithm are:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of neighbors (k) you want to base your output on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The integration function (for example, the average or the most frequent value)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance function to use to find the nearest data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We talked about dynamic time warping in *Chapter 4*, *Introduction to Machine
    Learning for Time-Series*, as a measure that can be used to compare the similarity
    (or, equivalently, the distance) between two time-series. These sequences can
    even be of different lengths. Dynamic time warping has proven itself to be an
    exceptionally strong distance measure for time-series.
  prefs: []
  type: TYPE_NORMAL
- en: We can use kNN in combination with dynamic time warping as a distance measure
    to find similar time-series, and this method has proven itself hard to beat, although
    the state of the art has since surpassed it.
  prefs: []
  type: TYPE_NORMAL
- en: Silverkite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Silverkite algorithm ships together with the Greykite library released by
    LinkedIn. It was explicitly designed with the goals in mind of being fast, accurate,
    and intuitive. The algorithm is described in a 2021 publication ("*A flexible
    forecasting model for production systems*", by Reza Hosseini and others).
  prefs: []
  type: TYPE_NORMAL
- en: According to LinkedIn, it can handle different kinds of trends and seasonalities
    such as hourly, daily, weekly, repeated events, and holidays, and short-range
    effects. Within LinkedIn, it is used for both short-term, for example, a 1-day
    head, and long-term forecast horizons, such as 1 year ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases within LinkedIn include optimizing budget decisions, setting business
    metric targets, and providing sufficient infrastructure to handle peak traffic.
    Furthermore, a use case has been to model recoveries from the COVID-19 pandemic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time-series is modeled as an additive composite of trends, change points,
    and seasonality, where seasonality includes holiday/event effects. The trend is
    then modeled as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_07_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where K is the number of change points, and *t*[i] is the time index for the
    i-th change point. Therefore, ![](img/B17577_07_002.png) is an indicator function
    for the i-th change point. The function f(t) can be linear, square root, quadratic,
    any combination, or completely custom.
  prefs: []
  type: TYPE_NORMAL
- en: Silverkite also constructs indicator variables for holidays. Holidays can be
    specified by name or by country, or can even be completely custom.
  prefs: []
  type: TYPE_NORMAL
- en: Change points can be specified manually or candidates can be automatically detected
    with a regression model and subsequently selected using the Adaptive Lasso algorithm
    (Hui Zhou, 2006).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to trend, seasonality, and holiday, Silverkite includes an autoregressive
    term that is calculated based on windowed averages rather than taking lags independently
    ("*Selecting a binary Markov model for a precipitation process*", by Reza Hosseini
    and others, 2011).
  prefs: []
  type: TYPE_NORMAL
- en: 'This autoregressive term is specified using the Pasty library, using a formula
    mini-language, in a form like this string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this formula, y on the left-hand side is defined as the sum of three terms,
    `a`, `a:b`, and `np.log(x)`. The term `a:b` is an interaction between two factors,
    a and b. The model template itself in Pasty is highly customizable, so this interface
    provides a high degree of flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Silverkite comes with several model types, such as ridge regression,
    elastic net, and boosted trees, with supported loss functions, MSE, and quantile
    loss for robust regression.
  prefs: []
  type: TYPE_NORMAL
- en: According to a LinkedIn benchmark on several datasets, Silverkite outperforms
    both auto-Arima (the pmdarima library) and Prophet in terms of prediction error.
    Yet, Silverkite was about four times as fast as Prophet, which we'll introduce
    in *Chapter 9*, *Probabilistic Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**XGBoost** (short for **eXtreme Gradient Boosting**) is an efficient implementation
    of gradient boosting (Jerome Friedman, "*Greedy function approximation: a gradient
    boosting machine*", 2001) for classification and regression problems. Gradient
    boosting is also known as **Gradient Boosting Machine** (**GBM**) or **Gradient
    Boosted Regression Tree** (**GBRT**). A special case is LambdaMART for ranking
    applications. Apart from XGBoost; other implementations are Microsoft''s Light
    Gradient Boosting Machine (LightGBM), and Yandex''s Catboost.'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosted Trees is an ensemble of trees. This is similar to Bagging algorithms
    such as Random Forest; however, since this is a boosting algorithm, each tree
    is computed to incrementally reduce the error. With each new iteration a tree
    is greedily chosen and its prediction is added to the previous predictions based
    on a weight term. There is also a regularization term that penalizes complexity
    and reduces overfitting, similar to the Regularized Greedy Forest (RGF).
  prefs: []
  type: TYPE_NORMAL
- en: 'The **XGBoost** algorithm was published in 2016 by Tianqi Chen and Carlos Guestrin
    ("*XGBoost: A Scalable Tree Boosting System*") and pushed the envelope on many
    classification and regression benchmarks. It was used in many winning solutions
    to Kaggle problems. In fact, in 2015, of the 29 challenge-winning solutions, 17
    solutions used XGBoost.'
  prefs: []
  type: TYPE_NORMAL
- en: It was designed to be highly scalable and features extensions of the gradient
    boosting algorithm for weighted quantiles, along with improvements for scalability
    and parallelization based on smarter caching patterns, sharding, and the handling
    of sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: As a special case of regression, XGBoost can be used for forecasting. In this
    scenario, the model is trained based on past values to predict future values,
    and this can be applied to univariate as well as multivariate time-series.
  prefs: []
  type: TYPE_NORMAL
- en: Python exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's put into practice what we've learned in this chapter so far.
  prefs: []
  type: TYPE_NORMAL
- en: As for requirements, in this chapter, we'll be installing requirements for each
    section separately. The installation can be performed from the terminal, the notebook,
    or from the anaconda navigator.
  prefs: []
  type: TYPE_NORMAL
- en: In a few of the following sections, we'll demonstrate classification in a forecast,
    so some of these approaches will not be comparable. The reader is invited to do
    forecasts and classification using each approach and then compare results.
  prefs: []
  type: TYPE_NORMAL
- en: As a note of caution, both Kats and Greykite (at the time of writing) are very
    new libraries, so there might still be frequent changes to dependencies. They
    might pin your NumPy version or other commonly used libraries. Therefore, I'd
    recommend you install them in virtual environments separately for each section.
  prefs: []
  type: TYPE_NORMAL
- en: We'll go through this setup in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a Python virtual environment, all libraries, binaries, and scripts installed
    into it are¬†isolated from those installed in other virtual environments and from
    those installed in the system. This means that we can have different libraries,
    such as Kats and Greykite, installed without having to bother with compatibility
    issues between them or with other libraries installed on our computer.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through a quick tutorial introduction to using virtual environments
    with Jupyter notebooks using anaconda (similarly, you can use tools such as virtualenv
    or¬†pipenv).
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 1*, *Introduction to Time-Series with Python*, we went through the
    installation of Anaconda, so we'll skip the installation. Please refer to that
    chapter or¬†go to conda.io for instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a virtual environment, you have to specify a name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will create an eponymous directory (`myenv`), where all libraries and scripts
    will be installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to use this environment, we have to activate it first, which means
    that we set the `PATH` variable to include our newly created directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can now use tools such as pip, which will default to the one bundled with
    conda, or the conda command directly to install libraries.
  prefs: []
  type: TYPE_NORMAL
- en: We can install Jupyter or Jupyter labs into our environment and then start it.
    This means that our Jupyter environment will include all dependencies as we've
    installed them in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a kNN algorithm with dynamic time warping. As I've mentioned,
    this¬†algorithm often serves as a decent baseline for comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbors with dynamic time warping in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll classify failures from force and torque measurements
    of a robot over time.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use a very simple classifier, kNN, and perhaps we should give a heads-up
    that this method involves taking point-wise distances, which can often be a bottleneck
    for computations.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll combine TSFresh's feature extraction in a pipeline with
    a kNN algorithm. The time-series pipeline can really help make things easy, as
    you'll find when reading the code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install tsfresh and tslearn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We'll use the kNN classifier in tslearn. We could even have used the kNN classifier
    in scikit-learn, which allows a custom metric to be specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example, we will download a dataset of robotic execution failures from
    the UCI machine learning repository and store it locally. This dataset contains
    force and torque measurements on a robot after failure detection. For each sample,
    the task is to classify whether the robot will report a failure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The columns include the time and six time-series with signals from the sensors,
    `F_x`, `F_y`, `F_z`, `T_x`, `T_y`, and `T_z`. The target variable, `y`, which
    can take the values True or False, indicates if there was a failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s always important to check the frequency of the two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The mean of y is 0.24.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then extract time-series features using TSFresh, as discussed in *Chapter
    3, Preprocessing Time-Series*. We can impute missing values and select features
    based on relevance to the target. In TSFresh, the p-value from a statistical test
    is used to calculate the feature significance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can continue working with the `features_filtered` DataFrame, which contains
    our features ‚Äì sensor signals from before and TSFresh features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s find some good values for the number of neighbors by doing a grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We are using scikit-learn's `TimeSeriesSplit` to split the time-series. This
    is for the GridSearch.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we could have just split based on an index.
  prefs: []
  type: TYPE_NORMAL
- en: There are many parameters we could have tried, especially for the distance metric
    in¬†the kNN classifier. If you want to have a play with this, please see `TSLEARN_VALID_METRICS`
    for a complete list of metrics supported by tslearn.
  prefs: []
  type: TYPE_NORMAL
- en: Let's do a few forecasts of COVID cases. In the next section, we'll start with
    the Silverkite algorithm. Silverkite comes with the Greykite library released
    by LinkedIn¬†in 2021\.
  prefs: []
  type: TYPE_NORMAL
- en: Silverkite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing, Greykite is in version 0.1.1 ‚Äì it's not fully stable
    yet. Its dependencies might conflict with newer versions of commonly used libraries,
    including Jupyter Notebooks. Do not worry though if you install the library in
    your¬†virtual environment or on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just go ahead and install the library with all its dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that Greykite is installed, we can use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll load up the COVID cases from the *Our World in Data* dataset, probably
    one of the best sources of available COVID data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We are concentrating on cases in France.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by setting up the Greykite metadata parameters. We''ll then pass this
    object into the forecaster configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Our time column is `date` and our value column is `new_cases`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll now create the `forecaster` object, which creates forecasts and stores
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The forecast horizon is 90 days; we will forecast 90 days ahead. Our prediction
    interval is 95%. Both Silverkite and Prophet support quantifying uncertainty by
    means of prediction intervals. A coverage of 95% means that 95% of actuals should
    fall within the prediction interval. In Greykite, the _`components.uncertainty`
    model provides additional configuration options about uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: I've added a line to ignore warnings of the `UserWarning` type during training
    since otherwise, there are about 500 lines of warnings about 0s in the target
    column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot the original time-series from the result object. We can overlay
    our forecasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Please leave out the `renderer` argument if you are not on Google Colab!
  prefs: []
  type: TYPE_NORMAL
- en: 'We get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Forecast versus actual time-series (Silverkite)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The forecasts are in the `df` attribute of the `forecast` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the upper and lower confidence intervals of the forecasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_wsZ8St/Screenshot
    2021-08-30 at 21.25.06.png](img/B17577_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Table of forecast versus actual time-series (Silverkite)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We might want to get some performance metrics for our model. We can get the
    performance of the historical forecast on the holdout test set like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Our performance metrics look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_UgQpeb/Screenshot
    2021-08-30 at 21.28.22.png](img/B17577_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Performance metrics on the hold-out data (Silverkite)'
  prefs: []
  type: TYPE_NORMAL
- en: I've truncated the metrics to the first five.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply our model conveniently to new data like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The predictions look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_LxjFCi/Screenshot
    2021-08-30 at 21.31.08.png](img/B17577_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Forecast DataFrame (Silverkite)'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that your result might vary.
  prefs: []
  type: TYPE_NORMAL
- en: We can use other forecaster models by changing the `model_template` argument
    in the run configuration of the forecaster. For instance, we could set it to `ModelTemplateEnum.PROPHET.name`
    in order to take Facebook's Prophet model.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our tour of Silverkite. Next, we will forecast by applying a
    supervised regression method with XGBoost. Let's do some gradient boosting!
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use supervised machine learning for time-series forecasting as well.
    For this, we can use the dates and the previous values to predict the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install XGBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We'll use the Yahoo daily closing data in this example, as in other practice
    sections of¬†this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through the preparation and modeling step by step.
  prefs: []
  type: TYPE_NORMAL
- en: We first need to featurize the data. Here, we'll do this by extracting date
    features, but please see the section on kNNs, where TSFresh's feature extraction
    is used instead. You might want to change this example by combining the two feature
    extraction strategies or by relying on TSFresh entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will reload the new COVID cases from the *Our World in Data* dataset as
    before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For feature extraction, transformers are handy. A transformer is basically
    a class with `fit()` and `transform()` methods that can make the transformer adapt
    to a dataset and transform the data accordingly. Here''s the code for the `DateFeatures`
    transformer that annotates a dataset according to a date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This transformer is relatively simple in that it extracts a range of features
    for a date column such as hours, years, days, weekday, months, week of year, and
    quarter. These features can potentially be very powerful for describing or annotating
    the time-series data in a machine learning context.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete code for this example on GitHub. I am providing an
    additional transformer for cyclical features there that are omitted from this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the transformers as follows to the `date` column of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `remainder="passthrough"` argument is set in case we want to provide additional
    exogenous features for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a pipeline of these preprocessing steps together with a model
    so that it can be fitted and applied to prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The predictor is an XGBoost regressor. I didn't make much of an effort in terms
    of tweaking. The only parameter that we'll change is the number of estimators.
    We'll use an ensemble size (number of trees) of 1,000\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to split the dataset into training and test sets. This includes
    two issues:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to align the features with values ahead of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to split the dataset into two by a cutoff time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s first set the basic parameters for this. First, we want to predict into
    the future given a time horizon. Second, we need to decide how many data points
    we use for training and for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We take 90% of points for training, and we predict 90 days into the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This does both the alignment and horizon. Therefore, we have the datasets for
    testing and training, both with features and labels that we want to predict with
    XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can train our XGBoost regression model to predict values within our HORIZON
    into the future based on the features we produced with our transformer and the
    current values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can fit our pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the following pipeline parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_VPuhp5/Screenshot
    2021-08-30 at 23.08.52.png](img/B17577_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Pipeline parameters'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we create a series of the dates from beginning to end, we can get the predictions
    of the model for the whole time period:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict()` method of the pipeline applied to `X_test` gives us the forecast:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do the same for the actual cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can contrast the forecast with the actual values, `y_test`, in a plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the plot we are getting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_Vnji3v/Screenshot
    2021-08-30 at 23.29.17.png](img/B17577_07_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Forecast versus actual (XGBoost)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extract performance metrics over the test period like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We should be seeing something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll create an ensemble model for time-series forecasting in Kats.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles with Kats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Kats installation should be very easy in two steps. First, we install fbprophet,
    an old version of Facebook''s Prophet library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we install Kats with pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, on Colab, we can install Kats like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll load the COVID cases dataset as before. Here''s just the last line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We'll configure our ensemble model, fit it, and then do a forecast.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the configuration of our ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, we include only two different models, but we could have included other
    and more models, and we could have defined better parameters. This is an example
    only; for a more realistic exercise, which I leave to the reader, I'd suggest
    adding ARIMA and Theta models. We need to define hyperparameters for each forecasting
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to create ensemble parameters that define how the ensemble aggregate
    is to be calculated and how the decomposition should work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To use a time-series with Kats, we have to convert our data from a DataFrame
    or series to a Kats time-series object. We can convert our COVID case data as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: What is important for the conversion is the fact that Kats can infer the frequency
    of the index. This can be tested with `pd.infer_freq()`. In our case, `pd.infer_freq(df["date"])`
    should return `D` for daily frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can create our `KatsEnsemble` and fit it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can get separate predictions for each model using the `predict()` method.
    If we want to get the ensemble output, we have to call `aggregate()` after `predict()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We predict 90 days ahead. These predictions are stored as part of the model,
    so we don't need to capture the returned forecast. We can then aggregate the forecast
    from each model. Again, we don't need to get the returned DataFrame because this
    is stored inside the model object (`m.fcst_df`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, we plot the aggregated DataFrame using a Kats convenience function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_Ou1y0x/Screenshot
    2021-08-30 at 23.38.36.png](img/B17577_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Kats ensemble model forecast'
  prefs: []
  type: TYPE_NORMAL
- en: Since we can tweak this ensemble model by changing the base model parameter
    and adding new models, this can give us lots of room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: It's time to conclude this chapter with a summary of what we've learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve discussed popular time-series machine learning libraries
    in Python. We then discussed and tried out a k-nearest neighbor algorithm with
    dynamic time warping for the classification of robotic failures. We talked about
    validation in time-series forecasting and we tried three different methods for
    forecasting COVID cases: Silverkite, Gradient Boosting with XGBoost, and ensemble¬†models
    in Kats.'
  prefs: []
  type: TYPE_NORMAL

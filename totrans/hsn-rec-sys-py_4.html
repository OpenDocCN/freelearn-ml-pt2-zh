<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Content-Based Recommenders</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we built an IMDB Top 250 clone (a type of simple recommender) and a knowledge-based recommender that suggested movies based on timeline, genre, and duration. However, these systems were extremely primitive. The simple recommender did not take into consideration an individual user's preferences. The knowledge-based recommender did take account of the user's preference for genres, timelines, and duration, but the model and its recommendations still remained very generic.</p>
<p>Imagine that Alice likes the movies <em>The Dark Knight, </em><em>Iron Man</em>,<em> </em>and <em>Man of Steel. </em>It is pretty evident that Alice has a taste for superhero movies. However, our models from the previous chapter would not be able to capture this detail. The best it could do is suggest <em>action </em>movies (by making Alice input <em>action </em>as the preferred genre), which is a superset of superhero movies.</p>
<p class="mce-root CDPAlignCenter CDPAlign CDPAlignLeft">It is also possible that two movies have the same genre, timeline, and duration characteristics, but differ hugely in their audience. Consider <em>The</em> <em>Hangover </em>and <em>Forgetting Sarah Marshall, </em>for example. Both these movies were released in the first decade of the 21st century, both lasted around two hours, and both were comedies. However, the kind of audience that enjoyed these movies was very different.</p>
<p>An obvious fix to this problem is to ask the user for more metadata as input. For instance, if we introduced a <em>sub-genre </em>input, the user would be able to input values such as <em>superhero, black comedy,</em> and <em>romantic comedy, </em>and obtain more appropriate results, but this solution suffers heavily from the perspective of usability.</p>
<p>The first problem is that we do not possess data on <em>sub-genres.</em> Secondly, even if we did, our users are extremely unlikely to possess knowledge of their favorite movies' metadata. Finally, even if they did, they would certainly not have the patience to input it into a long form. Instead, what they would be more willing to do is tell you the movies they like/dislike and expect recommendations that match their tastes.</p>
<p>As we discussed in the first chapter, this is exactly what sites like Netflix do. When you sign up on Netflix for the first time, it doesn't have any information about your tastes for it to build a profile, leverage the power of its community, and give you recommendations with (a concept we'll explore in later chapters). Instead, what it does is ask you for a few movies you like and show you results that are most similar to those movies.</p>
<p>In this chapter, we are going to build two types of content-based recommender:</p>
<ul>
<li><strong>Plot description-based recommender: </strong>This model compares the descriptions and taglines of different movies, and provides recommendations that have the most similar plot descriptions.</li>
<li><strong>Metadata-based recommender: </strong>This model takes a host of features, such as genres, keywords, cast, and crew, into consideration and provides recommendations that are the most similar with respect to the aforementioned features.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will be required to have Python installed on a system. Finally, to use the Git repository of this book, the user needs to install Git.</p>
<p>The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Hands-On-Recommendation-Systems-with-Python">https://github.com/PacktPublishing/Hands-On-Recommendation-Systems-with-Python</a><a href="https://github.com/PacktPublishing/Hands-On-Recommendation-Systems-with-Python">.</a></p>
<p>Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2LOcac2">http://bit.ly/2LOcac2</a><a href="http://bit.ly/2LOcac2">.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exporting the clean DataFrame</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we performed a series of data wrangling and cleaning processes on our metadata in order to convert it into a form that was more usable. To avoid having to perform these steps again, let's save this cleaned DataFrame into a CSV file. As always, doing this with pandas happens to be extremely easy.</p>
<p>In the knowledge recommender<em> </em>notebook from Chapter 4<em>, </em>enter the following code in the last cell:</p>
<pre>#Convert the cleaned (non-exploded) dataframe df into a CSV file and save it in the data folder<br/>#Set parameter index to False as the index of the DataFrame has no inherent meaning.<br/>df.to_csv('../data/metadata_clean.csv', index=False)</pre>
<p>Your <kbd>data</kbd><em> </em>folder should now contain a new file, <kbd>metadata_clean.csv</kbd>.</p>
<p>Let's create a new folder, <kbd>Chapter 4</kbd><em>, </em>and open a new Jupyter Notebook within this folder. Let's now import our new file into this Notebook:</p>
<pre>import pandas as pd<br/>import numpy as np<br/><br/>#Import data from the clean file <br/>df = pd.read_csv('../data/metadata_clean.csv')<br/><br/>#Print the head of the cleaned DataFrame<br/>df.head()</pre>
<p>The cell should output a DataFrame that is already clean and in the desired form.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Document vectors</h1>
                </header>
            
            <article>
                
<p>Essentially, the models we are building compute the pairwise similarity between bodies of text. But how do we numerically quantify the similarity between two bodies of text?</p>
<p>To put it another way, consider three movies: A, B, and C. How can we mathematically prove that the plot of A is more similar to the plot of B than to that of C (or vice versa)? </p>
<p>The first step toward answering these questions is to represent the bodies of text (henceforth referred to as documents)<em> </em>as mathematical quantities. This is done by representing these documents as vectors<em>. </em>In other words, every document is depicted as a series of <em>n </em>numbers, where each number represents a dimension and <em>n </em>is the size of the vocabulary of all the documents put together.</p>
<p>But what are the values of these vectors? The answer to that question depends on the <em>vectorizer </em>we are using to convert our documents into vectors. The two most popular vectorizers are CountVectorizer and TF-IDFVectorizer<em>.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CountVectorizer</h1>
                </header>
            
            <article>
                
<p>CountVectorizer is the simplest type of vectorizer and is best explained with the help of an example. Imagine that we have three documents, A, B, and C, which are as follows: </p>
<ul>
<li><strong>A</strong>: The sun is a star.</li>
<li><strong>B</strong>: My love is like a red, red rose</li>
<li><strong>C</strong>: Mary had a little lamb</li>
</ul>
<p>We now have to convert these documents into their vector forms using CountVectorizer. The first step is to compute the size of the vocabulary. The vocabulary is the number of unique words present across all documents. Therefore, the vocabulary for this set of three documents is as follows: the, sun, is, a, star, my, love, like, red, rose, mary, had, little, lamb. Consequently, the size of the vocabulary is 14.</p>
<p>It is common practice to not include extremely common words such as a, the, is, had, my, and so on (also known as stop words) in the vocabulary. Therefore, eliminating the stop words, our vocabulary, <em>V,</em> is as follows:</p>
<p style="padding-left: 30px"><strong>V</strong>: like, little, lamb, love, mary, red, rose, sun, star</p>
<p>The size of our vocabulary is now nine. Therefore, our documents will be represented as nine-dimensional vectors, and each dimension here will represent the number of times a particular word occurs in a document. In other words, the first dimension will represent the number of times like<em> </em>occurs, the second will represent the number of times little occurs, and so on.</p>
<p>Therefore, using the CountVectorizer approach, A, B, and C will now be represented as follows:</p>
<ul>
<li><strong>A</strong>: (0, 0, 0, 0, 0, 0, 0, 1, 1)</li>
<li><strong>B</strong>: (1, 0, 0, 1, 0, 2, 1, 0, 0)</li>
<li><strong>C</strong>: (0, 1, 1, 0, 1, 0, 0, 0, 0)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TF-IDFVectorizer</h1>
                </header>
            
            <article>
                
<p>Not all words in a document carry equal weight. We already observed this when we eliminated the stop words from our vocabulary altogether. But the words that were in the vocabulary were all given equal weighting.</p>
<p>But should this always be the case?</p>
<p>For example, consider a corpus of documents on dogs. Now, it is obvious that all these documents will frequently contain the word dog. Therefore, the appearance of the word <em>dog </em>isn't as important as another word that only appears in a few documents.</p>
<p><strong>TF-IDF</strong><strong>Vectorizer</strong> <span>(</span><strong>Term Frequency-Inverse Document Frequency</strong><span>)</span>takes the aforementioned point into consideration and assigns weights to each word according to the following formula. For every word <em>i </em>in document <em>j</em>, the following applies:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3d0955be-dfd1-488a-9288-d0a0c83922e3.png" style="width:10.25em;height:1.75em;"/></div>
<p>In this formula, the following is true:</p>
<ul>
<li><em>w</em><sub><em>i, j</em> </sub>is the weight of word <em>i </em>in document <em>j<br/></em></li>
<li><em>df<sub>i</sub> </em>is the number of documents that contain the term <em>i<br/></em></li>
<li><em>N </em>is the total number of documents</li>
</ul>
<p>We won't go too much into the formula and the associated calculations. Just keep in mind that the weight of a word in a document is greater if it occurs more frequently in that document and is present in fewer documents. The weight <em>w</em><sub><em>i,j</em> </sub>takes values between <kbd>0</kbd> and <kbd>1</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b02a313f-7888-4ddd-8398-411cfa8e712f.png" style="width:49.42em;height:24.92em;"/></div>
<p>We will be using TF-IDFVectorizer because some words (pictured in the preceding word cloud) occur much more frequently in plot descriptions than others. It is therefore a good idea to assign weights to each word in a document according to the TF-IDF formula.</p>
<p>Another reason to use TF-IDF is that it speeds up the calculation of the cosine similarity score between a pair of documents. We will discuss this point in greater detail when we implement this in code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The cosine similarity score</h1>
                </header>
            
            <article>
                
<p>We will discuss similarity scores in detail in <a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank">Chapter 5</a>, <em>Getting Started with Data Mining Techniques</em>. Presently, we will make use of the <em>cosine similarity </em>metric to build our models. The cosine score is extremely robust and easy to calculate (especially when used in conjunction with TF-IDFVectorizer).</p>
<p>The cosine similarity score between two documents, <em>x </em>and <em>y, </em>is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/848e2bba-80ee-4352-94ed-ddbc3997b56a.png" style="width:11.33em;height:2.83em;"/></div>
<p>The cosine score can take any value between -1 and 1. The higher the cosine score, the more similar the documents are to each other. We now have a good theoretical base to proceed to build the content-based recommenders using Python.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plot description-based recommender</h1>
                </header>
            
            <article>
                
<p>Our plot description-based recommender will take in a movie title as an argument and output a list of movies that are most similar based on their plots. These are the steps we are going to perform in building this model:</p>
<ol>
<li>Obtain the data required to build the model</li>
<li>Create TF-IDF vectors for the plot description (or overview) of every movie</li>
<li>Compute the pairwise cosine similarity score of every movie</li>
<li>Write the recommender function that takes in a movie title as an argument and outputs movies most similar to it based on the plot</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>In its present form, the DataFrame, although clean, does not contain the features that are required to build the plot description-based recommender. Fortunately, these requisite features are available in the original metadata file.</p>
<p>All we have to do is import them and add them to our DataFrame:</p>
<pre>#Import the original file<br/>orig_df = pd.read_csv('../data/movies_metadata.csv', low_memory=False)<br/><br/>#Add the useful features into the cleaned dataframe<br/>df['overview'], df['id'] = orig_df['overview'], orig_df['id']<br/><br/>df.head()</pre>
<p>The DataFrame should now contain two new features: <kbd>overview</kbd> and <kbd>id</kbd>.<em> </em>We will use <kbd>overview</kbd><em> </em>in building this model and <kbd>id</kbd><em> </em>for building the next.</p>
<p>The <kbd>overview</kbd><em> </em>feature consists of strings and, ideally, we should clean them up by removing all punctuation and converting all the words to lowercase. However, as we will see shortly, all this will be done for us automatically by <kbd>scikit-learn</kbd><em>, </em>the library we're going to use heavily in building the models in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the TF-IDF matrix</h1>
                </header>
            
            <article>
                
<p>The next step is to create a DataFrame where each row represents the TF-IDF vector of the <kbd>overview</kbd> feature of the corresponding movie in our main DataFrame. To do this, we will use the <kbd>scikit-learn</kbd><em> </em>library, which gives us access to a TfidfVectorizer object to perform this process effortlessly:</p>
<pre>#Import TfIdfVectorizer from the scikit-learn library<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/><br/>#Define a TF-IDF Vectorizer Object. Remove all english stopwords<br/>tfidf = TfidfVectorizer(stop_words='english')<br/><br/>#Replace NaN with an empty string<br/>df['overview'] = df['overview'].fillna('')<br/><br/>#Construct the required TF-IDF matrix by applying the fit_transform method on the overview feature<br/>tfidf_matrix = tfidf.fit_transform(df['overview'])<br/><br/>#Output the shape of tfidf_matrix<br/>tfidf_matrix.shape<br/><br/><strong>OUTPUT:<br/>(45466, 75827)</strong></pre>
<p>We see that the vectorizer has created a 75,827-dimensional vector for the overview of every movie. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing the cosine similarity score</h1>
                </header>
            
            <article>
                
<p>The next step is to calculate the pairwise cosine similarity score of every movie. In other words, we are going to create a 45,466 × 45,466 matrix, where the cell in the <em>i<sup>th </sup></em>row and <em>j<sup>th</sup> </em>column represents the similarity score between movies <em>i </em>and <em>j. </em>We can easily see that this matrix is symmetric in nature and every element in the diagonal is 1, since it is the similarity score of the movie with itself.</p>
<p>Like TF-IDFVectorizer, <kbd>scikit-learn</kbd><em> </em>also has functionality for computing the aforementioned similarity matrix. Calculating the cosine similarity is, however, a computationally expensive process. Fortunately, since our movie plots are represented as TF-IDF vectors, their magnitude is always 1. Hence, we do not need to calculate the denominator in the cosine similarity formula as it will always be 1. Our work is now reduced to computing the much simpler and computationally cheaper dot product (a functionality that is also provided by <kbd>scikit-learn</kbd>):</p>
<pre># Import linear_kernel to compute the dot product<br/>from sklearn.metrics.pairwise import linear_kernel<br/><br/># Compute the cosine similarity matrix<br/>cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)</pre>
<p>Although we're computing the cheaper dot product, the process will still take a few minutes to complete. With the similarity scores of every movie with every other movie, we are now in a very good position to write our final recommender function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the recommender function</h1>
                </header>
            
            <article>
                
<p>The final step is to create our recommender function. However, before we do that, let's create a reverse mapping of movie titles and their respective indices. In other words, let's create a pandas series with the index as the movie title and the value as the corresponding index in the main DataFrame:</p>
<pre>#Construct a reverse mapping of indices and movie titles, and drop duplicate titles, if any<br/>indices = pd.Series(df.index, index=df['title']).drop_duplicates()</pre>
<p>We will perform the following steps in building the recommender function:</p>
<ol>
<li>Declare the title of the movie as an argument.</li>
<li>Obtain the index of the movie from the <kbd>indices</kbd><em> </em>reverse mapping.</li>
<li>Get the list of cosine similarity scores for that particular movie with all movies using <kbd>cosine_sim</kbd>. Convert this into a list of tuples where the first element is the position and the second is the similarity score.</li>
<li>Sort this list of tuples on the basis of the cosine similarity scores.</li>
<li>Get the top 10 elements of this list. Ignore the first element as it refers to the similarity score with itself (the movie most similar to a particular movie is obviously the movie itself).</li>
<li>Return the titles corresponding to the indices of the top 10 elements, excluding the first:</li>
</ol>
<pre># Function that takes in movie title as input and gives recommendations <br/>def content_recommender(title, cosine_sim=cosine_sim, df=df, indices=indices):<br/>    # Obtain the index of the movie that matches the title<br/>    idx = indices[title]<br/><br/>    # Get the pairwsie similarity scores of all movies with that movie<br/>    # And convert it into a list of tuples as described above<br/>    sim_scores = list(enumerate(cosine_sim[idx]))<br/><br/>    # Sort the movies based on the cosine similarity scores<br/>    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)<br/><br/>    # Get the scores of the 10 most similar movies. Ignore the first movie.<br/>    sim_scores = sim_scores[1:11]<br/><br/>    # Get the movie indices<br/>    movie_indices = [i[0] for i in sim_scores]<br/><br/>    # Return the top 10 most similar movies<br/>    return df['title'].iloc[movie_indices]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Congratulations! You've built your very first content-based recommender. Now it is time to see our recommender in action! Let's ask it for recommendations of movies similar to <kbd>The Lion King</kbd><em>:</em></p>
<pre>#Get recommendations for The Lion King<br/>content_recommender('The Lion King')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b4947e84-7420-41ae-be55-37bfda306563.png" style="width:38.33em;height:16.00em;"/></div>
<p>We see that our recommender has suggested all of <em>The Lion King's</em> sequels in its top-10 list. We also notice that most of the movies in the list have to do with lions.</p>
<p>It goes without saying that a person who loves <em>The Lion King</em> is very likely to have a thing for Disney movies. They may also prefer to watch animated movies. Unfortunately, our plot description recommender isn't able to capture all this information.</p>
<p>Therefore, in the next section, we will build a recommender that uses more advanced metadata, such as genres, cast, crew, and keywords (or sub-genres). This recommender will be able to do a much better job of identifying an individual's taste for a particular director, actor, sub-genre, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Metadata-based recommender</h1>
                </header>
            
            <article>
                
<p>We will largely follow the same steps as the plot description-based recommender to build our metadata-based model. The main difference, of course, is in the type of data we use to build the model.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>To build this model, we will be using the following metdata:</p>
<ul>
<li>The genre of the movie. </li>
<li>The director of the movie. This person is part of the crew.</li>
<li>The movie's three major stars. They are part of the cast.</li>
<li>Sub-genres or keywords.</li>
</ul>
<p>With the exception of genres, our DataFrames (both original and cleaned) do not contain the data that we require. Therefore, for this exercise, we will need to download two additional files: <kbd>credits.csv</kbd><em>, </em>which contains information on the cast and crew of the movies, and <kbd>keywords.csv</kbd><em>, </em>which contains information on the sub-genres.</p>
<div class="packt_infobox">You can download the necessary files from the following URL: <a href="https://www.kaggle.com/rounakbanik/the-movies-dataset/data">https://www.kaggle.com/rounakbanik/the-movies-dataset/data</a>.<a href="https://www.kaggle.com/rounakbanik/the-movies-dataset/data"/></div>
<p>Place both files in your <kbd>data</kbd><em> </em>folder. We need to perform a good amount of wrangling before the data is converted into a form that is usable. Let's begin!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The keywords and credits datasets</h1>
                </header>
            
            <article>
                
<p>Let's start by loading our new data into the existing Jupyter Notebook:</p>
<pre># Load the keywords and credits files<br/>cred_df = pd.read_csv('../data/credits.csv')<br/>key_df = pd.read_csv('../data/keywords.csv')<br/><br/>#Print the head of the credit dataframe<br/>cred_df.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/16fbcd9d-f350-4369-ace4-7cfb2161c48e.png" style="width:38.83em;height:10.83em;"/></div>
<pre>#Print the head of the keywords dataframe<br/>key_df.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/095b3e0a-e471-4a7c-b5d6-d5704da38a9e.png" style="width:22.50em;height:11.92em;"/></div>
<p>We can see that the cast, crew, and the keywords are in the familiar <kbd>list of dictionaries</kbd><em> </em>form. Just like <kbd>genres</kbd><em>, </em>we have to reduce them to a string or a list of strings.</p>
<p>Before we do this, however, we will join<em> </em>the three DataFrames so that all our features are in a single DataFrame. Joining pandas DataFrames is identical to joining tables in SQL. The key we're going to use to join the DataFrames is the <kbd>id</kbd><em> </em>feature. However, in order to use this, we first need to explicitly convert <span class="fontstyle0">is listed as an ID. This is clearly bad data. Therefore, we should fin</span><br/>
<em> </em>into an integer. We already know how to do this:</p>
<pre>#Convert the IDs of df into int<br/>df['id'] = df['id'].astype('int')</pre>
<p>Running the preceding code results in a <kbd>ValueError</kbd>. On closer inspection, we see that <em>1997-08-20</em> is listed as an ID. This is clearly bad data. Therefore, we should find all the rows with bad IDs and remove them in order for the code execution to be successful:</p>
<pre># Function to convert all non-integer IDs to NaN<br/>def clean_ids(x):<br/>    try:<br/>        return int(x)<br/>    except:<br/>        return np.nan<br/><br/>#Clean the ids of df<br/>df['id'] = df['id'].apply(clean_ids)<br/><br/>#Filter all rows that have a null ID<br/>df = df[df['id'].notnull()]</pre>
<p>We are now in a good position to convert the IDs of all three DataFrames into integers and merge them into a single DataFrame:</p>
<pre># Convert IDs into integer<br/>df['id'] = df['id'].astype('int')<br/>key_df['id'] = key_df['id'].astype('int')<br/>cred_df['id'] = cred_df['id'].astype('int')<br/><br/># Merge keywords and credits into your main metadata dataframe<br/>df = df.merge(cred_df, on='id')<br/>df = df.merge(key_df, on='id')<br/><br/>#Display the head of the merged df<br/>df.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ce2ccf31-5289-4d31-91e7-d9404323db07.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wrangling keywords, cast, and crew</h1>
                </header>
            
            <article>
                
<p>Now that we have all the desired features in a single DataFrame, let's  convert them into a form that is more usable. More specifically, these are the transformations we will be looking to perform:</p>
<ul>
<li>Convert <kbd>keywords</kbd><em> </em>into a list of strings where each string is a keyword (similar to genres). <span><span>We will include only the top three keywords. Therefore, this list can have a maximum of three elements.</span></span></li>
<li>Convert <kbd>cast</kbd><em> </em>into a list of strings where each string is a star. Like <kbd>keywords</kbd><em>,</em> we will only include the top three stars in our cast. </li>
<li>Convert <kbd>crew</kbd><em> </em>into <kbd>director</kbd><em>. </em>In other words, we will extract only the director of the movie and ignore all other crew members.</li>
</ul>
<p>The first step is to convert these stringified objects into native Python objects:</p>
<pre># Convert the stringified objects into the native python objects<br/>from ast import literal_eval<br/><br/>features = ['cast', 'crew', 'keywords', 'genres']<br/>for feature in features:<br/>    df[feature] = df[feature].apply(literal_eval)</pre>
<p>Next, let's extract the director from our <kbd>crew</kbd> list. To do this, we will first examine the structure of the dictionary in the <kbd>crew</kbd><em> </em>list:</p>
<pre>#Print the first cast member of the first movie in df<br/>df.iloc[0]['crew'][0]<br/><br/><strong>OUTPUT:<br/>{'credit_id': '52fe4284c3a36847f8024f49',</strong><br/><strong> 'department': 'Directing',</strong><br/><strong> 'gender': 2,</strong><br/><strong> 'id': 7879,</strong><br/><strong> 'job': 'Director',</strong><br/><strong> 'name': 'John Lasseter',</strong><br/><strong> 'profile_path': '/7EdqiNbr4FRjIhKHyPPdFfEEEFG.jpg'}</strong></pre>
<p>We see that this dictionary consists of <kbd>job</kbd><em> </em>and <kbd>name</kbd><em> </em>keys. Since we're only interested in the director, we will loop through all the crew members in a particular list and extract the <kbd>name</kbd><em> </em>when the <kbd>job</kbd><em> </em>is <kbd>Director</kbd>. Let's write a function that does this:</p>
<pre># Extract the director's name. If director is not listed, return NaN<br/>def get_director(x):<br/>    for crew_member in x:<br/>        if crew_member['job'] == 'Director':<br/>            return crew_member['name']<br/>    return np.nan<br/><br/></pre>
<p>Now that we have the <kbd>get_director</kbd><em> </em>function, we can define the new <kbd>director</kbd><em> </em>feature:</p>
<pre>#Define the new director feature<br/>df['director'] = df['crew'].apply(get_director)<br/><br/>#Print the directors of the first five movies<br/>df['director'].head()<br/><br/><strong>OUTPUT:<br/>0 John Lasseter<br/>1 Joe Johnston<br/>2 Howard Deutch<br/>3 Forest Whitaker<br/>4 Charles Shyer<br/>Name: director, dtype: object</strong></pre>
<p>Both <kbd>keywords</kbd><em> </em>and <kbd>cast</kbd><em> </em>are dictionary lists as well. And, in both cases, we need to extract the top three <kbd>name</kbd> attributes of each list. Therefore, we can write a single function to wrangle both these features. Also, just like <kbd>keywords</kbd><em> </em>and <kbd>cast</kbd><em>, </em>we will only consider the top three genres for every movie:</p>
<pre># Returns the list top 3 elements or entire list; whichever is more.<br/>def generate_list(x):<br/>    if isinstance(x, list):<br/>        names = [ele['name'] for ele in x]<br/>        #Check if more than 3 elements exist. If yes, return only first three. <br/>        #If no, return entire list.<br/>        if len(names) &gt; 3:<br/>            names = names[:3]<br/>        return names<br/><br/>    #Return empty list in case of missing/malformed data<br/>    return []</pre>
<p>We will use this function to wrangle our <kbd>cast</kbd><em> </em>and <kbd>keywords</kbd><em> </em>features. We will also only consider the first three <kbd>genres</kbd><em> </em>listed:</p>
<pre>#Apply the generate_list function to cast and keywords<br/>df['cast'] = df['cast'].apply(generate_list)<br/>df['keywords'] = df['keywords'].apply(generate_list)<br/><br/>#Only consider a maximum of 3 genres<br/>df['genres'] = df['genres'].apply(lambda x: x[:3])</pre>
<p>Let's now take a look at a sample of our wrangled data:</p>
<pre><span class="hljs-comment"># Print the new features of the first 5 movies along with title</span><span><br/>df[[</span><span class="hljs-string">'title'</span><span>, </span><span class="hljs-string">'cast'</span><span>, </span><span class="hljs-string">'director'</span><span>, </span><span class="hljs-string">'keywords', 'genres'</span><span>]].head(</span><span class="hljs-number">3</span><span>)</span></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2e13c055-03c2-4dfe-a215-c10fe68ce6a6.png"/></div>
<p>In the subsequent steps, we are going to use a vectorizer to build document vectors. If two actors had the same first name (say, Ryan Reynolds and Ryan Gosling), the vectorizer will treat both Ryans as the same, although they are clearly different entities. This will impact the quality of the recommendations we receive. If a person likes Ryan Reynolds' movies, it doesn't imply that they like movies by all Ryans. </p>
<p><span>Therefore, the last step is to strip the spaces between keywords, and actor and director names, and convert them all into lowercase. Therefore, the two Ryans in the preceding example will become <em>ryangosling</em> and <em>ryanreynolds</em>, and our vectorizer will now be able to distinguish between them:</span></p>
<pre># Function to sanitize data to prevent ambiguity. <br/># Removes spaces and converts to lowercase<br/>def sanitize(x):<br/>    if isinstance(x, list):<br/>        #Strip spaces and convert to lowercase<br/>        return [str.lower(i.replace(" ", "")) for i in x]<br/>    else:<br/>        #Check if director exists. If not, return empty string<br/>        if isinstance(x, str):<br/>            return str.lower(x.replace(" ", ""))<br/>        else:<br/>            return ''</pre>
<pre>#Apply the generate_list function to cast, keywords, director and genres<br/>for feature in ['cast', 'director', 'genres', 'keywords']:<br/>    df[feature] = df[feature].apply(sanitize)</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the metadata soup</h1>
                </header>
            
            <article>
                
<p>In the plot description-based recommender, we worked with a single <em>overview </em>feature, which was a body of text. Therefore, we were able to apply our vectorizer directly.</p>
<p>However, this is not the case with our metadata-based recommender. We have four features to work with, of which three are lists and one is a string. What we need to do is create a <kbd>soup</kbd><em> </em>that contains the actors, director, keywords, and genres. This way, we can feed this soup into our vectorizer and perform similar follow-up steps to before:</p>
<pre>#Function that creates a soup out of the desired metadata<br/>def create_soup(x):<br/>    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])</pre>
<p>With this function in hand, we create the <kbd>soup</kbd><em> </em>feature:</p>
<pre># Create the new soup feature<br/>df['soup'] = df.apply(create_soup, axis=1)</pre>
<p>Let's now take a look at one of the <kbd>soup</kbd><em> </em>values. It should be a string containing words that represent genres, cast, and keywords:</p>
<pre>#Display the soup of the first movie<br/>df.iloc[0]['soup']<br/><br/><strong>OUTPUT:<br/>'jealousy toy boy tomhanks timallen donrickles johnlasseter animation comedy family'</strong></pre>
<p>With the <kbd>soup</kbd><em> </em>created, we are now in a good position to create our document vectors, compute similarity scores, and build the metadata-based recommender function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating the recommendations</h1>
                </header>
            
            <article>
                
<p>The next steps are almost identical to the corresponding steps from the previous section.</p>
<p>Instead of using TF-IDFVectorizer, we will be using CountVectorizer. This is because using TF-IDFVectorizer will accord less weight to actors and directors who have acted and directed in a relatively larger number of movies.</p>
<p>This is not desirable, as we do not want to penalize artists for directing or appearing in more movies:</p>
<pre>#Define a new CountVectorizer object and create vectors for the soup<br/>count = CountVectorizer(stop_words='english')<br/>count_matrix = count.fit_transform(df['soup'])</pre>
<p>Unfortunately, using CountVectorizer means that we are forced to use the more computationally expensive <kbd>cosine_similarity</kbd><em> </em>function to compute our scores:</p>
<pre>#Import cosine_similarity function<br/>from sklearn.metrics.pairwise import cosine_similarity<br/><br/>#Compute the cosine similarity score (equivalent to dot product for tf-idf vectors)<br/>cosine_sim2 = cosine_similarity(count_matrix, count_matrix)</pre>
<p>Since we dropped a few movies with bad indices, we need to construct our reverse mapping again. Let's do that as the next step:</p>
<pre># Reset index of your df and construct reverse mapping again<br/>df = df.reset_index()<br/>indices2 = pd.Series(df.index, index=df['title'])</pre>
<p>With the new reverse mapping constructed and the similarity scores computed, we can reuse the <kbd>content_recommender</kbd><em> </em>function defined in the previous section by passing in <kbd>cosine_sim2</kbd><em> </em>as an argument. Let's now try out our new model by asking recommendations for the same movie, <kbd>The Lion King</kbd><em>:</em></p>
<pre>content_recommender('The Lion King', cosine_sim2, df, indices2)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5097b401-71d0-4e11-86f2-cd9e5613cd87.png" style="width:30.00em;height:11.75em;"/></div>
<p>The recommendations given in this case are vastly different to the ones that our plot description-based recommender gave. We see that it has been able to capture more information than just lions. Most of the movies in the list are animated and feature anthropomorphic characters.</p>
<p>Personally, I found the <em>Pokemon: Arceus and the Jewel of Life </em>recommendation especially interesting. Both this movie and <em>The Lion King</em> feature cartoon anthropomorphic characters who return after a few years to exact revenge on those who had wronged them. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Suggestions for improvements</h1>
                </header>
            
            <article>
                
<p>The content-based recommenders we've built in this chapter are, of course, nowhere near the powerful models used in the industry. There is still plenty of scope for improvement. In this section, I will suggest a few ideas for upgrading the recommenders that you've already built:</p>
<ul>
<li>
<p><strong>Experiment with the number of keywords, genres, and cast</strong>:<strong> </strong>In the model that we built, we considered at most three keywords, genres, and actors for our movies. This was, however, an arbitrary decision. It is a good idea to experiment with the number of these features in order to be considered for the metadata soup.</p>
</li>
<li>
<p><strong>Come up with more well-defined sub-genres</strong>:<strong> </strong>Our model only considered the first three keywords that appeared in the keywords list. There was, however, no justification for doing so. In fact, it is entirely possible that certain keywords appeared in only one movie (thus rendering them useless). A much more potent technique would be to define, as with the genres, a definite number of sub-genres and assign only these sub-genres to the movies.</p>
</li>
<li>
<p><strong>Give more weight to the director</strong>:<strong> </strong>Our model gave as much importance to the director as to the actors. However, you can argue that the character of a movie is determined more by the former. We can give more emphasis to the director by mentioning this individual multiple times in our soup instead of just once. Experiment with the number of repetitions of the director in the soup.</p>
</li>
<li>
<p><strong>Consider other members of the crew</strong>:<strong> </strong>The director isn't the only person that gives the movie its character. You can also consider adding other crew members, such as producers and screenwriters, to your soup.</p>
</li>
<li>
<p><strong>Experiment with other metadata</strong>:<strong> </strong>We only considered genres, keywords, and credits while building our metadata model. However, our dataset contains plenty of other features, such as production companies, countries, and languages. You may consider these data points, too, as they may be able to capture important information (such as if two movies are produced by <em>Pixar).</em></p>
</li>
<li>
<p><strong>Introduce a popularity filter</strong>:<strong> </strong>It is entirely possible that two movies have the same genres and sub-genres, but differ wildly in quality and popularity. In such cases, you may want to introduce a popularity filter that considers the <em>n </em>most similar movies, computes a weighted rating, and displays the top five results. You have already learned how to do this in the previous chapter.</p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We have come a long way in this chapter. We first learned about document vectors and gained a brief introduction to the cosine similarity score. Next, we built a recommender that identified movies with similar plot descriptions. We then proceeded to build a more advanced model that leveraged the power of other metadata, such as genres, keywords, and credits. Finally, we discussed a few methods by which we could improve our existing system.</p>
<p>With this, we formally come to an end of our tour of content-based recommendation system. In the next chapters, we will cover what is arguably the most popular recommendation model in the industry today: collaborative filtering.</p>


            </article>

            
        </section>
    </body></html>
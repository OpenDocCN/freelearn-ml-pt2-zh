["```py\n>>> import torch, torchvision\n>>> from torchvision import transforms\n>>> image_path = './'\n>>> transform = transforms.Compose([transforms.ToTensor()])\n>>> train_dataset = torchvision.datasets.FashionMNIST(root=image_path,\n                                                      train=True,\n                                                      transform=transform,\n                                                      download=True)\n>>> test_dataset = torchvision.datasets.FashionMNIST(root=image_path,\n                                                     train=False,\n                                                     transform=transform,\n                                                     download=False) \n```", "```py\n>>> print(train_dataset)\nDataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: ./\n    Split: Train\n    StandardTransform\nTransform: Compose(\n    ToTensor()\n   )\n>>> print(test_dataset)\nDataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: ./\n    Split: Test\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n           ) \n```", "```py\n>>> from torch.utils.data import DataLoader\n>>> batch_size = 64\n>>> torch.manual_seed(42)\n>>> train_dl = DataLoader(train_dataset, batch_size, shuffle=True) \n```", "```py\n>>> data_iter = iter(train_dl)\n>>> images, labels = next(data_iter)\n>>> print(labels)\ntensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1, 2, 3, 2, 3, 3, 7, 9, 9, 3, 2, 4, 6, 3, 5, 5, 3, 2, 0, 0, 8, 4, 2, 8, 5, 9, 2, 4, 9, 4, 4, 3, 4, 9, 7, 2, 0, 4, 5, 4, 8, 2, 6, 7, 0, 2, 0, 6, 3, 3, 5, 6, 0, 0, 8]) \n```", "```py\n>>> class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \n```", "```py\n>>> print(images[0].shape)\ntorch.Size([1, 28, 28])\n>>> print(torch.max(images), torch.min(images))\ntensor(1.) tensor(0.) \n```", "```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> npimg = images[1].numpy()\n>>> plt.imshow(np.transpose(npimg, (1, 2, 0)))\n>>> plt.colorbar()\n>>> plt.title(class_names[labels[1]])\n>>> plt.show() \n```", "```py\n>>> for i in range(16):\n...     plt.subplot(4, 4, i + 1)\n...     plt.subplots_adjust(hspace=.3)\n...     plt.xticks([])\n...     plt.yticks([])\n...     npimg = images[i].numpy()\n...     plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\"Greys\")\n...     plt.title(class_names[labels[i]])\n... plt.show() \n```", "```py\n>>> import torch.nn as nn\n>>> model = nn.Sequential() \n```", "```py\n>>> model.add_module('conv1',\n                     nn.Conv2d(in_channels=1,\n                               out_channels=32,\n                               kernel_size=3)\n                    )\n>>> model.add_module('relu1', nn.ReLU()) \n```", "```py\n>>> model.add_module('pool1', nn.MaxPool2d(kernel_size=2)) \n```", "```py\n>>> model.add_module('conv2',\n                     nn.Conv2d(in_channels=32,\n                               out_channels=64,\n                               kernel_size=3)\n                    )\n>>> model.add_module('relu2', nn.ReLU()) \n```", "```py\n>>> model.add_module('pool2', nn.MaxPool2d(kernel_size=2)) \n```", "```py\n>>> model.add_module('conv3',\n                     nn.Conv2d(in_channels=64,\n                               out_channels=128,\n                               kernel_size=3)\n                    )\n>>> model.add_module('relu3', nn.ReLU()) \n```", "```py\n>>> x = torch.rand((64, 1, 28, 28))\n>>> print(model(x).shape)\ntorch.Size([64, 128, 3, 3]) \n```", "```py\n>>> model.add_module('flatten', nn.Flatten()) \n```", "```py\n>>> print(model(x).shape)\ntorch.Size([64, 1152]) \n```", "```py\n>>> model.add_module('fc1', nn.Linear(1152, 64))\n>>> model.add_module('relu4', nn.ReLU()) \n```", "```py\n>>> model.add_module('fc2', nn.Linear(64, 10))\n>>> model.add_module('output', nn.Softmax(dim = 1)) \n```", "```py\n>>> print(model)\nSequential(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (relu1): ReLU()\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (relu2): ReLU()\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n  (relu3): ReLU()\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (fc1): Linear(in_features=1152, out_features=64, bias=True)\n  (relu4): ReLU()\n  (fc2): Linear(in_features=64, out_features=10, bias=True)\n  (output): Softmax(dim=1)\n)_________________________________________________________________ \n```", "```py\n>>> pip install torchsummary\n>>> from torchsummary import summary\n>>> summary(model, input_size=(1, 28, 28), batch_size=-1, device=\"cpu\")\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 26, 26]             320\n              ReLU-2           [-1, 32, 26, 26]               0\n         MaxPool2d-3           [-1, 32, 13, 13]               0\n            Conv2d-4           [-1, 64, 11, 11]          18,496\n              ReLU-5           [-1, 64, 11, 11]               0\n         MaxPool2d-6             [-1, 64, 5, 5]               0\n            Conv2d-7            [-1, 128, 3, 3]          73,856\n              ReLU-8            [-1, 128, 3, 3]               0\n           Flatten-9                 [-1, 1152]               0\n           Linear-10                   [-1, 64]          73,792\n             ReLU-11                   [-1, 64]               0\n           Linear-12                   [-1, 10]             650\n          Softmax-13                   [-1, 10]               0\n================================================================\nTotal params: 167,114\nTrainable params: 167,114\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.53\nParams size (MB): 0.64\nEstimated Total Size (MB): 1.17\n---------------------------------------------------------------- \n```", "```py\n>>> device = torch.device(\"cuda:0\")\n# device = torch.device(\"cpu\")\n>>> model = model.to(device)\n>>> loss_fn = nn.CrossEntropyLoss()\n>>> optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n```", "```py\n>>> def train(model, optimizer, num_epochs, train_dl):\n        for epoch in range(num_epochs):\n            loss_train = 0\n            accuracy_train = 0\n            for x_batch, y_batch in train_dl:\n                x_batch = x_batch.to(device)\n                y_batch = y_batch.to(device)\n                pred = model(x_batch)\n                loss = loss_fn(pred, y_batch)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                loss_train += loss.item() * y_batch.size(0)\n                is_correct = (torch.argmax(pred, dim=1) ==\n                                               y_batch).float()\n                accuracy_train += is_correct.sum().cpu()\n            loss_train /= len(train_dl.dataset)\n            accuracy_train /= len(train_dl.dataset)\n\n            print(f'Epoch {epoch+1} - loss: {loss_train:.4f} - accuracy:\n                  {accuracy_train:.4f}') \n```", "```py\n>>> num_epochs = 30\n>>> train(model, optimizer, num_epochs, train_dl)\nEpoch 1 - loss: 1.7253 - accuracy: 0.7385\nEpoch 2 - loss: 1.6333 - accuracy: 0.8287\n…\nEpoch 10 - loss: 1.5572 - accuracy: 0.9041\n…\nEpoch 20 - loss: 1.5344 - accuracy: 0.9270\n...\nEpoch 29 - loss: 1.5249 - accuracy: 0.9362\nEpoch 30 - loss: 1.5249 - accuracy: 0.9363 \n```", "```py\n>>> test_dl = DataLoader(test_dataset, batch_size, shuffle=False)\n>>> def evaluate_model(model, test_dl):\n        accuracy_test = 0\n        with torch.no_grad():\n            for x_batch, y_batch in test_dl:\n                pred = model.cpu()(x_batch)\n                is_correct = torch.argmax(pred, dim=1) == y_batch\n                accuracy_test += is_correct.float().sum().item()\n        print(f'Accuracy on test set: {100 * accuracy_test / 10000} %')\n>>> evaluate_model(model, test_dl)\nAccuracy on test set: 90.25 % \n```", "```py\n>>> conv3_weight = model.conv3.weight.data\n>>> print(conv3_weight.shape) \ntorch.Size([128, 64, 3, 3]) \n```", "```py\n>>> n_filters = 16\n>>> for i in range(n_filters):\n...     weight = conv3_weight[i].cpu().numpy()\n...     plt.subplot(4, 4, i+1)\n...     plt.xticks([])\n...     plt.yticks([])\n...     plt.imshow(weight[0], cmap='gray')\n... plt.show() \n```", "```py\n>>> image = images[1]\n>>> img_flipped = transforms.functional.hflip(image) \n```", "```py\n>>> def display_image_greys(image):\n    npimg = image.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\"Greys\")\n    plt.xticks([])\n    plt.yticks([])\n>>> plt.figure(figsize=(8, 8))\n>>> plt.subplot(1, 2, 1)\n>>> display_image_greys(image)\n>>> plt.subplot(1, 2, 2)\n>>> display_image_greys(img_flipped)\n>>> plt.show() \n```", "```py\n>>> torch.manual_seed(42)\n>>>  flip_transform =\n               transforms.Compose([transforms.RandomHorizontalFlip()])\n>>> plt.figure(figsize=(10, 10))\n>>> plt.subplot(1, 4, 1)\n>>> display_image_greys(image)\n>>> for i in range(3):\n        plt.subplot(1, 4, i+2)\n        img_flip = flip_transform(image)\n        display_image_greys(img_flip) \n```", "```py\n>>> torch.manual_seed(42)\n>>> rotate_transform =\n            transforms.Compose([transforms. RandomRotation(20)])\n>>> plt.figure(figsize=(10, 10))\n>>> plt.subplot(1, 4, 1)\n>>> display_image_greys(image)\n>>> for i in range(3):\n        plt.subplot(1, 4, i+2)\n        img_rotate = rotate_transform(image)\n    display_image_greys(img_rotate) \n```", "```py\n>>> torch.manual_seed(42)\n>>> crop_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=(28, 28), scale=(0.7, 1))])\n>>> plt.figure(figsize=(10, 10))\n>>> plt.subplot(1, 4, 1)\n>>> display_image_greys(image)\n>>> for i in range(3):\n        plt.subplot(1, 4, i+2)\n        img_crop = crop_transform(image)\n        display_image_greys(img_crop) \n```", "```py\n    >>> torch.manual_seed(42)\n    >>> transform_train = transforms.Compose([\n                           transforms.RandomHorizontalFlip(),\n                           transforms.RandomRotation(10),\n                           transforms.RandomResizedCrop(size=(28, 28),\n                                                        scale=(0.9, 1)),\n                           transforms.ToTensor(),\n        ]) \n    ```", "```py\n    >>> train_dataset_aug = torchvision.datasets.FashionMNIST(\n                                                   root=image_path,\n                                                   train=True,\n                                                   transform=transform_train,\n                                                   download=False)\n    >>> from torch.utils.data import Subset\n    >>> train_dataset_aug_small = Subset(train_dataset_aug, torch.arange(500)) \n    ```", "```py\n    >>> train_dl_aug_small = DataLoader(train_dataset_aug_small,\n                                        batch_size, \n                                        shuffle=True) \n    ```", "```py\n    >>> model = nn.Sequential()\n    >>> ...(here we skip repeating the same code)\n    >>> model = model.to(device)\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n    ```", "```py\n    >>> train(model, optimizer, 1000, train_dl_aug_small)\n    Epoch 1 - loss: 2.3013 - accuracy: 0.1400\n    ...\n    Epoch 301 - loss: 1.6817 - accuracy: 0.7760\n    ...\n    Epoch 601 - loss: 1.5006 - accuracy: 0.9620\n    ...\n    Epoch 1000 - loss: 1.4904 - accuracy: 0.9720 \n    ```", "```py\n    >>> evaluate_model(model, test_dl)\n    Accuracy on test set: 79.24% \n    ```", "```py\n    >>> from torchvision.models import resnet18\n    >>> my_resnet = resnet18(weights='IMAGENET1K_V1') \n    ```", "```py\n    >>> my_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) \n    ```", "```py\nself.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) \n```", "```py\n    >>> num_ftrs = my_resnet.fc.in_features\n    >>> my_resnet.fc = nn.Linear(num_ftrs, 10) \n    ```", "```py\n    >>> my_resnet = my_resnet.to(device)\n    >>> optimizer = torch.optim.Adam(my_resnet.parameters(), lr=0.001)\n    >>> train(my_resnet, optimizer, 10, train_dl)\n    Epoch 1 - loss: 0.4797 - accuracy: 0.8256\n    Epoch 2 - loss: 0.3377 - accuracy: 0.8791\n    Epoch 3 - loss: 0.2921 - accuracy: 0.8944\n    Epoch 4 - loss: 0.2629 - accuracy: 0.9047\n    Epoch 5 - loss: 0.2336 - accuracy: 0.9157\n    Epoch 6 - loss: 0.2138 - accuracy: 0.9221\n    Epoch 7 - loss: 0.1911 - accuracy: 0.9301\n    Epoch 8 - loss: 0.1854 - accuracy: 0.9312\n    Epoch 9 - loss: 0.1662 - accuracy: 0.9385\n    Epoch 10 - loss: 0.1575 - accuracy: 0.9427 \n    ```", "```py\n    >>> evaluate_model(my_resnet, test_dl)\n    Accuracy on test set: 91.01 % \n    ```"]
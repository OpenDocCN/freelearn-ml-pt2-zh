<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Finding Groups of Data &#x2013; Clustering with k-means"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Finding Groups of Data – Clustering with k-means</h1></div></div></div><p>Have you ever spent time watching a large crowd? If so, you are likely to have seen some recurring personalities. Perhaps a certain type of person, identified by a freshly pressed suit and a briefcase, comes to typify the "fat cat" business executive. A twenty-something wearing skinny jeans, a flannel shirt, and sunglasses might be dubbed a "hipster," while a woman unloading children from a minivan may be labeled a "soccer mom."</p><p>Of course, these types of stereotypes are dangerous to apply to individuals, as no two people are exactly alike. Yet understood as a way to describe a collective, the labels capture some underlying aspect of similarity among the individuals within the group.</p><p>As you will soon learn, the act of clustering, or spotting patterns in data, is not much different from spotting patterns in groups of people. In this chapter, you will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The ways clustering tasks differ from the classification tasks we examined previously</li><li class="listitem" style="list-style-type: disc">How clustering defines a group, and how such groups are identified by k-means, a classic and easy-to-understand clustering algorithm</li><li class="listitem" style="list-style-type: disc">The steps needed to apply clustering to a real-world task of identifying marketing segments among teenage social media users</li></ul></div><p>Before jumping into action, we'll begin by taking an in-depth look at exactly what clustering entails.</p><div class="section" title="Understanding clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec41"/>Understanding clustering</h1></div></div></div><p>Clustering is an <a id="id764" class="indexterm"/>unsupervised machine learning task that automatically divides the data into <span class="strong"><strong>clusters,</strong></span> or groups of similar items. It does this without having been told how the groups should look ahead of time. As we may not even know what we're looking for, clustering is used for knowledge discovery rather than prediction. It provides an insight into the natural groupings found within data.</p><p>Without advance knowledge of what comprises a cluster, how can a computer possibly know where one group ends and another begins? The answer is simple. Clustering is guided by the principle that items inside a cluster should be very similar to each other, but very different from those outside. The definition of similarity might vary across applications, but the basic idea is always the same—group the data so that the related elements are placed together.</p><p>The resulting clusters <a id="id765" class="indexterm"/>can then be used for action. For instance, you might find clustering methods employed in the following applications:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Segmenting customers into groups with similar demographics or buying patterns for targeted marketing campaigns</li><li class="listitem" style="list-style-type: disc">Detecting anomalous behavior, such as unauthorized network intrusions, by identifying patterns of use falling outside the known clusters</li><li class="listitem" style="list-style-type: disc">Simplifying extremely large datasets by grouping features with similar values into a smaller number of homogeneous categories</li></ul></div><p>Overall, clustering is useful whenever diverse and varied data can be exemplified by a much smaller number of groups. It results in meaningful and actionable data structures that reduce complexity and provide insight into patterns of relationships.</p><div class="section" title="Clustering as a machine learning task"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec101"/>Clustering as a machine learning task</h2></div></div></div><p>Clustering is <a id="id766" class="indexterm"/>somewhat different from the classification, numeric prediction, and pattern detection tasks we examined so far. In each of these cases, the result is a model that relates features to an outcome or features to other features; conceptually, the model describes the existing patterns within data. In contrast, clustering creates new data. Unlabeled examples are given a cluster label that has been inferred entirely from the relationships within the data. For this reason, you will, sometimes, see the clustering task referred to as <span class="strong"><strong>unsupervised classification</strong></span> because, in a sense, it classifies unlabeled examples.</p><p>The catch is that the class labels obtained from an unsupervised classifier are without intrinsic meaning. Clustering will tell you which groups of examples are closely related—for instance, it might return the groups A, B, and C—but it's up to you to apply an actionable and meaningful label. To see how this impacts the clustering task, let's consider a hypothetical example.</p><p>Suppose you were organizing a conference on the topic of data science. To facilitate professional networking and collaboration, you planned to seat people in groups according to one of three research specialties: computer and/or database science, math and statistics, and machine learning. Unfortunately, after sending out the conference invitations, you realize that you had forgotten to include a survey asking which discipline the attendee would prefer to be seated with.</p><p>In a stroke of brilliance, you realize that you might be able to infer each scholar's research specialty by examining his or her publication history. To this end, you begin collecting data on the number of articles each attendee published in computer science-related journals and the number of articles published in math or statistics-related journals. Using the data collected for several scholars, you create a scatterplot:</p><div class="mediaobject"><img src="graphics/B03905_09_01.jpg" alt="Clustering as a machine learning task"/></div><p>As expected, there seems to be a pattern. We might guess that the upper-left corner, which represents people with many computer science publications but few articles on math, could be a cluster of computer scientists. Following this logic, the lower-right corner might be a group of mathematicians. Similarly, the upper-right corner, those with both math and computer science experience, may be machine learning experts.</p><p>Our groupings <a id="id767" class="indexterm"/>were formed visually; we simply identified clusters as closely grouped data points. Yet in spite of the seemingly obvious groupings, we unfortunately have no way to know whether they are truly homogeneous without personally asking each scholar about his/her academic specialty. The labels we applied required us to make qualitative, presumptive judgments about the types of people that would fall into the group. For this reason, you might imagine the cluster labels in uncertain terms, as follows:</p><div class="mediaobject"><img src="graphics/B03905_09_02.jpg" alt="Clustering as a machine learning task"/></div><p>Rather than defining the group boundaries subjectively, it would be nice to use machine learning to define them objectively. Given the axis-parallel splits in the preceding diagram, our problem seems like an obvious application for the decision trees described in <a class="link" href="ch05.html" title="Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules">Chapter 5</a>, <span class="emphasis"><em>Divide and Conquer – Classification Using Decision Trees and Rules</em></span>. This might <a id="id768" class="indexterm"/>provide us with a rule in the form "if a scholar has few math publications, then he/she is a computer science expert." Unfortunately, there's a problem with this plan. As we do not have data on the true class value for each point, a supervised learning algorithm would have no ability to learn such a pattern, as it would have no way of knowing what splits would result in homogenous groups.</p><p>On the other hand, clustering algorithms use a process very similar to what we did by visually inspecting the scatterplot. Using a measure of how closely the examples are related, homogeneous groups can be identified. In the next section, we'll start looking at how clustering algorithms are implemented.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip104"/>Tip</h3><p>This example <a id="id769" class="indexterm"/>highlights an interesting application of clustering. If you begin with unlabeled data, you can use clustering to create class labels. From there, you could apply a supervised learner such as decision trees to find the most important predictors of these classes. This is called <span class="strong"><strong>semi-supervised learning</strong></span>.</p></div></div></div><div class="section" title="The k-means clustering algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec102"/>The k-means clustering algorithm</h2></div></div></div><p>The <span class="strong"><strong>k-means </strong></span><a id="id770" class="indexterm"/>
<span class="strong"><strong>algorithm</strong></span> is perhaps the most commonly used clustering method. Having been studied for several decades, it serves as the foundation for <a id="id771" class="indexterm"/>many more sophisticated clustering techniques. If you understand the simple principles it uses, you will have the knowledge needed to understand nearly any clustering algorithm in use today. Many such methods are listed on the following site, the <span class="strong"><strong>CRAN Task View</strong></span> for clustering at <a class="ulink" href="http://cran.r-project.org/web/views/Cluster.html">http://cran.r-project.org/web/views/Cluster.html</a>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note36"/>Note</h3><p>As k-means has evolved over time, there are many implementations of the algorithm. One popular approach is described in : Hartigan JA, Wong MA. A k-means clustering algorithm. Applied <span class="emphasis"><em>Statistics</em></span>. 1979; 28:100-108.</p></div></div><p>Even though clustering methods have advanced since the inception of k-means, this is not to imply that k-means is obsolete. In fact, the method may be more popular now than ever. The following table lists some reasons why k-means is still used widely:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strengths</p>
</th><th style="text-align: left" valign="bottom">
<p>Weaknesses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Uses simple principles that can be explained in non-statistical terms</li><li class="listitem" style="list-style-type: disc">Highly flexible, and can be adapted with simple adjustments to address nearly all of its shortcomings</li><li class="listitem" style="list-style-type: disc">Performs well enough under many real-world use cases</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Not as sophisticated as more modern clustering algorithms</li><li class="listitem" style="list-style-type: disc">Because it uses an element of random chance, it is not guaranteed to find the optimal set of clusters</li><li class="listitem" style="list-style-type: disc">Requires a reasonable guess as to how many clusters naturally exist in the data</li><li class="listitem" style="list-style-type: disc">Not ideal for non-spherical clusters or clusters of widely varying density</li></ul></div>
</td></tr></tbody></table></div><p>If the name k-means sounds familiar to you, you may be recalling the k-NN algorithm discussed in <a class="link" href="ch03.html" title="Chapter 3. Lazy Learning – Classification Using Nearest Neighbors">Chapter 3</a>, <span class="emphasis"><em>Lazy Learning – Classification Using Nearest Neighbors</em></span>. As you will soon see, k-means shares more in common with the k-nearest neighbors than just the letter k.</p><p>The k-means algorithm assigns each of the <span class="emphasis"><em>n</em></span> examples to one of the <span class="emphasis"><em>k</em></span> clusters, where <span class="emphasis"><em>k</em></span> is a number that has been determined ahead of time. The goal is to minimize the differences within each cluster and maximize the differences between the clusters.</p><p>Unless <span class="emphasis"><em>k</em></span> and <span class="emphasis"><em>n</em></span> are extremely small, it is not feasible to compute the optimal clusters across all the possible combinations of examples. Instead, the algorithm uses a heuristic process that finds <span class="strong"><strong>locally optimal</strong></span> solutions. Put simply, this means that it starts with an initial guess for the cluster assignments, and then modifies the assignments slightly to see whether the changes improve the homogeneity within the clusters.</p><p>We will cover the process in depth shortly, but the algorithm essentially involves two phases. First, it assigns examples to an initial set of <span class="emphasis"><em>k</em></span> clusters. Then, it updates the assignments by adjusting <a id="id772" class="indexterm"/>the cluster boundaries according to the examples that currently fall into the cluster. The process of updating and assigning occurs several times until changes no longer improve the cluster fit. At this point, the process stops and the clusters are finalized.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip105"/>Tip</h3><p>Due to the heuristic nature of k-means, you may end up with somewhat different final results by making only slight changes to the starting conditions. If the results vary dramatically, this could indicate a problem. For instance, the data may not have natural groupings or the value of <span class="emphasis"><em>k</em></span> has been poorly chosen. With this in mind, it's a good idea to try a cluster analysis more than once to test the robustness of your findings.</p></div></div><p>To see how the process of assigning and updating works in practice, let's revisit the case of the hypothetical data science conference. Though this is a simple example, it will illustrate the basics of how k-means operates under the hood.</p><div class="section" title="Using distance to assign and update clusters"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec52"/>Using distance to assign and update clusters</h3></div></div></div><p>As with <a id="id773" class="indexterm"/>k-NN, k-means <a id="id774" class="indexterm"/>treats feature values as coordinates in a multidimensional feature space. For the conference data, there are only two features, so we can represent the feature space as a two-dimensional scatterplot as depicted previously.</p><p>The k-means algorithm begins by choosing <span class="emphasis"><em>k</em></span> points in the feature space to serve as the cluster centers. These centers are the catalyst that spurs the remaining examples to fall into place. Often, the points are chosen by selecting <span class="emphasis"><em>k</em></span> random examples from the training dataset. As we hope to identify three clusters, according to this method, <span class="emphasis"><em>k = 3</em></span> points will be selected at random. These points are indicated by the star, triangle, and diamond in the following diagram:</p><div class="mediaobject"><img src="graphics/B03905_09_03.jpg" alt="Using distance to assign and update clusters"/></div><p>It's worth noting that although the three cluster centers in the preceding diagram happen to be widely spaced apart, this is not always necessarily the case. Since they are selected at random, the three centers could have just as easily been three adjacent points. As the k-means algorithm is highly sensitive to the starting position of the cluster centers, this means that random chance may have a substantial impact on the final set of clusters.</p><p>To address this <a id="id775" class="indexterm"/>problem, k-means can be modified to use different methods for choosing the initial centers. For <a id="id776" class="indexterm"/>example, one variant chooses random values occurring anywhere in the feature space (rather than only selecting among the values observed in the data). Another option is to skip this step altogether; by randomly assigning each example to a cluster, the algorithm can jump ahead immediately to the update phase. Each of these approaches adds a particular bias to the final set of clusters, which you may be able to use to improve your results.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note37"/>Note</h3><p>In 2007, an algorithm <a id="id777" class="indexterm"/>called <span class="strong"><strong>k-means++</strong></span> was introduced, which proposes an alternative method for selecting the initial cluster centers. It purports to be an efficient way to get much closer to the optimal clustering solution while reducing the impact of random chance. For more information, refer to <span class="emphasis"><em>Arthur D, Vassilvitskii S</em></span>. k-means++: The advantages of careful seeding. <span class="emphasis"><em>Proceedings of the eighteenth annual ACM-SIAM symposium on discrete algorithms</em></span>. 2007:1027–1035.</p></div></div><p>After choosing the initial cluster centers, the other examples are assigned to the cluster center that is nearest according to the distance function. You will remember that we studied distance functions while learning about k-Nearest Neighbors. Traditionally, k-means uses Euclidean distance, but Manhattan distance or Minkowski distance are also sometimes used.</p><p>Recall that if <span class="emphasis"><em>n</em></span> <a id="id778" class="indexterm"/>indicates the <a id="id779" class="indexterm"/>number of features, the formula for Euclidean distance between example <span class="emphasis"><em>x</em></span> and example <span class="emphasis"><em>y</em></span> is:</p><div class="mediaobject"><img src="graphics/B03905_09_04.jpg" alt="Using distance to assign and update clusters"/></div><p>For instance, if we are comparing a guest with five computer science publications and one math publication to a guest with zero computer science papers and two math papers, we could compute this in R as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sqrt((5 - 0)^2 + (1 - 2)^2)</strong></span>
<span class="strong"><strong>[1] 5.09902</strong></span>
</pre></div><p>Using this distance function, we find the distance between each example and each cluster center. The example is then assigned to the nearest cluster center.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip106"/>Tip</h3><p>Keep in mind that as we are using distance calculations, all the features need to be numeric, and the values should be normalized to a standard range ahead of time. The methods discussed in <a class="link" href="ch03.html" title="Chapter 3. Lazy Learning – Classification Using Nearest Neighbors">Chapter 3</a>, <span class="emphasis"><em>Lazy Learning – Classification Using Nearest Neighbors</em></span>, will prove helpful for this task.</p></div></div><p>As shown in the following diagram, the three cluster centers partition the examples into three segments labeled <span class="strong"><strong>Cluster A</strong></span>, <span class="strong"><strong>Cluster B</strong></span>, and <span class="strong"><strong>Cluster C</strong></span>. The dashed lines indicate the boundaries for the <a id="id780" class="indexterm"/>
<span class="strong"><strong>Voronoi diagram</strong></span> created by the cluster centers. The Voronoi diagram indicates the areas that are closer to one cluster center than any other; the vertex where all the three boundaries meet is the maximal distance from all three cluster centers. Using these boundaries, we can easily see the regions claimed by each of the initial k-means seeds:</p><div class="mediaobject"><img src="graphics/B03905_09_05.jpg" alt="Using distance to assign and update clusters"/></div><p>Now that the initial assignment phase has been completed, the k-means algorithm proceeds to the <a id="id781" class="indexterm"/>update phase. The first step of updating the clusters involves shifting the initial centers to a new <a id="id782" class="indexterm"/>location, known as the <a id="id783" class="indexterm"/>
<span class="strong"><strong>centroid</strong></span>, which is calculated as the average position of the points currently assigned to that cluster. The following diagram illustrates how as the cluster centers shift to the new centroids, the boundaries in the Voronoi diagram also shift and a point that was once in <span class="strong"><strong>Cluster B</strong></span> (indicated by an arrow) is added to <span class="strong"><strong>Cluster A</strong></span>:</p><div class="mediaobject"><img src="graphics/B03905_09_06.jpg" alt="Using distance to assign and update clusters"/></div><p>As a result of this reassignment, the k-means algorithm will continue through another update phase. After shifting the cluster centroids, updating the cluster boundaries, and reassigning points into new clusters (as indicated by arrows), the figure looks like this:</p><div class="mediaobject"><img src="graphics/B03905_09_07.jpg" alt="Using distance to assign and update clusters"/></div><p>Because two more points were reassigned, another update must occur, which moves the centroids and updates the cluster boundaries. However, because these changes result in no reassignments, the k-means algorithm stops. The cluster assignments are now final:</p><div class="mediaobject"><img src="graphics/B03905_09_08.jpg" alt="Using distance to assign and update clusters"/></div><p>The final clusters <a id="id784" class="indexterm"/>can be reported in one of the two ways. First, you might simply report the cluster assignments <a id="id785" class="indexterm"/>such as A, B, or C for each example. Alternatively, you could report the coordinates of the cluster centroids after the final update. Given either reporting method, you are able to define the cluster boundaries by calculating the centroids or assigning each example to its nearest cluster.</p></div><div class="section" title="Choosing the appropriate number of clusters"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec53"/>Choosing the appropriate number of clusters</h3></div></div></div><p>In the <a id="id786" class="indexterm"/>introduction to k-means, we learned that the algorithm is sensitive to the randomly-chosen cluster centers. Indeed, if we had selected a different combination of three starting points in the previous example, we may have found clusters that split the data differently from what we had expected. Similarly, k-means is sensitive to the number of clusters; the choice requires a delicate balance. Setting <span class="emphasis"><em>k </em></span>to be very large will improve the homogeneity of the clusters, and at the same time, it risks overfitting the data.</p><p>Ideally, you will have <span class="emphasis"><em>a priori</em></span> knowledge (a prior belief) about the true groupings and you can apply this information to choosing the number of clusters. For instance, if you were clustering movies, you might begin by setting <span class="emphasis"><em>k</em></span> equal to the number of genres considered for the Academy Awards. In the data science conference seating problem that we worked through previously, <span class="emphasis"><em>k</em></span> might reflect the number of academic fields of study that were invited.</p><p>Sometimes the number of clusters is dictated by business requirements or the motivation for the analysis. For example, the number of tables in the meeting hall could dictate how many groups of people should be created from the data science attendee list. Extending this idea to another business case, if the marketing department only has resources to create three distinct advertising campaigns, it might make sense to set <span class="emphasis"><em>k = 3</em></span> to assign all the potential customers to one of the three appeals.</p><p>Without any prior knowledge, one rule of thumb suggests setting <span class="emphasis"><em>k</em></span> equal to the square root of <span class="emphasis"><em>(n / 2)</em></span>, where <span class="emphasis"><em>n</em></span> is the number of examples in the dataset. However, this rule of thumb is likely to result in an unwieldy number of clusters for large datasets. Luckily, there are other statistical methods that can assist in finding a suitable k-means cluster set.</p><p>A technique <a id="id787" class="indexterm"/>known as the <span class="strong"><strong>elbow method</strong></span> attempts to gauge how the homogeneity or heterogeneity within the clusters changes for various values of <span class="emphasis"><em>k</em></span>. As illustrated in the following diagrams, the homogeneity within clusters is expected to increase as additional clusters are added; similarly, heterogeneity will also continue to decrease with more clusters. As you could continue to see improvements until each example is in its own cluster, the goal is not to maximize homogeneity or minimize heterogeneity, but rather to find <span class="emphasis"><em>k</em></span> so that there are diminishing returns beyond that point. This value of <span class="emphasis"><em>k</em></span> is known as the <span class="strong"><strong>elbow point</strong></span> because it looks like an elbow.</p><div class="mediaobject"><img src="graphics/B03905_09_09.jpg" alt="Choosing the appropriate number of clusters"/></div><p>There are numerous statistics to measure homogeneity and heterogeneity within the clusters that can be used with the elbow method (the following information box provides a citation for more detail). Still, in practice, it is not always feasible to iteratively test a large number of <span class="emphasis"><em>k</em></span> values. This is in part because clustering large datasets can be fairly time consuming; clustering the data repeatedly is even worse. Regardless, applications requiring the exact optimal set of clusters are fairly rare. In most clustering applications, it suffices to choose a <span class="emphasis"><em>k</em></span> value based on convenience rather than strict performance requirements.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note38"/>Note</h3><p>For a very thorough review of the vast assortment of cluster performance measures, refer to: <span class="emphasis"><em>Halkidi M, Batistakis Y, Vazirgiannis M</em></span>. On clustering validation techniques. <span class="emphasis"><em>Journal of Intelligent Information Systems</em></span>. 2001; 17:107-145.</p></div></div><p>The process of <a id="id788" class="indexterm"/>setting <span class="emphasis"><em>k</em></span> itself can sometimes lead to interesting insights. By observing how the characteristics of the clusters change as <span class="emphasis"><em>k</em></span> is varied, one might infer where the data have naturally defined boundaries. Groups that are more tightly clustered will change a little, while less homogeneous groups will form and disband over time.</p><p>In general, it may be wise to spend little time worrying about getting <span class="emphasis"><em>k</em></span> exactly right. The next example will demonstrate how even a tiny bit of subject-matter knowledge borrowed from a Hollywood film can be used to set <span class="emphasis"><em>k</em></span> such that actionable and interesting clusters are found. As clustering is unsupervised, the task is really about what you make of it; the value is in the insights you take away from the algorithm's findings.</p></div></div></div></div>
<div class="section" title="Example &#x2013; finding teen market segments using k-means clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec42"/>Example – finding teen market segments using k-means clustering</h1></div></div></div><p>Interacting <a id="id789" class="indexterm"/>with friends on a <a id="id790" class="indexterm"/>
<span class="strong"><strong>social networking service</strong></span> (<span class="strong"><strong>SNS</strong></span>), such as Facebook, Tumblr, and Instagram has become a rite of passage for teenagers around the world. Having a relatively large amount of disposable income, these adolescents are a coveted demographic for businesses hoping to sell snacks, beverages, electronics, and hygiene products.</p><p>The many millions of teenage consumers using such sites have attracted the attention of marketers struggling to find an edge in an increasingly competitive market. One way to gain this edge is to identify segments of teenagers who share similar tastes, so that clients can avoid targeting advertisements to teens with no interest in the product being sold. For instance, sporting apparel is likely to be a difficult sell to teens with no interest in sports.</p><p>Given the text of teenagers' SNS pages, we can identify groups that share common interests such as <a id="id791" class="indexterm"/>sports, religion, or music. Clustering can automate the process of discovering the natural segments in this population. However, it will be up to us to decide whether or not the clusters are interesting and how we can use them for advertising. Let's try this process from start to finish.</p><div class="section" title="Step 1 – collecting data"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec103"/>Step 1 – collecting data</h2></div></div></div><p>For this <a id="id792" class="indexterm"/>analysis, we will use a dataset representing a random sample of 30,000 U.S. high school students who had profiles on a well-known SNS in 2006. To protect the users' anonymity, the SNS will remain unnamed. However, at the time the data was collected, the SNS was a popular web destination for US teenagers. Therefore, it is reasonable to assume that the profiles represent a fairly wide cross section of American adolescents in 2006.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip107"/>Tip</h3><p>This dataset was compiled by Brett Lantz while conducting sociological research on the teenage identities at the University of Notre Dame. If you use the data for research purposes, please cite this book chapter. The full dataset is available at the Packt Publishing website with the filename <code class="literal">snsdata.csv</code>. To follow along interactively, this chapter assumes that you have saved this file to your R working directory.</p></div></div><p>The data was sampled evenly across four high school graduation years (2006 through 2009) representing the senior, junior, sophomore, and freshman classes at the time of data collection. Using an automated web crawler, the full text of the SNS profiles were downloaded, and each teen's gender, age, and number of SNS friends was recorded.</p><p>A text mining tool was used to divide the remaining SNS page content into words. From the top 500 words appearing across all the pages, 36 words were chosen to represent five categories of interests: namely extracurricular activities, fashion, religion, romance, and antisocial behavior. The 36 words include terms such as <span class="emphasis"><em>football</em></span>, <span class="emphasis"><em>sexy</em></span>, <span class="emphasis"><em>kissed</em></span>, <span class="emphasis"><em>bible</em></span>, <span class="emphasis"><em>shopping</em></span>, <span class="emphasis"><em>death</em></span>, and <span class="emphasis"><em>drugs</em></span>. The final dataset indicates, for each person, how many times each word appeared in the person's SNS profile.</p></div><div class="section" title="Step 2 – exploring and preparing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec104"/>Step 2 – exploring and preparing the data</h2></div></div></div><p>We can <a id="id793" class="indexterm"/>use the <a id="id794" class="indexterm"/>default settings of <code class="literal">read.csv()</code> to load the data into a data frame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; teens &lt;- read.csv("snsdata.csv")</strong></span>
</pre></div><p>Let's also take a quick look at the specifics of the data. The first several lines of the <code class="literal">str()</code> output are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(teens)</strong></span>
<span class="strong"><strong>'data.frame':  30000 obs. of  40 variables:</strong></span>
<span class="strong"><strong> $ gradyear    : int  2006 2006 2006 2006 2006 2006 2006 2006 ...</strong></span>
<span class="strong"><strong> $ gender      : Factor w/ 2 levels "F","M": 2 1 2 1 NA 1 1 2 ...</strong></span>
<span class="strong"><strong> $ age         : num  19 18.8 18.3 18.9 19 ...</strong></span>
<span class="strong"><strong> $ friends     : int  7 0 69 0 10 142 72 17 52 39 ...</strong></span>
<span class="strong"><strong> $ basketball  : int  0 0 0 0 0 0 0 0 0 0 ...</strong></span>
</pre></div><p>As we had expected, the data include 30,000 teenagers with four variables indicating personal characteristics and 36 words indicating interests.</p><p>Do you notice anything strange around the <code class="literal">gender</code> row? If you were looking carefully, you may have noticed the <code class="literal">NA</code> value, which is out of place compared to the <code class="literal">1</code> and <code class="literal">2</code> values. The <code class="literal">NA</code> is R's way of telling us that the record has a missing value—we do not know the person's gender. Until now, we haven't dealt with missing data, but it can be a significant problem for many types of analyses.</p><p>Let's see how substantial this problem is. One option is to use the <code class="literal">table()</code> command, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(teens$gender)</strong></span>
<span class="strong"><strong>    F     M</strong></span>
<span class="strong"><strong>22054  5222</strong></span>
</pre></div><p>Although this command tells us how many <code class="literal">F</code> and <code class="literal">M</code> values are present, the <code class="literal">table()</code> function excluded the <code class="literal">NA</code> values rather than treating it as a separate category. To include the <code class="literal">NA</code> values (if there are any), we simply need to add an additional parameter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(teens$gender, useNA = "ifany")</strong></span>
<span class="strong"><strong>    F     M  &lt;NA&gt;</strong></span>
<span class="strong"><strong>22054  5222  2724</strong></span>
</pre></div><p>Here, we see that 2,724 records (9 percent) have missing gender data. Interestingly, there are over four times as many females as males in the SNS data, suggesting that males are not as inclined to use SNS websites as females.</p><p>If you examine the other variables in the data frame, you will find that besides <code class="literal">gender</code>, only <code class="literal">age</code> has missing values. For numeric data, the <code class="literal">summary()</code> command tells us the number of missing <code class="literal">NA</code> values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(teens$age)</strong></span>
<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's</strong></span>
<span class="strong"><strong>  3.086  16.310  17.290  17.990  18.260 106.900    5086</strong></span>
</pre></div><p>A total of 5,086 records (17 percent) have missing ages. Also concerning is the fact that the minimum and maximum values seem to be unreasonable; it is unlikely that a 3 year old or a 106<a id="id795" class="indexterm"/> year old is attending high school. To ensure that these extreme values don't cause problems for the analysis, we'll need to clean them up before moving on.</p><p>A more <a id="id796" class="indexterm"/>reasonable range of ages for the high school students includes those who are at least 13 years old and not yet 20 years old. Any age value falling outside this range should be treated the same as missing data—we cannot trust the age provided. To recode the age variable, we can use the <code class="literal">ifelse()</code> function, assigning <code class="literal">teen$age</code> the value of <code class="literal">teen$age</code> if the age is at least 13 and less than 20 years; otherwise, it will receive the value <code class="literal">NA</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; teens$age &lt;- ifelse(teens$age &gt;= 13 &amp; teens$age &lt; 20,</strong></span>
<span class="strong"><strong>                        teens$age, NA)</strong></span>
</pre></div><p>By rechecking the <code class="literal">summary()</code> output, we see that the age range now follows a distribution that looks much more like an actual high school:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(teens$age)</strong></span>
<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's</strong></span>
<span class="strong"><strong>  13.03   16.30   17.26   17.25   18.22   20.00    5523</strong></span>
</pre></div><p>Unfortunately, now we've created an even larger missing data problem. We'll need to find a way to deal with these values before continuing with our analysis.</p><div class="section" title="Data preparation – dummy coding missing values"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec54"/>Data preparation – dummy coding missing values</h3></div></div></div><p>An easy <a id="id797" class="indexterm"/>solution for handling the missing values is to exclude any record with a missing value. However, if you think through the implications of this practice, you might think twice before doing so—just because it is easy does not mean it is a good idea! The problem with this approach is that even if the missingness is not extensive, you can easily exclude large portions of the data.</p><p>For example, suppose that in our data, the people with the <code class="literal">NA</code> values for gender are completely different from those with missing age data. This would imply that by excluding those missing either gender or age, you would exclude <span class="emphasis"><em>9% + 17% = 26%</em></span> of the data, or over 7,500 records. And this is for missing data on only two variables! The larger the number of missing values present in a dataset, the more likely it is that any given record will be excluded. Fairly soon, you will be left with a tiny subset of data, or worse, the remaining records will be systematically different or non-representative of the full population.</p><p>An alternative <a id="id798" class="indexterm"/>solution for categorical variables like gender is to treat a missing value as a separate category. For instance, rather than limiting to female and male, we can add an additional category for the unknown gender. This allows us to utilize dummy coding, which was covered in <a class="link" href="ch03.html" title="Chapter 3. Lazy Learning – Classification Using Nearest Neighbors">Chapter 3</a>, <span class="emphasis"><em>Lazy Learning – Classification Using Nearest Neighbors</em></span>.</p><p>If you recall, dummy coding involves creating a separate binary (1 or 0) valued dummy variable for each level of a nominal feature except one, which is held out to serve as the reference group. The reason one category can be excluded is because its status can be inferred from the other categories. For instance, if someone is not female and not unknown gender, they must be male. Therefore, in this case, we need to only create dummy variables for female and unknown gender:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; teens$female &lt;- ifelse(teens$gender == "F" &amp;</strong></span>
<span class="strong"><strong>                           !is.na(teens$gender), 1, 0)</strong></span>
<span class="strong"><strong>&gt; teens$no_gender &lt;- ifelse(is.na(teens$gender), 1, 0)</strong></span>
</pre></div><p>As you might expect, the <code class="literal">is.na()</code> function tests whether gender is equal to <code class="literal">NA</code>. Therefore, the first statement assigns <code class="literal">teens$female</code> the value <code class="literal">1</code> if gender is equal to <code class="literal">F</code> and the gender is not equal to <code class="literal">NA</code>; otherwise, it assigns the value <code class="literal">0</code>. In the second statement, if <code class="literal">is.na()</code> returns <code class="literal">TRUE</code>, meaning the gender is missing, the <code class="literal">teens$no_gender</code> variable is assigned <code class="literal">1</code>; otherwise, it is assigned the value <code class="literal">0</code>. To confirm that we did the work correctly, let's compare our constructed dummy variables to the original <code class="literal">gender</code> variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(teens$gender, useNA = "ifany")</strong></span>
<span class="strong"><strong>    F     M  &lt;NA&gt;</strong></span>
<span class="strong"><strong>22054  5222  2724</strong></span>
<span class="strong"><strong>&gt; table(teens$female, useNA = "ifany")</strong></span>
<span class="strong"><strong>    0     1</strong></span>
<span class="strong"><strong> 7946 22054</strong></span>
<span class="strong"><strong>&gt; table(teens$no_gender, useNA = "ifany")</strong></span>
<span class="strong"><strong>    0     1</strong></span>
<span class="strong"><strong>27276  2724</strong></span>
</pre></div><p>The number <a id="id799" class="indexterm"/>of <code class="literal">1</code> values for <code class="literal">teens$female</code> and <code class="literal">teens$no_gender</code> matches the number of <code class="literal">F</code> and <code class="literal">NA</code> values, respectively, so we should be able to trust our work.</p></div><div class="section" title="Data preparation – imputing the missing values"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec55"/>Data preparation – imputing the missing values</h3></div></div></div><p>Next, let's <a id="id800" class="indexterm"/>eliminate the 5,523 missing age values. As age is numeric, it doesn't make sense to create an additional category for the unknown values—where would you rank "unknown" relative to the other ages? Instead, we'll use a different strategy known as <span class="strong"><strong>imputation</strong></span>, which involves <a id="id801" class="indexterm"/>filling in the missing data with a guess as to the true value.</p><p>Can you think of a way we might be able to use the SNS data to make an informed guess about a teenager's age? If you are thinking of using the graduation year, you've got the right idea. Most people in a graduation cohort were born within a single calendar year. If we can identify the typical age for each cohort, we would have a fairly reasonable estimate of the age of a student in that graduation year.</p><p>One way to find a typical value is by calculating the average or mean value. If we try to apply the <code class="literal">mean()</code> function, as we did for previous analyses, there's a problem:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mean(teens$age)</strong></span>
<span class="strong"><strong>[1] NA</strong></span>
</pre></div><p>The issue is that the mean is undefined for a vector containing missing data. As our age data contains missing values, <code class="literal">mean(teens$age)</code> returns a missing value. We can correct this by adding an additional parameter to remove the missing values before calculating the mean:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mean(teens$age, na.rm = TRUE)</strong></span>
<span class="strong"><strong>[1] 17.25243</strong></span>
</pre></div><p>This reveals that the average student in our data is about 17 years old. This only gets us part of the way there; we actually need the average age for each graduation year. You might be tempted to calculate the mean four times, but one of the benefits of R is that there's usually a way to avoid repeating oneself. In this case, the <code class="literal">aggregate()</code> function is the tool for the job. It computes statistics for subgroups of data. Here, it calculates the mean age by graduation year after removing the <code class="literal">NA</code> values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; aggregate(data = teens, age ~ gradyear, mean, na.rm = TRUE)</strong></span>
<span class="strong"><strong>  gradyear      age</strong></span>
<span class="strong"><strong>1     2006 18.65586</strong></span>
<span class="strong"><strong>2     2007 17.70617</strong></span>
<span class="strong"><strong>3     2008 16.76770</strong></span>
<span class="strong"><strong>4     2009 15.81957</strong></span>
</pre></div><p>The mean age differs by roughly one year per change in graduation year. This is not at all surprising, but a helpful finding for confirming our data is reasonable.</p><p>The <code class="literal">aggregate()</code> output is a data frame. This is helpful for some purposes, but would require extra work to merge <a id="id802" class="indexterm"/>back onto our original data. As an alternative, we can use the <code class="literal">ave()</code> function, which returns a vector with the group means repeated so that the result is equal in length to the original vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; ave_age &lt;- ave(teens$age, teens$gradyear, FUN =</strong></span>
<span class="strong"><strong>                  function(x) mean(x, na.rm = TRUE))</strong></span>
</pre></div><p>To impute these means onto the missing values, we need one more <code class="literal">ifelse()</code> call to use the <code class="literal">ave_age</code> value only if the original age value was <code class="literal">NA</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; teens$age &lt;- ifelse(is.na(teens$age), ave_age, teens$age)</strong></span>
</pre></div><p>The <code class="literal">summary()</code> results show that the missing values have now been eliminated:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(teens$age)</strong></span>
<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.</strong></span>
<span class="strong"><strong>  13.03   16.28   17.24   17.24   18.21   20.00</strong></span>
</pre></div><p>With the data ready for analysis, we are ready to dive into the interesting part of this project. Let's see whether our efforts have paid off.</p></div></div><div class="section" title="Step 3 – training a model on the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec105"/>Step 3 – training a model on the data</h2></div></div></div><p>To cluster the <a id="id803" class="indexterm"/>teenagers into marketing segments, we will use an implementation of k-means in the <code class="literal">stats</code> package, which should be included in your R installation by default. If by chance you do not have this package, you can install it as you would any other package and load it using the <code class="literal">library(stats)</code> command. Although there is no shortage of k-means functions available in various R packages, the <code class="literal">kmeans()</code> function in the <code class="literal">stats</code> package is widely used and provides a vanilla implementation of the algorithm.</p><div class="mediaobject"><img src="graphics/B03905_09_10.jpg" alt="Step 3 – training a model on the data"/></div><p>The <code class="literal">kmeans()</code> function requires a data frame containing only numeric data and a parameter specifying the desired number of clusters. If you have these two things ready, the actual process of building the model is simple. The trouble is that choosing the right combination of data and clusters can be a bit of an art; sometimes a great deal of trial and error is involved.</p><p>We'll start <a id="id804" class="indexterm"/>our cluster analysis by considering only the 36 features that represent the number of times various interests appeared on the teen SNS profiles. For convenience, let's make a data frame containing only these features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; interests &lt;- teens[5:40]</strong></span>
</pre></div><p>If you recall from <a class="link" href="ch03.html" title="Chapter 3. Lazy Learning – Classification Using Nearest Neighbors">Chapter 3</a>, <span class="emphasis"><em>Lazy Learning – Classification Using Nearest Neighbors</em></span>, a common practice employed prior to any analysis using distance calculations is to normalize or z-score standardize the features so that each utilizes the same range. By doing so, you can avoid a problem in which some features come to dominate solely because they have a larger range of values than the others.</p><p>The process of z-score standardization rescales features so that they have a mean of zero and a standard deviation of one. This transformation changes the interpretation of the data in a way that may be useful here. Specifically, if someone mentions football three times on their profile, without additional information, we have no idea whether this implies they like football more or less than their peers. On the other hand, if the z-score is three, we know that that they mentioned football many more times than the average teenager.</p><p>To apply the z-score standardization to the <code class="literal">interests</code> data frame, we can use the <code class="literal">scale()</code> function with <code class="literal">lapply()</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; interests_z &lt;- as.data.frame(lapply(interests, scale))</strong></span>
</pre></div><p>Since <code class="literal">lapply()</code> returns a matrix, it must be coerced back to data frame form using the <code class="literal">as.data.frame()</code> function.</p><p>Our last decision involves deciding how many clusters to use for segmenting the data. If we use too many clusters, we may find them too specific to be useful; conversely, choosing too few may result in heterogeneous groupings. You should feel comfortable experimenting <a id="id805" class="indexterm"/>with the values of <span class="emphasis"><em>k</em></span>. If you don't like the result, you can easily try another value and start over.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip108"/>Tip</h3><p>Choosing the number of clusters is easier if you are familiar with the analysis population. Having a hunch about the true number of natural groupings can save you some trial and error.</p></div></div><p>To help us predict the number of clusters in the data, I'll defer to one of my favorite films, The <span class="emphasis"><em>Breakfast Club</em></span>, a coming-of-age comedy released in 1985 and directed by John Hughes. The teenage characters in this movie are identified in terms of five stereotypes: a brain, an athlete, a basket case, a princess, and a criminal. Given that these identities prevail throughout popular teen fiction, five seems like a reasonable starting point for <span class="emphasis"><em>k</em></span>.</p><p>To use the k-means algorithm to divide the teenagers' interest data into five clusters, we use the <code class="literal">kmeans()</code> function on the <code class="literal">interests</code> data frame. Because the k-means algorithm utilizes random starting points, the <code class="literal">set.seed()</code> function is used to ensure that the results match the output in the examples that follow. If you recall from the previous chapters, this command initializes R's random number generator to a specific sequence. In the absence of this statement, the results will vary each time the k-means algorithm is run:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(2345)</strong></span>
<span class="strong"><strong>&gt; teen_clusters &lt;- kmeans(interests_z, 5)</strong></span>
</pre></div><p>The result of the <a id="id806" class="indexterm"/>k-means clustering process is a list named <code class="literal">teen_clusters</code> that stores the properties of each of the five clusters. Let's dig in and see how well the algorithm has divided the teens' interest data.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip109"/>Tip</h3><p>If you find that your results differ from those shown here, ensure that the <code class="literal">set.seed(2345)</code> command is run immediately prior to the <code class="literal">kmeans()</code> function.</p></div></div></div><div class="section" title="Step 4 – evaluating model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec106"/>Step 4 – evaluating model performance</h2></div></div></div><p>Evaluating <a id="id807" class="indexterm"/>clustering results can be somewhat subjective. Ultimately, the success or failure of the model hinges on whether the clusters are useful for their intended purpose. As the goal of this analysis was to identify clusters of teenagers with similar interests for marketing purposes, we will largely measure our success in qualitative terms. For other clustering applications, more quantitative measures of success may be needed.</p><p>One of the most basic ways to evaluate the utility of a set of clusters is to examine the number of examples falling in each of the groups. If the groups are too large or too small, they are not likely to be very useful. To obtain the size of the <code class="literal">kmeans()</code> clusters, use the <code class="literal">teen_clusters$size</code> component as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; teen_clusters$size</strong></span>
<span class="strong"><strong>[1]   871   600  5981  1034 21514</strong></span>
</pre></div><p>Here, we see the five clusters we requested. The smallest cluster has 600 teenagers (2 percent) while the largest cluster has 21,514 (72 percent). Although the large gap between the number of people in the largest and smallest clusters is slightly concerning, without examining these groups more carefully, we will not know whether or not this indicates a problem. It may be the case that the clusters' size disparity indicates something real, such as a big group of teens that share similar interests, or it may be a random fluke caused by the initial k-means cluster centers. We'll know more as we start to look at each cluster's homogeneity.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip110"/>Tip</h3><p>Sometimes, k-means may find extremely small clusters—occasionally, as small as a single point. This can happen if one of the initial cluster centers happens to fall on an outlier far from the rest of the data. It is not always clear whether to treat such small clusters as a true finding that represents a cluster of extreme cases, or a problem caused by random chance. If you encounter this issue, it may be worth re-running the k-means algorithm with a different random seed to see whether the small cluster is robust to different starting points.</p></div></div><p>For a more in-depth look at the clusters, we can examine the coordinates of the cluster centroids using the <code class="literal">teen_clusters$centers</code> component, which is as follows for the first four interests:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; teen_clusters$centers</strong></span>
<span class="strong"><strong>   basketball   football      soccer    softball</strong></span>
<span class="strong"><strong>1  0.16001227  0.2364174  0.10385512  0.07232021</strong></span>
<span class="strong"><strong>2 -0.09195886  0.0652625 -0.09932124 -0.01739428</strong></span>
<span class="strong"><strong>3  0.52755083  0.4873480  0.29778605  0.37178877</strong></span>
<span class="strong"><strong>4  0.34081039  0.3593965  0.12722250  0.16384661</strong></span>
<span class="strong"><strong>5 -0.16695523 -0.1641499 -0.09033520 -0.11367669</strong></span>
</pre></div><p>The rows of the output (labeled <code class="literal">1</code> to <code class="literal">5</code>) refer to the five clusters, while the numbers across each row indicate the cluster's average value for the interest listed at the top of the column. As the values are z-score standardized, positive values are above the overall mean level for all the teens and negative values are below the overall mean. For example, the third row has the highest value in the basketball column, which means that cluster <code class="literal">3</code> has the highest average interest in basketball among all the clusters.</p><p>By examining <a id="id808" class="indexterm"/>whether the clusters fall above or below the mean level for each interest category, we can begin to notice patterns that distinguish the clusters from each other. In practice, this involves printing the cluster centers and searching through them for any patterns or extreme values, much like a word search puzzle but with numbers. The following screenshot shows a highlighted pattern for each of the five clusters, for 19 of the 36 teen interests:</p><div class="mediaobject"><img src="graphics/B03905_09_11.jpg" alt="Step 4 – evaluating model performance"/></div><p>Given this subset of the interest data, we can already infer some characteristics of the clusters. <span class="strong"><strong>Cluster 3</strong></span> is substantially above the mean interest level on all the sports. This suggests that this may be a group of <span class="strong"><strong>Athletes</strong></span> per <span class="emphasis"><em>The Breakfast Club</em></span> stereotype. <span class="strong"><strong>Cluster 1</strong></span> includes the most mentions of "cheerleading," the word "hot," and is above the average level of football interest. Are these the so-called <span class="strong"><strong>Princesses</strong></span>?</p><p>By continuing to <a id="id809" class="indexterm"/>examine the clusters in this way, it is possible to construct a table listing the dominant interests of each of the groups. In the following table, each cluster is shown with the features that most distinguish it from the other clusters, and The <span class="emphasis"><em>Breakfast Club</em></span> identity that most accurately captures the group's characteristics.</p><p>Interestingly, <span class="strong"><strong>Cluster 5</strong></span> is distinguished by the fact that it is unexceptional; its members had lower-than-average levels of interest in every measured activity. It is also the single largest group in terms of the number of members. One potential explanation is that these users created a profile on the website but never posted any interests.</p><div class="mediaobject"><img src="graphics/B03905_09_12.jpg" alt="Step 4 – evaluating model performance"/></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip111"/>Tip</h3><p>When sharing the results of a segmentation analysis, it is often helpful to apply informative labels that simplify and capture the essence of the groups such as <span class="emphasis"><em>The Breakfast Club</em></span> typology applied here. The risk in adding such labels is that they can obscure the groups' nuances by stereotyping the group members. As such labels can bias our thinking, important patterns can be missed if labels are taken as the whole truth.</p></div></div><p>Given the table, a <a id="id810" class="indexterm"/>marketing executive would have a clear depiction of five types of teenage visitors to the social networking website. Based on these profiles, the executive could sell targeted advertising impressions to businesses with products relevant to one or more of the clusters. In the next section, we will see how the cluster labels can be applied back to the original population for such uses.</p></div><div class="section" title="Step 5 – improving model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec107"/>Step 5 – improving model performance</h2></div></div></div><p>Because <a id="id811" class="indexterm"/>clustering creates new information, the performance of a clustering algorithm depends at least somewhat on both the quality of the clusters themselves as well as what is done with that information. In the preceding section, we already demonstrated that the five clusters provided useful and novel insights into the interests of teenagers. By that measure, the algorithm appears to be performing quite well. Therefore, we can now focus our effort on turning these insights into action.</p><p>We'll begin by applying the clusters back onto the full dataset. The <code class="literal">teen_clusters</code> object created by the <code class="literal">kmeans()</code> function includes a component named <code class="literal">cluster</code> that contains the <a id="id812" class="indexterm"/>cluster assignments for all 30,000 individuals in the sample. We can add this as a column on the <code class="literal">teens</code> data frame with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; teens$cluster &lt;- teen_clusters$cluster</strong></span>
</pre></div><p>Given this new data, we can start to examine how the cluster assignment relates to individual characteristics. For example, here's the personal information for the first five teens in the SNS data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; teens[1:5, c("cluster", "gender", "age", "friends")]</strong></span>
<span class="strong"><strong>  cluster gender    age friends</strong></span>
<span class="strong"><strong>1       5      M 18.982       7</strong></span>
<span class="strong"><strong>2       3      F 18.801       0</strong></span>
<span class="strong"><strong>3       5      M 18.335      69</strong></span>
<span class="strong"><strong>4       5      F 18.875       0</strong></span>
<span class="strong"><strong>5       4   &lt;NA&gt; 18.995      10</strong></span>
</pre></div><p>Using the <code class="literal">aggregate()</code> function, we can also look at the demographic characteristics of the clusters. The mean age does not vary much by cluster, which is not too surprising as these teen identities are often determined before high school. This is depicted as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; aggregate(data = teens, age ~ cluster, mean)</strong></span>
<span class="strong"><strong>  cluster      age</strong></span>
<span class="strong"><strong>1       1 16.86497</strong></span>
<span class="strong"><strong>2       2 17.39037</strong></span>
<span class="strong"><strong>3       3 17.07656</strong></span>
<span class="strong"><strong>4       4 17.11957</strong></span>
<span class="strong"><strong>5       5 17.29849</strong></span>
</pre></div><p>On the other hand, there are some substantial differences in the proportion of females by cluster. This is a very interesting finding as we didn't use gender data to create the clusters, yet the clusters are still predictive of gender:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; aggregate(data = teens, female ~ cluster, mean)</strong></span>
<span class="strong"><strong>  cluster    female</strong></span>
<span class="strong"><strong>1       1 0.8381171</strong></span>
<span class="strong"><strong>2       2 0.7250000</strong></span>
<span class="strong"><strong>3       3 0.8378198</strong></span>
<span class="strong"><strong>4       4 0.8027079</strong></span>
<span class="strong"><strong>5       5 0.6994515</strong></span>
</pre></div><p>Recall that overall about 74 percent of the SNS users are female. <span class="strong"><strong>Cluster 1</strong></span>, the so-called <span class="strong"><strong>Princesses</strong></span>, is nearly 84 percent female, while <span class="strong"><strong>Cluster 2</strong></span> and <span class="strong"><strong>Cluster 5</strong></span> are only about 70 percent female. These disparities imply that there are differences in the interests that teen boys and girls discuss on their social networking pages.</p><p>Given our success in <a id="id813" class="indexterm"/>predicting gender, you might also suspect that the clusters are predictive of the number of friends the users have. This hypothesis seems to be supported by the data, which is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; aggregate(data = teens, friends ~ cluster, mean)</strong></span>
<span class="strong"><strong>  cluster  friends</strong></span>
<span class="strong"><strong>1       1 41.43054</strong></span>
<span class="strong"><strong>2       2 32.57333</strong></span>
<span class="strong"><strong>3       3 37.16185</strong></span>
<span class="strong"><strong>4       4 30.50290</strong></span>
<span class="strong"><strong>5       5 27.70052</strong></span>
</pre></div><p>On an average, <span class="strong"><strong>Princesses</strong></span> have the most friends (41.4), followed by <span class="strong"><strong>Athletes</strong></span> (37.2) and <span class="strong"><strong>Brains</strong></span> (32.6). On the low end are <span class="strong"><strong>Criminals</strong></span> (30.5) and <span class="strong"><strong>Basket Cases</strong></span> (27.7). As with gender, the connection between a teen's number of friends and their predicted cluster is remarkable, given that we did not use the friendship data as an input to the clustering algorithm. Also interesting is the fact that the number of friends seems to be related to the stereotype of each clusters' high school popularity; the stereotypically popular groups tend to have more friends.</p><p>The association among group membership, gender, and number of friends suggests that the clusters can be useful predictors of behavior. Validating their predictive ability in this way may make the clusters an easier sell when they are pitched to the marketing team, ultimately improving the performance of the algorithm.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec43"/>Summary</h1></div></div></div><p>Our findings support the popular adage that "birds of a feather flock together." By using machine learning methods to cluster teenagers with others who have similar interests, we were able to develop a typology of teen identities that was predictive of personal characteristics, such as gender and the number of friends. These same methods can be applied to other contexts with similar results.</p><p>This chapter covered only the fundamentals of clustering. As a very mature machine learning method, there are many variants of the k-means algorithm as well as many other clustering algorithms that bring unique biases and heuristics to the task. Based on the foundation in this chapter, you will be able to understand and apply other clustering methods to new problems.</p><p>In the next chapter, we will begin to look at methods for measuring the success of a learning algorithm, which are applicable across many machine learning tasks. While our process has always devoted some effort to evaluating the success of learning, in order to obtain the highest degree of performance, it is crucial to be able to define and measure it in the strictest terms.</p></div></body></html>
<html><head></head><body>
		<div class="Content" id="_idContainer084">
			<h1 id="_idParaDest-71"><em class="italics"><a id="_idTextAnchor073"/>Chapter 4</em></h1>
		</div>
		<div class="Content" id="_idContainer085">
			<h1 id="_idParaDest-72"><a id="_idTextAnchor074"/>Dimension Reduction and PCA</h1>
		</div>
		<div class="Content" id="_idContainer086">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Apply dimension reduction techniques.</li>
				<li class="bullets">Describe the concepts behind principal components and dimensionality reduction.</li>
				<li class="bullets">Apply principal component analysis (PCA) when solving problems using scikit-learn.</li>
				<li class="bullets">Compare manual PCA versus scikit-learn.</li>
			</ul>
			<p>In this chapter, we will look at dimension reduction and different dimension reduction techniques.</p>
		</div>
		<div class="Content" id="_idContainer131">
			<h2 id="_idParaDest-73"><a id="_idTextAnchor075"/>Introduction</h2>
			<p>This chapter is the first of a series of three chapters that investigate the use of different feature sets (or spaces) in our unsupervised learning algorithms, and we will start with a discussion around dimensionality reduction, specifically, PCA. We will then extend upon our understanding of the benefits of the different feature spaces through an exploration of two independently powerful machine learning architectures in neural network-based auto-encoders. Neural networks certainly have a well-deserved reputation for being powerful models in supervised learning problems, and, through the use of an autoencoder stage, have been shown to be sufficiently flexible for their application to unsupervised learning problems. Finally, we will build on our neural network implementation and dimensionality reduction as we cover t-distributed nearest neighbors in the final chapter of this micro-series. </p>
			<h3 id="_idParaDest-74"><a id="_idTextAnchor076"/>What Is Dimensionality Reduction?</h3>
			<p>Dimensionality reduction is an important tool in any data scientists' toolkit, and, due to its wide variety of use cases, is essentially assumed knowledge within the field. So, before we can consider reducing the dimensionality and why we would want to reduce it, we must first have a good understanding of what dimensionality is. To put it simply, dimensionality is the number of dimensions, features, or variables associated with a sample of data. Often, this can be thought of as a number of columns in a spreadsheet, where each sample is on a new row, and each column describes some attribute of the sample. The following table is an example:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer087">
					<img alt="" src="image/C12626_04_01.jpg"/>
				</div>
			</div>
			<h6>Figure 4.1: Two samples of data with three different features</h6>
			<p>In <em class="italics">Figure 4.1</em>, we have two samples of data, each with three independent features or dimensions. Depending upon the problem being solved, or the origin of this dataset, we may want to reduce the number of dimensions per sample without losing the provided information. This is where dimensionality reduction can be helpful.</p>
			<p>But how exactly can dimensionality reduction help us in solving problems? We will cover the applications in more detail in the following section; but let's say that we had a very large dataset of time series data, such as echo-cardiogram or ECG (also known as an EKG in some countries) signals as shown in the following figure:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer088">
					<img alt="" src="image/C12626_04_02.jpg"/>
				</div>
			</div>
			<h6>Figure 4.2: Electrocardiogram (ECG or EKG)</h6>
			<p>These signals were captured from your company's new model of watch, and we need to look for signs of a heart attack or stroke. In looking through the dataset, we can make a few observations:</p>
			<ul>
				<li>Most of the individual heartbeats are very similar.</li>
				<li>There is some noise in the data from the recording system or from the patient moving during the recording.</li>
				<li>Despite the noise, the heartbeat signals are still visible.</li>
				<li>Th<a id="_idTextAnchor077"/>ere is a lot of data – too much to be able to process using the hardware available on the watch.</li>
			</ul>
			<p>It is in such a situation that dimensionality reduction really shines! By using dimensionality reduction, we are able to remove much of the noise from the signal, which, in turn, will assist with the performance of the algorithms that are applied to the data as well as reduce the size of the dataset to allow for reduced hardware requirements. The techniques that we are going to discuss in this chapter, in particular, PCA and autoencoders, have been well applied in research and industry to effectively process, cluster, and classify such datasets. By the end of this chapter, you will be able to apply these techniques to your own data and hopefully see an increase in the performance of your own machine learning systems.</p>
			<h3 id="_idParaDest-75"><a id="_idTextAnchor078"/>Applications of Dimensionality Reduction</h3>
			<p>Before we start a detailed investigation of dimensionality reduction and PCA, we will discuss some of the common applications for these techniques:</p>
			<ul>
				<li><strong class="keyword">Pre-processing/feature engineering</strong>: One of the most common implementations is in the pre-processing or feature engineering stages of developing a machine learning solution. The quality of the information provided during the algorithm development, as well as the correlation between the input data and the desired result, is critical in order for a high-performing solution to be designed. In this situation, PCA can provide assistance, as we are able to isolate the most important components of information from the data and provide this to the model so that only the most relevant information is being provided. This can also have a secondary benefit in that we have reduced the number of features being provided to the model, so there can be a corresponding reduction in the number of calculations to be completed. This can reduce the overall training time for the system.</li>
				<li><strong class="keyword">Noise reduction</strong>: Dimensionality reduction can also be used as an effective noise reduction/filtering technique. It is expected that the noise within a signal or dataset does not comprise a large component of the variation within the data. Thus, we can remove some of the noise from the signal by removing the smaller components of variation and then restoring the data back to the original dataspace. In the following example, the image on the left has been filtered to the first 20 most significant sources of data, which gives us the image on the right. We can see that the quality of the image has reduced, but the critical information is still there:</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer089">
					<img alt="Figure 4.3: An image filtered with dimensionality reduction. Left: The original image (Photo by Arthur Brognoli from Pexels), Right: The filtered image" src="image/C12626_04_03.jpg"/>
				</div>
			</div>
			<h6>Figure 4.3: An image filtered with dimensionality reduction. Left: The original image (Photo by Arthur Brognoli from Pexels), Right: The filtered image</h6>
			<h4>Note</h4>
			<p class="callout">This photograph was taken by Arthur Brognoli from Pexels and is available for free use under <a href="https://www.pexels.com/photo-license/">https://www.pexels.com/photo-license/</a>.</p>
			<ul>
				<li><strong class="keyword">Generating plausible artificial datasets</strong>: As PCA divides the dataset into the components of information (or variation), we can investigate the effects of each components or generate new dataset samples by adjusting the ratios between the eigenvalues. We can scale these components, which, in effect, increases or decreases the importance of that specific component. This is also referred to as <strong class="keyword">statistical shape modelling</strong>, as one common method is to use it to create plausible variants of shapes. It is also used detect facial landmarks in images in the process of <strong class="keyword">active shape modelling</strong>.</li>
				<li><strong class="keyword">Financial modelling/risk analysis</strong>: Dimensionality reduction provides a useful toolkit for the finance industry, as being able to consolidate a large number of individual market metrics or signals into a smaller number of components allows for faster, and more efficient computations. Similarly, the components can be used to highlight those higher-risk products/companies.</li>
			</ul>
			<h3 id="_idParaDest-76"><a id="_idTextAnchor079"/>The Curse of Dimensionality</h3>
			<p>Before we can understand the benefits to using dimensionality reduction techniques, we must first understand why the dimensionality of feature sets need to be reduced at all. The <strong class="keyword">curse of dimensionality</strong> is a phrase commonly used to describe issues that arise when working with data that has a high number of dimensions in the feature space; for example, the number of attributes that are collected for each sample. Consider a dataset of point locations within a game of <em class="italics">Pac-Man</em>. Your character, Pac-Man, occupies a position within the virtual world defined by two dimensions or coordinates (<em class="italics">x</em>, <em class="italics">y</em>). Let's say that we are creating a new computer enemy: an AI-driven ghost to play against, and that it requires some information regarding our character to make its own game logic decisions. For the bot to be effective, we require the player's position (<em class="italics">x</em>, <em class="italics">y</em>) and their velocity in each of the directions (<em class="italics">vx</em>, <em class="italics">vy</em>) in addition to the players last five (<em class="italics">x</em>, <em class="italics">y</em>) positions, the number of remaining hearts, and the number of remaining power-pellets in the maze (power-pellets temporarily allow Pac-Man to eat ghosts). Now, for each moment in time, our bot requires 16 individual features (or dimensions) to make its decisions. This is clearly a lot more than just the two dimensions as provided by the position.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer090">
					<img alt="" src="image/C12626_04_04.jpg"/>
				</div>
			</div>
			<h6><a id="_idTextAnchor080"/></h6>
			<h6>Figure 4.4: Dimensions in a PacMan game</h6>
			<p>To explain the concept of dimensionality reduction, we will consider a fictional dataset (see <a href="C12626_04_ePub_Final_SZ.xhtml#_idTextAnchor082"><em class="italics">Figure 4.5</em></a>) of <em class="italics">x</em> and <em class="italics">y</em> coordinates as features, giving two dimensions in the feature space. It should be noted that this example is by no means a mathematical proof, but is rather intended to provide a means of visualizing the effect of increased dimensionality. In this dataset, we have six individual samples (or points) and we can visualize the currently occupied volume within the feature space of approximately (3 – 1) x (4 – 2) = 2 x 2 = 4 squared units.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer091">
					<img alt="" src="image/C12626_04_05.jpg"/>
				</div>
			</div>
			<h6><a id="_idTextAnchor081"/>Figur<a id="_idTextAnchor082"/>e 4.5: Data in a 2D feature space</h6>
			<p>Suppose, the dataset comprises the same number of points, but with an additional feature (the <em class="italics">z</em> coordinate) to each sample. The occupied data volume is now approximately 2 x 2 x 2 = 8 cubed units. So, we now have the same number of samples, but the space enclosing the dataset is now larger. As such, the data takes up less relative volume in the available space and is now sparser. This is the curse of dimensionality; as we increase the number of available features, we increase the sparsity of the data, and, in turn, make statistically valid correlations more difficult. Looking back to our example of creating a video game bot to play against a human player, we have 12 features that are a mix of different feature types: speed, velocity, acceleration, skill level, selected weapon, and available ammunition. Depending on the range of possible values for each of these features and the variance to the dataset provided by each feature, the data could be extremely sparse. Even within the constrained world of Pac-Man, the potential variance of each of the features could be quite large, some much larger than others.</p>
			<p>So, without dealing with the sparsity of the dataset, we have more information with the additional feature(s), but may not be able to improve the performance of our machine learning model, as the statistical correlations are more difficult. What we would like to do is keep the useful information provided by the extra features but minimize the negative effect of sparsity. This is exactly what dimensionality reduction techniques are designed to do and these can be extremely powerful in increasing the performance of your machine learning model.</p>
			<p>Thro<a id="_idTextAnchor083"/>ughout this chapter, we will discuss a number of different dimensionality reduction techniques and will cover one of the most important and useful methods, PCA, in greater detail with a worked example.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor084"/>Overview of Dimensionality Reduction Techniques</h2>
			<p>As discussed in the Introduction section, the goal of any dimensionality reduction technique is to manage the sparsity of the dataset while keeping the useful information that is provided, so dimensionality reduction is typically an important pre-processing step used before a classification stage. Most dimensionality reduction techniques aim to complete this task using a process of <strong class="keyword">feature projection</strong>, which adjusts the data from the higher dimensional space into a space with fewer dimensions to remove the sparsity from the data. Again, as a means of visualizing the projection process, consider a sphere in a 3D space. We can project the sphere into lower 2D space into a circle with some information loss (the value for the <em class="italics">z</em> coordinate) but retaining much of the information that describes its original shape. We still know the origin, radius, and manifold (outline) of the shape, and it is still very clear that it is a circle. So, if we were given just the 2D projection, it would also be possible to re-create the original 3D shape with this information. So, depending upon the problem that we are trying to solve, we may have reduced the dimensionality while retaining the important information:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer092">
					<img alt="Figure 4.6: A projection of a 3D sphere into a 2D space" src="image/C12626_04_06.jpg"/>
				</div>
			</div>
			<h6>Figure 4.6: A projection of a 3D sphere into a 2D space</h6>
			<p>The secondary benefit that can be obtained by pre-processing the dataset with a dimensionality reduction stage is the improved computational performance that can be achieved. As the data has been projected into a lower dimensional space, it will contain fewer, but potentially more powerful, features. The fact that there are fewer features means that, during later classification or regression stages, the size of the dataset being processed is significantly smaller. This will potentially reduce the required system resources and processing time for classification/regression, and, in some cases, the dimensionality reduction technique can also be used  directly to complete the analysis..</p>
			<p>This analogy also introduces one of the important considerations of dimensionality reduction. We are always trying to balance the information loss resulting from the projection into lower dimensional space with reducing the sparsity of the data. Depending upon the nature of the problem and the dataset being used, the correct balance could present itself and be relatively straightforward. In some applications, this decision may rely on the outcome of additional validation methods, such as cross-validation (particularly in supervised learning problems) or the assessment of experts in your problem domain.</p>
			<p>One way we like to think about this trade-off in dimensionality reduction is to consider compressing a file or image on a computer for transfer. Dimensionality reduction techniques, such as PCA, are essentially methods of compressing information into a smaller size for transfer, and, in many compression methods, some losses occur as a result of the compression process. Sometimes, these losses are acceptable; if we are transferring a 50 MB image and need to shrink it to 5 MB for transfer, we can expect to still be able to see the main subject of the image, but perhaps some smaller background features will become too blurry to see. We would also not expect to be able to restore the original image to a pixel-perfect representation from the compressed version, but we could expect to restore it with some additional artefacts, such as blurring.</p>
			<h3 id="_idParaDest-78"><a id="_idTextAnchor085"/>Dimensionality Reduction and Unsupervised Learning</h3>
			<p>Dimensionality reduction techniques have many uses in machine learning, as the ability to extract the useful information of a dataset can provide performance boosts in many machine learning problems. They are particularly useful in unsupervised learning as opposed to supervised learning methods, as the dataset does not contain any ground truth labels or targets to achieve. In unsupervised learning, the training environment is being used to organize the data in a way that is appropriate for the problem being solved (for example, clustering in a classification problem), which is typically based on the most important information in the dataset. Dimensionality reduction provides an effective means of extracting the important information, and, as there are a number of different methods that we could use, it is beneficial to review some of the available options:</p>
			<ul>
				<li><strong class="keyword">Linear Discriminant Analysis</strong> (<strong class="keyword">LDA</strong>): This is a particularly handy technique that can be used for both classification as well as dimensionality reduction. LDA will be covered in more detail in <em class="italics">Chapter 7</em>: <em class="italics">Topic Modeling</em>.</li>
				<li><strong class="keyword">Non-negative matrix factorization</strong> (<strong class="keyword">NNMF</strong>): Like many of the dimensionality reduction techniques, this relies upon the properties of linear algebra to reduce the number of features in the dataset. NNMF will also be covered in more detail in <em class="italics">Chapter 7</em>, <em class="italics">Topic Modeling</em>.</li>
				<li><strong class="keyword">Singular Value Decomposition</strong> (<strong class="keyword">SVD</strong>): This is somewhat related to PCA (which is covered in more detail in this chapter) and is also a matrix decomposition process not too dissimilar to NNMF. </li>
				<li><strong class="keyword">Independant Component Analysis</strong> (<strong class="keyword">ICA</strong>): This also shares some similarities to SVD and PCA, but relaxing the assumption of the data being a Gaussian distribution allows for non-Gaussian data to be separated.</li>
			</ul>
			<p>Each of the methods described so far all use linear separation to reduce the sparsity of the data in their original implementation. Some of these methods also have variants that use non-linear kernel functions in the separation process, providing the ability to reduce the sparsity in a non-linear fashion. Depending on the dataset being used, a non-linear kernel may be more effective at extracting the most useful information from the signal.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor086"/>PCA</h2>
			<p>As we described previously, PCA is a commonly used and very effective dimensionality reduction technique, which often forms a pre-processing stage for a number of machine learning models and techniques. For this reason, we will dedicate this section of the book to looking at PCA in more detail than any of the other methods. PCA reduces the sparsity in the dataset by separating the data into a series of components where each component represents a source of information within the data. As its name suggests, the first component produced in PCA, <strong class="keyword">the principal component</strong> comprises the majority of information or variance within the data. The principal component can often be thought of as contributing the most amount of interesting information in addition to the mean. With each subsequent component, less information, but more subtlety, is contributed to the compressed data. If we consider all of these components together, there will be no benefit from using PCA, as the original dataset will be returned. To clarify this process and the information returned by PCA, we will use a worked example, completing the PCA calculations by hand. But first, we must review some foundational statistical concepts, which are required to execute the PCA calculations.</p>
			<h3 id="_idParaDest-80"><a id="_idTextAnchor087"/>Mean</h3>
			<p>The mean, or the average value, is simply the addition of all values divided by the number of values in the set.</p>
			<h3 id="_idParaDest-81"><a id="_idTextAnchor088"/>Standard Deviation</h3>
			<p>Often referred to as the spread of the data and related to the variance, the standard deviation is a measure of how much of the data lies within proximity to the mean. In a normally distributed dataset, approximately 68% of the dataset lies within one standard deviation of the mean.</p>
			<p>The relationship between the variance and standard deviation is quite a simple one – the variance is the standard deviation squared.</p>
			<h3 id="_idParaDest-82"><a id="_idTextAnchor089"/>Covariance</h3>
			<p>Where standard deviation or variance is the spread of the data calculated on a single dimension, the covariance is the variance of one dimension (or feature) against another. When the covariance of a dimension is computed against itself, the result is the same as simply calculating the variance for the dimension. </p>
			<h3 id="_idParaDest-83"><a id="_idTextAnchor090"/>Covariance Matrix</h3>
			<p>A covariance matrix is a matrix representation of the possible covariance values that can be computed for a dataset. Other than being particularly useful in data exploration, covariance matrices are also required for executing the PCA of a dataset. To determine the variance of one feature with respect to another, we simply look up the corresponding value in the covariance matrix. Referring to <em class="italics">Figure 4.7</em> we can see that, in column 1, row 2, the value is the variance of feature or dataset <em class="italics">Y</em> with respect to <em class="italics">X</em> (<em class="italics">cov(Y, X))</em>. We can also see that there is a diagonal column of covariance values computed against the same feature or dataset; for example, <em class="italics">cov(X, X)</em>. In this situation, the value is simply the variance of <em class="italics">X</em>.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer093">
					<img alt="Figure 4.7: The covariance matrix" src="image/C12626_04_07.jpg"/>
				</div>
			</div>
			<h6>Figure 4.7: The covariance matrix</h6>
			<p>Typically, the exact values of each of the covariances are not as interesting as looking at the magnitude and relative size of each of the covariances within the matrix. A large value of the covariance of one feature against another would suggest that one feature changes significantly with respect to the other, while a value close to zero would signify very little change. The other interesting aspect of the covariance to look for is the sign associated with the covariance; a positive value indicates that as one feature increases or decreases then so does the other, while a negative covariance indicates that the two features diverge from each other with one increasing as the other decreases or vice versa.</p>
			<p>Thankfully, <strong class="inline">numpy</strong> and <strong class="inline">scipy</strong> provide functions to efficiently make these calculations for you. In the next exercise, we will compute these values in Python.</p>
			<h3 id="_idParaDest-84">Exercis<a id="_idTextAnchor091"/>e 11: Understanding the Foundational Concepts of Statistics</h3>
			<p>In this exercise, we will briefly review how to compute some of the foundational statistical concepts using both the <strong class="inline">numpy</strong> and <strong class="inline">pandas</strong> Python packages. In this exercise, we will use dataset of measurements of different Iris flower species, created in 1936 by the British biologist and statistician Sir Ronald Fisher. The dataset, which can be found in the accompanying source code, comprises four individual measurements (sepal width and length, and petal width and length) of three different Iris flower varieties: Iris setosa, Iris versicolor, and Iris virginica.</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>. It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise11">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise11</a>.</p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Import the <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong> packages for use:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Load the dataset and preview the first five lines of data:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer094"><img alt="Figure 4.8: The head of the data" src="image/C12626_04_08.jpg"/></div><h6>Figure 4.8: The head of the data</h6></li>
				<li>We only require the <strong class="inline">Sepal Length</strong> and <strong class="inline">Sepal Width</strong> features, so remove the other columns:<p class="snippet">df = df[['Sepal Length', 'Sepal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer095"><img alt="Figure 4.9: The head after cleaning the data" src="image/C12626_04_09.jpg"/></div><h6>Figure 4.9: The head after cleaning the data</h6></li>
				<li>Visualize the dataset by plotting the <strong class="inline">Sepal Length</strong> versus <strong class="inline">Sepal Width</strong> values:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.scatter(df['Sepal Length'], df['Sepal Width']);</p><p class="snippet">plt.xlabel('Sepal Length (mm)');</p><p class="snippet">plt.ylabel('Sepal Width (mm)');</p><p class="snippet">plt.title('Sepal Length versus Width');</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer096"><img alt="Figure 4.10: Plot of the data" src="image/C12626_04_10.jpg"/></div><h6>Figure 4.10: Plot of the data</h6></li>
				<li>Compute the mean value using the <strong class="inline">pandas</strong> method:<p class="snippet">df.mean()</p><p>The output is as follows:</p><p class="snippet">Sepal Length    5.843333</p><p class="snippet">Sepal Width     3.054000</p><p class="snippet">dtype: float64</p></li>
				<li>Compute the mean value using the <strong class="inline">numpy</strong> method:<p class="snippet">np.mean(df.values, axis=0)</p><p>The output is as follows:</p><p class="snippet">array([5.84333333, 3.054     ])</p></li>
				<li>Compute the standard deviation value using the <strong class="inline">pandas</strong> method:<p class="snippet">df.std()</p><p>The output is as follows:</p><p class="snippet">Sepal Length    0.828066</p><p class="snippet">Sepal Width     0.433594</p><p class="snippet">dtype: float64</p></li>
				<li>Compute the standard deviation value using the <strong class="inline">numpy</strong> method:<p class="snippet">np.std(df.values, axis=0)</p><p>The output is as follows:</p><p class="snippet">array([0.82530129, 0.43214658])</p></li>
				<li>Compute the variance values using the <strong class="inline">pandas</strong> method:<p class="snippet">df.var()</p><p>The output is as follows:</p><p class="snippet">Sepal Length    0.685694</p><p class="snippet">Sepal Width     0.188004</p><p class="snippet">dtype: float64</p></li>
				<li>Compute the variance values using the <strong class="inline">numpy</strong> method:<p class="snippet">np.var(df.values, axis=0)</p><p>The output is as follows:</p><p class="snippet">array([0.68112222, 0.18675067])</p></li>
				<li>Compute the covariance matrix using the <strong class="inline">pandas</strong> method:<p class="snippet">df.cov()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer097"><img alt="Figure 4.11: Covariance matrix using the Pandas method" src="image/C12626_04_11.jpg"/></div><h6>Figure 4.11: Covariance matrix using the Pandas method</h6></li>
				<li>Compute the covariance matrix using the <strong class="inline">numpy</strong> method:<p class="snippet">np.cov(df.values.T)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer098">
					<img alt="Figure 4.12: The covariance matrix using the NumPy method" src="image/C12626_04_12.jpg"/>
				</div>
			</div>
			<h6>Figure 4.12: The covariance matrix using the NumPy method</h6>
			<p>Now that we know how to compute the foundational statistic values, we will turn our attention to the remaining components of PCA.</p>
			<h3 id="_idParaDest-85"><a id="_idTextAnchor092"/>Eigenvalues and Eigenvectors</h3>
			<p>The mathematical concept of eigenvalues and eigenvectors is a very important one in the fields of physics and engineering, and they also form the final steps in computing the principal components of a dataset. The exact mathematical definition of eigenvalues and eigenvectors is outside the scope of this book, as it is quite involved and requires a reasonable understanding of linear algebra. The linear algebra equation to decompose a dataset (<em class="italics">a)</em> into eigenvalues (<em class="italics">S</em>) and eigenvectors (<em class="italics">U</em>) is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer099">
					<img alt="" src="image/C12626_04_13.jpg"/>
				</div>
			</div>
			<h6>Figure 4.13: An eigenvector/eigenvalue decomposition</h6>
			<p>In <em class="italics">Figure 4.13</em>, <em class="italics">U</em> and <em class="italics">V</em> are related as the left and right values of dataset <em class="italics">a</em>. If <em class="italics">a</em> has the shape <em class="italics">m x n</em>, then <em class="italics">U</em> will contain values in the shape <em class="italics">m x m</em> and <em class="italics">V</em> in the shape <em class="italics">n x n</em>.</p>
			<p>Put simply, in the context of PCA:</p>
			<ul>
				<li><strong class="keyword">Eigenvectors</strong> (<em class="italics">U</em>) are the components contributing information to the dataset as described in the first paragraph of this section on principal components. Each eigenvector describes some amount of variability within the dataset.</li>
				<li><strong class="keyword">Eigenvalues</strong> (<em class="italics">S</em>) are the individual values that describe how much contribution each eigenvector provides to the dataset. As we described previously, the signal eigenvector that describes the largest contribution is referred to as the principal component, and as such, will have the largest eigenvalue. Accordingly, the eigenvector with the smallest eigenvalue contributes the least amount of variance or information to the data.</li>
			</ul>
			<h3 id="_idParaDest-86"><a id="_idTextAnchor093"/>Exercise 12: Computing Eigenvalues and Eigenvectors</h3>
			<p>As we discussed previously, deriving and computing the eigenvalues and eigenvectors manually is a little involved and is not in the scope of this book. Thankfully, <strong class="inline">numpy</strong> provides all the functionality for us to compute these values. Again, we will use the Iris dataset for this example:</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>.</p>
			<p class="callout">It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise12">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise12</a>.</p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Import the <strong class="inline">pandas</strong> and <strong class="inline">numpy</strong> packages:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p></li>
				<li>Load the dataset:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer100"><img alt="Figure 4.14: The first five rows of the dataset" src="image/C12626_04_14.jpg"/></div><h6>Figure 4.14: The first five rows of the dataset</h6></li>
				<li>Again, we only require the <strong class="inline">Sepal Length</strong> and <strong class="inline">Sepal Width</strong> features, so remove the other columns:<p class="snippet">df = df[['Sepal Length', 'Sepal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer101"><img alt="Figure 4.15: The Sepal Length and Sepal Width feature" src="image/C12626_04_15.jpg"/></div><h6>Figure 4.15: The Sepal Length and Sepal Width feature</h6></li>
				<li>From NumPy's linear algebra module, use the single value decomposition function to compute the <strong class="inline">eigenvalues</strong> and <strong class="inline">eigenvectors</strong>:<p class="snippet">eigenvectors, eigenvalues, _ = np.linalg.svd(df.values, full_matrices=False)</p><h4>Note</h4><p class="callout">The use of the <strong class="inline">full_matrices=False</strong> function argument is a flag for the function to return the eigenvectors in the shape we need; that is, # Samples x # Features.</p></li>
				<li>Look at the eigenvalues; we can see that the first value is the largest, so the first eigenvector contributes the most information:<p class="snippet">eigenvalues</p><p>The output is as follows:</p><p class="snippet">array([81.25483015,  6.96796793])</p></li>
				<li>It is handy to look at eigenvalues as a percentage of the total variance within the dataset. We will use a cumulative sum function to do this:<p class="snippet">eigenvalues = np.cumsum(eigenvalues)</p><p class="snippet">eigenvalues</p><p>The output is as follows:</p><p class="snippet">array([81.25483015, 88.22279808])</p></li>
				<li>Divide by the last or maximum value to convert to a percentage:<p class="snippet">eigenvalues /= eigenvalues.max()</p><p class="snippet">eigenvalues</p><p>The output is as follows:</p><p class="snippet">array([0.92101851, 1.        ])</p><p>We can see here that the first (or principal) component comprises 92% of the variation within the data, and thus, most of the information.</p></li>
				<li>Now, let's look at the <strong class="inline">eigenvectors</strong>:<p class="snippet">eigenvectors</p><p>A section of the output is as follows:</p><div class="IMG---Figure" id="_idContainer102"><img alt="" src="image/C12626_04_16.jpg"/></div><h6>Figure 4.16: Eigenvectors</h6></li>
				<li>Confirm that the shape of the eigenvector matrix is in the for <strong class="inline"># Samples x # Features</strong>; that is, <strong class="inline">150</strong> x <strong class="inline">2</strong>:<p class="snippet">eigenvectors.shape</p><p>The output is as follows:</p><p class="snippet">(150, 2)</p></li>
				<li>So, from the eigenvalues, we saw that the principal component was the first eigenvector. Look at the values for the first eigenvector:<p class="snippet">P = eigenvectors[0]</p><p class="snippet">P</p><p>The output is as follows:</p><p class="snippet">array([-0.07553027, -0.11068158])</p></li>
			</ol>
			<p>We have decomposed the dataset down into the principal components, and, using the eigenvectors, we can further reduce the dimensionality of the available data. In the later examples, we will consider PCA and apply this technique to an example dataset.</p>
			<h3 id="_idParaDest-87"><a id="_idTextAnchor094"/>The Process of PCA</h3>
			<p>Now we have all of the pieces ready to complete PCA to reduce the number of dimensions in a dataset.</p>
			<p>The overall algorithm for completing PCA is as follows:</p>
			<ol>
				<li value="1">Import the required Python packages (<strong class="inline">numpy</strong> and <strong class="inline">pandas</strong>).</li>
				<li>Load the entire dataset.</li>
				<li>From the available data, select the features that you wish to use in the dimensionality reduction.<h4>Note</h4><p class="callout">If there is a significant difference in the scale between the features of the dataset; for example, one feature ranges in values between 0 and 1, and another between 100 and 1,000, you may need to normalize one of the features, as such differences in magnitude can eliminate the effect of the smaller features. In such a situation, you may need to divide the larger feature into its maximum value. </p><p class="callout">As an example, have a look at this:</p><p class="callout"><strong class="inline">x1 = [0.1, 0.23, 0.54, 0.76, 0.78]</strong></p><p class="callout"><strong class="inline">x2 = [121, 125, 167, 104, 192]</strong></p><p class="callout"><strong class="inline">x2 = x2 / np.max(x2) # Normalise x2 to be between 0 and 1</strong></p></li>
				<li>Compute the <strong class="inline">covariance</strong> matrix of the selected (and possibly normalized) data.</li>
				<li>Compute the eigenvalues and eigenvectors of the <strong class="inline">covariance</strong> matrix.</li>
				<li>Sort the eigenvalues (and corresponding eigenvectors) from highest to lowest value eigenvalue.</li>
				<li>Compute the eigenvalues as a percentage of the total variance within the dataset.</li>
				<li>Select the number of eigenvalues (and corresponding eigenvectors) required to comprise a pre-determined value of a minimum composition variance.<h4>Note</h4><p class="callout">At this stage, the sorted eigenvalues represent a percentage of the total variance within the dataset. As such, we can use these values to select the number of eigenvectors required, either for the problem being solved or to sufficiently reduce the size of the dataset being applied in the model. For example, say that we required at least 90% of the variance to be accounted for within the output of PCA. We would then select the number of eigenvalues (and corresponding eigenvectors) that comprise at least 90% of the variance.</p></li>
				<li>Multiply the dataset by the selected eigenvectors and you have completed a PCA, reducing the number of features representing the data.</li>
				<li>Plot the result.</li>
			</ol>
			<p>Before moving on to the next exercise, note that <strong class="keyword">transpose</strong> is a term from linear algebra that means to swap the rows with the columns and vice versa. Say we had a matrix of <img alt="" src="image/C12626_04_Formula_01.png"/>, then the transpose of <em class="italics">X</em> would be <img alt="" src="image/C12626_04_Formula_02.png"/>.</p>
			<h3 id="_idParaDest-88"><a id="_idTextAnchor095"/>Exercise 13: Manually Executing PCA</h3>
			<p>For this exercise, we will be completing PCA manually, again using the Iris dataset. For this example, we want to sufficiently reduce the number of dimensions within the dataset to comprise at least 75% of the available variance:</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>. It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise13">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise13</a>. </p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Import the <strong class="inline">pandas</strong> and <strong class="inline">numpy</strong> packages:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Load the datas<a id="_idTextAnchor096"/>et:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer105"><img alt="Figure 4.17: The first five rows of the dataset" src="image/C12626_04_17.jpg"/></div><h6>Figure 4.17: The first five rows of the dataset</h6></li>
				<li>Again, we only require the <strong class="inline">Sepal Length</strong> and <strong class="inline">Sepal Width</strong> features, so remove the other columns. In this example, we are not normalizing the selected dataset:<p class="snippet">df = df[['Sepal Length', 'Sepal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer106"><img alt="Figure 4.18: The sepal length and sepal width feature" src="image/C12626_04_18.jpg"/></div><h6>Figure 4.18: The sepal length and sepal width feature</h6></li>
				<li>Compute the <strong class="inline">covariance</strong> matrix for the selected data. Note that we need to take the transpose of the <strong class="inline">covariance</strong> matrix to ensure that it is based on the number of features (2) and not samples (150):<p class="snippet">data = np.cov(df.values.T)</p><p class="snippet"># The transpose is required to ensure the covariance matrix is </p><p class="snippet">#based on features, not samples data</p><p class="snippet">data</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer107"><img alt="" src="image/C12626_04_19.jpg"/></div><h6>Figure 4.19: The covariance matrix for the selected data</h6></li>
				<li>Compute the eigenvectors and eigenvalues for the covariance matrix, Again, use the <strong class="inline">full_matrices</strong> function argument:<p class="snippet">eigenvectors, eigenvalues, _ = np.linalg.svd(data, full_matrices=False)</p></li>
				<li>What are the eigenvalues? These are returned sorted from the highest to lowest value:<p class="snippet">eigenvalues</p><p>The output is as follows:</p><p class="snippet">array([0.6887728 , 0.18492474])</p></li>
				<li>What are the corresponding eigenvectors?<p class="snippet">eigenvectors</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer108"><img alt="" src="image/C12626_04_20.jpg"/></div><h6>Figure 4.20: Eigenvectors</h6></li>
				<li>Compute the eigenvalue<a id="_idTextAnchor097"/>s as a percentage of the variance within the dataset:<p class="snippet">eigenvalues = np.cumsum(eigenvalues)</p><p class="snippet">eigenvalues /= eigenvalues.max()</p><p class="snippet">eigenvalues</p><p>The output is as follows:</p><p class="snippet">array([0.78834238, 1.        ])</p></li>
				<li>As per the introduction to the exercise, we need to describe the data with at least 75% of the available variance. As per <em class="italics">Step </em><a href="C12626_04_ePub_Final_SZ.xhtml#_idTextAnchor097"><em class="italics">7</em></a>, the principal component comprises 78% of the available variance. As such, we require only the principal component from the dataset. What are the principal components?<p class="snippet">P = eigenvectors[0]</p><p class="snippet">P</p><p>The output is as follows:</p><p class="snippet">array([-0.99693955,  0.07817635])</p><p>Now, we can apply the dimensionality reduction process. Execute a matrix multiplication of the principal component with the transpose of the dataset.</p><h4>Note</h4><p class="callout">The dimensionality reduction process is a matrix multiplication of the selected eigenvectors and the data to be transformed.</p></li>
				<li>Without taking the transpose of the <strong class="inline">df.values</strong> matrix, multiplication could not occur:<p class="snippet">x_t_p = P.dot(df.values.T)</p><p class="snippet">x_t_p</p><p>A section of the output is as follows:</p><div class="IMG---Figure" id="_idContainer109"><img alt="" src="image/C12626_04_21.jpg"/></div><h6>Figure 4.21: The result of matrix multiplication</h6><h4>Note</h4><p class="callout">The transpose of the dataset is required to execute matrix multiplication, as the <strong class="bold">inner dimensions of the matrix must be the same</strong> for matrix multiplication to occur. For <strong class="bold">A.dot(B)</strong> to be valid, <strong class="bold">A</strong> must have the shape <em class="italics">m x n</em> and <strong class="bold">B</strong> must have the shape <em class="italics">n x p</em>. In this example, the inner dimensions of <strong class="bold">A</strong> and <strong class="bold">B</strong> are both <em class="italics">n</em>. </p><p>In the following example, the output of the PCA is a single-column, 150-sample dataset. As such, we have just reduced the size of the initial dataset by half, comprising approximately 79% of the variance within the data:</p><div class="IMG---Figure" id="_idContainer110"><img alt="Figure 4.22: The output of PCA" src="image/C12626_04_22.jpg"/></div><h6>Figure 4.22: The output of PCA</h6></li>
				<li>Plot the values of the principal component:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(x_t_p);</p><p class="snippet">plt.title('Principal Component of Selected Iris Dataset');</p><p class="snippet">plt.xlabel('Sample');</p><p class="snippet">plt.ylabel('Component Value');</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 4.23: The Iris dataset transformed by using a manual PCA" src="image/C12626_04_23.jpg"/>
				</div>
			</div>
			<h6>Figure 4.23: The Iris dataset transformed by using a manual PCA</h6>
			<p>In this exercise, we simply computed the covariance matrix of the dataset without applying any transformations to the dataset beforehand. If the two features have roughly the same mean and standard deviation, this is perfectly fine. However, if one feature is much larger in value (and has a somewhat different mean) than the other, then this feature may dominate the other when decomposing into components. This could have the effect of removing the information provided by the smaller feature altogether. One simple normalization technique before computing the covariance matrix would be to subtract the respective means from the features, thus centering the dataset around zero. We will demonstrate this in <em class="italics">Exercise 15</em>, <em class="italics">Visualizing Variance Reduction with Manual PCA</em>.</p>
			<h3 id="_idParaDest-89"><a id="_idTextAnchor098"/>Exercise 14: Scikit-Learn PCA</h3>
			<p>Typically, we will not complete PCA manually, especially when scikit-learn provides an optimized API with convenient methods that will allow us to easily transform the data to and from the reduced-dimensional space. In this exercise, we will look at using a scikit-learn PCA on the Iris dataset in more detail:</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>.</p>
			<p class="callout">It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise14">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise14</a>.</p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Import the <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">PCA</strong> modules from the <strong class="inline">sklearn</strong> packages:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.decomposition import PCA</p></li>
				<li>Load the dataset:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer112"><img alt="Figure 4.24: The first five rows of the dataset" src="image/C12626_04_24.jpg"/></div><h6>Figure 4.24: The first five rows of the dataset</h6></li>
				<li>Again, we only require the <strong class="inline">Sepal Length</strong> and <strong class="inline">Sepal Width</strong> features, so remove the other columns. In this example, we are not normalizing the selected dataset:<p class="snippet">df = df[['Sepal Length', 'Sepal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer113"><img alt="Figure 4.25: The Sepal Length and Sepal Width features" src="image/C12626_04_25.jpg"/></div><h6>Figure 4.25: The Sepal Length and Sepal Width features</h6></li>
				<li>Fit the data to a scikit-learn PCA model of the covariance data. Using the default values, as we have here, produces the maximum number of eigenvalues and eigenvectors possible for the dataset:<p class="snippet">model = PCA()</p><p class="snippet">model.fit(df.values)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer114"><img alt="Figure 4.26: Fitting data to a PCA model" src="image/C12626_04_26.jpg"/></div><h6>Figure 4.26: Fitting data to a PCA model</h6><p>Here, <strong class="inline">copy</strong> indicates that the data fit within the model is copied before any calculations are applied. <strong class="inline">iterated_power</strong> shows that the <strong class="inline">Sepal Length</strong> and <strong class="inline">Sepal Width</strong> features<strong class="inline"> </strong>is the number of principal components to keep. The default value is <strong class="inline">None</strong>, which selects the number of components as one less than the minimum of either the number of samples or number of features. <strong class="inline">random_state</strong> allows the user to specify a seed for the random number generator used by the SVD solver. <strong class="inline">svd_solver</strong> specifies the SVD solver to be used during PCA. <strong class="inline">tol</strong> is the tolerance values used by the SVD solver. With <strong class="inline">whiten</strong>, the component vectors are multiplied by the square root of the number of samples. This will remove some information, but can improve the performance of some downstream estimators.</p></li>
				<li>The percentage of variance described by the components (eigenvalues) is contained within the <strong class="inline">explained_variance_ratio_</strong> property. Display the values for <strong class="inline">explained_variance_ratio_</strong>:<p class="snippet">model.explained_variance_ratio_</p><p>The output is as follows:</p><p class="snippet">array([0.78834238, 0.21165762])</p></li>
				<li>Display the eigenvectors via the <strong class="inline">components_</strong> property:<p class="snippet">model.components_</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer115"><img alt="Figure 4.27: Eigenvectors" src="image/C12626_04_27.jpg"/></div><h6>Figure 4.27: Eigenvectors</h6></li>
				<li>In this exercise, we will again only use the primary component, so we will create a new <strong class="inline">PCA</strong> model, this time specifying the number of components (eigenvectors/eigenvalues) to be <strong class="inline">1</strong>:<p class="snippet">model = PCA(n_components=1)</p></li>
				<li>Use the <strong class="inline">fit</strong> method to fit the <strong class="inline">covariance</strong> matrix to the <strong class="inline">PCA</strong> model and generate the corresponding eigenvalues/eigenvectors:<p class="snippet">model.fit(df.values)</p><div class="IMG---Figure" id="_idContainer116"><img alt="Figure 4.28: The maximum number of eigenvalues and eigenvectors" src="image/C12626_04_28.jpg"/></div><h6>Figure 4.28: The maximum number of eigenvalues and eigenvectors</h6><p>The model is fitted using a number of default parameters, as listed in the preceding output. <strong class="inline">copy = True</strong> is the data provided to the <strong class="inline">fit</strong> method, which is copied before PCA is applied. <strong class="inline">iterated_power='auto'</strong> is used to define the number of iterations by the internal SVD solver. <strong class="inline">n_components=1</strong> specifies that the PCA model is to return only the principal component. <strong class="inline">random_state=None</strong> specifies the random number generator to be used by the internal SVD solver if required. <strong class="inline">svd_solver='auto'</strong> is the type of SVD solver used. <strong class="inline">tol=0.0</strong> is the tolerance value for the SVD solver to deem converged. <strong class="inline">whiten=False</strong> specifies that the eigenvectors are not to be modified. If set to <strong class="inline">True</strong>, whitening further modifies the components by multiplying by the square root of the number of samples and dividing by the singular values. This can help to improve the performance of later algorithm steps.</p><p>Typically, you will not need to worry about adjusting any of these parameters, other than the number of components (<strong class="inline">n_components</strong>), which you can pass to the <strong class="inline">fit</strong> method, for example, <strong class="inline">model.fit(data, n_components=2)</strong>.</p></li>
				<li>Display the eigenvectors using the <strong class="inline">components_</strong> property:<p class="snippet">model.components_</p><p>The output is as follows:</p><p class="snippet">array([[ 0.99693955, -0.07817635]])</p></li>
				<li>Transform the Iris dataset into the lower space by using the <strong class="inline">fit_transform</strong> method of the model on the dataset. Assign the transformed values to the <strong class="inline">data_t</strong> variable.<p class="snippet">data_t = model.fit_transform(df.values)</p></li>
				<li>Plot the transformed values to visualize the result:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(data_t);</p><p class="snippet">plt.xlabel('Sample');</p><p class="snippet">plt.ylabel('Transformed Data');</p><p class="snippet">plt.title('The dataset transformed by the principal component');</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer117">
					<img alt="Figure 4.29: The Iris dataset transformed using the scikit-learn PCA" src="image/C12626_04_29.jpg"/>
				</div>
			</div>
			<h6>Figure 4.29: The Iris dataset transformed using the scikit-learn PCA</h6>
			<p>Congratulations! You have just reduced the dimensionality of the Iris dataset using manual PCA, as well as the scikit-learn API. But before we celebrate too early, compare <a href="">Figure 4.23</a> and <a href="">Figure 4.29</a>; these plots should be identical, shouldn't they? We used two separate methods to complete a PCA on the same dataset and selected the principal component for both. In the next activity, we will investigate why there are differences between the two.</p>
			<h3 id="_idParaDest-90"><a id="_idTextAnchor099"/>Activity 6: Manual PCA versus scikit-learn</h3>
			<p>Suppose that you have been asked to port some legacy code from an older application executing PCA manually, to a newer application that uses scikit-learn. During the porting process, you notice some differences between the output of the manual PCA versus your port. Why is there a difference between the output of our manual PCA and scikit-learn? Compare the results of the two approaches on the Iris dataset. What are the differences between them?</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>. It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity06">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity06</a>. </p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Import the <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong> plotting libraries and the scikit-learn <strong class="inline">PCA</strong> model.</li>
				<li>Load the dataset and select only the sepal features as per the previous exercises. Display the first five rows of the data.</li>
				<li>Compute the <strong class="inline">covariance</strong> matrix for the data.</li>
				<li>Transform the data using the scikit-learn API and only the first principal component. Store the transformed data in the <strong class="inline">sklearn_pca</strong> variable.</li>
				<li>Transform the data using the manual PCA and only the first principal component. Store the transformed data in the <strong class="inline">manual_pca</strong> variable.</li>
				<li>Plot the <strong class="inline">sklearn_pca</strong> and <strong class="inline">manual_pca</strong> values on the same plot to visualize the difference.</li>
				<li>Notice that the two plots look almost identical, but with some key differences. What are these differences?</li>
				<li>See whether you can modify the output of the manual PCA process to bring it in line with the scikit-learn version.<h4>Note</h4><p class="callout">As a hint, the scikit-learn API subtracts the mean of the data prior to the transform.</p></li>
			</ol>
			<p>Expected output: By the end of this activity, you will have transformed the dataset using both the manual and scikit-learn PCA methods. You will have produced a plot demonstrating that the two reduced datasets are, in fact, identical, and you should have an understanding of why they initially looked quite different. The final plot should look similar to the following:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer118">
					<img alt="Figure 4.30: The expected final plot" src="image/C12626_04_30.jpg"/>
				</div>
			</div>
			<h6>Figure 4.30: The expected final plot</h6>
			<p>This plot will demonstrate that the dimensionality reduction completed by the two methods are, in fact, the same.</p>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 324.</p>
			<h3 id="_idParaDest-91"><a id="_idTextAnchor100"/>Restoring the Compressed Dataset</h3>
			<p>Now that we have covered a few different examples of transforming a dataset into a lower-dimensional space, we should consider what practical effect this transformation has had on the data. Using PCA as a pre-processing step to condense the number of features in the data will result in some of the variance being discarded. The following exercise will walk us through this process so that we can see how much information has been discarded by the transformation.</p>
			<h3 id="_idParaDest-92"><a id="_idTextAnchor101"/>Exercise 15: Visualizing Variance Reduction with Manual PCA</h3>
			<p>One of the most important aspects of dimensionality reduction is understanding how much information has been removed from the dataset as a result of the dimensionality reduction process. Removing too much information will add additional challenges to later processing, while not removing enough defeats the purpose of PCA or other techniques. In this exercise, we will visualize the amount of information that has been removed from the Iris dataset as a result of PCA:</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>. </p>
			<p class="callout">It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise15">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise15</a>.</p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Import the <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong> plotting libraries:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Read in the <strong class="inline">Sepal</strong> features from the Iris dataset:<p class="snippet">df = pd.read_csv('iris-data.csv')[['Sepal Length', 'Sepal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer119"><img alt="Figure 4.31: Sepal features" src="image/C12626_04_31.jpg"/></div><h6>Figure 4.31: Sepal features</h6></li>
				<li>Centre the dataset around zero by subtracting the respective means:<h4>Note</h4><p class="callout">As discussed at the end of <em class="italics">Exercise 13</em>, <em class="italics">Manually Executing PCA</em>, here, we are centering the data before computing the covariance matrix.</p><p class="snippet">means = np.mean(df.values, axis=0)</p><p class="snippet">means</p><p>The output is as follows:</p><p class="snippet">array([5.84333333, 3.054     ])</p><p>To calculate the data and print the results, use the following code:</p><p class="snippet">data = df.values - means</p><p class="snippet">data</p><p>A section of the output is as follows:</p><div class="IMG---Figure" id="_idContainer120"><img alt="Figure 4.32: Section of the output" src="image/C12626_04_32.jpg"/></div><h6>Figure 4.32: Section of the output</h6></li>
				<li>Use manual PCA to transform the data on the basis of the first principal component:<p class="snippet">eigenvectors, eigenvalues, _ = np.linalg.svd(np.cov(data.T), full_matrices=False)</p><p class="snippet">P = eigenvectors[0]</p><p class="snippet">P</p><p>The output is as follows:</p><p class="snippet">array([-0.99693955,  0.07817635])</p></li>
				<li>Transform the data into the lower-dimensional space:<p class="snippet">data_transformed = P.dot(data.T)</p></li>
				<li>Reshape the principal components for later use:<p class="snippet">P = P.reshape((-1, 1))</p></li>
				<li>To compute the inverse transform of the reduced dataset, we need to restore the selected eigenvectors into the higher-dimensional space. To do this, we will invert the matrix. Matrix inversion is another linear algebra technique that we will only cover very briefly. A square matrix, <em class="italics">A</em>, is said to be invertible if there exists another square matrix, <em class="italics">B</em>, and if <em class="italics">AB=BA=I</em>, where <em class="italics">I</em> is a special matrix known as an identity matrix, consisting of values of <strong class="inline">1</strong> only through the center diagonal:<p class="snippet">P_transformed = np.linalg.pinv(P)</p><p class="snippet">P_transformed</p><p>The output is as follows:</p><p class="snippet">array([[-0.99693955,  0.07817635]])</p></li>
				<li>Prepare the transformed data for use in the matrix multiplication:<p class="snippet">data_transformed = data_transformed.reshape((-1, 1))</p></li>
				<li>Compute the inverse transform of the reduced data and plot the result to visualize the effect of removing the variance from the data:<p class="snippet">data_restored = data_transformed.dot(P_transformed)</p><p class="snippet">data_restored</p><p>A section of the output is as follows:</p><div class="IMG---Figure" id="_idContainer121"><img alt="Figure 4.33: The inverse transform of the reduced data" src="image/C12626_04_33.jpg"/></div><h6>Figure 4.33: The inverse transform of the reduced data</h6></li>
				<li>Add the <strong class="inline">means</strong> back to the transformed data:<p class="snippet">data_restored += means</p></li>
				<li>Visualize the result by plotting the original dataset and the transformed dataset:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(data_restored[:,0], data_restored[:,1], linestyle=':', label='PCA restoration');</p><p class="snippet">plt.scatter(df['Sepal Length'], df['Sepal Width'], marker='*', label='Original');</p><p class="snippet">plt.legend();</p><p class="snippet">plt.xlabel('Sepal Length');</p><p class="snippet">plt.ylabel('Sepal Width');</p><p class="snippet">plt.title('Inverse transform after removing variance');</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer122"><img alt="Figure 4.34: The inverse transform after removing variance" src="image/C12626_04_34.jpg"/></div><h6>Figure 4.34: The inverse transform after removing variance</h6></li>
				<li>There are only two components of variation in this dataset. If we do not remove any of the components, what will be the result of the inverse transform? Again, transform the data into the lower-dimensional space, but this time, use all of the eigenvectors:<p class="snippet">P = eigenvectors</p><p class="snippet">data_transformed = P.dot(data.T)</p></li>
				<li>Transpose <strong class="inline">data_transformed</strong> to put it into the correct shape for matrix multiplication:<p class="snippet">data_transformed = data_transformed.T</p></li>
				<li>Now, restore the data back to the higher-dimensional space: <p class="snippet">data_restored = data_transformed.dot(P)</p><p class="snippet">data_restored</p><p>A section of the output is as follows:</p><div class="IMG---Figure" id="_idContainer123"><img alt="Figure 4.35: The restored data" src="image/C12626_04_35.jpg"/></div><h6>Figure 4.35: The restored data</h6></li>
				<li>Add the means back to the restored data:<p class="snippet">data_restored += means</p></li>
				<li>Visualize the restored data in the context of the original dataset:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.scatter(data_restored[:,0], data_restored[:,1], marker='d', label='PCA restoration', c='k');</p><p class="snippet">plt.scatter(df['Sepal Length'], df['Sepal Width'], marker='o', label='Original', c='k');</p><p class="snippet">plt.legend();</p><p class="snippet">plt.xlabel('Sepal Length');</p><p class="snippet">plt.ylabel('Sepal Width');</p><p class="snippet">plt.title('Inverse transform after removing variance');</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer124">
					<img alt="" src="image/C12626_04_36.jpg"/>
				</div>
			</div>
			<h6>Figure 4.36: The inverse transform after removing the variance</h6>
			<p>If we compare the two plots produced in this exercise, we can see that the PCA reduced, and the restored dataset is essentially a negative linear trend line between the two feature sets. We can compare this to the dataset restored from all available components, where we have recreated the original dataset as a whole.</p>
			<h3 id="_idParaDest-93"><a id="_idTextAnchor102"/>Exercise 16: Visualizing Variance Reduction with</h3>
			<p>In this exercise, we will again visualize the effect of reducing the dimensionality of the dataset; however, this time, we will be using the scikit-learn API. This is this method that you will commonly use in practical applications, due to the power and simplicity of the scikit-learn model:</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>. It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise16">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise16</a>.</p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Import the <strong class="inline">pandas</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong> plotting libraries and the <strong class="inline">PCA</strong> model from scikit-learn:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.decomposition import PCA</p></li>
				<li>Read in the <strong class="inline">Sepal</strong> features from the Iris dataset:<p class="snippet">df = pd.read_csv('iris-data.csv')[['Sepal Length', 'Sepal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer125"><img alt="Figure 4.37: The Sepal features from the Iris dataset" src="image/C12626_04_37.jpg"/></div><h6>Figure 4.37: The Sepal features from the Iris dataset</h6></li>
				<li>Use the scitkit-learn API to transform the data on the basis of the first principal component:<p class="snippet">model = PCA(n_components=1)</p><p class="snippet">data_p = model.fit_transform(df.values)</p><p>The output is as follows:</p></li>
				<li>Compute the inverse transform of the reduced data and plot the result to visualize the effect of removing the variance from the data:<p class="snippet">data = model.inverse_transform(data_p);</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(data[:,0], data[:,1], linestyle=':', label='PCA restoration');</p><p class="snippet">plt.scatter(df['Sepal Length'], df['Sepal Width'], marker='*', label='Original');</p><p class="snippet">plt.legend();</p><p class="snippet">plt.xlabel('Sepal Length');</p><p class="snippet">plt.ylabel('Sepal Width');</p><p class="snippet">plt.title('Inverse transform after removing variance');</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer126"><img alt="Figure 4.38: The inverse transform after removing the variance" src="image/C12626_04_38.jpg"/></div><h6>Figure 4.38: The inverse transform after removing the variance</h6></li>
				<li>There are only two components of variation in this dataset. If we do not remove any of the components, what will the result of the inverse transform be?<p class="snippet">model = PCA()</p><p class="snippet">data_p = model.fit_transform(df.values)</p><p class="snippet">data = model.inverse_transform(data_p);</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.scatter(data[:,0], data[:,1], marker='d', label='PCA restoration', c='k');</p><p class="snippet">plt.scatter(df['Sepal Length'], df['Sepal Width'], marker='o', label='Original', c='k');</p><p class="snippet">plt.legend();</p><p class="snippet">plt.xlabel('Sepal Length');</p><p class="snippet">plt.ylabel('Sepal Width');</p><p class="snippet">plt.title('Inverse transform after removing variance');</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer127">
					<img alt="Figure 4.39: The inverse transform after removing the variance" src="image/C12626_04_39.jpg"/>
				</div>
			</div>
			<h6>Figure 4.39: The inverse transform after removing the variance</h6>
			<p>Again, we have demonstrated the effect of removing information from the dataset and the ability to recreate the original data using all the available eigenvectors.</p>
			<p>The previous exercises specified the reduction of the dimensionality using PCA to two dimensions, partly to allow the results to be easily visualized. We can, however, use PCA to reduce the dimensions to any value less than that of the original set. The following example demonstrates how PCA can be used to reduce a dataset to three dimensions, allowing visualizations.</p>
			<h3 id="_idParaDest-94"><a id="_idTextAnchor103"/>Exercise 17: Plotting 3D Plots in Matplotlib</h3>
			<p>Creating 3D scatter plots in matplotlib are unfortunately not as simple as providing a series of (<em class="italics">x</em>, <em class="italics">y</em>, <em class="italics">z</em>)<em class="italics"> </em>coordinates to a scatter plot. In this exercise we will work through a simple 3D plotting example, using the Iris dataset:</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>.</p>
			<p class="callout">It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise17">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise17</a>.</p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Import <strong class="inline">pandas</strong> and <strong class="inline">matplotlib</strong>. To enable 3D plotting, you will also need to import <strong class="inline">Axes3D</strong>:<p class="snippet">from mpl_toolkits.mplot3d import Axes3D</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Read in the dataset and select the <strong class="inline">Sepal Length</strong>, <strong class="inline">Sepal Width</strong>, and <strong class="inline">Petal Width</strong> columns<p class="snippet">df = pd.read_csv('iris-data.csv')[['Sepal Length', 'Sepal Width', 'Petal Width']]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer128"><img alt="Figure 4.40: The first five rows of the data" src="image/C12626_04_40.jpg"/></div><h6>Figure 4.40: The first five rows of the data</h6></li>
				<li>Plot the data in three dimensions and use the <strong class="inline">projection='3d'</strong> argument with the <strong class="inline">add_subplot</strong> method to create the 3D plot:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_subplot(111, projection='3d') # Where Axes3D is required</p><p class="snippet">ax.scatter(df['Sepal Length'], df['Sepal Width'], df['Petal Width']);</p><p class="snippet">ax.set_xlabel('Sepal Length (mm)');</p><p class="snippet">ax.set_ylabel('Sepal Width (mm)');</p><p class="snippet">ax.set_zlabel('Petal Width (mm)');</p><p class="snippet">ax.set_title('Expanded Iris Dataset');</p><p>The plot will look as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer129">
					<img alt="Figure 4.41: The expanded Iris dataset" src="image/C12626_04_41.jpg"/>
				</div>
			</div>
			<h6>Figure 4.41: The expanded Iris dataset</h6>
			<h4>Note</h4>
			<p class="callout">While the Axes3D was imported but not directly used, it is required for configuring the plot window in three dimensions. If the import of Axes3D was omitted, the <strong class="inline">projection='3d'</strong> argument would return an <strong class="inline">AttributeError</strong>.</p>
			<h3 id="_idParaDest-95">Activity 7: <a id="_idTextAnchor104"/>PCA Using the Expanded Iris Dataset</h3>
			<p>In this activity, we are going to use the complete Iris dataset to look at the effect of selecting a differing number of components in the PCA decomposition. This activity aims to simulate the process that is typically completed in a real-world problem as we try to determine the optimum number of components to select, attempting to balance the extent of dimensionality reduction and information loss. Henceforth, we will be using the scikit-learn PCA model:</p>
			<h4>Note</h4>
			<p class="callout">This dataset is taken from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a>.</p>
			<p class="callout">It can be downloaded from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity07">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity07</a>.</p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
			<ol>
				<li value="1">Import <strong class="inline">pandas</strong> and <strong class="inline">matplotlib</strong>. To enable 3D plotting, you will also need to import <strong class="inline">Axes3D</strong>.</li>
				<li>Read in the dataset and select the <strong class="inline">Sepal Length</strong>, <strong class="inline">Sepal Width</strong>, and <strong class="inline">Petal Width</strong> columns.</li>
				<li>Plot the data in three dimensions.</li>
				<li>Create a <strong class="inline">PCA</strong> model without specifying the number of components.</li>
				<li>Fit the model to the dataset.</li>
				<li>Display the eigenvalues or <strong class="inline">explained_variance_ratio_</strong>.</li>
				<li>We want to reduce the dimensionality of the dataset but still keep at least 90% of the variance. What are the minimum number of components required to keep 90% of the variance?</li>
				<li>Create a new <strong class="inline">PCA</strong> model, this time specifying the number of components required to keep at least 90% of the variance.</li>
				<li>Transform the data using the new model.</li>
				<li>Plot the transformed data.</li>
				<li>Restore the transformed data to the original dataspace.</li>
				<li>Plot the restored data in three dimensions in one subplot and the original data in a second subplot to visualize the effect of removing some of the variance:<p class="snippet">fig = plt.figure(figsize=(10, 14))</p><p class="snippet"># Original Data</p><p class="snippet">ax = fig.add_subplot(211, projection='3d')</p><p class="snippet"># Transformed Data</p><p class="snippet">ax = fig.add_subplot(212, projection='3d')</p></li>
			</ol>
			<p>Expected Output: The final plot will look as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer130">
					<img alt="Figure 4.42: Expected plots" src="image/C12626_04_42.jpg"/>
				</div>
			</div>
			<h6>Figure 4.42: Expected plots</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 328.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor105"/>Summary</h2>
			<p>In this chapter, we covered the process of dimensionality reduction and PCA. We completed a number of exercises and developed the skills to reduce the size of a dataset by extracting only the most important components of variance within the data, using both a manual PCA process and the model provided by scikit-learn. During this chapter, we also returned the reduced datasets back to the original dataspace and observed the effect of removing the variance on the original data. Finally, we discussed a number of potential applications for PCA and other dimensionality reduction processes. In our next chapter, we will introduce neural network-based autoencoders and use the Keras package to implement them.</p>
		</div>
	</body></html>
["```py\nimport requests \nimport pandas as pd \nimport json \npd.set_option('display.max_colwidth', 200) \n\nCONSUMER_KEY = 'enter_your_consumer_key_here \n\nauth_params = {'consumer_key': CONSUMER_KEY, 'redirect_uri': 'https://www.twitter.com/acombs'} \n\ntkn = requests.post('https://getpocket.com/v3/oauth/request', data=auth_params) \n\ntkn.text \n```", "```py\n# below we parse out the access code from the tkn.text string \nACCESS_CODE = tkn.text.split('=')[1] \n\nusr_params = {'consumer_key': CONSUMER_KEY, 'code': ACCESS_CODE} \n\nusr = requests.post('https://getpocket.com/v3/oauth/authorize', data=usr_params) \n\nusr.text \n```", "```py\n# below we parse out the access token from the usr.text string \nACCESS_TOKEN = usr.text.split('=')[1].split('&amp;')[0] \n\nno_params = {'consumer_key': CONSUMER_KEY, \n'access_token': ACCESS_TOKEN, \n'tag': 'n'} \n\nno_result = requests.post('https://getpocket.com/v3/get', data=no_params) \n\nno_result.text \n```", "```py\nno_jf = json.loads(no_result.text) \nno_jd = no_jf['list'] \n\nno_urls=[] \nfor i in no_jd.values(): \n    no_urls.append(i.get('resolved_url')) no_urls \n```", "```py\nno_uf = pd.DataFrame(no_urls, columns=['urls']) \nno_uf = no_uf.assign(wanted = lambda x: 'n') no_uf \n```", "```py\nyes_params = {'consumer_key': CONSUMER_KEY, \n'access_token': ACCESS_TOKEN, \n'tag': 'y'} \nyes_result = requests.post('https://getpocket.com/v3/get', data=yes_params) \n\nyes_jf = json.loads(yes_result.text) \nyes_jd = yes_jf['list'] \n\nyes_urls=[] \nfor i in yes_jd.values(): \n    yes_urls.append(i.get('resolved_url')) \n\nyes_uf = pd.DataFrame(yes_urls, columns=['urls']) \nyes_uf = yes_uf.assign(wanted = lambda x: 'y') \n\nyes_uf \n```", "```py\ndf = pd.concat([yes_uf, no_uf]) \n\ndf.dropna(inplace=True) \n\ndf \n```", "```py\nimport urllib \n\nEMBEDLY_KEY = 'your_embedly_api_key_here' \n\ndef get_html(x): \n    try: \n        qurl = urllib.parse.quote(x) \n        rhtml = requests.get('https://api.embedly.com/1/extract?url=' + qurl + '&amp;key=' + EMBEDLY_KEY) \n        ctnt = json.loads(rhtml.text).get('content') \n    except: \n        return None \n    return ctnt \n```", "```py\nfrom bs4 import BeautifulSoup \n\ndef get_text(x): \n    soup = BeautifulSoup(x, 'html5lib') \n    text = soup.get_text() \n    return text \n\ndf.loc[:,'text'] = df['html'].map(get_text) \n\ndf \n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer \n\nvect = TfidfVectorizer(ngram_range=(1,3), stop_words='english', min_df=3) \n\ntv = vect.fit_transform(df['text']) \n```", "```py\nfrom sklearn.svm import LinearSVC \n\nclf = LinearSVC() \nmodel = clf.fit(tv, df['wanted']) \n```", "```py\nimport gspread \n\nfrom oauth2client.service_account import ServiceAccountCredentials \nJSON_API_KEY = 'the/path/to/your/json_api_key/here' \n\nscope = ['https://spreadsheets.google.com/feeds', \n         'https://www.googleapis.com/auth/drive'] \n\ncredentials = ServiceAccountCredentials.from_json_keyfile_name(JSON_API_KEY, scope) \ngc = gspread.authorize(credentials) \n```", "```py\nws = gc.open(\"NewStories\") \nsh = ws.sheet1 \n\nzd = list(zip(sh.col_values(2),sh.col_values(3), sh.col_values(4))) \n\nzf = pd.DataFrame(zd, columns=['title','urls','html']) \nzf.replace('', pd.np.nan, inplace=True) \nzf.dropna(inplace=True) \n\nzf \n```", "```py\nzf.loc[:,'text'] = zf['html'].map(get_text) \n\nzf.reset_index(drop=True, inplace=True) \n\ntest_matrix = vect.transform(zf['text']) \n\ntest_matrix \n```", "```py\nresults = pd.DataFrame(model.predict(test_matrix), columns=['wanted']) \n\nresults \n```", "```py\nrez = pd.merge(results,zf, left_index=True, right_index=True) \n\nrez \n```", "```py\nchange_to_no = [130, 145, 148, 163, 178, 199, 219, 222, 223, 226, 235, 279, 348, 357, 427, 440, 542, 544, 546, 568, 614, 619, 660, 668, 679, 686, 740, 829] \n\nchange_to_yes = [0, 9, 29, 35, 42, 71, 110, 190, 319, 335, 344, 371, 385, 399, 408, 409, 422, 472, 520, 534, 672] \n\nfor i in rez.iloc[change_to_yes].index: \n    rez.iloc[i]['wanted'] = 'y' \n\nfor i in rez.iloc[change_to_no].index: \n    rez.iloc[i]['wanted'] = 'n' \n\nrez \n```", "```py\ncombined = pd.concat([df[['wanted', 'text']], rez[['wanted', 'text']]]) combined \n```", "```py\ntvcomb = vect.fit_transform(combined['text'], combined['wanted']) \n\nmodel = clf.fit(tvcomb, combined['wanted']) \n```", "```py\nimport pickle \n\npickle.dump(model, open(r'/input/a/path/here/to/news_model_pickle.p', 'wb')) \n\npickle.dump(vect, open(r'/input/a/path/here/to/news_vect_pickle.p', 'wb')) \n```", "```py\nimport pandas as pd \nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.svm import LinearSVC \nimport schedule \nimport time \nimport pickle \nimport json \nimport gspread \nfrom oauth2client.service_account import ServiceAccountCredentials \nimport requests \nfrom bs4 import BeautifulSoup \n\ndef fetch_news(): \n\n    try: \n        vect = pickle.load(open(r'/your/path/to/news_vect_pickle.p', 'rb')) \n        model = pickle.load(open(r'/your/path/to /news_model_pickle.p', 'rb')) \n\n        JSON_API_KEY = r'/your/path/to/API KEY.json' \n\n        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive'] \n\n        credentials = ServiceAccountCredentials.from_json_keyfile_name(JSON_API_KEY, scope) \n        gc = gspread.authorize(credentials) \n\n        ws = gc.open(\"NewStories\") \n        sh = ws.sheet1 \n        zd = list(zip(sh.col_values(2),sh.col_values(3), sh.col_values(4))) \n        zf = pd.DataFrame(zd, columns=['title','urls','html']) \n        zf.replace('', pd.np.nan, inplace=True) \n        zf.dropna(inplace=True) \n\n        def get_text(x): \n            soup = BeautifulSoup(x, 'html5lib') \n            text = soup.get_text() \n            return text \n\n        zf.loc[:,'text'] = zf['html'].map(get_text) \n\n        tv = vect.transform(zf['text']) \n        res = model.predict(tv) \n\n        rf = pd.DataFrame(res, columns=['wanted']) \n        rez = pd.merge(rf, zf, left_index=True, right_index=True) \n\n        rez = rez.iloc[:20,:] \n\n        news_str = '' \n for t, u in zip(rez[rez['wanted']=='y']['title'], rez[rez['wanted']=='y']['urls']): \n            news_str = news_str + t + '\\n' + u + '\\n' \n\n        payload = {\"value1\" : news_str} \n        r = requests.post('https://maker.ifttt.com/trigger/news_event/with/key/bNHFwiZx0wMS7EnD425n3T', data=payload) \n\n        # clean up worksheet \n        lenv = len(sh.col_values(1)) \n        cell_list = sh.range('A1:F' + str(lenv)) \n        for cell in cell_list: \n            cell.value = \"\" \n        sh.update_cells(cell_list) \n        print(r.text) \n\n    except: \n        print('Action Failed') \n\nschedule.every(480).minutes.do(fetch_news) \n\nwhile 1: \n    schedule.run_pending() \n    time.sleep(1) \n```"]
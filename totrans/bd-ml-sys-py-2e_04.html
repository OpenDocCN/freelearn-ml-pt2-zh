<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 4. Topic Modeling"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Topic Modeling</h1></div></div></div><p>In the previous chapter, we grouped text documents using clustering. This is a very useful tool, but it is not always the best. Clustering results in each text belonging to exactly one cluster. This book is about machine learning and Python. Should it be grouped with other Python-related works or with machine-related works? In a physical bookstore, we will need a single place to stock the book. In an Internet store, however, the answer is <span class="emphasis"><em>this book is about both machine learning and Python</em></span> and the book should be listed in both the sections in an online bookstore. This does not mean that the book will be listed in all the sections, of course. We will not list this book with other baking books.</p><p>In this chapter, we will learn methods that do not cluster documents into completely separate groups but allow each document to refer to several <span class="strong"><strong>topics</strong></span>. These<a id="id203" class="indexterm"/> topics will be identified automatically from a collection of text documents. These documents may be whole books or shorter pieces of text such as a blogpost, a news story, or an e-mail.</p><p>We would also like to be able to infer the fact that these documents may have topics that are central to them, while referring to other topics only in passing. This book mentions plotting every so often, but it is not a central topic as machine learning is. This means that documents have topics that are central to them and others that are more peripheral. The subfield of machine learning that deals with these problems is called <span class="strong"><strong>topic modeling</strong></span><a id="id204" class="indexterm"/> and is the subject of this chapter.</p><div class="section" title="Latent Dirichlet allocation"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec26"/>Latent Dirichlet allocation</h1></div></div></div><p>
<span class="strong"><strong>LDA and LDA—</strong></span>unfortunately, there are two methods in machine learning with the initials LDA: latent Dirichlet allocation, which is a topic modeling method and linear discriminant analysis, which is a<a id="id205" class="indexterm"/> classification method. They are completely unrelated, except for the fact that the initials LDA can refer to either. In certain situations, this can be confusing. The scikit-learn tool has a submodule, <code class="literal">sklearn.lda</code>, which implements linear discriminant analysis. At the moment, scikit-learn does not implement latent Dirichlet allocation.</p><p>The topic model we will look at is <span class="strong"><strong>latent Dirichlet allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>). The mathematical ideas behind LDA are fairly <a id="id206" class="indexterm"/>complex, and we will not go into the details here.</p><p>For those who are interested, and adventurous enough, <a id="id207" class="indexterm"/>Wikipedia will provide all the equations behind these algorithms: <a class="ulink" href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a>.</p><p>However, we can understand the ideas behind LDA intuitively at a high-level. LDA belongs to a class of models that are called generative models as they have a sort of fable, which explains how the data was generated. This generative story is a simplification of reality, of course, to make machine learning easier. In the LDA fable, we first create topics by assigning probability weights to words. Each topic will assign different weights to different words. For example, a Python topic will assign high probability to the word "variable" and a low probability to the word "inebriated". When we wish to generate a new document, we first choose the topics it will use and then mix words from these topics.</p><p>For example, let's say we have only three topics that books discuss:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Machine learning</li><li class="listitem" style="list-style-type: disc">Python</li><li class="listitem" style="list-style-type: disc">Baking</li></ul></div><p>For each topic, we have a list of words associated with it. This book will be a mixture of the first two topics, perhaps 50 percent each. The mixture does not need to be equal, it can also be a 70/30 split. When we are generating the actual text, we generate word by word; first we decide which topic this word will come from. This is a random decision based on the topic weights. Once a topic is chosen, we generate a word from that topic's list of words. To be precise, we choose a word in English with the probability given by the topic.</p><p>In this model, the order of words does not matter. This is a <span class="emphasis"><em>bag of words</em></span> model as we have already seen in the previous chapter. It is a crude simplification of language, but it often works well enough, because just knowing which words were used in a document and their frequencies are enough to make machine learning decisions.</p><p>In the real world, we do not know what the topics are. Our task is to take a collection of text and to reverse engineer this fable in order to discover what topics are out there and simultaneously figure out which topics each document uses.</p><div class="section" title="Building a topic model"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec29"/>Building a topic model</h2></div></div></div><p>Unfortunately, scikit-learn <a id="id208" class="indexterm"/>does not support latent Dirichlet allocation. Therefore, we are going to use the gensim package in Python. Gensim is developed by Radim Řehůřek who is a machine learning researcher and consultant in the United Kingdom. We must start by installing it. We can achieve this by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pip install gensim</strong></span>
</pre></div><p>As input data, we <a id="id209" class="indexterm"/>are going to use a collection of news reports from the <a id="id210" class="indexterm"/>
<span class="strong"><strong>Associated Press</strong></span> (<span class="strong"><strong>AP</strong></span>). This is a standard dataset for text modeling research, which was used in some of the initial works on topic models. After downloading the data, we can load it by running the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from gensim import corpora, models</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; corpus = corpora.BleiCorpus('./data/ap/ap.dat',
    './data/ap/vocab.txt')</strong></span>
</pre></div><p>The <code class="literal">corpus</code> variable holds all of the text documents and has loaded them in a format that makes for easy processing. We can now build a topic model using this object as input:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; model = models.ldamodel.LdaModel(</strong></span>
<span class="strong"><strong>              corpus,</strong></span>
<span class="strong"><strong>              num_topics=100,</strong></span>
<span class="strong"><strong>              id2word=corpus.id2word)</strong></span>
</pre></div><p>This single constructor call will statistically infer which topics are present in the corpus. We can explore the resulting model in many ways. We can see the list of topics a document refers to using the <code class="literal">model[doc]</code> syntax, as shown in the following example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  &gt;&gt;&gt; doc = corpus.docbyoffset(0)</strong></span>
<span class="strong"><strong>  &gt;&gt;&gt; topics = model[doc]</strong></span>
<span class="strong"><strong>  &gt;&gt;&gt; print(topics)</strong></span>
<span class="strong"><strong>[(3, 0.023607255776894751),</strong></span>
<span class="strong"><strong> (13, 0.11679936618551275),</strong></span>
<span class="strong"><strong> (19, 0.075935855202707139),</strong></span>
<span class="strong"><strong>....</strong></span>
<span class="strong"><strong> (92, 0.10781541687001292)]</strong></span>
</pre></div><p>The result will almost surely look different on our computer! The learning algorithm uses some random numbers and every time you learn a new topic model on the same input data, the result is different. Some of the qualitative properties of the model will be stable across different runs if your data is well behaved. For example, if you are using the topics to compare documents, as we do here, then the similarities should be robust and change only slightly. On the other hand, the order of the different topics will be completely different.</p><p>The format of the result is a list of pairs: <code class="literal">(topic_index, topic_weight)</code>. We can see that only a few topics are used for each document (in the preceding example, there is no weight for topics 0, 1, and 2; the weight for those topics is 0). The topic model is a sparse model, as although there are many possible topics; for each document, only a few of them are used. This is not strictly true as all the topics have a nonzero probability in the LDA model, but some of them have such a small probability that we can round it to zero as a good approximation.</p><p>We can <a id="id211" class="indexterm"/>explore this further by plotting a histogram of the number of topics that each document refers to:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; num_topics_used = [len(model[doc]) for doc in corpus]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.hist(num_topics_used)</strong></span>
</pre></div><p>You will get the following plot:</p><div class="mediaobject"><img src="images/2772OS_04_01.jpg" alt="Building a topic model"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip06"/>Tip</h3><p>
<span class="strong"><strong>Sparsity</strong></span><a id="id212" class="indexterm"/> means that while you may have large matrices and vectors, in principle, most of the values are zero (or so small that we can round them to zero as a good approximation). Therefore, only a few things are relevant at any given time.</p><p>Often problems that seem too big to solve are actually feasible because the data is sparse. For example, even though any web page can link to any other web page, the graph of links is actually very sparse as each web page will link to a very tiny fraction of all other web pages.</p></div></div><p>In the preceding graph, we can see that about 150 documents have 5 topics, while the majority deals with around 10 to 12 of them. No document talks about more than 20 different topics.</p><p>To a large<a id="id213" class="indexterm"/> extent, this is due to the value of the parameters that were used, namely, the <code class="literal">alpha</code> parameter. The exact meaning of alpha is a bit abstract, but bigger values for alpha will result in more topics per document.</p><p>Alpha needs to be a value greater than zero, but is typically set to a lesser value, usually, less than one. The smaller the value of <code class="literal">alpha</code>, the fewer topics each document will be expected to discuss. By default, gensim will set <code class="literal">alpha</code> to <code class="literal">1/num_topics</code>, but you can set it explicitly by passing it as an argument in the <code class="literal">LdaModel</code> constructor as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; model = models.ldamodel.LdaModel(</strong></span>
<span class="strong"><strong>              corpus,</strong></span>
<span class="strong"><strong>              num_topics=100,</strong></span>
<span class="strong"><strong>              id2word=corpus.id2word,</strong></span>
<span class="strong"><strong>              alpha=1)</strong></span>
</pre></div><p>In this case, this is a larger alpha than the default, which should lead to more topics per document. As we can see in the combined histogram given next, gensim behaves as we expected and assigns more topics to each document:</p><div class="mediaobject"><img src="images/2772OS_04_02.jpg" alt="Building a topic model"/></div><p>Now, we can see in the<a id="id214" class="indexterm"/> preceding histogram that many documents touch upon 20 to 25 different topics. If you set the value lower, you will observe the opposite (downloading the code from the online repository will allow you to play around with these values).</p><p>What are these topics? Technically, as we discussed earlier, they are multinomial distributions over words, which means that they assign a probability to each word in the vocabulary. Words with high probability are more associated with that topic than words with lower probability.</p><p>Our brains are not very good at reasoning with probability distributions, but we can readily make sense of a list of words. Therefore, it is typical to summarize topics by the list of the most highly weighted words.</p><p>In the following table, we display the first ten topics:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Topic no.</p>
</th><th style="text-align: left" valign="bottom">
<p>Topic</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>dress military soviet president new state capt carlucci states leader stance government</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>koch zambia lusaka oneparty orange kochs party i government mayor new political</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>human turkey rights abuses royal thompson threats new state wrote garden president</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>bill employees experiments levin taxation federal measure legislation senate president whistleblowers sponsor</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>ohio july drought jesus disaster percent hartford mississippi crops northern valley virginia</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>united percent billion year president world years states people i bush news</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>b hughes affidavit states united ounces squarefoot care delaying charged unrealistic bush</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>yeutter dukakis bush convention farm subsidies uruguay percent secretary general i told</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>kashmir government people srinagar india dumps city two jammukashmir group moslem pakistan</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>workers vietnamese irish wage immigrants percent bargaining last island police hutton I</p>
</td></tr></tbody></table></div><p>Although daunting <a id="id215" class="indexterm"/>at first glance, when reading through the list of words, we can clearly see that the topics are not just random words, but instead these are logical groups. We can also see that these topics refer to older news items, from when the Soviet Union still existed and Gorbachev was its Secretary General. We can also represent the topics as word clouds, making more likely words larger. For example, this is the visualization of a topic which deals with the Middle East and politics:</p><div class="mediaobject"><img src="images/2772OS_04_03.jpg" alt="Building a topic model"/></div><p>We can also see that some of the words should perhaps be removed (for example, the word "I") as they <a id="id216" class="indexterm"/>are not so informative, they are stop words. When building topic modeling, it can be useful to filter out stop words, as otherwise, you might end up with a topic consisting entirely of stop words. We may also wish to preprocess the text to stems in order to normalize plurals and verb forms. This process was covered in the previous chapter and you can refer to it for details. If you are interested, you can download the code from the companion website of the book and try all these variations to draw different pictures.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>Building a word cloud like the previous one can be done with several different pieces of software. For the graphics in this chapter, we used a Python-based tool called pytagcloud. This package requires a few dependencies to install and is not central to machine learning, so we won't consider it in the main text; however, we have all of the code available in the online code repository to generate the figures in this chapter.</p></div></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Comparing documents by topics"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec27"/>Comparing documents by topics</h1></div></div></div><p>Topics can be <a id="id217" class="indexterm"/>useful on their own to build the sort of small vignettes with words that are shown in the previous screenshot. These visualizations <a id="id218" class="indexterm"/>can be used to navigate a large collection of documents. For example, a website can display the different topics as different word clouds, allowing a user to click through to the documents. In fact, they have been used in just this way to analyze large collections of documents.</p><p>However, topics are often just an intermediate tool to another end. Now that we have an estimate for each document of how much of that document comes from each topic, we can compare the documents in topic space. This simply means that instead of comparing word to word, we say that two documents are similar if they talk about the same topics.</p><p>This can <a id="id219" class="indexterm"/>be very powerful as two text documents that share<a id="id220" class="indexterm"/> few words may actually refer to the same topic! They may just refer to it using different constructions (for example, one document may read "the President of the United States" while the other will use the name "Barack Obama").</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Topic models are good on their own to build visualizations and explore data. They are also very useful as an intermediate step in many other tasks.</p></div></div><p>At this point, we can redo the exercise we performed in the last chapter and look for the most similar post to an input query, by using the topics to define similarity. Whereas, earlier we compared two documents by comparing their word vectors directly, we can now compare two documents by comparing their topic vectors.</p><p>For this, we are going to project the documents to the topic space. That is, we want to have a vector of topics that summarize the document. How to perform these types of <span class="strong"><strong>dimensionality reduction</strong></span><a id="id221" class="indexterm"/> in general is an important task in itself and we have a chapter entirely devoted to this task. For the moment, we just show how topic models can be used for exactly this purpose; once topics have been computed for each document, we can perform operations on its topic vector and forget about the original words. If the topics are meaningful, they will be potentially more informative than the raw words. Additionally, this may bring computational advantages, as it is much faster to compare 100 vectors of topic weights than vectors of the size of the vocabulary (which will contain thousands of terms).</p><p>Using gensim, we have seen earlier how to compute the topics corresponding to all the documents in the corpus. We will now compute these for all the documents and store it in a NumPy arrays and compute all pairwise distances:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from gensim import matutils</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; topics = matutils.corpus2dense(model[corpus], num_terms=model.num_topics)</strong></span>
</pre></div><p>Now, <code class="literal">topics</code> is a matrix of topics. We can use the <code class="literal">pdist</code> function in SciPy to compute all pairwise distances. That is, with a single function call, we compute all the values of <code class="literal">sum((topics[ti] – topics[tj])**2)</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from scipy.spatial import distance</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pairwise = distance.squareform(distance.pdist(topics))</strong></span>
</pre></div><p>Now, we will employ one last little trick; we will set the diagonal elements of the <code class="literal">distance</code> matrix to a high value (it just needs to be larger than the other values in the matrix):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; largest = pairwise.max()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for ti in range(len(topics)):</strong></span>
<span class="strong"><strong>...     pairwise[ti,ti] = largest+1</strong></span>
</pre></div><p>And we are done! For <a id="id222" class="indexterm"/>each document, we can look up the closest element easily (this is a type of nearest neighbor classifier):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> &gt;&gt;&gt; def closest_to(doc_id):</strong></span>
<span class="strong"><strong> ...    return pairwise[doc_id].argmin()</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>Note that this will not work if we had not set the diagonal elements to a large value: the function will always return the same element as it is the one most similar to itself (except in the weird case where two elements had exactly the same topic distribution, which is very rare unless they are exactly the same).</p></div></div><p>For example, here<a id="id223" class="indexterm"/> is one possible query document (it is the second document in our collection):</p><div class="informalexample"><pre class="programlisting">From: geb@cs.pitt.edu (Gordon Banks)
Subject: Re: request for information on "essential tremor" and Indrol?

In article &lt;1q1tbnINNnfn@life.ai.mit.edu&gt; sundar@ai.mit.edu writes:

Essential tremor is a progressive hereditary tremor that gets worse
when the patient tries to use the effected member.  All limbs, vocal
cords, and head can be involved.  Inderal is a beta-blocker and
is usually effective in diminishing the tremor.  Alcohol and mysoline
are also effective, but alcohol is too toxic to use as a treatment.
--
------------------------------------------------------------------
----------
Gordon Banks  N3JXP      | "Skepticism is the chastity of the intellect, and
geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too soon."
  ----------------------------------------------------------------
------------</pre></div><p>If we ask for the most similar document to <code class="literal">closest_to(1)</code>, we receive the following document as a result:</p><div class="informalexample"><pre class="programlisting">From: geb@cs.pitt.edu (Gordon Banks)
Subject: Re: High Prolactin

In article &lt;93088.112203JER4@psuvm.psu.edu&gt; JER4@psuvm.psu.edu (John E. Rodway) writes:
&gt;Any comments on the use of the drug Parlodel for high prolactin in the blood?
&gt;

It can suppress secretion of prolactin.  Is useful in cases of galactorrhea.
Some adenomas of the pituitary secret too much.

--
------------------------------------------------------------------
----------
Gordon Banks  N3JXP      | "Skepticism is the chastity of the intellect, and
geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too soon."</pre></div><p>The <a id="id224" class="indexterm"/>system <a id="id225" class="indexterm"/>returns a post by the same author discussing medications.</p><div class="section" title="Modeling the whole of Wikipedia"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec30"/>Modeling the whole of Wikipedia</h2></div></div></div><p>While the initial LDA implementations can be slow, which limited their use to small document <a id="id226" class="indexterm"/>collections, modern algorithms work well with very large collections of data. Following the documentation of gensim, we are going to build a topic model for the whole of the English-language Wikipedia. This takes hours, but can be done even with just a laptop! With a cluster of machines, we can make it go much faster, but we will look at that sort of processing environment in a later chapter.</p><p>First, we download the<a id="id227" class="indexterm"/> whole Wikipedia dump from <a class="ulink" href="http://dumps.wikimedia.org">http://dumps.wikimedia.org</a>. This is a large file (currently over 10 GB), so it may take a while, unless your Internet connection is very fast. Then, we will index it with a gensim tool:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python -m gensim.scripts.make_wiki \</strong></span>
<span class="strong"><strong>       enwiki-latest-pages-articles.xml.bz2 wiki_en_output</strong></span>
</pre></div><p>Run the previous line on the command shell, not on the Python shell. After a few hours, the index will be saved in the same directory. At this point, we can build the final topic model. This process looks exactly like what we did for the small AP dataset. We first import a few packages:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import logging, gensim</strong></span>
</pre></div><p>Now, we set up logging, using the standard Python logging module (which gensim uses to print out status messages). This step is not strictly necessary, but it is nice to have a little more output to know what is happening:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; logging.basicConfig(</strong></span>
<span class="strong"><strong>    format='%(asctime)s : %(levelname)s : %(message)s',</strong></span>
<span class="strong"><strong>    level=logging.INFO)</strong></span>
</pre></div><p>Now we <a id="id228" class="indexterm"/>load the preprocessed data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; id2word = gensim.corpora.Dictionary.load_from_text(</strong></span>
<span class="strong"><strong>              'wiki_en_output_wordids.txt')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; mm = gensim.corpora.MmCorpus('wiki_en_output_tfidf.mm')</strong></span>
</pre></div><p>Finally, we build the LDA model as we did earlier:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; model = gensim.models.ldamodel.LdaModel(</strong></span>
<span class="strong"><strong>          corpus=mm,</strong></span>
<span class="strong"><strong>          id2word=id2word,</strong></span>
<span class="strong"><strong>          num_topics=100,</strong></span>
<span class="strong"><strong>          update_every=1,</strong></span>
<span class="strong"><strong>          chunksize=10000,</strong></span>
<span class="strong"><strong>          passes=1)</strong></span>
</pre></div><p>This will again take a couple of hours. You will see the progress on your console, which can give you an indication of how long you still have to wait.</p><p>Once it is done, we can save the topic model to a file, so we don't have to redo it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  &gt;&gt;&gt; model.save('wiki_lda.pkl')</strong></span>
</pre></div><p>If you exit your session and come back later, you can load the model again using the following command (after the appropriate imports, naturally):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  &gt;&gt;&gt; model = gensim.models.ldamodel.LdaModel.load('wiki_lda.pkl')</strong></span>
</pre></div><p>The <code class="literal">model</code> object can be used to explore the collection of documents, and build the <code class="literal">topics</code> matrix as we did earlier.</p><p>We can see that this is still a sparse model even if we have many more documents than we had earlier (over 4 million as we are writing this):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  &gt;&gt;&gt; lens = (topics &gt; 0).sum(axis=0)</strong></span>
<span class="strong"><strong>  &gt;&gt;&gt; print(np.mean(lens))</strong></span>
<span class="strong"><strong>  6.41</strong></span>
<span class="strong"><strong>  &gt;&gt;&gt; print(np.mean(lens &lt;= 10))</strong></span>
<span class="strong"><strong>  0.941</strong></span>
</pre></div><p>So, the average document mentions 6.4 topics and 94 percent of them mention 10 or fewer topics.</p><p>We can ask what the most talked about topic in Wikipedia is. We will first compute the total weight for each topic (by summing up the weights from all the documents) and then retrieve the words corresponding to the most highly weighted topic. This is performed using the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; weights = topics.sum(axis=0)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; words = model.show_topic(weights.argmax(), 64)</strong></span>
</pre></div><p>Using the<a id="id229" class="indexterm"/> same tools as we did earlier to build up a visualization, we can see that the most talked about topic is related to music and is a very coherent topic. A full 18 percent of Wikipedia pages are partially related to this topic (5.5 percent of all the words in Wikipedia are assigned to this topic). Take a look at the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_04_04.jpg" alt="Modeling the whole of Wikipedia"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>These plots and numbers were obtained when the book was being written. As Wikipedia keeps changing, your results will be different. We expect that the trends will be similar, but the details may vary.</p></div></div><p>Alternatively, we <a id="id230" class="indexterm"/>can look at the least talked about topic:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  &gt;&gt;&gt; words = model.show_topic(weights.argmin(), 64)</strong></span>
</pre></div><div class="mediaobject"><img src="images/2772OS_04_05.jpg" alt="Modeling the whole of Wikipedia"/></div><p>The least talked about topic is harder to interpret, but many of its top words refer to airports in eastern countries. Just 1.6 percent of documents touch upon it, and it represents just 0.1 percent of the words.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Choosing the number of topics"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Choosing the number of topics</h1></div></div></div><p>So far in<a id="id231" class="indexterm"/> the chapter, we have used a fixed number of topics for our analyses, namely 100. This was a purely arbitrary number, we could have just as well used either 20 or 200 topics. Fortunately, for many uses, this number does not really matter. If you are going to only use the topics as an intermediate step, as we did previously when finding similar posts, the final behavior of the system is rarely very sensitive to the exact number of topics used in the model. This means that as long as you use enough topics, whether you use 100 topics or 200, the recommendations that result from the process will not be very different; 100 is often a good enough number (while 20 is too few for a general collection of text documents). The same is true of setting the <code class="literal">alpha</code> value. While playing around with it can change the topics, the final results are again robust against this change.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>Topic modeling is often an end towards a goal. In that case, it is not always very important exactly which parameter values are used. A different number of topics or values for parameters such as <code class="literal">alpha</code> will result in systems whose end results are almost identical in their final results.</p></div></div><p>On the <a id="id232" class="indexterm"/>other hand, if you are going to explore the topics directly, or build a visualization tool that exposes them, you should probably try a few values and see which gives you the most useful or most appealing results.</p><p>Alternatively, there are a few methods that will automatically determine the number of topics for you, depending on the dataset. One popular model is called the <a id="id233" class="indexterm"/>
<span class="strong"><strong>hierarchical Dirichlet process</strong></span>. Again, the full mathematical model behind it is complex and beyond the scope of this book. However, the fable we can tell is that instead of having the topics fixed first as in the LDA generative story, the topics themselves were generated along with the data, one at a time. Whenever the writer starts a new document, they have the option of using the topics that already exist or to create a completely new one. When more topics have already been created, the probability of creating a new one, instead of reusing what exists goes down, but the possibility always exists.</p><p>This means <span class="emphasis"><em>that the more documents we have, the more topics we will end up with</em></span>. This is one of those statements that is unintuitive at first but makes perfect sense upon reflection. We are grouping documents and the more examples we have, the more we can break them up. If we only have a few examples of news articles, then "Sports" will be a topic. However, as we have more, we start to break it up into the individual modalities: "Hockey", "Soccer", and so on. As we have even more data, we can start to tell nuances apart, articles about individual teams and even individual players. The same is true for people. In a group of many different backgrounds, with a few "computer people", you might put them together; in a slightly larger group, you will have separate gatherings for programmers and systems administrators; and in the real-world, we even have different gatherings for Python and Ruby programmers.</p><p>The <span class="strong"><strong>hierarchical Dirichlet process</strong></span> (<span class="strong"><strong>HDP</strong></span>) is available in gensim. Using it is trivial. To adapt the code we wrote for LDA, we just need to replace the call to <code class="literal">gensim.models.ldamodel.LdaModel</code> with a call to the <code class="literal">HdpModel</code> constructor as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  &gt;&gt;&gt; hdp = gensim.models.hdpmodel.HdpModel(mm, id2word)</strong></span>
</pre></div><p>That's it (except that it takes a bit longer to compute—there are no free lunches). Now, we can use this model in much the same way as we used the LDA model, except that we did not need to <a id="id234" class="indexterm"/>specify the number of topics.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Summary</h1></div></div></div><p>In this chapter, we discussed topic modeling. Topic modeling is more flexible than clustering as these methods allow each document to be partially present in more than one group. To explore these methods, we used a new package, gensim.</p><p>Topic modeling was first developed and is easier to understand in the case of text, but in the computer vision chapter we will see how some of these techniques may be applied to images as well. Topic models are very important in modern computer vision research. In fact, unlike the previous chapters, this chapter was very close to the cutting edge of research in machine learning algorithms. The original LDA algorithm was published in a scientific journal in 2003, but the method that gensim uses to be able to handle Wikipedia was only developed in 2010 and the HDP algorithm is from 2011. The research continues and you can find many variations and models with wonderful names such as <span class="emphasis"><em>the Indian buffet process</em></span> (not to be confused with the <span class="emphasis"><em>Chinese restaurant process</em></span>, which is a different model), or <span class="emphasis"><em>Pachinko allocation</em></span> (Pachinko being a type of Japanese game, a cross between a slot-machine and pinball).</p><p>We have now gone through some of the major machine learning modes: classification, clustering, and topic modeling.</p><p>In the next chapter, we go back to classification, but this time, we will be exploring advanced algorithms and approaches.</p></div></div>
</body></html>
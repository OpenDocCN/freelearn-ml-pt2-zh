<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Boosting</h1>
                </header>
            
            <article>
                
<p>The second generative method we will discuss is boosting. Boosting aims to combine a number of weak learners into a strong ensemble. It is able to reduce bias, but also variance. Here, weak learners are individual models that perform slightly better than random. For example, in a classification dataset with two classes and an equal number of instances belonging to each class, a weak learner will be able to classify the dataset with an accuracy of slightly more than 50%.</p>
<p>In this chapter, we will present two classic boosting algorithms, Gradient Boosting and AdaBoost. Furthermore, we will explore the use of scikit-learn implementations for classification and regression. Finally, we will experiment with a recent boosting algorithm and its implementation, XGBoost.</p>
<p>The main topics covered are as follows:</p>
<ul>
<li>The motivation behind using boosting ensembles</li>
<li>The various algorithms</li>
<li>Leveraging scikit-learn to create boosting ensembles in Python</li>
<li>Utilizing the XGBoost library for Python</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter06">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter06</a></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/2ShWstT">http://bit.ly/2ShWstT</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaBoost</h1>
                </header>
            
            <article>
                
<p>AdaBoost is one of the most popular boosting algorithms. Similar to bagging, the main idea behind the algorithm is to create a number of uncorrelated weak learners and then combine their predictions. The main difference with bagging is that instead of creating a number of independent bootstrapped train sets, the algorithm sequentially trains each weak learner, assigns weights to all instances, samples the next train set based on the instance's weights, and repeats the whole process. As a base learner algorithm, usually decision trees consisting of a single node are used. These decision trees, with a depth of a single level, are called <strong>decision stumps</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weighted sampling</h1>
                </header>
            
            <article>
                
<p class="mce-root">Weighted sampling is the sampling process were each candidate has a corresponding weight, which determines its probability of being sampled. The weights are normalized, in order for their sum to equal one. Then, the normalized weights correspond to the probability that any individual will be sampled. For a simple example with three candidates, assuming weights of 1, 5, and 10, the following table depicts the normalized weights and the corresponding probability that any candidate will be chosen.</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Candidate</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Weight</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Normalized weight</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Probability</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.0625</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>6.25%</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3125</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>31.25%</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>10</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.625</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>62.50%</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Instance weights to probabilities</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the ensemble</h1>
                </header>
            
            <article>
                
<p>Assuming a classification problem, the AdaBoost algorithm can be described on a high-level basis, from its basic steps. For regression purposes, the steps are similar:</p>
<ol>
<li>Initialize all of the train set instance's weights equally, so their sum equals 1.</li>
<li>Generate a new set by sampling with replacement, according to the weights.</li>
<li>Train a weak learner on the sampled set.</li>
</ol>
<ol start="4">
<li>Calculate its error on the original train set.</li>
<li>Add the weak learner to the ensemble and save its error rate.</li>
<li>Adjust the weights, increasing the weights of misclassified instances and decreasing the weights of correctly classified instances.</li>
<li>Repeat from <em>Step 2</em>.</li>
<li>The weak learners are combined by voting. Each learner's vote is weighted, according to its error rate.</li>
</ol>
<p>The whole process is depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-604 image-border" src="assets/9001110e-758f-4230-aea5-a7077ac7453e.png" style="width:37.42em;height:38.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The process of creating the ensemble for the nth learner</div>
<p>In essence, this makes each new classifier focus on the instances that the previous learners could not handle correctly. Assuming a binary classification problem, we may start with a dataset that looks like the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-605 image-border" src="assets/3dbe0e7c-9aa8-4189-866c-201c38baac6b.png" style="width:20.83em;height:13.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Our initial dataset</div>
<p>Here, all weights are equal. The first decision stump decides to partition the problem space as follows. The dotted line represents the decision boundary. The two black <strong>+</strong> and <strong>-</strong> symbols denote the sub-space that the decision stump classifies every instance as positive or negative, respectively. This leaves two misclassified instances. These instance weights will be increased, while all other weights will be decreased:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-606 image-border" src="assets/8d4ca482-4848-4991-8382-8f7fb365979c.png" style="width:20.92em;height:17.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The first decision stump's space partition and errors</div>
<p>By creating another dataset, where the two misclassified instances are dominant (they may be included several times, as we sample with replacement and their weights are larger than the other instances), the second decision stump partitions the space, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-607 image-border" src="assets/d2e1c6fb-3344-4296-9ab2-196542b77cf5.png" style="width:20.92em;height:16.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The second decision stump's space partition and errors</div>
<p>Finally, after repeating the process for a third decision stump, the final ensemble has partitioned the space as depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-608 image-border" src="assets/9290b515-08ea-41cf-8f59-160bdef9c1c5.png" style="width:20.92em;height:15.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The final ensemble's partition of the problem space</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing AdaBoost in Python</h1>
                </header>
            
            <article>
                
<p>In order to better understand how AdaBoost works, we will present a basic implementation in Python. We will use the breast cancer classification dataset for this example. As always, we first load the libraries and data:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from copy import deepcopy<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn import metrics<br/>import numpy as np<br/>bc = load_breast_cancer()<br/>train_size = 400<br/>train_x, train_y = bc.data[:train_size], bc.target[:train_size]<br/>test_x, test_y = bc.data[train_size:], bc.target[train_size:]<br/>np.random.seed(123456)</pre>
<p>We then create the ensemble. First, we declare the ensemble's size and the base learner type. As mentioned earlier, we use decision stumps (decision trees only a single level deep).</p>
<p>Furthermore, we create a NumPy array for the data instance weights, the learners' weights, and the learners' errors:</p>
<pre># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 3<br/>base_classifier = DecisionTreeClassifier(max_depth=1)<br/># Create the initial weights<br/>data_weights = np.zeros(train_size) + 1/train_size<br/># Create a list of indices for the train set<br/>indices = [x for x in range(train_size)]<br/>base_learners = []<br/>learners_errors = np.zeros(ensemble_size)<br/>learners_weights = np.zeros(ensemble_size)</pre>
<p class="mce-root">For each base learner, we will create a <kbd>deepcopy</kbd> of the original classifier, train it on a sample dataset, and evaluate it. First, we create the copy and sample with replacement from the original test set, according to the instance's weights:</p>
<pre class="mce-root"><br/># Create each base learner<br/>for i in range(ensemble_size):<br/>    weak_learner = deepcopy(base_classifier)<br/>    # Choose the samples by sampling with replacement.<br/>    # Each instance's probability is dictated by its weight.<br/>    data_indices = np.random.choice(indices, train_size, p=data_weights)<br/>    sample_x, sample_y = train_x[data_indices], train_y[data_indices]</pre>
<p class="mce-root">We then fit the learner on the sampled dataset and predict on the original train set. We use the <kbd>predictions</kbd> to see which instances are correctly classified and which instances are misclassified:</p>
<pre class="mce-root">    # Fit the weak learner and evaluate it<br/>    weak_learner.fit(sample_x, sample_y)<br/>    predictions = weak_learner.predict(train_x)<br/>    errors = predictions != train_y<br/>    corrects = predictions == train_y</pre>
<p class="mce-root">In the following, the weighted errors are classified. Both <kbd>errors</kbd> and <kbd>corrects</kbd> are lists of Booleans (<kbd>True</kbd> or <kbd>False</kbd>), but Python handles them as 1 and 0. This allows us to multiply element-wise with <kbd>data_weights</kbd>. The learner's error is then calculated with the average weighted error:</p>
<pre class="mce-root"><br/>    # Calculate the weighted errors<br/>    weighted_errors = data_weights*errors<br/>    # The base learner's error is the average of the weighted errors<br/>    learner_error = np.mean(weighted_errors)<br/>    learners_errors[i] = learner_error</pre>
<p class="mce-root">Finally, the learner's weight can be calculated as half the natural logarithm of the weighted accuracy over the weighted error. In turn, we can use the learner's weight to calculate the new data weights. For erroneously classified instances, the new weight equals the natural exponent of the old weight times the learner's weight. For correctly classified instances, the negative multiple is used instead. Finally, the new weights are normalized and the base learner is added to the <kbd>base_learners</kbd> list:</p>
<pre class="mce-root">    # The learner's weight<br/>    learner_weight = np.log((1-learner_error)/learner_error)/2<br/>    learners_weights[i] = learner_weight<br/>    # Update the data weights<br/>    data_weights[errors] = np.exp(data_weights[errors] * learner_weight)<br/>    data_weights[corrects] = np.exp(-data_weights[corrects] * learner_weight)<br/>    data_weights = data_weights/sum(data_weights)<br/>    # Save the learner<br/>    base_learners.append(weak_learner)</pre>
<p class="mce-root">In order to make predictions with the ensemble, we combine each individual prediction through a weighted majority voting. As this is a binary classification problem, if the weighted average is more than <kbd>0.5</kbd>, the instance is classified as <kbd>0</kbd>; otherwise, it's classified as <kbd>1</kbd>:</p>
<pre class="mce-root"># --- SECTION 3 ---<br/># Evaluate the ensemble<br/>ensemble_predictions = []<br/>for learner, weight in zip(base_learners, learners_weights):<br/>    # Calculate the weighted predictions<br/>    prediction = learner.predict(test_x)<br/>    ensemble_predictions.append(prediction*weight)<br/>    # The final prediction is the weighted mean of the individual predictions<br/>    ensemble_predictions = np.mean(ensemble_predictions, axis=0) &gt;= 0.5<br/>    ensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)<br/><br/># --- SECTION 4 ---<br/># Print the accuracy<br/>print('Boosting: %.2f' % ensemble_acc)</pre>
<p class="mce-root">The final accuracy achieved by this ensemble is 95%.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strengths and weaknesses</h1>
                </header>
            
            <article>
                
<p>Boosting algorithms are able to reduce both bias and variance. For a long time, they were considered immune to overfitting, but in fact they can overfit, although they are extremely robust. One possible explanation is that the base learners, in order to classify outliers, create very strong and complicated rules that rarely fit other instances. In the following diagram, an example is depicted. The ensemble has generated a set of rules in order to correctly classify the outlier, but the rules are so strong that only an identical example (that is, with the exact same feature values) could fit into the sub-space defined by the rules:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-609 image-border" src="assets/74776589-d19e-405e-9f91-67f8937812d0.png" style="width:27.92em;height:20.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Generated rules for an outlier</div>
<p>One disadvantage of many boosting algorithms is that they are not easily parallelized, as the models are created in a sequential fashion. Furthermore, they pose the usual problems of ensemble learning techniques, such as reduction in interpretability and additional computational costs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient boosting</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Gradient boosting is another boosting algorithm. It is a more generalized boosting framework compared to AdaBoost, which also makes it more complicated and math-intensive. Instead of trying to emphasize problematic instances by assigning weights and resampling the dataset, gradient boosting builds each base learner on the previous learner's errors. Furthermore, gradient boosting uses decision trees of varying depths. In this section, we will present gradient boosting, without delving much into the math involved. Instead, we will present the basic concepts, as well as a custom Python implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the ensemble</h1>
                </header>
            
            <article>
                
<p>The gradient boosting algorithm (for regression purposes) starts by calculating the mean of the target variable for the train set and uses it as an initial prediction. Then, it calculates the difference of each instance's target from the prediction (mean), in order to calculate the error. These errors are also called <strong>pseudo-residuals</strong>.</p>
<p>Following that, it creates a decision tree that tries to predict the pseudo-residuals. By repeating this process, a number of times, the whole ensemble is created. Similar to AdaBoost, gradient boosting assigns a weight to each tree. Contrary to AdaBoost, this weight does not depend on the tree's performance. Instead, it is a constant term, which is called <strong>learning rate</strong>. Its purpose is to increase the ensemble's generalization ability, by restricting its over-fitting power. The algorithm's steps are as follows:</p>
<ol>
<li>Define the learning rate (smaller than 1) and the ensemble's size.</li>
<li>Calculate the train set's target mean.</li>
<li>Using the mean as a very simple initial prediction, calculate each instance's target difference from the mean. These errors are called pseudo-residuals.</li>
<li>Build a decision tree, by using the original train set's features and the pseudo-residuals as targets.</li>
</ol>
<ol start="5">
<li>Make predictions on the train set, using the decision tree (we try to predict the pseudo-residuals). Multiply the predicted values by the learning rate.</li>
<li>Add the multiplied values to the previously stored predicted values. Use the newly calculated values as predictions.</li>
<li>Calculate the new pseudo-residuals using the calculated predictions.</li>
<li>Repeat from <em>Step 4</em> until the desired ensemble size is achieved.</li>
</ol>
<p>Note that in order to produce the final ensemble's predictions, each base learner's prediction is multiplied by the learning rate and added to the previous learner's prediction. The calculated mean can be regarded as the first base learner's prediction.</p>
<p>At each step <em>s</em>, for a learning rate <em>lr</em>, the prediction is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5f94494d-65fa-4f84-a396-2b9fb0b4e72a.png" style="width:26.50em;height:1.50em;"/></p>
<p>The residuals are calculated as the difference from the actual target value <em>t</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/78a02149-b4ed-4ee2-9fbc-f0f792bd2e71.png" style="width:6.50em;height:1.25em;"/></p>
<p class="NormalPACKT">The whole process is depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-610 image-border" src="assets/136039d5-8c1c-4600-a6e8-a6e166d33c8f.png" style="width:24.50em;height:54.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Steps to create a gradient boosting ensemble</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>As this is a hands-on book, we will not go further into the mathematical aspect of the algorithm. Nonetheless, for the mathematically curious or inclined, we recommend the following papers. The first is a more regression-specific framework, while the second is more general:</p>
<ul>
<li>Friedman, J.H., 2001. Greedy function approximation: a gradient boosting machine. <em>Annals of statistics</em>, pp.1189-1232.</li>
<li>Mason, L., Baxter, J., Bartlett, P.L. and Frean, M.R., 2000. Boosting algorithms as gradient descent. In <em>Advances in neural information processing systems </em>(pp. 512-518).</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing gradient boosting in Python</h1>
                </header>
            
            <article>
                
<p>Although gradient boosting can be complex and mathematically intensive, if we focus on conventional regression problems, it can be quite simple. In order to demonstrate this, we present a custom implementation in Python, using standard scikit-learn decision trees. For our implementation, we will use the diabetes regression dataset. First, we load the libraries and data, and set the seed for NumPy's random number generator:</p>
<pre><br/># --- SECTION 1 ---<br/># Libraries and data loading<br/>from copy import deepcopy<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn import metrics<br/>import numpy as np<br/>diabetes = load_diabetes()<br/>train_size = 400<br/>train_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]<br/>test_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]<br/>np.random.seed(123456)</pre>
<p class="mce-root">Following this, we define the ensemble's size, learning rate, and the Decision Tree's maximum depth. Furthermore, we create a list to store the individual base learners, as well as a NumPy array to store the previous predictions.</p>
<p class="mce-root">As mentioned earlier, our initial prediction is the train set's target mean. Instead of defining a maximum depth, we could also define a maximum number of leaf nodes by passing the <kbd>max_leaf_nodes=3</kbd> argument to the constructor:</p>
<pre class="mce-root"><br/># --- SECTION 2 ---<br/># Create the ensemble<br/># Define the ensemble's size, learning rate and decision tree depth<br/>ensemble_size = 50<br/>learning_rate = 0.1<br/>base_classifier = DecisionTreeRegressor(max_depth=3)<br/># Create placeholders for the base learners and each step's prediction<br/>base_learners = []<br/># Note that the initial prediction is the target variable's mean<br/>previous_predictions = np.zeros(len(train_y)) + np.mean(train_y)</pre>
<p class="mce-root">The next step is to create and train the ensemble. We start by calculating the pseudo-residuals, using the previous predictions. We then create a deep copy of the base learner class and train it on the train set, using the pseudo-residuals as targets:</p>
<pre class="mce-root"># Create the base learners<br/>for _ in range(ensemble_size):<br/>    # Start by calculating the pseudo-residuals<br/>    errors = train_y - previous_predictions<br/>    # Make a deep copy of the base classifier and train it on the<br/>    # pseudo-residuals<br/>    learner = deepcopy(base_classifier)<br/>    learner.fit(train_x, errors)<br/><span>    predictions = learner.predict(train_x) </span><br/></pre>
<p class="mce-root">Finally, we use the trained base learner in order to predict the pseudo-residuals on the train set. We multiply the predictions by the learning rate and add them to our previous predictions. Finally, we append the base learner to the <kbd>base_learners</kbd> list:</p>
<pre class="mce-root"><br/>    # Multiply the predictions with the learning rate and add the results<br/>    # to the previous prediction<br/>    previous_predictions = previous_predictions + learning_rate*predictions<br/>    # Save the base learner<br/>    base_learners.append(learner)</pre>
<p class="mce-root">In order to make predictions with our ensemble and evaluate it, we use the test set's features in order to predict pseudo-residuals, multiply them by the learning rate, and add them to the train set's target mean. It is important to use the original train set's mean as a starting point, because each tree predicts deviation from that original mean:</p>
<pre class="mce-root"><br/># --- SECTION 3 ---<br/># Evaluate the ensemble<br/># Start with the train set's mean<br/>previous_predictions = np.zeros(len(test_y)) + np.mean(train_y)<br/># For each base learner predict the pseudo-residuals for the test set and<br/># add them to the previous prediction, <br/># after multiplying with the learning rate<br/>for learner in base_learners:<br/>    predictions = learner.predict(test_x)<br/>    previous_predictions = previous_predictions + learning_rate*predictions<br/><br/># --- SECTION 4 ---<br/># Print the metrics<br/>r2 = metrics.r2_score(test_y, previous_predictions)<br/>mse = metrics.mean_squared_error(test_y, previous_predictions)<br/>print('Gradient Boosting:')<br/>print('R-squared: %.2f' % r2)<br/>print('MSE: %.2f' % mse)</pre>
<p class="mce-root">The algorithm is able to achieve an R-squared value of 0.59 and an MSE of 2253.34 with this particular setup.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using scikit-learn</h1>
                </header>
            
            <article>
                
<p>Although for educational purposes it is useful to code our own algorithms, scikit-learn has some very good implementations for both classification and regression problems. In this section, we will go through the implementations, as well as see how we can extract information about the generated ensembles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using AdaBoost</h1>
                </header>
            
            <article>
                
<p>Scikit-learn's Adaboost implementations exist in the <kbd>sklearn.ensemble</kbd> package, in the <kbd>AdaBoostClassifier</kbd> and <kbd>AdaBoostRegressor</kbd> classes.</p>
<p>Like all scikit-learn classifiers, we use the <kbd>fit</kbd> and <kbd>predict</kbd> functions in order to train the classifier and predict on the test set. The first parameter is the base classifier that the algorithm will use. The <kbd>algorithm="SAMME"</kbd> parameter forces the classifier to use a discrete boosting algorithm. For this example, we use the hand-written digits recognition problem:</p>
<pre class="mce-root"># --- SECTION 1 ---<br/># Libraries and data loading<br/>import numpy as np<br/><br/>from sklearn.datasets import load_digits<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import AdaBoostClassifier<br/>from sklearn import metrics<br/><br/>digits = load_digits()<br/>train_size = 1500<br/>train_x, train_y = digits.data[:train_size], digits.target[:train_size]<br/>test_x, test_y = digits.data[train_size:], digits.target[train_size:]<br/>np.random.seed(123456)<br/><br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 200<br/>ensemble = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),<br/>                              algorithm="SAMME",<br/>                              n_estimators=ensemble_size)<br/><br/># --- SECTION 3 ---<br/># Train the ensemble<br/>ensemble.fit(train_x, train_y)<br/><br/># --- SECTION 4 ---<br/># Evaluate the ensemble<br/>ensemble_predictions = ensemble.predict(test_x)<br/>ensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)<br/><br/># --- SECTION 5 ---<br/># Print the accuracy<br/>print('Boosting: %.2f' % ensemble_acc)</pre>
<p class="mce-root">This results in an ensemble with 81% accuracy on the test set. One advantage of using the provided implementation is that we can access and plot each individual base learner's errors and weights. We can access them through <kbd>ensemble.estimator_errors_</kbd> and <kbd>ensemble.estimator_weights_</kbd>, respectively. By plotting the weights, we can gauge where the ensemble stops to benefit from additional base learners. By creating an ensemble of 1,000 base learners, we see that from approximately the 200 base learners mark, the weights are stabilized. Thus, there is little point in adding more than 200. This is further confirmed by the fact that the ensemble of size 1,000 achieves an 82% accuracy, a small increase over the 81% achieved with 200 base learners:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-611 image-border" src="assets/5ff5bdde-8046-483f-be0d-5d27d68ae112.png" style="width:40.83em;height:30.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Base learner weights for an ensemble of 1,000 base learners</div>
<p>The regression implementation adheres to the same principles. Here, we test the algorithm on the diabetes dataset:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from copy import deepcopy<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.ensemble import AdaBoostRegressor<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn import metrics<br/><br/>import numpy as np<br/><br/>diabetes = load_diabetes()<br/><br/>train_size = 400<br/>train_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]<br/>test_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]<br/><br/>np.random.seed(123456)<br/><br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 1000<br/>ensemble = AdaBoostRegressor(n_estimators=ensemble_size)<br/><br/># --- SECTION 3 ---<br/># Evaluate the ensemble<br/>ensemble.fit(train_x, train_y)<br/>predictions = ensemble.predict(test_x)<br/><br/># --- SECTION 4 ---<br/># Print the metrics<br/>r2 = metrics.r2_score(test_y, predictions)<br/>mse = metrics.mean_squared_error(test_y, predictions)<br/><br/>print('Gradient Boosting:')<br/>print('R-squared: %.2f' % r2)<br/>print('MSE: %.2f' % mse)</pre>
<p>The ensemble generates an R-squared of 0.59 and an MSE of 2256.5. By plotting the weights of the base learners, we see that the algorithm has stopped early, due to negligible improvement in predictive power, after the 151<sup>st</sup> base learner. This is indicated by the zero valued weights in the plot. Furthermore, by printing the length of <kbd>ensemble.estimators_</kbd>, we observe that its length is only 151. This is the equivalent of the <kbd>base_learners</kbd> list in our implementation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-612 image-border" src="assets/61ab17f3-3d12-412e-ba29-ef48a5fad8bc.png" style="width:41.83em;height:31.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Base learner weights for the regression Adaboost</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using gradient boosting</h1>
                </header>
            
            <article>
                
<p>Scikit-learn also implements gradient boosting regression and classification. They too are included in the <kbd>ensemble</kbd> package, under <kbd>GradientBoostingRegressor</kbd> and <kbd>GradientBoostingClassifier</kbd>, respectively. The two classes store the errors at each step, in the <kbd>train_score_</kbd> attribute of the object. Here, we present an example for the diabetes regression dataset. The train and validation processes follow the scikit-learn standard, using the <kbd>fit</kbd> and <kbd>predict</kbd> functions. The only parameter that needs to be specified is the learning rate, which is passed to the <kbd>GradientBoostingRegressor</kbd> constructor through the <kbd>learning_rate</kbd> parameter:</p>
<pre class="mce-root"># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.ensemble import GradientBoostingRegressor<br/>from sklearn import metrics<br/>import numpy as np<br/>diabetes = load_diabetes()<br/>train_size = 400<br/>train_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]<br/>test_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]<br/>np.random.seed(123456)<br/><br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 200<br/>learning_rate = 0.1<br/>ensemble = GradientBoostingRegressor(n_estimators=ensemble_size,<br/> learning_rate=learning_rate)<br/><br/># --- SECTION 3 ---<br/># Evaluate the ensemble<br/>ensemble.fit(train_x, train_y)<br/>predictions = ensemble.predict(test_x)<br/><br/># --- SECTION 4 ---<br/># Print the metrics<br/>r2 = metrics.r2_score(test_y, predictions)<br/>mse = metrics.mean_squared_error(test_y, predictions)<br/>print('Gradient Boosting:')<br/>print('R-squared: %.2f' % r2)<br/>print('MSE: %.2f' % mse)</pre>
<p class="mce-root">The ensemble achieves an R-squared of 0.44 and an MSE of 3092. Furthermore, if we use matplotlib to plot <kbd>ensemble.train_score_</kbd>, we can see that diminishing returns appear after around 20 base learners. If we further analyze the errors, by calculating the improvements (difference between base learners), we see that after 25 base learners there are cases where adding a base learner worsens the performance.</p>
<p class="mce-root">Although on average the performance continues to increase, after 50 base learners there is no significant improvement. Thus, we repeat the experiment, with <kbd>ensemble_size = 50</kbd>, yielding an R-squared of 0.61 and an MSE of 2152:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-613 image-border" src="assets/37438d81-ae5f-4ac4-a072-0f60b54b397e.png" style="width:36.50em;height:28.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Errors and differences for gradient boost regression</div>
<p>For the classification example, we use the hand-written digit classification dataset. Again, we define the <kbd>n_estimators</kbd> and <kbd>learning_rate</kbd> parameters:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/><span>import numpy as np</span><br/><br/>from sklearn.datasets import load_digits<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn import metrics<br/><br/><br/>digits = load_digits()<br/><br/>train_size = 1500<br/>train_x, train_y = digits.data[:train_size], digits.target[:train_size]<br/>test_x, test_y = digits.data[train_size:], digits.target[train_size:]<br/><br/>np.random.seed(123456)<br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 200<br/>learning_rate = 0.1<br/>ensemble = GradientBoostingClassifier(n_estimators=ensemble_size,<br/> learning_rate=learning_rate)<br/><br/># --- SECTION 3 ---<br/># Train the ensemble<br/>ensemble.fit(train_x, train_y)<br/><br/># --- SECTION 4 ---<br/># Evaluate the ensemble<br/>ensemble_predictions = ensemble.predict(test_x)<br/><br/>ensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)<br/><br/># --- SECTION 5 ---<br/># Print the accuracy<br/>print('Boosting: %.2f' % ensemble_acc)</pre>
<p>The accuracy achieved with the specific ensemble size is 89%. By plotting the errors and their differences, we see that there are again diminishing returns, but there are no cases where performance significantly drops. Thus, we do not expect a predictive performance improvement by reducing the ensemble size.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">XGBoost</h1>
                </header>
            
            <article>
                
<p>XGBoost is a boosting library with parallel, GPU, and distributed execution support. It has helped many machine learning engineers and data scientists to win Kaggle.com competitions. Furthermore, it provides an interface that resembles scikit-learn's interface. Thus, someone already familiar with the interface is able to quickly utilize the library. Additionally, it allows for very fine control over the ensemble's creation. It supports monotonic constraints (that is, the predicted value should only increase or decrease, relative to a specific feature), as well as feature interaction constraints (for example, if a decision tree creates a node that splits by age, it should not use sex as a splitting feature for all children of that specific node). Finally, it adds an additional regularization parameter, gamma, which further reduces the overfitting capabilities of the generated ensemble. The corresponding paper is Chen, T. and Guestrin, C., 2016, August. Xgboost: A scalable tree boosting system. In <em>Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</em> (pp. 785-794). ACM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using XGBoost for regression</h1>
                </header>
            
            <article>
                
<p>We will present a simple regression example with XGBoost, using the diabetes dataset. As it will be shown, its usage is quite simple and similar to the scikit-learn classifiers. XGBoost implements regression with <kbd>XGBRegressor</kbd>. The constructor has a respectably large number of parameters, which are very well-documented in the official documentation. In our example, we will use the <kbd>n_estimators</kbd>, <kbd>n_jobs</kbd>, <kbd>max_depth</kbd>, and <kbd>learning_rate</kbd> parameters. Following scikit-learn's conventions, they define the ensemble size, the number of parallel processes, the tree's maximum depth, and the learning rate, respectively:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.datasets import load_diabetes<br/>from xgboost import XGBRegressor<br/>from sklearn import metrics<br/>import numpy as np<br/>diabetes = load_diabetes()<br/>train_size = 400<br/>train_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]<br/>test_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]<br/>np.random.seed(123456)<br/><br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 200<br/>ensemble = XGBRegressor(n_estimators=ensemble_size, n_jobs=4,<br/>                        max_depth=1, learning_rate=0.1,<br/><span> objective ='reg:squarederror'</span>)</pre>
<p class="mce-root">The rest of the code evaluates the generated <kbd>ensemble</kbd>, and is similar to any of the previous examples:</p>
<pre class="mce-root"><br/># --- SECTION 3 ---<br/># Evaluate the ensemble<br/>ensemble.fit(train_x, train_y)<br/>predictions = ensemble.predict(test_x)<br/><br/># --- SECTION 4 ---<br/># Print the metrics<br/>r2 = metrics.r2_score(test_y, predictions)<br/>mse = metrics.mean_squared_error(test_y, predictions)<br/>print('Gradient Boosting:')<br/>print('R-squared: %.2f' % r2)<br/>print('MSE: %.2f' % mse)</pre>
<p class="mce-root">XGBoost achieves an R-squared of 0.65 and an MSE of 1932.9, the best performance out of all the boosting methods we tested and implemented in this chapter. Furthermore, we did not fine-tune any of its parameters, which further displays its modeling power.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using XGBoost for classification</h1>
                </header>
            
            <article>
                
<p>For classification purposes, the corresponding class is implemented in <kbd>XGBClassifier</kbd>. The constructor's parameters are the same as the regression implementation. For our example, we use the hand-written digit classification problem. We set the <kbd>n_estimators</kbd> parameter to <kbd>100</kbd> and <kbd>n_jobs</kbd> to <kbd>4</kbd>. The rest of the code follows the usual template:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.datasets import load_digits<br/>from xgboost import XGBClassifier<br/>from sklearn import metrics<br/>import numpy as np<br/>digits = load_digits()<br/>train_size = 1500<br/>train_x, train_y = digits.data[:train_size], digits.target[:train_size]<br/>test_x, test_y = digits.data[train_size:], digits.target[train_size:]<br/>np.random.seed(123456)<br/><br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 100<br/>ensemble = XGBClassifier(n_estimators=ensemble_size, n_jobs=4)<br/><br/># --- SECTION 3 ---<br/># Train the ensemble<br/>ensemble.fit(train_x, train_y)<br/><br/># --- SECTION 4 ---<br/># Evaluate the ensemble<br/>ensemble_predictions = ensemble.predict(test_x)<br/>ensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)<br/><br/># --- SECTION 5 ---<br/># Print the accuracy<br/>print('Boosting: %.2f' % ensemble_acc)</pre>
<p class="mce-root">The ensemble correctly classifies the test set with 89% accuracy, also the highest achieved for any boosting algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other boosting libraries</h1>
                </header>
            
            <article>
                
<p>Two other boosting libraries that are gaining popularity are Microsoft's LightGBM and Yandex' CatBoost. Both of these libraries can match (and even outperform) XGBoost, under certain circumstances. Nonetheless, XGBoost is the best of all three out of the box, without the need of fine-tuning and special data treatment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented one of the most powerful ensemble learning techniques, boosting. We presented two popular boosting algorithms, AdaBoost and gradient boosting. We presented custom implementations for both algorithms, as well as usage examples for the scikit-learn implementations. Furthermore, we briefly presented XGBoost, a library dedicated to regularized, distributed boosting. XGBoost was able to outperform all other methods and implementations on both regression as well as classification problems.</p>
<p>AdaBoost creates a number of base learners by employing weak learners (slightly better than random guessing). Each new base learner is trained on a weighted sample from the original train set. Weighted sampling from a dataset assigns a weight to each instance and then samples from the dataset, using the weights in order to calculate the probability that each instance will be sampled.</p>
<p>The data weights are calculated based on the previous base learner's errors. The base learner's error is also used to calculate the learner's weight. The base learners' predictions are combined through voting, using each learner's weight. Gradient boosting builds its ensemble by training each new base learner using the previous prediction's errors as a target. The initial prediction is the train dataset's target mean. Boosting methods cannot be parallelized in the degree that bagging methods can be. Although robust to overfitting, boosting methods can overfit.</p>
<p>In scikit-learn, AdaBoost implementations store the individual learners' weights, which can be used to identify the point where additional base learners do not contribute to the ensemble's predictive power. Gradient Boosting implementations store the ensemble's error at each step (base learner), which can also help to identify an optimal number of base learners. XGBoost is a library dedicated to boosting, with regularization capabilities that further reduce the overfitting ability of the ensembles. XGBoost is frequently a part of winning machine learning models in many Kaggle competitions.</p>


            </article>

            
        </section>
    </body></html>
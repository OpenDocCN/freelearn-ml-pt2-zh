["```py\n    >>> from torchtext.datasets import IMDB\n    >>> train_dataset = list(IMDB(split='train'))\n    >>> test_dataset = list(IMDB(split='test'))\n    >>> print(len(train_dataset), len(test_dataset))\n    25000 25000 \n    ```", "```py\nconda install -c torchtext\nconda install -c conda-forge portalocker \n```", "```py\npip install portalocker \n```", "```py\n    >>> import re\n    >>> from collections import Counter, OrderedDict\n    >>> def tokenizer(text):\n            text = re.sub('<[^>]*>', '', text)\n            emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n            text = re.sub('[\\W]+', ' ', text.lower()) +\\\n                                  ' '.join(emoticons).replace('-', '')\n            tokenized = text.split()\n            return tokenized\n    >>>\n    >>> token_counts = Counter()\n    >>> train_labels = []\n    >>> for label, line in train_dataset:\n            train_labels.append(label)\n            tokens = tokenizer(line)\n            token_counts.update(tokens)\n    >>> print('Vocab-size:', len(token_counts))\n    Vocab-size: 75977\n    >>> print(Counter(train_labels))\n    Counter({1: 12500, 2: 12500}) \n    ```", "```py\n    >>> from torchtext.vocab import vocab\n    >>> sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1],\n                                       reverse=True)\n    >>> ordered_dict = OrderedDict(sorted_by_freq_tuples)\n    >>> vocab_mapping = vocab(ordered_dict) \n    ```", "```py\n    >>> vocab_mapping.insert_token(\"<pad>\", 0) \n    ```", "```py\n    >>> vocab_mapping.insert_token(\"<unk>\", 1)\n    >>> vocab_mapping.set_default_index(1) \n    ```", "```py\n>>> print([vocab_mapping[token] for token in ['this', 'is', 'an', 'example']])\n[11, 7, 35, 462]\n>>> print([vocab_mapping[token] for token in ['this', 'is', 'example2']])\n[11, 7, 1] \n```", "```py\n    >>> import torch\n    >>> import torch.nn as nn\n    >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    >>> text_transform = lambda x: [vocab[token] for token in tokenizer(x)]\n    >>> def collate_batch(batch):\n            label_list, text_list, lengths = [], [], []\n            for _label, _text in batch:\n                label_list.append(1\\. if _label == 2 else 0.)\n                processed_text = [vocab_mapping[token] for token in tokenizer(_text)] \n                text_list.append(torch.tensor(processed_text, dtype=torch.int64))\n                lengths.append(len(processed_text))\n            label_list = torch.tensor(label_list)\n            lengths = torch.tensor(lengths)\n            padded_text_list = nn.utils.rnn.pad_sequence(\n                                    text_list, batch_first=True)\n            return padded_text_list.to(device), label_list.to(device),\n                                                          lengths.to(device) \n    ```", "```py\n>>> from torch.utils.data import DataLoader\n>>> torch.manual_seed(0)\n>>> dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True,\n                            collate_fn=collate_batch)\n>>> text_batch, label_batch, length_batch = next(iter(dataloader))\n>>> print(text_batch)\ntensor([[   46,     8,   287,    21,    16,     2,    76,  3987,     3,   226, 10,   381,     2,   461,    14,    65,     9,  1208,    17,     8, 13,   856,     2,   156,    70,   398,    50,    32,  2338,    67, 103,     6,   110,    19,     9,     2,   130,     2,   153,    12, 14,    65,  1002,    14,     4,  1143,   226,     6,  1061,    31, 2,  1317,   293,    10,    61,   542,  1459,    24,     6,   105,\n...\n...\n...\n0,     0,     0,     0,     0,     0,     0,     0,     0,     0, 0,     0,     0,     0,     0,     0,     0,     0,     0,     0, 0,     0,     0,     0,     0,     0,     0]], device='cuda:0')\n>>> print(label_batch)\n>>> tensor([0., 1., 1., 0.], device='cuda:0')\n>>> print(length_batch)\ntensor([106,  76, 247, 158], device='cuda:0')\n>>> print(text_batch.shape)\ntorch.Size([4, 247]) \n```", "```py\n    >>> batch_size = 32\n    >>> train_dl = DataLoader(train_dataset, batch_size=batch_size, \n                              shuffle=True, collate_fn=collate_batch)\n    >>> test_dl = DataLoader(test_dataset, batch_size=batch_size,\n                             shuffle=False, collate_fn=collate_batch) \n    ```", "```py\n    >>> vocab_size = len(vocab_mapping)\n    >>> embed_dim = 32 \n    ```", "```py\n>>> rnn_hidden_dim = 50\n>>> fc_hidden_dim = 32 \n```", "```py\n    >>> class RNN(nn.Module):\n            def __init__(self, vocab_size, embed_dim, rnn_hidden_dim, fc_hidden_dim):\n                super().__init__()\n                self.embedding = nn.Embedding(vocab_size,\n                                              embed_dim,\n                                              padding_idx=0)\n                self.rnn = nn.LSTM(embed_dim, rnn_hidden_dim,\n                                   batch_first=True)\n                self.fc1 = nn.Linear(rnn_hidden_dim, fc_hidden_dim)\n                self.relu = nn.ReLU()\n                self.fc2 = nn.Linear(fc_hidden_dim, 1)\n                self.sigmoid = nn.Sigmoid()\n            def forward(self, text, lengths):\n                out = self.embedding(text)\n                out = nn.utils.rnn.pack_padded_sequence(\n                                                out, lengths.cpu().numpy(),\n                                              enforce_sorted=False,\n                                              batch_first=True)\n                out, (hidden, cell) = self.rnn(out)\n                out = hidden[-1, :, :]\n                out = self.fc1(out)\n                out = self.relu(out)\n                out = self.fc2(out)\n                out = self.sigmoid(out)\n                return out \n    ```", "```py\n    >>> model = RNN(vocab_size, embed_dim, rnn_hidden_dim, fc_hidden_dim)\n    >>> model = model.to(device) \n    ```", "```py\n    >>> loss_fn = nn.BCELoss()\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.003) \n    ```", "```py\n    >>> def train(model, dataloader, optimizer):\n            model.train()\n            total_acc, total_loss = 0, 0\n            for text_batch, label_batch, length_batch in dataloader:\n                optimizer.zero_grad()\n                pred = model(text_batch, length_batch)[:, 0]\n                loss = loss_fn(pred, label_batch)\n                loss.backward()\n                optimizer.step()\n                total_acc += ((pred>=0.5).float() == label_batch)\n                                                      .float().sum().item()\n                total_loss += loss.item()*label_batch.size(0)\n            total_loss /= len(dataloader.dataset)\n            total_acc /= len(train_dl.dataset)\n            print(f'Epoch {epoch+1} - loss: {total_loss:.4f} - accuracy:\n                   {total_acc:.4f}') \n    ```", "```py\n    >>> torch.manual_seed(0)\n    >>> num_epochs = 10\n    >>> for epoch in range(num_epochs):\n            train(model, train_dl, optimizer)\n    Epoch 1 - loss: 0.5899 - accuracy: 0.6707\n    Epoch 2 - loss: 0.4354 - accuracy: 0.8019\n    Epoch 3 - loss: 0.2762 - accuracy: 0.8888\n    Epoch 4 - loss: 0.1766 - accuracy: 0.9341\n    Epoch 5 - loss: 0.1215 - accuracy: 0.9563\n    Epoch 6 - loss: 0.0716 - accuracy: 0.9761\n    Epoch 7 - loss: 0.0417 - accuracy: 0.9868\n    Epoch 8 - loss: 0.0269 - accuracy: 0.9912\n    Epoch 9 - loss: 0.0183 - accuracy: 0.9943\n    Epoch 10 - loss: 0.0240 - accuracy: 0.9918 \n    ```", "```py\n    >>> def evaluate(model, dataloader):\n            model.eval()\n            total_acc = 0\n            with torch.no_grad():\n            for text_batch, label_batch, lengths in dataloader:\n                pred = model(text_batch, lengths)[:, 0]\n                total_acc += ((pred>=0.5).float() == label_batch)\n                                                     .float().sum().item()\n    print(f'Accuracy on test set: {100 * total_acc/len(dataloader.dataset)} %')\n    >>> evaluate(model, test_dl)\n    Accuracy on test set: 86.1 % \n    ```", "```py\n>>> nn.LSTM(embed_dim, rnn_hidden_dim, num_layers=2, batch_first=True) \n```", "```py\n    >>> data_raw = pd.read_csv('19900101_20230630.csv', index_col='Date')\n    >>> data = generate_features(data_raw)\n    >>> start_train = '1990-01-01'\n    >>> end_train = '2022-12-31'\n    >>> start_test = '2023-01-01'\n    >>> end_test = '2023-06-30'\n    >>> data_train = data.loc[start_train:end_train]\n    >>> X_train = data_train.drop('close', axis=1).values\n    >>> y_train = data_train['close'].values\n    >>> data_test = data.loc[start_test:end_test]\n    >>> X_test = data_test.drop('close', axis=1).values\n    >>> y_test = data_test['close'].values \n    ```", "```py\n>>> from sklearn.preprocessing import StandardScaler\n>>> scaler = StandardScaler()\n>>> X_scaled_train = torch.FloatTensor(scaler.fit_transform(X_train))\n>>> X_scaled_test = torch.FloatTensor(scaler.transform(X_test))\n>>> y_train_torch = torch.FloatTensor(y_train)\n>>> y_test_torch = torch.FloatTensor(y_test) \n```", "```py\n    >>> def create_sequences(data, labels, seq_length):\n            sequences = []\n            for i in range(len(data) - seq_length):\n                seq = data[i:i+seq_length]\n                label = labels[i+seq_length-1]\n                sequences.append((seq, label))\n            return sequences\n    >>> seq_length = 5\n    >>> sequence_train = create_sequences(X_scaled_train, y_train_torch, seq_length)\n    >>> sequence_test = create_sequences(X_scaled_test, y_test_torch, seq_length) \n    ```", "```py\n    >>> batch_size = 128\n    >>> train_dl = DataLoader(sequence_train, batch_size=batch_size,\n                              shuffle=True) \n    ```", "```py\n    >>> class RNN(nn.Module):\n            def __init__(self, input_dim, rnn_hidden_dim, fc_hidden_dim):\n                super().__init__()\n                self.rnn = nn.LSTM(input_dim, rnn_hidden_dim, 2,\n                                   batch_first=True)\n                self.fc1 = nn.Linear(rnn_hidden_dim, fc_hidden_dim)\n                self.relu = nn.ReLU()\n                self.fc2 = nn.Linear(fc_hidden_dim, 1)\n            def forward(self, x):\n                out, (hidden, cell) = self.rnn(x)\n                out = hidden[-1, :, :]\n                out = self.fc1(out)\n                out = self.relu(out)\n                out = self.fc2(out)\n                return out \n    ```", "```py\n    >>> rnn_hidden_dim = 16\n    >>> fc_hidden_dim = 16\n    >>> model = RNN(X_train.shape[1], rnn_hidden_dim, fc_hidden_dim)\n    >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    >>> model = model.to(device)\n    >>> loss_fn = nn.MSELoss()\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n    ```", "```py\n    >>> def train(model, dataloader, optimizer):\n            model.train()\n            total_loss = 0\n            for seq, label in dataloader:\n                optimizer.zero_grad()\n                pred = model(seq.to(device))[:, 0]\n                loss = loss_fn(pred, label.to(device))\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()*label.size(0)\n            return total_loss/len(dataloader.dataset)\n    >>> num_epochs = 1000\n    >>> for epoch in range(num_epochs):\n    >>>     loss = train(model, train_dl, optimizer)\n    >>>     if epoch % 100 == 0:\n    >>>         print(f'Epoch {epoch+1} - loss: {loss:.4f}')\n    Epoch 1 - loss: 24611083.8868\n    Epoch 101 - loss: 5483.5394\n    Epoch 201 - loss: 11613.8535\n    Epoch 301 - loss: 4459.1431\n    Epoch 401 - loss: 4646.8745\n    Epoch 501 - loss: 4046.1726\n    Epoch 601 - loss: 3583.5710\n    Epoch 701 - loss: 2846.1768\n    Epoch 801 - loss: 2417.1702\n    Epoch 901 - loss: 2814.3970 \n    ```", "```py\n    >>> predictions, y = [], []\n    >>> for seq, label in sequence_test:\n            with torch.no_grad():\n                pred = model.cpu()(seq.view(1, seq_length, X_test.shape[1]))[:, 0]\n                predictions.append(pred)\n                y.append(label)\n    >>> from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n    >>> print(f'R^2: {r2_score(y, predictions):.3f}')\n    R^2: 0.897 \n    ```", "```py\n    >>> with open('warpeace_input.txt', 'r', encoding=\"utf8\") as fp:\n            raw_text = fp.read()\n    >>> raw_text = raw_text.lower() \n    ```", "```py\n    >>> print(raw_text[:200])\n    \"well, prince, so genoa and lucca are now just family estates of the\n    buonapartes. but i warn you, if you don't tell me that this means war,\n    if you still try to defend the infamies and horrors perpetr \n    ```", "```py\n    >>> all_words = raw_text.split()\n    >>> unique_words = list(set(all_words))\n    >>> print(f'Number of unique words: {len(unique_words)}')\n    Number of unique words: 39830 \n    ```", "```py\n>>> n_chars = len(raw_text)\n>>> print(f'Total characters: {n_chars}')\nTotal characters: 3196213 \n```", "```py\n    >>> chars = sorted(list(set(raw_text)))\n    >>> vocab_size = len(chars)\n    >>> print(f'Total vocabulary (unique characters): {vocab_size}')\n    Total vocabulary (unique characters): 57\n    >>> print(chars)\n    ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'ä', 'é', 'ê', '\\ufeff'] \n    ```", "```py\n>>> index_to_char = dict((i, c) for i, c in enumerate(chars))\n>>> char_to_index = dict((c, i) for i, c in enumerate(chars))\n>>> print(char_to_index)\n{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'a': 26, 'b': 27, 'c': 28, 'd': 29, 'e': 30, 'f': 31, 'g': 32, 'h': 33, 'i': 34, 'j': 35, 'k': 36, 'l': 37, 'm': 38, 'n': 39, 'o': 40, 'p': 41, 'q': 42, 'r': 43, 's': 44, 't': 45, 'u': 46, 'v': 47, 'w': 48, 'x': 49, 'y': 50, 'z': 51, 'à': 52, 'ä': 53, 'é': 54, 'ê': 55, '\\ufeff': 56} \n```", "```py\n>>> import numpy as np\n>>> text_encoded = np.array(char_to_index[ch] for ch in raw_text],\n                            dtype=np.int32)\n>>> seq_length = 40\n>>> chunk_size = seq_length + 1\n>>> text_chunks = np.array([text_encoded[i:i+chunk_size]\n                     for i in range(len(text_encoded)-chunk_size+1)]) \n```", "```py\n>>> import torch\n>>> from torch.utils.data import Dataset\n>>> class SeqDataset(Dataset):\n    def __init__(self, text_chunks):\n        self.text_chunks = text_chunks\n    def __len__(self):\n        return len(self.text_chunks)\n    def __getitem__(self, idx):\n        text_chunk = self.text_chunks[idx]\n        return text_chunk[:-1].long(), text_chunk[1:].long()\n>>> seq_dataset = SeqDataset(torch. from_numpy (text_chunks))\n>>> batch_size = 64\n>>> seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True,\n                       drop_last=True) \n```", "```py\n>>> class RNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, rnn_hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn_hidden_dim = rnn_hidden_dim\n        self.rnn = nn.LSTM(embed_dim, rnn_hidden_dim,\n                           batch_first=True)\n        self.fc = nn.Linear(rnn_hidden_dim, vocab_size)\n    def forward(self, x, hidden, cell):\n        out = self.embedding(x).unsqueeze(1)\n        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n        out = self.fc(out).reshape(out.size(0), -1)\n        return out, hidden, cell\n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(1, batch_size, self.rnn_hidden_dim)\n        cell = torch.zeros(1, batch_size, self.rnn_hidden_dim)\n        return hidden, cell \n```", "```py\n    >>> embed_dim = 256\n    >>> rnn_hidden_dim = 512\n    >>> model = RNN(vocab_size, embed_dim, rnn_hidden_dim)\n    >>> model = model.to(device)\n    >>> model\n    RNN(\n      (embedding): Embedding(57, 256)\n      (rnn): LSTM(256, 512, batch_first=True)\n      (fc): Linear(in_features=512, out_features=57, bias=True)\n    ) \n    ```", "```py\n    >>> loss_fn = nn.CrossEntropyLoss()\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.003) \n    ```", "```py\n    >>> num_epochs = 10000\n    >>> for epoch in range(num_epochs):\n            hidden, cell = model.init_hidden(batch_size)\n            seq_batch, target_batch = next(iter(seq_dl))\n            seq_batch = seq_batch.to(device)\n            target_batch = target_batch.to(device)\n            optimizer.zero_grad()\n            loss = 0\n            for c in range(seq_length):\n                pred, hidden, cell = model(seq_batch[:, c],\n                                           hidden.to(device),\n                                           cell.to(device))\n                loss += loss_fn(pred, target_batch[:, c])\n            loss.backward()\n            optimizer.step()\n            loss = loss.item()/seq_length\n            if epoch % 500 == 0:\n                print(f'Epoch {epoch} - loss: {loss:.4f}')\n    Epoch 0 - loss: 4.0255\n    Epoch 500 - loss: 1.4560\n    Epoch 1000 - loss: 1.2794\n    ...\n    8500 loss: - 1.2557\n    Epoch 9000 - loss: 1.2014\n    Epoch 9500 - loss: 1.2442 \n    ```", "```py\n    >>> from torch.distributions.categorical import Categorical\n    >>> def generate_text(model, starting_str, len_generated_text=500):\n        encoded_input = torch.tensor([char_to_index[s] for s in starting_str])\n        encoded_input = torch.reshape(encoded_input, (1, -1))\n        generated_str = starting_str\n        model.eval()\n        hidden, cell = model.init_hidden(1)\n        for c in range(len(starting_str)-1):\n            _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) \n        last_char = encoded_input[:, -1]\n            for _ in range(len_generated_text):\n                logits, hidden, cell = model(last_char.view(1), hidden, cell)\n                logits = torch.squeeze(logits, 0)\n                last_char = Categorical(logits=logits).sample()\n                generated_str += str(index_to_char[last_char.item()])\n            return generated_str\n    >>> model.to('cpu')\n    >>> print(generate_text(model, 'the emperor', 500))\n    the emperor!\" said he.\n    \"finished! it's all with moscow, it's not get bald hills!\" he added the civer with whom and desire to change. they really asked the imperor's field!\" she said. alpaty. there happed the cause of the longle matestood itself. \"the mercy tiresist between paying so impressions, and till the staff offsicilling petya, the chief dear body, returning quite dispatchma--he turned and ecstatically. \"ars doing her dome.\" said rostov, and the general feelings of the bottom would be the pickled ha \n    ```"]
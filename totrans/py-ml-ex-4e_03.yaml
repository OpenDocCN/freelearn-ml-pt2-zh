- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting Online Ad Click-Through with Tree-Based Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we built a movie recommender. In this chapter and
    the next, we will be solving one of the most data-driven problems in digital advertising:
    ad click-through prediction—given a user and the page they are visiting, this
    predicts how likely it is that they will click on a given ad. We will focus on
    learning tree-based algorithms (including decision trees, random forest models,
    and boosted trees) and utilize them to tackle this billion-dollar problem.'
  prefs: []
  type: TYPE_NORMAL
- en: We will be exploring decision trees from the root to the leaves, as well as
    the aggregated version, a forest of trees. This won’t be a theory-only chapter,
    as there are a lot of hand calculations and implementations of tree models from
    scratch included. We will be using scikit-learn and XGBoost, a popular Python
    package for tree-based algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of ad click-through prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring a decision tree from the root to the leaves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a decision tree from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a decision tree with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting ad click-through with a decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembling decision trees – random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembling decision trees – gradient-boosted trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief overview of ad click-through prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Online display advertising is a multibillion-dollar industry. Online display
    ads come in different formats, including banner ads composed of text, images,
    and flash, and rich media such as audio and video. Advertisers, or their agencies,
    place ads on a variety of websites, and even mobile apps, across the internet
    in order to reach potential customers and deliver an advertising message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Online display advertising has served as one of the greatest examples of machine
    learning utilization. Obviously, advertisers and consumers are keenly interested
    in well-targeted ads. In the last 20 years, the industry has relied heavily on
    the ability of machine learning models to predict the effectiveness of ad targeting:
    how likely it is that an audience of a certain age group will be interested in
    this product, that customers with a certain household income will purchase this
    product after seeing the ad, that frequent sports site visitors will spend more
    time reading this ad, and so on. The most common measurement of effectiveness
    is the **Click-Through Rate** (**CTR**), which is the ratio of clicks on a specific
    ad to its total number of views. In general cases without clickbait or spammy
    content, a higher CTR indicates that an ad is targeted well and that an online
    advertising campaign is successful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click-through prediction entails both the promises and challenges of machine
    learning. It mainly involves the binary classification of whether a given ad on
    a given page (or app) will be clicked on by a given user, with predictive features
    from the following three aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Ad content and information (category, position, text, format, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Page content and publisher information (category, context, domain, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User information (age, gender, location, income, interests, search history,
    browsing history, device, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suppose we, as an agency, are operating ads on behalf of several advertisers,
    and our job is to place the right ads for the right audience. Let’s say that we
    have an existing dataset in hand (the following small chunk is an example; the
    number of predictive features can easily go into the thousands in reality) taken
    from millions of records of campaigns run a month ago, and we need to develop
    a classification model to learn and predict future ad placement outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, number, screenshot, font  Description automatically
    generated](img/B21047_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Ad samples for training and prediction'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 3.1*, the features are mostly categorical. In fact,
    data can be either numerical or categorical. Let’s explore this in more detail
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with two types of data – numerical and categorical
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At first glance, the features in the preceding dataset are **categorical** –
    for example, male or female, one of four age groups, one of the predefined site
    categories, and whether the user is interested in sports. Such data is different
    from the **numerical** feature data we have worked with until now.
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical features**, also known as **qualitative features**, represent
    distinct characteristics or groups with a countable number of options. Categorical
    features may or may not have a logical order. For example, household income from
    low to medium to high is an **ordinal** feature, while the category of an ad is
    not ordinal.'
  prefs: []
  type: TYPE_NORMAL
- en: Numerical (also called **quantitative**) features, on the other hand, have mathematical
    meaning as a measurement and, of course, are ordered. For instance, counts of
    items (e.g., number of children in a family, number of bedrooms in a house, and
    number of days until an event ) are discrete numerical features; the height of
    individuals, temperature, and the weight of objects are continuous numerical.
    The cardiotocography dataset ([https://archive.ics.uci.edu/ml/datasets/Cardiotocography](https://archive.ics.uci.edu/ml/datasets/Cardiotocography))
    contains both discrete (such as the number of accelerations per second or the
    number of fetal movements per second) and continuous (such as the mean value of
    long-term variability) numerical features.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical features can also take on numerical values. For example, 1 to 12
    can represent months of the year, and 1 and 0 can indicate adult and minor. Still,
    these values do not have mathematical implications.
  prefs: []
  type: TYPE_NORMAL
- en: The Naïve Bayes classifier you learned about previously works for both numerical
    and categorical features as the likelihoods, *P*(*x* |*y*) or *P*(*feature* |*class*),
    are calculated in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Now, say we are thinking of predicting click-through using Naïve Bayes and trying
    to explain the model to our advertising clients. However, our clients may find
    it difficult to understand the prior and the likelihood of individual attributes
    and their multiplication. Is there a classifier that is easy to interpret and
    explain to clients, and that is able to directly handle categorical data? Decision
    trees are the answer!
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a decision tree from the root to the leaves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a tree-like graph, that is, a sequential diagram illustrating
    all of the possible decision alternatives and their corresponding outcomes. Starting
    from the **root** of a tree, every internal **node** represents the basis on which
    a decision is made. Each branch of a node represents how a choice may lead to
    the next node. And, finally, each **terminal node**, the **leaf**, represents
    the outcome produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we have just made a couple of decisions that brought us to the
    point of using a decision tree to solve our advertising problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Using a decision tree to find the right algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: The first condition, or the root, is whether the feature type is numerical or
    categorical. Let’s assume our ad clickstream data contains mostly categorical
    features, so it goes to the right branch. In the next node, our work needs to
    be interpretable by non-technical clients, so, it goes to the right branch and
    reaches the leaf for choosing the decision tree classifier.
  prefs: []
  type: TYPE_NORMAL
- en: You can also look at the paths and see what kinds of problems they can fit in.
    A decision tree classifier operates in the form of a decision tree. It maps observations
    to class assignments (symbolized as leaf nodes) through a series of tests (represented
    as internal nodes) based on feature values and corresponding conditions (represented
    as branches). In each node, a question regarding the values and characteristics
    of a feature is asked; depending on the answer to the question, the observations
    are split into subsets. Sequential tests are conducted until a conclusion about
    the observations’ target label is reached. The paths from the root to the end
    leaves represent the decision-making process and the classification rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a more simplified scenario, as shown in *Figure 3.3*, where we want to predict
    **Click** or **No click** on a self-driven car ad, we can manually construct a
    decision tree classifier that works for an available dataset. For example, if
    a user is interested in technology and has a car, they will tend to click on the
    ad; a person outside of this subset is unlikely to click on the ad, hypothetically.
    We then use the trained tree to predict two new inputs, whose results are **Click**
    and **No click**, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Predicting Click/No Click with a trained decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: 'After a decision tree has been constructed, classifying a new sample is straightforward,
    as you just saw: starting from the root, apply the test condition and follow the
    branch accordingly until a leaf node is reached, and the class label associated
    will be assigned to the new sample.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we build an appropriate decision tree?
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a decision tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A decision tree is constructed by partitioning the training samples into successive
    subsets. The partitioning process is repeated in a recursive fashion on each subset.
    For each partitioning at a node, a condition test is conducted based on the value
    of a feature of the subset. When the subset shares the same class label, or when
    no further splitting can improve the class purity of this subset, recursive partitioning
    on this node is finished.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important note**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Class purity** refers to the homogeneity of the target variable (class labels)
    within a subset of data. A subset is considered to have high class purity if the
    majority of its instances belong to the same class. In other words, a subset with
    high class purity contains mostly instances of the same class label, while a subset
    with low class purity contains instances from multiple classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretically, to partition a feature (numerical or categorical) with *n* different
    values, there are *n* different methods of binary splitting (**Yes** or **No**
    to the condition test, as illustrated in *Figure 3.4*), not to mention other ways
    of splitting (for example, three- and four-way splitting in *Figure 3.4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Examples of binary splitting and multiway splitting'
  prefs: []
  type: TYPE_NORMAL
- en: Without considering the order of features that partitioning is taking place
    on, there are already *n*^m possible trees for an *m*-dimensional dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many algorithms have been developed to efficiently construct an accurate decision
    tree. Popular ones include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterative Dichotomiser 3** (**ID3**): This algorithm uses a greedy search
    in a top-down manner by selecting the best attribute to split the dataset on with
    each iteration without backtracking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C4.5**: This is an improved version of ID3 that introduces backtracking.
    It traverses the constructed tree and replaces the branches with leaf nodes if
    purity is improved this way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification and Regression Tree** (**CART**): This constructs the tree
    using binary splitting, which we will discuss in more detail shortly. CART’s flexibility,
    efficiency, interpretability, and robustness make it a popular choice for various
    classification and regression tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chi-squared Automatic Interaction Detector** (**CHAID**): This algorithm
    is often used in direct marketing. It involves complicated statistical concepts,
    but basically, it determines the optimal way of merging predictive variables in
    order to best explain the outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basic idea of these algorithms is to grow the tree greedily by making a
    series of local optimizations when choosing the most significant feature to use
    to partition the data. The dataset is then split based on the optimal value of
    that feature. We will discuss the measurement of a significant feature and the
    optimal splitting value of a feature in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will study the CART algorithm in more detail, and we will implement
    it as the most notable decision tree algorithm after that. It constructs the tree
    using binary splitting and grows each node into left and right children. In each
    partition, it greedily searches for the most significant combination of a feature
    and its value; all different possible combinations are tried and tested using
    a measurement function. With the selected feature and value as a splitting point,
    the algorithm then divides the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Samples with the feature of this value (for a categorical feature) or a greater
    value (for a numerical feature) become the right child
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining samples become the left child
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This partitioning process repeats and recursively divides up the input samples
    into two subgroups. The splitting process stops at a subgroup where either of
    the following two criteria is met:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The minimum number of samples for a new node**: When the number of samples
    is not greater than the minimum number of samples required for a further split,
    the partitioning stops in order to prevent the tree from excessively tailoring
    to the training set and, as a result, overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The maximum depth of the tree**: A node stops growing when its depth, which
    is defined as the number of partitions taking place from the top down, starting
    from the root node and ending in a terminal node, meets the maximum tree depth.
    Deeper trees are more specific to the training set and can lead to overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A node with no branches becomes a leaf, and the dominant class of samples at
    this node is the prediction. Once all the splitting processes have finished, the
    tree is constructed and is portrayed with the assigned labels at the terminal
    nodes and the splitting points (feature and value) at all the internal nodes above.
  prefs: []
  type: TYPE_NORMAL
- en: We will implement the CART decision tree algorithm from scratch after studying
    the metrics of selecting the optimal splitting feature and value, as promised.
  prefs: []
  type: TYPE_NORMAL
- en: The metrics for measuring a split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When selecting the best combination of a feature and a value as the splitting
    point, two criteria, such as **Gini Impurity** and **Information Gain**, can be
    used to measure the quality of separation.
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gini Impurity, as its name implies, measures the impurity rate of the class
    distribution of data points, or the class mixture rate. For a dataset with *K*
    classes, suppose that data from class *k(1 ≤ k ≤ K)* takes up a fraction *f*[k]*(0
    ≤ f*[k] *≤ 1)* of the entire dataset; then the *Gini Impurity* of this dataset
    is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: A lower Gini Impurity indicates a purer dataset. For example, when the dataset
    contains only one class, say, the fraction of this class is `1` and that of the
    others is `0`, its Gini Impurity becomes 1 – (1² + 0²) = 0\. In another example,
    a dataset records a large number of coin flips, and heads and tails each take
    up half of the samples. The Gini Impurity is 1 – (0.5² + 0.5²) = 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'In binary cases, Gini Impurity, under different values of the positive class
    fraction, can be visualized with the following code blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The fraction of the positive class varies from `0` to `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Gini Impurity is calculated accordingly, followed by the plot of **Gini
    Impurity** versus **positive fraction**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `1-pos_fraction` is the negative fraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to *Figure 3.5* for the end result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing line, plot, diagram, screenshot  Description automatically
    generated](img/B21047_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Gini Impurity versus positive fraction'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, in binary cases, if the positive fraction is 50%, the impurity
    will be the highest at `0.5`; if the positive fraction is 100% or 0%, it will
    reach `0` impurity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the labels of a dataset, we can implement the Gini Impurity calculation
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Test it out with some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In order to evaluate the quality of a split, we simply add up the Gini Impurity
    of all resulting subgroups, combining the proportions of each subgroup as corresponding
    weight factors. And again, the smaller the weighted sum of the Gini Impurity,
    the better the split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following self-driving car ad example. Here, we split the
    data based on a user’s gender and interest in technology, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, number, font  Description automatically
    generated](img/B21047_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Splitting the data based on gender or interest in tech'
  prefs: []
  type: TYPE_NORMAL
- en: 'The weighted Gini Impurity of the first split can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second split is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, splitting data based on the user’s interest in technology is a better
    strategy than gender.
  prefs: []
  type: TYPE_NORMAL
- en: Information Gain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another metric, **Information Gain**, measures the improvement of purity after
    splitting or, in other words, the reduction of uncertainty due to a split. Higher
    Information Gain implies better splitting. We obtain the Information Gain of a
    split by comparing the **entropy** before and after the split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy is a probabilistic measure of uncertainty. Given a *K*-class dataset,
    and *f*[k] *(0 ≤* *f*[k] *≤ 1)* denoted as the fraction of data from class *k
    (1 ≤ k ≤ K)*, the *entropy* of the dataset is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lower entropy implies a purer dataset with less ambiguity. In a perfect case,
    where the dataset contains only one class, the entropy is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the coin flip example, the entropy becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can visualize how entropy changes with different values of the
    positive class fraction in binary cases using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, plot, line, diagram  Description automatically
    generated](img/B21047_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Entropy versus positive fraction'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, in binary cases, if the positive fraction is 50%, the entropy
    will be the highest at `1`; if the positive fraction is 100% or 0%, it will reach
    `0` entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the labels of a dataset, the `entropy` calculation function can be implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Test it out with some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have fully understood entropy, we can look into how Information
    Gain measures how much uncertainty was reduced after splitting, which is defined
    as the difference in entropy before a split (parent) and after a split (children):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Information Gain* = *Entropy*(*before*) - *Entropy*(*after*) = *Entropy*(*parent*)
    - *Entropy*(*children*)'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy after a split is calculated as the weighted sum of the entropy of each
    child, which is similar to the weighted Gini Impurity.
  prefs: []
  type: TYPE_NORMAL
- en: During the process of constructing a node in a tree, our goal is to search for
    the splitting point where the maximum Information Gain is obtained. As the entropy
    of the parent node is unchanged, we just need to measure the entropy of the resulting
    children due to a split. The best split is the one with the lowest entropy of
    its resulting children.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this better, let’s look at the self-driving car ad example again.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first option, the entropy after the split can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second way of splitting is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For exploration purposes, we can also calculate the Information Gain with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_009.png)'
  prefs: []
  type: TYPE_IMG
- en: '#1 *Information Gain*=0.971-0.951=0.020'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 *Information Gain*=0.971-0.551=0.420'
  prefs: []
  type: TYPE_NORMAL
- en: According to the **information Gain = entropy-based evaluation**, the second
    split is preferable, which is the conclusion of the Gini Impurity criterion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the choice between the two metrics, Gini Impurity and Information
    Gain, has little effect on the performance of the trained decision tree. They
    both measure the weighted impurity of the children after a split. We can combine
    them into one function to calculate the weighted impurity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Test it with the example we just hand-calculated, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have a solid understanding of partitioning evaluation metrics,
    let’s implement the CART tree algorithm from scratch in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a decision tree from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We develop the CART tree algorithm by hand on a toy dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: An example of ad data'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we decide on the first splitting point, the root, by trying
    out all possible values for each of the two features. We utilize the `weighted_impurity`
    function we just defined to calculate the weighted Gini Impurity for each possible
    combination, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we partition according to whether the user interest is tech, we have the
    1^(st), 5^(th), and 6^(th) samples for one group and the remaining samples for
    another group. Then the classes for the first group are `[1, 1, 0]`, and the classes
    for the second group are `[0, 0, 0, 1]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If we partition according to whether the user’s interest is fashion, we have
    the 2^(nd) and 3^(rd) samples for one group and the remaining samples for another
    group. Then the classes for the first group are `[0, 0]`, and the classes for
    the second group are `[1, 0, 1, 0, 1]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The root goes to the user interest feature with the fashion value, as this
    combination achieves the lowest weighted impurity or the highest Information Gain.
    We can now build the first level of the tree, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B21047_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Partitioning the data according to “Is interested in fashion?”'
  prefs: []
  type: TYPE_NORMAL
- en: If we are satisfied with a one-level-deep tree, we can stop here by assigning
    the right branch label **0** and the left branch label **1** as the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can go further down the road, constructing the second level
    from the left branch (the right branch cannot be split further):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With the second splitting point specified by `(occupation, professional)` with
    the lowest Gini Impurity, our tree becomes this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, number  Description automatically
    generated](img/B21047_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Further partitioning of the data according to “Is occupation professional?”'
  prefs: []
  type: TYPE_NORMAL
- en: We can repeat the splitting process as long as the tree does not exceed the
    maximum depth and the node contains enough samples.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the process of the tree construction has been made clear, it is time
    for coding.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with defining a utility function to split a node into left and right
    children based on a feature and a value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We check whether the feature is numerical or categorical and split the data
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the splitting measurement and generation functions available, we now define
    the greedy search function, which tries out all possible splits and returns the
    best one given a selection criterion, along with the resulting children:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The selection and splitting process occurs in a recursive manner on each of
    the subsequent children. When a stopping criterion is met, the process stops at
    a node, and the major label is assigned to this leaf node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And, finally, the recursive function links all of them together:'
  prefs: []
  type: TYPE_NORMAL
- en: It assigns a leaf node if one of two child nodes is empty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It assigns a leaf node if the current branch depth exceeds the maximum depth
    allowed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It assigns a leaf node if the node does not contain sufficient samples required
    for a further split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, it proceeds with a further split with the optimal splitting point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This can be done with the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The function first extracts the left and right children from the node dictionary.
    It then checks whether either the left or right child is empty. If so, it assigns
    a leaf node to the corresponding child. Next, it checks whether the current depth
    exceeds the maximum depth allowed for the tree. If so, it assigns leaf nodes to
    both children. If the left child has enough samples to split (greater than `min_size`),
    it computes the best split using the `get_best_split` function. If the resulting
    split produces empty children, it assigns a leaf node to the corresponding child;
    otherwise, it recursively calls the `split` function on the left child. Similar
    steps are repeated for the right child.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the entry point of the tree’s construction is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s test it with the preceding hand-calculated example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that the resulting tree from the model is identical to what we constructed
    by hand, we write a function displaying the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test it with a numerical example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The resulting trees from our decision tree model are the same as those we hand-crafted.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a more solid understanding of decision trees after implementing
    one from scratch, we can move on with implementing a decision tree with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a decision tree with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we’ll use scikit-learn’s decision tree module ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)),
    which is already well developed and optimized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize the tree we just built, we utilize the built-in `export_graphviz`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this will generate a file called `tree.dot`, which can be converted
    into a PNG image file using **Graphviz** (the introduction and installation instructions
    can be found at [http://www.graphviz.org](http://www.graphviz.org)) by running
    the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to *Figure 3.11* for the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, handwriting, font, screenshot  Description automatically
    generated](img/B21047_03_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Tree visualization'
  prefs: []
  type: TYPE_NORMAL
- en: The generated tree is essentially the same as the one we had before.
  prefs: []
  type: TYPE_NORMAL
- en: I know you can’t wait to employ a decision tree to predict ad click-through.
    Let’s move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting ad click-through with a decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After several examples, it is now time to predict ad click-through using the
    decision tree algorithm you have just thoroughly learned about and practiced with.
    We will use the dataset from a Kaggle machine learning competition, *Click-Through
    Rate Prediction* ([https://www.kaggle.com/c/avazu-ctr-prediction](https://www.kaggle.com/c/avazu-ctr-prediction)).
    The dataset can be downloaded from [https://www.kaggle.com/c/avazu-ctr-prediction/data](https://www.kaggle.com/c/avazu-ctr-prediction/data).
  prefs: []
  type: TYPE_NORMAL
- en: Only the `train.gz` file contains labeled samples, so we only need to download
    this and unzip it (it will take a while). In this chapter, we will focus on only
    the first 300,000 samples from the `train.csv` file unzipped from `train.gz`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fields in the raw file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B21047_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Description and example values of the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'We take a glance at the head of the file by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Rather than a simple `head train`, the output is cleaner as all the columns
    are aligned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, black and white, menu  Description
    automatically generated](img/B21047_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: The first few rows of the data'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t be scared by the anonymized and hashed values. They are categorical features,
    and each of their possible values corresponds to a real and meaningful value,
    but it is presented this way due to the privacy policy. Possibly, `C1` means user
    gender, and `1005` and `1002` represent male and female, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s start by reading the dataset using `pandas`. That’s right, `pandas`
    is extremely good at handling data in a tabular format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The first 300,000 lines of the file are loaded and stored in a DataFrame. Take
    a quick look at the first five rows of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The target variable is the `click` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For the remaining columns, there are several columns that should be removed
    from the features (`id`, `hour`, `device_id`, and `device_ip`) as they do not
    contain much useful information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Each sample has `19` predictive attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to split the data into training and testing sets. Normally, we
    do this by randomly picking samples. However, in our case, the samples are in
    chronological order, as indicated in the `hour` field. Obviously, we cannot use
    future samples to predict past ones. Hence, we take the first 90% as training
    samples and the rest as testing samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned earlier, decision tree models can take in categorical features.
    However, because the tree-based algorithms in scikit-learn (the current version
    is 1.4.1 as of early 2024) only allow numeric input, we need to transform the
    categorical features into numerical ones. But note that, in general, we do not
    need to do this; for example, the decision tree classifier we developed from scratch
    earlier can directly take in categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: We will now transform string-based categorical features into one-hot encoded
    vectors using the `OneHotEncoder` module from `scikit-learn`. One-hot encoding
    was briefly mentioned in *Chapter 1*, *Getting Started with Machine Learning and
    Python*. To recap, it basically converts a categorical feature with *k* possible
    values into *k* binary features. For example, the site category feature with three
    possible values, `news`, `education`, and `sports`, will be encoded into three
    binary features, such as `is_news`, `is_education`, and `is_sports`, whose values
    are either `1` or `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize a `OneHotEncoder` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We fit it on the training set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Each converted sample is a sparse vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We transform the testing set using the trained one-hot encoder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Remember, we specified the `handle_unknown='ignore'` parameter in the one-hot
    encoder earlier. This is to prevent errors due to any unseen categorical values.
    To use the previous site category example, if there is a sample with the value
    `movie`, all of the three converted binary features (`is_news`, `is_education`,
    and `is_sports`) become `0`. If we do not specify `ignore`, an error will be raised.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way we have conducted cross-validation so far is to explicitly split data
    into folds and repetitively write a `for` loop to consecutively examine each hyperparameter.
    To make this less redundant, we’ll introduce a more elegant approach utilizing
    the `GridSearchCV` module from scikit-learn. `GridSearchCV` handles the entire
    process implicitly, including data splitting, fold generation, cross-training
    and validation, and finally, an exhaustive search over the best set of parameters.
    What is left for us is just to specify the hyperparameter(s) to tune and the values
    to explore for each individual hyperparameter. For demonstration purposes, we
    will only tweak the `max_depth` hyperparameter (other hyperparameters, such as
    `min_samples_split` and `class_weight`, are also highly recommended):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We pick three options for the maximal depth – `3`, `10`, and unbounded. We
    initialize a decision tree model with Gini Impurity as the metric and `30` as
    the minimum number of samples required to split further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The classification metric should be the AUC of the ROC curve, as it is an imbalanced
    binary case (only 51,211 out of 300,000 training samples are clicks, which is
    a 17% positive CTR; I encourage you to figure out the class distribution yourself).
    As for grid search, we use three-fold (as the training set is relatively small)
    cross-validation and select the best-performing hyperparameter measured by the
    AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note, `n_jobs=-1` means that we use all of the available CPU processors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the model with the optimal parameter to predict any future test cases
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The AUC we can achieve with the optimal decision tree model is 0.72\. This
    does not seem to be very high, but click-through involves many intricate human
    factors, which is why predicting it is not an easy task. Although we can further
    optimize the hyperparameters, an AUC of 0.72 is actually pretty good. As a comparison,
    randomly selecting 17% of the samples to be clicked on will generate an AUC of
    `0.499`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Our decision tree model significantly outperforms the random predictor. Looking
    back, we can see that a decision tree is a sequence of greedy searches for the
    best splitting point at each step, based on the training dataset. However, this
    tends to cause overfitting as it is likely that the optimal points only work well
    for the training samples. Fortunately, ensembling is the technique to correct
    this, and random forest is an ensemble tree model that usually outperforms a simple
    decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two best practices for getting data ready for tree-based algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encode categorical features**: As mentioned earlier, we need to encode categorical
    features before feeding them into the models. One-hot encoding and label encoding
    are popular choices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale numerical features**: We need to pay attention to the scales of numerical
    features to prevent features with larger scales from dominating the splitting
    decisions in the tree. Normalization or standardization are commonly used for
    this purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembling decision trees – random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **ensemble** technique of **bagging** (which stands for **bootstrap aggregating**),
    which I briefly mentioned in *Chapter 1*, *Getting Started with Machine Learning
    and Python*, can effectively overcome overfitting. To recap, different sets of
    training samples are randomly drawn with replacements from the original training
    data; each resulting set is used to fit an individual classification model. The
    results of these separately trained models are then combined together through
    a **majority vote** to make the final decision.
  prefs: []
  type: TYPE_NORMAL
- en: Tree bagging, as described in the preceding paragraph, reduces the high variance
    that a decision tree model suffers from and, hence, in general, performs better
    than a single tree. However, in some cases, where one or more features are strong
    indicators, individual trees are constructed largely based on these features and,
    as a result, become highly correlated. Aggregating multiple correlated trees will
    not make much difference. To force each tree to become uncorrelated, random forest
    only considers a random subset of the features when searching for the best splitting
    point at each node. Individual trees are now trained based on different sequential
    sets of features, which guarantees more diversity and better performance. Random
    forest is a variant of the tree bagging model with additional **feature-based
    bagging**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To employ random forest in our click-through prediction project, we can use
    the package from scikit-learn. Similarly to the way we implemented the decision
    tree in the preceding section, we only tweak the `max_depth` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides `max_depth`, `min_samples_split`, and `class_weight`, which are important
    hyperparameters related to a single decision tree, hyperparameters that are related
    to a random forest (a set of trees) such as `n_estimators` are also highly recommended.
    We fine-tune `max_depth` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the model with the optimal parameter `None` for `max_depth` (the nodes
    are expanded until another stopping criterion is met) to predict any future unseen
    cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: It turns out that the random forest model gives a substantial lift to the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize several critical hyperparameters to tune:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`: This is the deepest individual tree. It tends to overfit if it
    is too deep or underfit if it is too shallow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: This hyperparameter represents the minimum number of samples
    required for further splitting at a node. Too small a value tends to cause overfitting,
    while too large a value is likely to introduce underfitting. `10`, `30`, and `50`
    might be good options to start with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding two hyperparameters are generally related to individual decision
    trees. The following two parameters are more related to a random forest or collection
    of trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features`: This parameter represents the number of features to consider
    for each best splitting point search. Typically, for an *m*-dimensional dataset,
    ![](img/B21047_03_010.png) (rounded) is a recommended value for `max_features`.
    This can be specified as `max_features="sqrt"` in scikit-learn. Other options
    include `log2`, 20%, and 50% of the original features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimators`: This parameter represents the number of trees considered for
    majority voting. Generally speaking, the more trees, the better the performance
    but the longer the computation time. It is usually set as `100`, `200`, `500`,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we’ll discuss gradient-boosted trees.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling decision trees – gradient-boosted trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Boosting**, which is another ensemble technique, takes an iterative approach
    instead of combining multiple learners in parallel. In boosted trees, individual
    trees are no longer trained separately. Specifically, in **Gradient-Boosted Trees**
    (**GBT**) (also called **Gradient-Boosting Machines**), individual trees are trained
    in succession where a tree aims to correct the errors made by the previous tree.
    The following two diagrams illustrate the difference between random forest and
    GBT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The random forest model builds each tree independently using a different subset
    of the dataset, and then combines the results at the end by majority votes or
    averaging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a tree  Description automatically generated with low confidence](img/B21047_03_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: The random forest workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'The GBT model builds one tree at a time and combines the results along the
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_03_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: The GBT workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'GBT works by iteratively improving the ensemble’s predictions through the addition
    of sequentially trained decision trees, with each tree focusing on the residuals
    of the previous ones. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialization**: The process starts with an initial simple model, often
    a single decision tree, which serves as the starting point for the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential training**: Subsequent decision trees are trained sequentially,
    with each tree attempting to correct the errors of the previous ones. Each new
    tree is trained on the residuals (the differences between the actual and predicted
    values) of the ensemble’s predictions from the previous trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additive modeling**: Each new decision tree is added to the ensemble in a
    way that minimizes the overall error. The trees are typically shallow, with a
    limited number of nodes, to avoid overfitting and improve generalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: GBT introduces a learning rate parameter, which controls
    the contribution of each tree to the ensemble. A lower learning rate leads to
    slower learning but can enhance the overall performance and stability of the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble prediction**: The final prediction is made by combining the predictions
    of all the trees in the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use the `XGBoost` package ([https://xgboost.readthedocs.io/en/latest/](https://xgboost.readthedocs.io/en/latest/))
    to implement GBT. We first install the `XGBoost Python API` via the following
    command with `conda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use `pip`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run into a problem, please install or upgrade `CMake` (a cross-platform
    build system generator), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now take a look at the following steps. You will see how we predict clicks
    using GBT:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import XGBoost and initialize a GBT model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We set the learning rate to `0.1`, which determines how fast or slow we want
    to proceed with learning in each step (in each tree, in GBT). We will discuss
    the learning rate in more detail in *Chapter 4*, *Predicting Online Ad Click-Through
    with Logistic Regression*. `max_depth` for individual trees is set to 10\. Additionally,
    1,000 trees will be trained in sequence in our GBT model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we train the GBT model on the training set we prepared previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the trained model to make predictions on the testing set and calculate
    the ROC AUC accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are able to achieve `0.77` AUC using the XGBoost GBT model.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned about another type of tree ensembling, GBT, and
    applied it to our ad click-through prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, you’ve learned about several tree-based algorithms in the chapter – awesome!
    But picking the right one can be tricky. Here is a practical guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision tree (CART)**: This is the most simple and interpretable algorithm.
    We usually use it for smaller datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest**: This is more robust to overfitting, and can handle larger
    or complex datasets well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GBT**: This is considered the most powerful algorithm for complex problems,
    and the most popular tree-based algorithm in the industry. At the same time, however,
    it can be prone to overfitting. Hence, using hyperparameter tuning and regularization
    techniques to avoid overfitting is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with an introduction to a typical machine learning
    problem, online ad click-through prediction, and its inherent challenges, including
    categorical features. We then looked at tree-based algorithms that can take in
    both numerical and categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we had an in-depth discussion about the decision tree algorithm: its
    mechanics, its different types, how to construct a tree, and two metrics (Gini
    Impurity and entropy) that measure the effectiveness of a split at a node. After
    constructing a tree by hand, we implemented the algorithm from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to use the decision tree package from scikit-learn and
    applied it to predict the CTR. We continued to improve performance by adopting
    the feature-based random forest bagging algorithm. Finally, the chapter ended
    with several ways in which to tune a random forest model, along with two different
    ways of ensembling decision trees, random forest and GBT modeling. Bagging and
    boosting are two approaches to model ensembling that can improve learning performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'More practice is always good for honing your skills. I recommend that you complete
    the following exercises before moving on to the next chapter, where we will solve
    ad click-through prediction using another algorithm: **logistic regression**.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the decision tree click-through prediction project, can you also tweak other
    hyperparameters, such as `min_samples_split` and `class_weight`? What is the highest
    AUC you are able to achieve?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the random forest-based click-through prediction project, can you also tweak
    other hyperparameters, such as `min_samples_split`, `max_features`, and `n_estimators`,
    in scikit-learn? What is the highest AUC you are able to achieve?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the GBT-based click-through prediction project, what hyperparameters can
    you tweak? What is the highest AUC you are able to achieve? You can read [https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)
    to figure it out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1878468721786989681.png)'
  prefs: []
  type: TYPE_IMG

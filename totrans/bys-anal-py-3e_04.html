<html><head></head><body>
<section id="chapter-5-comparing-models" class="level2 chapterHead" data-number="1.9">&#13;
<h1 class="chapterHead" data-number="1.9">ChapterÂ 5<br/>&#13;
<span id="x1-950005"/>Comparing Models</h1>&#13;
<blockquote>&#13;
<p>A map is not the territory it represents, but, if correct, it has a similar structure to the territory. â€“ Alfred Korzybski</p>&#13;
</blockquote>&#13;
<p>Models should be designed as approximations to help us understand a particular problem or a class of related problems. Models are not designed to be verbatim copies of the <em>real world</em>. Thus, all models are wrong in the same sense that maps are not the territory. But not all models are equally wrong; some models will be better than others at describing a given problem.</p>&#13;
<p>In the previous chapters, we focused our attention on the inference problem, that is, how to learn the values of parameters from data. In this chapter, we are going to focus on a complementary problem: how to compare two or more models for the same data. As we will learn, this is both a central problem in data analysis and a tricky one. In this chapter, we are going to keep examples super simple, so we can focus on the technical aspects of model comparison. In the forthcoming chapters, we are going to apply what we learn here to more complex examples.</p>&#13;
<p>In this chapter, we will explore the following topics:</p>&#13;
<ul>&#13;
<li><p>Overfitting and underfitting</p></li>&#13;
<li><p>Information criteria</p></li>&#13;
<li><p>Cross-validation</p></li>&#13;
<li><p>Bayes factors</p></li>&#13;
</ul>&#13;
<p><span id="x1-95001r214"/></p>&#13;
<section id="posterior-predictive-checks-1" class="level3 sectionHead" data-number="1.9.1">&#13;
<h2 class="sectionHead" data-number="1.9.1">5.1 <span id="x1-960001"/>Posterior predictive checks</h2>&#13;
<p>We have previously introduced and discussed posterior predictive checks as a way to assess how well a model <span id="dx1-96001"/>explains the data used to fit a model. The purpose of this type of testing is not to determine whether a model is incorrect; we already know this! The goal of the exercise is to understand how well we are capturing the data. By performing posterior predictive checks, we aim to better understand the limitations of a model. Once we understand the limitations, we can simply acknowledge them or try to remove them by improving the model. It is expected that a model will not be able to reproduce all aspects of a problem and this is usually not a problem as models are built with a purpose in mind. As different models often capture different aspects of data, we can compare models using posterior predictive checks.</p>&#13;
<p>Letâ€™s look at a simple example. We have a dataset with two variables, <code>x </code>and <code>y</code>. We are going to fit these data with a linear model:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file139.jpg" class="math-display" alt="y = ğ›¼ + ğ›½x "/>&#13;
</div>&#13;
<p>We will also fit the data using a quadratic model, that is, a model with one more term than the linear model. For this extra term, we just take <em>x</em> to the power of 2 and add a <em>Î²</em> coefficient:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file140.jpg" class="math-display" alt="y = ğ›¼ + ğ›½0x + ğ›½1x2 "/>&#13;
</div>&#13;
<p>We can write these models in PyMC as usual; refer to the following code block. The only difference from all previous models we have <span id="dx1-96002"/>seen so far is that we pass the argument <code>idata_kwargs="log_likelihood": True </code>to <code>pm.sample</code>. This extra step will store the log-likelihood in the <code>InferenceData </code>object, and we will use this info later:</p>&#13;
<p><span id="x1-96003r1"/> <span id="x1-96004"/><strong>CodeÂ 5.1</strong></p>&#13;
<pre id="listing-58" class="source-code"><code>with pm.Model() as model_l:Â </code>&#13;
<code>Â Â Â Â <em>Î±</em>Â = pm.Normal("<em>Î±</em>", mu=0, sigma=1)Â </code>&#13;
<code>Â Â Â Â <em>Î²</em>Â = pm.Normal("<em>Î²</em>", mu=0, sigma=10)Â </code>&#13;
<code>Â Â Â Â ÏƒÂ = pm.HalfNormal("Ïƒ", 5)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â Î¼Â = <em>Î±</em>Â + <em>Î²</em>Â * x_c[0]Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â y_pred = pm.Normal("y_pred", mu=Î¼, sigma=Ïƒ, observed=y_c)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â idata_l = pm.sample(2000, idata_kwargs={"log_likelihood": True})Â </code>&#13;
<code>Â Â Â Â idata_l.extend(pm.sample_posterior_predictive(idata_l))Â </code>&#13;
<code>Â </code>&#13;
<code>with pm.Model() as model_q:Â </code>&#13;
<code>Â Â Â Â <em>Î±</em>Â = pm.Normal("<em>Î±</em>", mu=0, sigma=1)Â </code>&#13;
<code>Â Â Â Â <em>Î²</em>Â = pm.Normal("<em>Î²</em>", mu=0, sigma=10, shape=order)Â </code>&#13;
<code>Â Â Â Â ÏƒÂ = pm.HalfNormal("Ïƒ", 5)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â Î¼Â = <em>Î±</em>Â + pm.math.dot(<em>Î²</em>, x_c)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â y_pred = pm.Normal("y_pred", mu=Î¼, sigma=Ïƒ, observed=y_c)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â idata_q = pm.sample(2000, idata_kwargs={"log_likelihood": True})Â </code>&#13;
<code>Â Â Â Â idata_q.extend(pm.sample_posterior_predictive(idata_q))</code></pre>&#13;
<p><em>Figure <a href="#x1-96028r1">5.1</a></em> shows the mean fit from both models. Visually, both models seem to provide a reasonable fit to the data. At least for me, it is not that easy to see which model is best. What do you think?</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file141.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-96028r1"/><strong>FigureÂ 5.1</strong>: Mean fit for <code>model_l </code>(linear) and <code>model_q </code>(quadratic)</p>&#13;
<p>To gain further insights, we can do a posterior predictive check. <em>Figure <a href="#x1-96030r2">5.2</a></em> shows KDEs for the observed and predicted data. Here, it is <span id="dx1-96029"/>easy to see that <code>model_q</code>, the quadratic model, provides a better fit to the data. We can also see there is a lot of uncertainty, in particular at the tails of the distributions. This is because we have a small number of data points.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file142.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-96030r2"/><strong>FigureÂ 5.2</strong>: Posterior predictive checks for <code>model_l </code>and <code>model_q </code>created with the <code>az.plot_ppc </code>function</p>&#13;
<p>Posterior predictive checks are a very versatile idea. We can compare observed and predicted data in so many ways. For instance, instead of comparing the densities of the distributions, we can compare summary statistics. In the top panel of <em>Figure <a href="#x1-96032r3">5.3</a></em>, we have the distributions of means for both models. The dot over the x axis indicates the observed value. We can see that both models capture the mean very well, with the quadratic model having less variance. That both models capture the mean very well is not surprising as we are explicitly modeling the mean. In the <span id="dx1-96031"/>bottom panel, we have the distributions of the interquartile range. This comparison favors the linear model instead.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file143.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-96032r3"/><strong>FigureÂ 5.3</strong>: Posterior predictive checks for <code>model_l </code>and <code>model_q </code>created with the <code>az.plot_bpv </code>function</p>&#13;
<p>In general, a statistic that is <em>orthogonal</em> to what the model is explicitly modeling will be more informative for evaluating the model. When in doubt, it may be convenient to evaluate more than one statistic. A useful question is to ask yourself what aspects of the data you are interested in capturing.</p>&#13;
<p>To generate <em>Figure <a href="#x1-96032r3">5.3</a></em>, we used the <code>az.plot_bpv </code>ArviZ function. An excerpt of the full code to generate that figure is the following:</p>&#13;
<p><span id="x1-96033r2"/> <span id="x1-96034"/><strong>CodeÂ 5.2</strong></p>&#13;
<pre id="listing-59" class="source-code"><code>idatas = [idata_l, idata_q]Â </code>&#13;
<code>Â </code>&#13;
<code>def iqr(x, a=-1):Â </code>&#13;
<code>Â Â Â Â """interquartile range"""Â </code>&#13;
<code>Â Â Â Â return np.subtract(*np.percentile(x, [75, 25], axis=a))Â </code>&#13;
<code>Â </code>&#13;
<code>for idata in idatas:Â </code>&#13;
<code>Â Â Â Â az.plot_bpv(idata, kind="t_stat", t_stat="mean", ax=axes[0])Â </code>&#13;
<code>Â </code>&#13;
<code>for idata in idatas:Â </code>&#13;
<code>Â Â Â Â az.plot_bpv(idata, kind="t_stat", t_stat=iqr, ax=axes[1])</code></pre>&#13;
<p>Notice that we use the <code>kind="t_stat" </code>argument to indicate that we are going to use a summary statistic. We can pass a string as in <code>t_stat="mean"</code>, to indicate that we want to use the mean as the summary statistic. Or, we can use a user-defined function, as in <code>t_stat=iqr</code>.</p>&#13;
<p>You may have noticed that <em>Figure <a href="#x1-96032r3">5.3</a></em> also includes a legend with <code>bpv </code>values. <strong>bpv</strong> stands for Bayesian p-value. This is a <span id="dx1-96046"/>numerical way of summarizing a comparison between simulated and observed data. To obtain them, a summary statistic <em>T</em> is chosen, such as the mean, median, standard deviation, or whatever you may think is worth comparing. Then <em>T</em> is calculated for the observed data <em>T</em><sub>obs</sub> and for the simulated data <em>T</em><sub>sim</sub>. Finally, we ask ourselves the question â€what is the probability that <em>T</em><sub>sim</sub> is less than or equal to <em>T</em><sub>obs</sub>?â€. If the observed values agree with the predicted ones, the expected value will be 0.5. In other words, half of the predictions will be below the <span id="dx1-96047"/>observations and half will be above. This quantity is known as the <strong>Bayesian</strong> <strong>p-value</strong>:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file144.jpg" class="math-display" alt="Bayesian p-value â‰œ p(Tsim â‰¤ Tobs | ËœY) "/>&#13;
</div>&#13;
<p>There is yet another way to compute a Bayesian p-value. Instead of using a summary statistic, we can use the entire distribution. In this case, we can ask ourselves the question â€what is the probability of predicting a lower or equal value for <strong>each observed value</strong>?â€. If the model is well calibrated, these probabilities <span id="dx1-96048"/>should be the same for all observed values. Because the model is capturing all observations equally well, we should expect a Uniform distribution. ArviZ can help us with the computations; this time we need to use the <code>az.plot_bpv </code>function with the <code>kind="p_value" </code>argument (which is the default). <em>Figure <a href="#x1-96050r4">5.4</a></em> shows the results of <span id="dx1-96049"/>this calculation. The white line indicates the expected Uniform distribution and the gray band shows the expected deviation given the finite size of the sample. It can be seen that these models are very similar.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file145.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-96050r4"/><strong>FigureÂ 5.4</strong>: Posterior predictive checks for <code>model_l </code>and <code>model_q </code>created with the <code>az.plot_bpv </code>function</p>&#13;
<div id="tcolobox-11" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Not Those p-values</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>For those who are familiar with p-values and their use in frequentist statistics, there are a couple of clarifications. What is <em>Bayesian</em> about these p-values is that we are NOT using a sampling distribution but the posterior predictive distribution. Additionally, we are not doing a null hypothesis test, nor trying to declare that a difference is â€significant.â€ We are simply trying to quantify how well the model explains the data.</p>&#13;
</div>&#13;
</div>&#13;
<p>Posterior predictive checks provide a very flexible framework for evaluating and comparing models, either using plots or numerical summaries such as Bayesian p-values, or a combination of both. The concept is general enough to allow an analyst to use their imagination to find different ways to <span id="dx1-96051"/>explore the modelâ€™s predictions and use the ones that best suit their modeling goals.</p>&#13;
<p>In the following sections, we will explore other methods for comparing models. These new methods can be used in combination with posterior predictive checks. <span id="x1-96052r217"/></p>&#13;
</section>&#13;
<section id="the-balance-between-simplicity-and-accuracy" class="level3 sectionHead" data-number="1.9.2">&#13;
<h2 class="sectionHead" data-number="1.9.2">5.2 <span id="x1-970002"/>The balance between simplicity and accuracy</h2>&#13;
<p>When choosing between <span id="dx1-97001"/>alternative explanations, there is a principle known as Occamâ€™s razor. In very general terms, this principle establishes that given two or more equivalent explanations for the same phenomenon, the simplest is the preferred explanation. A common criterion of simplicity is the number of parameters in a model.</p>&#13;
<p>There are many justifications for this heuristic. We are not going to discuss any of them; we are just going to accept them as a reasonable guide.</p>&#13;
<p>Another factor that we generally have to take into <span id="dx1-97002"/>account when comparing models is their accuracy, that is, how good a model is at fitting the data. According to this criterion, if we have two (or more) models and one of them explains the data better than the other, then that is the preferred model.</p>&#13;
<p>Intuitively, it seems that when comparing models, we tend to prefer those that best fit the data and those that are simple. But what should we do if these two principles lead us to different models? Or, more generally, is there a quantitative way to balance both contributions? The short answer is yes, and in fact, there is more than one way to do it. But first, letâ€™s see an example to gain intuition. <span id="x1-97003r207"/></p>&#13;
<section id="many-parameters-may-lead-to-overfitting" class="level4 subsectionHead" data-number="1.9.2.1">&#13;
<h3 class="subsectionHead" data-number="1.9.2.1">5.2.1 <span id="x1-980001"/>Many parameters (may) lead to overfitting</h3>&#13;
<p><em>Figure <a href="#x1-98002r5">5.5</a></em> shows three models <span id="dx1-98001"/>with an increasing number of parameters. The first one (order 0) is just a constant value: whatever the value of <em>X</em>, the model always predicts the same value for <em>Y</em> . The second model (order 1) is a linear model, as we saw in <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>. The last one (order 5) is a polynomial model of order 5. We will discuss polynomial regression in more depth in <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em>, but for the moment, we just need to know that the core of the model has the form <em>Î±</em> + <em>Î²</em><sub>0</sub><em>x</em> + <em>Î²</em><sub>0</sub><em>x</em><sup>2</sup> + <em>Î²</em><sub>0</sub><em>x</em><sup>3</sup> + <em>Î²</em><sub>0</sub><em>x</em><sup>4</sup> + <em>Î²</em><sub>0</sub><em>x</em><sup>5</sup>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file146.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-98002r5"/><strong>FigureÂ 5.5</strong>: Three models for a simple dataset</p>&#13;
<p>In <em>Figure <a href="#x1-98002r5">5.5</a></em>, we can see that the increase in the complexity of the model (number of parameters) is accompanied by a greater accuracy reflected in the coefficient of determination <em>R</em><sup>2</sup>. This is a way to measure the fit of a model (for more information, please read <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" class="url">https://en.wikipedia.org/wiki/Coefficient_of_determination</a>). In fact, we can see that the polynomial of order 5 fits the <span id="dx1-98003"/>data perfectly, obtaining <em>R</em><sup>2</sup> = 1.</p>&#13;
<p>Why can the polynomial of order 5 capture the data without errors? The reason is that we have the same number of parameters as data, that is, six. Therefore, the model is simply acting as an alternative way of expressing the data. The model is not learning patterns about the data, it is memorizing the data! This can be problematic. The easier way to notice this is by thinking about what will happen to a model that memorizes data when presented with new, unobserved data. What do you think will happen?</p>&#13;
<p>Well, the performance is expected to be bad, like someone who just memorizes the questions for an exam only to find the questions have been changed at the last minute! This situation is represented in <em>Figure <a href="#x1-98004r6">5.6</a></em>; here, we have added two new data points. Maybe we got the money to perform a new experiment or our boss just sent us new data. We can see that the model of order 5, which was able to exactly fit the data, now has a worse performance than the linear model, as measured by <em>R</em><sup>2</sup>. From this simple example, we can see that a model with the best fit is not always the ideal one.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file147.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-98004r6"/><strong>FigureÂ 5.6</strong>: Three models for a simple dataset, plus two new points</p>&#13;
<p>Loosely speaking, when a model fits the dataset used to learn the parameters of that model very well but fits new datasets very poorly, we have overfitting. This is a very common problem when analyzing data. A useful way to think about overfitting is to consider a dataset as having two components: the signal and the noise. The signal is what we want to capture (or learn) from the data. If we use a dataset, it is because we believe there is a signal there, otherwise it will be an <span id="dx1-98005"/>exercise in futility. Noise, on the other hand, is not useful and is the product of measurement errors, limitations in the way the data was generated or captured, the presence of corrupted data, etc. A model overfits when it is so flexible (for a dataset) that it is capable of learning noise. This has the consequence that the signal is hidden.</p>&#13;
<p>This is a practical justification for Occamâ€™s razor, and also a warning that, at least in principle, it is always possible to create a model so complex that it explains all the details in a dataset, even the most irrelevant ones â€” like the cartographers in Borgesâ€™ tale, who crafted a map of the Empire as vast as the Empire itself, perfectly replicating every detail. <span id="x1-98006r223"/></p>&#13;
</section>&#13;
<section id="too-few-parameters-lead-to-underfitting" class="level4 subsectionHead" data-number="1.9.2.2">&#13;
<h3 class="subsectionHead" data-number="1.9.2.2">5.2.2 <span id="x1-990002"/>Too few parameters lead to underfitting</h3>&#13;
<p>Continuing with the same example <span id="dx1-99001"/>but at the other extreme of complexity, we have the model of order 0. This model is simply a Gaussian disguised as a linear model. This model is only capable of capturing the value of the mean of <em>Y</em> and is therefore totally indifferent to the values of <em>X</em>. We say that this model has underfitted the data. Models that underfit can also be misleading, especially if <span id="dx1-99002"/>we are unaware of it. <span id="x1-99003r222"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="measures-of-predictive-accuracy" class="level3 sectionHead" data-number="1.9.3">&#13;
<h2 class="sectionHead" data-number="1.9.3">5.3 <span id="x1-1000003"/>Measures of predictive accuracy</h2>&#13;
<p>â€Everything should be made as <span id="dx1-100001"/>simple as possible, but not simplerâ€ is a quote often attributed to Einstein. As in a healthy diet, when modeling, we have to maintain a balance. Ideally, we would like to have a model that neither underfits nor overfits the data. We want to somehow balance simplicity and goodness of fit.</p>&#13;
<p>In the previous example, it is relatively easy to see that the model of order 0 is too simple, while the model of order 5 is too complex. In order to get a general approach that will allow us to rank models, we need to formalize our intuition about this balance of simplicity and accuracy.</p>&#13;
<p>Letâ€™s look at a couple of terms that will be useful to us:</p>&#13;
<ul>&#13;
<li><p><strong>Within-sample accuracy</strong>: The accuracy is <span id="dx1-100002"/>measured with the same data used to fit the model.</p></li>&#13;
<li><p><strong>Out-of-sample accuracy</strong>: The accuracy measured with <span id="dx1-100003"/>data not used to fit the model.</p></li>&#13;
</ul>&#13;
<p>The within-sample accuracy will, on average, be greater than the out-of-sample accuracy. That is why using the within-sample accuracy to evaluate a model, in general, will lead us to think that we have a better model than we really have. Using out-of-sample accuracy is therefore a good idea to avoid fooling ourselves. However, leaving data out means we will have less data to inform our models, which is a luxury we generally cannot afford. Since this is a central problem in data analysis, there are several proposals to address it. Two very popular approaches are:</p>&#13;
<ul>&#13;
<li><p><strong>Information criteria</strong>: This is a general <span id="dx1-100004"/>term thatâ€™s used to refer to various expressions that approximate out-of-sample accuracy as in-sample accuracy plus a term that penalizes model complexity.</p></li>&#13;
<li><p><strong>Cross-validation</strong>: This is an empirical <span id="dx1-100005"/>strategy based on dividing the available data into separate subsets that are alternatively used to fit and evaluate the models.</p></li>&#13;
</ul>&#13;
<p>Letâ€™s look at both of those approaches in more detail in the following sections. <span id="x1-100006r224"/></p>&#13;
<section id="information-criteria" class="level4 subsectionHead" data-number="1.9.3.1">&#13;
<h3 class="subsectionHead" data-number="1.9.3.1">5.3.1 <span id="x1-1010001"/>Information criteria</h3>&#13;
<p>Information criteria are a collection of <span id="dx1-101001"/>closely related tools used to compare models in terms of goodness-of-fit and model complexity. In other words, the information criteria formalize the intuition that we developed at the beginning of the chapter. The exact way in which these quantities are derived <span id="dx1-101002"/>has to do with a field known as Information Theory ([<a href="Bibliography.xhtml#Xmackay_2003">MacKay</a>,Â <a href="Bibliography.xhtml#Xmackay_2003">2003</a>]), which is fun, but we will pursue a more intuitive explanation.</p>&#13;
<p>One way to measure how well a model fits the data is to calculate the root mean square error between the data and the predictions made by the model:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file148.jpg" class="math-display" alt=" n 1-âˆ‘ 2 n (yi âˆ’ E(yi | Î¸)) i=1 "/>&#13;
</div>&#13;
<p>E(<em>y</em><sub><em>i</em></sub><span class="cmsy-10x-x-109">|</span><em>Î¸</em>) is the predicted value given the estimated parameters. It is important to note that this is essentially the average of the squared difference between the observed and predicted data. Taking the square of the errors ensures that the differences do not cancel out and emphasizes large errors compared to other alternatives such as calculating the absolute value.</p>&#13;
<p>The root mean square error may be familiar to you. It is a very popular measure â€“ so popular that we may have never spent time thinking about it. But if we do, we will see that, in principle, there is nothing special about it and we could well devise other similar expressions. When we adopt a probabilistic approach, as we do in this book, a more general (and <em>natural</em>) expression is the following:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file149.jpg" class="math-display" alt="âˆ‘n log p(yi | Î¸) i=1 "/>&#13;
</div>&#13;
<p>That is, we compute the likelihood for each of the <em>n</em> observations. We take the sum instead of the product because we are working with logarithms. Why do we say this is <em>natural</em>? Because we can think that, when choosing a likelihood for a model, we are implicitly choosing how we want to penalize deviations between the data and predictions. In fact, when <em>p</em>(<em>y</em><sub><em>i</em></sub><span class="cmsy-10x-x-109">|</span><em>Î¸</em>) is a Gaussian, then the <span id="dx1-101003"/>above expression will be <span id="dx1-101004"/>proportional to the root mean square error.</p>&#13;
<p>Now, letâ€™s shift our focus to a detailed exploration of a few specific information criteria.</p>&#13;
<section id="akaike-information-criterion" class="level5 likesubsubsectionHead" data-number="1.9.3.1.1">&#13;
<h4 class="likesubsubsectionHead" data-number="1.9.3.1.1"><span id="x1-1020001"/>Akaike Information Criterion</h4>&#13;
<p><strong>Akaike Information Criterion</strong> (<strong>AIC</strong>) is a well-known <span id="dx1-102001"/>and widely used information criterion outside the Bayesian universe <span id="dx1-102002"/>and is defined as:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file150.jpg" class="math-display" alt=" âˆ‘n Ë† AIC = âˆ’ 2 log p(yi |Î¸mle)+ 2k i=1 "/>&#13;
</div>&#13;
<p><em>k</em> is the number of model parameters and <img src="../media/hat_theta.png" style="width:0.50em;"/><sub><em>mle</em></sub> is the maximum likelihood estimate for <em>Î¸</em>.</p>&#13;
<p>Maximum likelihood <span id="dx1-102003"/>estimation is common practice for non-Bayesians and is, in general, equivalent to Bayesian <strong>maximum a posteriori</strong> (<strong>MAP</strong>) estimation when <em>flat</em> priors are used. It is important to note that <img src="../media/hat_theta.png" style="width:0.50em;"/><sub><em>mle</em></sub> is a point estimate and not a distribution.</p>&#13;
<p>The factor <span class="cmsy-10x-x-109">âˆ’</span>2 is just a constant, and we could omit it but usually donâ€™t. What is important, from a practical point of view, is that the first term takes into account how well the model fits the data, while the second term penalizes the complexity of the model. Therefore, if two models fit the data equally well, AIC says that we should choose the model with the fewest parameters.</p>&#13;
<p>AIC works fine in non-Bayesian approaches but is problematic otherwise. One reason is that it does not use the posterior distribution of <em>Î¸</em> and therefore discards information. Also, AIC, from a Bayesian perspective, assumes that priors are <em>flat</em> and therefore AIC is incompatible with informative and slightly informative priors like those used in this book. Also, the number of parameters in a model is not a good measure of the modelâ€™s complexity when using informative priors or structures like hierarchical structures, as these are ways of reducing the effective <span id="dx1-102004"/>number of parameters, also <span id="dx1-102005"/>known as <em>regularization</em>. We will return to this idea of <span id="dx1-102006"/>regularization later.</p>&#13;
</section>&#13;
<section id="widely-applicable-information-criteria" class="level5 likesubsubsectionHead" data-number="1.9.3.1.2">&#13;
<h4 class="likesubsubsectionHead" data-number="1.9.3.1.2"><span id="x1-1030001"/>Widely applicable information criteria</h4>&#13;
<p><strong>Widely applicable information criteria</strong> (<strong>WAIC</strong>) is something like the Bayesian version of AIC. It also has <span id="dx1-103001"/>two terms, one that measures how <span id="dx1-103002"/>good the fit is and the other that penalizes complex models. But WAIC uses the full posterior distribution to estimate both terms. The following expression assumes that the posterior distribution is represented as a sample of size <em>S</em> (as obtained from an MCMC method):</p>&#13;
<div class="math-display">&#13;
<img src="../media/file151.jpg" class="math-display" alt=" ( ) âˆ‘n 1 âˆ‘S s âˆ‘ n ( S s) W AIC = âˆ’ 2 log S- p(yi | Î¸ ) + 2 Vs=1logp(yi | Î¸) i s=1 i "/>&#13;
</div>&#13;
<p>The first term is similar to the Akaike criterion, except it is evaluated for all the observations and all the samples of the posterior. The second term is a bit more difficult to justify without getting into technicalities. But it can be interpreted as the effective number of parameters. What is important from a practical point of view is that WAIC uses the entire posterior (and not a point estimate) for the calculation of both terms, so WAIC can be applied to virtually any Bayesian model.</p>&#13;
</section>&#13;
<section id="other-information-criteria" class="level5 likesubsubsectionHead" data-number="1.9.3.1.3">&#13;
<h4 class="likesubsubsectionHead" data-number="1.9.3.1.3"><span id="x1-1040001"/>Other information criteria</h4>&#13;
<p>Another widely used information criterion is the <strong>Deviance Information</strong> <strong>Criterion</strong> (<strong>DIC</strong>). If we use the <em>bayes-o-meter</em><sup>TM</sup>, DIC is more Bayesian than AIC but less than WAIC. Although still popular, WAIC and mainly LOO (see the next section) have been shown to be more useful both theoretically and empirically than DIC. Therefore, we do not recommend its use.</p>&#13;
<p>Another widely used criterion is <strong>Bayesian Information Criteria</strong> (<strong>BIC</strong>). Like logistic regression and my motherâ€™s <em>dry soup</em>, this name can be misleading. BIC was proposed as a way to correct some of the problems with AIC and the authors proposed a Bayesian justification for it. But BIC is not really Bayesian in the sense that, like AIC, it assumes flat priors and uses maximum likelihood estimation.</p>&#13;
<p>But more importantly, BIC differs from AIC and WAIC in its objective. AIC, WAIC, and LOO (see next section) try to reflect which model generalizes better to other data (predictive accuracy), while BIC tries to identify which is the <em>correct</em> model and therefore is more related to Bayes factors. <span id="x1-104001r226"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="cross-validation" class="level4 subsectionHead" data-number="1.9.3.2">&#13;
<h3 class="subsectionHead" data-number="1.9.3.2">5.3.2 <span id="x1-1050002"/>Cross-validation</h3>&#13;
<p>Cross-validation is a simple and, in <span id="dx1-105001"/>most cases, effective solution for comparing models. We take our data and divide it into K slices. We try to keep the slices more or less the same (in size and sometimes also in <span id="dx1-105002"/>other characteristics, such as the number of classes). We then use K-1 slices to train the model and slice to test it. This process is the systematically repeated omission, for each iteration, of a different slice from the training set and using that slice as the evaluation set. This is repeated until we have completed K fit-and-evaluation rounds, as can be seen in <em>Figure <a href="#x1-105004r7">5.7</a></em>. The accuracy of the model will be the average over the accuracy for each of the K rounds. This is <span id="dx1-105003"/>known as K-fold cross-validation. Finally, once we have performed cross-validation, we use all the data for one last fit and this is the model that is used to make predictions or for any other purpose.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file152.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-105004r7"/><strong>FigureÂ 5.7</strong>: K-fold cross-validation</p>&#13;
<p>When K equals the number of <span id="dx1-105005"/>data points, we get what is known as <strong>leave-one-out cross-validation</strong> (<strong>LOOCV</strong>), meaning we fit the model to all but one data point each time.</p>&#13;
<p>Cross-validation is a routine practice in machine learning, and we have barely described the most essential aspects of this practice. There are many other variants of the schema presented here. For more information, you can read <a href="Bibliography.xhtml#Xjames_2023">James etÂ al.</a>Â [<a href="Bibliography.xhtml#Xjames_2023">2023</a>] or <a href="Bibliography.xhtml#Xraschka_2022">Raschka etÂ al.</a>Â [<a href="Bibliography.xhtml#Xraschka_2022">2022</a>].</p>&#13;
<p>Cross-validation is a very simple and useful idea, but for some models or for large amounts of data, the computational cost of cross-validation <span id="dx1-105006"/>may be beyond our means. Many people have tried to find simpler <span id="dx1-105007"/>quantities to calculate, like Information Criteria. In the next section, we discuss a method to approximate cross-validation from a single fit to all the data.</p>&#13;
<section id="approximating-cross-validation" class="level5 likesubsubsectionHead" data-number="1.9.3.2.1">&#13;
<h4 class="likesubsubsectionHead" data-number="1.9.3.2.1"><span id="x1-1060002"/>Approximating cross-validation</h4>&#13;
<p>Cross-validation is a nice idea, but it <span id="dx1-106001"/>can be expensive, particularly variants like leave-one-out-cross-validation. Luckily, it is possible to approximate it using the information from a single fit to the data! The method for doing <span id="dx1-106002"/>this is called â€Pareto smooth importance sampling leave-one-out cross-validation.â€ The name is so long that in practice we call it LOO. Conceptually, what we are trying to calculate is:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file153.jpg" class="math-display" alt=" n âˆ« âˆ‘ ELPDLOO -CV = log p(yi | Î¸) p(Î¸ | yâˆ’i)dÎ¸ i=1 "/>&#13;
</div>&#13;
<p>This is the Expected Log-Pointwise-predictive Density (ELPD). We add the subscript <em>LOO-CV</em> to make it explicit we are computing the ELPD using leave-one-out cross-validation. The <sub><span class="cmsy-8">âˆ’</span><em>i</em></sub> means that we leave the observation <em>i</em> out.</p>&#13;
<p>This expression is very similar to the one for the posterior predictive distribution. The difference is that, now, we want to compute the posterior predictive distribution for observation <em>y</em><sub><em>i</em></sub> from a posterior distribution computed without the observation <em>y</em><sub><em>i</em></sub>. The first approximation we take is to prevent the explicit computation of the integral by taking samples from the posterior distribution. Thus, we can write:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file154.jpg" class="math-display" alt=" n ( s ) âˆ‘ ( 1-âˆ‘ j ) log S p(yi | Î¸âˆ’i) i j "/>&#13;
</div>&#13;
<p>Here, the sum is over <em>S</em> posterior samples. We have been using MCMC samples in this book a lot. So, this approximation should not sound unfamiliar to you. The tricky part comes next.</p>&#13;
<p>It is possible to approximate <img src="../media/Formula_03.PNG" style="width:4.5em; vertical-align: -0.20em;"/> using importance sampling. We are not going to discuss the details of that statistical method, but we are going to see how importance sampling is a way of approximating a target distribution by re-weighting values obtained from another distribution. This method is useful when we do not know how to sample from the target distribution but we know how to sample from another distribution. Importance sampling works best when the <span id="dx1-106003"/>known distribution is <em>wider</em> than the target one.</p>&#13;
<p>In our case, the known distribution, once a model has been fitted, is the log-likelihood for all the observations. And we want to approximate the log-likelihood if we had dropped one observation. For this, we need to estimate the â€importanceâ€ (or weight) that each observation has in determining the posterior distribution. The â€importanceâ€ of a given observation is proportional to the effect the variable will produce on the posterior if removed. Intuitively, a relatively unlikely observation is more important (or carries more weight) than an expected one. Luckily, these weights are easy to compute once we have computed the posterior distribution. In fact, the weight of the observation <em>i</em> for the <em>s</em> posterior sample is:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file155.jpg" class="math-display" alt=" 1 ws = -------- p(yi | Î¸s) "/>&#13;
</div>&#13;
<p>This <em>w</em><sub><em>s</em></sub> may not be reliable. The main issue is that sometimes a few <em>w</em><sub><em>s</em></sub> could be so large that they dominate our calculations, making them unstable. To tame these crazy weights, we can use Pareto smoothing. This solution consists of replacing some of these weights with weights obtained from fitting a Pareto distribution. Why a Pareto distribution? Because the theory indicates that the weights should follow this distribution.</p>&#13;
<p>So, for each observation, <em>y</em><sub><em>i</em></sub>, the largest weights are used to estimate a Pareto distribution, and that distribution is used to replace those weights with â€smoothedâ€ weights. This procedure gives robustness to the estimation of the ELPD and also provides a way to diagnose the approximation, i.e., to get a warning that the LOO method may be failing. For this, we need to pay attention to the values of <em>k</em>, which is a parameter of the Pareto distribution. Values of <em>k</em> greater than 0.7 <span id="dx1-106004"/>indicate that we may have very influential observations. <span id="x1-106005r225"/></p>&#13;
</section>&#13;
</section>&#13;
</section>&#13;
<section id="calculating-predictive-accuracy-with-arviz" class="level3 sectionHead" data-number="1.9.4">&#13;
<h2 class="sectionHead" data-number="1.9.4">5.4 <span id="x1-1070004"/>Calculating predictive accuracy with ArviZ</h2>&#13;
<p>Fortunately, calculating WAIC and LOO with ArviZ is very simple. We just need to be sure that the Inference Data has the log-likelihood group. When computing a posterior with PyMC, this can be achieved by doing <code>pm.sample(idata_kwargs="log_likelihood": True)</code>. Now, letâ€™s see how to compute LOO:</p>&#13;
<p><span id="x1-107001r3"/> <span id="x1-107002"/><strong>CodeÂ 5.3</strong></p>&#13;
<pre id="listing-60" class="source-code"><code>az.loo(idata_l)</code></pre>&#13;
<pre class="console"><code>&#13;
ComputedÂ fromÂ 8000Â posteriorÂ samplesÂ andÂ 33Â observationsÂ log-likelihoodÂ matrix.&#13;
&#13;
Â Â Â Â Â Â Â Â Â EstimateÂ Â Â Â Â Â Â SE elpd_looÂ Â Â -14.31Â Â Â Â Â 2.67&#13;
p_looÂ Â Â Â Â Â Â Â 2.40Â Â Â Â Â Â Â Â -&#13;
------&#13;
&#13;
ParetoÂ kÂ diagnosticÂ values:&#13;
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CountÂ Â Â Pct. (-Inf,Â 0.5]Â Â Â (good)Â Â Â Â Â Â Â 33Â Â 100.0%&#13;
Â (0.5,Â 0.7]Â Â Â (ok)Â Â Â Â Â Â Â Â Â Â 0Â Â Â Â 0.0%&#13;
Â Â Â (0.7,Â 1]Â Â Â (bad)Â Â Â Â Â Â Â Â Â 0Â Â Â Â 0.0%&#13;
Â Â Â (1,Â Inf)Â Â Â (veryÂ bad)Â Â Â Â 0Â Â Â Â 0.0%</code></pre>&#13;
<p>The output of <code>az.loo </code>has two sections. In the first section, we get a table with two rows. The first row is the ELPD (<code>elpd_loo</code>) and the second one is the effective number of parameters (<code>p_loo</code>). In the second section, we have the Pareto k diagnostic. This is a measure of the reliability of the LOO approximation. Values of k greater than 0.7 indicate that we possibly have very influential observations. In this case, we have 33 observations and all of them are good, so we can trust the approximation.</p>&#13;
<p>To compute WAIC, you can use <code>az.waic</code>; the output will be similar, except that we will not get the Pareto k diagnostic, or any similar diagnostics. This is a downside of WAIC: we do not get any information about the <span id="dx1-107004"/>reliability of the approximation.</p>&#13;
<p>If we compute LOO for the quadratic model, we will get a similar output, but the ELPD will be higher (around -4), indicating that the quadratic model is better.</p>&#13;
<p>Values of ELPD are not that useful <span id="dx1-107005"/>by themselves and must be interpreted in relation to other ELPD values. That is why ArviZ provides two helper functions to facilitate this comparison. Letâ€™s look at <code>az.compare </code>first:</p>&#13;
<p><span id="x1-107006r4"/> <span id="x1-107007"/><strong>CodeÂ 5.4</strong></p>&#13;
<pre id="listing-61" class="source-code"><code>cmp_df = az.compare({"model_l": idata_l, "model_q": idata_q})</code></pre>&#13;
<table id="TBL-8" class="tabular">&#13;
<tbody>&#13;
<tr id="TBL-8-1-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-8-1-1" class="td11" style="text-align: left; white-space: nowrap;"/>&#13;
<td id="TBL-8-1-2" class="td11" style="text-align: right; white-space: nowrap;"><strong>rank</strong></td>&#13;
<td id="TBL-8-1-3" class="td11" style="text-align: right; white-space: nowrap;"><strong>elpd_loo</strong></td>&#13;
<td id="TBL-8-1-4" class="td11" style="text-align: right; white-space: nowrap;"><strong>p_loo</strong></td>&#13;
<td id="TBL-8-1-5" class="td11" style="text-align: right; white-space: nowrap;"><strong>elpd_diff</strong></td>&#13;
<td id="TBL-8-1-6" class="td11" style="text-align: right; white-space: nowrap;"><strong>weight</strong></td>&#13;
<td id="TBL-8-1-7" class="td11" style="text-align: right; white-space: nowrap;"><strong>se</strong></td>&#13;
<td id="TBL-8-1-8" class="td11" style="text-align: right; white-space: nowrap;"><strong>dse</strong></td>&#13;
<td id="TBL-8-1-9" class="td11" style="text-align: left; white-space: nowrap;"><strong>warning</strong></td>&#13;
<td id="TBL-8-1-10" class="td11" style="text-align: left; white-space: nowrap;"><strong>scale</strong></td>&#13;
</tr>&#13;
<tr id="TBL-8-2-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-8-2-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>model_q</strong></td>&#13;
<td id="TBL-8-2-2" class="td11" style="text-align: right; white-space: nowrap;">0</td>&#13;
<td id="TBL-8-2-3" class="td11" style="text-align: right; white-space: nowrap;">-4.6</td>&#13;
<td id="TBL-8-2-4" class="td11" style="text-align: right; white-space: nowrap;">2.68</td>&#13;
<td id="TBL-8-2-5" class="td11" style="text-align: right; white-space: nowrap;">0</td>&#13;
<td id="TBL-8-2-6" class="td11" style="text-align: right; white-space: nowrap;">1</td>&#13;
<td id="TBL-8-2-7" class="td11" style="text-align: right; white-space: nowrap;">2.36</td>&#13;
<td id="TBL-8-2-8" class="td11" style="text-align: right; white-space: nowrap;">0</td>&#13;
<td id="TBL-8-2-9" class="td11" style="text-align: left; white-space: nowrap;">False</td>&#13;
<td id="TBL-8-2-10" class="td11" style="text-align: left; white-space: nowrap;">log</td>&#13;
</tr>&#13;
<tr id="TBL-8-3-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-8-3-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>model_l</strong></td>&#13;
<td id="TBL-8-3-2" class="td11" style="text-align: right; white-space: nowrap;">1</td>&#13;
<td id="TBL-8-3-3" class="td11" style="text-align: right; white-space: nowrap;">-14.3</td>&#13;
<td id="TBL-8-3-4" class="td11" style="text-align: right; white-space: nowrap;">2.42</td>&#13;
<td id="TBL-8-3-5" class="td11" style="text-align: right; white-space: nowrap;">9.74</td>&#13;
<td id="TBL-8-3-6" class="td11" style="text-align: right; white-space: nowrap;">3.0e-14</td>&#13;
<td id="TBL-8-3-7" class="td11" style="text-align: right; white-space: nowrap;">2.67</td>&#13;
<td id="TBL-8-3-8" class="td11" style="text-align: right; white-space: nowrap;">2.65</td>&#13;
<td id="TBL-8-3-9" class="td11" style="text-align: left; white-space: nowrap;">False</td>&#13;
<td id="TBL-8-3-10" class="td11" style="text-align: left; white-space: nowrap;">log</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p>In the rows, we have the compared models, and in the columns, we have:</p>&#13;
<ul>&#13;
<li><p><code>rank</code>: The order of the models (from best to worst).</p></li>&#13;
<li><p><code>elpd_loo</code>: The point estimate of the ELPD</p></li>&#13;
<li><p><code>p_loo</code>: The effective numbers parameters.</p></li>&#13;
<li><p><code>elpd_diff</code>: The difference between the ELPD of the best model and the other models.</p></li>&#13;
<li><p><code>weight</code>: The relative weight of each model. If we wanted to make predictions by combining the different models instead of choosing just one, this would be the weight that we should assign to each model. In this case, we see that the polynomial model takes all the weight.</p></li>&#13;
<li><p><code>se</code>: The standard error of the ELPD.</p></li>&#13;
<li><p><code>dse</code>: The standard error of the differences.</p></li>&#13;
<li><p><code>warning</code>: A warning about high k values.</p></li>&#13;
<li><p><code>scale</code>: The scale on which the ELPD is calculated.</p></li>&#13;
</ul>&#13;
<p>The other helper function provided by ArviZ is <code>az.compareplot</code>. This function provides similar information to <code>az.compare</code>, but graphically. <em>Figure <a href="#x1-107011r8">5.8</a></em> shows the output of this function. Notice that:</p>&#13;
<ul>&#13;
<li><p>The empty circles represent the ELPD values and the black lines are the standard error.</p></li>&#13;
<li><p>The highest value of the ELPD is indicated with a vertical dashed gray line to facilitate comparison with other values.</p></li>&#13;
<li><p>For all models except <em>the best</em>, we <span id="dx1-107009"/>also get a triangle indicating the value of the ELPD difference between each model and the <em>best</em> model. The gray error bar indicates the standard error of the differences <span id="dx1-107010"/>between the point estimates.</p></li>&#13;
</ul>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file156.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-107011r8"/><strong>FigureÂ 5.8</strong>: Output of <code>az.compareplot(cmp_df)</code></p>&#13;
<p>The easiest way to use LOO (or WAIC) is to choose a single model. Just choose the model with the highest ELPD value. If we follow this rule, we will have to accept that the quadratic model is the best. Even if we take into account the standard errors, we can see that they do not overlap. This gives us some certainty that indeed the models are <em>different enough</em> from each other. If instead, the standard errors overlap, we should provide a more nuanced answer. <span id="x1-107012r232"/></p>&#13;
</section>&#13;
<section id="model-averaging" class="level3 sectionHead" data-number="1.9.5">&#13;
<h2 class="sectionHead" data-number="1.9.5">5.5 <span id="x1-1080005"/>Model averaging</h2>&#13;
<p>Model selection is attractive for its simplicity, but we might be <span id="dx1-108001"/>missing information about uncertainty in our models. This is somewhat similar to calculating the full posterior and then just keeping the posterior mean; this can lead us to be overconfident about what we think we know.</p>&#13;
<p>An alternative is to select a single model but to report and analyze the different models together with the values of the calculated information criteria, their standard errors, and perhaps also the posterior predictive checks. It is important to put all these numbers and tests in the context of our problem so that we and our audience can get a better idea of the possible limitations and shortcomings of the models. For those working in academia, these elements can be used to add elements to the discussion section of a paper, presentation, thesis, etc. In industry, this can be useful for informing stakeholders about the advantages and limitations of models, predictions, and conclusions.</p>&#13;
<p>Another possibility is to average the models. In this way, we keep the uncertainty about the goodness of fit of each model. We then obtain a meta-model (and meta-predictions) using a weighted average of each model. ArviZ provides a <span id="dx1-108002"/>function for this task, <code>az.weight_predictions</code>, which takes as arguments a list of InferenceData objects and a list of weights. The weights can be calculated using the <code>az.compare </code>function. For example, if we want to average the two models we have been using, we can do the following:</p>&#13;
<p><span id="x1-108003r5"/> <span id="x1-108004"/><strong>CodeÂ 5.5</strong></p>&#13;
<pre id="listing-62" class="source-code"><code>idata_w = az.weight_predictions(idatas, weights=[0.35, 0.65])</code></pre>&#13;
<p><em>Figure <a href="#x1-108006r9">5.9</a></em> shows the results of this calculation. The light gray dashed line is the weighted average of the two models, the black solid line is the linear model, and the gray solid line is the quadratic one.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file157.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-108006r9"/><strong>FigureÂ 5.9</strong>: Weighted average of the linear and quadratic models</p>&#13;
<p>There are other ways to average models, such as explicitly building a meta-model that includes all models of interest as particular cases. For example, an order 2 polynomial contains a linear model as a particular case, or a hierarchical model is the continuous version between two extremes, a grouped model and an ungrouped model. <span id="x1-108007r237"/></p>&#13;
</section>&#13;
<section id="bayes-factors" class="level3 sectionHead" data-number="1.9.6">&#13;
<h2 class="sectionHead" data-number="1.9.6">5.6 <span id="x1-1090006"/>Bayes factors</h2>&#13;
<p>An alternative to LOO, cross-validation, and information criteria is Bayes factors. It is common for Bayes factors to show up in the <span id="dx1-109001"/>literature as a Bayesian alternative to frequentist hypothesis testing.</p>&#13;
<p>The <em>Bayesian way</em> of comparing <em>k</em> models is to <span id="dx1-109002"/>calculate the <strong>marginal</strong> <strong>likelihood</strong> of each model <em>p</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>M</em><sub><em>k</em></sub>), i.e., the probability of the observed data <em>Y</em> given the model <em>M</em><sub><em>k</em></sub>. The marginal likelihood is the normalization constant of Bayesâ€™ theorem. We can see this if we write Bayesâ€™ theorem and make explicit the fact that all inferences depend on the model.</p>&#13;
<div class="math-display">&#13;
<img src="../media/file158.jpg" class="math-display" alt="p(Î¸ | Y,Mk ) = p(Y-| Î¸,Mk-)p(Î¸-| Mk-) p(Y | Mk ) "/>&#13;
</div>&#13;
<p>where, <em>y</em> is the data, <em>Î¸</em> is the parameters, and <em>M</em><sub><em>k</em></sub> is a model out of <em>k</em> competing models.</p>&#13;
<p>If our main objective is to choose only one model, the <em>best</em> from a set of models, we can choose the one with the largest value of <em>p</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>M</em><sub><em>k</em></sub>). This is fine if we assume that all models have the same prior probability. Otherwise, we must calculate:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file159.jpg" class="math-display" alt="p(Mk | y) âˆ p(y | Mk )p(Mk ) "/>&#13;
</div>&#13;
<p>If, instead, our main objective is to compare models to determine which are more likely and to what extent, this can be achieved using the Bayes factors:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file160.jpg" class="math-display" alt="BF01 = p(y | M0-) p(y | M1 ) "/>&#13;
</div>&#13;
<p>That is the ratio between the marginal <span id="dx1-109003"/>likelihood of two models. The higher the value of <em>BF</em><sub>01</sub>, the <em>better</em> the model in the numerator (<em>M</em><sub>0</sub> in this example). To facilitate the interpretation of the Bayes factors, and to put numbers into words, Harold Jeffreys proposed a scale for their interpretation, with levels of <em>support</em> or <em>strength</em> (see <em>Table <a href="#x1-109004r1">5.1</a></em>).</p>&#13;
<table id="TBL-9" class="tabular">&#13;
<tbody>&#13;
<tr id="TBL-9-1-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-9-1-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>Bayes Factor</strong></td>&#13;
<td id="TBL-9-1-2" class="td11" style="text-align: center; white-space: nowrap;"><strong>Support</strong></td>&#13;
</tr>&#13;
<tr id="TBL-9-2-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-9-2-1" class="td11" style="text-align: left; white-space: nowrap;">1â€“3</td>&#13;
<td id="TBL-9-2-2" class="td11" style="text-align: center; white-space: nowrap;">Anecdotal</td>&#13;
</tr>&#13;
<tr id="TBL-9-3-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-9-3-1" class="td11" style="text-align: left; white-space: nowrap;">3â€“10</td>&#13;
<td id="TBL-9-3-2" class="td11" style="text-align: center; white-space: nowrap;">Moderate</td>&#13;
</tr>&#13;
<tr id="TBL-9-4-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-9-4-1" class="td11" style="text-align: left; white-space: nowrap;">10â€“30</td>&#13;
<td id="TBL-9-4-2" class="td11" style="text-align: center; white-space: nowrap;">Strong</td>&#13;
</tr>&#13;
<tr id="TBL-9-5-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-9-5-1" class="td11" style="text-align: left; white-space: nowrap;">30â€“100</td>&#13;
<td id="TBL-9-5-2" class="td11" style="text-align: center; white-space: nowrap;">Very Strong</td>&#13;
</tr>&#13;
<tr id="TBL-9-6-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-9-6-1" class="td11" style="text-align: left; white-space: nowrap;"><em>&gt;</em>100</td>&#13;
<td id="TBL-9-6-2" class="td11" style="text-align: center; white-space: nowrap;">Extreme</td>&#13;
</tr>&#13;
&#13;
</tbody>&#13;
</table>&#13;
<p class="IMG---Caption"><span id="x1-109004r1"/> <span id="x1-109005"/><strong>TableÂ 5.1</strong>: Support for model <em>M</em><sub>0</sub>, the one in the numerator</p>&#13;
<p>Keep in mind that if you get numbers below 1, then the support is for <em>M</em><sub>1</sub>, i.e., the model in the denominator. Tables are also available for those cases, but notice that you can simply take the inverse of the obtained value.</p>&#13;
<p>It is very important to remember that these rules are just conventions â€“ simple guides at best. Results should always be put in the context of our problems and should be accompanied by enough detail so that others can assess for themselves whether they agree with our conclusions. The proof necessary to ensure something in particle physics, or in court, or to decide to carry out an evacuation in the <span id="dx1-109006"/>face of a looming natural catastrophe is not the same. <span id="x1-109007r230"/></p>&#13;
<section id="some-observations" class="level4 subsectionHead" data-number="1.9.6.1">&#13;
<h3 class="subsectionHead" data-number="1.9.6.1">5.6.1 <span id="x1-1100001"/>Some observations</h3>&#13;
<p>We will now briefly discuss some key facts about the marginal likelihood:</p>&#13;
<ul>&#13;
<li><p>The good: Occamâ€™s razor included. Models with lots of parameters have a higher penalty than models with few parameters. The intuitive reason is that the greater the number of parameters, the more the prior <em>extends</em> with respect to the likelihood. An example where it is easy to see this is with nested models: for example, a polynomial of order 2 â€containsâ€ the models polynomial of order 1 and polynomial of order 0.</p></li>&#13;
<li><p>The bad: For many problems, the marginal likelihood cannot be calculated analytically. Also, approximating it numerically is usually a difficult task that in the best of cases requires specialized methods and, in the worst case, the estimates are either impractical or unreliable. In fact, the popularity of the MCMC methods is that they allow obtaining the posterior distribution without the need to calculate the marginal likelihood.</p></li>&#13;
<li><p>The ugly: The marginal likelihood depends <em>very sensitively</em> on the prior distribution of the parameters in each model <em>p</em>(<em>Î¸</em><sub><em>k</em></sub><span class="cmsy-10x-x-109">|</span><em>M</em><sub><em>k</em></sub>).</p></li>&#13;
</ul>&#13;
<p>It is important to note that the <em>good</em> and the <em>ugly</em> points are related. Using marginal likelihood to compare models is a good idea because it already includes a penalty for complex models (which helps us prevent overfitting), and at the same time, a change in the prior will affect the marginal likelihood calculations. At first, this sounds a bit silly; we already know that priors affect calculations (otherwise we could just avoid them). But we are talking about changes in the prior that would have a small effect in the posterior but a great impact on the value of the marginal likelihood.</p>&#13;
<p>The use of Bayes factors is often a watershed among Bayesians. The difficulty of its calculation and the sensitivity to the priors are some of the arguments against it. Another reason is that, like p-values and hypothesis testing in general, Bayes factors favor dichotomous thinking over the estimation of the â€effect size.â€ In other words, instead of asking ourselves questions like: How many more years of life can a cancer treatment provide? We end up asking if the difference between treating and not treating a patient is â€statistically significant.â€ Note that this last question can be useful in some contexts. The point is that in many other contexts, this type of question is not the question that interests us; weâ€™re only interested in the one that we were taught to answer. <span id="x1-110001r242"/></p>&#13;
</section>&#13;
<section id="calculation-of-bayes-factors" class="level4 subsectionHead" data-number="1.9.6.2">&#13;
<h3 class="subsectionHead" data-number="1.9.6.2">5.6.2 <span id="x1-1110002"/>Calculation of Bayes factors</h3>&#13;
<p>As we have already mentioned, marginal likelihood (and the Bayes factors derived from it) is generally not available in closed form, except for some models. For this reason, many numerical methods have been devised for its calculation. Some of these methods are so simple and naive ( <a href="https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever" class="url">https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever</a>) that they work very poorly in practice.</p>&#13;
<section id="analytically" class="level5 likesubsubsectionHead" data-number="1.9.6.2.1">&#13;
<h4 class="likesubsubsectionHead" data-number="1.9.6.2.1"><span id="x1-1120002"/>Analytically</h4>&#13;
<p>For some models, such as the <span id="dx1-112001"/>BetaBinomial model, we can calculate the marginal likelihood analytically. If we write this model as:</p>&#13;
<table style="border:none;">&#13;
<tbody>&#13;
<tr class="odd">&#13;
<td class="align-label" style="border:none;"/>&#13;
<td class="align-odd" style="border:none;"><em>Î¸</em> <span class="cmsy-10x-x-109">âˆ¼ </span><em>Beta</em>(<em><em>Î±</em>,<em>Î²</em></em>)</td>&#13;
<td class="align-label" style="border:none;"/>&#13;
</tr>&#13;
<tr class="even">&#13;
<td class="align-label" style="border:none;"/>&#13;
<td class="align-odd" style="border:none;"><em>y</em> <span class="cmsy-10x-x-109">âˆ¼ </span><em>Bin</em>(<em>n</em> = 1<em>,p</em> = <em>Î¸</em>)</td>&#13;
<td class="align-label" style="border:none;"/>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p>then the marginal likelihood will be:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file161.jpg" class="math-display" alt=" (n) B (ğ›¼+ h, ğ›½ + n âˆ’ h) p(y) = ------------------- h B (ğ›¼,ğ›½) "/>&#13;
</div>&#13;
<p><em>B</em> is the beta function (not to be confused with the <em>Beta</em> distribution), <em>n</em> is the number of attempts, and <em>h</em> is the success number.</p>&#13;
<p>Since we only care about the relative value of the marginal likelihood under two different models (for the same data), we can omit the binomial coefficient <img src="../media/file162.jpg" class="frac" alt="(n) h"/>, so we can write:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file163.jpg" class="math-display" alt=" B (ğ›¼ + h, ğ›½ + n âˆ’ h) p(y) âˆ ------B-(ğ›¼,-ğ›½)------ "/>&#13;
</div>&#13;
<p>This expression has been coded in the next code block but with a twist. We will use the <code>betaln </code>function, which returns the natural logarithm of the <code>beta </code>function, it is common in statistics to do calculations on a</p>&#13;
<p><span id="dx1-112002"/>logarithmic scale. This reduces numerical problems when working with probabilities.</p>&#13;
<p><span id="x1-112003r6"/> <span id="x1-112004"/><strong>CodeÂ 5.6</strong></p>&#13;
<pre id="listing-63" class="source-code"><code>from scipy.special import betalnÂ </code>&#13;
<code>Â </code>&#13;
<code>def beta_binom(prior, y):Â </code>&#13;
<code>Â Â Â Â """Â </code>&#13;
<code>Â Â Â Â Calculate the marginal probability, analytically, for a BetaBinomial model.Â </code>&#13;
<code>Â Â Â Â prior : tupleÂ </code>&#13;
<code>Â Â Â Â Â Â alpha and beta parameters for the beta priorÂ </code>&#13;
<code>Â Â Â Â y : arrayÂ </code>&#13;
<code>Â Â Â Â Â Â array with "1" and "0" corresponding to success and failure respectivelyÂ </code>&#13;
<code>Â Â Â Â """Â </code>&#13;
<code>Â Â Â Â alpha, beta = priorÂ </code>&#13;
<code>Â Â Â Â h = np.sum(y)Â </code>&#13;
<code>Â Â Â Â n = len(y)Â </code>&#13;
<code>Â Â Â Â p_y = np.exp(betaln(alpha + h, beta + n - h) - betaln(alpha, beta))Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â return p_y</code></pre>&#13;
<p>Our data for this example consists of 100 coin tosses and the same number of heads and tails. We will compare two models, one with a Uniform prior and one with a <em>more concentrated</em> prior around <em>Î¸</em> = 0<em>.</em>5:</p>&#13;
<p><span id="x1-112021r7"/> <span id="x1-112022"/><strong>CodeÂ 5.7</strong></p>&#13;
<pre id="listing-64" class="source-code"><code>y = np.repeat([1, 0], [50, 50]) Â # 50 heads, 50 tailsÂ </code>&#13;
<code>priors = ((1, 1), (30, 30)) Â # uniform prior, peaked prior</code></pre>&#13;
<p><em>Figure <a href="#x1-112026r10">5.10</a></em> shows the two priors. The <span id="dx1-112025"/>Uniform prior is the black line, and the peaked prior is the gray line.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file164.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-112026r10"/><strong>FigureÂ 5.10</strong>: Uniform and peaked priors</p>&#13;
<p>Now, we can calculate the marginal likelihood for each model and the Bayes factor, which turns out to be 5:</p>&#13;
<p><span id="x1-112027r8"/> <span id="x1-112028"/><strong>CodeÂ 5.8</strong></p>&#13;
<pre id="listing-65" class="source-code"><code>BF = beta_binom(priors[1], y) / beta_binom(priors[0], y)Â </code>&#13;
<code>print(round(BF))</code></pre>&#13;
<pre class="console"><code> 5</code></pre>&#13;
<p>We see that the model with the prior beta(30<em>,</em>30), more concentrated, has <span class="cmsy-10x-x-109">â‰ˆ </span>5 times more support than the model with the beta(1<em>,</em>1). This is to be expected since the prior for the first case is concentrated around <em>Î¸</em> = 0<em>.</em>5 and the data <em>Y</em> have the same number of heads and tails, that is, they agree with a value of <em>Î¸</em> around 0.5.</p>&#13;
</section>&#13;
<section id="sequential-monte-carlo" class="level5 likesubsubsectionHead" data-number="1.9.6.2.2">&#13;
<h4 class="likesubsubsectionHead" data-number="1.9.6.2.2"><span id="x1-1130002"/>Sequential Monte Carlo</h4>&#13;
<p><span id="dx1-113001"/> <span id="dx1-113002"/></p>&#13;
<p>The <strong>Sequential Monte Carlo</strong> (<strong>SMC</strong>) method is a sampling method that works by progressing through a series of successive stages that bridge one distribution that is easy to sample from and the posterior of interest. In practice, the starting distribution is usually the prior. A byproduct of the SMC sampler is the estimate of the marginal likelihood.</p>&#13;
<p><span id="x1-113003r9"/> <span id="x1-113004"/><strong>CodeÂ 5.9</strong></p>&#13;
<pre id="listing-66" class="source-code"><code>models = []Â </code>&#13;
<code>idatas = []Â </code>&#13;
<code>for alpha, beta in priors:Â </code>&#13;
<code>Â Â Â Â with pm.Model() as model:Â </code>&#13;
<code>Â Â Â Â Â Â Â Â a = pm.Beta("a", alpha, beta)Â </code>&#13;
<code>Â Â Â Â Â Â Â Â yl = pm.Bernoulli("yl", a, observed=y)Â </code>&#13;
<code>Â Â Â Â Â Â Â Â idata = pm.sample_smc(random_seed=42)Â </code>&#13;
<code>Â Â Â Â Â Â Â Â models.append(model)Â </code>&#13;
<code>Â Â Â Â Â Â Â Â idatas.append(idata)Â </code>&#13;
<code>Â </code>&#13;
<code>BF_smc = np.exp(Â </code>&#13;
<code>Â Â Â Â idatas[1].sample_stats["log_marginal_likelihood"].mean()Â </code>&#13;
<code>Â Â Â Â - idatas[0].sample_stats["log_marginal_likelihood"].mean()Â </code>&#13;
<code>)Â </code>&#13;
<code>print(np.round(BF_smc).item())</code></pre>&#13;
<pre class="console"><code> 5.0</code></pre>&#13;
<p>As we can see from the preceding code block, SMC also gives us a Bayes factor of 5, the same answer as the analytical calculation! The advantage of using SMC to calculate marginal likelihood is that we can use it for a wider range of models since we no longer need to know an expression in closed form. The price we pay for this flexibility is a higher computational cost. Also, keep in mind that SMC (with an independent Metropolis-Hastings kernel, as implemented in PyMC) is not as efficient as NUTS. As the dimensionality of the problem increases, a more precise estimate of the posterior and the marginal likelihood will require a larger number of samples of the posterior.</p>&#13;
<div id="tcolobox-12" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Log Space</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>In computational statistics, we usually <span id="dx1-113020"/>perform computations in log space. This helps provide numerical stability and computational efficiency, among other things. See, for example, the preceding code block; you can see that we calculated a difference (instead of a division) and then we took the exponential before returning the result.</p>&#13;
</div>&#13;
</div>&#13;
</section>&#13;
<section id="savagedickey-ratio" class="level5 likesubsubsectionHead" data-number="1.9.6.2.3">&#13;
<h4 class="likesubsubsectionHead" data-number="1.9.6.2.3"><span id="x1-1140002"/>Savageâ€“Dickey ratio</h4>&#13;
<p><span id="dx1-114001"/> <span id="dx1-114002"/></p>&#13;
<p>For the above examples, we have compared two BetaBinomial models. We could have compared two completely different models, but there are times when we want to compare a null hypothesis <code>H_0 </code>(or null model) against an alternative <em>H_1</em> hypothesis. For example, to answer the question â€Is this coin biased?â€, we could compare the value <em>Î¸</em> = 0<em>.</em>5 (representing no bias) with the output of a model in which we allow <em>Î¸</em> to vary. For this type of comparison, the null model is nested within the alternative, which means that the null is a particular value of the model we are building. In those cases, calculating the Bayes factor is very easy and does not require any special methods. We only need to compare the prior and posterior evaluated at the null value (for example, <em>Î¸</em> = 0<em>.</em>5) under the alternative model. We can see that this is true from the following expression:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file165.jpg" class="math-display" alt=" p(y | H )p(Î¸ = 0.5 | y, H ) BF01 = ------0---------------1- p(y | H1 ) p(Î¸ = 0.5 | H1 ) "/>&#13;
</div>&#13;
<p>This is true only when <em>H</em><sub>0</sub> is a particular case of <em>H</em><sub>1</sub>, (<a href="https://statproofbook.github.io/P/bf-sddr" class="url">https://statproofbook.github.io/P/bf-sddr</a>). Next, letâ€™s do it with PyMC and ArviZ. We only need to sample the prior and posterior for a model. Letâ€™s try the BetaBinomial model with a Uniform prior:</p>&#13;
<p><span id="x1-114003r10"/> <span id="x1-114004"/><strong>CodeÂ 5.10</strong></p>&#13;
<pre id="listing-67" class="source-code"><code>with pm.Model() as model_uni:Â </code>&#13;
<code>Â Â Â Â a = pm.Beta("a", 1, 1)Â </code>&#13;
<code>Â Â Â Â yl = pm.Bernoulli("yl", a, observed=y)Â </code>&#13;
<code>Â Â Â Â idata_uni = pm.sample(2000, random_seed=42)Â </code>&#13;
<code>Â Â Â Â idata_uni.extend(pm.sample_prior_predictive(8000))Â </code>&#13;
<code>Â </code>&#13;
<code>az.plot_bf(idata_uni, var_name="a", ref_val=0.5)</code></pre>&#13;
<p>The result is shown in <em>Figure <a href="#x1-114012r11">5.11</a></em>. We can see one KDE for the prior (black) and one for the posterior (gray). The two black dots show that we evaluated both distributions at the value 0.5. We can see that the Bayes factor in favor of the null hypothesis, <code>BF_01</code>, is <span class="cmsy-10x-x-109">â‰ˆ </span>8, which we can interpret as <em>moderate evidence</em> in favor of the null hypothesis (see <em>Table <a href="#x1-109004r1">5.1</a></em>).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file166.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-114012r11"/><strong>FigureÂ 5.11</strong>: Bayes factor for the BetaBinomial model with Uniform prior</p>&#13;
<p>As we have already discussed, the Bayes factors measure which model, as a whole, is better at explaining the data. This includes the prior, even if the prior has a relatively low impact on the computation of the posterior. We can also see this prior effect by comparing a second model to the null model.</p>&#13;
<p>If, instead, our model were a BetaBinomial with a Beta prior (30, 30), the <code>BF_01</code> would be lower (<em>anecdotal</em> on the Jeffrey scale). This is because, according to this model, the value of <em>Î¸</em> = 0<em>.</em>5 is much more likely a priori than for a Uniform prior, and therefore the prior and posterior will be much more similar. That is, it is not very <em>surprising</em> to see that the posterior is concentrated around 0.5 after collecting data. Donâ€™t just believe me, letâ€™s calculate it:</p>&#13;
<p><span id="x1-114013r11"/> <span id="x1-114014"/><strong>CodeÂ 5.11</strong></p>&#13;
<pre id="listing-68" class="source-code"><code>with pm.Model() as model_conc:Â </code>&#13;
<code>Â Â Â Â a = pm.Beta("a", 30, 30)Â </code>&#13;
<code>Â Â Â Â yl = pm.Bernoulli("yl", a, observed=y)Â </code>&#13;
<code>Â Â Â Â idata_conc = pm.sample(2000, random_seed=42)Â </code>&#13;
<code>Â Â Â Â idata_conc.extend(pm.sample_prior_predictive(8000))Â </code>&#13;
<code>Â </code>&#13;
<code>az.plot_bf(idata_conc, var_name="a", ref_val=0.5)</code></pre>&#13;
<p><em>Figure <a href="#x1-114022r12">5.12</a></em> shows the result. We can see that the <code>BF_01 </code>is <span class="cmsy-10x-x-109">â‰ˆ </span>1<em>.</em>6, which we can interpret as <em>anecdotal evidence</em> in favor of the null hypothesis (see the Jeffreysâ€™ scale, discussed earlier).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file167.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-114022r12"/><strong>FigureÂ 5.12</strong>: Bayes factor for the BetaBinomial model with peaked prior</p>&#13;
<p><span id="x1-114023r240"/></p>&#13;
</section>&#13;
</section>&#13;
</section>&#13;
<section id="bayes-factors-and-inference" class="level3 sectionHead" data-number="1.9.7">&#13;
<h2 class="sectionHead" data-number="1.9.7">5.7 <span id="x1-1150007"/>Bayes factors and inference</h2>&#13;
<p>So far, we have used Bayes <span id="dx1-115001"/>factors to judge which model seems to be better at explaining the data, and we found that one of the models is <span class="cmsy-10x-x-109">â‰ˆ </span>5 times <em>better</em> than the other.</p>&#13;
<p>But what about the posterior we get from these models? How different are they? <em>Table <a href="#x1-115002r2">5.2</a></em> summarizes these two posteriors:</p>&#13;
<table id="TBL-10" class="tabular">&#13;
<tbody>&#13;
<tr id="TBL-10-1-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-10-1-1" class="td11" style="text-align: left; white-space: nowrap;"/>&#13;
<td id="TBL-10-1-2" class="td11" style="text-align: right; white-space: nowrap;"><strong>mean</strong></td>&#13;
<td id="TBL-10-1-3" class="td11" style="text-align: right; white-space: nowrap;"><strong>sd</strong></td>&#13;
<td id="TBL-10-1-4" class="td11" style="text-align: right; white-space: nowrap;"><strong>hdi_3%</strong></td>&#13;
<td id="TBL-10-1-5" class="td11" style="text-align: right; white-space: nowrap;"><strong>hdi_97%</strong></td>&#13;
</tr>&#13;
<tr id="TBL-10-2-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-10-2-1" class="td 11" style="text-align: left; white-space: nowrap;"><strong>uniform</strong></td>&#13;
<td id="TBL-10-2-2" class="td11" style="text-align: right; white-space: nowrap;">0.5</td>&#13;
<td id="TBL-10-2-3" class="td11" style="text-align: right; white-space: nowrap;">0.05</td>&#13;
<td id="TBL-10-2-4" class="td11" style="text-align: right; white-space: nowrap;">0.4</td>&#13;
<td id="TBL-10-2-5" class="td11" style="text-align: right; white-space: nowrap;">0.59</td>&#13;
</tr>&#13;
<tr id="TBL-10-3-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-10-3-1" class="td11" style="text-align: left; white-space: nowrap;"><strong>peaked</strong></td>&#13;
<td id="TBL-10-3-2" class="td11" style="text-align: right; white-space: nowrap;">0.5</td>&#13;
<td id="TBL-10-3-3" class="td11" style="text-align: right; white-space: nowrap;">0.04</td>&#13;
<td id="TBL-10-3-4" class="td11" style="text-align: right; white-space: nowrap;">0.42</td>&#13;
<td id="TBL-10-3-5" class="td11" style="text-align: right; white-space: nowrap;">0.57</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="IMG---Caption"><span id="x1-115002r2"/> <span id="x1-115003"/><strong>TableÂ 5.2</strong>: Statistics for the models with uniform and peaked priors computed using the ArviZ summary function</p>&#13;
<p>We can argue that the results are quite similar; we have the same mean value for <em>Î¸</em> and a slightly wider posterior for <code>model_0</code>, as expected since this model has a wider prior. We can also check the posterior predictive distribution to see how similar they are (see <em>Figure <a href="#x1-115004r13">5.13</a></em>).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file168.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-115004r13"/><strong>FigureÂ 5.13</strong>: Posterior predictive distributions for models with uniform and peaked priors</p>&#13;
<p>In this example, the observed data is <span id="dx1-115005"/>more consistent with <code>model_1</code>, because the prior is concentrated around the correct value of <em>Î¸</em>, while <code>model_0</code>, assigns the same probability to all possible values of <em>Î¸</em>. This difference between the models is captured by the Bayes factor. We could say that the Bayes factors measure which model, as a whole, is better for explaining the data. This includes the <span id="dx1-115006"/>details of the prior, no matter how similar the model predictions are. In many scenarios, this is not what interests us when comparing models, and instead, we prefer to evaluate models in terms of how similar their predictions are. For those cases, we can use LOO. <span id="x1-115007r259"/></p>&#13;
</section>&#13;
<section id="regularizing-priors" class="level3 sectionHead" data-number="1.9.8">&#13;
<h2 class="sectionHead" data-number="1.9.8">5.8 <span id="x1-1160008"/>Regularizing priors</h2>&#13;
<p>Using informative and weakly informative priors is a way of introducing bias in a model and, if done properly, this can be <span id="dx1-116001"/>really good because bias prevents overfitting and thus contributes to models being able to make predictions that generalize well. This idea of adding a bias element to reduce generalization errors without affecting the ability of the model to adequately model a problem is <span id="dx1-116002"/>known as <strong>regularization</strong>. This regularization often takes the form of a term penalizing certain values for the parameters in a model, like too-big coefficients in a regression model. Restricting parameter values is a way of reducing the data a model can represent, thus reducing the chances that a model will capture noise instead of the signal.</p>&#13;
<p>This regularization idea is so powerful and useful that it has been discovered several times, including outside the Bayesian framework. For regression models, and outside Bayesian statistics, two popular regularization methods are ridge regression and lasso regression. From the Bayesian point of view, ridge regression can be interpreted as using Normal distributions for the <em>Î²</em> coefficients of a linear model, with a small standard deviation that pushes the coefficients toward zero. In this sense, we have been doing something very close to ridge regression for every single linear model in this book (except the examples in this chapter that use SciPy!).</p>&#13;
<p>On the other hand, lasso regression can be interpreted from a Bayesian point of view as the MAP of the posterior computed from a model with Laplace priors for the <em>Î²</em> coefficients. The Laplace distribution looks similar to the <span id="dx1-116003"/>Gaussian distribution but with a sharp peak at zero. You can also interpret it as two <em>back-to-back</em> Exponential distributions (try <code>pz.Laplace(0, 1).plot_pdf()</code>). The Laplace distribution concentrates its probability mass much closer to zero compared to the Gaussian distribution. The idea of using such a prior is to provide both regularization and variable selection. The idea is that since we have this peak at zero, we expect the prior distribution to induce sparsity, that is, we create a model with a lot of parameters and the prior will automatically make most of them zero, keeping only the relevant variables contributing to the output of the model.</p>&#13;
<p>Unfortunately, contrary to ridge regression, this idea does not directly translate from the frequentist realm to the Bayesian one. Nevertheless, there are Bayesian priors that can be used for inducing sparsity and performing variable selection, like the horseshoe prior. If you want to learn more about the horseshoe and other shrinkage priors, you may find the article by <a href="Bibliography.xhtml#XPiironen2017">Piironen and Vehtari</a>Â [<a href="Bibliography.xhtml#XPiironen2017">2017</a>] at <a href="https://arxiv.org/abs/1707.01694" class="url">https://arxiv.org/abs/1707.01694</a> very interesting. In the next chapter, we will discuss more about variable selection. Just one final note: it is important to notice that the classical versions of ridge and lasso regressions correspond to single-point estimates, while the Bayesian versions yield full posterior distributions. <span id="x1-116004r261"/></p>&#13;
</section>&#13;
<section id="summary-4" class="level3 sectionHead" data-number="1.9.9">&#13;
<h2 class="sectionHead" data-number="1.9.9">5.9 <span id="x1-1170009"/>Summary</h2>&#13;
<p>In this chapter, we have seen how to compare models using posterior predictive checks, information criteria, approximated cross-validation, and Bayes factors.</p>&#13;
<p>Posterior predictive check is a general concept and practice that can help us understand how well models are capturing different aspects of the data. We can perform posterior predictive checks with just one model or with many models, and thus we can use it as a method for model comparison. Posterior predictive checks are generally done via visualizations, but numerical summaries like Bayesian values can also be helpful.</p>&#13;
<p>Good models have a good balance between complexity and predictive accuracy. We exemplified this feature by using the classical example of polynomial regression. We discussed two methods to estimate the out-of-sample accuracy without leaving data aside: cross-validation and information criteria. From a practical point of view, information criteria is a family of theoretical methods looking to balance two contributions: a measurement of how well a model fits the data and a penalization term for complex models. We briefly discussed AIC, for its historical importance, and then WAIC, which is a better method for Bayesian models as it takes into account the entire posterior distribution and uses a more sophisticated method to compute the effective number of parameters.</p>&#13;
<p>We also discussed cross-validation, and we saw we can approximate leave-one-out cross-validation using LOO. Both WAIC and LOO tend to produce very similar results, but LOO can be more reliable. So we recommend its use. Both WAIC and LOO can be used for model selection and model averaging. Instead of selecting a single best model, model averaging is about combining all available models by taking a weighted average of them.</p>&#13;
<p>A different approach to model selection, comparison, and model averaging is Bayes factors, which are the ratio of the marginal likelihoods of two models. Bayes factor computations can be really challenging. In this chapter, we showed two routes to compute them with PyMC and ArviZ: using the sampling method known as Sequential Monte Carlo and using the Savageâ€“Dickey ratio. The first method can be used for any model as long as Sequential Monte Carlo provides a good posterior. With the current implementation of SMC in PyMC, this can be challenging for high-dimensional models or hierarchical models. The second method can only be used when the null model is a particular case of the alternative model. Besides being computationally challenging, Bayes factors are problematic to use given that they are very (overly) sensitive to prior specifications.</p>&#13;
<p>We have shown that Bayes factors and LOO/WAIC are the answers to two related but different questions. The former is focused on identifying the right model and the other is on identifying the model with lower generalization loss, i.e., the model making the best predictions. None of these methods are free of problems, but WAIC, and in particular LOO, are much more robust than the others in practice. <span id="x1-117001r262"/></p>&#13;
</section>&#13;
<section id="exercises-4" class="level3 sectionHead" data-number="1.9.10">&#13;
<h2 class="sectionHead" data-number="1.9.10">5.10 <span id="x1-11800010"/>Exercises</h2>&#13;
<ol>&#13;
<li><div id="x1-118002x1">&#13;
<p>This exercise is about regularization priors. In the code that generates the <code>x_c, y_c </code>data (see <a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>), change <code>order=2 </code>to another value, such as <code>order=5</code>. Then, fit <code>model_q</code> and plot the resulting curve. Repeat this, but now using a prior for <em>Î²</em> with <code>sd=100 </code>instead of <code>sd=1 </code>and plot the resulting curve. How do the curves differ? Try this out with <code>sd=np.array([10, 0.1, 0.1, 0.1, 0.1])</code>, too.</p>&#13;
</div></li>&#13;
<li><div id="x1-118004x2">&#13;
<p>Repeat the previous exercise but increase the amount of data to 500 data points.</p>&#13;
</div></li>&#13;
<li><div id="x1-118006x3">&#13;
<p>Fit a cubic model (order 3), compute WAIC and LOO, plot the results, and compare them with the linear and quadratic models.</p>&#13;
</div></li>&#13;
<li><div id="x1-118008x4">&#13;
<p>Use <code>pm.sample_posterior_predictive() </code>to rerun the PPC example, but this time, plot the values of <code>y </code>instead of the values of the mean.</p>&#13;
</div></li>&#13;
<li><div id="x1-118010x5">&#13;
<p>Read and run the posterior predictive example from PyMCâ€™s documentation at <a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html" class="url">https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html</a>. Pay special attention to the use of shared variables and <code>pm.MutableData</code>.</p>&#13;
</div></li>&#13;
<li><div id="x1-118012x6">&#13;
<p>Go back to the code that generated <em>Figure <a href="#x1-98002r5">5.5</a></em> and <em>Figure <a href="#x1-98004r6">5.6</a></em> and modify it to get new sets of six data points. Visually evaluate how the different polynomials fit these new datasets. Relate the results to the discussions in this book.</p>&#13;
</div></li>&#13;
<li><div id="x1-118014x7">&#13;
<p>Read and run the model averaging example from PyMCâ€™s documentation at <a href="https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html" class="url">https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html</a>.</p>&#13;
</div></li>&#13;
<li><div id="x1-118016x8">&#13;
<p>Compute the Bayes factor for the coin problem using a uniform prior, Beta(1, 1), and priors such as Beta(0.5, 0.5). Set 15 heads and 30 coins. Compare this result with the inference we got in the first chapter of this book.</p>&#13;
</div></li>&#13;
<li><div id="x1-118018x9">&#13;
<p>Repeat the last example where we compare Bayes factors and Information Criteria, but now reduce the sample size.</p>&#13;
</div></li>&#13;
</ol>&#13;
</section>&#13;
<section id="join-our-community-discord-space-5" class="level3 likesectionHead" data-number="1.9.11">&#13;
<h2 class="likesectionHead" data-number="1.9.11"><span id="x1-11900010"/>Join our community Discord space</h2>&#13;
<p>Join our Discord community to meet like-minded people and learn alongside more than 5000 members at: <a href="https://packt.link/bayesian">https://packt.link/bayesian</a></p>&#13;
<p><img src="../media/file1.png" alt="PIC"/> <span id="x1-119001r216"/></p>&#13;
</section>&#13;
</section>&#13;
</body></html>
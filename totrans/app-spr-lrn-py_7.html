<html><head></head><body>
		<div id="_idContainer375" class="Content">
			<h1 id="_idParaDest-172"><em class="italics"><a id="_idTextAnchor192"/>Appendix</em></h1>
		</div>
		<div>
			<div id="_idContainer376" class="Content">
			</div>
		</div>
		<div id="_idContainer377" class="Content">
			<h2>About</h2>
			<p>This section is included to assist the students to perform the activities in the book. It includes detailed steps that are to be performed by the students to achieve the objectives of the activities.</p>
		</div>
		<div id="_idContainer445" class="Content">
			<h2 id="_idParaDest-173"><a id="_idTextAnchor193"/>Chapter 1: Python Machine Learning Toolkit</h2>
			<h3 id="_idParaDest-174"><a id="_idTextAnchor194"/>Activity 1: pandas Functions</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li>Open a new Jupyter n<a id="_idTextAnchor195"/>otebook.</li>
				<li>Use pandas to load the Titanic dataset:<p class="snippet">import pandas as pd</p><p class="snippet">df = pd.read_csv('titanic.csv')</p><p>Use the <strong class="inline">head()</strong> function on the dataset as follows:</p><p class="snippet"># Have a look at the first 5 sample of the data</p><p class="snippet">df.head()</p><p>The output will be as follows:</p><div id="_idContainer378" class="IMG---Figure"><img src="image/C12622_01_65.jpg" alt="Figure 1.65: First five rows&#13;&#10;"/></div><h6>Figure 1.65: First five rows</h6><p>Use the <strong class="inline">describe</strong> function as follows:</p><p class="snippet">df.describe(include='all')</p><p>The output will be as follows:</p><div id="_idContainer379" class="IMG---Figure"><img src="image/C12622_01_66.jpg" alt="Figure 1.66: Output of describe()&#13;&#10;"/></div><h6>Figure 1.66: Output of describe()</h6></li>
				<li>We don't need the <strong class="inline">Unnamed: 0</strong> column. We can remove the column without using the <strong class="inline">del</strong> command, as follows:<p class="snippet">df = df[df.columns[1:]] # Use the columns</p><p class="snippet">df.head()</p><p>The output will be as follows:</p><div id="_idContainer380" class="IMG---Figure"><img src="image/C12622_01_67.jpg" alt="Figure 1.67: First five rows after deleting the Unnamed: 0 column&#13;&#10;"/></div><h6>Figure 1.67: First five rows after deleting the Unnamed: 0 column</h6></li>
				<li>Compute the mean, standard deviation, minimum, and maximum values for the columns of the DataFrame without using <strong class="inline">describe</strong>:<p class="snippet">df.mean()</p><p class="snippet">Fare        33.295479</p><p class="snippet">Pclass       2.294882</p><p class="snippet">Age         29.881138</p><p class="snippet">Parch        0.385027</p><p class="snippet">SibSp        0.498854</p><p class="snippet">Survived     0.383838</p><p class="snippet">dtype: float64</p><p class="snippet">df.std()</p><p class="snippet">Fare        51.758668</p><p class="snippet">Pclass       0.837836</p><p class="snippet">Age         14.413493</p><p class="snippet">Parch        0.865560</p><p class="snippet">SibSp        1.041658</p><p class="snippet">Survived     0.486592</p><p class="snippet">dtype: float64</p><p class="snippet">df.min()</p><p class="snippet">Fare        0.00</p><p class="snippet">Pclass      1.00</p><p class="snippet">Age         0.17</p><p class="snippet">Parch       0.00</p><p class="snippet">SibSp       0.00</p><p class="snippet">Survived    0.00</p><p class="snippet">dtype: float64</p><p class="snippet">df.max()</p><p class="snippet">Fare        512.3292</p><p class="snippet">Pclass        3.0000</p><p class="snippet">Age          80.0000</p><p class="snippet">Parch         9.0000</p><p class="snippet">SibSp         8.0000</p><p class="snippet">Survived      1.0000</p><p class="snippet">dtype: float64</p></li>
				<li>What about the 33, 66, and 99% quartiles? Use the <strong class="inline">quantile</strong> method as follows:<p class="snippet">df.quantile(0.33)</p><p class="snippet">Fare         8.559325</p><p class="snippet">Pclass       2.000000</p><p class="snippet">Age         23.000000</p><p class="snippet">Parch        0.000000</p><p class="snippet">SibSp        0.000000</p><p class="snippet">Survived     0.000000</p><p class="snippet">Name: 0.33, dtype: float64</p><p class="snippet">df.quantile(0.66)</p><p class="snippet">Fare        26.0</p><p class="snippet">Pclass       3.0</p><p class="snippet">Age         34.0</p><p class="snippet">Parch        0.0</p><p class="snippet">SibSp        0.0</p><p class="snippet">Survived     1.0</p><p class="snippet">Name: 0.66, dtype: float64</p><p class="snippet">df.quantile(0.99)</p><p class="snippet">Fare        262.375</p><p class="snippet">Pclass        3.000</p><p class="snippet">Age          65.000</p><p class="snippet">Parch         4.000</p><p class="snippet">SibSp         5.000</p><p class="snippet">Survived      1.000</p><p class="snippet">Name: 0.99, dtype: float64</p></li>
				<li>How many passengers were from each class? Let's see, using the <strong class="inline">groupby</strong> method:<p class="snippet">class_groups = df.groupby('Pclass')</p><p class="snippet">for name, index in class_groups:</p><p class="snippet">    print(f'Class: {name}: {len(index)}')</p><p class="snippet">Class: 1: 323</p><p class="snippet">Class: 2: 277</p><p class="snippet">Class: 3: 709</p></li>
				<li>How many passengers were from each class? You can find the answer by using selecting/indexing methods to count the members of each class:<p class="snippet">for clsGrp in df.Pclass.unique():</p><p class="snippet">    num_class = len(df[df.Pclass == clsGrp])</p><p class="snippet">    print(f'Class {clsGrp}: {num_class}')</p><p class="snippet">Class 3: 709</p><p class="snippet">Class 1: 323</p><p class="snippet">Class 2: 277</p><p>The answers to <em class="italics">Step 6</em> and <em class="italics">Step 7</em> do match.</p></li>
				<li>Determine who the eldest passenger in third class was:<p class="snippet">third_class = df.loc[(df.Pclass == 3)]</p><p class="snippet">third_class.loc[(third_class.Age == third_class.Age.max())]</p><p>The output will be as follows:</p><div id="_idContainer381" class="IMG---Figure"><img src="image/C12622_01_68.jpg" alt="Figure 1.68: Eldest passenger in third class&#13;&#10;"/></div><h6>Figure 1.68: Eldest passenger in third class</h6></li>
				<li>For a number of machine learning problems, it is very common to scale the numerical values between 0 and 1. Use the <strong class="inline">agg</strong> method with Lambda functions to scale the <strong class="inline">Fare</strong> and <strong class="inline">Age</strong> columns between 0 and 1:<p class="snippet">fare_max = df.Fare.max()</p><p class="snippet">age_max = df.Age.max()</p><p class="snippet">df.agg({</p><p class="snippet">    'Fare': lambda x: x / fare_max, </p><p class="snippet">    'Age': lambda x: x / age_max,</p><p class="snippet">}).head()</p><p>The output will be as follows:</p><div id="_idContainer382" class="IMG---Figure"><img src="image/C12622_01_69.jpg" alt="Figure 1.69: Scaling numerical values between 0 and 1&#13;&#10;"/></div><h6>Figure 1.69: Scaling numerical values between 0 and 1</h6></li>
				<li>There is one individual in the dataset without a listed <strong class="inline">Fare</strong> value:<p class="snippet">df_nan_fare = df.loc[(df.Fare.isna())]</p><p class="snippet">df_nan_fare</p><p>This is the output:</p></li>
			</ol>
			<div>
				<div id="_idContainer383" class="IMG---Figure">
					<img src="image/C12622_01_70.jpg" alt="Figure 1.70: Individual without a listed Fare value&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 1.70: Individual without a listed Fare value</h6>
			<p>Replace the <strong class="inline">NaN</strong> values of this row in the main DataFrame with the mean <strong class="inline">Fare</strong> value for those corresponding with the same class and <strong class="inline">Embarked</strong> location using the <strong class="inline">groupby</strong> method:</p>
			<p class="snippet">embarked_class_groups = df.groupby(['Embarked', 'Pclass'])</p>
			<p class="snippet">indices = embarked_class_groups.groups[(df_nan_fare.Embarked.values[0], df_nan_fare.Pclass.values[0])]</p>
			<p class="snippet">mean_fare = df.iloc[indices].Fare.mean()</p>
			<p class="snippet">df.loc[(df.index == 1043), 'Fare'] = mean_fare</p>
			<p class="snippet">df.iloc[1043]</p>
			<p>The output will be as follows:</p>
			<p class="snippet">Cabin                      NaN</p>
			<p class="snippet">Embarked                     S</p>
			<p class="snippet">Fare                   14.4354</p>
			<p class="snippet">Pclass                       3</p>
			<p class="snippet">Ticket                    3701</p>
			<p class="snippet">Age                       60.5</p>
			<p class="snippet">Name        Storey, Mr. Thomas</p>
			<p class="snippet">Parch                        0</p>
			<p class="snippet">Sex                       male</p>
			<p class="snippet">SibSp                        0</p>
			<p class="snippet">Survived                   NaN</p>
			<p class="snippet">Name: 1043, dtype: object</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor196"/>Chapter 2: Exploratory Data Analysis and Visualization</h2>
			<h3 id="_idParaDest-176"><a id="_idTextAnchor197"/>Activity 2: Summary Statistics and Missing Values</h3>
			<p><strong class="bold">Solution</strong></p>
			<p>The steps to complete this activity are as follows:</p>
			<ol>
				<li value="1">Read the data. Use pandas' <strong class="inline">.read_csv</strong> method to read the CSV file into a pandas DataFrame:<p class="snippet">data = pd.read_csv('house_prices.csv')</p></li>
				<li>Use pandas' <strong class="inline">.info()</strong> and <strong class="inline">.describe()</strong> methods to view the summary statistics of the dataset:<p class="snippet">data.info()</p><p class="snippet">data.describe().T</p><p>The output of <strong class="inline">info()</strong> will be:</p><div id="_idContainer384" class="IMG---Figure"><img src="image/C12622_02_39.jpg" alt="Figure 2.39: The output of the info() method&#13;&#10;"/></div><h6>Figure 2.39: The output of the info() method</h6><p>The output of <strong class="inline">describe()</strong> will be:</p><div id="_idContainer385" class="IMG---Figure"><img src="image/C12622_02_40.jpg" alt="Figure 2.40: The output of the describe() method&#13;&#10;"/></div><h6>Figure 2.40: The output of the describe() method</h6></li>
				<li>Find the total count and total percentage of missing values in each column of the DataFrame and display them for columns having at least one null value, in descending order of missing percentages.<p>As we did in <em class="italics">Exercise 12: Visualizing Missing Values</em>, we will use the <strong class="inline">.isnull()</strong> function on the DataFrame to get a mask, find the count of null values in each column by using the <strong class="inline">.sum()</strong> function over the mask DataFrame and the fraction of null values by using <strong class="inline">.mean()</strong> over the mask DataFrame and multiply by 100 to convert it to a percentage. Then, we use <strong class="inline">pd.concat()</strong> to combine the total and percentage of null values into a single DataFrame and sort the rows by percentage of missing values:</p><p class="snippet">mask = data.isnull()</p><p class="snippet">total = mask.sum()</p><p class="snippet">percent = 100*mask.mean()</p><p class="snippet">missing_data = pd.concat([total, percent], axis=1,join='outer',</p><p class="snippet">               keys=['count_missing', 'perc_missing'])</p><p class="snippet">missing_data.sort_values(by='perc_missing', ascending=False, inplace=True)</p><p class="snippet">missing_data[missing_data.count_missing &gt; 0]</p><p>The output will be:</p><div id="_idContainer386" class="IMG---Figure"><img src="image/C12622_02_41.jpg" alt="Figure 2.41: Total count and percentage of missing values in each column&#13;&#10;"/></div><h6>Figure 2.41: Total count and percentage of missing values in each column</h6></li>
				<li>Plot the nullity matrix and nullity correlation heatmap. First, we find the list of column names for those having at least one null value. Then, we use the <strong class="inline">missingno</strong> library to plot the nullity matrix (as we did in <em class="italics">Exercise 12: Visualizing Missing Values</em>) for a sample of 500 points, and the nullity correlation heatmap for the data in those columns:<p class="snippet">nullable_columns = data.columns[mask.any()].tolist()</p><p class="snippet">msno.matrix(data[nullable_columns].sample(500))</p><p class="snippet">plt.show()</p><p class="snippet">msno.heatmap(data[nullable_columns], figsize=(18,18))</p><p class="snippet">plt.show()</p><p>The nullity matrix will look like this:</p><div id="_idContainer387" class="IMG---Figure"><img src="image/C12622_02_42.jpg" alt="Figure 2.42: Nullity matrix&#13;&#10;"/></div><h6>Figure 2.42: Nullity matrix</h6><p>The nullity correlation heatmap will look like this:</p><div id="_idContainer388" class="IMG---Figure"><img src="image/C12622_02_43.jpg" alt="Figure 2.43: Nullity correlation heatmap&#13;&#10;"/></div><h6>Figure 2.43: Nullity correlation heatmap</h6></li>
				<li>Delete the columns having more than 80% of values missing. Use the <strong class="inline">.loc</strong> operator on the DataFrame we created in <em class="italics">Step 3</em> to select only those columns that had less than 80% of values missing:<p class="snippet">data = data.loc[:,missing_data[missing_data.perc_missing &lt; 80].index]</p></li>
				<li>Replace null values in the <strong class="inline">FireplaceQu</strong> column with NA values. Use the <strong class="inline">.fillna()</strong> method to replace null values with the <strong class="inline">NA</strong> string:<p class="snippet">data['FireplaceQu'] = data['FireplaceQu'].fillna('NA')</p></li>
			</ol>
			<h3 id="_idParaDest-177"><a id="_idTextAnchor198"/>Activity 3: Visually Representing the Distribution of Values</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Plot a histogram using Matplotlib for the target variable, <strong class="inline">SalePrice</strong>. First, we initialize the figure using the <strong class="inline">plt.figure</strong> command and set the figure size. Then, we use Matplotlib's <strong class="inline">.hist()</strong> function as our primary plotting function, to which we pass the <strong class="inline">SalePrice</strong> series object for plotting the histogram. Lastly, we specify the axes labels and show the plot:<p class="snippet">plt.figure(figsize=(8,6))</p><p class="snippet">plt.hist(data.SalePrice, bins=range(0,800000,50000))</p><p class="snippet">plt.ylabel('Number of data points')</p><p class="snippet">plt.xlabel('SalePrice')</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer389" class="IMG---Figure"><img src="image/C12622_02_44.jpg" alt="Figure 2.44: Histogram for the target variable&#13;&#10;"/></div><h6>Figure 2.44: Histogram for the target variable</h6></li>
				<li>Find the number of unique values within each column having the object type. Create a new DataFrame called <strong class="inline">object_variables</strong> by using the <strong class="inline">.select_dtypes</strong> function on the original DataFrame to select those columns with the <strong class="inline">numpy.object</strong> data type. Then, find the number of unique values for each column in this DataFrame by using the <strong class="inline">.nunique()</strong> function, and sort the resultant series:<p class="snippet">object_variables = data.select_dtypes(include=[np.object])</p><p class="snippet">object_variables.nunique().sort_values()</p><p>The output will be:</p><div id="_idContainer390" class="IMG---Figure"><img src="image/C12622_02_45.jpg" alt="Figure 2.45: Number of unique values within each column having the object type&#13;&#10;"/></div><h6>Figure 2.45: Number of unique values within each column having the object type</h6></li>
				<li>Create a DataFrame representing the number of occurrences for each categorical value in the <strong class="inline">HouseStyle</strong> column. Use the <strong class="inline">.value_counts()</strong> function to calculate the frequencies of each value in decreasing order in the form of a pandas series, then reset the index to give us a DataFrame and sort the values by the index:<p class="snippet">counts = data.HouseStyle.value_counts(dropna=False)</p><p class="snippet">counts.reset_index().sort_values(by='index')</p><p>The output will be:</p><div id="_idContainer391" class="IMG---Figure"><img src="image/C12622_02_46.jpg" alt="Figure 2.46: Number of occurrences for each categorical value in the HouseStyle column&#13;&#10;"/></div><h6>Figure 2.46: Number of occurrences for each categorical value in the HouseStyle column</h6></li>
				<li>Plot a pie chart representing these counts. As in <em class="italics">Step 1</em>, we initialize the image using <strong class="inline">plt.figure()</strong> and use the <strong class="inline">plt.title()</strong> and <strong class="inline">plt.show()</strong> methods to set the figure title and display it respectively. The primary plotting function used is <strong class="inline">plt.pie()</strong>, to which we pass the series we created in the previous step:<p class="snippet">plt.figure(figsize=(10,10))</p><p class="snippet">plt.pie(counts, labels=counts.index)</p><p class="snippet">plt.title('Pie chart showing counts for\nHouseStyle categories')</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer392" class="IMG---Figure"><img src="image/C12622_02_47.jpg" alt="Figure 2.47: Pie chart representing the counts&#13;&#10;"/></div><h6>Figure 2.47: Pie chart representing the counts</h6></li>
				<li>Find the number of unique values within each column having the number type. As done in <em class="italics">Step 2</em>, now select columns having the <strong class="inline">numpy.number</strong> data type and find the number of unique values in each column using <strong class="inline">.nunique()</strong>. Sort the resultant series in descending order:<p class="snippet">numeric_variables = data.select_dtypes(include=[np.number])</p><p class="snippet">numeric_variables.nunique().sort_values(ascending=False)</p><p>The output will be as follows:</p><div id="_idContainer393" class="IMG---Figure"><img src="image/C12622_02_48.jpg" alt="Figure 2.48: Number of unique values within each column having the number type&#13;&#10;"/></div><h6>Figure 2.48: Number of unique values within each column having the number type</h6></li>
				<li>Plot a histogram using Seaborn for the <strong class="inline">LotArea</strong> variable. Use Seaborn's <strong class="inline">.distplot()</strong> function as the primary plotting function, to which the <strong class="inline">LotArea</strong> series in the DataFrame needs to be passed (without any null values; use <strong class="inline">.dropna()</strong> on the series to remove them). To improve the plot view, also set the <strong class="inline">bins</strong> parameter and specify the <em class="italics">X</em> axis limits using <strong class="inline">plt.xlim()</strong>:<p class="snippet">plt.figure(figsize=(10,7))</p><p class="snippet">sns.distplot(data.LotArea.dropna(), , bins=range(0,100000,1000))</p><p class="snippet">plt.xlim(0,100000)</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer394" class="IMG---Figure"><img src="image/C12622_02_49.jpg" alt="Figure 2.49: Histogram for the LotArea variable&#13;&#10;"/></div><h6>Figure 2.49: Histogram for the LotArea variable</h6></li>
				<li>Calculate the skew and kurtosis values for the values in each column:<p class="snippet">data.skew().sort_values()</p><p class="snippet">data.kurt()</p><p>The output for sk<a id="_idTextAnchor199"/>ew values will be:</p></li>
			</ol>
			<div>
				<div id="_idContainer395" class="IMG---Figure">
					<img src="image/C12622_02_50.jpg" alt="Figure 2.50: Skew values for each column&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.50: Skew values for each column</h6>
			<p>The output for kurtosis values will be:</p>
			<div>
				<div id="_idContainer396" class="IMG---Figure">
					<img src="image/C12622_02_51.jpg" alt="Figure 2.51: Kurtosis values for each column&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.51: Kurtosis values for each column</h6>
			<h3 id="_idParaDest-178"><a id="_idTextAnchor200"/>Activity 4: Relationships Within the Data</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Plot the correlation heatmap for the dataset. As we did in <em class="italics">Exercise 23: Correlation Heatmap</em>, plot the heatmap using Seaborn's <strong class="inline">.heatmap()</strong> function and pass the feature correlation matrix (as determined by using pandas' <strong class="inline">.corr()</strong> function on the DataFrame). Additionally, set the color map to <strong class="inline">RdBu</strong> using the <strong class="inline">cmap</strong> parameter and the minimum and maximum values on the color scale to <strong class="inline">-1</strong> and <strong class="inline">1</strong> using the <strong class="inline">vmin</strong> and <strong class="inline">vmax</strong> parameters respectively:<p class="snippet">plt.figure(figsize = (12,10))</p><p class="snippet">sns.heatmap(data.corr(), square=True, cmap="RdBu", vmin=-1, vmax=1)</p><p class="snippet">plt.show()</p><p>The output will be:</p><div id="_idContainer397" class="IMG---Figure"><img src="image/C12622_02_52.jpg" alt="Figure 2.52: Heatmap for the dataset&#13;&#10;"/></div><h6>Figure 2.52: Heatmap for the dataset</h6></li>
				<li>Plot a more compact heatmap having annotations for correlation values using the following subset of features:<p class="snippet">feature_subset = [</p><p class="snippet">    'GarageArea', 'GarageCars','GarageCond','GarageFinish','GarageQual','GarageType',</p><p class="snippet">    'GarageYrBlt','GrLivArea','LotArea','MasVnrArea','SalePrice'</p><p class="snippet">]</p><p>Now do the same as in the previous step, this time selecting only the above columns in the dataset, and adding an <strong class="inline">annot</strong> parameter with the <strong class="inline">True</strong> value to the primary plotting function, all else remaining the same:</p><p class="snippet">plt.figure(figsize = (12,10))</p><p class="snippet">sns.heatmap(data[feature_subset].corr(), square=True, annot=True, cmap="RdBu", vmin=-1, vmax=1)</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer398" class="IMG---Figure"><img src="image/C12622_02_53.jpg" alt="Figure 2.53: Heatmap with annotations for correlation values&#13;&#10;"/></div><h6>Figure 2.53: Heatmap with annotations for correlation values</h6></li>
				<li>Display the pairplot for the same subset of features, with the KDE plot on the diagonals and scatter plot elsewhere. Use Seaborn's <strong class="inline">.pairplot()</strong> function to plot the pairplot for the non-null values in the selected columns of the DataFrame. To make the diagonal plots KDE plots, pass <strong class="inline">kde</strong> to the <strong class="inline">diag_kind</strong> parameter and to set all other plots as scatter plots, pass <strong class="inline">scatter</strong> to the <strong class="inline">kind</strong> parameter:<p class="snippet">sns.pairplot(data[feature_subset].dropna(), kind ='scatter', diag_kind='kde')</p><p class="snippet">plt.show()</p><p>The output will be:</p><div id="_idContainer399" class="IMG---Figure"><img src="image/C12622_02_54.jpg" alt="Figure 2.54: Pairplot for the same subset of features&#13;&#10;"/></div><h6>Figure 2.54: Pairplot for the same subset of features</h6></li>
				<li>Create a boxplot to show the variation in <strong class="inline">SalePrice</strong> for each category of <strong class="inline">GarageCars</strong>. The primary plotting function used here will be Seaborn's <strong class="inline">.boxplot()</strong> function, to which we pass the DataFrame along with parameters <strong class="inline">x</strong> and <strong class="inline">y</strong>, the former is the categorical variable and the latter is the continuous variable over which we want to see the variation within each category, that is, <strong class="inline">GarageCars</strong> and <strong class="inline">SalePrice</strong> respectively:<p class="snippet">plt.figure(figsize=(10, 10))</p><p class="snippet">sns.boxplot(x='GarageCars', y="SalePrice", data=data)</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer400" class="IMG---Figure"><img src="image/C12622_02_55.jpg" alt="Figure 2.55: Boxplot showing variation in SalePrice for each category of GarageCars&#13;&#10;"/></div><h6>Figure 2.55: Boxplot showing variation in SalePrice for each category of GarageCars</h6></li>
				<li>Plot a line graph using Seaborn to show the variation in <strong class="inline">SalePrice</strong> for older and more recently built flats. Here, we will plot a line plot using Seaborn's <strong class="inline">.lineplot()</strong> function. Since we want to see the variation in <strong class="inline">SalePrice</strong>, we take this as the <em class="italics">y</em> variable, and as the variation is across a period of time, we take <strong class="inline">YearBuilt</strong> as the <em class="italics">x</em> variable. Keeping this in mind, we pass the respective series as values to the <strong class="inline">y</strong> and <strong class="inline">x</strong> parameters for the primary plotting function. We also pass a <strong class="inline">ci=None</strong> parameter to hide the standard deviation indicator around the line in the plot:<p class="snippet">plt.figure(figsize=(10,7))</p><p class="snippet">sns.lineplot(x=data.YearBuilt, y=data.SalePrice, ci=None)</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer401" class="IMG---Figure">
					<img src="image/C12622_02_56.jpg" alt="Figure 2.56: Line graph showing the variation in SalePrice for older and more recently built flats&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.56: Line graph showing the variation in SalePrice for older and more recently built flats</h6>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor201"/>Chapter 3: Regression Analysis</h2>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor202"/>Activity 5: Plotting Data with a Moving Average</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Load the dataset into a pandas DataFrame from the CSV file:<p class="snippet">df = pd.read_csv('austin_weather.csv')</p><p class="snippet">df.head()</p><p>The output will show the initial five rows of the <strong class="inline">austin_weather.csv</strong> file:</p><div id="_idContainer402" class="IMG---Figure"><img src="image/C12622_03_74.jpg" alt="Figure 3.74: The first five rows of the Austin weather data&#13;&#10;"/></div><h6>Figure 3.74: The first five rows of the Austin weather data</h6></li>
				<li>Since we only need the <strong class="inline">Date</strong> and <strong class="inline">TempAvgF</strong> columns, we'll remove all others from the dataset:<p class="snippet">df = df[['Date', 'TempAvgF']]</p><p class="snippet">df.head()</p><p>The output will be:</p><div id="_idContainer403" class="IMG---Figure"><img src="image/C12622_03_75.jpg" alt="Figure 3.75: Date and TempAvgF columns of the Austin weather data&#13;&#10;"/></div><h6>Figure 3.75: Date and TempAvgF columns of the Austin weather data</h6></li>
				<li>Initially, we are only interested in the first year's data, so we need to extract that information only. Create a column in the DataFrame for the year value, extract the year value as an integer from the strings in the <strong class="inline">Date</strong> column, and assign these values to the <strong class="inline">Year</strong> column. Note that temperatures are recorded daily:<p class="snippet">df['Year'] = [int(dt[:4]) for dt in df.Date]</p><p class="snippet">df.head()</p><p>The output will be:</p><div id="_idContainer404" class="IMG---Figure"><img src="image/C12622_03_76.jpg" alt="Figure 3.76: Extracting the year&#13;&#10;"/></div><h6>Figure 3.76: Extracting the year</h6></li>
				<li>Repeat this process to extract the month values and store the values as integers in a <strong class="inline">Month</strong> column:<p class="snippet">df['Month'] = [int(dt[5:7]) for dt in df.Date]</p><p class="snippet">df.head()</p><p>The output will be:</p><div id="_idContainer405" class="IMG---Figure"><img src="image/C12622_03_77.jpg" alt="Figure 3.77: Extracting the month&#13;&#10;"/></div><h6>Figure 3.77: Extracting the month</h6></li>
				<li>Copy the first year's worth of data to a DataFrame:<p class="snippet">df_first_year = df[:365]</p><p class="snippet">df_first_year.head()</p><p>The output will be as follows:</p><div id="_idContainer406" class="IMG---Figure"><img src="image/C12622_03_78.jpg" alt="Figure 3.78: Copied data to new dataframe&#13;&#10;"/></div><h6>Figure 3.78: Copied data to new DataFrame</h6></li>
				<li>Compute a 20-day moving average filter:<p class="snippet">window = 20</p><p class="snippet">rolling = df_first_year.TempAvgF.rolling(window).mean();</p><p class="snippet">rolling.head(n=20)</p><p>The output will be:</p><p class="snippet">0       NaN</p><p class="snippet">1       NaN</p><p class="snippet">2       NaN</p><p class="snippet">3       NaN</p><p class="snippet">4       NaN</p><p class="snippet">5       NaN</p><p class="snippet">6       NaN</p><p class="snippet">7       NaN</p><p class="snippet">8       NaN</p><p class="snippet">9       NaN</p><p class="snippet">10      NaN</p><p class="snippet">11      NaN</p><p class="snippet">12      NaN</p><p class="snippet">13      NaN</p><p class="snippet">14      NaN</p><p class="snippet">15      NaN</p><p class="snippet">16      NaN</p><p class="snippet">17      NaN</p><p class="snippet">18      NaN</p><p class="snippet">19    47.75</p><p class="snippet">Name: TempAvgF, dtype: float64</p></li>
				<li>Plot the raw data and the moving average signal, with the <em class="italics">x</em> axis as the day number in the year:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(range(1, 366), df_first_year.TempAvgF, label='Raw Data');</p><p class="snippet">ax.plot(range(1, 366), rolling, c='r', label=f'{window} day moving average');</p><p class="snippet">ax.set_title('Daily Mean Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Day')</p><p class="snippet">ax.set_ylabel('Temperature (degF)')</p><p class="snippet">ax.set_xticks(range(1, 366), 10)</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer407" class="IMG---Figure">
					<img src="image/C12622_03_79.jpg" alt="Figure 3.79: Scatter plot of temperature throughout the year&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.79: Scatter plot of temperature throughout the year</h6>
			<h3 id="_idParaDest-181"><a id="_idTextAnchor203"/>Activity 6: Linear Regression Using the Least Squares Method</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Visualize the measurements:<p class="snippet">df.head()</p><p>The output will be as follows:</p><div id="_idContainer408" class="IMG---Figure"><img src="image/C12622_03_80.jpg" alt="Figure 3.80: First five rows of activity2_measurements.csv dataset&#13;&#10;"/></div><h6>Figure 3.80: First five rows of activity2_measurements.csv dataset</h6></li>
				<li>Visualize the rolling average values:<p class="snippet">rolling.head(n=30)</p><p>The output will be as follows:</p><div id="_idContainer409" class="IMG---Figure"><img src="image/C12622_03_81.jpg" alt="Figure 3.81: Rolling head average&#13;&#10;"/></div><h6>Figure 3.81: Rolling head average</h6></li>
				<li>Create a linear regression model using the default parameters; that is, calculate a <em class="italics">y</em> intercept for the model and do not normalize the data:<p class="snippet">model = LinearRegression()</p><p class="snippet">model</p><p>The output will be as follows:</p><p class="snippet">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,</p><p class="snippet">         normalize=False)</p></li>
				<li>Now fit the model, where the input data is the day number for the year (1 to 365) and the output is the average temperature. To make later calculations easier, insert a column (<strong class="inline">DayOfYear</strong>) that corresponds with the day of the year for that measurement:<p class="snippet">df_first_year.loc[:,'DayOfYear'] = [i + 1 for i in df_first_year.index]</p><p class="snippet">df_first_year.head()</p><p>The output will be as follows:</p><div id="_idContainer410" class="IMG---Figure"><img src="image/C12622_03_82.jpg" alt="Figure 3.82: Adding day of year column&#13;&#10;"/></div><h6>Figure 3.82: Adding day of year column</h6></li>
				<li>Fit the model with the <strong class="inline">DayOfYear</strong> values as the input and <strong class="inline">df_first_year.TempAvgF</strong> as the output:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">model.fit(df_first_year.DayOfYear.values.reshape((-1, 1)), df_first_year.TempAvgF)</p><p>The output will be as follows:</p><p class="snippet">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,</p><p class="snippet">         normalize=False)</p></li>
				<li>Print the parameters of the model:<p class="snippet">print(f'm = {model.coef_[0]}')</p><p class="snippet">print(f'c = {model.intercept_}')</p><p class="snippet">print('\nModel Definition')</p><p class="snippet">print(f'y = {model.coef_[0]:0.4}x + {model.intercept_:0.4f}')</p><p>The output will be:</p><p class="snippet">m = 0.04909173467448788</p><p class="snippet">c = 60.28196597922625</p><p class="snippet">Model Definition</p><p class="snippet">y = 0.04909x + 60.2820</p></li>
				<li>We can calculate the trendline values by using the first, middle, and last values (days in years) in the linear equation:<p class="snippet">trend_x = np.array([</p><p class="snippet">    1,</p><p class="snippet">    182.5,</p><p class="snippet">    365</p><p class="snippet">])</p><p class="snippet">trend_y = model.predict(trend_x.reshape((-1, 1)))</p><p class="snippet">trend_y</p><p>The output will be as follows:</p><p class="snippet">array([60.33105771, 69.24120756, 78.20044914])</p></li>
				<li>Plot these values with the trendline:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_first_year.DayOfYear, df_first_year.TempAvgF, label='Raw Data');</p><p class="snippet">ax.plot(df_first_year.DayOfYear, rolling, c='r', label=f'{window} day moving average');</p><p class="snippet">ax.plot(trend_x, trend_y, c='k', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Daily Mean Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Day')</p><p class="snippet">ax.set_ylabel('Temperature (degF)')</p><p class="snippet">ax.set_xticks(range(1, 366), 10)</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p><div id="_idContainer411" class="IMG---Figure"><img src="image/C12622_03_83.jpg" alt="Figure 3.83: Scatterplot of temperature thought the year with the predicted trendline&#13;&#10;"/></div><h6>Figure 3.83: Scatterplot of temperature thought the year with the predicted trendline</h6></li>
				<li>Evaluate the performance of the model. How well does the model fit the data? Calculate the r2 score to find out:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">r2 = model.score(df_first_year.DayOfYear.values.reshape((-1, 1)), df_first_year.TempAvgF)</p><p class="snippet">print(f'r2 score = {r2:0.4f}')</p><p>The output will be:</p><p class="snippet">r2 score = 0.1222</p></li>
			</ol>
			<h3 id="_idParaDest-182"><a id="_idTextAnchor204"/>Activity 7: Dummy Variables</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Plot the raw data (<strong class="inline">df</strong>) and moving average (<strong class="inline">rolling</strong>):<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_first_year.DayOfYear, df_first_year.TempAvgF, label='Raw Data');</p><p class="snippet">ax.plot(df_first_year.DayOfYear, rolling, c='r', label=f'{window} day moving average');</p><p class="snippet">ax.set_title('Daily Mean Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Day')</p><p class="snippet">ax.set_ylabel('Temperature (degF)')</p><p class="snippet">ax.set_xticks(range(1, 366), 10)</p><p class="snippet">ax.legend();</p><p>The output will be:</p><div id="_idContainer412" class="IMG---Figure"><img src="image/C12622_03_84.jpg" alt="Figure 3.84: Scatterplot of Temperature throughout the year&#13;&#10;"/></div><h6>Figure 3.84: Scatterplot of Temperature throughout the year</h6></li>
				<li>Looking at the preceding plot, there seems to be an inflection point around day 250. Create a dummy variable to introduce this feature into the linear model:<p class="snippet">df_first_year.loc[:,'inflection'] = [1 * int(i &lt; 250) for i in df_first_year.DayOfYear]</p></li>
				<li>Check the first and last samples to confirm that the dummy variable is correct. Check the first five samples:<p class="snippet">df_first_year.head()</p><p>The output will be as follows:</p><div id="_idContainer413" class="IMG---Figure"><img src="image/C12622_03_85.jpg" alt="Figure 3.85: First five columns&#13;&#10;"/></div><h6>Figure 3.85: First five columns</h6><p>Then, check the last five samples:</p><p class="snippet">df_first_year.tail()</p><p>The output will be:</p><div id="_idContainer414" class="IMG---Figure"><img src="image/C12622_03_86.jpg" alt="Figure 3.86: Last five columns&#13;&#10;"/></div><h6>Figure 3.86: Last five columns</h6></li>
				<li>Use a least squares linear regression model and fit the model to the <strong class="inline">DayOfYear</strong> values and the dummy variable to predict <strong class="inline">TempAvgF</strong>:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">model = LinearRegression()</p><p class="snippet">model.fit(df_first_year[['DayOfYear', 'inflection']], df_first_year.TempAvgF)</p><p>The output will be as follows:</p><p class="snippet">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,</p><p class="snippet">         normalize=False)</p></li>
				<li>Compute the r2 score:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">r2 = model.score(df_first_year[['DayOfYear', 'inflection']], df_first_year.TempAvgF)</p><p class="snippet">print(f'r2 score = {r2:0.4f}')</p><p>The output will be as follows:</p><p class="snippet">r2 score = 0.3631</p></li>
				<li>Using the <strong class="inline">DayOfYear</strong> values, create a set of predictions using the model to construct a trendline:<p class="snippet">trend_y = model.predict(df_first_year[['DayOfYear', 'inflection']].values)</p><p class="snippet">trend_y</p><p>The output will be:</p><p class="snippet">array([51.60311133, 51.74622654, 51.88934175, 52.03245696, 52.17557217,</p><p class="snippet">       52.31868739, 52.4618026 , 52.60491781, 52.74803302, 52.89114823,</p><p class="snippet">       53.03426345, 53.17737866, 53.32049387, 53.46360908, 53.60672429,</p><p class="snippet">       53.7498395 , 53.89295472, 54.03606993, 54.17918514, 54.32230035,</p><p class="snippet">       54.46541556, 54.60853078, 54.75164599, 54.8947612 , 55.03787641,</p><p class="snippet">…</p><p class="snippet">…</p><p class="snippet">       73.88056649, 74.0236817 , 74.16679692, 74.30991213, 74.45302734,</p><p class="snippet">       74.59614255, 74.73925776, 74.88237297, 75.02548819, 75.1686034 ,</p><p class="snippet">       75.31171861, 75.45483382, 75.59794903, 75.74106425, 75.88417946,</p><p class="snippet">       76.02729467, 76.17040988, 76.31352509, 76.4566403 , 76.59975552,</p><p class="snippet">       76.74287073, 76.88598594, 77.02910115, 77.17221636, 77.31533157])</p></li>
				<li>Plot the trendline against the data and the moving average:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_first_year.DayOfYear, df_first_year.TempAvgF, label='Raw Data');</p><p class="snippet">ax.plot(df_first_year.DayOfYear, rolling, c='r', label=f'{window} day moving average');</p><p class="snippet">ax.plot(df_first_year.DayOfYear, trend_y, c='k', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Daily Mean Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Day')</p><p class="snippet">ax.set_ylabel('Temperature (degF)')</p><p class="snippet">ax.set_xticks(range(1, 366), 10)</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer415" class="IMG---Figure">
					<img src="image/C12622_03_87.jpg" alt="Figure 3.87: Predicted trendline&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.87: Predicted trendline</h6>
			<h3 id="_idParaDest-183"><a id="_idTextAnchor205"/>Activity 8: Other Model Types with Linear Regression</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Use a sine curve function as the basis of the model:<p class="snippet"># Using a sine curve</p><p class="snippet">df_first_year['DayOfYear2'] = np.sin(df_first_year['DayOfYear'] / df_first_year['DayOfYear'].max())</p><p class="snippet">df_first_year.head()</p><p>The output will be as follows:</p><div id="_idContainer416" class="IMG---Figure"><img src="image/C12622_03_88.jpg" alt="Figure 3.88: First five rows&#13;&#10;"/></div><h6>Figure 3.88: First five rows</h6></li>
				<li>Fit the model:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">model = LinearRegression()</p><p class="snippet">model.fit(df_first_year[['DayOfYear2', 'DayOfYear']], df_first_year.TempAvgF)</p><p>The output will be as follows:</p><p class="snippet">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,</p><p class="snippet">         normalize=False)</p></li>
				<li>Print the parameters of the model:<p class="snippet">print(f'a = {model.coef_[0]}')</p><p class="snippet">print(f'm = {model.coef_[1]}')</p><p class="snippet">print(f'c = {model.intercept_}')</p><p class="snippet">print('\nModel Definition')</p><p class="snippet">print(f'y = {model.coef_[0]:0.4}x^2 + {model.coef_[1]:0.4}x + {model.intercept_:0.4f}')</p><p>The output will be as follows:</p><p class="snippet">a = 634.322313570282</p><p class="snippet">m = -1.4371290614190075</p><p class="snippet">c = 39.93286585807408</p><p class="snippet">Model Definition</p><p class="snippet">y = 634.3x^2 + -1.437x + 39.9329</p></li>
				<li>Compute the r2 value to measure the performance:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">r2 = model.score(df_first_year[['DayOfYear2', 'DayOfYear']], df_first_year.TempAvgF)</p><p class="snippet">print(f'r2 score = {r2:0.4f}')</p><p>The output will be:</p><p class="snippet">r2 score = 0.7047</p></li>
				<li>Construct the trendline values:<p class="snippet">trend_y = model.predict(df_first_year[['DayOfYear2', 'DayOfYear']].values)</p><p class="snippet">trend_y</p><p>The output will be:</p><p class="snippet">array([40.23360397, 40.53432905, 40.83502803, 41.13568788, 41.43629555,</p><p class="snippet">       41.736838  , 42.03730219, 42.33767507, 42.6379436 , 42.93809474,</p><p class="snippet">       43.23811546, 43.5379927 , 43.83771344, 44.13726463, 44.43663324,</p><p class="snippet">       44.73580624, 45.03477059, 45.33351327, 45.63202123, 45.93028146,</p><p class="snippet">       46.22828093, 46.52600661, 46.82344549, 47.12058453, 47.41741073,</p><p class="snippet">…</p><p class="snippet">…</p><p class="snippet">       59.96306563, 59.55705293, 59.14720371, 58.73351024, 58.31596484,</p><p class="snippet">       57.89455987, 57.46928769, 57.04014072, 56.60711138, 56.17019215,</p><p class="snippet">       55.7293755 , 55.28465397, 54.83602011, 54.38346649, 53.92698572,</p><p class="snippet">       53.46657045, 53.00221334, 52.53390709, 52.06164442, 51.58541811,</p><p class="snippet">       51.10522093, 50.62104569, 50.13288526, 49.6407325 , 49.14458033])</p></li>
				<li>Plot the trendline with the raw data and the moving average:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_first_year.DayOfYear, df_first_year.TempAvgF, label='Raw Data');</p><p class="snippet">ax.plot(df_first_year.DayOfYear, rolling, c='r', label=f'{window} day moving average');</p><p class="snippet">ax.plot(df_first_year.DayOfYear, trend_y, c='k', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Daily Mean Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Day')</p><p class="snippet">ax.set_ylabel('Temperature (degF)')</p><p class="snippet">ax.set_xticks(range(1, 366), 10)</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer417" class="IMG---Figure">
					<img src="image/C12622_03_89.jpg" alt="Figure 3.89: Predicted trendline&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.89: Predicted trendline</h6>
			<h3 id="_idParaDest-184"><a id="_idTextAnchor206"/>Activity 9: Gradient Descent</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Create a generic gradient descent model and normalize the day of year values as between 0 and 1:<p class="snippet">grad_model = SGDRegressor(max_iter=None, tol=1e-3)</p><p class="snippet">_x = df_first_year.DayOfYear / df_first_year.DayOfYear.max()</p></li>
				<li>Fit the model:<p class="snippet">grad_model.fit(_x.values.reshape((-1, 1)), df_first_year.TempAvgF)</p><p>The output will be as follows:</p><p class="snippet">SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,</p><p class="snippet">       eta0=0.01, fit_intercept=True, l1_ratio=0.15,</p><p class="snippet">       learning_rate='invscaling', loss='squared_loss', max_iter=None,</p><p class="snippet">       n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,</p><p class="snippet">       random_state=None, shuffle=True, tol=None, validation_fraction=0.1,</p><p class="snippet">       verbose=0, warm_start=False)</p></li>
				<li>Print the details of the model:<p class="snippet">print(f'm = {grad_model.coef_[0]}')</p><p class="snippet">print(f'c = {grad_model.intercept_[0]}')</p><p class="snippet">print('\nModel Definition')</p><p class="snippet">print(f'y = {grad_model.coef_[0]:0.4}x + {grad_model.intercept_[0]:0.4f}')</p><p>The output will be as follows:</p><p class="snippet">m = 26.406162532140563</p><p class="snippet">c = 55.07470859678077</p><p class="snippet">Model Definition</p><p class="snippet">y = 26.41x + 55.0747</p></li>
				<li>Prepare the <em class="italics">x</em> (<strong class="inline">_trend_x</strong>) trendline values by dividing them by the maximum. Predict <strong class="inline">y_trend_values</strong> using the gradient descent model:<p class="snippet">_trend_x = trend_x / trend_x.max()</p><p class="snippet">trend_y = grad_model.predict(_trend_x.reshape((-1, 1)))</p><p class="snippet">trend_y</p><p>The output will be as follows:</p><p class="snippet">array([55.14705425, 68.27778986, 81.48087113])</p></li>
				<li>Plot the data and the moving average with the trendline:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_first_year.DayOfYear, df_first_year.TempAvgF, label='Raw Data');</p><p class="snippet">ax.plot(df_first_year.DayOfYear, rolling, c='r', label=f'{window} day moving average');</p><p class="snippet">ax.plot(trend_x, trend_y, c='k', linestyle='--', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Daily Mean Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Day')</p><p class="snippet">ax.set_ylabel('Temperature (degF)')</p><p class="snippet">ax.set_xticks(range(1, 366), 10)</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer418" class="IMG---Figure">
					<img src="image/C12622_03_90.jpg" alt="Figure 3.90: Gradient descent predicted trendline&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.90: Gradient descent predicted trendline</h6>
			<h3 id="_idParaDest-185"><a id="_idTextAnchor207"/>Activity 10: Autoregressors</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Plot the complete set of average temperature values (<strong class="inline">df.TempAvgF</strong>) with years on the <em class="italics">x</em> axis:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(df.TempAvgF.values);</p><p class="snippet">yrs = [yr for yr in df.Year.unique()]</p><p class="snippet">plt.xticks(np.arange(0, len(df), len(df) // len(yrs)), yrs);</p><p class="snippet">plt.title('Austin Texas Average Daily Temperature');</p><p class="snippet">plt.xlabel('Year');</p><p class="snippet">plt.ylabel('Temperature (F)');</p><p>The output will be:</p><div id="_idContainer419" class="IMG---Figure"><img src="image/C12622_03_91.jpg" alt="Figure 3.91: Plot of temperature through the year&#13;&#10;"/></div><h6>Figure 3.91: Plot of temperature through the year</h6></li>
				<li>Create a 20-day lag and plot the lagged data on the original dataset:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(df.TempAvgF.values, label='Original Dataset');</p><p class="snippet">plt.plot(df.TempAvgF.shift(20), c='r', linestyle='--',</p><p class="snippet">    label='Lag 20');</p><p class="snippet">yrs = [yr for yr in df.Year.unique()]</p><p class="snippet">plt.xticks(np.arange(0, len(df), len(df) // len(yrs)), yrs);</p><p class="snippet">plt.title('Austin Texas Average Daily Temperature');</p><p class="snippet">plt.xlabel('Year');</p><p class="snippet">plt.ylabel('Temperature (F)');</p><p class="snippet">plt.legend();</p><p>The output will be:</p><div id="_idContainer420" class="IMG---Figure"><img src="image/C12622_03_92.jpg" alt="Figure 3.92: Plot of temperature through the years with a 20-day lag&#13;&#10;"/></div><h6>Figure 3.92: Plot of temperature through the years with a 20-day lag</h6></li>
				<li>Construct an autocorrelation plot to see whether the average temperature can be used with an autoregressor. Where is the lag acceptable and where is it not for an autoregressor?<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">pd.plotting.autocorrelation_plot(df.TempAvgF);</p><p>We'll get the following output:</p><div id="_idContainer421" class="IMG---Figure"><img src="image/C12622_03_93.jpg" alt="Figure 3.93: Plot of autocorrelation versus lag&#13;&#10;"/></div><h6>Figure 3.93: Plot of autocorrelation versus lag</h6><p>The lag is acceptable only when the autocorrelation line lies outside the 99% confidence bounds, as represented by the dashed lines.</p></li>
				<li>Chose an acceptable lag and an unacceptable lag and construct lag plots using these values for acceptable lag:<p class="snippet">plt.figure(figsize=(10,7))</p><p class="snippet">ax = pd.plotting.lag_plot(df.TempAvgF, lag=5);</p><p>We'll get the following output:</p><div id="_idContainer422" class="IMG---Figure"><img src="image/C12622_03_94.jpg" alt="Figure 3.94: Plot of acceptable lag&#13;&#10;"/></div><h6>Figure 3.94: Plot of acceptable lag</h6><p>Use these values for unacceptable lag:</p><p class="snippet">plt.figure(figsize=(10,7))</p><p class="snippet">ax = pd.plotting.lag_plot(df.TempAvgF, lag=1000);</p><p>We'll get the following output:</p><div id="_idContainer423" class="IMG---Figure"><img src="image/C12622_03_95.jpg" alt="Figure 3.95: Plot of unacceptable lag&#13;&#10;"/></div><h6>Figure 3.95: Plot of unacceptable lag</h6></li>
				<li>Create an autoregressor model, note the selected lag, calculate the R2 value, and plot the autoregressor model with the original plot. The model is to project past the available data by 1,000 samples:<p class="snippet">from statsmodels.tsa.ar_model import AR</p><p class="snippet">model = AR(df.TempAvgF)</p></li>
				<li>Fit the model to the data:<p class="snippet">model_fit = model.fit()</p><p class="snippet">print('Lag: %s' % model_fit.k_ar)</p><p class="snippet">print('Coefficients: %s' % model_fit.params)</p><p>The output will be:</p><p class="snippet">Lag: 23</p><p class="snippet">Coefficients: const           1.909395</p><p class="snippet">L1.TempAvgF     0.912076</p><p class="snippet">L2.TempAvgF    -0.334043</p><p class="snippet">L3.TempAvgF     0.157353</p><p class="snippet">L4.TempAvgF     0.025721</p><p class="snippet">L5.TempAvgF     0.041342</p><p class="snippet">L6.TempAvgF     0.030831</p><p class="snippet">L7.TempAvgF    -0.021230</p><p class="snippet">L8.TempAvgF     0.020324</p><p class="snippet">L9.TempAvgF     0.025147</p><p class="snippet">L10.TempAvgF    0.059739</p><p class="snippet">L11.TempAvgF   -0.017337</p><p class="snippet">L12.TempAvgF    0.043553</p><p class="snippet">L13.TempAvgF   -0.027795</p><p class="snippet">L14.TempAvgF    0.053547</p><p class="snippet">L15.TempAvgF    0.013070</p><p class="snippet">L16.TempAvgF   -0.033157</p><p class="snippet">L17.TempAvgF   -0.000072</p><p class="snippet">L18.TempAvgF   -0.026307</p><p class="snippet">L19.TempAvgF    0.025258</p><p class="snippet">L20.TempAvgF    0.038341</p><p class="snippet">L21.TempAvgF    0.007885</p><p class="snippet">L22.TempAvgF   -0.008889</p><p class="snippet">L23.TempAvgF   -0.011080</p><p class="snippet">dtype: float64</p></li>
				<li>Create a set of predictions for 1,000 days after the last sample:<p class="snippet">predictions = model_fit.predict(start=model_fit.k_ar, end=len(df) + 1000)</p><p class="snippet">predictions[:10].values</p><p>The output will be:</p><p class="snippet">array([54.81171857, 56.89097085, 56.41891585, 50.98627626, 56.11843512,</p><p class="snippet">       53.20665111, 55.13941554, 58.4679288 , 61.92497136, 49.46049801])</p></li>
				<li>Plot the predictions, as well as the original dataset:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(df.TempAvgF.values, label='Original Dataset');</p><p class="snippet">plt.plot(predictions, c='g', linestyle=':', label='Predictions');</p><p class="snippet">yrs = [yr for yr in df.Year.unique()]</p><p class="snippet">plt.xticks(np.arange(0, len(df), len(df) // len(yrs)), yrs);</p><p class="snippet">plt.title('Austin Texas Average Daily Temperature');</p><p class="snippet">plt.xlabel('Year');</p><p class="snippet">plt.ylabel('Temperature (F)');</p><p class="snippet">plt.legend();</p><p>The output will be:</p><div id="_idContainer424" class="IMG---Figure"><img src="image/C12622_03_96.jpg" alt="Figure 3.96: Plot of temperature through the years&#13;&#10;"/></div><h6>Figure 3.96: Plot of temperature through the years</h6></li>
				<li>Enhance the view to look for differences by showing the 100th to 200th samples:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(df.TempAvgF.values, label='Original Dataset');</p><p class="snippet">plt.plot(predictions, c='g', linestyle=':', label='Predictions');</p><p class="snippet">yrs = [yr for yr in df.Year.unique()]</p><p class="snippet">plt.xticks(np.arange(0, len(df), len(df) // len(yrs)), yrs);</p><p class="snippet">plt.title('Austin Texas Average Daily Temperature');</p><p class="snippet">plt.xlabel('Year');</p><p class="snippet">plt.ylabel('Temperature (F)');</p><p class="snippet">plt.xlim([100, 200])</p><p class="snippet">plt.legend();</p><p>We'll get the following output:</p></li>
			</ol>
			<div>
				<div id="_idContainer425" class="IMG---Figure">
					<img src="image/C12622_03_97.jpg" alt="Figure 3.97: Plot of predictions with the original dataset&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.97: Plot of predictions with the original dataset</h6>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor208"/>Chapter 4: Classification</h2>
			<h3 id="_idParaDest-187"><a id="_idTextAnchor209"/>Activity 11: Linear Regression Classifier – Two-Class Classifier</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Import the required dependencies:<p class="snippet">import struct</p><p class="snippet">import numpy as np</p><p class="snippet">import gzip</p><p class="snippet">import urllib.request</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from array import array</p><p class="snippet">from sklearn.linear_model import LinearRegression</p></li>
				<li>Load the MNIST data into memory:<p class="snippet">with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size, rows, cols = struct.unpack("&gt;IIII", f.read(16))</p><p class="snippet">    img = np.array(array("B", f.read())).reshape((size, rows, cols))</p><p class="snippet">with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size = struct.unpack("&gt;II", f.read(8))</p><p class="snippet">    labels = np.array(array("B", f.read()))</p><p class="snippet">with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size, rows, cols = struct.unpack("&gt;IIII", f.read(16))</p><p class="snippet">    img_test = np.array(array("B", f.read())).reshape((size, rows, cols))</p><p class="snippet">with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size = struct.unpack("&gt;II", f.read(8))</p><p class="snippet">    labels_test = np.array(array("B", f.read()))</p></li>
				<li>Visualize a sample of the data:<p class="snippet">for i in range(10):</p><p class="snippet">    plt.subplot(2, 5, i + 1)</p><p class="snippet">    plt.imshow(img[i], cmap='gray');</p><p class="snippet">    plt.title(f'{labels[i]}');</p><p class="snippet">    plt.axis('off')</p><p>We'll get the following output:</p><div id="_idContainer426" class="IMG---Figure"><img src="image/C12622_04_76.jpg" alt="Figure 4.76: Sample data&#13;&#10;"/></div><h6>Figure 4.76: Sample data</h6></li>
				<li>Construct a linear classifier model to classify the digits zero and one. The model we are going to create is to determine whether the samples are either the digits zero or one. To do this, we first need to select only those samples:<p class="snippet">samples_0_1 = np.where((labels == 0) | (labels == 1))[0]</p><p class="snippet">images_0_1 = img[samples_0_1]</p><p class="snippet">labels_0_1 = labels[samples_0_1]</p><p class="snippet">samples_0_1_test = np.where((labels_test == 0) | (labels_test == 1))</p><p class="snippet">images_0_1_test = img_test[samples_0_1_test].reshape((-1, rows * cols))</p><p class="snippet">labels_0_1_test = labels_test[samples_0_1_test]</p></li>
				<li>Visualize the selected information. Here's the code for zero:<p class="snippet">sample_0 = np.where((labels == 0))[0][0]</p><p class="snippet">plt.imshow(img[sample_0], cmap='gray');</p><p>The output will be as follows:</p><div id="_idContainer427" class="IMG---Figure"><img src="image/C12622_04_77.jpg" alt="Figure 4.77: First sample data&#13;&#10;"/></div><h6>Figure 4.77: First sample data</h6><p>Here's the code for one:</p><p class="snippet">sample_1 = np.where((labels == 1))[0][0]</p><p class="snippet">plt.imshow(img[sample_1], cmap='gray');</p><p>The output will be:</p><div id="_idContainer428" class="IMG---Figure"><img src="image/C12622_04_78.jpg" alt="Figure 4.78: Second sample data&#13;&#10;"/></div><h6>Figure 4.78: Second sample data</h6></li>
				<li>In order to provide the image information to the model, we must first flatten the data out so that each image is 1 x 784 pixels in shape:<p class="snippet">images_0_1 = images_0_1.reshape((-1, rows * cols))</p><p class="snippet">images_0_1.shape</p><p>The output will be:</p><p class="snippet">(12665, 784)</p></li>
				<li>Let's construct the model; use the <strong class="inline">LinearRegression</strong> API and call the <strong class="inline">fit</strong> function:<p class="snippet">model = LinearRegression()</p><p class="snippet">model.fit(X=images_0_1, y=labels_0_1)</p><p>The output will be:</p><p class="snippet">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,</p><p class="snippet">         normalize=False)</p></li>
				<li>Determine the R2 score against the training set:<p class="snippet">model.score(X=images_0_1, y=labels_0_1)</p><p>The output will be:</p><p class="snippet">0.9705320567708795</p></li>
				<li>Determine the label predictions for each of the training samples, using a threshold of 0.5. Values greater than 0.5 classify as one; values less than or equal to 0.5 classify as zero:<p class="snippet">y_pred = model.predict(images_0_1) &gt; 0.5</p><p class="snippet">y_pred = y_pred.astype(int)</p><p class="snippet">y_pred</p><p>The output will be:</p><p class="snippet">array([0, 1, 1, ..., 1, 0, 1])</p></li>
				<li>Compute the classification accuracy of the predicted training values versus the ground truth:<p class="snippet">np.sum(y_pred == labels_0_1) / len(labels_0_1)</p><p>The output will be:</p><p class="snippet">0.9947887879984209</p></li>
				<li>Compare the performance against the test set:<p class="snippet">y_pred = model.predict(images_0_1_test) &gt; 0.5</p><p class="snippet">y_pred = y_pred.astype(int)</p><p class="snippet">np.sum(y_pred == labels_0_1_test) / len(labels_0_1_test)</p><p>The output will be:</p><p class="snippet">0.9938534278959811</p></li>
			</ol>
			<h3 id="_idParaDest-188"><a id="_idTextAnchor210"/>Activity 12: Iris Classification Using Logistic Regression</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Import the required packages. For this activity, we will require the pandas package for loading the data, the Matplotlib package for plotting, and scikit-learn for creating the logistic regression model. Import all the required packages and relevant modules for these tasks:<p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.linear_model import LogisticRegression</p></li>
				<li>Load the Iris dataset using pandas and examine the first five rows:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df.head()</p><p>The output will be:</p><div id="_idContainer429" class="IMG---Figure"><img src="image/C12622_04_79.jpg" alt="Figure 4.79: The first five rows of the Iris dataset&#13;&#10;"/></div><h6>Figure 4.79: The first five rows of the Iris dataset</h6></li>
				<li>The next step is feature engineering. We need to select the most appropriate features that will provide the most powerful classification model. Plot a number of different features versus the allocated species classifications, for example, sepal length versus petal length and species. Visually inspect the plots and look for any patterns that could indicate separation between each of the species:<p class="snippet">markers = {</p><p class="snippet">    'Iris-setosa': {'marker': 'x'},</p><p class="snippet">    'Iris-versicolor': {'marker': '*'},</p><p class="snippet">    'Iris-virginica': {'marker': 'o'},</p><p class="snippet">}</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for name, group in df.groupby('Species'):</p><p class="snippet">    plt.scatter(group['Sepal Width'], group['Petal Length'], </p><p class="snippet">                label=name,</p><p class="snippet">                marker=markers[name]['marker'],</p><p class="snippet">               )</p><p class="snippet">    </p><p class="snippet">plt.title('Species Classification Sepal Width vs Petal Length');</p><p class="snippet">plt.xlabel('Sepal Width (mm)');</p><p class="snippet">plt.ylabel('Petal Length (mm)');</p><p class="snippet">plt.legend();</p><p>The output will be:</p><div id="_idContainer430" class="IMG---Figure"><img src="image/C12622_04_80.jpg" alt="Figure 4.80: Species classification plot&#13;&#10;"/></div><h6>Figure 4.80: Species classification plot</h6></li>
				<li>Select the features by writing the column names in the following list:<p class="snippet">selected_features = [</p><p class="snippet">    'Sepal Width', # List features here</p><p class="snippet">    'Petal Length'</p><p class="snippet">]</p></li>
				<li>Before we can construct the model, we must first convert the <strong class="inline">species</strong> values into labels that can be used within the model. Replace the <strong class="inline">Iris-setosa</strong> species string with the value <strong class="inline">0</strong>, the <strong class="inline">Iris-versicolor</strong> species string with the value <strong class="inline">1</strong>, and the <strong class="inline">Iris-virginica</strong> species string with the value <strong class="inline">2</strong>:<p class="snippet">species = [</p><p class="snippet">    'Iris-setosa', # 0</p><p class="snippet">    'Iris-versicolor', # 1</p><p class="snippet">    'Iris-virginica', # 2</p><p class="snippet">]</p><p class="snippet">output = [species.index(spec) for spec in df.Species]</p></li>
				<li>Create the model using the <strong class="inline">selected_features</strong> and the assigned <strong class="inline">species</strong> labels:<p class="snippet">model = LogisticRegression(multi_class='auto', solver='lbfgs')</p><p class="snippet">model.fit(df[selected_features], output)</p><p>The output will be:</p><p class="snippet">LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,</p><p class="snippet">          intercept_scaling=1, max_iter=100, multi_class='auto',</p><p class="snippet">          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',</p><p class="snippet">          tol=0.0001, verbose=0, warm_start=False)</p></li>
				<li>Compute the accuracy of the model against the training set:<p class="snippet">model.score(df[selected_features], output)</p><p>The output will be:</p><p class="snippet">0.9533333333333334</p></li>
				<li>Construct another model using your second choice <strong class="inline">selected_features</strong> and compare the performance:<p class="snippet">selected_features = [</p><p class="snippet">    'Sepal Length', # List features here</p><p class="snippet">    'Petal Width'</p><p class="snippet">]</p><p class="snippet">model.fit(df[selected_features], output)</p><p class="snippet">model.score(df[selected_features], output)</p><p>The output will be:</p><p class="snippet">0.96</p></li>
				<li>Construct another model using all available information and compare the performance:<p class="snippet">selected_features = [</p><p class="snippet">    'Sepal Length', # List features here</p><p class="snippet">    'Sepal Width'</p><p class="snippet">]</p><p class="snippet">model.fit(df[selected_features], output)</p><p class="snippet">model.score(df[selected_features], output)</p><p>The output will be:</p><p class="snippet">0.82</p></li>
			</ol>
			<h3 id="_idParaDest-189"><a id="_idTextAnchor211"/>Activity 13: K-NN Multiclass Classifier</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Import the following packages:<p class="snippet">import struct</p><p class="snippet">import numpy as np</p><p class="snippet">import gzip</p><p class="snippet">import urllib.request</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from array import array</p><p class="snippet">from sklearn.neighbors import KNeighborsClassifier as KNN</p></li>
				<li>Load the MNIST data into memory.<p>Training images:</p><p class="snippet">with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size, rows, cols = struct.unpack("&gt;IIII", f.read(16))</p><p class="snippet">    img = np.array(array("B", f.read())).reshape((size, rows, cols))</p><p>Training labels:</p><p class="snippet">with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size = struct.unpack("&gt;II", f.read(8))</p><p class="snippet">    labels = np.array(array("B", f.read()))</p><p>Test images:</p><p class="snippet">with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size, rows, cols = struct.unpack("&gt;IIII", f.read(16))</p><p class="snippet">    img_test = np.array(array("B", f.read())).reshape((size, rows, cols))</p><p>Test labels:</p><p class="snippet">with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size = struct.unpack("&gt;II", f.read(8))</p><p class="snippet">    labels_test = np.array(array("B", f.read()))</p></li>
				<li>Visualize a sample of the data:<p class="snippet">for i in range(10):</p><p class="snippet">    plt.subplot(2, 5, i + 1)</p><p class="snippet">    plt.imshow(img[i], cmap='gray');</p><p class="snippet">    plt.title(f'{labels[i]}');</p><p class="snippet">    plt.axis('off')</p><p>The output will be:</p><div id="_idContainer431" class="IMG---Figure"><img src="image/C12622_04_81.jpg" alt="Figure 4.81: Sample images&#13;&#10;"/></div><h6>Figure 4.81: Sample images</h6></li>
				<li>Construct a K-NN classifier, with three nearest neighbors to classify the MNIST dataset. Again, to save processing power, randomly sample 5,000 images for use in training:<p class="snippet">selection = np.random.choice(len(img), 5000)</p><p class="snippet">selected_images = img[selection]</p><p class="snippet">selected_labels = labels[selection]</p></li>
				<li>In order to provide the image information to the model, we must first flatten the data out so that each image is 1 x 784 pixels in shape:<p class="snippet">selected_images = selected_images.reshape((-1, rows * cols))</p><p class="snippet">selected_images.shape</p><p>The output will be:</p><p class="snippet">(5000, 784)</p></li>
				<li>Build the three-neighbor KNN model and fit the data to the model. Note that, in this activity, we are providing 784 features or dimensions to the model, not simply 2:<p class="snippet">model = KNN(n_neighbors=3)</p><p class="snippet">model.fit(X=selected_images, y=selected_labels)</p><p>The output will be:</p><p class="snippet">KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',</p><p class="snippet">           metric_params=None, n_jobs=None, n_neighbors=3, p=2,</p><p class="snippet">           weights='uniform')</p></li>
				<li>Determine the score against the training set:<p class="snippet">model.score(X=selected_images, y=selected_labels)</p><p>The output will be:</p><p class="snippet">0.9692</p></li>
				<li>Display the first two predictions for the model against the training data:<p class="snippet">model.predict(selected_images)[:2]</p><p class="snippet">plt.subplot(1, 2, 1)</p><p class="snippet">plt.imshow(selected_images[0].reshape((28, 28)), cmap='gray');</p><p class="snippet">plt.axis('off');</p><p class="snippet">plt.subplot(1, 2, 2)</p><p class="snippet">plt.imshow(selected_images[1].reshape((28, 28)), cmap='gray');</p><p class="snippet">plt.axis('off');</p><p>The output will be as follows:</p><div id="_idContainer432" class="IMG---Figure"><img src="image/C12622_04_82.jpg" alt="Figure 4.82: First predicted values&#13;&#10;"/></div><h6>Figure 4.82: First predicted values</h6></li>
				<li>Compare the performance against the test set:<p class="snippet">model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)</p><p>The output will be:</p><p class="snippet">0.9376</p></li>
			</ol>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor212"/>Chapter 5: Ensemble Modeling</h2>
			<h3 id="_idParaDest-191"><a id="_idTextAnchor213"/>Activity 14: Stacking with Standalone and Ensemble Algorithms</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Import the relevant libraries:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import seaborn as sns</p><p class="snippet">%matplotlib inline</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">from sklearn.metrics import mean_absolute_error</p><p class="snippet">from sklearn.model_selection import KFold</p><p class="snippet">from sklearn.linear_model import LinearRegression</p><p class="snippet">from sklearn.tree import DecisionTreeRegressor</p><p class="snippet">from sklearn.neighbors import KNeighborsRegressor</p><p class="snippet">from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor</p></li>
				<li>Read the data and print the first five rows:<p class="snippet">data = pd.read_csv('house_prices.csv')</p><p class="snippet">data.head()</p><p>The output will be as follows:</p><div id="_idContainer433" class="IMG---Figure"><img src="image/C12622_05_19.jpg" alt="Figure 5.19: The first 5 rows&#13;&#10;"/></div><h6>Figure 5.19: The first 5 rows</h6></li>
				<li>Preprocess the dataset to remove null values and one-hot encode categorical variables to prepare the data for modeling.<p>First, we remove all columns where more than 10% of the values are null. To do this, calculate the fraction of missing values by using the <strong class="inline">.isnull()</strong> method to get a mask DataFrame and apply the <strong class="inline">.mean()</strong> method to get the fraction of null values in each column. Multiply the result by 100 to get the series as percentage values.</p><p>Then, find the subset of the series having a percentage value lower than 10 and save the index (which will give us the column names) as a list. Print the list to see the columns we get:</p><p class="snippet">perc_missing = data.isnull().mean()*100</p><p class="snippet">cols = perc_missing[perc_missing &lt; 10].index.tolist() </p><p class="snippet">cols</p><p>The output will be:</p><div id="_idContainer434" class="IMG---Figure"><img src="image/C12622_05_20.jpg" alt="Figure 5.20: Output of preprocessing the dataset&#13;&#10;"/></div><h6>Figure 5.20: Output of preprocessing the dataset</h6><p>As the first column is <strong class="inline">id</strong>, we will exclude this column as well, since it will not add any value to the model.</p><p>We will subset the data to include all columns in the <strong class="inline">col</strong> list except the first element, which is <strong class="inline">id</strong>:</p><p class="snippet">data = data.loc[:, cols[1:]]</p><p>For the categorical variables, we replace null values with a string, <strong class="inline">NA</strong>, and one-hot encode the columns using pandas' <strong class="inline">.get_dummies()</strong> method, while for the numerical variables we will replace the null values with <strong class="inline">-1</strong>. Then, we combine the numerical and categorical columns to get the final DataFrame:</p><p class="snippet">data_obj = pd.get_dummies(data.select_dtypes(include=[np.object]).fillna('NA'))</p><p class="snippet">data_num = data.select_dtypes(include=[np.number]).fillna(-1)</p><p class="snippet">data_final = pd.concat([data_obj, data_num], axis=1)</p></li>
				<li>Divide the dataset into train and validation DataFrames.<p>We use scikit-learn's <strong class="inline">train_test_split()</strong> method to divide the final DataFrame into training and validation sets in the ratio 4:1. We further split each of the two sets into their respective <strong class="inline">x</strong> and <strong class="inline">y</strong> values to represent the features and target variable respectively:</p><p class="snippet">train, val = train, val = train_test_split(data_final, test_size=0.2, random_state=11)</p><p class="snippet">x_train = train.drop(columns=['SalePrice'])</p><p class="snippet">y_train = train['SalePrice'].values</p><p class="snippet">x_val = val.drop(columns=['SalePrice'])</p><p class="snippet">y_val = val['SalePrice'].values</p></li>
				<li>Initialize dictionaries in which to store train and validation MAE values. We will create two dictionaries, in which we will store the MAE values on the train and validation datasets:<p class="snippet">train_mae_values, val_mae_values = {}, {}</p></li>
				<li>Train a decision tree model and save the scores. We will use scikit-learn's <strong class="inline">DecisionTreeRegressor</strong> class to train a regression model using a single decision tree:<p class="snippet"># Decision Tree</p><p class="snippet">dt_params = {</p><p class="snippet">    'criterion': 'mae',</p><p class="snippet">    'min_samples_leaf': 10,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p><p class="snippet">dt = DecisionTreeRegressor(**dt_params)</p><p class="snippet">dt.fit(x_train, y_train)</p><p class="snippet">dt_preds_train = dt.predict(x_train)</p><p class="snippet">dt_preds_val = dt.predict(x_val)</p><p class="snippet">train_mae_values['dt'] = mean_absolute_error(y_true=y_train, y_pred=dt_preds_train)</p><p class="snippet">val_mae_values['dt'] = mean_absolute_error(y_true=y_val, y_pred=dt_preds_val)</p></li>
				<li>Train a k-nearest neighbors model and save the scores. We will use scikit-learn's <strong class="inline">kNeighborsRegressor</strong> class to train a regression model with <em class="italics">k=5</em>:<p class="snippet"># k-Nearest Neighbors</p><p class="snippet">knn_params = {</p><p class="snippet">    'n_neighbors': 5</p><p class="snippet">}</p><p class="snippet">knn = KNeighborsRegressor(**knn_params)</p><p class="snippet">knn.fit(x_train, y_train)</p><p class="snippet">knn_preds_train = knn.predict(x_train)</p><p class="snippet">knn_preds_val = knn.predict(x_val)</p><p class="snippet">train_mae_values['knn'] = mean_absolute_error(y_true=y_train, y_pred=knn_preds_train)</p><p class="snippet">val_mae_values['knn'] = mean_absolute_error(y_true=y_val, y_pred=knn_preds_val)</p></li>
				<li>Train a Random Forest model and save the scores. We will use scikit-learn's <strong class="inline">RandomForestRegressor</strong> class to train a regression model using bagging:<p class="snippet"># Random Forest</p><p class="snippet">rf_params = {</p><p class="snippet">    'n_estimators': 50,</p><p class="snippet">    'criterion': 'mae',</p><p class="snippet">    'max_features': 'sqrt',</p><p class="snippet">    'min_samples_leaf': 10,</p><p class="snippet">    'random_state': 11,</p><p class="snippet">    'n_jobs': -1</p><p class="snippet">}</p><p class="snippet">rf = RandomForestRegressor(**rf_params)</p><p class="snippet">rf.fit(x_train, y_train)</p><p class="snippet">rf_preds_train = rf.predict(x_train)</p><p class="snippet">rf_preds_val = rf.predict(x_val)</p><p class="snippet">train_mae_values['rf'] = mean_absolute_error(y_true=y_train, y_pred=rf_preds_train)</p><p class="snippet">val_mae_values['rf'] = mean_absolute_error(y_true=y_val, y_pred=rf_preds_val)</p></li>
				<li>Train a gradient boosting model and save the scores. We will use scikit-learn's <strong class="inline">GradientBoostingRegressor</strong> class to train a boosted regression model:<p class="snippet"># Gradient Boosting</p><p class="snippet">gbr_params = {</p><p class="snippet">    'n_estimators': 50,</p><p class="snippet">    'criterion': 'mae',</p><p class="snippet">    'max_features': 'sqrt',</p><p class="snippet">    'max_depth': 3,</p><p class="snippet">    'min_samples_leaf': 5,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p><p class="snippet">gbr = GradientBoostingRegressor(**gbr_params)</p><p class="snippet">gbr.fit(x_train, y_train)</p><p class="snippet">gbr_preds_train = gbr.predict(x_train)</p><p class="snippet">gbr_preds_val = gbr.predict(x_val)</p><p class="snippet">train_mae_values['gbr'] = mean_absolute_error(y_true=y_train, y_pred=gbr_preds_train)</p><p class="snippet">val_mae_values['gbr'] = mean_absolute_error(y_true=y_val, y_pred=gbr_preds_val)</p></li>
				<li>Prepare the training and validation datasets with the four meta estimators having the same hyperparameters that were used in the previous steps. We will create a <strong class="inline">num_base_predictors</strong> variable that represents the number of base estimators we have in the stacked model to help calculate the shape of the datasets for training and validation. This step can be coded almost identically to the exercise in the chapter, with a different number (and type) of base estimators.</li>
				<li>First, we create a new training set with additional columns for predictions from base predictors, in the same way as was done previously:<p class="snippet">num_base_predictors = len(train_mae_values) # 4</p><p class="snippet">x_train_with_metapreds = np.zeros((x_train.shape[0], x_train.shape[1]+num_base_predictors))</p><p class="snippet">x_train_with_metapreds[:, :-num_base_predictors] = x_train</p><p class="snippet">x_train_with_metapreds[:, -num_base_predictors:] = -1</p><p>Then, we train the base models using the k-fold strategy. We save the predictions in each iteration in a list, and iterate over the list to assign the predictions to the columns in that fold:</p><p class="snippet">kf = KFold(n_splits=5, random_state=11)</p><p class="snippet">for train_indices, val_indices in kf.split(x_train):</p><p class="snippet">    kfold_x_train, kfold_x_val = x_train.iloc[train_indices], x_train.iloc[val_indices]</p><p class="snippet">    kfold_y_train, kfold_y_val = y_train[train_indices], y_train[val_indices]</p><p class="snippet">    </p><p class="snippet">    predictions = []</p><p class="snippet">    </p><p class="snippet">    dt = DecisionTreeRegressor(**dt_params)</p><p class="snippet">    dt.fit(kfold_x_train, kfold_y_train)</p><p class="snippet">    predictions.append(dt.predict(kfold_x_val))</p><p class="snippet">    knn = KNeighborsRegressor(**knn_params)</p><p class="snippet">    knn.fit(kfold_x_train, kfold_y_train)</p><p class="snippet">    predictions.append(knn.predict(kfold_x_val))</p><p class="snippet">    gbr = GradientBoostingRegressor(**gbr_params)</p><p class="snippet">    rf.fit(kfold_x_train, kfold_y_train)</p><p class="snippet">    predictions.append(rf.predict(kfold_x_val))</p><p class="snippet">    gbr = GradientBoostingRegressor(**gbr_params)</p><p class="snippet">    gbr.fit(kfold_x_train, kfold_y_train)</p><p class="snippet">    predictions.append(gbr.predict(kfold_x_val))</p><p class="snippet">    </p><p class="snippet">    for i, preds in enumerate(predictions):</p><p class="snippet">        x_train_with_metapreds[val_indices, -(i+1)] = preds</p><p>After that, we create a new validation set with additional columns for predictions from base predictors:</p><p class="snippet">x_val_with_metapreds = np.zeros((x_val.shape[0], x_val.shape[1]+num_base_predictors))</p><p class="snippet">x_val_with_metapreds[:, :-num_base_predictors] = x_val</p><p class="snippet">x_val_with_metapreds[:, -num_base_predictors:] = -1</p></li>
				<li>Lastly, we fit the base models on the complete training set to get meta features for the validation set:<p class="snippet">predictions = []</p><p class="snippet">    </p><p class="snippet">dt = DecisionTreeRegressor(**dt_params)</p><p class="snippet">dt.fit(x_train, y_train)</p><p class="snippet">predictions.append(dt.predict(x_val))</p><p class="snippet">knn = KNeighborsRegressor(**knn_params)</p><p class="snippet">knn.fit(x_train, y_train)</p><p class="snippet">predictions.append(knn.predict(x_val))</p><p class="snippet">gbr = GradientBoostingRegressor(**gbr_params)</p><p class="snippet">rf.fit(x_train, y_train)</p><p class="snippet">predictions.append(rf.predict(x_val))</p><p class="snippet">gbr = GradientBoostingRegressor(**gbr_params)</p><p class="snippet">gbr.fit(x_train, y_train)</p><p class="snippet">predictions.append(gbr.predict(x_val))</p><p class="snippet">for i, preds in enumerate(predictions):</p><p class="snippet">    x_val_with_metapreds[:, -(i+1)] = preds</p></li>
				<li>Train a linear regression model as the stacked model. To train the stacked model, we train the logistic regression model on all the columns of the training dataset, plus the meta predictions from the base estimators. We then use the final predictions to calculate the MAE values, which we store in the same <strong class="inline">train_mae_values</strong> and <strong class="inline">val_mae_values</strong> dictionaries:<p class="snippet">lr = LinearRegression(normalize=False)</p><p class="snippet">lr.fit(x_train_with_metapreds, y_train)</p><p class="snippet">lr_preds_train = lr.predict(x_train_with_metapreds)</p><p class="snippet">lr_preds_val = lr.predict(x_val_with_metapreds)</p><p class="snippet">train_mae_values['lr'] = mean_absolute_error(y_true=y_train, y_pred=lr_preds_train)</p><p class="snippet">val_mae_values['lr'] = mean_absolute_error(y_true=y_val, y_pred=lr_preds_val)</p></li>
				<li>Visualize the train and validation errors for each individual model and the stacked model. Then, we will convert the dictionaries into two series and combine them to form two columns of a Pandas DataFrame:<p class="snippet">mae_scores = pd.concat([pd.Series(train_mae_values, name='train'), </p><p class="snippet">                        pd.Series(val_mae_values, name='val')], </p><p class="snippet">                       axis=1)</p><p class="snippet">mae_scores</p><p>The output will be as follows:</p><div id="_idContainer435" class="IMG---Figure"><img src="image/C12622_05_21.jpg" alt="Figure 5.21: The train and validation errors for each individual model and the stacked model&#13;&#10;"/></div><h6>Figure 5.21: The train and validation errors for each individual model and the stacked model</h6></li>
				<li>We then plot a bar chart from this DataFrame to visualize the MAE values for the train and validation sets using each model:<p class="snippet">mae_scores.plot(kind='bar', figsize=(10,7))</p><p class="snippet">plt.ylabel('MAE')</p><p class="snippet">plt.xlabel('Model')</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer436" class="IMG---Figure">
					<img src="image/C12622_05_22.jpg" alt="Figure 5.22: Bar chart visualizing the MAE values&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 5.22: Bar chart visualizing the MAE values</h6>
			<p>As we can see in the plot, the linear regression stacked model has the lowest value of mean absolute error on both training and validation datasets, even compared to the other ensemble models (Random Forest and gradient boosted regressor).</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor214"/>Chapter 6: Model Evaluation</h2>
			<h3 id="_idParaDest-193"><a id="_idTextAnchor215"/>Activity 15: Final Test Project</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Import the relevant libraries:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import json</p><p class="snippet">%matplotlib inline</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.preprocessing import OneHotEncoder</p><p class="snippet">from sklearn.model_selection import RandomizedSearchCV, train_test_split</p><p class="snippet">from sklearn.ensemble import GradientBoostingClassifier</p><p class="snippet">from sklearn.metrics import (accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve)</p></li>
				<li>Read the <strong class="inline">attrition_train.csv</strong> dataset. Read the CSV file into a DataFrame and print the <strong class="inline">.info()</strong> of the DataFrame:<p class="snippet">data = pd.read_csv('attrition_train.csv')</p><p class="snippet">data.info()</p><p>The output will be as follows:</p><div id="_idContainer437" class="IMG---Figure"><img src="image/C12622_06_33.jpg" alt="Figure 6.33: Output of info()&#13;&#10;"/></div><h6>Figure 6.33: Output of info()</h6></li>
				<li>Read the JSON file with the details of the categorical variables. The JSON file contains a dictionary, where the keys are the column names of the categorical features and the corresponding values are the list of categories in the feature. This file will help us one-hot encode the categorical features into numerical features. Use the <strong class="inline">json</strong> library to load the file object into a dictionary, and print the dictionary:<p class="snippet">with open('categorical_variable_values.json', 'r') as f:</p><p class="snippet">    cat_values_dict = json.load(f)</p><p class="snippet">cat_values_dict</p><p>The output will be as follows:</p><div id="_idContainer438" class="IMG---Figure"><img src="image/C12622_06_34.jpg" alt="Figure 6.34: The JSON file&#13;&#10;"/></div><h6>Figure 6.34: The JSON file</h6></li>
				<li>Process the dataset to convert all features to numerical values. First, find the number of columns that will stay in their original form (that is, numerical features) and that need to be one-hot encoded (that is, the categorical features). <strong class="inline">data.shape[1]</strong> gives us the number of columns in <strong class="inline">data</strong>, and we subtract <strong class="inline">len(cat_values_dict)</strong> from it to get the number of numerical columns. To find the number of categorical columns, we simply count the total number of categories across all categorical variables from the <strong class="inline">cat_values_dict</strong> dictionary:<p class="snippet">num_orig_cols = data.shape[1] - len(cat_values_dict)</p><p class="snippet">num_enc_cols = sum([len(cats) for cats in cat_values_dict.values()])</p><p class="snippet">print(num_orig_cols, num_enc_cols)</p><p>The output will be:</p><p class="snippet">26 24</p><p>Create a NumPy array of zeros as a placeholder, with a shape equal to the total number of columns, as determined previously, minus one (because the <strong class="inline">Attrition</strong> target variable is also included here). For the numerical columns, we then create a mask that selects the numerical columns from the DataFrame and assigns them to the first <strong class="inline">num_orig_cols-1</strong> columns in the array, <strong class="inline">X</strong>:</p><p class="snippet">X = np.zeros(shape=(data.shape[0], num_orig_cols+num_enc_cols-1))</p><p class="snippet">mask = [(each not in cat_values_dict and each != 'Attrition') for each in data.columns]</p><p class="snippet">X[:, :num_orig_cols-1] = data.loc[:, data.columns[mask]]</p><p>Next, we initialize the <strong class="inline">OneHotEncoder</strong> class from scikit-learn with a list containing the list of values in each categorical column. Then, we transform the categorical columns to one-hot encoded columns and assign them to the remaining columns in <strong class="inline">X</strong>, and save the values of the target variable in the <strong class="inline">y</strong> variable:</p><p class="snippet">cat_cols = list(cat_values_dict.keys())</p><p class="snippet">cat_values = [cat_values_dict[col] for col in data[cat_cols].columns]</p><p class="snippet">ohe = OneHotEncoder(categories=cat_values, sparse=False, )</p><p class="snippet">X[:, num_orig_cols-1:] = ohe.fit_transform(X=data[cat_cols])</p><p class="snippet">y = data.Attrition.values</p><p class="snippet">print(X.shape)</p><p class="snippet">print(y.shape)</p><p>The output will be:</p><p class="snippet">(1176, 49)</p><p class="snippet">(1176,)</p></li>
				<li>Choose a base model and define the range of hyperparameter values corresponding to the model to be searched over for hyperparameter tuning. Let's use a gradient boosted classifier as our model. We then define ranges of values for all hyperparameters we want to tune in the form of a dictionary:<p class="snippet">meta_gbc = GradientBoostingClassifier()</p><p class="snippet">param_dist = {</p><p class="snippet">    'n_estimators': list(range(10, 210, 10)),</p><p class="snippet">    'criterion': ['mae', 'mse'],</p><p class="snippet">    'max_features': ['sqrt', 'log2', 0.25, 0.3, 0.5, 0.8, None],</p><p class="snippet">    'max_depth': list(range(1, 10)),</p><p class="snippet">    'min_samples_leaf': list(range(1, 10))</p><p class="snippet">}</p></li>
				<li>Define the parameters with which to initialize the <strong class="inline">RandomizedSearchCV</strong> object and use K-fold cross-validation to find the best model hyperparameters. Define the parameters required for random search, including <strong class="inline">cv</strong> as <strong class="inline">5</strong>, indicating that the hyperparameters should be chosen by evaluating the performance using 5-fold cross-validation. Then, initialize the <strong class="inline">RandomizedSearchCV</strong> object and use the <strong class="inline">.fit()</strong> method to begin the optimization:<p class="snippet">rand_search_params = {</p><p class="snippet">    'param_distributions': param_dist,</p><p class="snippet">    'scoring': 'accuracy',</p><p class="snippet">    'n_iter': 100,</p><p class="snippet">    'cv': 5,</p><p class="snippet">    'return_train_score': True,</p><p class="snippet">    'n_jobs': -1,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p><p class="snippet">random_search = RandomizedSearchCV(meta_gbc, **rand_search_params)</p><p class="snippet">random_search.fit(X, y)</p><p>The output will be as follows:</p><div id="_idContainer439" class="IMG---Figure"><img src="image/C12622_06_35.jpg" alt="Figure 6.35: Output of the optimization process&#13;&#10;"/></div><h6>Figure 6.35: Output of the optimization process</h6><p>Once the tuning is complete, find the position (iteration number) at which the highest mean test score was obtained. Find the corresponding hyperparameters and save them to a dictionary:</p><p class="snippet">idx = np.argmax(random_search.cv_results_['mean_test_score'])</p><p class="snippet">final_params = random_search.cv_results_['params'][idx]</p><p class="snippet">final_params</p><p>The output will be:</p><div id="_idContainer440" class="IMG---Figure"><img src="image/C12622_06_36.jpg" alt="Figure 6.36: The hyperparameters dictionary&#13;&#10;"/></div><h6>Figure 6.36: The hyperparameters dictionary</h6></li>
				<li>Split the dataset into training and validation sets and train a new model using the final hyperparameters on the training dataset. Use scikit-learn's <strong class="inline">train_test_split()</strong> method to split <strong class="inline">X</strong> and <strong class="inline">y</strong> into train and test components, with test comprising 15% of the dataset:<p class="snippet">train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.15, random_state=11)</p><p class="snippet">print(train_X.shape, train_y.shape, val_X.shape, val_y.shape)</p><p>The output will be:</p><p class="snippet">((999, 49), (999,), (177, 49), (177,))</p><p>Train the gradient boosted classification model using the final hyperparameters and make predictions on the training and validation sets. Also calculate the probability on the validation set:</p><p class="snippet">gbc = GradientBoostingClassifier(**final_params)</p><p class="snippet">gbc.fit(train_X, train_y)</p><p class="snippet">preds_train = gbc.predict(train_X)</p><p class="snippet">preds_val = gbc.predict(val_X)</p><p class="snippet">pred_probs_val = np.array([each[1] for each in gbc.predict_proba(val_X)])</p></li>
				<li>Calculate the accuracy, precision, and recall for predictions on the validation set, and print the confusion matrix:<p class="snippet">print('train accuracy_score = {}'.format(accuracy_score(y_true=train_y, y_pred=preds_train)))</p><p class="snippet">print('validation accuracy_score = {}'.format(accuracy_score(y_true=val_y, y_pred=preds_val)))</p><p class="snippet">print('confusion_matrix: \n{}'.format(confusion_matrix(y_true=val_y, y_pred=preds_val)))</p><p class="snippet">print('precision_score = {}'.format(precision_score(y_true=val_y, y_pred=preds_val)))</p><p class="snippet">print('recall_score = {}'.format(recall_score(y_true=val_y, y_pred=preds_val)))</p><p>The output will be as follows:</p><div id="_idContainer441" class="IMG---Figure"><img src="image/C12622_06_37.jpg" alt="Figure 6.37: Accuracy, precision, recall, and the confusion matrix&#13;&#10;"/></div><h6>Figure 6.37: Accuracy, precision, recall, and the confusion matrix</h6></li>
				<li>Experiment with varying thresholds to find the optimal point with high recall.<p>Plot the precision-recall curve:</p><p class="snippet">plt.figure(figsize=(10,7))</p><p class="snippet">precision, recall, thresholds = precision_recall_curve(val_y, pred_probs_val)</p><p class="snippet">plt.plot(recall, precision)</p><p class="snippet">plt.xlabel('Recall')</p><p class="snippet">plt.ylabel('Precision')</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer442" class="IMG---Figure"><img src="image/C12622_06_38.jpg" alt="Figure 6.38: The precision-recall curve&#13;&#10;"/></div><h6>Figure 6.38: The precision-recall curve</h6><p>Plot the variation in precision and recall with increasing threshold values:</p><p class="snippet">PR_variation_df = pd.DataFrame({'precision': precision, 'recall': recall}, index=list(thresholds)+[1])</p><p class="snippet">PR_variation_df.plot(figsize=(10,7))</p><p class="snippet">plt.xlabel('Threshold')</p><p class="snippet">plt.ylabel('P/R values')</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer443" class="IMG---Figure"><img src="image/C12622_06_39.jpg" alt="Figure 6.39: Variation in precision and recall with increasing threshold values&#13;&#10;"/></div><h6>Figure 6.39: Variation in precision and recall with increasing threshold values</h6></li>
				<li>Finalize a threshold that will be used for predictions on the test dataset. Let's finalize a value, say, 0.3. This value is entirely dependent on what you feel would be optimal based on your exploration in the previous step:<p class="snippet">final_threshold = 0.3</p></li>
				<li>Read and process the test dataset to convert all features to numerical values. This will be done in a manner similar to that in <em class="italics">step 4</em>, with the only difference that we don't need to account for the target variable column, as the dataset does not contain it:<p class="snippet">test = pd.read_csv('attrition_test.csv')</p><p class="snippet">test.info()</p><p class="snippet">num_orig_cols = test.shape[1] - len(cat_values_dict)</p><p class="snippet">num_enc_cols = sum([len(cats) for cats in cat_values_dict.values()])</p><p class="snippet">print(num_orig_cols, num_enc_cols)</p><p class="snippet">test_X = np.zeros(shape=(test.shape[0], num_orig_cols+num_enc_cols))</p><p class="snippet">mask = [(each not in cat_values_dict) for each in test.columns]</p><p class="snippet">test_X[:, :num_orig_cols] = test.loc[:, test.columns[mask]]</p><p class="snippet">cat_cols = list(cat_values_dict.keys())</p><p class="snippet">cat_values = [cat_values_dict[col] for col in test[cat_cols].columns]</p><p class="snippet">ohe = OneHotEncoder(categories=cat_values, sparse=False, )</p><p class="snippet">test_X[:, num_orig_cols:] = ohe.fit_transform(X=test[cat_cols])</p><p class="snippet">print(test_X.shape)</p></li>
				<li>Predict the final values on the test dataset and save them to a file. Use the final threshold value determined in <em class="italics">step 10</em> to find the classes for each value in the training set. Then, write the final predictions to the <strong class="inline">final_predictions.csv</strong> file:<p class="snippet">pred_probs_test = np.array([each[1] for each in gbc.predict_proba(test_X)])</p><p class="snippet">preds_test = (pred_probs_test &gt; final_threshold).astype(int)</p><p class="snippet">with open('final_predictions.csv', 'w') as f:</p><p class="snippet">    f.writelines([str(val)+'\n' for val in preds_test])</p><p>The output will be a CSV file, as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer444" class="IMG---Figure">
					<img src="image/C12622_06_40.jpg" alt="Figure 6.40: The CSV file"/>
				</div>
			</div>
			<h6>Figure 6.40: The CSV file</h6>
		</div>
	</body></html>
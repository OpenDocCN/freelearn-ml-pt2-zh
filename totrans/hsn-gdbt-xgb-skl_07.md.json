["```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets\niris = datasets.load_iris()\n```", "```py\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\n```", "```py\ndf.head()\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=2)\n```", "```py\n    from xgboost import XGBClassifier\n    ```", "```py\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    xgb = XGBClassifier(booster='gbtree', objective='multi:softprob', max_depth=6, learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)\n    ```", "```py\n    xgb.fit(X_train, y_train)\n    ```", "```py\n    y_pred = xgb.predict(X_test)\n    ```", "```py\n    score = accuracy_score(y_pred, y_test)\n    ```", "```py\n    print('Score: ' + str(score))\n    Score: 0.9736842105263158\n    ```", "```py\nX,y = datasets.load_diabetes(return_X_y=True)\n```", "```py\n    from sklearn.model_selection import cross_val_score\n    from xgboost import XGBRegressor\n    ```", "```py\n    xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror', max_depth=6, learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)\n    ```", "```py\n    scores = cross_val_score(xgb, X, y, scoring='neg_mean_squared_error', cv=5)\n    ```", "```py\n    rmse = np.sqrt(-scores)\n    print('RMSE:', np.round(rmse, 3))\n    print('RMSE mean: %0.3f' % (rmse.mean()))\n    ```", "```py\n    RMSE: [63.033 59.689 64.538 63.699 64.661]\n    RMSE mean: 63.124\n    ```", "```py\npd.DataFrame(y).describe()\n```", "```py\ndf = pd.read_csv('atlas-higgs-challenge-2014-v2.csv.gz', nrows=250000, compression='gzip')\ndf.head()\n```", "```py\ndel df[‹Weight›]\ndel df[‹KaggleSet›]\ndf = df.rename(columns={«KaggleWeight»: «Weight»})\n```", "```py\nlabel_col = df['Label']\ndel df['Label']\ndf['Label'] = label_col\n```", "```py\ndf.head()\n```", "```py\ndf.info()\n```", "```py\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 250000 entries, 0 to 249999\nData columns (total 33 columns):\n #   Column                       Non-Null Count   Dtype  \n---  ------                       --------------   -----  \n 0   EventId                      250000 non-null  int64  \n 1   DER_mass_MMC                 250000 non-null  float64\n 2   DER_mass_transverse_met_lep  250000 non-null  float64\n 3   DER_mass_vis                 250000 non-null  float64\n 4   DER_pt_h                     250000 non-null  float64\n…\n 28  PRI_jet_subleading_eta       250000 non-null  float64\n 29  PRI_jet_subleading_phi       250000 non-null  float64\n 30  PRI_jet_all_pt               250000 non-null  float64\n 31  Weight                       250000 non-null  float64\n 32  Label                        250000 non-null  object  \ndtypes: float64(30), int64(3)\nmemory usage: 62.9 MB\n```", "```py\ndf['Label'].replace(('s', 'b'), (1, 0), inplace=True)\n```", "```py\nX = df.iloc[:,1:31]\ny = df.iloc[:,-1]\n```", "```py\ndf['test_Weight'] = df['Weight'] * 550000 / len(y)\n```", "```py\ns = np.sum(df[df['Label']==1]['test_Weight'])\nb = np.sum(df[df['Label']==0]['test_Weight'])\n```", "```py\nb/s\n593.9401931492318\n```", "```py\n    import xgboost as xgb\n    ```", "```py\n    xgb_clf = xgb.DMatrix(X, y, missing=-999.0, weight=df['test_Weight'])\n    ```", "```py\n    param = {}\n    ```", "```py\n    param['objective'] = 'binary:logitraw'\n    ```", "```py\n    param['scale_pos_weight'] = b/s\n    ```", "```py\n    param['eta'] = 0.1\n    ```", "```py\n    param['max_depth'] = 6\n    ```", "```py\n    param['eval_metric'] = 'auc'\n    ```", "```py\n    plst = list(param.items())+[('eval_metric', 'ams@0.15')]\n    ```", "```py\n    watchlist = [ (xg_clf, 'train') ]\n    ```", "```py\n    num_round = 120\n    ```", "```py\n    print ('loading data end, start to boost trees')\n    bst = xgb.train( plst, xgmat, num_round, watchlist )\n    bst.save_model('higgs.model')\n    print ('finish training')\n    ```", "```py\n    [110]\ttrain-auc:0.94505\ttrain-ams@0.15:5.84830\n    [111]\ttrain-auc:0.94507\ttrain-ams@0.15:5.85186\n    [112]\ttrain-auc:0.94519\ttrain-ams@0.15:5.84451\n    [113]\ttrain-auc:0.94523\ttrain-ams@0.15:5.84007\n    [114]\ttrain-auc:0.94532\ttrain-ams@0.15:5.85800\n    [115]\ttrain-auc:0.94536\ttrain-ams@0.15:5.86228\n    [116]\ttrain-auc:0.94550\ttrain-ams@0.15:5.91160\n    [117]\ttrain-auc:0.94554\ttrain-ams@0.15:5.91842\n    [118]\ttrain-auc:0.94565\ttrain-ams@0.15:5.93729\n    [119]\ttrain-auc:0.94580\ttrain-ams@0.15:5.93562\n    finish training\n    ```"]
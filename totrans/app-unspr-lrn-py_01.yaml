- en: '*Chapter 1*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Distinguish between supervised learning and unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement k-means clustering algorithms using built-in Python packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the Silhouette Score for your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will have a look at the concept of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Have you ever been asked to take a look at some data and come up empty handed?
    Maybe you were not familiar with the dataset, or maybe you didn't even know where
    to start. This may have been extremely frustrating, and even embarrassing, depending
    on who asked you to take care of the task.
  prefs: []
  type: TYPE_NORMAL
- en: You are not alone, and, interestingly enough, there are many times the data
    itself is simply too confusing to be made sense of. As you try and figure out
    what all those numbers in your spreadsheet mean, you're most likely mimicking
    what many unsupervised algorithms do when they try to find meaning in data. The
    reality is that many datasets in the real world don't have any rhyme or reason
    to them. You will be tasked with analyzing them with little background preparation.
    Don't fret, however – this book will prepare you so that you'll never be frustrated
    again when dealing with data exploration tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For this book, we have developed some best-in-class content to help you understand
    how unsupervised algorithms work and where to use them. We'll cover some of the
    foundations of finding clusters in your data, how to reduce the size of your data
    so it's easier to understand, and how each of these sides of unsupervised learning
    can be applied in the real world. We hope you will come away from this book with
    a strong real-world understanding of unsupervised learning, the problems that
    it can solve, and those it cannot.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for joining us and we hope you enjoy the ride!
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning versus Supervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Unsupervised learning** is one of the most exciting areas of development
    in machine learning today. If you have explored machine learning bookwork before,
    you are probably familiar with the common breakout of problems in either supervised
    or unsupervised learning. **Supervised learning** encompasses the problem set
    of having a labeled dataset that can be used to either classify (for example,
    predicting smokers and non-smokers if you''re looking at a lung health dataset)
    or fit a regression line on (for example, predicting the sale price of a home
    based on how many bedrooms it has). This model most closely mirrors an intuitive
    human approach to learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to learn how to not burn your food with a basic understanding
    of cooking, you could build a dataset by putting your food on the burner and seeing
    how long it takes (input) for your food to burn (output). Eventually, as you continue
    to burn your food, you will build a mental model of when burning will occur and
    avoid it in the future. Development in supervised learning was once fast-paced
    and valuable, but it has since simmered down in recent years – many of the obstacles
    to knowing your data have already been tackled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: Differences between unsupervised and supervised learning'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.1: Differences between unsupervised and supervised learning'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conversely, unsupervised learning encompasses the problem set of having a tremendous
    amount of data that is unlabeled. Labeled data, in this case, would be data that
    has a supplied "target" outcome that you are trying to find the correlation to
    with supplied data (you know that you are looking for whether your food was burned
    in the preceding example). Unlabeled data is when you do not know what the "target"
    outcome is, and you only have supplied input data.
  prefs: []
  type: TYPE_NORMAL
- en: Building upon the previous example, imagine you were just dropped on planet
    Earth with zero knowledge of how cooking works. You are given 100 days, a stove,
    and a fridge full of food without any instructions on what to do. Your initial
    exploration of a kitchen could go in infinite directions – on day 10, you may
    finally learn how to open the fridge; on day 30, you may learn that food can go
    on the stove; and after many more days, you may unwittingly make an edible meal.
    As you can see, trying to find meaning in a kitchen devoid of adequate informational
    structure leads to very noisy data that is completely irrelevant to actually preparing
    a meal.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning can be an answer to this problem. By looking back at your
    100 days of data, **clustering** can be used to find patterns of similar days
    where a meal was produced, and you can easily review what you did on those days.
    However, unsupervised learning isn't a magical answer –simply finding clusters
    can be just as likely to help you to find pockets of similar yet ultimately useless
    data.
  prefs: []
  type: TYPE_NORMAL
- en: This challenge is what makes unsupervised learning so exciting. How can we find
    smarter techniques to speed up the process of finding clusters of information
    that are beneficial to our end goals?
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being able to find groups of similar data that exist in your dataset can be
    extremely valuable if you are trying to find its underlying meaning. If you were
    a store owner and you wanted to understand which customers are more valuable without
    a set idea of what valuable is, clustering would be a great place to start to
    find patterns in your data. You may have a few high-level ideas of what denotes
    a valuable customer, but you aren't entirely sure in the face of a large mountain
    of available data. Through clustering you can find commonalities among similar
    groups in your data. If you look more deeply at a cluster of similar people, you
    may learn that everyone in that group visits your website for longer periods of
    time than others. This can show you what the value is and also provides a clean
    sample size for future supervised learning experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following figure shows two scatterplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figures 1.2: Two distinct scatterplots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figures 1.2: Two distinct scatterplots'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following figure separates the scatterplots into two distinct clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: Scatterplots clearly showing clusters that exist in a provided
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.3: Scatterplots clearly showing clusters that exist in a provided
    dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Both figures display randomly generated number pairs (x,y coordinates) pulled
    from a Gaussian distribution. Simply by glancing at *Figure 1.2*, it should be
    plainly obvious where the clusters exist in your data – in real life, it will
    never be this easy. Now that you know that the data can be clearly separated into
    two clusters, you can start to understand what differences exist between the two
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: Rewinding a bit from where unsupervised learning fits into the larger machine
    learning environment, let's begin by understanding the building blocks of clustering.
    The most basic definition finds clusters simply as groupings of similar data as
    subsets of a larger dataset. As an example, imagine that you had a room with 10
    people in it and each person had a job either in finance or as a scientist. If
    you told all of the financial workers to stand together and all the scientists
    to do the same, you would have effectively formed two clusters based on job types.
    Finding clusters can be immensely valuable in identifying items that are more
    similar, and, on the other end of the scale, quite different from each other.
  prefs: []
  type: TYPE_NORMAL
- en: Two-Dimensional Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand this, imagine that you were given a simple 1,000-row dataset
    by your employer that had two columns of numerical data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figures 1.4: Two-dimensional raw data in a NumPy array'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figures 1.4: Two-dimensional raw data in a NumPy array'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At first glance, this dataset provides no real structure or understanding –
    confusing to say the least!
  prefs: []
  type: TYPE_NORMAL
- en: 'A **dimension** in a dataset is another way of simply counting the number of
    features available. In most organized data tables, you can view the number of
    features as the number of columns. So, using the 1,000-row dataset example of
    size (1,000 x 2), you will have 1,000 observations across two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: You begin by plotting the first column against the second column to get a better
    idea of what the data structure looks like. There will be plenty of times where
    the cause of differences between groups will prove to be underwhelming, however
    the cases that have differences that you can take action on are extremely rewarding!
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1: Identifying Clusters in Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are given two-dimensional plots. Please look at the provided two-dimensional
    graphs and identify the clusters, to drive the point home that machine learning
    is important. Without using any algorithmic approaches, identify where the clusters
    exist in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This exercise will help start to build your intuition of how we identify clusters
    using our own eyes and thought processes. As you complete the exercises, think
    of the rationale of why a group of data points should be considered a cluster
    versus a group that should not be considered a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the clusters in the following scatterplot:![Figure1.5 Two-dimensional
    scatterplot
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12626_01_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure1.5 Two-dimensional scatterplot
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The clusters are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.6: Clusters in the scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_01_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.6: Clusters in the scatterplot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Identify the clusters in the scatterplot:![Figure1.7: Two-dimensional scatterplot'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12626_01_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure1.7: Two-dimensional scatterplot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The clusters are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.8: Clusters in the scatterplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_01_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.8: Clusters in the scatterplot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Identify the clusters in the scatterplot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure1.9: Two-dimensional scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure1.9: Two-dimensional scatterplot'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The clusters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10: Clusters in the scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.10: Clusters in the scatterplot'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Most of these examples were likely quite easy for you to understand – and that's
    the point! The human brain and eyes are incredible at finding patterns in the
    real world. Within milliseconds of viewing each plot, you could tell what fitted
    together and what didn't. While it is easy for you, a computer does not have the
    ability to see and process plots in the same manner that we do. However, this
    is not always a bad thing – look back at Figure 1.10\. Were you able to find the
    six discrete clusters in the data just by looking at the plot? You probably found
    only three to four clusters in this figure, while a computer is able to see all
    six. The human brain is magnificent, but it also lacks the nuances that come within
    a strictly logic-based approach. Through algorithmic clustering, you will learn
    how to build a model that works even better than a human at these tasks!
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the algorithm in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to k-means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hopefully, by now, you can see that finding clusters is extremely valuable in
    a machine learning workflow. However, how can you actually find these clusters?
    One of the most basic yet popular approaches is by using a cluster analysis called
    **k-means clustering**. k-means works by searching for K clusters in your data
    and the workflow is actually quite intuitive – we will start with the no-math
    introduction to k-means, followed by an implementation in Python.
  prefs: []
  type: TYPE_NORMAL
- en: No-Math k-means Walkthrough
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is the no-math algorithm of k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pick K centroids (K = expected distinct # of clusters).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly place K centroids anywhere amongst your existing training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the Euclidean distance from each centroid to all the points in your
    training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training data points get grouped in with their nearest centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amongst the data points grouped into each centroid, calculate the mean data
    point and move your centroid to that location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this process until convergence, or when the membership in each group
    no longer changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'And that''s it! Here is the process laid out step-by-step with a simple cluster
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11: Original raw data charted on x,y coordinates'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.11: Original raw data charted on x,y coordinates'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Provided with the original data in Figure 1.11, we can show the iterative process
    of k-means by showing the predicted clusters in each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12: Reading from left to right – red points are randomly initialized
    centroids, and the closest data points are assigned to groupings of each centroid'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.12: Reading from left to right – red points are randomly initialized
    centroids, and the closest data points are assigned to groupings of each centroid'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: k-means Clustering In-Depth Walkthrough
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand k-means at a deeper level, let''s walk through the example given
    in the introductory section again with some of the math that supports k-means.
    The key component at play is the Euclidean distance formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13: Euclidean distance formula'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.13: Euclidean distance formula'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Centroids are randomly set at the beginning as points in your n-dimensional
    space. Each of these centers is fed into the preceding formula as (a,b), and a
    point in your space is fed in as (x,y). Distances are calculated between each
    point and the coordinates of every centroid, with the centroid the shortest distance
    away chosen as the point's group.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Random Centroids: [ (2,5) , (8,3) , (4, 5) ]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Arbitrary point x: (0, 8)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distance from point to each centroid: [ 3.61, 9.43, 5.00 ]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Point x is assigned to Centroid 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alternative Distance Metric – Manhattan Distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Euclidean distance is the most common distance metric for many machine learning
    applications and is often known colloquially as the distance metric; however,
    it is not the only, or even the best, distance metric for every situation. Another
    popular distance metric in use for clustering is **Manhattan distance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Manhattan distance is called as such because the intuition behind the metric
    is as though you were driving a car through a metropolis (such as New York City)
    that has many square blocks. Euclidean distance relies on diagonals due to it
    being based on Pythagorean theorem, while Manhattan distance constrains distance
    to only right angles. The formula for Manhattan distance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14: Manhattan distance formula'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.14: Manhattan distance formula'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, ![](img/C12626_01_Formula_01.png) are vectors as in Euclidean distance.
    Building upon our examples of Euclidean distance, where we want to find the distance
    between two points, if ![](img/C12626_01_Formula_02.png) and ![](img/C12626_01_Formula_03.png),
    then the Manhattan distance would equal ![](img/C12626_01_Formula_04.png). This
    functionality scales to any number of dimensions. In practice, Manhattan distance
    may outperform Euclidean distance when it comes to higher dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper Dimensions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding examples are clear to visualize when your data is only two-dimensional.
    This is for convenience, to help drive the point home of how k-means works and
    could lead you into a false understanding of how easy clustering is. In many of
    your own applications, your data will likely be orders of magnitude larger to
    the point that it cannot be perceived by visualization (anything beyond three
    dimensions will be imperceivable to humans). In the previous examples, you could
    mentally work out a few two-dimensional lines to separate the data into its own
    groups. At higher dimensions, you will need to be aided by a computer to find
    an n-dimensional hyperplane that adequately separates the dataset. In practice,
    this is where clustering methods such as k-means provide significant value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.15: Two-dimensional, three-dimensional, and n-dimensional plots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.15: Two-dimensional, three-dimensional, and n-dimensional plots'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next exercise, we will calculate Euclidean distance. We will use the
    `NumPy` and `Math` packages. `NumPy` is a scientific computing package for Python
    that pre-packages common mathematical functions in highly-optimized formats. By
    using a package such as `NumPy` or `Math`, we help cut down the time spent creating
    custom math functions from scratch and instead focus on developing our solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2: Calculating Euclidean Distance in Python'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will create an example point along with three sample centroids
    to help illustrate how Euclidean distance works. Understanding this distance formula
    is foundational to the rest of our work in clustering.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this exercise, we will be able to implement Euclidean distance
    from scratch and fully understand what it does to points in a feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will be using the standard Python built-in `math` package.
    There are no prerequisites for using the `math` package and it is included in
    all standard installations of Python. As the name suggests, this package is very
    useful, allowing to use a variety of basic math building blocks off the shelf,
    such as exponentials, square roots, and others:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and create a naïve formula that captures the direct
    math of Euclidean distance, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This approach is considered naïve because it performs element-wise calculations
    on your data points (slow) compared to a more real-world implementation using
    vectors and matrix math to achieve significant performance increases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the data points in Python as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the formula you created to calculate the Euclidean distance between the
    example point and each of the three centroids you were provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since Python is zero-indexed, a position of zero as the minimum in our list
    of centroid distances signals to us that the example point, x, will be assigned
    to the number one centroid of three.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This process is repeated for every point in the dataset until each point is
    assigned to a cluster. After each point is assigned, the mean point is calculated
    among all of the points within each cluster. The calculation of the mean among
    these points is the same as calculating a mean between single integers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have found clusters in your data using Euclidean distance as the
    primary metric, think back to how you did this easily in *Exercise 2*, *Calculating
    Euclidean Distance in Python*. It is very intuitive for our human minds to see
    groups of dots on a plot and determine which dots belong to discrete clusters.
    However, how do we ask a naïve computer to repeat this same task? By understanding
    this exercise, you help teach a computer an approach to forming clusters of its
    own with the notion of distance. We will build upon how we use these distance
    metrics in the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3: Forming Clusters with the Notion of Distance'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By understanding this exercise, you''ll help to teach a computer an approach
    to forming clusters of its own with the notion of distance. We will build upon
    how we use these distance metrics in this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Store the points [ (0,8), (3,8), (3,4) ] that are assigned to cluster one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the mean point between all of the points to find the new centroid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After a new centroid is calculated, you will repeat the cluster membership calculation
    seen in *Exercise 2*, *Calculating Euclidean Distance in Python*, and then the
    previous two steps to find the new cluster centroid. Eventually, the new cluster
    centroid will be the same as the one you had entering the problem, and the exercise
    will be complete. How many times this repeats depends on the data you are clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have moved the centroid location to the new mean point of (2, 6.67),
    you can compare it to the initial list of centroids you entered the problem with.
    If the new mean point is different than the centroid that is currently in your
    list, that means you have to go through another iteration of the preceding two
    exercises. Once the new mean point you calculate is the same as the centroid you
    started the problem with, you have completed a run of k-means and reached a point
    called **convergence**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will implement k-means from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4: Implementing k-means from Scratch'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will have a look at the implementation of k-means from
    scratch. This exercise relies on scikit-learn, an open-source Python package that
    enables the fast prototyping of popular machine learning models. Within scikit-learn,
    we will be using the `datasets` functionality to create a synthetic blob dataset.
    In addition to harnessing the power of scikit-learn, we will also rely on Matplotlib,
    a popular plotting library for Python that makes it easy for us to visualize our
    data. To do this, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a random cluster dataset to experiment on X = coordinate points, y
    = cluster labels, and define random centroids:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the coordinate points as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.16: Plot of the coordinates'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_01_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.16: Plot of the coordinates'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the array of `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the coordinate points with the correct cluster labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.17: Plot of the coordinates with correct cluster labels'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.17: Plot of the coordinates with correct cluster labels'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 5: Implementing k-means with Optimization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s recreate these results on our own! We will go over an example implementing
    this with some optimizations. This exercise is built on top of the previous exercise
    and should be performed in the same Jupyter notebook. For this exercise, we will
    rely on SciPy, a Python package that allows easy access to highly optimized versions
    of scientific calculations. In particular, we will be implementing Euclidean distance
    with `cdist`, the functionally of which replicates the barebones implementation
    of our distance metric in a much more efficient manner:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A non-vectorized implementation of Euclidean distance is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, implement the optimized Euclidean distance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the values of X:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the distances and choose the index of the shortest distance as a
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `k_means` function as follows and initialize k-centroids randomly.
    Repeat the process until the difference between new/old `centroids` equal `0`
    using the `while` loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Do not break this code, as it might lead to an error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Zip together the historical steps of centers and their labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first plot is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.18: First scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.18: First scatterplot'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The second plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.19: Second scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.19: Second scatterplot'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The third plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.20: Third scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.20: Third scatterplot'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see in the above figures, k-means takes an iterative approach to
    refining optimal clusters based on distance. The algorithm starts with random
    initialization and depending on the complexity of the data, quickly finds the
    separations that make the most sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering Performance: Silhouette Score'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding the performance of unsupervised learning methods is inherently
    much more difficult than supervised learning methods because, often, there is
    no clear-cut "best" solution. For supervised learning, there are many robust performance
    metrics – the most straightforward of these being accuracy in the form of comparing
    model-predicted labels to actual labels and seeing how many the model got correct.
    Unfortunately, for clustering, we do not have labels to rely on and need to build
    an understanding of how "different" our clusters are. We achieve this with the
    Silhouette Score metric. Inherent to this approach, we can also use Silhouette
    Scores to find optimal "K" numbers of clusters for our unsupervised learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: The Silhouette metric works by analyzing how well a point fits within its cluster.
    The metric ranges from -1 to 1 – If the average silhouette score across your clustering
    is one, then you will have achieved perfect clusters and there will be minimal
    confusion about which point belongs where. If you think of the plots in our last
    exercise, the Silhouette score will be much closer to one, since the blobs are
    tightly condensed and there is a fair amount of distance between each blob. This
    is very rare though – the Silhouette Score should be treated as an attempt at
    doing the best you can, since hitting one is highly unlikely.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, the Silhouette Score calculation is quite straightforward via
    the Simplified Silhouette Index (SSI), as ![](img/C12626_01_Formula_05.png) where
    ![](img/C12626_01_Formula_06.png) is the distance from point *i* to its own cluster
    centroid and ![](img/C12626_01_Formula_07.png) is the distance from point i to
    the nearest cluster centroid.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition captured here is that ![](img/C12626_01_Formula_08.png) represents
    how cohesive point *i*'s cluster is as a clear cluster, and ![](img/C12626_01_Formula_07.png)
    represents how far apart the clusters lie. We will use the optimized implementation
    of `silhouette_score` in scikit-learn for *Activity 1*, *Implementing k-means
    Clustering*. Using it is simple and only requires you to pass in the feature array
    and the predicted cluster labels from your k-means clustering method.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will use the pandas library to read a CSV. Pandas is
    a Python library that makes data wrangling easier through the use of DataFrames.
    To read data in Python, you will use `variable_name = pd.read_csv('file_name.csv',
    header=None)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6: Calculating the Silhouette Score'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we're going to learn how to calculate the Silhouette Score
    of a dataset with a fixed number of clusters. For this, we will use the Iris dataset,
    which is available at [https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset was downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
    It can be accessed at [https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06).
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the Iris data file using pandas, a package that makes data wrangling much
    easier through the use of DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate the `X` features, since we want to treat this as an unsupervised learning
    problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Bring back the `k_means` function we made earlier for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert our Iris `X` feature DataFrame to a `NumPy` matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run our `k_means` function on the Iris matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the Silhouette Score for the `PetalLengthCm` and `PetalWidthCm` columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is similar to:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this exercise, we calculated the Silhouette Score for the `PetalLengthCm`
    and `PetalWidthCm` columns of the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1: Implementing k-means Clustering'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Scenario**: You are asked in an interview to implement a k-means clustering
    algorithm from scratch to prove that you understand how it works. We will be using
    the Iris dataset provided by the UCI ML repository. The Iris dataset is a classic
    in the data science world and has features that are used to predict Iris species.
    The download location can be found later in this activity.'
  prefs: []
  type: TYPE_NORMAL
- en: For this activity, you are able to use Matplotlib, NumPy, scikit-learn metrics,
    and pandas.
  prefs: []
  type: TYPE_NORMAL
- en: By loading and reshaping data easily, you can focus more on learning k-means
    instead of writing dataloader functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Iris data columns are provided as follows for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Aim**: To truly understand how something works, you need to build it from
    scratch. Take what you have learned in the previous sections and implement k-means
    from scratch in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please open your favorite editing platform and try the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `NumPy` or the `math` package and the Euclidean distance formula and write
    a function that calculates the distance between two coordinates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a function that calculates the distance from centroids to each of the
    points in your dataset and returns the cluster membership.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write a k-means function that takes in a dataset and the number of clusters
    (K) and returns the final cluster centroids, as well as the data points that make
    up that cluster''s membership. After implementing k-means from scratch, apply
    your custom algorithm to the Iris dataset, located here: [https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset was downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
    It can be accessed at [https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Remove the classes supplied in this dataset and see if your k-means algorithm
    can group the different Iris species into their proper groups just based on plant
    characteristics!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the Silhouette Score using the scikit-learn implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Outcome: By completing this exercise, you will gain hands-on experience of
    tuning a k-means clustering algorithm for a real-world dataset. The Iris dataset
    is seen as a classic "hello world" type problem in the data science space and
    is helpful for testing foundational techniques on. Your final clustering algorithm
    should do a decent job of finding the three clusters of Iris species types that
    exist in the data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21: Expected plot of three clusters of Iris species'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_01_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.21: Expected plot of three clusters of Iris species'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 306.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have explored what clustering is and why it is important
    in a variety of data challenges. Building upon this foundation of clustering knowledge,
    you implemented k-means, which is one of the simplest yet most popular methods
    of unsupervised learning. If you have reached this summary and can repeat what
    k-means does step-by-step to your fellow classmate, good job! If not, please go
    back and review the previous material – the content only grows in complexity from
    here. From here, we will be moving on to hierarchical clustering, which, in one
    configuration, reuses the centroid learning approach that we used in k-means.
    We will build upon this approach by outlining additional clustering methodologies
    and approaches in the next chapter.
  prefs: []
  type: TYPE_NORMAL

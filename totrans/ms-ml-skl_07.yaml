- en: Chapter 7. Dimensionality Reduction with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss a technique for reducing the dimensions of
    data called **Principal Component Analysis** (**PCA**). Dimensionality reduction
    is motivated by several problems. First, it can be used to mitigate problems caused
    by the curse of dimensionality. Second, dimensionality reduction can be used to
    compress data while minimizing the amount of information that is lost. Third,
    understanding the structure of data with hundreds of dimensions can be difficult;
    data with only two or three dimensions can be visualized easily. We will use PCA
    to visualize a high-dimensional dataset in two dimensions, and build a face recognition
    system.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall from [Chapter 3](ch03.html "Chapter 3. Feature Extraction and Preprocessing"),
    *Feature Extraction and Preprocessing*, that problems involving high-dimensional
    data can be affected by the curse of dimensionality. As the dimensions of a data
    set increases, the number of samples required for an estimator to generalize increases
    exponentially. Acquiring such large data may be infeasible in some applications,
    and learning from large data sets requires more memory and processing power. Furthermore,
    the sparseness of data often increases with its dimensions. It can become more
    difficult to detect similar instances in high-dimensional space as all of the
    instances are similarly sparse.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis, also known as the Karhunen-Loeve Transform, is
    a technique used to search for patterns in high-dimensional data. PCA is commonly
    used to explore and visualize high-dimensional data sets. It can also be used
    to compress data, and process data before it is used by another estimator. PCA
    reduces a set of possibly-correlated, high-dimensional variables to a lower-dimensional
    set of linearly uncorrelated synthetic variables called **principal components**.
    The lower-dimensional data will preserve as much of the variance of the original
    data as possible.
  prefs: []
  type: TYPE_NORMAL
- en: PCA reduces the dimensions of a data set by projecting the data onto a lower-dimensional
    subspace. For example, a two dimensional data set could be reduced by projecting
    the points onto a line; each instance in the data set would then be represented
    by a single value rather than a pair of values. A three-dimensional dataset could
    be reduced to two dimensions by projecting the variables onto a plane. In general,
    an *n*-dimensional dataset can be reduced by projecting the dataset onto a *k*-dimensional
    subspace, where *k* is less than *n*. More formally, PCA can be used to find a
    set of vectors that span a subspace, which minimizes the sum of the squared errors
    of the projected data. This projection will retain the greatest proportion of
    the original data set's variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that you are a photographer for a gardening supply catalog, and that
    you are tasked with photographing a watering can. The watering can is three-dimensional,
    but the photograph is two-dimensional; you must create a two-dimensional representation
    that describes as much of the watering can as possible. The following are four
    possible pictures that you could use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An overview of PCA](img/8365OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the first photograph, the back of the watering can is visible, but the front
    cannot be seen. The second picture is angled to look directly down the spout of
    the watering can; this picture provides information about the front of the can
    that was not visible in the first photograph, but now the handle cannot be seen.
    The height of the watering can cannot be discerned from the bird's eye view of
    the third picture. The fourth picture is the obvious choice for the catalog; the
    watering can's height, top, spout, and handle are all discernible in this image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The motivation of PCA is similar; it can project data in a high-dimensional
    space to a lower-dimensional space that retains as much of the variance as possible.
    PCA rotates the data set to align with its principal components to maximize the
    variance contained within the first several principal components. Assume that
    we have the data set that is plotted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An overview of PCA](img/8365OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The instances approximately form a long, thin ellipse stretching from the origin
    to the top right of the plot. To reduce the dimensions of this data set, we must
    project the points onto a line. The following are two lines that the data could
    be projected onto. Along which line do the instances vary the most?
  prefs: []
  type: TYPE_NORMAL
- en: '![An overview of PCA](img/8365OS_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The instances vary more along the dashed line than the dotted line. In fact,
    the dashed line is the first principal component. The second principal component
    must be orthogonal to the first principal component; that is, the second principal
    component must be statistically independent, and will appear to be perpendicular
    to the first principal component when it is plotted, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An overview of PCA](img/8365OS_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each subsequent principal component preserves the maximum amount of the remaining
    variance; the only constraint is that each must be orthogonal to the other principal
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Now assume that the data set is three dimensional. The scatter plot of the points
    looks like a flat disc that has been rotated slightly about one of the axes.
  prefs: []
  type: TYPE_NORMAL
- en: '![An overview of PCA](img/8365OS_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The points can be rotated and translated such that the tilted disk lies almost
    exactly in two dimensions. The points now form an ellipse; the third dimension
    contains almost no variance and can be discarded.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is most useful when the variance in a data set is distributed unevenly across
    the dimensions. Consider a three-dimensional data set with a spherical convex
    hull. PCA cannot be used effectively with this data set because there is equal
    variance in each dimension; none of the dimensions can be discarded without losing
    a significant amount of information.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to visually identify the principal components of data sets with only
    two or three dimensions. In the next section, we will discuss how to calculate
    the principal components of high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Performing Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several terms that we must define before discussing how principal
    component analysis works.
  prefs: []
  type: TYPE_NORMAL
- en: Variance, Covariance, and Covariance Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that **variance** is a measure of how a set of values are spread out.
    Variance is calculated as the average of the squared differences of the values
    and mean of the values, as per the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variance, Covariance, and Covariance Matrices](img/8365OS_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Covariance** is a measure of how much two variables change together; it is
    a measure of the strength of the correlation between two sets of variables. If
    the covariance of two variables is zero, the variables are uncorrelated. Note
    that uncorrelated variables are not necessarily independent, as correlation is
    only a measure of linear dependence. The covariance of two variables is calculated
    using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variance, Covariance, and Covariance Matrices](img/8365OS_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If the covariance is nonzero, the sign indicates whether the variables are
    positively or negatively correlated. When two variables are positively correlated,
    one increases as the other increases. When variables are negatively correlated,
    one variable decreases relative to its mean as the other variable increases relative
    to its mean. A **covariance** **matrix** describes the covariance values between
    each pair of dimensions in a data set. The element ![Variance, Covariance, and
    Covariance Matrices](img/8365OS_07_33.jpg) indicates the covariance of the ![Variance,
    Covariance, and Covariance Matrices](img/8365OS_07_34.jpg) and ![Variance, Covariance,
    and Covariance Matrices](img/8365OS_07_35.jpg) dimensions of the data. For example,
    a covariance matrix for a three-dimensional data is given by the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variance, Covariance, and Covariance Matrices](img/8365OS_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s calculate the covariance matrix for the following data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2 | 0 | −1.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.2 | 0.2 | −1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.4 | 0.1 | −1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.9 | 0 | −1.2 |'
  prefs: []
  type: TYPE_TB
- en: 'The means of the variables are 2.125, 0.075, and -1.275\. We can then calculate
    the covariances of each pair of variables to produce the following covariance
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variance, Covariance, and Covariance Matrices](img/8365OS_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can verify our calculations using NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Eigenvectors and eigenvalues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A vector is described by a **direction** and **magnitude**, or length. An **eigenvector**
    of a matrix is a non-zero vector that satisfies the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![Eigenvectors and eigenvalues](img/8365OS_07_30.jpg)
    is an eigenvector, *A* is a square matrix, and ![Eigenvectors and eigenvalues](img/8365OS_07_31.jpg)
    is a scalar called an **eigenvalue**. The direction of an eigenvector remains
    the same after it has been transformed by *A*; only its magnitude has changed,
    as indicated by the eigenvalue; that is, multiplying a matrix by one of its eigenvectors
    is equal to scaling the eigenvector. The prefix *eigen* is the German word for
    *belonging to* or *peculiar to*; the eigenvectors of a matrix are the vectors
    that *belong* to and characterize the structure of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvectors and eigenvalues can only be derived from square matrices, and not
    all square matrices have eigenvectors or eigenvalues. If a matrix does have eigenvectors
    and eigenvalues, it will have a pair for each of its dimensions. The principal
    components of a matrix are the eigenvectors of its covariance matrix, ordered
    by their corresponding eigenvalues. The eigenvector with the greatest eigenvalue
    is the first principal component; the second principal component is the eigenvector
    with the second greatest eigenvalue, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the eigenvectors and eigenvalues of the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall that the product of *A* and any eigenvector of *A* must be equal to
    the eigenvector multiplied by its eigenvalue. We will begin by finding the eigenvalues,
    which we can find using the following characteristic equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_12.jpg)![Eigenvectors and eigenvalues](img/8365OS_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The characteristic equation states that the determinant of the matrix, that
    is, the difference between the data matrix and the product of the identity matrix
    and an eigenvalue is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Both of the eigenvalues for this matrix are equal to **-1**. We can now use
    the eigenvalues to solve the eigenvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we set the equation equal to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting our values for *A* produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can then substitute the first eigenvalue in our first eigenvalue to solve
    the eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation can be rewritten as a system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Any non-zero vector that satisfies the preceding equations, such as the following,
    can be used as an eigenvector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'PCA requires unit eigenvectors, or eigenvectors that have a length equal to
    **1**. We can normalize an eigenvector by dividing it by its norm, which is given
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The norm of our vector is equal to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This produces the following unit eigenvector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can verify that our solutions for the eigenvectors are correct using NumPy.
    The `eig` function returns a tuple of the eigenvalues and eigenvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Dimensionality reduction with Principal Component Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s use principal component analysis to reduce the following two-dimensional
    data set to one dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '| x1 | x2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.9 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.4 | 2.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.2 | 1.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.8 | 1.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 | 0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.5 | 2.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.3 | 1.1 |'
  prefs: []
  type: TYPE_TB
- en: 'The first step of PCA is to subtract the mean of each explanatory variable
    from each observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| x1 | x2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.9 - 1.17 = -0.27 | 1 - 1.3 = -0.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.4 - 1.17 = 1.23 | 2.6 - 1.3 = 1.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.2 - 1.17 = 0.03 | 1.7 - 1.3 = 0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 - 1.17 = -0.67 | -0.7 - 1.3 = 0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 - 1.17 = -0.87 | -0.7 - 1.3 = 0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.8 - 1.17 = 0.63 | 1.4 - 1.3 = 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 - 1.17 = -0.67 | 0.6 - 1.3 = -0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.3 - 1.17 = -0.87 | 0.6 - 1.3 = -0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.5 - 1.17 = 1.33 | 2.6 - 1.3 = 1.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.3 - 1.17 = 0.13 | 1.1 - 1.3 = -0.2 |'
  prefs: []
  type: TYPE_TB
- en: Next, we must calculate the principal components of the data. Recall that the
    principal components are the eigenvectors of the data's covariance matrix ordered
    by their eigenvalues. The principal components can be found using two different
    techniques. The first technique requires calculating the covariance matrix of
    the data. Since the covariance matrix will be square, we can calculate the eigenvectors
    and eigenvalues using the approach described in the previous section. The second
    technique uses singular value decomposition of the data matrix to find the eigenvectors
    and square roots of the eigenvalues of the covariance matrix. We will work through
    an example using the first technique, and then describe the second technique that
    is used by scikit-learn's implementation of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following matrix is the covariance matrix for the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the technique described in the previous section, the eigenvalues are
    1.250 and 0.034\. The following are the unit eigenvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will project the data onto the principal components. The first eigenvector
    has the greatest eigenvalue and is the first principal component. We will build
    a transformation matrix in which each column of the matrix is the eigenvector
    for a principal component. If we were reducing a five-dimensional data set to
    three dimensions, we would build a matrix with three columns. In this example,
    we will project our two-dimensional data set onto one dimension, so we will use
    only the eigenvector for the first principal component. Finally, we will find
    the dot product of the data matrix and transformation matrix. The following is
    the result of projecting our data onto the first principal component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Many implementations of PCA, including the one of scikit-learn, use singular
    value decomposition to calculate the eigenvectors and eigenvalues. SVD is given
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The columns of ![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_27.jpg)
    are called left singular vectors of the data matrix, the columns of ![Dimensionality
    reduction with Principal Component Analysis](img/8365OS_07_28.jpg) are its right
    singular vectors, and the diagonal entries of ![Dimensionality reduction with
    Principal Component Analysis](img/8365OS_07_29.jpg) are its singular values. While
    the singular vectors and values of a matrix are useful in some applications of
    signal processing and statistics, we are only interested in them as they relate
    to the eigenvectors and eigenvalues of the data matrix. Specifically, the left
    singular vectors are the eigenvectors of the covariance matrix and the diagonal
    elements of ![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_29.jpg)
    are the square roots of the eigenvalues of the covariance matrix. Calculating
    SVD is beyond the scope of this chapter; however, eigenvectors found using SVD
    should be similar to those derived from a covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Using PCA to visualize high-dimensional data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is easy to discover patterns by visualizing data with two or three dimensions.
    A high-dimensional dataset cannot be represented graphically, but we can still
    gain some insights into its structure by reducing it to two or three principal
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Collected in 1936, Fisher''s Iris data set is a collection of fifty samples
    from each of the three species of Iris: Iris setosa, Iris virginica, and Iris
    versicolor. The explanatory variables are measurements of the length and width
    of the petals and sepals of the flowers. The Iris dataset is commonly used to
    test classification models, and is included with scikit-learn. Let''s reduce the
    `iris` dataset''s four dimensions so that we can visualize it in two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we load the built-in iris data set and instantiate a `PCA` estimator.
    The `PCA` class takes a number of principal components to retain as a hyperparameter.
    Like the other estimators, `PCA` exposes a `fit_transform()` method that returns
    the reduced data matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we assemble and plot the reduced data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The reduced instances are plotted in the following figure. Each of the dataset's
    three classes is indicated by its own marker style. From this two-dimensional
    view of the data, it is clear that one of the classes can be easily separated
    from the other two overlapping classes. It would be difficult to notice this structure
    without a graphical representation. This insight can inform our choice of classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Using PCA to visualize high-dimensional data](img/8365OS_07_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Face recognition with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s apply PCA to a face-recognition problem. Face recognition is the
    supervised classification task of identifying a person from an image of his or
    her face. In this example, we will use a data set called *Our Database of Faces*
    from AT&T Laboratories, Cambridge. The data set contains ten images each of forty
    people. The images were created under different lighting conditions, and the subjects
    varied their facial expressions. The images are gray scale and 92 x 112 pixels
    in dimension. The following is an example image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Face recognition with PCA](img/8365OS_07_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: While these images are small, a feature vector that encodes the intensity of
    every pixel will have 10,304 dimensions. Training from such high-dimensional data
    could require many samples to avoid over-fitting. Instead, we will use PCA to
    compactly represent the images in terms of a small number of principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can reshape the matrix of pixel intensities for an image into a vector,
    and create a matrix of these vectors for all of the training images. Each image
    is a linear combination of this data set''s principal components. In the context
    of face recognition, these principal components are called **eigenfaces**. The
    eigenfaces can be thought of as standardized components of faces. Each face in
    the data set can be expressed as some combination of the eigenfaces, and can be
    approximated as a combination of the most important eigenfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by loading the images into `NumPy` arrays, and reshaping their matrices
    into vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We then randomly split the images into training and test sets, and fit the
    `PCA` object on the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We reduce all of the instances to 150 dimensions and train a logistic regression
    classifier. The data set contains forty classes; scikit-learn automatically creates
    binary classifiers using the one versus all strategy behind the scenes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate the performance of the classifier using cross-validation
    and a test set. The average per-class F1 score of the classifier trained on the
    full data was 0.94, but required significantly more time to train and could be
    prohibitively slow in an application with more training instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we examined the problem of dimensionality reduction. High-dimensional
    data cannot be visualized easily. High-dimensional data sets may also suffer from
    the curse of dimensionality; estimators require many samples to learn to generalize
    from high-dimensional data. We mitigated these problems using a technique called
    principal component analysis, which reduces a high-dimensional, possibly-correlated
    data set to a lower-dimensional set of uncorrelated principal components by projecting
    the data onto a lower-dimensional subspace. We used principal component analysis
    to visualize the four-dimensional Iris data set in two dimensions, and build a
    face-recognition system. In the next chapter, we will return to supervised learning.
    We will discuss an early classification algorithm called the perceptron, which
    will prepare us to discuss more advanced models in the last few chapters.
  prefs: []
  type: TYPE_NORMAL

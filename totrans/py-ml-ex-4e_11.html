<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer403">
    <h1 class="chapterNumber">11</h1>
    <h1 class="chapterTitle" id="_idParaDest-265">Categorizing Images of Clothing with Convolutional Neural Networks</h1>
    <p class="normal">The previous chapter wrapped up our coverage of the best practices for general machine learning. Starting from this chapter, we will dive into the more advanced topics of deep learning and reinforcement learning.</p>
    <p class="normal">When we deal with image classification, we usually flatten the images, get vectors of pixels, and feed them to a neural network (or another model). Although this might do the job, we lose critical spatial information. In this chapter, we will use <strong class="keyWord">Convolutional Neural Networks</strong> (<strong class="keyWord">CNNs</strong>) to extract rich and distinguishable representations<a id="_idIndexMarker1102"/> from images. You will see how CNN representations make a “9” a “9”, a “4” a “4”, a cat a cat, or a dog a dog.</p>
    <p class="normal">We will start by exploring individual building blocks in the CNN architecture. Then, we will develop a CNN classifier in PyTorch to categorize clothing images and demystify the convolutional mechanism. Finally, we will introduce data augmentation to boost the performance of CNN models.</p>
    <p class="normal">We will cover the following topics in this chapter:</p>
    <ul>
      <li class="bulletList">Getting started with CNN building blocks</li>
      <li class="bulletList">Architecting a CNN for classification</li>
      <li class="bulletList">Exploring the clothing image dataset</li>
      <li class="bulletList">Classifying clothing images with CNNs</li>
      <li class="bulletList">Boosting the CNN classifier with data augmentation</li>
      <li class="bulletList">Advancing the CNN classifier with transfer learning</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-266">Getting started with CNN building blocks</h1>
    <p class="normal">Although regular hidden <a id="_idIndexMarker1103"/>layers (the fully connected layers we have seen so far) do a good job of extracting features from data at certain levels, these representations might not be useful in differentiating images of different classes. CNNs can be used to extract richer, more distinguishable representations that, for example, make a car a car, a plane a plane, or the handwritten letters “y” and “z” recognizably a “y” and a “z,” and so on. CNNs are a type of neural network that is biologically inspired by the human visual cortex. To demystify CNNs, I will start by introducing the components of a typical CNN, including the convolutional layer, the non-linear layer, and the pooling layer.</p>
    <h2 class="heading-2" id="_idParaDest-267">The convolutional layer</h2>
    <p class="normal">The <strong class="keyWord">convolutional layer</strong> is the first layer<a id="_idIndexMarker1104"/> in a CNN, or the first few layers<a id="_idIndexMarker1105"/> in a CNN if it has multiple convolutional layers.</p>
    <p class="normal">CNNs, specifically their convolutional layers, mimic the way our visual cells work, as follows:</p>
    <ul>
      <li class="bulletList">Our visual cortex has a set of complex neuronal cells that are sensitive to specific sub-regions<a id="_idIndexMarker1106"/> of the visual field and that are called <strong class="keyWord">receptive fields</strong>. For instance, some cells only respond in the presence of vertical edges; some cells fire only when they are exposed to horizontal edges; some react more strongly when they are shown edges of a certain orientation. These cells are organized together to produce the entire visual perception, with each cell being specialized in a specific component. A convolutional layer in a CNN is composed of a set of filters that act like those cells in humans’ visual cortexes.</li>
      <li class="bulletList">A simple cell only responds when edge-like patterns are presented within its receptive sub-regions. A more complex cell is sensitive to larger sub-regions, and as a result, can respond to edge-like patterns across the entire visual field. A stack of convolutional layers is a bunch of complex cells that can detect patterns in a bigger scope.</li>
    </ul>
    <p class="normal">The convolutional layer processes input images or matrices and mimics how neural cells react to the particular regions they are attuned to by employing a convolutional operation on the input. Mathematically, it computes the <strong class="keyWord">dot product</strong> between the nodes of the convolutional layer<a id="_idIndexMarker1107"/> and individual small regions in the input layer. The small region<a id="_idIndexMarker1108"/> is the receptive field, and the nodes of the convolutional layer can be viewed as the values on a filter. As the filter<a id="_idIndexMarker1109"/> moves along on the input layer, the dot product between the filter and the current receptive field (sub-region) is computed. A new layer called the <strong class="keyWord">feature map</strong> is obtained<a id="_idIndexMarker1110"/> after the filter has convolved over all the sub-regions. Let’s look at a simple example, as follows:</p>
    <figure class="mediaobject"><img alt="A diagram of a network  Description automatically generated with low confidence" src="../Images/B21047_11_01.png"/></figure>
    <p class="packt_figref">Figure 11.1: How a feature map is generated</p>
    <p class="normal">In this example, layer <em class="italic">l</em> has 5 nodes and the filter is composed of 3 nodes [<em class="italic">w</em><sub class="subscript">1</sub>, <em class="italic">w</em><sub class="subscript">2</sub>, <em class="italic">w</em><sub class="subscript">3</sub>]. We first compute the dot product between the filter and the first three nodes in layer <em class="italic">l</em> and obtain the first node in the output feature map; then, we compute the dot product between the filter and the middle three nodes and generate the second node in the output feature map; finally, the third node is generated from the convolution on the last three nodes in layer <em class="italic">l</em>.</p>
    <p class="normal">Now, we’ll take a closer look at how convolution works in the following example:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B21047_11_02.png"/></figure>
    <p class="packt_figref">Figure 11.2: How convolution works</p>
    <p class="normal">In this example, a 3*3 filter is sliding around a 5*5 input matrix from the top-left sub-region to the bottom-right sub-region. For each sub-region, the dot product is computed using the filter. Take the top-left sub-region (in the orange rectangle) as an example: we have 1 * 1 + 1 * 0 + 1 * 1 = 2, therefore the top-left node (in the upper-left orange rectangle) in the feature map is of value 2. For the next leftmost sub-region (in the blue dash rectangle), we calculate the convolution as 1 * 1 + 1 * 1 + 1 * 1 = 3, so the value of the next node (in the upper-middle blue dash rectangle) in the resulting feature map becomes 3. At the end, a 3*3 feature map is generated as a result.</p>
    <p class="normal">So what do we use convolutional layers<a id="_idIndexMarker1111"/> for? They are actually<a id="_idIndexMarker1112"/> used to extract features such as edges and curves. The pixel in the output feature map will be of high value if the corresponding receptive field contains an edge or curve that is recognized by the filter. For instance, in the preceding example, the filter portrays a backslash-shape “\” diagonal edge; the receptive field in the blue dash rectangle contains a similar curve and hence the highest intensity 3 is created. However, the receptive field in the top-right corner does not contain such a backslash shape, hence it results in a pixel of value 0 in the output feature map. The convolutional layer acts as a curve detector or a shape detector.</p>
    <p class="normal">Also, a convolutional layer usually has multiple filters detecting different curves and shapes. In the simple preceding example, we only apply one filter and generate one feature map, which indicates how well the shape in the input image resembles the curve represented in the filter. In order to detect more patterns from the input data, we can employ more filters, such as horizontal, vertical curve, 30-degree, and right-angle shape.</p>
    <p class="normal">Additionally, we can stack several convolutional layers to produce higher-level representations such as the overall shape and contour. Chaining more layers will result in larger receptive fields that are able to capture more global patterns.</p>
    <p class="normal">Right after<a id="_idIndexMarker1113"/> each convolutional<a id="_idIndexMarker1114"/> layer, we often apply a non-linear layer.</p>
    <h2 class="heading-2" id="_idParaDest-268">The non-linear layer</h2>
    <p class="normal">The non-linear layer<a id="_idIndexMarker1115"/> is basically<a id="_idIndexMarker1116"/> the activation layer we saw in <em class="chapterRef">Chapter 6</em>, <em class="italic">Predicting Stock Prices with Artificial Neural Networks</em>. It is used to introduce non-linearity, obviously. Recall that in the convolutional layer, we only perform linear operations (multiplication and addition). And no matter how many linear hidden layers a neural network has, it will just behave as a single-layer perceptron. Hence, we need a non-linear activation right after the convolutional layer. Again, ReLU is the most popular candidate for the non-linear layer in deep neural networks.</p>
    <h2 class="heading-2" id="_idParaDest-269">The pooling layer</h2>
    <p class="normal">Normally after one<a id="_idIndexMarker1117"/> or more convolutional<a id="_idIndexMarker1118"/> layers (along with non-linear activation), we can directly use the derived features for classification. For example, we can apply a softmax layer in the multiclass classification case. But let’s do some math first.</p>
    <p class="normal">Given 28 * 28 input images, supposing that we apply 20 5 * 5 filters in the first convolutional layer, we will obtain 20 output feature maps and each feature map layer will be of size (28 – 5 + 1) * (28 – 5 + 1) = 24 * 24 = 576. This means that the number of features as inputs for the next layer increases to 11,520 (20 * 576) from 784 (28 * 28). We then apply 50 5 * 5 filters in the second convolutional layer. The size of the output grows to 50 * 20 * (24 – 5 + 1) * (24 – 5 + 1) = 400,000. This is a lot higher than our initial size of 784. We can see that the dimensionality increases dramatically with every convolutional layer before the final softmax layer. This can be problematic as it leads to overfitting easily, not to mention the cost of training such a large number of weights.</p>
    <p class="normal">To address the issue of drastically growing dimensionality, we often employ a <strong class="keyWord">pooling layer</strong> after the convolutional and non-linear layers. The pooling<a id="_idIndexMarker1119"/> layer is also called the <strong class="keyWord">downsampling layer</strong>. As you can imagine, it reduces the dimensions of the feature maps. This is done by aggregating the statistics of features over sub-regions. Typical pooling methods include:</p>
    <ul>
      <li class="bulletList">Max pooling, which takes the max values over all non-overlapping sub-regions</li>
      <li class="bulletList">Mean pooling, which takes the mean values over all non-overlapping sub-regions</li>
    </ul>
    <p class="normal">In the following example, we apply<a id="_idIndexMarker1120"/> a 2 * 2 max-pooling<a id="_idIndexMarker1121"/> filter on a 4 * 4 feature map and output a 2 * 2 one:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, number  Description automatically generated" src="../Images/B21047_11_03.png"/></figure>
    <p class="packt_figref">Figure 11.3: How max pooling works</p>
    <p class="normal">Besides dimensionality reduction, the pooling layer has another advantage: translation invariance. This means that its output doesn’t change even if the input matrix undergoes a small amount of translation. For example, if we shift the input image a couple of pixels to the left or right, as long as the highest pixels remain the same in the sub-regions, the output of the max-pooling layer will still be the same. In other words, the prediction becomes less position-sensitive with pooling layers. The following example illustrates how max pooling achieves translation invariance.</p>
    <p class="normal">Here is the 4 * 4 original image, along with the output from max pooling with a 2 * 2 filter:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, diagram, number  Description automatically generated" src="../Images/B21047_11_04.png"/></figure>
    <p class="packt_figref">Figure 11.4: The original image and the output from max pooling</p>
    <p class="normal">And if we shift the image 1 pixel to the right, we have the following shifted image and the corresponding output:</p>
    <figure class="mediaobject"><img alt="A picture containing text, diagram, number, screenshot  Description automatically generated" src="../Images/B21047_11_05.png"/></figure>
    <p class="packt_figref">Figure 11.5: The shifted image and the output</p>
    <p class="normal">We have the same output<a id="_idIndexMarker1122"/> even if we horizontally move the input image. Pooling layers<a id="_idIndexMarker1123"/> increase the robustness of image translation.</p>
    <p class="normal">You’ve now learned about all of the components of a CNN. It was easier than you thought, right? Let’s see how they compose a CNN next.</p>
    <h1 class="heading-1" id="_idParaDest-270">Architecting a CNN for classification</h1>
    <p class="normal">Putting the three types of convolutional-related layers<a id="_idIndexMarker1124"/> together, along with the fully connected layer(s), we can structure the CNN model for classification as follows:</p>
    <figure class="mediaobject"><img alt="A picture containing text, screenshot, diagram, plot  Description automatically generated" src="../Images/B21047_11_06.png"/></figure>
    <figure class="mediaobject">Figure 11.6: CNN architecture</figure>
    <p class="normal">In this example, the input images are first fed into a convolutional layer (with ReLU activation) composed of a bunch of filters. The coefficients of the convolutional filters are trainable. A well-trained initial convolutional layer is able to derive good low-level representations of the input images, which will be critical to downstream convolutional layers if there are any, and also downstream classification tasks. Each resulting feature map is then downsampled by the pooling layer.</p>
    <p class="normal">Next, the aggregated feature maps are fed into the second convolutional layer. Similarly, the second pooling layer reduces the size of the output feature maps. You can chain as many pairs of convolutional and pooling layers as you want. The second (or more, if any) convolutional layer tries to compose high-level representations, such as the overall shape and contour, through a series of low-level representations derived from previous layers.</p>
    <p class="normal">Up until this point, the feature maps are matrices. We need to flatten them into a vector before performing any downstream classification. The flattened features are just treated as the input to one or more fully connected hidden layers. We can think of a CNN as a hierarchical feature extractor on top of a regular neural network. CNNs are well suited to exploiting strong and unique features that differentiate images.</p>
    <p class="normal">The network ends up with a logistic function if we deal with a binary classification problem, a softmax function for a multiclass case, or a set of logistic functions for multi-label cases.</p>
    <p class="normal">By now, you should have a good understanding of CNNs<a id="_idIndexMarker1125"/> and should be ready to solve the clothing image classification problem. Let’s start by exploring the dataset.</p>
    <h1 class="heading-1" id="_idParaDest-271">Exploring the clothing image dataset</h1>
    <p class="normal">The clothing dataset<a id="_idIndexMarker1126"/> Fashion-MNIST (<a href="https://github.com/zalandoresearch/fashion-mnist"><span class="url">https://github.com/zalandoresearch/fashion-mnist</span></a>) is a dataset of images from Zalando (Europe’s biggest online fashion retailer). It consists of 60,000 training samples and 10,000 test samples. Each sample is a 28 * 28 grayscale image, associated with a label from the following 10 classes, each representing articles of clothing:</p>
    <ul>
      <li class="bulletList">0: T-shirt/top</li>
      <li class="bulletList">1: Trouser</li>
      <li class="bulletList">2: Pullover</li>
      <li class="bulletList">3: Dress</li>
      <li class="bulletList">4: Coat</li>
      <li class="bulletList">5: Sandal</li>
      <li class="bulletList">6: Shirt</li>
      <li class="bulletList">7: Sneaker</li>
      <li class="bulletList">8: Bag</li>
      <li class="bulletList">9: Ankle boot</li>
    </ul>
    <p class="normal">Zalando aims to make the dataset as popular as the handwritten digits MNIST dataset for benchmarking algorithms and hence calls it Fashion-MNIST.</p>
    <p class="normal">You can download the dataset from the direct links in the <em class="italic">Get the data</em> section using the GitHub link or simply import it from PyTorch, which already includes the dataset and its data loader API. We will take the latter approach, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch, torchvision</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torchvision </span><span class="hljs-con-keyword">import</span><span class="language-python"> transforms</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">image_path = </span><span class="hljs-con-string">'./'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">transform = transforms.Compose([transforms.ToTensor()])</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dataset = torchvision.datasets.FashionMNIST(root=image_path,</span>
                                                      train=True,
                                                      transform=transform,
                                                      download=True)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_dataset = torchvision.datasets.FashionMNIST(root=image_path,</span>
                                                     train=False,
                                                     transform=transform,
                                                     download=False)
</code></pre>
    <p class="normal">We just import <code class="inlineCode">torchvision</code>, a package in PyTorch that provides access to datasets, model architectures, and various image transformation<a id="_idIndexMarker1127"/> utilities for computer vision tasks.</p>
    <div class="note">
      <p class="normal">The <code class="inlineCode">torchvision</code> library includes<a id="_idIndexMarker1128"/> the following key components:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Datasets and data loaders</strong>: <code class="inlineCode">torchvision.datasets</code> provides access to standard datasets for tasks like image classification, object detection, semantic segmentation, etc. Examples include MNIST, CIFAR-10, ImageNet, <code class="inlineCode">FashionMNIST</code>, etc. <code class="inlineCode">torch.utils.data.DataLoader</code> helps in creating data loaders to efficiently load and preprocess batches of data from datasets.</li>
        <li class="bulletList"><strong class="keyWord">Transformations</strong>: <code class="inlineCode">torchvision.transforms</code> offers a variety of image transformations for data augmentation, normalization, and preprocessing. Common transformations include resizing, cropping, normalization, and more.</li>
        <li class="bulletList"><strong class="keyWord">Model architectures</strong>: <code class="inlineCode">torchvision.models</code> provides pre-trained model architectures for various computer vision tasks.</li>
        <li class="bulletList"><strong class="keyWord">Utilities</strong>: <code class="inlineCode">torchvision.utils</code> includes utility functions for visualizing images, converting images into different formats, and more. </li>
      </ul>
    </div>
    <p class="normal">The Fashion-MNIST dataset we just loaded comes with a pre-specified training and test dataset partitioning scheme. The training set is stored at <code class="inlineCode">image_path</code>. Then we convert them into Tensor format. Output these two dataset objects to obtain additional details:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(train_dataset)</span>
Dataset FashionMNIST
    Number of datapoints: 60000
    Root location: ./
    Split: Train
    StandardTransform
Transform: Compose(
    ToTensor()
   )
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(test_dataset)</span>
Dataset FashionMNIST
    Number of datapoints: 10000
    Root location: ./
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
           )
</code></pre>
    <p class="normal">As you can see, there are 60,000 training samples and 10,000 test samples.</p>
    <p class="normal">Next, we load the training set into batches of 64 samples, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torch.utils.data </span><span class="hljs-con-keyword">import</span><span class="language-python"> DataLoader</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">batch_size = </span><span class="hljs-con-number">64</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dl = DataLoader(train_dataset, batch_size, shuffle=</span><span class="hljs-con-literal">True</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">In PyTorch, <code class="inlineCode">DataLoader</code> is a utility<a id="_idIndexMarker1129"/> that provides an efficient way to load and preprocess data from a dataset during training or evaluation of machine learning models. It essentially wraps around a dataset and provides methods to iterate over batches of data. This is particularly useful when working with large datasets that do not fit entirely in memory.</p>
    <div class="note">
      <p class="normal">Key features<a id="_idIndexMarker1130"/> of DataLoader:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Batching</strong>: It automatically divides the dataset into batches of specified size, allowing you to work with mini-batches of data during training.</li>
        <li class="bulletList"><strong class="keyWord">Shuffling</strong>: You can set the <code class="inlineCode">shuffle</code> parameter to <code class="inlineCode">True</code> to shuffle the data before each epoch, which<a id="_idIndexMarker1131"/> helps in reducing bias and improving convergence.</li>
      </ul>
    </div>
    <p class="normal">Feel free to inspect the image samples and their labels from the first batch, for example:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_iter = </span><span class="hljs-con-built_in">iter</span><span class="language-python">(train_dl)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">images, labels = </span><span class="hljs-con-built_in">next</span><span class="language-python">(data_iter)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(labels)</span>
tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1, 2, 3, 2, 3, 3, 7, 9, 9, 3, 2, 4, 6, 3, 5, 5, 3, 2, 0, 0, 8, 4, 2, 8, 5, 9, 2, 4, 9, 4, 4, 3, 4, 9, 7, 2, 0, 4, 5, 4, 8, 2, 6, 7, 0, 2, 0, 6, 3, 3, 5, 6, 0, 0, 8])
</code></pre>
    <p class="normal">The label arrays do not include<a id="_idIndexMarker1132"/> class names. Hence, we define them as follows and will use them for plotting later on:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">class_names = [</span><span class="hljs-con-string">'T-shirt/top'</span><span class="language-python">, </span><span class="hljs-con-string">'</span><span class="hljs-con-string">Trouser'</span><span class="language-python">, </span><span class="hljs-con-string">'Pullover'</span><span class="language-python">, </span><span class="hljs-con-string">'Dress'</span><span class="language-python">, </span><span class="hljs-con-string">'Coat'</span><span class="language-python">, </span><span class="hljs-con-string">'Sandal'</span><span class="language-python">, </span><span class="hljs-con-string">'Shirt'</span><span class="language-python">, </span><span class="hljs-con-string">'Sneaker'</span><span class="language-python">, </span><span class="hljs-con-string">'Bag'</span><span class="language-python">, </span><span class="hljs-con-string">'Ankle boot'</span><span class="language-python">]</span>
</code></pre>
    <p class="normal">Take a look at the format of the image data as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(images[</span><span class="hljs-con-number">0</span><span class="language-python">].shape)</span>
torch.Size([1, 28, 28])
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(torch.</span><span class="hljs-con-built_in">max</span><span class="language-python">(images), torch.</span><span class="hljs-con-built_in">min</span><span class="language-python">(images))</span>
tensor(1.) tensor(0.)
</code></pre>
    <p class="normal">Each image is represented as 28 * 28 pixels, whose values are in the range <code class="inlineCode">[0, 1]</code>.</p>
    <p class="normal">Let’s now display an image as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> numpy </span><span class="hljs-con-keyword">as</span><span class="language-python"> np</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> matplotlib.pyplot </span><span class="hljs-con-keyword">as</span><span class="language-python"> plt</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">npimg = images[</span><span class="hljs-con-number">1</span><span class="language-python">].numpy()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.imshow(np.transpose(npimg, (</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">2</span><span class="language-python">, </span><span class="hljs-con-number">0</span><span class="language-python">)))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.colorbar()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.title(class_names[labels[</span><span class="hljs-con-number">1</span><span class="language-python">]])</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <div class="note">
      <p class="normal">In PyTorch, <code class="inlineCode">np.transpose(npimg, (1, 2, 0))</code> is used when visualizing images using <code class="inlineCode">matplotlib</code>. <code class="inlineCode">(1, 2, 0)</code> is a tuple representing the new order of dimensions. In PyTorch, images are represented in the format <code class="inlineCode">(channels, height, width)</code>. However, matplotlib expects images to be in the format <code class="inlineCode">(height, width, channels)</code>. <code class="inlineCode">np.transpose(npimg, (1, 2, 0))</code> is used to rearrange the dimensions of the image array to match the format that matplotlib expects.</p>
    </div>
    <p class="normal">Refer to the following sneaker image – the end result:</p>
    <figure class="mediaobject"><img alt="A green and blue color scheme  Description automatically generated with medium confidence" src="../Images/B21047_11_07.png"/></figure>
    <p class="packt_figref">Figure 11.7: A training sample from Fashion-MNIST</p>
    <p class="normal">Similarly, we display<a id="_idIndexMarker1133"/> the first 16 training samples, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">16</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.subplot(</span><span class="hljs-con-number">4</span><span class="language-python">, </span><span class="hljs-con-number">4</span><span class="language-python">, i + </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.subplots_adjust(hspace=</span><span class="hljs-con-number">.3</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.xticks([])</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.yticks([])</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    npimg = images[i].numpy()</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.imshow(np.transpose(npimg, (</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">2</span><span class="language-python">, </span><span class="hljs-con-number">0</span><span class="language-python">)), cmap=</span><span class="hljs-con-string">"Greys"</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.title(class_names[labels[i]])</span>
<span class="hljs-con-meta">...</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">Refer to the following<a id="_idIndexMarker1134"/> image for the result:</p>
    <figure class="mediaobject"><img alt="A collection of different clothes  Description automatically generated" src="../Images/B21047_11_08.png"/></figure>
    <p class="packt_figref">Figure 11.8: 16 training samples from Fashion-MNIST</p>
    <p class="normal">In the next section, we will be building our CNN model to classify these clothing images.</p>
    <h1 class="heading-1" id="_idParaDest-272">Classifying clothing images with CNNs</h1>
    <p class="normal">As mentioned, the CNN model<a id="_idIndexMarker1135"/> has two main components: the feature extractor<a id="_idIndexMarker1136"/> composed of a set of convolutional and pooling layers, and the classifier backend, similar to a regular neural network.</p>
    <p class="normal">Let’s start this project by architecting the CNN model.</p>
    <h2 class="heading-2" id="_idParaDest-273">Architecting the CNN model</h2>
    <p class="normal">We import the necessary module<a id="_idIndexMarker1137"/> and initialize a Sequential-based model:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch.nn </span><span class="hljs-con-keyword">as</span><span class="language-python"> nn</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = nn.Sequential()</span>
</code></pre>
    <p class="normal">For the convolutional extractor, we are going to use three convolutional layers. We start with the first convolutional layer with 32 small-sized 3 * 3 filters. This is implemented with the following code:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'conv1'</span><span class="language-python">,</span>
                     nn.Conv2d(in_channels=1,
                               out_channels=32,
                               kernel_size=3)
                    )
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'relu1'</span><span class="language-python">, nn.ReLU())</span>
</code></pre>
    <p class="normal">Note that we use ReLU as the activation function.</p>
    <p class="normal">The convolutional layer is followed by a max-pooling layer with a 2 * 2 filter:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'pool1'</span><span class="language-python">, nn.MaxPool2d(kernel_size=</span><span class="hljs-con-number">2</span><span class="language-python">))</span>
</code></pre>
    <p class="normal">Here comes the second convolutional layer. It has 64 3 * 3 filters and comes with a ReLU activation function as well:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'conv2'</span><span class="language-python">,</span>
                     nn.Conv2d(in_channels=32,
                               out_channels=64,
                               kernel_size=3)
                    )
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'relu2'</span><span class="language-python">, nn.ReLU())</span>
</code></pre>
    <p class="normal">The second convolutional layer is followed by another max-pooling layer with a 2 * 2 filter:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'pool2'</span><span class="language-python">, nn.MaxPool2d(kernel_size=</span><span class="hljs-con-number">2</span><span class="language-python">))</span>
</code></pre>
    <p class="normal">We continue adding the third convolutional layer. It has 128 3 * 3 filters at this time:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'conv3'</span><span class="language-python">,</span>
                     nn.Conv2d(in_channels=64,
                               out_channels=128,
                               kernel_size=3)
                    )
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'relu3'</span><span class="language-python">, nn.ReLU())</span>
</code></pre>
    <p class="normal">Let’s take a pause here and see what the resulting filter maps are. We feed a random batch (of 64 samples) into the model we have built so far:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">x = torch.rand((</span><span class="hljs-con-number">64</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">28</span><span class="language-python">, </span><span class="hljs-con-number">28</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(model(x).shape)</span>
torch.Size([64, 128, 3, 3])
</code></pre>
    <p class="normal">By providing the input shape as <code class="inlineCode">(64, 1, 28, 28)</code>, which means 64 images within the batch, and image size 28 * 28, the output has a shape of <code class="inlineCode">(64, 128, 3, 3)</code>, indicating feature maps with 128 channels and a spatial size of 3 * 3.</p>
    <p class="normal">Next, we need to flatten these small 128 * 3 * 3 spatial representations to provide features to the downstream classifier backend:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'flatten'</span><span class="language-python">, nn.Flatten())</span>
</code></pre>
    <p class="normal">As a result, we have a flattened output of shape <code class="inlineCode">(64, 1152)</code>, as computed by the following code:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(model(x).shape)</span>
torch.Size([64, 1152])
</code></pre>
    <p class="normal">For the classifier backend, we just use one hidden layer with 64 nodes:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'fc1'</span><span class="language-python">, nn.Linear(</span><span class="hljs-con-number">1152</span><span class="language-python">, </span><span class="hljs-con-number">64</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'relu4'</span><span class="language-python">, nn.ReLU())</span>
</code></pre>
    <p class="normal">The hidden layer here is the regular fully connected dense<a id="_idIndexMarker1138"/> layer, with ReLU as the activation function.</p>
    <p class="normal">Finally, the output layer has 10 nodes representing 10 different classes in our case, along with a softmax activation:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'</span><span class="hljs-con-string">fc2'</span><span class="language-python">, nn.Linear(</span><span class="hljs-con-number">64</span><span class="language-python">, </span><span class="hljs-con-number">10</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.add_module(</span><span class="hljs-con-string">'output'</span><span class="language-python">, nn.Softmax(dim = </span><span class="hljs-con-number">1</span><span class="language-python">))</span>
</code></pre>
    <p class="normal">Let’s take a look at the model architecture, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(model)</span>
Sequential(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))
  (relu1): ReLU()
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))
  (relu2): ReLU()
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
  (relu3): ReLU()
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): Linear(in_features=1152, out_features=64, bias=True)
  (relu4): ReLU()
  (fc2): Linear(in_features=64, out_features=10, bias=True)
  (output): Softmax(dim=1)
)_________________________________________________________________
</code></pre>
    <p class="normal">If you want to display each layer<a id="_idIndexMarker1139"/> in detail, including the shape of its output, and the number of its trainable parameters, you can use the <code class="inlineCode">torchsummary</code> library. You can install it via <code class="inlineCode">pip</code> and use it as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> pip install torchsummary
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torchsummary </span><span class="hljs-con-keyword">import</span><span class="language-python"> summary</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">summary(model, input_size=(</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">28</span><span class="language-python">, </span><span class="hljs-con-number">28</span><span class="language-python">), batch_size=-</span><span class="hljs-con-number">1</span><span class="language-python">, device=</span><span class="hljs-con-string">"cpu"</span><span class="language-python">)</span>
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 26, 26]             320
              ReLU-2           [-1, 32, 26, 26]               0
         MaxPool2d-3           [-1, 32, 13, 13]               0
            Conv2d-4           [-1, 64, 11, 11]          18,496
              ReLU-5           [-1, 64, 11, 11]               0
         MaxPool2d-6             [-1, 64, 5, 5]               0
            Conv2d-7            [-1, 128, 3, 3]          73,856
              ReLU-8            [-1, 128, 3, 3]               0
           Flatten-9                 [-1, 1152]               0
           Linear-10                   [-1, 64]          73,792
             ReLU-11                   [-1, 64]               0
           Linear-12                   [-1, 10]             650
          Softmax-13                   [-1, 10]               0
================================================================
Total params: 167,114
Trainable params: 167,114
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.53
Params size (MB): 0.64
Estimated Total Size (MB): 1.17
----------------------------------------------------------------
</code></pre>
    <p class="normal">As you may notice, the output from a convolutional layer is three-dimensional, where the first two are the dimensions of the feature maps and the third is the number of filters used in the convolutional layer. The size (the first two dimensions) of the max-pooling output is half of its input feature map in the example. Feature maps are downsampled by the pooling layer. You may want to see how many parameters there would be to be trained if you took out all the pooling layers. Actually, it is 4,058,314! So, the benefits of applying pooling are obvious: avoiding overfitting and reducing training costs.</p>
    <p class="normal">You may wonder why the number of convolutional filters keeps increasing over the layers. Recall that each convolutional layer attempts to capture patterns of a specific hierarchy. The first convolutional layer captures low-level patterns, such as edges, dots, and curves. Then, the subsequent layers combine those patterns extracted in previous layers to form high-level patterns, such as shapes and contours. As we move forward in these convolutional layers, there are more and more combinations of patterns to capture in most cases. As a result, we need to keep increasing (or at least not decreasing) the number of filters<a id="_idIndexMarker1140"/> in the convolutional layers.</p>
    <h2 class="heading-2" id="_idParaDest-274">Fitting the CNN model</h2>
    <p class="normal">Now it’s time to train<a id="_idIndexMarker1141"/> the model we just built.</p>
    <p class="normal">First, we compile the model with Adam as the optimizer, cross-entropy as the loss function, and classification accuracy as the metric:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">device = torch.device(</span><span class="hljs-con-string">"cuda:0"</span><span class="language-python">)</span>
# device = torch.device("cpu")
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = model.to(device)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">loss_fn = nn.CrossEntropyLoss()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">0.001</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">Here, we employ the GPU for training, so we run <code class="inlineCode">torch.device("cuda:0")</code> to specify the GPU device (the first device, with index 0) and allocate tensors on it. Opting for the CPU is a working, but comparatively slower option.</p>
    <p class="normal">Next, we train the model<a id="_idIndexMarker1142"/> by defining the following function:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">train</span><span class="language-python">(</span><span class="hljs-con-params">model, optimizer, num_epochs, train_dl</span><span class="language-python">):</span>
        for epoch in range(num_epochs):
            loss_train = 0
            accuracy_train = 0
            for x_batch, y_batch in train_dl:
                x_batch = x_batch.to(device)
                y_batch = y_batch.to(device)
                pred = model(x_batch)
                loss = loss_fn(pred, y_batch)
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                loss_train += loss.item() * y_batch.size(0)
                is_correct = (torch.argmax(pred, dim=1) ==
                                               y_batch).float()
                accuracy_train += is_correct.sum().cpu()
            loss_train /= len(train_dl.dataset)
            accuracy_train /= len(train_dl.dataset)
       
            print(f'Epoch {epoch+1} - loss: {loss_train:.4f} - accuracy:
                  {accuracy_train:.4f}')
</code></pre>
    <p class="normal">We will train the CNN model for 30 iterations and monitor the learning progress:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">num_epochs = </span><span class="hljs-con-number">30</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train(model, optimizer, num_epochs, train_dl)</span>
Epoch 1 - loss: 1.7253 - accuracy: 0.7385
Epoch 2 - loss: 1.6333 - accuracy: 0.8287
…
Epoch 10 - loss: 1.5572 - accuracy: 0.9041
…
Epoch 20 - loss: 1.5344 - accuracy: 0.9270
<span class="hljs-con-meta">...</span>
Epoch 29 - loss: 1.5249 - accuracy: 0.9362
Epoch 30 - loss: 1.5249 - accuracy: 0.9363
</code></pre>
    <p class="normal">We are able to achieve an accuracy of around 94% on the training set. If you want to check the performance on the test set, you can do the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_dl = DataLoader(test_dataset, batch_size, shuffle=</span><span class="hljs-con-literal">False</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">evaluate_model</span><span class="language-python">(</span><span class="hljs-con-params">model, test_dl</span><span class="language-python">):</span>
        accuracy_test = 0
        with torch.no_grad():
            for x_batch, y_batch in test_dl:
                pred = model.cpu()(x_batch)
                is_correct = torch.argmax(pred, dim=1) == y_batch
                accuracy_test += is_correct.float().sum().item()
        print(f'Accuracy on test set: {100 * accuracy_test / 10000} %')
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">evaluate_model(model, test_dl)</span>
Accuracy on test set: 90.25 %
</code></pre>
    <p class="normal">The model achieves an accuracy<a id="_idIndexMarker1143"/> of 90% on the test dataset. Note that this result may vary due to factors like differences in hidden layer initializations, or non-deterministic operations in GPUs.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">Operations that are better<a id="_idIndexMarker1144"/> suited to execution on a GPU compared to a CPU typically involve parallelizable tasks that benefit from the massive parallelism and computational power offered by GPU architectures. Here are some examples:</p>
      <ul>
        <li class="bulletList">Matrix and convolutional operations</li>
        <li class="bulletList">Processing large batches of data simultaneously. Tasks that involve batch processing, such as training and inference on mini-batches in machine learning models, benefit from the parallel processing capabilities of GPUs. </li>
        <li class="bulletList">Forward and backward propagation in neural networks, which are typically faster on GPUs due to hardware acceleration.</li>
      </ul>
    </div>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">Operations that are better<a id="_idIndexMarker1145"/> suited to execution on a CPU compared to a GPU typically involve less parallelizable tasks and require more sequential processing or small data sizes. Here are some examples:</p>
      <ul>
        <li class="bulletList">Preprocessing such as data loading, feature extraction, and data augmentation.</li>
        <li class="bulletList">Inference on small models. For small models or inference tasks with low computational requirements, performing operations on the CPU can be more cost-effective.</li>
        <li class="bulletList">Control flow operations. Operations<a id="_idIndexMarker1146"/> involving conditional statements or loops are generally more efficient on the CPU due to its sequential processing nature.</li>
      </ul>
    </div>
    <p class="normal">You have seen how the trained<a id="_idIndexMarker1147"/> model performs, and you may wonder what the convolutional filters look like. You will find out in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-275">Visualizing the convolutional filters</h2>
    <p class="normal">We extract the convolutional filters<a id="_idIndexMarker1148"/> from the trained model and visualize them with the following steps.</p>
    <p class="normal">From the model summary, we know that the layers of <code class="inlineCode">conv1</code>, <code class="inlineCode">conv2</code>, and <code class="inlineCode">conv3</code> in the model are convolutional layers. Using the third convolutional layer as an example, we obtain its filters as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">conv3_weight = model.conv3.weight.data</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(conv3_weight.shape) </span>
torch.Size([128, 64, 3, 3])
</code></pre>
    <p class="normal">It’s apparent that there are 128 filters, where each filter possesses dimensions of 3x3 and contains 64 channels.</p>
    <p class="normal">Next, for simplification, we visualize only the first channel from the first 16 filters in four rows and four columns:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">n_filters = </span><span class="hljs-con-number">16</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(n_filters):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    weight = conv3_weight[i].cpu().numpy()</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.subplot(</span><span class="hljs-con-number">4</span><span class="language-python">, </span><span class="hljs-con-number">4</span><span class="language-python">, i+</span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.xticks([])</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.yticks([])</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.imshow(weight[</span><span class="hljs-con-number">0</span><span class="language-python">], cmap=</span><span class="hljs-con-string">'gray'</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">Refer to the following screenshot for the end result:</p>
    <figure class="mediaobject"><img alt="A picture containing square, pattern, rectangle, symmetry  Description automatically generated" src="../Images/B21047_11_09.png"/></figure>
    <p class="packt_figref">Figure 11.9: Trained convolutional filters</p>
    <p class="normal">In a convolutional filter, the dark squares<a id="_idIndexMarker1149"/> represent small weights and the white squares indicate large weights. Based on this intuition, we can see that the second filter in the second row detects the vertical line in a receptive field, while the third filter in the first row detects a gradient from light at the bottom right to dark at the top left.</p>
    <p class="normal">In the previous example, we trained the clothing image classifier with 60,000 labeled samples. However, it is not easy to gather such a big labeled dataset in reality. Specifically, image labeling is expensive<a id="_idIndexMarker1150"/> and time-consuming. How can we effectively train an image classifier with a limited number of samples? One solution is data augmentation.</p>
    <h1 class="heading-1" id="_idParaDest-276">Boosting the CNN classifier with data augmentation</h1>
    <p class="normal"><strong class="keyWord">Data augmentation</strong> means expanding the size of an existing<a id="_idIndexMarker1151"/> training dataset<a id="_idIndexMarker1152"/> in order to improve the generalization performance. It overcomes the cost involved in collecting and labeling more data. In PyTorch, we use the <code class="inlineCode">torchvision.transforms</code> module to implement image augmentation in real time.</p>
    <h2 class="heading-2" id="_idParaDest-277">Flipping for data augmentation</h2>
    <p class="normal">There are many ways to augment<a id="_idIndexMarker1153"/> image data. The simplest one is probably flipping an image horizontally or vertically. For instance, we will have a new image if we flip an existing image horizontally. To create a horizontally flipped image, we utilize <code class="inlineCode">transforms.functional.hflip</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">image = images[</span><span class="hljs-con-number">1</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">img_flipped = transforms.functional.hflip(image)</span>
</code></pre>
    <p class="normal">Let’s take a look at the flipped image:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">display_image_greys</span><span class="language-python">(</span><span class="hljs-con-params">image</span><span class="language-python">):</span>
    npimg = image.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap="Greys")
    plt.xticks([])
    plt.yticks([])
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.figure(figsize=(</span><span class="hljs-con-number">8</span><span class="language-python">, </span><span class="hljs-con-number">8</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.subplot(</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">2</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">display_image_greys(image)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.subplot(</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">2</span><span class="language-python">, </span><span class="hljs-con-number">2</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">display_image_greys(img_flipped)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">Refer to the following screenshot for the end result:</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer screen  Description automatically generated" src="../Images/B21047_11_10.png"/></figure>
    <p class="packt_figref">Figure 11.10: Horizontally flipped image for data augmentation</p>
    <p class="normal">In training using data augmentation, we will create manipulated<a id="_idIndexMarker1154"/> images using a random generator. For horizontal flipping, we will use <code class="inlineCode">transforms.RandomHorizontalFlip</code>, which randomly flips images horizontally with a 50% chance, effectively augmenting the dataset. Let’s see three output samples:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python"> flip_transform =</span>
               transforms.Compose([transforms.RandomHorizontalFlip()])
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.figure(figsize=(</span><span class="hljs-con-number">10</span><span class="language-python">, </span><span class="hljs-con-number">10</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.subplot(</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">4</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">display_image_greys(image)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">3</span><span class="language-python">):</span>
        plt.subplot(1, 4, i+2)
        img_flip = flip_transform(image)
        display_image_greys(img_flip)
</code></pre>
    <p class="normal">Refer to the following screenshot for the end result:</p>
    <figure class="mediaobject"><img alt="A close-up of a pair of shoes  Description automatically generated" src="../Images/B21047_11_11.png"/></figure>
    <p class="packt_figref">Figure 11.11: Randomly horizontally flipped images for data augmentation</p>
    <p class="normal">As you can see, the generated images are either horizontally flipped or not flipped.</p>
    <p class="normal">In general, the horizontally flipped images convey the same message as the original ones. Vertically flipped images are not frequently seen, although you can generate them using <code class="inlineCode">transforms.RandomVerticalFlip</code>. It is also worth noting that flipping only works in orientation-insensitive cases, such as classifying cats and dogs or recognizing parts of cars. On the contrary, it is dangerous to do so in cases where orientation matters, such as classifying<a id="_idIndexMarker1155"/> between right and left turn signs.</p>
    <h2 class="heading-2" id="_idParaDest-278">Rotation for data augmentation</h2>
    <p class="normal">Instead of rotating every 90 degrees<a id="_idIndexMarker1156"/> as in horizontal or vertical flipping, a small-to-medium degree rotation can also be applied in image data augmentation. Let’s look at random rotation using <code class="inlineCode">transforms</code>. We use <code class="inlineCode">RandomRotation</code> in the following example:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">rotate_transform =</span>
            transforms.Compose([transforms. RandomRotation(20)])
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.figure(figsize=(</span><span class="hljs-con-number">10</span><span class="language-python">, </span><span class="hljs-con-number">10</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.subplot(</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">4</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">display_image_greys(image)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">3</span><span class="language-python">):</span>
        plt.subplot(1, 4, i+2)
        img_rotate = rotate_transform(image)
    display_image_greys(img_rotate)
</code></pre>
    <p class="normal">Refer to the following screenshot for the end result:</p>
    <figure class="mediaobject"><img alt="A close-up of a pair of images  Description automatically generated" src="../Images/B21047_11_12.png"/></figure>
    <p class="packt_figref">Figure 11.12: Rotated images for data augmentation</p>
    <p class="normal">In the preceding example, the image<a id="_idIndexMarker1157"/> is rotated by any degree ranging from -20 (counterclockwise) to 20 (clockwise).</p>
    <h2 class="heading-2" id="_idParaDest-279">Cropping for data augmentation</h2>
    <p class="normal">Cropping is another commonly used augmentation<a id="_idIndexMarker1158"/> method. It generates new images by selecting a segment of the original image. Typically, this process is accompanied by resizing the cropped area to a predetermined output size to ensure uniform dimensions.</p>
    <p class="normal">Now, let’s explore how to utilize <code class="inlineCode">transforms.RandomResizedCrop</code> to randomly select the aspect ratio of the cropped section and subsequently resize the result to match the original dimensions:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">crop_transform = transforms.Compose([</span>
        transforms.RandomResizedCrop(size=(28, 28), scale=(0.7, 1))])
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.figure(figsize=(</span><span class="hljs-con-number">10</span><span class="language-python">, </span><span class="hljs-con-number">10</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.subplot(</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">4</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">display_image_greys(image)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">3</span><span class="language-python">):</span>
        plt.subplot(1, 4, i+2)
        img_crop = crop_transform(image)
        display_image_greys(img_crop)
</code></pre>
    <p class="normal">Here, <code class="inlineCode">size</code> specifies the size of the output image after cropping and resizing; <code class="inlineCode">scale</code> defines the range of scaling for cropping. If set to (<code class="inlineCode">min_scale</code>, <code class="inlineCode">max_scale</code>), the crop area’s size will be randomly chosen to be between <code class="inlineCode">min_scale</code> and <code class="inlineCode">max_scale</code> times the original image’s size.</p>
    <p class="normal">Refer to the following screenshot for the end result:</p>
    <figure class="mediaobject"><img alt="A close-up of a pair of shoes  Description automatically generated" src="../Images/B21047_11_13.png"/></figure>
    <p class="packt_figref">Figure 11.13: Cropped images for data augmentation</p>
    <p class="normal">As you can see, <code class="inlineCode">scale=(0.7, 1.0)</code> indicates that the crop<a id="_idIndexMarker1159"/> area’s size can vary between 70% and 100% of the original image’s size.</p>
    <h1 class="heading-1" id="_idParaDest-280">Improving the clothing image classifier with data augmentation</h1>
    <p class="normal">Armed with several common augmentation<a id="_idIndexMarker1160"/> methods, we will now apply<a id="_idIndexMarker1161"/> them to train our image classifier on a small dataset in the following steps:</p>
    <ol>
      <li class="numberedList" value="1">We start by constructing the transform function by combining all the data augmentation techniques we just discussed:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">transform_train = transforms.Compose([</span>
                       transforms.RandomHorizontalFlip(),
                       transforms.RandomRotation(10),
                       transforms.RandomResizedCrop(size=(28, 28),
                                                    scale=(0.9, 1)),
                       transforms.ToTensor(),
    ])
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, we employ horizontal flip, rotation of up to 10 degrees, and cropping, with dimensions ranging from 90% to 100% of the original size.</p>
    <ol>
      <li class="numberedList" value="2">We reload the training dataset with this transform function and only use 500 samples for training:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dataset_aug = torchvision.datasets.FashionMNIST(</span>
<span class="language-python">                                               root=image_path,</span>
                                               train=True,
                                               transform=transform_train,
                                               download=False)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torch.utils.data </span><span class="hljs-con-keyword">import</span><span class="language-python"> Subset</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dataset_aug_small = Subset(train_dataset_aug, torch.arange(</span><span class="hljs-con-number">500</span><span class="language-python">))</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We will see how data augmentation improves generalization and performance with a very small training set available.</p>
    <ol>
      <li class="numberedList" value="3">Load this small but augmented training set into batches of 64 samples as we did previously:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dl_aug_small = DataLoader(train_dataset_aug_small,</span>
                                    batch_size, 
                                    shuffle=True)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Note that even for the same original image, iterating using this data loader will produce different augmented images, which could be flipped, rotated, or cropped within the specified ranges.</p>
    <ol>
      <li class="numberedList" value="4">Next, we initialize the CNN model<a id="_idIndexMarker1162"/> using the same architecture<a id="_idIndexMarker1163"/> we used previously and the optimizer accordingly:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = nn.Sequential()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">...(here we skip repeating the same code)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = model.to(device)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">0.001</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Now we train the model on the augmented small dataset:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train(model, optimizer, </span><span class="hljs-con-number">1000</span><span class="language-python">, train_dl_aug_small)</span>
Epoch 1 - loss: 2.3013 - accuracy: 0.1400
<span class="hljs-con-meta">...</span>
Epoch 301 - loss: 1.6817 - accuracy: 0.7760
<span class="hljs-con-meta">...</span>
Epoch 601 - loss: 1.5006 - accuracy: 0.9620
<span class="hljs-con-meta">...</span>
Epoch 1000 - loss: 1.4904 - accuracy: 0.9720
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We train the model for 1,000 iterations.</p>
    <ol>
      <li class="numberedList" value="6">Let’s see how it performs on the test set:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">evaluate_model(model, test_dl)</span>
Accuracy on test set: 79.24%
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The model with data augmentation has a classification accuracy of 79.24% on the test set. Note that this result may vary.</p>
    <p class="normal">We also experimented with training without data augmentation, resulting in a test set accuracy of approximately 76%. When employing data augmentation, the accuracy improved to 79%. As always, feel free to fine-tune the hyperparameters<a id="_idIndexMarker1164"/> as we did in <em class="chapterRef">Chapter 6</em>, <em class="italic">Predicting Stock Prices with Artificial Neural Networks</em>, and see if you can further improve the classification<a id="_idIndexMarker1165"/> performance.</p>
    <p class="normal">Transfer learning is an alternative method to enhance the performance of a CNN classifier. Let’s proceed to the following section.</p>
    <h1 class="heading-1" id="_idParaDest-281">Advancing the CNN classifier with transfer learning</h1>
    <p class="normal"><strong class="keyWord">Transfer learning </strong>is a machine learning technique<a id="_idIndexMarker1166"/> where a model trained<a id="_idIndexMarker1167"/> on one task is adapted or fine-tuned for a second related task. In transfer learning, the knowledge acquired during the training of the first task (source task) is leveraged to improve the learning of the second task (target task). This can be particularly useful when you have limited data for the target task because it allows you to transfer knowledge from a larger or more diverse dataset.</p>
    <p class="normal">The typical workflow of transfer learning involves:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Pretrained model</strong>: Start with a pretrained model<a id="_idIndexMarker1168"/> that has already been trained on a large and relevant dataset for a different but related task. This model is often a deep neural network, such as a CNN model for image tasks.</li>
      <li class="numberedList"><strong class="keyWord">Feature Extraction</strong>: Use the pretrained model as a feature extractor. Remove the final classification layers (if they exist) and use the output of one of the intermediate layers as a feature representation for your data. These features can capture high-level patterns and information from the source task.</li>
      <li class="numberedList"><strong class="keyWord">Fine-Tuning</strong>: Add new layers to the feature extractor. These new layers are specific to your target task and are typically randomly initialized. You then train the entire model, including the feature extractor and the new layers, on your target dataset. Fine-tuning allows the model to adapt to the specifics of the target task.</li>
    </ol>
    <p class="normal">Prior to implementing transfer learning for our clothing image classification task, let’s begin by exploring the evolution of CNN architectures and pretrained models. Even the early CNN architecture is still actively used today! The key point here is that all of these architectures are valuable tools in the modern<a id="_idIndexMarker1169"/> DL toolbox, particularly when employed for transfer<a id="_idIndexMarker1170"/> learning tasks.</p>
    <h2 class="heading-2" id="_idParaDest-282">Development of CNN architectures and pretrained models</h2>
    <p class="normal">The concept of CNNs<a id="_idIndexMarker1171"/> for image processing<a id="_idIndexMarker1172"/> dates back to the 1990s. Early architectures like <strong class="keyWord">LeNet-5</strong> (1998) demonstrated the potential<a id="_idIndexMarker1173"/> of deep neural networks for image classification. LeNet-5 consists of two sets of convolutional layers, followed by two fully connected layers and one output layer. Each convolutional layer uses 5x5 kernels. LeNet-5 played a significant role in demonstrating the effectiveness of deep learning for image classification tasks. It was able to achieve high accuracy on the MNIST dataset, a widely used benchmark dataset for handwritten digit recognition.</p>
    <p class="normal">The achievements of LeNet-5 paved the way for the creation <a id="_idIndexMarker1174"/>of more complex architectures, such as <strong class="keyWord">AlexNet</strong> (2012). It consists of eight layers – five sets of convolutional layers followed by three fully connected layers. It used a ReLU activation function for the first time in a deep CNN and utilized dropout in the fully connected layers to prevent overfitting. Data augmentation techniques, such as random cropping and horizontal flipping, were employed to improve the model’s generalization. The success of AlexNet triggered a renewed interest in neural networks and led to the development of even deeper and more complex architectures, including VGGNet, GoogLeNet, and ResNet, which have become foundational in computer vision.</p>
    <p class="normal"><strong class="keyWord">VGGNet</strong> was introduced by the Visual Geometry Group<a id="_idIndexMarker1175"/> at the University of Oxford in 2014. VGGNet follows a straightforward and uniform architecture. It consists of a series of convolutional layers, followed by max-pooling layers, with a stack of fully connected layers at the end. It primarily uses 3x3 convolutional filters, allowing the network to capture fine-grained spatial information. The most commonly used versions are VGG16 and VGG19, which have 16 and 19 layers in the network. They are often used as a starting point for transfer learning in various computer vision tasks.</p>
    <p class="normal">In the same year, <strong class="keyWord">GoogLeNet</strong>, better known as <strong class="keyWord">Inception</strong>, was developed<a id="_idIndexMarker1176"/> by Google. The hallmark of GoogLeNet is the inception<a id="_idIndexMarker1177"/> module. Instead of using a single convolutional layer with a fixed filter size, inception modules use multiple filter sizes (1x1, 3x3, 5x5) in parallel. These parallel operations capture features at different scales and provide richer representations. Similar to VGGNet, pretrained GoogLeNet comes in different versions, such as InceptionV1, InceptionV2, InceptionV3, and Inception V4, each with variations and improvements in architecture.</p>
    <p class="normal"><strong class="keyWord">ResNet</strong>, short for <strong class="keyWord">Residual Network</strong>, was introduced<a id="_idIndexMarker1178"/> by Kaiming He et al. in 2015, to address the vanishing gradient problem – gradients of the loss function becoming extremely small in CNNs. Its core innovation is the use of residual connections. These blocks allow the network to skip certain layers during training. Instead of directly learning the desired mapping from input to output, residual blocks learn a residual mapping, which is added to the original input. Deeper networks were made possible this way. Its pretrained models come in various versions, such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and the extremely deep ResNet-152. Again, the numbers denote the depth of the network. </p>
    <p class="normal">The development of CNN architectures<a id="_idIndexMarker1179"/> and pretrained models<a id="_idIndexMarker1180"/> continues with innovations like EfficientNet, MobileNet, and custom architectures for specific tasks. For instance, <strong class="keyWord">MobileNet</strong> models are designed to be highly<a id="_idIndexMarker1181"/> efficient in terms of computational resources and memory usage. They are tailored for deployment on devices with limited hardware capabilities, such as smartphones, IoT devices, and edge devices.</p>
    <div class="note">
      <p class="normal">You can see all available<a id="_idIndexMarker1182"/> pretrained models in PyTorch on this page: <span class="url">https://pytorch.org/vision/stable/models.html#classification</span>.</p>
    </div>
    <p class="normal">The evolution of CNN architectures and the availability of pretrained models have revolutionized computer vision tasks. They have significantly<a id="_idIndexMarker1183"/> improved the state of the art in image<a id="_idIndexMarker1184"/> classification, object detection, segmentation, and many other applications.</p>
    <p class="normal">Now, let’s explore using a pretrained model to enhance our clothing image classifier.</p>
    <h2 class="heading-2" id="_idParaDest-283">Improving the clothing image classifier by fine-tuning ResNets</h2>
    <p class="normal">We will use the<a id="_idIndexMarker1185"/> pre-trained ResNets, ResNet-18 to be<a id="_idIndexMarker1186"/> specific, for transfer learning in the following steps:</p>
    <ol>
      <li class="numberedList" value="1">We start by importing the pretrained ResNet-18 model from <code class="inlineCode">torchvision</code>:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torchvision.models </span><span class="hljs-con-keyword">import</span><span class="language-python"> resnet18</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">my_resnet = resnet18(weights=</span><span class="hljs-con-string">'IMAGENET1K_V1'</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, <code class="inlineCode">IMAGENET1K</code> refers to the pretrained model that was trained on the ImageNet-1K dataset (<a href="https://www.image-net.org/download.php"><span class="url">https://www.image-net.org/download.php</span></a>) and <code class="inlineCode">V1</code> refers to version 1 of the pretrained model.</p>
    <p class="normal-one">This is the pretrained model step.</p>
    <ol>
      <li class="numberedList" value="2">Since the ImageNet-1K dataset comprises RGB images, the first convolutional layer in the original <code class="inlineCode">ResNet</code> is designed for three-dimensional inputs. However, our <code class="inlineCode">FashionMNIST</code> dataset contains grayscale images, so we need to modify it to accept one-dimensional inputs:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">my_resnet.conv1 = nn.Conv2d(</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">64</span><span class="language-python">, kernel_size=</span><span class="hljs-con-number">7</span><span class="language-python">, stride=</span><span class="hljs-con-number">2</span><span class="language-python">, padding=</span><span class="hljs-con-number">3</span><span class="language-python">, bias=</span><span class="hljs-con-literal">False</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We just change the first argument, the input dimension, from 3 to 1 in the original definition of the first convolutional:</p>
    <pre class="programlisting con-one"><code class="hljs-con">self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
</code></pre>
    <ol>
      <li class="numberedList" value="3">Change the output layer to output 10 classes from 1,000 classes:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">num_ftrs = my_resnet.fc.in_features</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">my_resnet.fc = nn.Linear(num_ftrs, </span><span class="hljs-con-number">10</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, we only update the output size of the output layer.</p>
    <p class="normal-one">Steps 2 and 3 prepare for the <strong class="keyWord">fine-tuning</strong> process.</p>
    <ol>
      <li class="numberedList" value="4">Finally, we <strong class="keyWord">fine-tune</strong> the adapted pretrained<a id="_idIndexMarker1187"/> model by training<a id="_idIndexMarker1188"/> it on the full training set:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">my_resnet = my_resnet.to(device)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(my_resnet.parameters(), lr=</span><span class="hljs-con-number">0.001</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train(my_resnet, optimizer, </span><span class="hljs-con-number">10</span><span class="language-python">, train_dl)</span>
Epoch 1 - loss: 0.4797 - accuracy: 0.8256
Epoch 2 - loss: 0.3377 - accuracy: 0.8791
Epoch 3 - loss: 0.2921 - accuracy: 0.8944
Epoch 4 - loss: 0.2629 - accuracy: 0.9047
Epoch 5 - loss: 0.2336 - accuracy: 0.9157
Epoch 6 - loss: 0.2138 - accuracy: 0.9221
Epoch 7 - loss: 0.1911 - accuracy: 0.9301
Epoch 8 - loss: 0.1854 - accuracy: 0.9312
Epoch 9 - loss: 0.1662 - accuracy: 0.9385
Epoch 10 - loss: 0.1575 - accuracy: 0.9427
</code></pre>
      </li>
    </ol>
    <p class="normal-one">After only 10 iterations, an accuracy of 94% is achieved with the fine-tuned ResNet model.</p>
    <ol>
      <li class="numberedList" value="5">How about its performance on the test set? Let’s see the following:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">evaluate_model(my_resnet, test_dl)</span>
Accuracy on test set: 91.01 %
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We are able to boost the accuracy on the test set from 90% to 91%, with only 10 training iterations.</p>
    <p class="normal">Transfer learning with CNNs<a id="_idIndexMarker1189"/> is a powerful technique<a id="_idIndexMarker1190"/> that allows you to leverage pretrained models and adapt them for your specific image classification tasks.</p>
    <h1 class="heading-1" id="_idParaDest-284">Summary</h1>
    <p class="normal">In this chapter, we worked on classifying clothing images using CNNs. We started with a detailed explanation of the individual components of a CNN model and learned how CNNs are inspired by the way our visual cells work. We then developed a CNN model to categorize fashion-MNIST clothing images from Zalando. We also talked about data augmentation and several popular image augmentation methods. We practiced transfer learning with ResNets, after discussing the evolution of CNN architectures and pretrained models.</p>
    <p class="normal">In the next chapter, we will focus on another type of deep learning network: <strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>). CNNs and RNNs are the two most powerful deep neural networks that make deep learning so popular nowadays.</p>
    <h1 class="heading-1" id="_idParaDest-285">Exercises</h1>
    <ol>
      <li class="numberedList" value="1">As mentioned before, can you try to fine-tune the CNN image classifier and see if you can beat what we have achieved?</li>
      <li class="numberedList">Can you also employ the dropout technique to improve the CNN model?</li>
      <li class="numberedList">Can you experiment with using the pretrained Vision Transformer model: <a href="https://huggingface.co/google/vit-base-patch16-224?"><span class="url">https://huggingface.co/google/vit-base-patch16-224?</span></a></li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-286">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/yuxi"><span class="url">https://packt.link/yuxi</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code187846872178698968.png"/></p>
  </div>
</body></html>
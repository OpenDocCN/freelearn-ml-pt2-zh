- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning for Time-Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a subfield of machine learning concerned with algorithms relating
    to neural networks. Neural networks, or, more precisely, **artificial neural networks**
    (**ANNs**) got their name because of the loose association with biological neural
    networks in the human brain.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, deep learning has been enhancing the state of the art across
    the bench in many application domains. This is true for unstructured datasets
    such as text, images, video, and audio; however, tabular datasets and time-series
    have so far shown themselves to be less amenable to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning brings a very high level of flexibility and can offer advantages
    of both online learning, as discussed in *Chapter 8*, *Online Learning for Time-Series*,
    and probabilistic approaches, as discussed in *Chapter 9*, *Probabilistic Models
    for Time-Series*. However, with its highly parameterized models, finding the right
    model can be a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Among the contributions deep learning has been able to bring to time-series
    are data augmentation, transfer learning, long sequence time-series forecasts,
    and data generation with **generative adversarial networks** (**GANs**). However,
    it's only very recently that deep learning approaches have become competitive
    in relation to forecasting, classification, and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll discuss deep learning applied to time-series, looking,
    in particular, at algorithms and approaches designed for time-series. We'll get
    into current challenges, promising avenues of research, and competitive approaches
    that bring deep learning to time-series. We'll go into detail about a lot of the
    recent innovations in deep learning for time-series.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning for time-series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: InceptionTime
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepAR
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: N-BEATS
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ConvNets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer architectures
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Informer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Python practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dilated causal convolutional neural network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with an introduction to deep learning and the core concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is based on fundamental concepts that find their roots early in
    the 20^(th) century – the wiring between neurons. Neurons communicate chemically
    and electrically through so-called neurites.
  prefs: []
  type: TYPE_NORMAL
- en: This wiring was first described and drawn by Santiago Ramón y Cajal, a Spanish
    neuroscientist. He charted the anatomy of the brain and the structure of neural
    networks in the brain. He received the Nobel Prize in Physiology or Medicine in
    1906, which he shared with Camillo Golgi, who invented the stains for neurons
    based on potassium dichromate and silver nitrate that Ramón y Cajal applied in
    his microscopy studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chart below is just one of his elaborate drawings of the arborization of
    neural connections (called neurites – dendrites and axons) between neurons in
    the brain (source Wikimedia Commons):'
  prefs: []
  type: TYPE_NORMAL
- en: '![ile:Debuixos Santiago Ramón y Cajal.jpg](img/B17577_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Ramon y Cajal''s drawing of networks of neurons in the brain'
  prefs: []
  type: TYPE_NORMAL
- en: In the schematic, you can appreciate neurons as gray dots in layers of the brain.
    Between neurons are dendrites and axons, the wiring of the brain. Each neuron
    takes up what amounts to information about the environment through gate stations
    to neurites that are called synapses.
  prefs: []
  type: TYPE_NORMAL
- en: Ramón y Cajal and his pupils brought to life *cable theory*, where the electric
    current passing through neurites is modeled by mathematical models. The voltage
    arriving at neural sites through the dendrites that receive synaptic inputs at
    different sites and times was recognized as sensory and other information was
    transmitted between cells. This is the foundation of today's detailed neuron models
    employed in research to model synaptic and neural responses.
  prefs: []
  type: TYPE_NORMAL
- en: The basic function of neurons was formalized by Frank Rosenblatt in 1958 as
    the perceptron – a model that contains the essentials of most modern deep learning
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '![../perceptron.png](img/B17577_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: The perceptron model'
  prefs: []
  type: TYPE_NORMAL
- en: In the perceptron model, a neuron – illustrated by the oval in the middle –
    receives input from other neurons. In a model, these inputs could represent text,
    images, sounds, or any other type of information. These get integrated by summing
    them up. In this sum, each input from a neuron, *i*, comes with its weight, *w*[i],
    that marks its importance. This integrated input can then lead to a neural activation
    as given by the neuron's activation function, *g*.
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest case, the activation function could just be a threshold function
    so that the neuron gets activated if the weighted sum of the inputs exceeds a
    certain value. In modern neural networks, the activation functions are non-linear
    functions, such as sigmoid functions or the rectified linear function where the
    output is linear above a threshold and cropped below.
  prefs: []
  type: TYPE_NORMAL
- en: When the network is stimulated by data, input neurons are activated and feed
    second-order neurons, which then feed other neurons in turn until the output layers
    are activated. This is called *feedforward propagation*.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron consisted of a single layer of integrating neurons that sum input
    over their incoming connections. It was demonstrated by Marvin Minsky and Seymour
    Pappert in their book *Perceptrons* (1969) that these neurons, similar to a simple
    linear model, cannot approximate complex functions that are relevant in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer neural networks can overcome this limitation, however, and this is
    where we slowly enter into the realm of deep learning. These networks can be trained
    through an algorithm called *backpropagation* – often credited to Paul Werbos
    (1975). In backpropagation, outputs can be compared to targets and the error derivative
    can be fed back through the network to calculate adjustments to the weights in
    the connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another innovation in neural networks again comes from neuroscience. In the
    1950s and 1960s, David Hubel and Torsten Wiesel found that neurons in the cat
    visual cortex (V1) respond to small regions of the visual field. This region they
    termed the receptive field (1959, "Receptive fields of single neurons in the cat''s
    striate cortex"). They distinguished between two basic cell types:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple cells – these cells can be characterized largely by a summation over
    the inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex cells – cells that respond to a variety of stimuli across different
    locations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex cells have inspired computational layers in neural networks that employ
    convolutions, first by Kunihiko Fukushima in 1980\. We've discussed convolutions
    in *Chapter 3*, *Preprocessing Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks with convolutional layers are the predominant type of model
    for applications such as image processing, classification, and segmentation. Yann
    LeCun and colleagues introduced the LeNet architecture (1989), where convolution
    kernels are learned through backpropagation for the classification of images of
    hand-written numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning networks often come not just with layers, where inputs get propagated
    from one layer to the next (feedforward). The connections can also be recurrent,
    where they connect to the neurons of the same layer or even back to the same neuron.
  prefs: []
  type: TYPE_NORMAL
- en: A recurrent neural network framework, **long short-term memory** (**LSTM**)
    was proposed by Jürgen Schmidhuber and Sepp Hochreiter in 1997\. LSTMs can retrieve
    and learn information for a longer period of time compared to previous models.
    This model architecture was, for some time, powering industry models such as speech
    recognition software for Android smartphones, but has since been mostly replaced
    by convolutional models.
  prefs: []
  type: TYPE_NORMAL
- en: In 2012, AlexNet, created by Alex Krizhevsky in collaboration with Ilya Sutskever
    and Geoffrey Hinton, made a breakthrough in the ImageNet Large Scale Visual Recognition
    Challenge (short ImageNet), where millions of images are to be categorized between
    20,000 categories. AlexNet brought down the top-5 error rate from around 25% to
    about 15%. The model, utilizing massively parallel hardware powered by **Graphics
    Processing Units** (**GPUs**), combined fully connected layers with convolutions
    and pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: This was only the beginning of a radical performance improvement on different
    tasks, including images. The AlexNet performance was beaten the following year
    by ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: Since the ResNet paper is highly influential, it's worth taking a short detour
    to explain how it works. ResNets were introduced by Kaiming He and others at Microsoft
    Research in 2015 ("*Deep Residual Learning for Image Recognition*"). A common
    problem with deep neuron networks is that their performance can saturate and degrade
    with more layers added partly because of the vanishing gradient problem, where
    the error gradient calculated in the optimization will become too small to be
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by pyramidal cells in the brain, residual neural networks employ so-called
    *skip connections*, essentially shortcuts to jump intermediate layers. A ResNet
    is a network that contains blocks with skip connections (residual blocks), as
    indicated in this schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![../resnet%20(2).png](img/B17577_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Residual block with skip connections'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the residual block illustrated, the output of layer 2 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_10_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B17577_10_002.png) and ![](img/B17577_10_003.png) are the activation
    functions in layer 2 and the skip connections, respectively. ![](img/B17577_10_004.png)
    is often the identify function, where activations of layer 1 are unchanged. If
    the dimensionality between layer 1 and layer 2 outputs don't match, either padding
    or convolutions are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these skip connections, Kaiming He and others successfully trained networks
    with as many as 1,000 layers. The original ResNet from 2015 was very successful
    on images. Among other accolades it collected was winning several top competitions
    for image classification and object detection: ILSVRC 2015, ILSVRC 2015, COCO
    2015 competition in ImageNet Detection, ImageNet localization, Coco detection,
    and Coco segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve summarized the early history of ANNs and deep learning in the timeline
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![../timeline%20of%20deep%20learning%20(2).png](img/B17577_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Timeline of artificial neural networks and deep learning'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this is highly simplified, leaving out many important milestones.
    I've ended in 2015, when ResNet was presented.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a host of architectures and approaches in deep learning, and this
    chart displays a typology of these methodologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![deep%20learning%20models.png](img/B17577_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Typology of deep learning approaches'
  prefs: []
  type: TYPE_NORMAL
- en: We've mentioned a few of these approaches in this section, and we'll explain
    a few of these methods in more detail in the next sections as they are relevant
    to time-series.
  prefs: []
  type: TYPE_NORMAL
- en: The computational complexity of techniques based on deep neural networks is
    driven in the first instance by the dimension of the input data and depends on
    the number of hidden layers trained using backpropagation. High-dimensional data
    tend to require more hidden layers to ensure a higher hierarchy of feature learning,
    where each layer derives higher-level features based on the previous level. The
    training time and complexity increase with the number of neurons – the number
    of hyperparameters can sometimes reach the millions or billions.
  prefs: []
  type: TYPE_NORMAL
- en: The representational power of deep learning that constructs a stack of derived
    features as part of the learning allows modelers to get away from hand-crafted
    features. Further advantages of using deep learning models include their flexibility
    in terms of choosing architecture, hyperparameters such as activation functions,
    regularization, layer sizes, and loss objectives, but this is traded off against
    their complexity in terms of the number of parameters, and the difficulty of interrogating
    their inner workings.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning methods offer better representation and, in consequence, prediction
    on a multitude of time-series datasets compared to other machine learning approaches;
    however, they haven't found the impact so far that they had in other areas.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning for time-series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recent years have seen a proliferation of deep neural networks, with unprecedented
    improvements across various application domains, in particular images, natural
    language processing, and sound. The potential advantage of deep learning models
    is that they can be much more accurate than other types of models, thereby pushing
    the envelope in domains such as vision, sound, and **natural language processing**
    (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: In forecasting, especially demand forecasting, data is often highly erratic,
    discontinuous, or bursty, which violates the core assumptions of classical techniques,
    such as Gaussian errors, stationarity, or homoscedasticity, as discussed in *Chapter
    5*, *Forecasting of Time-Series*. Deep learning techniques applied to forecasting,
    classification, or regression tasks could overcome many of the challenges faced
    by classical approaches, and, most importantly, they could provide a way to model
    non-linear dynamics usually neglected by traditional methods such as Box-Jenkins,
    Exponential Smoothing (ES), or state-space models.
  prefs: []
  type: TYPE_NORMAL
- en: Many deep learning algorithms have been applied more recently to time-series,
    both with univariate and multivariate time-series. The model architectures encompass
    recurrent neural networks (RNNs), most prominently long short-term memory (LSTM)
    models, and transformer and convolutional models, or different types of autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: As regards their application to time-series, however, they haven't been able
    to challenge the top models in the field. For instance, as pointed out by Spyros
    Makridakis and others (2020), in the M4 competition, arguably the most important
    benchmark for univariate time-series forecasting, the best-ranking methods were
    ensembles of widely used classical statistical techniques rather than pure machine
    learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: This could have been at least partly due to the nature of the competition. As
    pointed out by Slawek Smyl, de-seasonalization of seasonal series was very important
    in the M4 competition, given that the series were provided as scalar vectors without
    timestamps, so there was no way to incorporate calendar features such as the day
    of the week or the month number.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the M4 competition, out of 60 competition entries, the first machine learning
    method ranked at place 23\. However, interestingly, the winner of the M4 competition
    was a hybrid between a dilated LSTM with attention and a Holt-Winters statistical
    model. Another top contender, developed by the research group around Rob Hyndman,
    applied a gradient boosted tree ensemble to outputs from traditional models (*FFORMA:
    Feature-based Forecast Model Averaging*, 2020).'
  prefs: []
  type: TYPE_NORMAL
- en: These rankings led Spyros Makridakis and others to conclude that hybrids or
    mixtures of classical and machine learning methods are the way forward. The search
    is ongoing for a deep learning architecture that could provide an inflection point
    in research and applications similar to that of AlexNet or Inception for the image
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 4*, *Introduction to Machine Learning with Time-Series*, we discussed
    first how difficult it is to beat baseline approaches such as Nearest Neighbor
    with **Dynamic Time Warping** (**DTW**) and then state-of-the-art approaches.
    The most competitive model in terms of performance is **HIVE-COTE** (**Hierarchical
    Vote Collective of Transformation-Based Ensembles**), which consists of ensembles
    of machine learning models – very expensive in terms of resources, owing to the
    number of computations and the long runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The sardonic reader might comment that this sounds like deep learning already
    and ask whether deep learning hasn't already taken over as the state-of-the-art
    method. Generally speaking, the complexity of deep learning models is much higher
    than that of traditional models or other machine learning techniques. The case
    can be made that this is one of the biggest distinguishing characteristics of
    deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Is there a deep learning model architecture of similar or lower complexity than
    HIVE that can achieve competitive results?
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve summarized a few libraries that implement algorithms with deep learning
    for time-series in this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Library | Maintainer | Algorithms | Framework |'
  prefs: []
  type: TYPE_TB
- en: '| dl-4-tsc | Hassan Ismail Fawaz | **Multi-Layer Perceptron** (**MLP**), **Fully
    Connected Network** (**FCN**), ResNet, Encoder (based on CNN), **Multi-Scale Convolutional
    Neural Network** (**MCNN**), **Time Le-Net** (**t-LeNet**), **Multi-Channel Deep
    Convolutional Neural Network** (**MCDCNN**), Time-CNN, **Time Warping Invariant
    Echo State Network** (**TWIESN**), InceptionTime | TensorFlow/Keras |'
  prefs: []
  type: TYPE_TB
- en: '| Sktime-DL | Students and staff at the University of East Anglia around Tony
    Bagnell | ResNet, CNN, InceptionTime (through an interface with another library)
    | TensorFlow/Keras |'
  prefs: []
  type: TYPE_TB
- en: '| Gluon-TS | Amazon Web Services – Labs | Gluon-TS specializes in probabilistic
    neural network models such as these: **Convolutional Neural Network** (**CNN**),
    DeepAR, **Recurrent Neural Network** (**RNN**), **Multi-Layer Perceptron** (**MLP**)
    | MXNET |'
  prefs: []
  type: TYPE_TB
- en: '| Pytorch Forecasting | Daniel Hoyos and others | Recurrent Networks (GRU,
    LSTM), Temporal Fusion Transformers, N-Beats, Multilayer Perceptron, DeepAR |
    PyTorch Lightning |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 10.6: Overview of several deep learning libraries for time-series'
  prefs: []
  type: TYPE_NORMAL
- en: Sktime-DL is an extension to sktime, maintained by the same research group.
    As of August 2021, this library is undergoing a rewrite.
  prefs: []
  type: TYPE_NORMAL
- en: Gluon-TS is based on the MXNET deep learning modeling framework, and – apart
    from the network architectures noted in the table – includes many other features,
    such as kernels for **Support Vector Machines** (**SVMs**) and **Gaussian Process**
    (**GP**), and distributions for probabilistic network models.
  prefs: []
  type: TYPE_NORMAL
- en: '*dl-4-tsc* is the GitHub companion repository for a review paper of many time-series
    deep learning algorithms, prepared by Hassan Ismail Fawaz and others (2019). It
    includes implementations in TensorFlow/Keras of their implementations. It is not
    a library *per se*, as it isn''t installed like a library and the models run with
    datasets; however, since the algorithms are implemented in TensorFlow and Keras,
    anyone with a knowledge of these will feel at home.'
  prefs: []
  type: TYPE_NORMAL
- en: Pytorch-forecasting, sktime-DL, and Gluon-TS come with their own abstractions
    of datasets that help with the automation of common tasks. While Sktime-DL builds
    on the sktime abstractions, Pytorch-Forecasting and Gluon-TS have batteries built-in
    for deep learning with utilities for common tasks such as the scaling and encoding
    of variables, normalizing the target variable, and downsampling. These abstractions
    come at the cost of a learning curve, however, and I should caution the impatient
    reader that it can take time to get up to speed with this, which is why I am omitting
    them from the practice part.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve omitted repositories from this table that only implement a single algorithm.
    In the next visualization, I have included some of these, such as the repositories
    for the Informer model or the neural prophet. In the following diagram, you can
    see the popularity of a few repositories for time-series deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![deep_learning-star_history.png](img/B17577_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Popularity of deep learning libraries for time-series'
  prefs: []
  type: TYPE_NORMAL
- en: As always, I've tried to choose the most popular repositories – and repositories
    that have been updated recently. You can see that Gluon-TS is the most popular
    repository. Of the repositories implementing several algorithms, Pytorch Forecasting
    comes closest and has been making inroads recently in terms of popularity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next sections, we''ll concentrate on recent and competitive approaches
    with deep learning on time-series. We''ll go through a few of the most prominent
    algorithms in more detail: Autoencoders, InceptionTime, DeepAR, N-BEATS, RNNs
    (most prominently LSTMs), ConvNets, and Transformers (including the Informer).'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Autoencoders** (**AEs**) are artificial neural networks that learn to efficiently
    compress and encode data and are trained on reconstruction errors. A basic linear
    AE is essentially functionally equivalent to a **Principal Component Analysis**
    (**PCA**), although, in practice, AEs are often regularized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AEs consist of two parts, the encoder and the decoder, as illustrated below
    (source: Wikipedia):'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/220px-Autoencoder_schema.png](img/B17577_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Autoencoder architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Encoders and decoders are often both of the same architecture, which depends
    on the domain. For example, as regards images, they often contain convolutions
    like `LeNet`. For modeling time dependence, they can include causal convolutions
    or recurrent layers as a way of modeling time dependence.
  prefs: []
  type: TYPE_NORMAL
- en: AEs are a natural way of reducing noise. They are often employed for anomaly
    detection in time-series.
  prefs: []
  type: TYPE_NORMAL
- en: InceptionTime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a massive test that took about a month to complete on a cluster of 60 GPUs,
    Hassan Ismail Fawaz and others at the Université de Haute Alsace ran deep learning
    algorithms on the univariate UCR/UEA time-series classification archive (85 time-series)
    and the 13 datasets of the **Multivariate Time-Series** (**MTS**) classification
    archive. They presented this work in their paper "*Deep learning for time-series
    classification: a review*" in 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: They conducted a systematic evaluation of 11 models, including LeNet, **Fully
    Connected Networks** (**FCNs**), Time-CNN, and ResNet. Only 9 of the algorithms
    completed all the tests. Compared to deep learning algorithms on the univariate
    datasets (UCR/UEA), ResNet won on 50 problems out of 85, and it was statistically
    better than the next best algorithm, the **fully convolutional neural network**
    (**FCNN**). At the same time, it was not statistically worse than HIVE-COTE, the
    top model in time-series classification. On the multivariate benchmark, the FCNN
    won, although they couldn't find any statistically significant differences between
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In another paper, "*InceptionTime: Finding AlexNet for Time-Series Classification,*"
    Hassan Ismail Fawaz and an extended group of researchers, including Geoff Webb
    and others from Monash University (who we encountered in *Chapter 3*, *Preprocessing
    Time-Series*), presented a new model that they called InceptionTime.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The name *InceptionTime* makes reference to the Inception model ("*Going Deeper
    with Convolutions*", 2014), a network presented by researchers at Google, and
    the universities of North Carolina and Michigan. The Inception architecture consists
    of feedforward and convolutional layers, similar to LeNet, which we mentioned
    earlier in this chapter. A 22-layer variant was therefore also called GoogleLetNet
    (alternatively: Inception model version 1). Roughly speaking, the inception model
    consists of modules ("inception modules") that concatenate convolutions of different
    sizes together.'
  prefs: []
  type: TYPE_NORMAL
- en: InceptionTime takes ensembles of inception-type models with different hyperparameters
    (filters of varying lengths). They experimented with the number of networks in
    the ensemble and the filter sizes and finally showed that their model significantly
    outperformed ResNet on the 85 datasets of the UCR archive, while being statistically
    on a par with HIVE-COTE – with a much-reduced training time compared to HIVE-COTE.
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DeepAR is a probabilistic forecasting method coming out of Amazon Research
    Germany. It is based on training an auto-regressive recurrent network model. In
    their article "*DeepAR: Probabilistic forecasting with autoregressive recurrent
    networks*" (2019), David Salinas, Valentin Flunkert, and Jan Gasthaus demonstrated
    through extensive empirical evaluation on several real-world forecasting datasets
    (parts, electricity, traffic, ec-sub, and ec) accuracy improvements of around
    15% compared to state-of-the-art methods.'
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR was designed for demand forecasting and consists of an RNN architecture
    and incorporates a negative binomial likelihood for unbounded count data that
    can stretch across several orders of magnitude. Monte Carlo sampling is used to
    compute quantile estimates for all sub-ranges in the prediction horizon. For the
    case when the magnitudes of the time-series vary widely, they also introduced
    a scaling of the mean and variance parameters of the negative binomial likelihood
    by factors that depend on the mean of the time-series and the output of the network.
  prefs: []
  type: TYPE_NORMAL
- en: N-BEATS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a model architecture for univariate times series point forecasting based
    on backward and forward residual links and a very deep multilayer network of fully
    connected ReLU neurons. N-BEATS uses deep learning primitives such as residual
    blocks instead of any time-series-specific components and is the first architecture
    to demonstrate that deep learning using no time-series-specific components can
    outperform well-established statistical approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Published in 2019 by a group around Yoshua Bengio ("*N-BEATS: Neural basis
    expansion analysis for interpretable time-series forecasting*"), this network
    reached state-of-the-art performance for two configurations and outperformed all
    other methods, including ensembles of traditional statistical methods in benchmarks
    over the M3, M4, and TOURISM competition datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: A common criticism of deep learning is the opaque nature of the learning or
    – inversely – a lack of transparency in terms of what the network does. N-BEATS
    can be made interpretable with few changes without losing significant accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RNNs, most prominently LSTMs, have been applied a lot to multivariate electricity
    consumption forecasting. Electricity forecasting is a long sequence time-series,
    where it's necessary to precisely capture the long-range correlation coupling
    between items of a sequence over time.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier works explored combinations of LSTMs with dilation, residual connections,
    and attention. These served as a basis for the winner of the M4 competition (Slawek
    Smyl, 2020).
  prefs: []
  type: TYPE_NORMAL
- en: Smyl introduced a mixture of a standard **Exponential Smoothing** (**ES**) model
    with LSTM networks. The ES equations enable the method to effectively capture
    the main components of the individual series, such as seasonality and baseline
    level, while the LSTM networks can model the nonlinear trend.
  prefs: []
  type: TYPE_NORMAL
- en: A problem with RNNs, including LSTMs, is that they cannot be parallelized easily
    at the cost of training time and computing resources. It has also been argued
    that LSTMs cannot capture long-range dependencies since they struggle with sequences
    longer than about 100 time steps. RNNs encode past hidden states to capture dependencies
    with previous items, and they show a decrease in performance due to long dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at the transformer architecture, which has been
    taking over from LSTM models both in terms of performance and, more recently,
    in popularity.
  prefs: []
  type: TYPE_NORMAL
- en: It has been shown that convolutional architectures can outperform recurrent
    networks on tasks such as audio processing and machine translation, and they have
    been applied to time-series tasks as well.
  prefs: []
  type: TYPE_NORMAL
- en: ConvNets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Researchers from Carnegie Mellon University and Intel Labs ("*An Empirical Evaluation
    of Generic Convolutional and Recurrent Networks for Sequence Modeling*" 2018)
    compared generic convolutional and recurrent networks such as LSTM/GRU for sequence
    modeling across a broad range of tasks. These were large textual datasets and
    posed sequence problems, such as the problem of addition, the copying of memory
    tasks, or polyphonic music. Problems and datasets such as these are commonly used
    to benchmark recurrent networks.
  prefs: []
  type: TYPE_NORMAL
- en: They found that a simple convolutional architecture, the **Temporal Convolutional
    Network** (**TCN**), performs better than RNNs on a vast range of tasks' canonical
    recurrent networks (such as LSTMs) while demonstrating a longer effective memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important characteristic of the convolutions in the TCNs is that they are
    causal. A convolution is causal if its output is the result of only current and
    past inputs. This is illustrated here (source: keras-tcn, GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://github.com/philipperemy/keras-tcn/raw/master/misc/Dilated_Conv.png](img/B17577_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: A time-causal convolution'
  prefs: []
  type: TYPE_NORMAL
- en: An output at time t is convolved only with elements from time t and earlier.
    This means that information from the future cannot leak to the past. A disadvantage
    of this basic design is that in order to achieve a long effective history size,
    we need an extremely deep network or very large filters.
  prefs: []
  type: TYPE_NORMAL
- en: Some advantages of convolutions over RNNs are parallelism, the flexible receptive
    field size (specifying how far the model can see), and stable gradients – backpropagation
    through time comes with the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer also addresses the perceived shortcomings of RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers, introduced in the article "*Attention is all you need*" (2017)
    by researchers at Google Brain and the University of Toronto, were designed to
    avoid recursion in order to allow parallel computation.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers introduced two building blocks – multi-head attention and positional
    embeddings. Rather than working sequentially, sequences are processed as a whole
    rather than item by item. They employ self-attention, where similarity scores
    between items in a sentence are stored.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers were introduced originally for machine translation, where they
    were shown to outperform Google's Neural Machine Translation models. The central
    piece is therefore the alignment of two sequences. Instead of recurrence, positional
    embeddings were introduced where weights encode information related to a specific
    position of a token in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers consist of stacked modules, first encoder and then decoder modules.
    Encoder modules each consist of a self-attention layer and a feed-forward layer,
    while decoder modules consist of self-attention, encoder-decoder attention, and
    a feed-forward layer. These modules can be stacked, thereby creating very large
    models that can learn massive datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have pushed the envelope in NLP, especially in translation and
    language understanding. Furthermore, OpenAI's powerful GPT-3 model for language
    generation is a transformer as well, as is DeepMind's AlphaFold 2, a model that
    predicts protein structure from their genetic sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have been able to maintain performance across longer sequences.
    However, they can capture only dependencies within the fixed input size used to
    train them. To work with even longer sentences beyond the fixed input width, architectures
    such as Transformer-XL reintroduce recursion by storing hidden states of already
    encoded sentences to leverage them in the subsequent encoding of the next sentences.
  prefs: []
  type: TYPE_NORMAL
- en: In the article "*Temporal Fusion Transformers for Interpretable Multi-horizon
    Time-Series Forecasting,*" researchers from the University of Oxford and Google
    Cloud AI introduced an attention-based architecture, which they called a **Temporal
    Fusion Transformer** (**TFT**). To learn temporal relationships at different scales,
    TFT uses recurrent layers for local processing and interpretable self-attention
    layers for long-term dependencies. Furthermore, a series of gating layers suppress
    unnecessary components.
  prefs: []
  type: TYPE_NORMAL
- en: On a variety of real-world datasets, they demonstrated significant performance
    improvements of their architecture over a broad benchmark set. Among other improvements,
    they outperformed Amazon's DeepAR by between 36 and 69%.
  prefs: []
  type: TYPE_NORMAL
- en: Informer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A problem with transformers is the quadratic time complexity and memory usage,
    along with the limitations of the encoder-decoder architecture. This complicates
    predictions over longer time periods, for example, 510 time steps of hourly electricity
    consumption. To address these issues, researchers from Beihang University, UC
    Berkeley, Rutgers, and SEDD Company designed an efficient transformer-based model
    for long sequence time-series forecasting, named Informer – "*Informer: Beyond
    Efficient Transformer for Long Sequence Time-Series Forecasting*". The paper obtained
    the Outstanding Paper Award at the AAAI conference in 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: The generative decoder alleviates the time complexity of the encoder-decoder
    by predicting long time-series sequences in one forward operation rather than
    step-by-step. They replaced the positional embeddings with a new self-attention
    mechanism called ProbSparse Self-Attention, which achieves ![](img/B17577_10_005.png),
    where *L* is the length of the sequence, instead of quadratic time complexity
    and memory usage, ![](img/B17577_10_006.png), while maintaining a comparable performance
    on sequence alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the self-attention distillation halves the cascading layer input, and
    efficiently handles extreme long input sequences. This reduces the complexity
    from ![](img/B17577_10_007.png), where J is the number of transformer layers,
    to ![](img/B17577_10_008.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Informer architecture is illustrated in this schema (from the official
    Informer repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://github.com/zhouhaoyi/Informer2020/raw/main/img/informer.png](img/B17577_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: The Informer architecture'
  prefs: []
  type: TYPE_NORMAL
- en: This diagram shows that the Informer significantly outperforms existing methods
    on datasets of long time-series forecasting such as Electricity Transformer Temperature
    (ETT), Electricity Consuming Load (ECL), and Weather.
  prefs: []
  type: TYPE_NORMAL
- en: 'On univariate datasets, they achieved superior performance compared with all
    competitors, except for two cases, where DeepAR was slightly better, as shown
    here (source: Informer GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_WVOQFx/Screenshot
    2021-08-01 at 22.05.06.png](img/B17577_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Univariate long sequence time-series forecasting performance'
  prefs: []
  type: TYPE_NORMAL
- en: Most significantly, they beat competitors such as ARIMA, prophet, LSTMs, and
    other transformer-based architectures.
  prefs: []
  type: TYPE_NORMAL
- en: On a multivariate benchmark, they also beat competitors including other transformer-based
    models and LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: We'll put some of this into practice now.
  prefs: []
  type: TYPE_NORMAL
- en: Python practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's model airplane passengers. We'll forecast the monthly number of passengers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is considered one of the classic time-series, published by George
    E.P. Box and Gwilym Jenkins alongside the book "Time-Series Analysis: Forecasting
    and Control" (1976). I have provided a copy of this dataset in the `chapter10`
    folder of the book''s GitHub repository. You can download it from there or use
    the URL directly in `pd.read_csv()`.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll first start with a simple FCN and then we'll apply a recurrent network,
    and finally, we'll apply a very recent architecture, a Dilated Causal Convolutional
    Neural Network.
  prefs: []
  type: TYPE_NORMAL
- en: The FCN is first.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this first practice session, we''ll use TensorFlow libraries, which we can
    quickly install from the terminal (or similarly from the anaconda navigator):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We'll execute the commands from the Python (or IPython) terminal, but equally,
    we could execute them from a Jupyter notebook (or a different environment).
  prefs: []
  type: TYPE_NORMAL
- en: The installation could take a while – the TensorFlow library is about 200 MB
    in size and comes with a few dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the dataset. Here, I am assuming that you''ve downloaded it onto
    your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let's try naively to just use an FCN, also known as an MLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set some imports and set a couple of global constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will use these constants for our model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropout (or: Dilution) is a regularization technique that can help reducing
    overfitting. Dropout means that during training, a fraction of the connections
    (in our case 20%) is randomly removed.'
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping is another form of regularization, where the training stops as
    defined by certain conditions. In our case, we've stated it should stop if our
    loss doesn't improve three times in a row. If the model stops improving, there's
    no point in continuing to train it, although we may be trapped in a local minimum
    of the error that we might be able to escape. One of the big advantages of early
    stopping is that it can help us quickly see whether a model is working.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define our model in this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With the Keras functional API, we've defined a two-layer neural network, where
    the hidden layer of `HIDDEN_NEURONS` neurons is activated by the Rectified Linear
    Unit (ReLU) function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s split our dataset into training and test sets. We will predict the number
    of passengers based on passengers in the previous time period (previous month):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We'll learn based on the first 75% of the dataset – this is the default value
    for the `test_size` parameter in the `train_test_split` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now train our naïve FCN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get an output of the loss and the metrics at each epoch as they finish:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_zCMSbK/Screenshot
    2021-08-06 at 22.50.40.png](img/B17577_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: Model training'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would see the error (loss) going down, and we'd see a low error
    at the end. I haven't included any code to fix the random number generator (`tf.random.set_seed`),
    so your output might differ.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then get the predictions for the test set like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, it would be good to visualize passenger predictions against the actual
    passenger values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let's visualize our predictions then!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![fcn_naive.png](img/B17577_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13: Predicted against actual airplane passengers: Naïve fully connected
    network'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the model has learned some of the monthly variability. However,
    it is systematically under-predicting – it has learned the baseline from the years
    1949-1958 of the training set, when far fewer passengers were traveling.
  prefs: []
  type: TYPE_NORMAL
- en: Let's make this a bit more sophisticated and better.
  prefs: []
  type: TYPE_NORMAL
- en: This first model was trained only on the immediate previous number of travelers.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we'll include the year and the month as predictor variables.
    The year can be used to model the trend, while the month is coupled to the monthly
    variability – so this seems a natural step.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will add month and year columns to the DataFrame based on the DateTimeIndex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can redefine our model – we need to add more input columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And we are ready for another training round:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how well the model predictions match the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![fcn_with_year_month.png](img/B17577_10_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.14: Predicted against actual airplane passengers; Fully connected
    network with the year and month'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, because of the high number of parameters, and the randomness
    involved in the learning process, the outcome might differ significantly between
    runs. This is indeed one of the problems associated with deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: This already looks much better. The year feature helped our model learn the
    baseline. The model has learned something about the monthly variability, but it's
    not enough to really approximate it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a less naïve version. We will change a few things in this model:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll add an embedding of the month feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll treat the year as a linear predictor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll add the previous month's passengers to our predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we'll scale our predictions based on the standard deviation in the
    training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That was quite a mouthful. Let's go through these a bit more slowly.
  prefs: []
  type: TYPE_NORMAL
- en: We fed the months as values from 1 to 12 into our previous model. However, we
    could intuitively guess that January (1) and December (12) are perhaps more similar
    than November (11) and December. We know that there are lots of travelers in both
    December and January, but perhaps a much lower volume in November. We can capture
    these relationships based on the data.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done in an embedding layer. An embedding layer is a mapping of distinct
    categories to real numbers. This mapping is updated as part of the network optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The year is closely related to the overall trend. Each year, the number of airline
    passengers increases. We can model this relationship non-linearly or linearly.
    Here, I've decided to just model a linear relationship between the year feature
    and the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between the previous month's passenger numbers and those in
    this month is again assumed to be linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can scale our predictions, similar to the inverse transformation
    of the standard transformation. You should remember the standard normalization
    from *Chapter 3**, Preprocessing Time-Series*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_10_009.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B17577_10_010.png) is the population mean and ![](img/B17577_10_011.png)
    is the population standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The inverse of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_10_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_10_013.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B17577_10_014.png) are the airline passengers at time t and ![](img/B17577_10_015.png)
    is the prediction based on the embedded month and the year.
  prefs: []
  type: TYPE_NORMAL
- en: We assume the network will learn to baseline, but might not learn the scale
    perfectly – so we'll help out.
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustration might help (from TensorBoard, TensorFlow''s visualization toolkit):'
  prefs: []
  type: TYPE_NORMAL
- en: '![../../Desktop/Screenshot%202021-08-06%20at%2022.08.12.pn](img/B17577_10_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.15: Model architecture: Fully connected network with embedding, scaling,
    and baseline'
  prefs: []
  type: TYPE_NORMAL
- en: We can see the three inputs, one of them (the months) going through an embedding
    layer, and another one through a (linear) projection. They all come together (concatenate)
    and go through a dense layer, where another math operation is performed on top.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll need a few more imports first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we redefine our network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We reinitialize our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'During training and for prediction purposes, we need to feed the three types
    of input separately like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You might notice that the training carries on for much longer in this configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chart illustrates the fit we are achieving with our new network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![fcn_more_sophisticated.png](img/B17577_10_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.16: Predicted against actual airplane passenger numbers: Fully connected
    network with embedding, scaling, and baseline'
  prefs: []
  type: TYPE_NORMAL
- en: This again looks much better than the previous network. We leave it as an exercise
    to the reader to try to improve this network further.
  prefs: []
  type: TYPE_NORMAL
- en: We'll set up an RNN next.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed in the theory section that recurrent neural networks can be very
    good at modeling long-term relationships between points in a time-series. Let's
    set up an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the same dataset as before – the univariate values of airline passengers.
    In this case, our network is going to need a sequence of points for each training
    sample. At each training step, the RNN is going to be trained on points (passengers)
    leading up to the next passenger number.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we can use TensorFlow (or even statsmodels' `lagmat()`) utility
    functions for this purpose (and we will use them in *Chapter 12*, *Case Studies*),
    but in this instance, we'll write this quickly ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll need to resample our passenger numbers thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This function does the job. It goes over all the points in the dataset and takes
    a sequence leading up to it. The number of points in the new sequence is defined
    by the parameter `lookback`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put it to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We are using a lookback of 10\. I've deliberately chosen a value that is not
    optimal. I'm leaving it as an exercise to the reader to choose one that's better
    and try it out.
  prefs: []
  type: TYPE_NORMAL
- en: The last line in the code above joins the targets (lookahead 1) together with
    the sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are ready to define our network, but let''s get the imports out of the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The network is defined by this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It's a bidirectional LSTM network. The result of the last layer is projected
    linearly as our output. I've set the activation function of the LSTM to `tanh`
    in case you want to run this on a GPU runtime, so it will benefit from NVIDIA's
    GPU-accelerated library cuDNN. We are extracting the same metrics as in the previous
    practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few more preliminaries that you should be familiar with from the previous
    section are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks quite good already – even though we made a few sub-optimal
    choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![rnn_passengers.png](img/B17577_10_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.17: Recurrent neural network for passenger forecasts'
  prefs: []
  type: TYPE_NORMAL
- en: Given how easy this was to set up, this already looks very promising.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now try a causal ConvNet!
  prefs: []
  type: TYPE_NORMAL
- en: Dilated causal convolutional neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This example is based on Krist Papadopoulos's SeriesNet implementation of the
    paper "*Conditional Time-Series Forecasting with Convolutional Neural Networks*,"
    by Anastasia Borovykh and others.
  prefs: []
  type: TYPE_NORMAL
- en: We'll implement this model together and we'll apply it to two datasets to see
    how it does out of the box. I won't be going through tweaking the data and architecture
    in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: What is perhaps surprising is how easy it is to do a causal convolution in TensorFlow.
    Conv1D comes with a parameter, `padding`, which can be specified as `'causal'`.
    This simply pads the layer's input with zeros according to the causal nature,
    where output at time t only depends on the previous time steps, <t. Please refer
    to the discussion in the ConvNets section in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This means that we can predict the values of early time steps in the frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea of this network is a residual block with causal convolutions.
    This code segment constructs the corresponding network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: I've simplified this a bit to make it easier to read.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network itself just stacks these layers as a SkipNet follows up with a
    convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is for a univariate time-series. For a multivariate time-series, some changes
    are necessary, which we won't cover here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s forecast passenger numbers again. We''ll load the DataFrame as in the
    previous practice sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll split this again into test and training sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll train the model with this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This function will do the forecast for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The forecast is created by predicting the immediate next future value based
    on the previous predictions. The parameter `horizon` is the forecast horizon.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll put this together as a single function for convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready for training. We''ll run everything like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The model is very deep, but is not that big in terms of parameters because of
    the convolutions. We'll see that there are 865 trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model fit is not that good though, neither in MSE nor does that graph look
    very impressive either:'
  prefs: []
  type: TYPE_NORMAL
- en: '![convnet_passengers.png](img/B17577_10_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.18: ConvNet prediction of passenger numbers'
  prefs: []
  type: TYPE_NORMAL
- en: This graph can be produced by running `show_result(y_test[:HORIZON], predictions[:HORIZON],
    "Passengers")`.
  prefs: []
  type: TYPE_NORMAL
- en: This highlights the fact that each model has its strengths and weaknesses and
    that without adapting a model to our dataset and careful preprocessing, we won't
    be able to get a good performance. It's left as an exercise to the reader to try
    and tweak this model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I've introduced many deep learning concepts relevant to time-series,
    and we've discussed many architectures and algorithms, such as autoencoders, InceptionTime,
    DeepAR, N-BEATS, ConvNets, and a few transformer architectures. Deep learning
    algorithms are indeed coming very close to the state of the art in time-series,
    and it's an exciting area of research and application.
  prefs: []
  type: TYPE_NORMAL
- en: In the practice section, I implemented a fully connected feedforward network
    and then an RNN before taking a causal ConvNet for a ride.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 12*, *Multivariate Forecasting*, we'll do some more deep learning,
    including a Transformer model and an LSTM.
  prefs: []
  type: TYPE_NORMAL

<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;The Fundamentals of Machine Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. The Fundamentals of Machine Learning</h1></div></div></div><p>In this chapter we will review the fundamental concepts in machine learning. We will discuss applications of machine learning algorithms, the supervised-unsupervised learning spectrum, uses of training and testing data, and model evaluation. Finally, we will introduce scikit-learn, and install the tools required in subsequent chapters.</p><p>Our imagination has <a class="indexterm" id="id0"/>long been captivated by visions of machines that can learn and imitate human intelligence. While visions of general artificial intelligence such as Arthur C. Clarke's HAL and Isaac Asimov's Sonny have yet to be realized, software programs that can acquire new knowledge and skills through experience are becoming increasingly common. We use such machine learning programs to discover new music that we enjoy, and to quickly find the exact shoes we want to purchase online. Machine learning <a class="indexterm" id="id1"/>programs allow us to dictate commands to our smartphones and allow our thermostats to set their own temperatures. Machine learning programs can decipher sloppily-written mailing addresses better than humans, and guard credit cards from fraud more vigilantly. From investigating new medicines to estimating the page views for versions of a headline, machine learning software is becoming central to many industries. Machine learning has even encroached on activities that have long been considered uniquely human, such as writing the sports column recapping the Duke basketball team's loss to UNC.</p><p>Machine learning is the design and study of software artifacts that use past experience to make future decisions; it is the study of programs that learn from data. The fundamental goal of machine learning is to <span class="emphasis"><em>generalize</em></span>, or to induce an unknown rule from examples of the rule's application. The canonical example of machine learning is spam filtering. By observing thousands of emails that have been previously labeled as either spam or ham, spam filters learn to classify new messages.</p><p>Arthur Samuel, a computer scientist who pioneered the study of artificial intelligence, said that machine learning is "the study that gives computers the ability to learn without being explicitly programmed." Throughout the 1950s and 1960s, Samuel developed programs that played checkers. While the rules of checkers are simple, complex strategies are required to defeat skilled opponents. Samuel never explicitly programmed these strategies, but through the experience of playing thousands of games, the program learned complex behaviors that allowed it to beat many human opponents.</p><p>A popular quote from computer scientist Tom Mitchell defines machine learning more formally: "A program can be said to learn from experience <span class="emphasis"><em>E</em></span> with respect to some class of tasks <span class="emphasis"><em>T</em></span> and performance measure <span class="emphasis"><em>P</em></span>, if its performance at tasks in <span class="emphasis"><em>T</em></span>, as measured by <span class="emphasis"><em>P</em></span>, improves with experience <span class="emphasis"><em>E</em></span>." For example, assume that you have a collection of pictures. Each picture depicts either a dog or cat. A task could be sorting the pictures into separate collections of dog and cat photos. A program could learn to perform this task by observing pictures that have already been sorted, and it could evaluate its performance by calculating the percentage of correctly classified pictures.</p><p>We will use Mitchell's <a class="indexterm" id="id2"/>definition of machine learning to organize this <a class="indexterm" id="id3"/>chapter. First, we will discuss types of experience, including <a class="indexterm" id="id4"/>
<span class="strong"><strong>supervised</strong></span> learning and <span class="strong"><strong>unsupervised</strong></span> learning. Next, we will discuss common tasks that can be performed by machine learning systems. Finally, we will discuss performance measures that can be used to assess machine learning systems.</p><div class="section" title="Learning from experience"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Learning from experience</h1></div></div></div><p>Machine learning systems are often described as learning from experience either with or without supervision from humans. In supervised learning problems, a program predicts an output for an input by learning from pairs of labeled inputs and outputs; that is, the program learns from examples of the right answers. In unsupervised learning, a program does not learn from labeled data. Instead, it attempts to discover patterns in the data. For example, assume that you have collected data describing the heights and weights of people. An example of an unsupervised learning problem is dividing the data points into groups. A program might produce groups that correspond to men and women, or children and adults.</p><p>Now assume that the data is also labeled with the person's sex. An example of a supervised learning problem is inducing a rule to predict whether a person is male or female based on his or her height and weight. We will discuss algorithms and examples of supervised and unsupervised learning in the following chapters.</p><p>Supervised learning and unsupervised learning can be thought of as occupying opposite ends of a spectrum. Some <a class="indexterm" id="id5"/>types of problems, called <span class="strong"><strong>semi-supervised</strong></span> learning problems, make use of both supervised and unsupervised data; these problems are located on the spectrum between supervised and unsupervised learning. An example of semi-supervised machine learning is reinforcement learning, in which a program receives feedback for its decisions, but the feedback may not be associated with a single decision. For example, a reinforcement learning program that learns to play a side-scrolling video game such as <span class="emphasis"><em>Super Mario Bros.</em></span> may receive a reward when it completes a level or exceeds a certain score, and a punishment when it loses a life. However, this supervised feedback is not associated with specific decisions to run, avoid Goombas, or pick up fire flowers. While this book will discuss semi-supervised learning, we will focus primarily on supervised and unsupervised learning, as these categories include most the common machine learning problems. In the next sections, we will review supervised and unsupervised learning in more detail.</p><p>A supervised learning <a class="indexterm" id="id6"/>program learns from labeled examples of the outputs that should be produced for an input. There are many names for the output of a machine learning program. Several disciplines converge in machine learning, and many of those disciplines use their own terminology. In this book, we will refer to the output <a class="indexterm" id="id7"/>as the <span class="strong"><strong>response variable</strong></span>. Other names for response variables include dependent variables, regressands, criterion variables, measured variables, responding variables, explained variables, outcome variables, experimental variables, labels, and output variables. Similarly, the input variables have several names. In this book, we will refer to the input variables as <span class="strong"><strong>features</strong></span>, and the phenomena they <a class="indexterm" id="id8"/>measure as <span class="strong"><strong>explanatory variables</strong></span>. Other names for explanatory variables include predictors, regressors, controlled variables, manipulated variables, and exposure variables. Response variables and explanatory variables may take real or discrete values.</p><p>The collection of examples that <a class="indexterm" id="id9"/>comprise supervised experience is called a <span class="strong"><strong>training set</strong></span>. A collection of examples that is used to assess the performance of a program is called a <a class="indexterm" id="id10"/>
<span class="strong"><strong>test set</strong></span>. The response variable can be thought of as the answer to the question posed by the explanatory variables. Supervised learning problems learn from a collection of answers to different questions; that is, supervised learning programs are provided with the correct answers and must learn to respond correctly to unseen, but similar, questions.</p></div></div>
<div class="section" title="Machine learning tasks"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Machine learning tasks</h1></div></div></div><p>Two of the most <a class="indexterm" id="id11"/>common supervised machine learning tasks are <span class="strong"><strong>classification</strong></span> and <span class="strong"><strong>regression</strong></span>. In classification tasks the program must learn to predict discrete values for the response variables from one or more explanatory variables. That is, the program must predict the most probable category, class, or label for new observations. Applications of classification include predicting whether a stock's price will rise or fall, or <a class="indexterm" id="id12"/>deciding if a news article <a class="indexterm" id="id13"/>belongs to the politics or leisure section. In regression problems the program must predict the value of a continuous response variable. Examples of regression problems include predicting the sales for a new product, or the salary for a job based on its description. Similar to classification, regression problems require supervised learning.</p><p>A common unsupervised learning task is to discover groups of related observations, called <span class="strong"><strong>clusters</strong></span>, within the training data. This task, called <span class="strong"><strong>clustering</strong></span> or cluster analysis, assigns observations to groups such that observations within groups are more similar to each other based on some similarity measure than they are to observations in other groups. Clustering is often used to explore a dataset. For example, given a collection of movie reviews, a clustering algorithm might discover sets of positive and negative reviews. The system will not be able to label the clusters as "positive" or "negative"; without supervision, it will only have knowledge that the grouped observations are similar to each other by some measure. A common application of clustering is discovering segments of customers within a market for a product. By understanding what attributes are common to particular groups of customers, marketers can decide what aspects of their campaigns need to be emphasized. Clustering is also used by Internet radio services; for example, given a collection of songs, a clustering algorithm might be able to group the songs according to their genres. Using different similarity measures, the same clustering algorithm might group the songs by their keys, or by the instruments they contain. </p><p>
<span class="strong"><strong>Dimensionality </strong></span>
<a class="indexterm" id="id14"/>
<span class="strong"><strong>reduction</strong></span> is another common unsupervised learning task. Some problems may contain thousands or even millions of explanatory variables, which can be computationally costly to work with. Additionally, the program's ability to generalize may be reduced if some of the explanatory variables capture noise or are irrelevant to the underlying relationship. Dimensionality reduction is the process of discovering the explanatory variables that account for the greatest changes in the response variable. Dimensionality reduction can also be used to visualize data. It is easy to visualize a regression problem such as predicting the price of a home from its size; the size of the home can be plotted on the graph's <span class="emphasis"><em>x</em></span> axis, and the price of the home can be plotted on the <span class="emphasis"><em>y</em></span> axis. Similarly, it is easy to visualize the housing price regression problem when a second explanatory variable is added. The number of bathrooms in the house could be plotted on the z axis, for instance. A problem with thousands of explanatory variables, however, becomes impossible to visualize.</p></div>
<div class="section" title="Training data and test data"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Training data and test data</h1></div></div></div><p>The observations in the training set comprise the experience that the algorithm uses to learn. In supervised learning problems, each observation consists of an observed response variable and one or more observed explanatory variables.</p><p>The test set is a similar collection of observations that is used to evaluate the performance of the model using some performance metric. It is important that no observations from the training set are included in the test set. If the test set does contain examples from the training set, it will be difficult to assess whether the algorithm has learned to generalize from the training set or has simply memorized it. A program that generalizes well will be able to effectively perform a task with new data. In contrast, a program that memorizes the training data by learning an overly complex model could predict the values of the response variable for the training set accurately, but will fail to predict the value of the response variable for new examples. </p><p>Memorizing the training set is called <a class="indexterm" id="id15"/>
<span class="strong"><strong>over-fitting</strong></span>. A program that memorizes its observations may not perform its task well, as it could memorize relations and structures that are noise or coincidence. Balancing memorization and generalization, or over-fitting and under-fitting, is a problem common to many machine learning algorithms. In later chapters we will discuss regularization, which can be applied to many models to reduce over-fitting.</p><p>In addition to the training <a class="indexterm" id="id16"/>and test data, a third set of observations, called a <span class="strong"><strong>validation</strong></span> or <a class="indexterm" id="id17"/>
<span class="strong"><strong>hold-out set</strong></span>, is sometimes required. The validation set is used to tune variables called <span class="strong"><strong>hyperparameters</strong></span>, which control how the <a class="indexterm" id="id18"/>model is learned. The program is still evaluated on the test set to provide an estimate of its performance in the real world; its performance on the validation set should not be used as an estimate of the model's real-world performance since the program has been tuned specifically to the validation data. It is common to partition a single set of supervised observations into training, validation, and test sets. There are no requirements for the sizes of the partitions, and they may vary according to the amount of data available. It is common to allocate 50 percent or more of the data to the training set, 25 percent to the test set, and the remainder to the validation set.</p><p>Some training sets may contain only a few hundred observations; others may include millions. Inexpensive storage, increased network connectivity, the ubiquity of sensor-packed smartphones, and shifting attitudes towards privacy have contributed to the contemporary state of big data, or training sets with millions or billions of examples. While this book will not work with datasets that require parallel processing on tens or hundreds of machines, the predictive power of many machine learning algorithms improves as the amount of training data increases. However, machine learning algorithms also follow the maxim "garbage in, garbage out." A student who studies for a test by reading a large, confusing textbook that contains many errors will likely not score better than a student who reads a short but well-written textbook. Similarly, an algorithm trained on a large collection of noisy, irrelevant, or incorrectly labeled data will not perform better than an algorithm trained on a smaller set of data that is more representative of problems in the real world.</p><p>Many supervised training sets are prepared manually, or by semi-automated processes. Creating a large collection of supervised data can be costly in some domains. Fortunately, several datasets are bundled with scikit-learn, allowing developers to focus on experimenting with models instead. During development, and particularly when training data is scarce, a practice called <a class="indexterm" id="id19"/>
<span class="strong"><strong>cross-validation</strong></span> can be used to train and validate an algorithm on the same data. In cross-validation, the training data is partitioned. The algorithm is trained using all but one of the partitions, and tested on the remaining partition. The partitions are then rotated several times so that the algorithm is trained <a class="indexterm" id="id20"/>and evaluated on all of the data. The following diagram <a class="indexterm" id="id21"/>depicts cross-validation with five partitions or <span class="strong"><strong>folds</strong></span>:</p><div class="mediaobject"><img alt="Training data and test data" src="graphics/8365OS_01_01.jpg"/></div><p>The original dataset is partitioned into five subsets of equal size, labeled <span class="strong"><strong>A</strong></span> through <span class="strong"><strong>E</strong></span>. Initially, the model is trained on partitions <span class="strong"><strong>B</strong></span> through <span class="strong"><strong>E</strong></span>, and tested on partition <span class="strong"><strong>A</strong></span>. In the next iteration, the model is trained on partitions <span class="strong"><strong>A</strong></span>, <span class="strong"><strong>C</strong></span>, <span class="strong"><strong>D</strong></span>, and <span class="strong"><strong>E</strong></span>, and tested on partition <span class="strong"><strong>B</strong></span>. The partitions are rotated until models have been trained and tested on all of the partitions. Cross-validation provides a more accurate estimate of the model's performance than testing a single partition of the data.</p></div>
<div class="section" title="Performance measures, bias, and variance"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec16"/>Performance measures, bias, and variance</h1></div></div></div><p>Many metrics can be <a class="indexterm" id="id22"/>used to measure whether or not a program is learning to perform its task more effectively. For supervised learning problems, many performance metrics measure the number of prediction errors. There are two fundamental causes of prediction error: a model's <span class="strong"><strong>bias</strong></span> and its <span class="strong"><strong>variance</strong></span>. Assume that you have many training <a class="indexterm" id="id23"/>sets that are all unique, but equally representative of the population. A model with a high bias will produce similar errors for an input regardless of the training set it was trained with; the model biases its own assumptions about the real relationship over the relationship demonstrated in the training data. A model with high variance, conversely, will <a class="indexterm" id="id24"/>produce different errors for an input depending on the training set that it was trained with. A model with high bias is inflexible, but a model with high variance may be so flexible that it models the noise in the training set. That is, a model with high variance over-fits the training data, while a model with high bias under-fits the training data. It can be helpful to visualize bias and variance as darts thrown at a dartboard. Each dart is analogous to a prediction from a different dataset. A model with high bias but low variance will throw darts that are far from the bull's eye, but tightly clustered. A model with high bias and high variance will throw darts all over the board; the darts are far from the bull's eye and each other. </p><p>A model with low bias and high variance will throw darts that are closer to the bull's eye, but poorly clustered. Finally, a model with low bias and low variance will throw darts that are tightly clustered around the bull's eye, as shown in the following diagram:</p><div class="mediaobject"><img alt="Performance measures, bias, and variance" src="graphics/8365OS_01_02.jpg"/></div><p>Ideally, a model will have both low bias and variance, but efforts to decrease one will frequently increase the other. This is known as the <span class="strong"><strong>bias-variance trade-off</strong></span>. We will discuss the biases and <a class="indexterm" id="id25"/>variances of many of the models introduced in this book.</p><p>Unsupervised learning problems do not have an error signal to measure; instead, performance metrics for unsupervised learning problems measure some attributes of the structure discovered in the data.</p><p>Most performance measures can only be calculated for a specific type of task. Machine learning systems should be evaluated using performance measures that represent the costs associated with making errors in the real world. While this may seem obvious, the following example describes the use of a performance measure that is appropriate for the task in general but not for its specific application.</p><p>Consider a classification task in which a machine learning system observes tumors and must predict whether these <a class="indexterm" id="id26"/>tumors are malignant or benign. <span class="strong"><strong>Accuracy</strong></span>, or the fraction of instances that were classified correctly, is an intuitive measure of the program's performance. While accuracy does measure the program's performance, it does not differentiate between malignant tumors that were classified as being benign, and benign tumors that were classified as being malignant. In some applications, the costs associated with all types of errors may be the same. In this problem, however, failing to identify malignant tumors is likely to be a more severe error than mistakenly classifying benign tumors as being malignant.</p><p>We can measure each of the possible prediction outcomes to create different views of the classifier's performance. When the system correctly classifies a tumor as being malignant, the prediction is called a <a class="indexterm" id="id27"/>
<span class="strong"><strong>true positive</strong></span>. When the system incorrectly classifies a benign <a class="indexterm" id="id28"/>tumor as being malignant, the prediction is a <span class="strong"><strong>false positive</strong></span>. Similarly, a <span class="strong"><strong>false negative</strong></span> is an incorrect prediction that the tumor is benign, and a <span class="strong"><strong>true negative</strong></span> is a correct prediction that a tumor is benign. These four outcomes can be used to calculate several common measures of classification performance, including <span class="strong"><strong>accuracy</strong></span>, <span class="strong"><strong>precision</strong></span>, and <span class="strong"><strong>recall</strong></span>.</p><p>Accuracy is calculated with the following formula, where <span class="emphasis"><em>TP</em></span> is the number of true positives, <span class="emphasis"><em>TN</em></span> is the number of true negatives, <span class="emphasis"><em>FP</em></span> is the number of false positives, and <span class="emphasis"><em>FN</em></span> is the number of false negatives:</p><div class="mediaobject"><img alt="Performance measures, bias, and variance" src="graphics/8365OS_01_03.jpg"/></div><p>Precision is the <a class="indexterm" id="id29"/>fraction of the tumors that were predicted to be malignant that are actually malignant. Precision is calculated with the following formula:</p><div class="mediaobject"><img alt="Performance measures, bias, and variance" src="graphics/8365OS_01_04.jpg"/></div><p>Recall is the fraction of <a class="indexterm" id="id30"/>malignant tumors that the system identified. Recall is calculated with the following formula:</p><div class="mediaobject"><img alt="Performance measures, bias, and variance" src="graphics/8365OS_01_05.jpg"/></div><p>In this example, precision measures the fraction of tumors that were predicted to be malignant that are actually malignant. Recall measures the fraction of truly malignant tumors that were detected.</p><p>The precision and recall measures could reveal that a classifier with impressive accuracy actually fails to detect most of the malignant tumors. If most tumors are benign, even a classifier that never predicts malignancy could have high accuracy. A different classifier with lower accuracy and higher recall might be better suited to the task, since it will detect more of the malignant tumors.</p><p>Many other performance measures for classification can be used; we will discuss some, including metrics for multilabel classification problems, in later chapters. In the next chapter, we will discuss some common performance measures for regression tasks. </p></div>
<div class="section" title="An introduction to scikit-learn"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec17"/>An introduction to scikit-learn</h1></div></div></div><p>Since its release in <a class="indexterm" id="id31"/>2007, scikit-learn has become one of the most popular open source machine learning libraries for Python. scikit-learn provides algorithms for machine learning tasks including classification, regression, dimensionality reduction, and clustering. It also provides modules for extracting features, processing data, and evaluating models.</p><p>Conceived as an extension to the SciPy library, scikit-learn is built on the popular Python libraries NumPy and matplotlib. NumPy extends Python to support efficient operations on large arrays and multidimensional matrices. matplotlib provides visualization tools, and SciPy provides modules for scientific computing.</p><p>scikit-learn is popular for academic research because it has a well-documented, easy-to-use, and versatile API. Developers can use scikit-learn to experiment with different algorithms by changing only a few lines of the code. scikit-learn wraps some popular implementations of machine learning algorithms, such as LIBSVM and LIBLINEAR. Other Python libraries, including NLTK, include wrappers for scikit-learn. scikit-learn also includes a variety of datasets, allowing developers to focus on algorithms rather than obtaining and cleaning data.</p><p>Licensed under the permissive BSD license, scikit-learn can be used in commercial applications without restrictions. Many of scikit-learn's algorithms are fast and scalable to all but massive datasets. Finally, scikit-learn is noted for its reliability; much of the library is covered by automated tests.</p></div>
<div class="section" title="Installing scikit-learn"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec18"/>Installing scikit-learn</h1></div></div></div><p>This book is written <a class="indexterm" id="id32"/>for version 0.15.1 of scikit-learn; use this version to ensure that the examples run correctly. If you have previously installed scikit-learn, you can retrieve the version number with the following code:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; sklearn.__version__
'0.15.1'</pre></div><p>If you have not previously installed scikit-learn, you can install it from a package manager or build it from the source. We will review the installation processes for Linux, OS X, and Windows <a class="indexterm" id="id33"/>in the following sections, but refer to <a class="ulink" href="http://scikit-learn.org/stable/install.html">http://scikit-learn.org/stable/install.html</a> for the latest instructions. The following instructions only <a class="indexterm" id="id34"/>assume that you have installed Python 2.6, Python 2.7, or Python 3.2 or newer. Go to <a class="ulink" href="http://www.python.org/download/">http://www.python.org/download/</a> for instructions on how to install Python.</p><div class="section" title="Installing scikit-learn on Windows"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec03"/>Installing scikit-learn on Windows</h2></div></div></div><p>scikit-learn <a class="indexterm" id="id35"/>requires Setuptools, a third-party package that <a class="indexterm" id="id36"/>supports packaging and installing software for Python. Setuptools can <a class="indexterm" id="id37"/>be installed on Windows by running the bootstrap script at <a class="ulink" href="https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py">https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py</a>.</p><p>Windows binaries for the 32- and 64-bit versions of scikit-learn are also available. If you cannot determine which version you need, install the 32-bit version. Both versions depend on NumPy 1.3 or newer. The <a class="indexterm" id="id38"/>32-bit version of NumPy can be <a class="indexterm" id="id39"/>downloaded from <a class="ulink" href="http://sourceforge.net/projects/numpy/files/NumPy/">http://sourceforge.net/projects/numpy/files/NumPy/</a>. The 64-bit version can be downloaded from <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn">http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn</a>.</p><p>A Windows <a class="indexterm" id="id40"/>installer for the 32-bit version of scikit-learn can be downloaded from <a class="ulink" href="http://sourceforge.net/projects/scikit-learn/files/">http://sourceforge.net/projects/scikit-learn/files/</a>. An installer for the 64-bit <a class="indexterm" id="id41"/>version of scikit-learn can be downloaded from <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn">http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn</a>.</p><p>scikit-learn can also <a class="indexterm" id="id42"/>be built from the source code on Windows. Building requires a C/C++ compiler such as MinGW (<a class="ulink" href="http://www.mingw.org/">http://www.mingw.org/</a>), NumPy, SciPy, and Setuptools.</p><p>To <a class="indexterm" id="id43"/>build, clone the Git repository from <a class="ulink" href="https://github.com/scikit-learn/scikit-learn">https://github.com/scikit-learn/scikit-learn</a> and execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python setup.py install</strong></span>
</pre></div></div><div class="section" title="Installing scikit-learn on Linux"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec04"/>Installing scikit-learn on Linux</h2></div></div></div><p>There are several <a class="indexterm" id="id44"/>options to install scikit-learn on Linux, depending on your distribution. The preferred option to install scikit-learn on Linux is to use <code class="literal">pip</code>. You <a class="indexterm" id="id45"/>may also install it using a package manager, or build scikit-learn from its source.</p><p>To install scikit-learn using <code class="literal">pip</code>, execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo pip install scikit-learn</strong></span>
</pre></div><p>To build scikit-learn, clone the Git repository from <a class="ulink" href="https://github.com/scikit-learn/scikit-learn">https://github.com/scikit-learn/scikit-learn</a>. Then install the following dependencies:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install python-dev python-numpy python-numpy-dev python-setuptools python-numpy-dev python-scipy libatlas-dev g++</strong></span>
</pre></div><p>Navigate to the repository's directory and execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python setup.py install</strong></span>
</pre></div></div><div class="section" title="Installing scikit-learn on OS X"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec05"/>Installing scikit-learn on OS X</h2></div></div></div><p>scikit-learn can <a class="indexterm" id="id46"/>be installed on OS X using Macports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo port install py26-sklearn</strong></span>
</pre></div><p>If Python 2.7 is installed, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo port install py27-sklearn</strong></span>
</pre></div><p>scikit-learn can <a class="indexterm" id="id47"/>also be installed using <code class="literal">pip</code> with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pip install scikit-learn</strong></span>
</pre></div></div><div class="section" title="Verifying the installation"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec06"/>Verifying the installation</h2></div></div></div><p>To verify that <a class="indexterm" id="id48"/>scikit-learn has been installed correctly, open a Python console and execute the following:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; sklearn.__version__
'0.15.1'</pre></div><p>To run scikit-learn's unit tests, first install the <code class="literal">nose</code> library. Then execute the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>nosetest sklearn –exe</strong></span>
</pre></div><p>Congratulations! You've successfully installed scikit-learn.</p></div></div>
<div class="section" title="Installing pandas and matplotlib"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec19"/>Installing pandas and matplotlib</h1></div></div></div><p>pandas is an open <a class="indexterm" id="id49"/>source library that provides data structures and analysis tools for Python. pandas is a powerful library, and several books describe how to use pandas for data analysis. We will use a few of panda's convenient tools for importing data and calculating summary statistics.</p><p>pandas can be installed on Windows, OS X, and Linux using <code class="literal">pip</code> with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pip install pandas</strong></span>
</pre></div><p>pandas can also be installed on Debian- and Ubuntu-based Linux distributions using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>apt-get install python-pandas</strong></span>
</pre></div><p>matplotlib is a library used to easily create plots, histograms, and other charts with Python. We will use it to visualize training data and models. matplotlib has several dependencies. Like pandas, matplotlib depends on NumPy, which should already be installed. On Debian- and Ubuntu-based Linux distributions, matplotlib and its dependencies can be installed using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>apt-get install python-matplotlib</strong></span>
</pre></div><p>Binaries <a class="indexterm" id="id50"/>for OS X and Windows can be downloaded from <a class="ulink" href="http://matplotlib.org/downloads.html">http://matplotlib.org/downloads.html</a>.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec20"/>Summary</h1></div></div></div><p>In this chapter we defined machine-learning as the design and study of programs that can improve their performance of a task by learning from experience. We discussed the spectrum of supervision in experience. At one end of the spectrum is supervised learning, in which a program learns from inputs that are labeled with their corresponding outputs. At the opposite end of the spectrum is unsupervised learning, in which the program must discover hidden structure in unlabeled data. Semi-supervised approaches make use of both labeled and unlabeled training data.</p><p>We discussed common types of machine learning tasks and reviewed example applications. In classification tasks the program must predict the value of a discrete response variable from the explanatory variables. In regression tasks the program must predict the value of a continuous response variable from the explanatory variables. In regression tasks, the program must predict the value of a continuous response variable from the explanatory variables. Unsupervised learning tasks include clustering, in which observations are organized into groups according to some similarity measure and dimensionality reduction, which reduces a set of explanatory variables to a smaller set of synthetic features that retain as much information as possible. We also reviewed the bias-variance trade-off and discussed common performance measures for different machine learning tasks.</p><p>We also discussed the history, goals, and advantages of scikit-learn. Finally, we prepared our development environment by installing scikit-learn and other libraries that are commonly used in conjunction with it. In the next chapter, we will discuss the regression task in more detail, and build our first machine learning model with scikit-learn.</p></div></body></html>
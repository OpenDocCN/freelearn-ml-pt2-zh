- en: Chapter 8. Finding Patterns – Market Basket Analysis Using Association Rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Think back to the last time you made an impulse purchase. Maybe you were waiting
    in the grocery store checkout lane and bought a pack of chewing gum or a candy
    bar. Perhaps on a late-night trip for diapers and formula you picked up a caffeinated
    beverage or a six-pack of beer. You might have even bought this book on a whim
    on a bookseller's recommendation. These impulse buys are no coincidence, as retailers
    use sophisticated data analysis techniques to identify patterns that will drive
    retail behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In years past, such recommendation systems were based on the subjective intuition
    of marketing professionals and inventory managers or buyers. More recently, as
    barcode scanners, computerized inventory systems, and online shopping trends have
    built a wealth of transactional data, machine learning has been increasingly applied
    to learn purchasing patterns. The practice is commonly known as **market basket
    analysis** due to the fact that it has been so frequently applied to supermarket
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the technique originated with shopping data, it is useful in other
    contexts as well. By the time you finish this chapter, you will be able to apply
    market basket analysis techniques to your own tasks, whatever they may be. Generally,
    the work involves:'
  prefs: []
  type: TYPE_NORMAL
- en: Using simple performance measures to find associations in large databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the peculiarities of transactional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing how to identify the useful and actionable patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results of a market basket analysis are actionable patterns. Thus, as we
    apply the technique, you are likely to identify applications to your work, even
    if you have no affiliation with a retail chain.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding association rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The building blocks of a market basket analysis are the items that may appear
    in any given transaction. Groups of one or more items are surrounded by brackets
    to indicate that they form a set, or more specifically, an **itemset** that appears
    in the data with some regularity. Transactions are specified in terms of itemsets,
    such as the following transaction that might be found in a typical grocery store:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding association rules](img/B03905_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The result of a market basket analysis is a collection of **association rules**
    that specify patterns found in the relationships among items he itemsets. Association
    rules are always composed from subsets of itemsets and are denoted by relating
    one itemset on the left-hand side (LHS) of the rule to another itemset on the
    right-hand side (RHS) of the rule. The LHS is the condition that needs to be met
    in order to trigger the rule, and the RHS is the expected result of meeting that
    condition. A rule identified from the example transaction might be expressed in
    the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding association rules](img/B03905_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In plain language, this association rule states that if peanut butter and jelly
    are purchased together, then bread is also likely to be purchased. In other words,
    "peanut butter and jelly imply bread."
  prefs: []
  type: TYPE_NORMAL
- en: Developed in the context of retail transaction databases, association rules
    are not used for prediction, but rather for unsupervised knowledge discovery in
    large databases. This is unlike the classification and numeric prediction algorithms
    presented in previous chapters. Even so, you will find that association rule learners
    are closely related to and share many features of the classification rule learners
    presented in [Chapter 5](ch05.html "Chapter 5. Divide and Conquer – Classification
    Using Decision Trees and Rules"), *Divide and Conquer – Classification Using Decision
    Trees and Rules*.
  prefs: []
  type: TYPE_NORMAL
- en: Because association rule learners are unsupervised, there is no need for the
    algorithm to be trained; data does not need to be labeled ahead of time. The program
    is simply unleashed on a dataset in the hope that interesting associations are
    found. The downside, of course, is that there isn't an easy way to objectively
    measure the performance of a rule learner, aside from evaluating them for qualitative
    usefulness—typically, an eyeball test of some sort.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although association rules are most often used for market basket analysis,
    they are helpful for finding patterns in many different types of data. Other potential
    applications include:'
  prefs: []
  type: TYPE_NORMAL
- en: Searching for interesting and frequently occurring patterns of DNA and protein
    sequences in cancer data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding patterns of purchases or medical claims that occur in combination with
    fraudulent credit card or insurance use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying combinations of behavior that precede customers dropping their cellular
    phone service or upgrading their cable television package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rule analysis is used to search for interesting connections among
    a very large number of elements. Human beings are capable of such insight quite
    intuitively, but it often takes expert-level knowledge or a great deal of experience
    to do what a rule learning algorithm can do in minutes or even seconds. Additionally,
    some datasets are simply too large and complex for a human being to find the needle
    in the haystack.
  prefs: []
  type: TYPE_NORMAL
- en: The Apriori algorithm for association rule learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as it is challenging for humans, transactional data makes association rule
    mining a challenging task for machines as well. Transactional datasets are typically
    extremely large, both in terms of the number of transactions as well as the number
    of items or features that are monitored. The problem is that the number of potential
    itemsets grows exponentially with the number of features. Given *k* items that
    can appear or not appear in a set, there are *2^k* possible itemsets that could
    be potential rules. A retailer that sells only 100 different items could have
    on the order of *2^100 = 1.27e+30* itemsets that an algorithm must evaluate—a
    seemingly impossible task.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than evaluating each of these itemsets one by one, a smarter rule learning
    algorithm takes advantage of the fact that, in reality, many of the potential
    combinations of items are rarely, if ever, found in practice. For instance, even
    if a store sells both automotive items and women's cosmetics, a set of *{motor
    oil, lipstick}* is likely to be extraordinarily uncommon. By ignoring these rare
    (and, perhaps, less important) combinations, it is possible to limit the scope
    of the search for rules to a more manageable size.
  prefs: []
  type: TYPE_NORMAL
- en: Much work has been done to identify heuristic algorithms for reducing the number
    of itemsets to search. Perhaps the most-widely used approach for efficiently searching
    large databases for rules is known as **Apriori**. Introduced in 1994 by Rakesh
    Agrawal and Ramakrishnan Srikant, the Apriori algorithm has since become somewhat
    synonymous with association rule learning. The name is derived from the fact that
    the algorithm utilizes a simple prior (that is, *a priori*) belief about the properties
    of frequent itemsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we discuss that in more depth, it''s worth noting that this algorithm,
    like all learning algorithms, is not without its strengths and weaknesses. Some
    of these are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Is capable of working with large amounts of transactional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results in rules that are easy to understand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful for "data mining" and discovering unexpected knowledge in databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Not very helpful for small datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires effort to separate the true insight from common sense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to draw spurious conclusions from random patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted earlier, the Apriori algorithm employs a simple *a priori* belief
    to reduce the association rule search space: all subsets of a frequent itemset
    must also be frequent. This heuristic is known as the **Apriori property**. Using
    this astute observation, it is possible to dramatically limit the number of rules
    to be searched. For example, the set *{motor oil, lipstick}* can only be frequent
    if both *{motor oil}* and *{lipstick}* occur frequently as well. Consequently,
    if either motor oil or lipstick is infrequent, any set containing these items
    can be excluded from the search.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For additional details on the Apriori algorithm, refer to: Agrawal R, Srikant
    R. Fast algorithms for mining association rules. *Proceedings of the 20th International
    Conference on Very Large Databases*. 1994:487-499.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this principle can be applied in a more realistic setting, let''s
    consider a simple transaction database. The following table shows five completed
    transactions in an imaginary hospital''s gift shop:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transaction number | Purchased items |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | *{flowers, get well card, soda}* |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | *{plush toy bear, flowers, balloons, candy bar}* |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | *{get well card, candy bar, flowers}* |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | *{plush toy bear, balloons, soda}* |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | *{flowers, get well card, soda}* |'
  prefs: []
  type: TYPE_TB
- en: By looking at the sets of purchases, one can infer that there are a couple of
    typical buying patterns. A person visiting a sick friend or family member tends
    to buy a get well card and flowers, while visitors to new mothers tend to buy
    plush toy bears and balloons. Such patterns are notable because they appear frequently
    enough to catch our interest; we simply apply a bit of logic and subject matter
    experience to explain the rule.
  prefs: []
  type: TYPE_NORMAL
- en: In a similar fashion, the Apriori algorithm uses statistical measures of an
    itemset's "interestingness" to locate association rules in much larger transaction
    databases. In the sections that follow, we will discover how Apriori computes
    such measures of interest and how they are combined with the Apriori property
    to reduce the number of rules to be learned.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring rule interest – support and confidence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whether or not an association rule is deemed interesting is determined by two
    statistical measures: support and confidence measures. By providing minimum thresholds
    for each of these metrics and applying the Apriori principle, it is easy to drastically
    limit the number of rules reported, perhaps even to the point where only the obvious
    or common sense rules are identified. For this reason, it is important to carefully
    understand the types of rules that are excluded under these criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **support** of an itemset or rule measures how frequently it occurs in
    the data. For instance the itemset *{get well card, flowers}*, has support of
    *3 / 5 = 0.6* in the hospital gift shop data. Similarly, the support for *{get
    well card} → {flowers}* is also 0.6\. The support can be calculated for any itemset
    or even a single item; for instance, the support for *{candy bar}* is *2 / 5 =
    0.4*, since candy bars appear in 40 percent of purchases. A function defining
    support for the itemset *X* can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring rule interest – support and confidence](img/B03905_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N* is the number of transactions in the database and *count(X)* is the
    number of transactions containing itemset *X*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A rule''s **confidence** is a measurement of its predictive power or accuracy.
    It is defined as the support of the itemset containing both *X* and *Y* divided
    by the support of the itemset containing only *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring rule interest – support and confidence](img/B03905_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, the confidence tells us the proportion of transactions where the
    presence of item or itemset *X* results in the presence of item or itemset *Y*.
    Keep in mind that the confidence that *X* leads to *Y* is not the same as the
    confidence that *Y* leads to *X*. For example, the confidence of *{flowers} →
    {get well card}* is *0.6 / 0.8 = 0.75*. In comparison, the confidence of *{get
    well card} → {flowers}* is *0.6 / 0.6 = 1.0*. This means that a purchase involving
    flowers is accompanied by a purchase of a get well card 75 percent of the time,
    while a purchase of a get well card is associated with flowers 100 percent of
    the time. This information could be quite useful to the gift shop management.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed similarities between support, confidence, and the Bayesian
    probability rules covered in [Chapter 4](ch04.html "Chapter 4. Probabilistic Learning
    – Classification Using Naive Bayes"), *Probabilistic Learning – Classification
    Using Naive Bayes*. In fact, *support(A, B)* is the same as *P(A∩B)* and *confidence(A
    → B)* is the same as *P(B|A)*. It is just the context that differs.
  prefs: []
  type: TYPE_NORMAL
- en: Rules like *{get well card} → {flowers}* are known as **strong rules**, because
    they have both high support and confidence. One way to find more strong rules
    would be to examine every possible combination of the items in the gift shop,
    measure the support and confidence value, and report back only those rules that
    meet certain levels of interest. However, as noted before, this strategy is generally
    not feasible for anything but the smallest of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will see how the Apriori algorithm uses the minimum
    levels of support and confidence with the Apriori principle to find strong rules
    quickly by reducing the number of rules to a more manageable level.
  prefs: []
  type: TYPE_NORMAL
- en: Building a set of rules with the Apriori principle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that the Apriori principle states that all subsets of a frequent itemset
    must also be frequent. In other words, if *{A, B}* is frequent, then *{A}* and
    *{B}* must both be frequent. Recall also that by definition, the support indicates
    how frequently an itemset appears in the data. Therefore, if we know that *{A}*
    does not meet a desired support threshold, there is no reason to consider *{A,
    B}* or any itemset containing *{A}*; it cannot possibly be frequent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Apriori algorithm uses this logic to exclude potential association rules
    prior to actually evaluating them. The actual process of creating rules occurs
    in two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying all the itemsets that meet a minimum support threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating rules from these itemsets using those meeting a minimum confidence
    threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first phase occurs in multiple iterations. Each successive iteration involves
    evaluating the support of a set of increasingly large itemsets. For instance,
    iteration 1 involves evaluating the set of 1-item itemsets (1-itemsets), iteration
    2 evaluates 2-itemsets, and so on. The result of each iteration *i* is a set of
    all the *i*-itemsets that meet the minimum support threshold.
  prefs: []
  type: TYPE_NORMAL
- en: All the itemsets from iteration *i* are combined in order to generate candidate
    itemsets for the evaluation in iteration *i + 1*. But the Apriori principle can
    eliminate some of them even before the next round begins. If *{A}*, *{B}*, and
    *{C}* are frequent in iteration 1 while *{D}* is not frequent, iteration 2 will
    consider only *{A, B}*, *{A, C}*, and *{B, C}*. Thus, the algorithm needs to evaluate
    only three itemsets rather than the six that would have been evaluated if the
    sets containing *D* had not been eliminated *a priori*.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with this thought, suppose during iteration 2, it is discovered that
    *{A, B}* and *{B, C}* are frequent, but *{A, C}* is not. Although iteration 3
    would normally begin by evaluating the support for *{A, B, C}*, it is not mandatory
    that this step should occur at all. Why not? The Apriori principle states that
    *{A, B, C}* cannot possibly be frequent, since the subset *{A, C}* is not. Therefore,
    having generated no new itemsets in iteration 3, the algorithm may stop.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the second phase of the Apriori algorithm may begin. Given the
    set of frequent itemsets, association rules are generated from all possible subsets.
    For instance, *{A, B}* would result in candidate rules for *{A} → {B}* and *{B}
    → {A}*. These are evaluated against a minimum confidence threshold, and any rule
    that does not meet the desired confidence level is eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: Example – identifying frequently purchased groceries with association rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted in this chapter's introduction, market basket analysis is used behind
    the scenes for the recommendation systems used in many brick-and-mortar and online
    retailers. The learned association rules indicate the combinations of items that
    are often purchased together. Knowledge of these patterns provides insight into
    new ways a grocery chain might optimize the inventory, advertise promotions, or
    organize the physical layout of the store. For instance, if shoppers frequently
    purchase coffee or orange juice with a breakfast pastry, it may be possible to
    increase profit by relocating pastries closer to coffee and juice.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will perform a market basket analysis of transactional
    data from a grocery store. However, the techniques could be applied to many different
    types of problems, from movie recommendations, to dating sites, to finding dangerous
    interactions among medications. In doing so, we will see how the Apriori algorithm
    is able to efficiently evaluate a potentially massive set of association rules.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our market basket analysis will utilize the purchase data collected from one
    month of operation at a real-world grocery store. The data contains 9,835 transactions
    or about 327 transactions per day (roughly 30 transactions per hour in a 12-hour
    business day), suggesting that the retailer is not particularly large, nor is
    it particularly small.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The dataset used here was adapted from the `Groceries` dataset in the `arules`
    R package. For more information, see: Hahsler M, Hornik K, Reutterer T. Implications
    of probabilistic data modeling for mining association rules. In: Gaul W, Vichi
    M, Weihs C, ed. *Studies in Classification, Data Analysis, and Knowledge Organization:
    from Data and Information Analysis to Knowledge Engineering*. New York: Springer;
    2006:598–605.'
  prefs: []
  type: TYPE_NORMAL
- en: The typical grocery store offers a huge variety of items. There might be five
    brands of milk, a dozen different types of laundry detergent, and three brands
    of coffee. Given the moderate size of the retailer, we will assume that they are
    not terribly concerned with finding rules that apply only to a specific brand
    of milk or detergent. With this in mind, all brand names can be removed from the
    purchases. This reduces the number of groceries to a more manageable 169 types,
    using broad categories such as chicken, frozen meals, margarine, and soda.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you hope to identify highly specific association rules—such as whether customers
    prefer grape or strawberry jelly with their peanut butter—you will need a tremendous
    amount of transactional data. Large chain retailers use databases of many millions
    of transactions in order to find associations among particular brands, colors,
    or flavors of items.
  prefs: []
  type: TYPE_NORMAL
- en: Do you have any guesses about which types of items might be purchased together?
    Will wine and cheese be a common pairing? Bread and butter? Tea and honey? Let's
    dig into this data and see whether we can confirm our guesses.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transactional data is stored in a slightly different format than that we used
    previously. Most of our prior analyses utilized data in the matrix form where
    rows indicated example instances and columns indicated features. Given the structure
    of the matrix format, all examples are required to have exactly the same set of
    features.
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, transactional data is a more free form. As usual, each row in
    the data specifies a single example—in this case, a transaction. However, rather
    than having a set number of features, each record comprises a comma-separated
    list of any number of items, from one to many. In essence, the features may differ
    from example to example.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To follow along with this analysis, download the `groceries.csv` file from the
    Packt Publishing website and save it in your R working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first five rows of the raw `grocery.csv` file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These lines indicate five separate grocery store transactions. The first transaction
    included four items: citrus fruit, semi-finished bread, margarine, and ready soups.
    In comparison, the third transaction included only one item: whole milk.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we try to load the data using the `read.csv()` function as we did in
    the prior analyses. R would happily comply and read the data into a matrix form
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 2 – exploring and preparing the data](img/B03905_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You will notice that R created four columns to store the items in the transactional
    data: `V1`, `V2`, `V3`, and `V4`. Although this may seem reasonable this, if we
    use the data in this form, we will encounter problems later may seem reasonable,
    R chose to create four variables because the first line had exactly four comma-separated
    values. However, we know that grocery purchases can contain more than four items;
    in the four column design such transactions will be broken across multiple rows
    in the matrix. We could try to remedy this by putting the transaction with the
    largest number of items at the top of the file, but this ignores another more
    problematic issue.'
  prefs: []
  type: TYPE_NORMAL
- en: By structuring data this way, R has constructed a set of features that record
    not just the items in the transactions, but also the order in which they appear.
    If we imagine our learning algorithm as an attempt to find a relationship among
    `V1`, `V2`, `V3`, and `V4`, then whole milk in `V1` might be treated differently
    than the whole milk appearing in `V2`. Instead, we need a dataset that does not
    treat a transaction as a set of positions to be filled (or not filled) with specific
    items, but rather as a market basket that either contains or does not contain
    each particular item.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – creating a sparse matrix for transaction data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The solution to this problem utilizes a data structure called a **sparse matrix**.
    You may recall that we used a sparse matrix to process text data in [Chapter 4](ch04.html
    "Chapter 4. Probabilistic Learning – Classification Using Naive Bayes"), *Probabilistic
    Learning – Classification Using Naive Bayes*. Just as with the preceding dataset,
    each row in the sparse matrix indicates a transaction. However, the sparse matrix
    has a column (that is, feature) for every item that could possibly appear in someone's
    shopping bag. Since there are 169 different items in our grocery store data, our
    sparse matrix will contain 169 columns.
  prefs: []
  type: TYPE_NORMAL
- en: Why not just store this as a data frame as we did in most of our analyses? The
    reason is that as additional transactions and items are added, a conventional
    data structure quickly becomes too large to fit in the available memory. Even
    with the relatively small transactional dataset used here, the matrix contains
    nearly 1.7 million cells, most of which contain zeros (hence, the name "sparse"
    matrix—there are very few nonzero values). Since there is no benefit to storing
    all these zero values, a sparse matrix does not actually store the full matrix
    in memory; it only stores the cells that are occupied by an item. This allows
    the structure to be more memory efficient than an equivalently sized matrix or
    data frame.
  prefs: []
  type: TYPE_NORMAL
- en: In order to create the sparse matrix data structure from the transactional data,
    we can use the functionality provided by the `arules` package. Install and load
    the package using the `install.packages("arules")` and `library(arules)` commands.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information on the arules package, refer to: Hahsler M, Gruen B, Hornik
    K. arules – a computational environment for mining association rules and frequent
    item sets. *Journal of Statistical Software. 2005; 14*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''re loading the transactional data, we cannot simply use the `read.csv()`
    function used previously. Instead, `arules` provides a `read.transactions()` function
    that is similar to `read.csv()` with the exception that it results in a sparse
    matrix suitable for transactional data. The `sep = ","` parameter specifies that
    items in the input file are separated by a comma. To read the `groceries.csv`
    data into a sparse matrix named `groceries`, type the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To see some basic information about the `groceries` matrix we just created,
    use the `summary()` function on the object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first block of information in the output (as shown previously) provides
    a summary of the sparse matrix we created. The output `9835 rows` refers to the
    number of transactions, and the output `169 columns` refers to the 169 different
    items that might appear in someone's grocery basket. Each cell in the matrix is
    `1` if the item was purchased for the corresponding transaction, or `0` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The **density** value of `0.02609146` (2.6 percent) refers to the proportion
    of nonzero matrix cells. Since there are *9,835 * 169 = 1,662,115* positions in
    the matrix, we can calculate that a total of *1,662,115 * 0.02609146 = 43,367*
    items were purchased during the store's 30 days of operation (ignoring the fact
    that duplicates of the same items might have been purchased). With an additional
    step, we can determine that the average transaction contained *43,367 / 8,835
    = 4.409* distinct grocery items. Of course, if we look a little further down the
    output, we'll see that the mean number of items per transaction has already been
    provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next block of the `summary()` output lists the items that were most commonly
    found in the transactional data. Since *2,513 / 9,835 = 0.2555*, we can determine
    that whole milk appeared in 25.6 percent of the transactions. The other vegetables,
    rolls/buns, soda, and yogurt round out the list of other common items, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we are presented with a set of statistics about the size of the transactions.
    A total of 2,159 transactions contained only a single item, while one transaction
    had 32 items. The first quartile and median purchase sizes are two and three items,
    respectively, implying that 25 percent of the transactions contained two or fewer
    items and the transactions were split in half between those with less than three
    items and those with more. The mean of 4.409 items per transaction matches the
    value we calculated by hand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `arules` package includes some useful features for examining transaction
    data. To look at the contents of the sparse matrix, use the `inspect()` function
    in combination with the vector operators. The first five transactions can be viewed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These transactions match our look at the original CSV file. To examine a particular
    item (that is, a column of data), it is possible use the `[row, column]` matrix
    notion. Using this with the `itemFrequency()` function allows us to see the proportion
    of transactions that contain the item. This allows us, for instance, to view the
    support level for the first three items in the grocery data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that the items in the sparse matrix are sorted in columns by alphabetical
    order. Abrasive cleaner and artificial sweeteners are found in about 0.3 percent
    of the transactions, while baby cosmetics are found in about 0.06 percent of the
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing item support – item frequency plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To present these statistics visually, use the `itemFrequencyPlot()` function.
    This allows you to produce a bar chart depicting the proportion of transactions
    containing certain items. Since the transactional data contains a very large number
    of items, you will often need to limit the ones appearing in the plot in order
    to produce a legible chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you require these items to appear in a minimum proportion of transactions,
    use `itemFrequencyPlot()` with the `support` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the following plot, this results in a histogram showing the eight
    items in the `groceries` data with at least 10 percent support:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing item support – item frequency plots](img/B03905_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you would rather limit the plot to a specific number of items, the `topN`
    parameter can be used with `itemFrequencyPlot()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram is then sorted by decreasing support, as shown in the following
    diagram of the top 20 items in the `groceries` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing item support – item frequency plots](img/B03905_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the transaction data – plotting the sparse matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to looking at the items, it''s also possible to visualize the entire
    sparse matrix. To do so, use the `image()` function. The command to display the
    sparse matrix for the first five transactions is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The resulting diagram depicts a matrix with 5 rows and 169 columns, indicating
    the 5 transactions and 169 possible items we requested. Cells in the matrix are
    filled with black for transactions (rows) where the item (column) was purchased.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the transaction data – plotting the sparse matrix](img/B03905_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although the preceding diagram is small and may be slightly hard to read, you
    can see that the first, fourth, and fifth transactions contained four items each,
    since their rows have four cells filled in. You can also see that rows three,
    five, two, and four have an item in common (on the right side of the diagram).
  prefs: []
  type: TYPE_NORMAL
- en: This visualization can be a useful tool for exploring data. For one, it may
    help with the identification of potential data issues. Columns that are filled
    all the way down could indicate items that are purchased in every transaction—a
    problem that could arise, perhaps, if a retailer's name or identification number
    was inadvertently included in the transaction dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, patterns in the diagram may help reveal actionable insights within
    the transactions and items, particularly if the data is sorted in interesting
    ways. For example, if the transactions are sorted by date, the patterns in black
    dots could reveal seasonal effects in the number or types of items purchased.
    Perhaps around Christmas or Hanukkah, toys are more common; around Halloween,
    perhaps candies become popular. This type of visualization could be especially
    powerful if the items were also sorted into categories. In most cases, however,
    the plot will look fairly random, like static on a television screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that this visualization will not be as useful for extremely large
    transaction databases, because the cells will be too small to discern. Still,
    by combining it with the `sample()` function, you can view the sparse matrix for
    a randomly sampled set of transactions. The command to create random selection
    of 100 transactions is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a matrix diagram with 100 rows and 169 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the transaction data – plotting the sparse matrix](img/B03905_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A few columns seem fairly heavily populated, indicating some very popular items
    at the store. But overall, the distribution of dots seems fairly random. Given
    nothing else of note, let's continue with our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With data preparation completed, we can now work at finding associations among
    shopping cart items. We will use an implementation of the Apriori algorithm in
    the `arules` package we''ve been using to explore and prepare the groceries data.
    You''ll need to install and load this package if you have not done so already.
    The following table shows the syntax to create sets of rules with the `apriori()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 3 – training a model on the data](img/B03905_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although running the `apriori()` function is straightforward, there can sometimes
    be a fair amount of trial and error needed to find the `support` and `confidence`
    parameters that produce a reasonable number of association rules. If you set these
    levels too high, you might find no rules or rules that are too generic to be very
    useful. On the other hand, a threshold too low might result in an unwieldy number
    of rules, or worse, the operation might take a very long time or run out of memory
    during the learning phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, if we attempt to use the default settings of `support = 0.1`
    and `confidence = 0.8`, we will end up with a set of zero rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, we need to widen the search a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you think about it, this outcome should not have been terribly surprising.
    Because `support = 0.1` by default, in order to generate a rule, an item must
    have appeared in at least *0.1 * 9,385 = 938.5* transactions. Since only eight
    items appeared this frequently in our data, it's no wonder that we didn't find
    any rules.
  prefs: []
  type: TYPE_NORMAL
- en: One way to approach the problem of setting a minimum support threshold is to
    think about the smallest number of transactions you would need before you would
    consider a pattern interesting. For instance, you could argue that if an item
    is purchased twice a day (about 60 times in a month of data), it may be an interesting
    pattern. From there, it is possible to calculate the support level needed to find
    only the rules matching at least that many transactions. Since 60 out of 9,835
    equals 0.006, we'll try setting the support there first.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the minimum confidence involves a delicate balance. On one hand, if
    confidence is too low, we might be overwhelmed with a large number of unreliable
    rules—such as dozens of rules indicating the items commonly purchased with batteries.
    How would we know where to target our advertising budget then? On the other hand,
    if we set confidence too high, we will be limited to the rules that are obvious
    or inevitable—similar to the fact that smoke detectors are always purchased in
    combination with batteries. In this case, moving the smoke detectors closer to
    the batteries is unlikely to generate additional revenue, since the two items
    already were almost always purchased together.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The appropriate minimum confidence level depends a great deal on the goals of
    your analysis. If you start with a conservative value, you can always reduce it
    to broaden the search if you aren't finding actionable intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with a confidence threshold of 0.25, which means that in order to
    be included in the results, the rule has to be correct at least 25 percent of
    the time. This will eliminate the most unreliable rules, while allowing some room
    for us to modify behavior with targeted promotions.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to generate some rules. In addition to the minimum `support`
    and `confidence` parameters, it is helpful to set `minlen = 2` to eliminate rules
    that contain fewer than two items. This prevents uninteresting rules from being
    created simply because the item is purchased frequently, for instance, *{}* *→*
    *whole milk*. This rule meets the minimum support and confidence because whole
    milk is purchased in over 25 percent of the transactions, but it isn't a very
    actionable insight.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full command to find a set of association rules using the Apriori algorithm
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This saves our rules in a `rules` object, can take a peek into by typing its
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our `groceryrules` object contains a set of 463 association rules. To determine
    whether any of them are useful, we'll have to dig deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To obtain a high-level overview of the association rules, we can use `summary()`
    as follows. The rule length distribution tells us how many rules have each count
    of items. In our rule set, 150 rules have only two items, while 297 have three,
    and 16 have four. The `summary` statistics associated with this distribution are
    also given:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted in the previous output, the size of the rule is calculated as the total
    of both the left-hand side (`lhs`) and right-hand side (`rhs`) of the rule. This
    means that a rule like *{bread}* *→* *{butter}* is two items and *{peanut butter,
    jelly}* *→* *{bread}* is three.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we see the summary statistics of the rule quality measures: support,
    confidence, and lift. The support and confidence measures should not be very surprising,
    since we used these as selection criteria for the rules. We might be alarmed if
    most or all of the rules had support and confidence very near the minimum thresholds,
    as this would mean that we may have set the bar too high. This is not the case
    here, as there are many rules with much higher values of each.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The third column is a metric we have not considered yet. The *lift* of a rule
    measures how much more likely one item or itemset is purchased relative to its
    typical rate of purchase, given that you know another item or itemset has been
    purchased. This is defined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 4 – evaluating model performance](img/B03905_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike confidence where the item order matters, *lift(X → Y)* is the same as
    *lift(Y → X)*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose at a grocery store, most people purchase milk and bread.
    By chance alone, we would expect to find many transactions with both milk and
    bread. However, if *lift(milk → bread)* is greater than one, it implies that the
    two items are found together more often than one would expect by chance. A large
    lift value is therefore a strong indicator that a rule is important, and reflects
    a true connection between the items.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final section of the `summary()` output, we receive mining information,
    telling us about how the rules were chosen. Here, we see that the `groceries`
    data, which contained 9,835 transactions, was used to construct rules with a minimum
    support of 0.0006 and minimum confidence of 0.25:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can take a look at specific rules using the `inspect()` function. For instance,
    the first three rules in the `groceryrules` object can be viewed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Step 4 – evaluating model performance](img/B03905_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first rule can be read in plain language as, "if a customer buys potted
    plants, they will also buy whole milk." With support of 0.007 and confidence of
    0.400, we can determine that this rule covers 0.7 percent of the transactions
    and is correct in 40 percent of purchases involving potted plants. The lift value
    tells us how much more likely a customer is to buy whole milk relative to the
    average customer, given that he or she bought a potted plant. Since we know that
    about 25.6 percent of the customers bought whole milk (`support`), while 40 percent
    of the customers buying a potted plant bought whole milk (`confidence`), we can
    compute the lift value as *0.40 / 0.256 = 1.56*, which matches the value shown.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the column labeled `support` indicates the support value for the rule,
    not the support value for the `lhs` or `rhs` alone).
  prefs: []
  type: TYPE_NORMAL
- en: In spite of the fact that the confidence and lift are high, does *{potted plants}*
    *→* *{whole milk}* seem like a very useful rule? Probably not, as there doesn't
    seem to be a logical reason why someone would be more likely to buy milk with
    a potted plant. Yet our data suggests otherwise. How can we make sense of this
    fact?
  prefs: []
  type: TYPE_NORMAL
- en: 'A common approach is to take the association rules and divide them into the
    following three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Actionable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trivial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inexplicable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obviously, the goal of a market basket analysis is to find **actionable** rules
    that provide a clear and useful insight. Some rules are clear, others are useful;
    it is less common to find a combination of both of these factors.
  prefs: []
  type: TYPE_NORMAL
- en: So-called **trivial rules** include any rules that are so obvious that they
    are not worth mentioning—they are clear, but not useful. Suppose you were a marketing
    consultant being paid large sums of money to identify new opportunities for cross-promoting
    items. If you report the finding that *{diapers}* *→* *{formula}*, you probably
    won't be invited back for another consulting job.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trivial rules can also sneak in disguised as more interesting results. For instance,
    say you found an association between a particular brand of children's cereal and
    a certain DVD movie. This finding is not very insightful if the movie's main character
    is on the front of the cereal box.
  prefs: []
  type: TYPE_NORMAL
- en: Rules are **inexplicable** if the connection between the items is so unclear
    that figuring out how to use the information is impossible or nearly impossible.
    The rule may simply be a random pattern in the data, for instance, a rule stating
    that *{pickles}* *→* *{chocolate ice cream}* may be due to a single customer,
    whose pregnant wife had regular cravings for strange combinations of foods.
  prefs: []
  type: TYPE_NORMAL
- en: The best rules are hidden gems—those undiscovered insights into patterns that
    seem obvious once discovered. Given enough time, one could evaluate each and every
    rule to find the gems. However, we (the one performing the market basket analysis)
    may not be the best judge of whether a rule is actionable, trivial, or inexplicable.
    In the next section, we'll improve the utility of our work by employing methods
    to sort and share the learned rules so that the most interesting results might
    float to the top.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Subject matter experts may be able to identify useful rules very quickly, but
    it would be a poor use of their time to ask them to evaluate hundreds or thousands
    of rules. Therefore, it's useful to be able to sort rules according to different
    criteria, and get them out of R into a form that can be shared with marketing
    teams and examined in more depth. In this way, we can improve the performance
    of our rules by making the results more actionable.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting the set of association rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending upon the objectives of the market basket analysis, the most useful
    rules might be the ones with the highest `support, confidence,` or `lift.` The
    `arules` package includes a `sort()` function that can be used to reorder the
    list of rules so that the ones with the highest or lowest values of the quality
    measure come first.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reorder the `groceryrules` object, we can apply `sort()` while specifying
    a `"support"`, `"confidence"`, or `"lift"` value to the by parameter. By combining
    the `sort` function with vector operators, we can obtain a specific number of
    interesting rules. For instance, the best five rules according to the lift statistic
    can be examined using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'These output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sorting the set of association rules](img/B03905_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These rules appear to be more interesting than the ones we looked at previously.
    The first rule, with a `lift` of about 3.96, implies that people who buy herbs
    are nearly four times more likely to buy root vegetables than the typical customer—perhaps
    for a stew of some sort? Rule two is also interesting. Whipped cream is over three
    times more likely to be found in a shopping cart with berries versus other carts,
    suggesting perhaps a dessert pairing?
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, the sort order is decreasing, meaning the largest values come first.
    To reverse this order, add an additional line, `parameterdecreasing = FALSE`.
  prefs: []
  type: TYPE_NORMAL
- en: Taking subsets of association rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose that given the preceding rule, the marketing team is excited about the
    possibilities of creating an advertisement to promote berries, which are now in
    season. Before finalizing the campaign, however, they ask you to investigate whether
    berries are often purchased with other items. To answer this question, we'll need
    to find all the rules that include berries in some form.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `subset()` function provides a method to search for subsets of transactions,
    items, or rules. To use it to find any rules with `berries` appearing in the rule,
    use the following command. It will store the rules in a new object titled `berryrules`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then inspect the rules as we did with the larger set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the following set of rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Taking subsets of association rules](img/B03905_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are four rules involving berries, two of which seem to be interesting
    enough to be called actionable. In addition to whipped cream, berries are also
    purchased frequently with yogurt—a pairing that could serve well for breakfast
    or lunch as well as dessert.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `subset()` function is very powerful. The criteria for choosing the subset
    can be defined with several keywords and operators:'
  prefs: []
  type: TYPE_NORMAL
- en: The keyword `items` explained previously, matches an item appearing anywhere
    in the rule. To limit the subset to where the match occurs only on the left- or
    right-hand side, use `lhs` and `rhs` instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operator `%in%` means that at least one of the items must be found in the
    list you defined. If you want any rules matching either berries or yogurt, you
    could write `items %in%c("berries", "yogurt”)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additional operators are available for partial matching (`%pin%`) and complete
    matching (`%ain%`). Partial matching allows you to find both citrus fruit and
    tropical fruit using one search: `items %pin% "fruit"`. Complete matching requires
    that all the listed items are present. For instance, `items %ain% c("berries",
    "yogurt")` finds only rules with both `berries` and `yogurt`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsets can also be limited by `support`, `confidence`, or `lift`. For instance,
    `confidence > 0.50` would limit you to the rules with confidence greater than
    50 percent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matching criteria can be combined with the standard R logical operators such
    as and (`&`), or (`|`), and not (`!`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these options, you can limit the selection of rules to be as specific
    or general as you would like.
  prefs: []
  type: TYPE_NORMAL
- en: Saving association rules to a file or data frame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To share the results of your market basket analysis, you can save the rules
    to a CSV file with the `write()` function. This will produce a CSV file that can
    be used in most spreadsheet programs including Microsoft Excel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes it is also convenient to convert the rules into an R data frame.
    This can be accomplished easily using the `as()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a data frame with the rules in the factor format, and numeric
    vectors for `support`, `confidence`, and `lift`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You might choose to do this if you want to perform additional processing on
    the rules or need to export them to another database.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Association rules are frequently used to find provide useful insights in the
    massive transaction databases of large retailers. As an unsupervised learning
    process, association rule learners are capable of extracting knowledge from large
    databases without any prior knowledge of what patterns to seek. The catch is that
    it takes some effort to reduce the wealth of information into a smaller and more
    manageable set of results. The Apriori algorithm, which we studied in this chapter,
    does so by setting minimum thresholds of interestingness, and reporting only the
    associations meeting these criteria.
  prefs: []
  type: TYPE_NORMAL
- en: We put the Apriori algorithm to work while performing a market basket analysis
    for a month's worth of transactions at a moderately sized supermarket. Even in
    this small example, a wealth of associations was identified. Among these, we noted
    several patterns that may be useful for future marketing campaigns. The same methods
    we applied are used at much larger retailers on databases many times this size.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine another unsupervised learning algorithm.
    Much like association rules, it is intended to find patterns within data. But
    unlike association rules that seek patterns within the features, the methods in
    the next chapter are concerned with finding connections among the examples.
  prefs: []
  type: TYPE_NORMAL

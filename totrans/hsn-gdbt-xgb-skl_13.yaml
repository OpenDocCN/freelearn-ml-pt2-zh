- en: '*Chapter 10*: XGBoost Model Deployment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter on XGBoost, you will put everything together and develop
    new techniques to build a robust machine learning model that is industry ready.
    Deploying models for industry is a little different than building models for research
    and competitions. In industry, automation is important since new data arrives
    frequently. More emphasis is placed on procedure, and less emphasis is placed
    on gaining minute percentage points by tweaking machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, in this chapter, you will gain significant experience with **one-hot
    encoding** and **sparse matrices**. In addition, you will implement and customize
    scikit-learn transformers to automate a machine learning pipeline to make predictions
    on data that is mixed with **categorical** and **numerical** columns. At the end
    of this chapter, your machine learning pipeline will be ready for any incoming
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding mixed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing scikit-learn transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finalizing an XGBoost model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a machine learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter may be found at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter10](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: Encoding mixed data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine that you are working for an EdTech company and your job is to predict
    student grades to target services aimed at bridging the tech skills gap. Your
    first step is to load data that contains student grades into `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Student Performance dataset, provided by your company, may be accessed by
    loading the `student-por.csv` file that has been imported for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing `pandas` and silencing warnings. Then, download the dataset
    and view the first five rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – The Student Performance dataset as is](img/B15551_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – The Student Performance dataset as is
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the world of industry, where data does not always appear as expected.
  prefs: []
  type: TYPE_NORMAL
- en: A recommended option is to view the CSV file. This can be done in Jupyter Notebooks
    by locating the folder for this chapter and clicking on the `student-por.csv`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – The Student Performance CSV file](img/B15551_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – The Student Performance CSV file
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding figure, the data is separated by semi-colons.
    CSV stands for `pandas` comes with a `sep` parameter, which stands for **separator**,
    that may be set to the semi-colon, (*;*), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – The Student Performance dataset ](img/B15551_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – The Student Performance dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now that the DataFrame looks as expected, with a mix of categorical and numerical
    values, we must clean up the **null values**.
  prefs: []
  type: TYPE_NORMAL
- en: Clearing null values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can view all columns of null values by calling the `.sum()` method on `df.insull()`.
    Here is an excerpt of the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can view the rows of these columns using conditional notation by placing
    `df.isna().any(axis=1)` inside of brackets with `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – The Student Performance null data](img/B15551_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – The Student Performance null data
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s preferable to see the null columns in the middle, which Jupyter removes
    by default on account of the number of columns. This is easily corrected by setting
    `max columns` to `None` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, running the code again shows all the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an excerpt of the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Null data from all rows of the Student Performance dataset](img/B15551_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Null data from all rows of the Student Performance dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all columns including the hidden null values under `'guardian'`
    are now displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical null values may be set to -999.0, or some other value, and XGBoost
    will find the best replacement for you using the `missing` hyperparameter as introduced
    in [*Chapter 5*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117), *XGBoost Unveiled*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to fill the `''age''` column with `-999.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, categorical columns may be filled by the mode. The mode is the most common
    occurrence in a column. Filling categorical columns with the mode may distort
    the resulting distribution, however, only if the number of null values is large.
    There are only two null values present, so our distribution will not be affected.
    Another option includes replacing categorical null values with the '`unknown`'
    string, which may become its own column after one-hot encoding. Note that XGBoost
    requires numerical input, so the `missing` hyperparameter cannot be directly applied
    to categorical columns as of 2020.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code converts the `''sex''` and `''guardian''` categorical columns
    to `mode`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our null values were in the first two rows, we can reveal that they have
    been changed using `df.head()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – The Student Performance dataset with the null values removed
    (first five rows only)](img/B15551_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – The Student Performance dataset with the null values removed (first
    five rows only)
  prefs: []
  type: TYPE_NORMAL
- en: The null values have all been cleared as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will convert all categorical columns to numerical columns using one-hot
    encoding.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously, we used `pd.get_dummies` to transform all categorical variables
    to numerical values of `0` and `1`, with `0` indicating absence and `1` indicating
    presence. While acceptable, this approach has some shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: The first shortcoming is that `pd.get_dummies` can be computationally expensive,
    as you may have found when waiting for code to run in previous chapters. The second
    shortcoming is that `pd.get_dummies` does not translate particularly well to scikit-learn's
    pipelines, a concept that we will explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: A nice alternative to `pd.get_dummies` is scikit-learn's `OneHotEncoder`. Like
    `pd.get_dummies`, one-hot encoding transforms all categorical values to `0` and
    `1`, with `0` indicating absence and `1` indicating presence, but unlike `pd.get_dummies`,
    it is not computationally expensive. `OneHotEncoder` uses a sparse matrix instead
    of a dense matrix to save space and time.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse matrices save space by only storing data with values that do not include
    0\. The same amount of information is conserved by using fewer bits.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, `OneHotEncoder` is a scikit-learn transformer, which means that
    it's specifically designed to work in machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In past versions of scikit-learn, `OneHotEncoder` only accepted numerical input.
    When that was the case, an intermediate step was taken with `LabelEncoder` to
    first convert all categorical columns into numerical columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use `OneHotEncoder` on specific columns, you may use the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert all categorical columns of the `dtype` object into a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import and initialize `OneHotEncoder`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `fit_transform` method on the columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`0` or `1`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you want to see what the `hot` sparse matrix actually looks like, you can
    print it out as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is an excerpt of the results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you want more information about the sparse matrix, just enter the following
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that the matrix is `649` by `43`, but only `11033` values have
    been stored, saving a significant amount of space. Note that for text data, which
    has many zeros, sparse matrices are very common.
  prefs: []
  type: TYPE_NORMAL
- en: Combining a one-hot encoded matrix and numerical columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a one-hot encoded sparse matrix, we must combine it with the
    numerical columns of the original DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s isolate the numerical columns. This may be done with the `exclude=["object"]`
    parameter as input for `df.select_dtypes`, which selects columns of certain types
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – The Student Performance dataset''s numerical columns](img/B15551_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – The Student Performance dataset's numerical columns
  prefs: []
  type: TYPE_NORMAL
- en: These are the columns we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'For data of this size, we have a choice of converting the sparse matrix to
    a regular DataFame, as seen in the preceding screenshot, or converting this DataFrame
    into a sparse matrix. Let''s pursue the latter, considering that DataFrames in
    industry can become enormous and saving space can be advantageous:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert the `cold_df` DataFrame to a compressed sparse matrix, import `csr_matrix`
    from `scipy.sparse` and place the DataFrame inside, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, stack both matrices, hot and cold, by importing and using `hstack`,
    which combines sparse matrices horizontally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that `final_sparse_matrix` works as expected by converting the sparse
    matrix into a dense matrix and by displaying the DataFrame as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.9 – The DataFrame of the final sparse matrix](img/B15551_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – The DataFrame of the final sparse matrix
  prefs: []
  type: TYPE_NORMAL
- en: The output is shifted to the right to show the one-hot encoded and numerical
    columns together.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data is ready for machine learning, let's automate the process
    using transformers and pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing scikit-learn transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a process for transforming the DataFrame into a machine learning-ready
    sparse matrix, it would be advantageous to generalize the process with transformers
    so that it can easily be repeated for new data coming in.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn transformers work with machine learning algorithms by using a `fit`
    method, which finds model parameters, and a `transform` method, which applies
    these parameters to data. These methods may be combined into a single `fit_transform`
    method that fits and transforms data in one line of code.
  prefs: []
  type: TYPE_NORMAL
- en: When used together, various transformers, including machine learning algorithms,
    may work together in the same pipeline for ease of use. Data is then placed in
    the pipeline that is fit and transformed to achieve the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn comes with many great transformers, such as `StandardScaler` and
    `Normalizer` to standardize and normalize data, respectively, and `SimpleImputer`
    to convert null values. You have to be careful, however, when data contains a
    mix of categorical and numerical columns, as is the case here. In some cases,
    the scikit-learn options may not be the best options for automation. In this case,
    it's worth creating your own transformers to do exactly what you want.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to creating your own transformers is to use scikit-learn's `TransformerMixin`
    as your superclass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a general code outline to create a customized transformer in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, you don't have to initialize anything, and `fit` can always
    return `self`. Simply put, you may place all your code for transforming the data
    under the `transform` method.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you see how customization works generally, let's create a customized
    transformer to handle different kinds of null values.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing a mixed null value imputer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's see how this works by creating a customized mixed null value imputer.
    Here, the reason for the customization is to handle different types of columns
    with different approaches to correcting null values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `TransformerMixin` and define a new class with `TransformerMixin` as
    the superclass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the class with `self` as input. It''s okay if this does nothing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `fit` method that takes `self` and `X` as input, with `y=None`, and
    returns `self`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `transform` method that takes `self` and `X` as input, with `y=None`,
    and transforms the data by returning a new `X`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We need to handle null values separately depending on the columns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are the steps to convert null values to the mode or `-999.0`, depending
    upon the column type:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) Loop through the columns by converting them to a list:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'b) Within the loop, access the columns that are strings by checking which columns
    are of the `object` dtype:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'c) Convert the null values of the string (`object`) columns to the mode:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'd) Otherwise, fill the columns with `-999.0`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, you may have wondered why `y=None` is used. The reason
    is that `y` will be needed as an input when including a machine learning algorithm
    in the pipeline. By setting `y` to `None`, changes will only be made to the predictor
    columns as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the customized imputer has been defined, it may be used by calling
    the `fit_transform` method on the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s reset the data by establishing a new DataFrame from the CSV file and
    transform the null values in one line of code using the customized `NullValueImputer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – The Student Performance DataFrame after NullValueImputer()](img/B15551_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – The Student Performance DataFrame after NullValueImputer()
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all null values have been cleared.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's transform the data into a one-hot encoded sparse matrix as before.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding mixed data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will apply similar steps here to those from the previous section by creating
    a customized transformer to one-hot encode the categorical columns before joining
    them with the numerical columns as a sparse matrix (a dense matrix is also okay
    for a dataset of this size):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a new class with `TransformerMixin` as the superclass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the class with `self` as input. It''s okay if this does nothing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `fit` method that takes `self` and `X` as input and returns `self`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `transform` method that takes `self` and `X` as input, transforms
    the data, and returns a new `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the steps to complete the transformation; start by accessing only
    the categorical columns, which are of the `object` type, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) Put the categorical columns in a list:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'b) Initialize `OneHotEncoder`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'c) Transform the categorical columns with `OneHotEncoder`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'd) Create a DataFrame of numerical columns only by excluding strings:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'e) Convert the numerical DataFrame into a sparse matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'f) Combine both sparse matrices into one:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'g) Convert this into a **Compressed Sparse Row** (**CSR**) matrix to limit
    errors. Note that XGBoost requires CSR matrices, and this conversion may happen
    automatically depending on your version of XGBoost:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can transform the `nvi` data with no null values by using the powerful
    `fit_transform` method on `SparseMatrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output, given here, is truncated to save space:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can verify that the data looks as expected by converting the sparse matrix
    back into a dense matrix as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the expected dense output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.11 – The sparse matrix converted into a dense matrix](img/B15551_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – The sparse matrix converted into a dense matrix
  prefs: []
  type: TYPE_NORMAL
- en: This appears correct. The figure shows a value of `0.0` for the 27th column
    and a value of `1.0` for the 28th column. The preceding one-hot encoded output
    excludes (`0`,`27`) and shows a value of `1.0` for (`0`,`28`), matching the dense
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data has been transformed, let's combine both preprocessing steps
    into a single pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When building machine learning models, it's standard to start by separating
    the data into `X` and `y`. When thinking about a pipeline, it makes sense to transform
    `X`, the predictor columns, and not `y`, the target column. Furthermore, it's
    important to hold out a test set for later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before placing data into the machine learning pipeline, let''s split the data
    into training and test sets and leave the test set behind. We start from the top
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, read the CSV file as a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When choosing `X` and `y` for the Student Performance dataset, it''s important
    to note that the last three columns all include student grades. Two potential
    studies are of value here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) Including previous grades as predictor columns
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Not including previous grades as predictor columns
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Assume that your EdTech company wants to make predictions based on socioeconomic
    variables, not on previous grades earned, so ignore the first two grade columns
    indexed as -`2` and -`3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the last column as `y`, and all columns except for the last three as
    `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now import `train_test_split` and split `X` and `y` into a training and a test
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let''s build the pipeline using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First import `Pipeline` from `sklearn.pipeline`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, assign tuples using the syntax (name, transformer) as parameters of `Pipeline`
    in sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, transform `X_train`, our predictor columns, by placing `X_train` inside
    the `fit_transform` method of `data_pipeline`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you have a numerical, sparse matrix with no null values that can be used
    as the predictor column for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, you have a pipeline that may be used to transform any incoming
    data in one line of code! Let's now finalize an XGBoost model to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Finalizing an XGBoost model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s time to build a robust XGBoost model to add to the pipeline. Go ahead
    and import `XGBRegressor`, `numpy`, `GridSearchCV`, `cross_val_score`, `KFold`,
    and `mean_squared_error` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Now let's build the model.
  prefs: []
  type: TYPE_NORMAL
- en: First XGBoost model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This Student Performance dataset has an interesting range of values for the
    predictor column, `y_train`, which can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the values range from `5`-`19` with `0` included.
  prefs: []
  type: TYPE_NORMAL
- en: Since the target column is ordinal, meaning the values are numerically ordered,
    regression is preferable to classification even though the outputs are limited.
    After training a model via regression, the final results may be rounded to give
    the final predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to score `XGBRegressor` with this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by setting up cross-validation using `KFold`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now define a cross-validation function that returns the `cross_val_score`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Establish a base score by calling `cross_val` with the `XGBRegressor` as input
    with `missing=-999.0` so that XGBoost can find the best replacement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a respectable starting score. A root mean squared error of `2.97` out
    of 19 possibilities indicates that the grades are within a couple of points of
    accuracy. This is almost 15%, which is accurate within one letter grade using
    the American A-B-C-D-F system. In industry, you may even include a confidence
    interval using statistics to deliver a prediction interval, a recommended strategy
    that is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a baseline score, let's fine-tune the hyperparameters to improve
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning the XGBoost hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by checking `n_estimators` with early stopping. Recall that to
    use early stopping, we may check one test fold. Creating the test fold requires
    splitting `X_train` and `y_train` further:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a second `train_test_split` that may be used to create a test set for
    validation purposes, making sure to keep the real test set hidden for later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now define a function that uses early stopping to return the optimal number
    of estimators for the regressor (see [*Chapter 6*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136)*,
    XGBoost Hyperparameters*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now run the `n_estimators` function, setting to `5000` as a maximum:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the last five rows of the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using our default model, 31 estimators currently gives the best estimate. That
    will be our starting point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, here is a `grid_search` function, which we have used multiple times,
    that searches a grid of hyperparameters and displays the best parameters and best
    score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are a few recommended steps for fine-tuning the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with `max_depth` ranging from `1` to `8` while setting `n_estimators`
    to `31`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Narrow `max_depth` from `1` to `3` while ranging `min_child_weight` from `1`
    to `5` and holding `n_esimtators` at `31`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is no improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You may guarantee some changes by forcing `min_child_weight` to take on a value
    of `2` or `3` while including a range of `subsample` from `0.5` to `0.9`. Furthermore,
    increasing `n_estimators` may help by giving the model more time to learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The score is nearly the same, but slightly worse.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Narrow `min_child_weight` and `subsample` while using a range of `0.5` to `0.9`
    for `colsample_bytree`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the best score so far.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Holding the best current values, try ranges from `0.6` to `1.0` with `colsample_bynode`
    and `colsample_bylevel`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is given here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The score has improved again.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Further experimentation with the base learner to `dart` and `gamma` resulted
    in no new gains.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the time and the scope of the project, it could be worth tuning
    hyperparameters further, and even trying them all together in `RandomizedSearch`.
    In industry, there is a good chance that you will have access to cloud computing,
    where inexpensive, preemptible **Virtual Machines** (**VMs**) will allow more
    hyperparameter searches to find even better results. Just note that scikit-learn
    currently does not offer a way to stop time-consuming searches to save the best
    parameters before the code completes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a robust model, we can move forward and test the model.
  prefs: []
  type: TYPE_NORMAL
- en: Testing model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a potential final model, it's important to test it against
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the test set was not transformed in our pipeline. Fortunately,
    at this point, it only takes one line of code to transform it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can initialize a model with the best-tuned hyperparameters selected
    in the previous section, fit it on the training set, and test it against the test
    set that was held back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The score is a little higher, although this could be on account of the fold.
  prefs: []
  type: TYPE_NORMAL
- en: If not, our model has fit the validation set a little too closely, which can
    happen when fine-tuning hyperparameters and adjusting them closely to improve
    the validation set. The model generalizes fairly well, but it could generalize
    better.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next steps, when considering whether the score can be improved upon,
    the following options are available:'
  prefs: []
  type: TYPE_NORMAL
- en: Return to hyperparameter fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the model as is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a quick adjustment based on hyperparameter knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quickly adjusting hyperparameters is viable since the model could be overfitting.
    For instance, increasing `min_child_weight` and lowering `subsample` should help
    the model to generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make that final adjustment for a final model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Note that the score has improved.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you should absolutely not go back and forth trying to improve the hold-out
    test score. It is acceptable to make a few adjustments after receiving the test
    score, however; otherwise, you could never improve upon the first result.
  prefs: []
  type: TYPE_NORMAL
- en: Now all that remains is to complete the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building a machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Completing the machine learning pipeline requires adding the machine learning
    model to the previous pipeline. You need a machine learning tuple after `NullValueImputer`
    and `SparseMatrix` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'This pipeline is now complete with a machine learning model, and it can be
    fit on any `X`, `y` combination, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can make predictions on any data whose target column is unknown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the first few rows of the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'To get realistic predictions, the data may be rounded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if new data comes through, it can be concatenated with the previous
    data and placed through the same pipeline for a stronger model, since the new
    model may be fit on more data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, this model may be used to make predictions on new data, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: There is one small catch.
  prefs: []
  type: TYPE_NORMAL
- en: What if you want to make a prediction on only one row of data? If you run a
    single row through the pipeline, the resulting sparse matrix will not have the
    correct number of columns, since it will only one-hot encode categories that are
    present in the single row. This will result in a *mismatch* error in the data,
    since the machine learning model has been fit to a sparse matrix that requires
    more rows of data.
  prefs: []
  type: TYPE_NORMAL
- en: A simple solution is to concatenate the new row of data with enough rows of
    data to guarantee that the full sparse matrix is present with all possible categorical
    columns transformed. We have seen that this works with 25 rows from `X_test` since
    there were no errors. Using 20 or fewer rows from `X_test` will result in a mismatch
    error in this particular case.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if you want to make a prediction with a single row of data, concatenate
    the single row with the first `25` rows of `X_test` and make a prediction as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: You now know how machine learning models may be included in pipelines to transform
    and make predictions on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on making it to the end of the book! This has been an extraordinary
    journey that began with basic machine learning and `pandas` and ended with building
    your own customized transformers, pipelines, and functions to deploy robust, fine-tuned
    XGBoost models in industry scenarios with sparse matrices to make predictions
    on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Along the way, you have learned the story of XGBoost, from the first decision
    trees through random forests and gradient boosting, before discovering the mathematical
    details and sophistication that has made XGBoost so special. You saw time and
    time again that XGBoost outperforms other machine learning algorithms, and you
    gained essential practice in tuning XGBoost's wide-ranging hyperparameters, including
    `n_estimators`, `max_depth`, `gamma`, `colsample_bylevel`, `missing`, and `scale_pos_weight`.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how physicists and astronomers obtained knowledge about our universe
    in historically important case studies, and you learned about the extensive range
    of XGBoost through imbalanced datasets and the application of alternative base
    learners. You even learned tricks of the trade from Kaggle competitions through
    advanced feature engineering, non-correlated ensembles, and stacking. Finally,
    you learned advanced automation processes for industry.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, your knowledge of XGBoost is at an advanced level. You can now
    use XGBoost efficiently, swiftly, and powerfully to tackle the machine learning
    problems that will come your way. Of course, XGBoost is not perfect. If you are
    dealing with unstructured data such as images or text, **neural networks** might
    serve you better. For most machine learning tasks, especially those with tabular
    data, XGBoost will usually give you an advantage.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in pursuing further studies with XGBoost, my personal
    recommendation is to enter Kaggle competitions. The reason is that Kaggle competitions
    consist of seasoned machine learning practitioners and competing against them
    will make you better. Furthermore, Kaggle competitions provide a structured machine
    learning environment consisting of many practitioners working on the same problem,
    which results in shared notebooks and forum discussions that can further boost
    the educational process. It's also where XGBoost first developed its extraordinary
    reputation with the Higgs boson competition, as outlined in this book.
  prefs: []
  type: TYPE_NORMAL
- en: You may now go confidently forward into the world of big data with XGBoost to
    advance research, enter competitions, and build machine learning models ready
    for production.
  prefs: []
  type: TYPE_NORMAL

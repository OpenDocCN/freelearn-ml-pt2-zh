["```py\n>>> mydata = pd.read_csv('20051201_20051210.csv', index_col='Date')\n>>> mydata\n               Open         High         Low          Close\nDate\n2005-12-01 2244.850098  2269.389893\t2244.709961\t2267.169922\t\n2005-12-02 2266.169922\t 2273.610107\t2261.129883\t2273.370117\n2005-12-05 2269.070068\t 2269.479980\t2250.840088\t2257.639893\n2005-12-06 2267.760010\t 2278.159912\t2259.370117\t2260.760010\n2005-12-07 2263.290039\t 2264.909912\t2244.620117\t2252.010010\n2005-12-08 2254.800049\t 2261.610107\t2233.739990\t2246.459961\n2005-12-09 2247.280029\t 2258.669922\t2241.030029\t2256.729980\n              Adj Close  Volume    \nDate\n2005-12-01 2267.169922\t  2010420000\n2005-12-02 2273.370117\t  1758510000\n2005-12-05 2257.639893\t  1659920000\n2005-12-06 2260.760010\t  1788200000\n2005-12-07 2252.010010\t  1733530000\n2005-12-08 2246.459961\t  1908360000\n2005-12-09 2256.729980\t  1658570000 \n```", "```py\n>>> def add_original_feature(df, df_new):\n...     df_new['open'] = df['Open']\n...     df_new['open_1'] = df['Open'].shift(1)\n...     df_new['close_1'] = df['Close'].shift(1)\n...     df_new['high_1'] = df['High'].shift(1)\n...     df_new['low_1'] = df['Low'].shift(1)\n...     df_new['volume_1'] = df['Volume'].shift(1) \n```", "```py\n>>> def add_avg_price(df, df_new):\n...     df_new['avg_price_5'] =\n                     df['Close'].rolling(5).mean().shift(1)\n...     df_new['avg_price_30'] =\n                     df['Close'].rolling(21).mean().shift(1)\n...     df_new['avg_price_365'] =\n                     df['Close'].rolling(252).mean().shift(1)\n...     df_new['ratio_avg_price_5_30'] =\n                 df_new['avg_price_5'] / df_new['avg_price_30']\n...     df_new['ratio_avg_price_5_365'] =\n                 df_new['avg_price_5'] / df_new['avg_price_365']\n...     df_new['ratio_avg_price_30_365'] =\n                df_new['avg_price_30'] / df_new['avg_price_365'] \n```", "```py\n>>> def add_avg_volume(df, df_new):\n...     df_new['avg_volume_5'] =\n                  df['Volume'].rolling(5).mean().shift(1)\n...     df_new['avg_volume_30'] = \n                  df['Volume'].rolling(21).mean().shift(1)\n...     df_new['avg_volume_365'] =\n                      df['Volume'].rolling(252).mean().shift(1)\n...     df_new['ratio_avg_volume_5_30'] =\n                df_new['avg_volume_5'] / df_new['avg_volume_30']\n...     df_new['ratio_avg_volume_5_365'] =\n               df_new['avg_volume_5'] / df_new['avg_volume_365']\n...     df_new['ratio_avg_volume_30_365'] =\n               df_new['avg_volume_30'] / df_new['avg_volume_365'] \n```", "```py\n>>> def add_std_price(df, df_new):\n...     df_new['std_price_5'] =\n               df['Close'].rolling(5).std().shift(1)\n...     df_new['std_price_30'] =\n               df['Close'].rolling(21).std().shift(1)\n...     df_new['std_price_365'] =\n               df['Close'].rolling(252).std().shift(1)\n...     df_new['ratio_std_price_5_30'] =\n               df_new['std_price_5'] / df_new['std_price_30']\n...     df_new['ratio_std_price_5_365'] =\n               df_new['std_price_5'] / df_new['std_price_365']\n...     df_new['ratio_std_price_30_365'] =\n               df_new['std_price_30'] / df_new['std_price_365'] \n```", "```py\n>>> def add_std_volume(df, df_new):\n...     df_new['std_volume_5'] =\n                 df['Volume'].rolling(5).std().shift(1)\n...     df_new['std_volume_30'] =\n                 df['Volume'].rolling(21).std().shift(1)\n...     df_new['std_volume_365'] =\n                 df['Volume'].rolling(252).std().shift(1)\n...     df_new['ratio_std_volume_5_30'] =\n                df_new['std_volume_5'] / df_new['std_volume_30']\n...     df_new['ratio_std_volume_5_365'] =\n                df_new['std_volume_5'] / df_new['std_volume_365']\n...     df_new['ratio_std_volume_30_365'] =\n               df_new['std_volume_30'] / df_new['std_volume_365'] \n```", "```py\n>>> def add_return_feature(df, df_new):\n...     df_new['return_1'] = ((df['Close'] - df['Close'].shift(1))  \n                               / df['Close'].shift(1)).shift(1)\n...     df_new['return_5'] = ((df['Close'] - df['Close'].shift(5))\n                               / df['Close'].shift(5)).shift(1)\n...     df_new['return_30'] = ((df['Close'] -\n           df['Close'].shift(21)) / df['Close'].shift(21)).shift(1)\n...     df_new['return_365'] = ((df['Close'] -\n         df['Close'].shift(252)) / df['Close'].shift(252)).shift(1)\n...     df_new['moving_avg_5'] =\n                    df_new['return_1'].rolling(5).mean().shift(1)\n...     df_new['moving_avg_30'] =\n                    df_new['return_1'].rolling(21).mean().shift(1)\n...     df_new['moving_avg_365'] =\n                   df_new['return_1'].rolling(252).mean().shift(1) \n```", "```py\n>>> def generate_features(df):\n...     \"\"\"\n...     Generate features for a stock/index based on historical price and performance\n...     @param df: dataframe with columns \"Open\", \"Close\", \"High\", \"Low\", \"Volume\", \"Adj Close\"\n...     @return: dataframe, data set with new features\n...     \"\"\"\n...     df_new = pd.DataFrame()\n...     # 6 original features\n...     add_original_feature(df, df_new)\n...     # 31 generated features\n...     add_avg_price(df, df_new)\n...     add_avg_volume(df, df_new)\n...     add_std_price(df, df_new)\n...     add_std_volume(df, df_new)\n...     add_return_feature(df, df_new)\n...     # the target\n...     df_new['close'] = df['Close']\n...     df_new = df_new.dropna(axis=0)\n...     return df_new \n```", "```py\n>>> data_raw = pd.read_csv('19900101_20230630.csv', index_col='Date')\n>>> data = generate_features(data_raw) \n```", "```py\n>>> print(data.round(decimals=3).head(5)) \n```", "```py\n>>> def compute_prediction(X, weights):\n...     \"\"\"\n...     Compute the prediction y_hat based on current weights\n...     \"\"\"\n...     return np.dot(X, weights) \n```", "```py\n>>> def update_weights_gd(X_train, y_train, weights,\nlearning_rate):\n...     predictions = compute_prediction(X_train, weights)\n...     weights_delta = np.dot(X_train.T, y_train - predictions)\n...     m = y_train.shape[0]\n...     weights += learning_rate / float(m) * weights_delta\n...     return weights \n```", "```py\n>>> def compute_loss(X, y, weights):\n...     \"\"\"\n...     Compute the loss J(w)\n...     \"\"\"\n...     predictions = compute_prediction(X, weights)\n...     return np.mean((predictions - y) ** 2 / 2.0) \n```", "```py\n>>> def train_linear_regression(X_train, y_train, max_iter, learning_rate, fit_intercept=False, display_loss=500):\n...     \"\"\"\n...     Train a linear regression model with gradient descent, and return trained model\n...     \"\"\"\n...     if fit_intercept:\n...         intercept = np.ones((X_train.shape[0], 1))\n...         X_train = np.hstack((intercept, X_train))\n...     weights = np.zeros(X_train.shape[1])\n...     for iteration in range(max_iter):\n...         weights = update_weights_gd(X_train, y_train,\n                                       weights, learning_rate)\n...         # Check the cost for every 500 (by default) iterations\n...         if iteration % 500 == 0:\n...             print(compute_loss(X_train, y_train, weights))\n...     return weights \n```", "```py\n>>> def predict(X, weights):\n...     if X.shape[1] == weights.shape[0] - 1:\n...         intercept = np.ones((X.shape[0], 1))\n...         X = np.hstack((intercept, X))\n...     return compute_prediction(X, weights) \n```", "```py\n>>> X_train = np.array([[6], [2], [3], [4], [1],\n                        [5], [2], [6], [4], [7]])\n>>> y_train = np.array([5.5, 1.6, 2.2, 3.7, 0.8,\n                        5.2, 1.5, 5.3, 4.4, 6.8]) \n```", "```py\n>>> weights = train_linear_regression(X_train, y_train,\n            max_iter=100, learning_rate=0.01, fit_intercept=True) \n```", "```py\n>>> X_test = np.array([[1.3], [3.5], [5.2], [2.8]])\n>>> predictions = predict(X_test, weights)\n>>> import matplotlib.pyplot as plt\n>>> plt.scatter(X_train[:, 0], y_train, marker='o', c='b')\n>>> plt.scatter(X_test[:, 0], predictions, marker='*', c='k')\n>>> plt.xlabel('x')\n>>> plt.ylabel('y')\n>>> plt.show() \n```", "```py\n>>> from sklearn import datasets\n>>> diabetes = datasets.load_diabetes()\n>>> print(diabetes.data.shape)\n(442, 10)\n>>> num_test = 30\n>>> X_train = diabetes.data[:-num_test, :]\n>>> y_train = diabetes.target[:-num_test] \n```", "```py\n>>> weights = train_linear_regression(X_train, y_train,\n              max_iter=5000, learning_rate=1, fit_intercept=True)\n2960.1229915\n1539.55080927\n1487.02495658\n1480.27644342\n1479.01567047\n1478.57496091\n1478.29639883\n1478.06282572\n1477.84756968\n1477.64304737\n>>> X_test = diabetes.data[-num_test:, :]\n>>> y_test = diabetes.target[-num_test:]\n>>> predictions = predict(X_test, weights)\n>>> print(predictions)\n[ 232.22305668 123.87481969 166.12805033 170.23901231\n  228.12868839 154.95746522 101.09058779 87.33631249\n  143.68332296 190.29353122 198.00676871 149.63039042\n   169.56066651 109.01983998 161.98477191 133.00870377\n   260.1831988 101.52551082 115.76677836 120.7338523\n   219.62602446 62.21227353 136.29989073 122.27908721\n   55.14492975 191.50339388 105.685612 126.25915035\n   208.99755875 47.66517424]\n>>> print(y_test)\n[ 261\\. 113\\. 131\\. 174\\. 257\\. 55\\. 84\\. 42\\. 146\\. 212\\. 233.\n  91\\. 111\\. 152\\. 120\\. 67\\. 310\\. 94\\. 183\\. 66\\. 173\\. 72.\n  49\\. 64\\. 48\\. 178\\. 104\\. 132\\. 220\\. 57.] \n```", "```py\n>>> from sklearn.linear_model import SGDRegressor\n>>> regressor = SGDRegressor(loss='squared_error',\n                         penalty='l2',\n                         alpha=0.0001,\n                         learning_rate='constant',\n                         eta0=0.2,\n                         max_iter=100,\n                         random_state=42) \n```", "```py\n>>> regressor.fit(X_train, y_train)\n>>> predictions = regressor.predict(X_test)\n>>> print(predictions)\n[213.10213626 108.68382244 152.18820636 153.81308148 208.42650616 137.24771808  88.91487772  73.83269079 131.35148348 173.65164632 178.16029669 135.26642772 152.92346973  89.39394334 149.98088897 117.62875063 241.90665387  86.59992328 101.90393228 105.13958969 202.13586812  50.60429115 121.43542595 106.34058448  41.11664041 172.53683431  95.43229463 112.59395222 187.40792     36.1586737 ] \n```", "```py\n>>> import tensorflow as tf\n>>> layer0 = tf.keras.layers.Dense(units=1,\n                      input_shape=[X_train.shape[1]])\n>>> model = tf.keras.Sequential(layer0) \n```", "```py\n>>> model.compile(loss='mean_squared_error',\n             optimizer=tf.keras.optimizers.Adam(1)) \n```", "```py\n>>> model.fit(X_train, y_train, epochs=100, verbose=True)\nEpoch 1/100\n412/412 [==============================] - 1s 2ms/sample - loss: 27612.9129\nEpoch 2/100\n412/412 [==============================] - 0s 44us/sample - loss: 23802.3043\nEpoch 3/100\n412/412 [==============================] - 0s 47us/sample - loss: 20383.9426\nEpoch 4/100\n412/412 [==============================] - 0s 51us/sample - loss: 17426.2599\nEpoch 5/100\n412/412 [==============================] - 0s 44us/sample - loss: 14857.0057\n……\nEpoch 96/100\n412/412 [==============================] - 0s 55us/sample - loss: 2971.6798\nEpoch 97/100\n412/412 [==============================] - 0s 44us/sample - loss: 2970.8919\nEpoch 98/100\n412/412 [==============================] - 0s 52us/sample - loss: 2970.7903\nEpoch 99/100\n412/412 [==============================] - 0s 47us/sample - loss: 2969.7266\nEpoch 100/100\n412/412 [==============================] - 0s 46us/sample - loss: 2970.4180 \n```", "```py\n>>> predictions = model.predict(X_test)[:, 0]\n>>> print(predictions)\n[231.52155  124.17711  166.71492  171.3975   227.70126  152.02522\n 103.01532   91.79277  151.07457  190.01042  190.60373  152.52274\n 168.92166  106.18033  167.02473  133.37477  259.24756  101.51256\n 119.43106  120.893005 219.37921   64.873634 138.43217  123.665634\n  56.33039  189.27441  108.67446  129.12535  205.06857   47.99469 ] \n```", "```py\n>>> def mse(targets):\n...     # When the set is empty\n...     if targets.size == 0:\n...         return 0\n...     return np.var(targets) \n```", "```py\n>>> def weighted_mse(groups):\n...     total = sum(len(group) for group in groups)\n...     weighted_sum = 0.0\n...     for group in groups:\n...         weighted_sum += len(group) / float(total) * mse(group)\n...     return weighted_sum \n```", "```py\n>>> print(f'{mse(np.array([1, 2, 3])):.4f}')\n0.6667\n>>> print(f'{weighted_mse([np.array([1, 2, 3]), np.array([1, 2])]):.4f}')\n0.5000 \n```", "```py\nMSE(type, semi) = weighted_mse([[600, 400, 700], [700, 800]]) = 10333\nMSE(bedroom, 2) = weighted_mse([[700, 400], [600, 800, 700]]) = 13000\nMSE(bedroom, 3) = weighted_mse([[600, 800], [700, 400, 700]]) = 16000\nMSE(bedroom, 4) = weighted_mse([[700], [600, 700, 800, 400]]) = 17500 \n```", "```py\nMSE(bedroom, 2) = weighted_mse([[], [600, 400, 700]]) = 15556\nMSE(bedroom, 3) = weighted_mse([[400], [600, 700]]) = 1667\nMSE(bedroom, 4) = weighted_mse([[400, 600], [700]]) = 6667 \n```", "```py\n>>> def split_node(X, y, index, value):\n...     x_index = X[:, index]\n...     # if this feature is numerical\n...     if type(X[0, index]) in [int, float]:\n...         mask = x_index >= value\n...     # if this feature is categorical\n...     else:\n...         mask = x_index == value\n...     # split into left and right child\n...     left = [X[~mask, :], y[~mask]]\n...     right = [X[mask, :], y[mask]]\n...     return left, right \n```", "```py\n>>> def get_best_split(X, y):\n...     \"\"\"\n...     Obtain the best splitting point and resulting children for the data set X, y\n...     @return: {index: index of the feature, value: feature value, children: left and right children}\n...     \"\"\"\n...     best_index, best_value, best_score, children =\n                                     None, None, 1e10, None\n...     for index in range(len(X[0])):\n...         for value in np.sort(np.unique(X[:, index])):\n...             groups = split_node(X, y, index, value)\n...             impurity = weighted_mse(\n                                [groups[0][1], groups[1][1]])\n...             if impurity < best_score:\n...                 best_index, best_value, best_score, children\n                                   = index, value, impurity, groups\n...     return {'index': best_index, 'value': best_value,\n                'children': children} \n```", "```py\n>>> def get_leaf(targets):\n...     # Obtain the leaf as the mean of the targets\n...     return np.mean(targets) \n```", "```py\n>>> def split(node, max_depth, min_size, depth):\n...     \"\"\"\n...     Split children of a node to construct new nodes or assign them terminals\n...     @param node: dict, with children info\n...     @param max_depth: maximal depth of the tree\n...     @param min_size: minimal samples required to further split a child\n...     @param depth: current depth of the node\n...     \"\"\"\n...     left, right = node['children']\n...     del (node['children'])\n...     if left[1].size == 0:\n...         node['right'] = get_leaf(right[1])\n...         return\n...     if right[1].size == 0:\n...         node['left'] = get_leaf(left[1])\n...         return\n...     # Check if the current depth exceeds the maximal depth\n...     if depth >= max_depth:\n...         node['left'], node['right'] = get_leaf(\n                             left[1]), get_leaf(right[1])\n...         return\n...     # Check if the left child has enough samples\n...     if left[1].size <= min_size:\n...         node['left'] = get_leaf(left[1])\n...     else:\n...         # It has enough samples, we further split it\n...         result = get_best_split(left[0], left[1])\n...         result_left, result_right = result['children']\n...         if result_left[1].size == 0:\n...             node['left'] = get_leaf(result_right[1])\n...         elif result_right[1].size == 0:\n...             node['left'] = get_leaf(result_left[1])\n...         else:\n...             node['left'] = result\n...             split(node['left'], max_depth, min_size, depth + 1)\n...     # Check if the right child has enough samples\n...     if right[1].size <= min_size:\n...         node['right'] = get_leaf(right[1])\n...     else:\n...         # It has enough samples, we further split it\n...         result = get_best_split(right[0], right[1])\n...         result_left, result_right = result['children']\n...         if result_left[1].size == 0:\n...             node['right'] = get_leaf(result_right[1])\n...         elif result_right[1].size == 0:\n...             node['right'] = get_leaf(result_left[1])\n...         else:\n...             node['right'] = result\n...             split(node['right'], max_depth, min_size,\n                       depth + 1) \n```", "```py\n>>> def train_tree(X_train, y_train, max_depth, min_size):\n...     root = get_best_split(X_train, y_train)\n...     split(root, max_depth, min_size, 1)\n...     return root \n```", "```py\n>>> X_train = np.array([['semi', 3],\n...                     ['detached', 2],\n...                     ['detached', 3],\n...                     ['semi', 2],\n...                     ['semi', 4]], dtype=object)\n>>> y_train = np.array([600, 700, 800, 400, 700])\n>>> tree = train_tree(X_train, y_train, 2, 2) \n```", "```py\n>>> CONDITION = {'numerical': {'yes': '>=', 'no': '<'},\n...              'categorical': {'yes': 'is', 'no': 'is not'}}\n>>> def visualize_tree(node, depth=0):\n...     if isinstance(node, dict):\n...         if type(node['value']) in [int, float]:\n...             condition = CONDITION['numerical']\n...         else:\n...             condition = CONDITION['categorical']\n...         print('{}|- X{} {} {}'.format(depth * ' ',\n                  node['index'] + 1, condition['no'],\n                  node['value']))\n...         if 'left' in node:\n...             visualize_tree(node['left'], depth + 1)\n...         print('{}|- X{} {} {}'.format(depth * ' ',\n                 node['index'] + 1, condition['yes'],\n                 node['value']))\n...         if 'right' in node:\n...             visualize_tree(node['right'], depth + 1)\n...     else:\n...         print('{}[{}]'.format(depth * ' ', node))\n>>> visualize_tree(tree)\n|- X1 is not detached\n  |- X2 < 3\n    [400.0]\n  |- X2 >= 3\n    [650.0]\n|- X1 is detached\n  [750.0] \n```", "```py\n>>> housing = datasets.fetch_california_housing() \n```", "```py\n>>> num_test = 10 # the last 10 samples as testing set\n>>> X_train = housing.data[:-num_test, :]\n>>> y_train = housing.target[:-num_test]\n>>> X_test = housing.data[-num_test:, :]\n>>> y_test = housing.target[-num_test:]\n>>> from sklearn.tree import DecisionTreeRegressor\n>>> regressor = DecisionTreeRegressor(max_depth=10,\n                                      min_samples_split=3,\n                                      random_state=42)\n>>> regressor.fit(X_train, y_train) \n```", "```py\n>>> predictions = regressor.predict(X_test)\n>>> print(predictions)\n[1.29568298 1.29568298 1.29568298 1.11946842 1.29568298 0.66193704 0.82554167 0.8546936  0.8546936  0.8546936 ] \n```", "```py\n>>> print(y_test)\n[1.12  1.072 1.156 0.983 1.168 0.781 0.771 0.923 0.847 0.894] \n```", "```py\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> regressor = RandomForestRegressor(n_estimators=100,\n                                  max_depth=10,\n                                  min_samples_split=3,\n                                  random_state=42)\n>>> regressor.fit(X_train, y_train)\n>>> predictions = regressor.predict(X_test)\n>>> print(predictions)\n[1.31785493 1.29359614 1.24146512 1.06039979 1.24015576 0.7915538 0.90307069 0.83535894 0.8956997  0.91264529] \n```", "```py\n    >>> diabetes = datasets.load_diabetes()\n    >>> num_test = 30 # the last 30 samples as testing set\n    >>> X_train = diabetes.data[:-num_test, :]\n    >>> y_train = diabetes.target[:-num_test]\n    >>> X_test = diabetes.data[-num_test:, :]\n    >>> y_test = diabetes.target[-num_test:]\n    >>> param_grid = {\n    ...     \"alpha\": [1e-07, 1e-06, 1e-05],\n    ...     \"penalty\": [None, \"l2\"],\n    ...     \"eta0\": [0.03, 0.05, 0.1],\n    ...     \"max_iter\": [500, 1000]\n    ... }\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> regressor = SGDRegressor(loss='squared_error',\n                                 learning_rate='constant',\n                                 random_state=42)\n    >>> grid_search = GridSearchCV(regressor, param_grid, cv=3) \n    ```", "```py\n    >>> grid_search.fit(X_train, y_train)\n    >>> print(grid_search.best_params_)\n    {'alpha': 1e-07, 'eta0': 0.05, 'max_iter': 500, 'penalty': None}\n    >>> regressor_best = grid_search.best_estimator_ \n    ```", "```py\n    >>> predictions = regressor_best.predict(X_test) \n    ```", "```py\n    >>> from sklearn.metrics import mean_squared_error,\n        mean_absolute_error, r2_score\n    >>> print(mean_squared_error(y_test, predictions))\n    1933.3953304460413\n    >>> print(mean_absolute_error(y_test, predictions))\n    35.48299900764652\n    >>> print(r2_score(y_test, predictions))\n    0.6247444629690868 \n    ```", "```py\n    >>> data_raw = pd.read_csv('19900101_20230630.csv', index_col='Date')\n    >>> data = generate_features(data_raw)\n    >>> start_train = '1990-01-01'\n    >>> end_train = '2022-12-31'\n    >>> start_test = '2023-01-01'\n    >>> end_test = '2023-06-30'\n    >>> data_train = data.loc[start_train:end_train]\n    >>> X_train = data_train.drop('close', axis=1).values\n    >>> y_train = data_train['close'].values\n    >>> print(X_train.shape)\n    (8061, 37)\n    >>> print(y_train.shape)\n    (8061,) \n    ```", "```py\n>>> data_train = data.loc[start_train:end_train]\n>>> X_train = data_train.drop('close', axis=1).values\n>>> y_train = data_train['close'].values\n>>> print(X_test.shape)\n(124, 37) \n```", "```py\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> scaler = StandardScaler() \n    ```", "```py\n    >>> X_scaled_train = scaler.fit_transform(X_train)\n    >>> X_scaled_test = scaler.transform(X_test) \n    ```", "```py\n    >>> param_grid = {\n    ...     \"alpha\": [1e-4, 3e-4, 1e-3],\n    ...     \"eta0\": [0.01, 0.03, 0.1],\n    ... }\n    >>> lr = SGDRegressor(penalty='l2', max_iter=5000, random_state=42) \n    ```", "```py\n    >>> from sklearn.model_selection import TimeSeriesSplit\n    >>> tscv = TimeSeriesSplit(n_splits=3)\n    >>> grid_search = GridSearchCV(lr, param_grid, cv=tscv, scoring='r2')\n    >>> grid_search.fit(X_scaled_train, y_train) \n    ```", "```py\n    >>> print(grid_search.best_params_)\n    {'alpha': 0.0001, 'eta0': 0.1}\n    >>> lr_best = grid_search.best_estimator_\n    >>> predictions_lr = lr_best.predict(X_scaled_test) \n    ```", "```py\n    >>> print(f'R^2: {r2_score(y_test, predictions_lr):.3f}')\n    R^2: 0.959 \n    ```", "```py\n    >>> param_grid = {\n    ...     'max_depth': [20, 30, 50],\n    ...     'min_samples_split': [2, 5, 10],\n    ...     'min_samples_leaf': [1, 3, 5]\n    ... }\n    >>> dt = DecisionTreeRegressor(random_state=42)\n    >>> grid_search = GridSearchCV(dt, param_grid, cv=tscv,\n                                   scoring='r2', n_jobs=-1)\n    >>> grid_search.fit(X_train, y_train) \n    ```", "```py\n    >>> print(grid_search.best_params_)\n    {'max_depth': 30, 'min_samples_leaf': 3, 'min_samples_split': 2}\n    >>> dt_best = grid_search.best_estimator_\n    >>> predictions_dt = dt_best.predict(X_test) \n    ```", "```py\n    >>> print(f'R^2: {r2_score(y_test, predictions_rf):.3f}')\n    R^2: 0.912 \n    ```", "```py\n    >>> param_grid = {\n    ...     'max_depth': [20, 30, 50],\n    ...     'min_samples_split': [2, 5, 10],\n    ...     'min_samples_leaf': [1, 3, 5]\n    ... }\n    >>> rf = RandomForestRegressor(n_estimators=30, n_jobs=-1, random_state=42)\n    >>> grid_search = GridSearchCV(rf, param_grid, cv=tscv,\n                                   scoring='r2', n_jobs=-1)\n    >>> grid_search.fit(X_train, y_train) \n    ```", "```py\n    >>> print(grid_search.best_params_)\n    {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5}\n    >>> rf_best = grid_search.best_estimator_\n    >>> predictions_rf = rf_best.predict(X_test) \n    ```", "```py\n    >>> print(f'R^2: {r2_score(y_test, predictions_rf):.3f}')\n    R^2: 0.937 \n    ```", "```py\n>>> plt.rc('xtick', labelsize=10)\n>>> plt.rc('ytick', labelsize=10)\n>>> plt.plot(data_test.index, y_test, c='k')\n>>> plt.plot(data_test.index, predictions_lr, c='b')\n>>> plt.plot(data_test.index, predictions_dt, c='g')\n>>> plt.plot(data_test.index, predictions_rf, c='r')\n>>> plt.xticks(range(0, 130, 10), rotation=60)\n>>> plt.xlabel('Date', fontsize=10)\n>>> plt.ylabel('Close price', fontsize=10)\n>>> plt.legend(['Truth', 'Linear regression', 'Decision tree', 'Random forest'], fontsize=10)\n>>> plt.show() \n```"]
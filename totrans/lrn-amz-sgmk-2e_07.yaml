- en: 'Chapter 5: Training CV Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to use SageMaker's built-in algorithms
    for traditional machine learning problems, including classification, regression,
    and anomaly detection. We saw that these algorithms work well on tabular data,
    such as CSV files. However, they are not well suited for image datasets, and they
    generally perform very poorly on **CV** (**CV**) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For a few years now, CV has taken the world by storm, and not a month goes by
    without a new breakthrough in extracting patterns from images and videos. In this
    chapter, you will learn about three built-in algorithms designed specifically
    for CV tasks. We'll discuss the types of problems that you can solve with them.
    We'll also spend a lot of time explaining how to prepare image datasets, as this
    crucial topic is often inexplicably overlooked. Of course, we'll train and deploy
    models too.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the CV built-in algorithms in Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing image datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the built-in CV algorithms: **image classification**, **object detection**,
    and **semantic segmentation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to install and configure the AWS **Command Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory, but
    strongly encouraged, as it includes many projects that we will need (Jupyter,
    `pandas`, `numpy`, and more).
  prefs: []
  type: TYPE_NORMAL
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the CV built-in algorithms in Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SageMaker includes three CV algorithms, based on proven deep learning networks.
    In this section, you''ll learn about these algorithms, what kind of problem they
    can help you solve, and what their training scenarios are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image classification** assigns one or more labels to an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object detection** detects and classifies objects in an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic segmentation** assigns every pixel of an image to a specific class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering the image classification algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting from an input image, the **image classification** algorithm predicts
    a probability for each class present in the training dataset. This algorithm is
    based on the **ResNet** convolutional neural network ([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)).
    Published in 2015, **ResNet** won the ILSVRC classification task that same year
    ([http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/)).
    Since then, it has become a popular and versatile choice for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Many hyperparameters can be set, including the depth of the network, which can
    range from 18 to 200 layers. In general, the more layers the network has, the
    better it will learn, at the expense of increased training times.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the **image classification** algorithm supports both **single-label**
    and **multi-label** classification. We will focus on single-label classification
    in this chapter. Working with several labels is very similar, and you'll find
    a complete example at [https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/).
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the object detection algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting from an input image, the **object detection** algorithm predicts both
    the class and the location of each object in the image. Of course, the algorithm
    can only detect object classes present in the training dataset. The location of
    each object is defined by a set of four coordinates, called a **bounding box**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm is based on the **Single Shot MultiBox Detector** (**SSD**)
    architecture ([https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)).
    For classification, you can pick from two base networks: **VGG-16** ([https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556))
    or **ResNet-50**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows an example of object detection (source: [https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg](https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Test image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_05_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Test image
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the semantic segmentation algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting from an input image, the **semantic segmentation** algorithm predicts
    the class of every pixel of the image. This is a much harder problem than image
    classification (which only considers the full image) or object detection (which
    only focuses on specific parts of the image). Using the probabilities contained
    in a prediction, it's possible to build **segmentation masks** that cover specific
    objects in the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three neural networks may be used for segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully Convolutional Networks** (**FCNs**): [https://arxiv.org/abs/1411.4038](https://arxiv.org/abs/1411.4038)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pyramid Scene Parsing** (**PSP**): [https://arxiv.org/abs/1612.01105](https://arxiv.org/abs/1612.01105)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepLab** v3: [https://arxiv.org/abs/1706.05587](https://arxiv.org/abs/1706.05587)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder network is **ResNet**, with either 50 or 101 layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the result of segmenting the previous image. We
    see the segmentation masks, and each class is assigned a unique color; the background
    is black, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Segmented test image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_05_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Segmented test image
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how we can train these algorithms on our own data.
  prefs: []
  type: TYPE_NORMAL
- en: Training with CV algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All three algorithms are based on **supervised learning**, so our starting
    point will be a labeled dataset. Of course, the nature of these labels will be
    different for each algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Class labels for **image classification**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bounding boxes and class labels for **object detection**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation masks and class labels for **semantic segmentation**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotating image datasets is a lot of work. If you need to build your own dataset,
    **Amazon SageMaker Ground Truth** can definitely help, and we studied it in [*Chapter
    2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030), *Handling Data Preparation
    Techniques*. Later in this chapter, we'll show you how to use image datasets labeled
    with Ground Truth.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to packaging datasets, the use of **RecordIO** files is strongly
    recommended ([https://mxnet.apache.org/api/faq/recordio](https://mxnet.apache.org/api/faq/recordio)).
    Packaging images in a small number of record-structured files makes it much easier
    to move datasets around and to split them for distributed training. Having said
    that, you can also train on individual image files if you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Once your dataset is ready in S3, you need to decide whether you'd like to train
    from scratch, or whether you'd like to start from a pretrained network.
  prefs: []
  type: TYPE_NORMAL
- en: Training from scratch is fine if you have plenty of data, and if you're convinced
    that there's value in building a specific model with it. However, this will take
    a lot of time, possibly hundreds of epochs, and hyperparameter selection will
    be absolutely critical in getting good results.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pretrained network is generally a better option, even if you have lots
    of data. Thanks to **transfer learning**, you can start from a model trained on
    a huge collection of images (think millions) and fine-tune it on your data and
    classes. Training will be much shorter, and you will get models with higher accuracy
    rates quicker.
  prefs: []
  type: TYPE_NORMAL
- en: Given the complexity of the models and the size of datasets, training with CPU
    instances is simply not an option. We'll use GPU instances for all examples.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, all three algorithms are based on **Apache MXNet**. This
    lets you export their models outside of SageMaker and deploy them anywhere you
    like.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we're going to zoom in on image datasets, and how to prepare
    them for training.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing image datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Input formats are more complex for image datasets than for tabular datasets,
    and we need to get them exactly right. The CV algorithms in SageMaker support
    three input formats:'
  prefs: []
  type: TYPE_NORMAL
- en: Image files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RecordIO** files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmented manifests built by **SageMaker Ground Truth**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you'll learn how to prepare datasets in these different formats.
    To the best of my knowledge, this topic has rarely been addressed in such detail.
    Get ready to learn a lot!
  prefs: []
  type: TYPE_NORMAL
- en: Working with image files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the simplest format, and it's supported by all three algorithms. Let's
    see how to use it with the **image classification** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Converting an image classification dataset to image format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A dataset in image format has to be stored in S3\. Image files don't need to
    be sorted in any way, and you simply could store all of them in the same bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'Images are described in a **list file**, a text file containing a line per
    image. For **image classification**, three columns are present: the unique identifier
    of the image, its class label, and its path. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first line tells us that `image2753.jpg` belongs to class 5 and has been
    assigned ID 1023.
  prefs: []
  type: TYPE_NORMAL
- en: You need a list file for each channel, so you would need one for the training
    dataset, one for the validation dataset, and so on. You can either write bespoke
    code to generate them, or you can use a simple program that is part of `im2rec`,
    and it's available in Python and C++. We'll use the Python version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the Dogs vs. Cats dataset available on **Kaggle** ([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)).
    This dataset is 812 MB. Unsurprisingly, it contains two classes: dogs and cats.
    It''s already split for training and testing (25,000 and 12,500 images, respectively).
    Here''s how:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a `kaggle` CLI ([https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On our local machine, we download and extract the training dataset (you can
    ignore the test set, which is only needed for the competition):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Dog and cat images are mixed up in the same folder. We create a subfolder for
    each class, and move the appropriate images there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll need validation images, so let''s move 1,250 random dog images and 1,250
    random cat images to specific directories. I''m using `bash` scripting here, but
    feel free to use any tool you like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We move the remaining 22,500 images to the training folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our dataset now looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We download the `im2rec` tool from GitHub ([https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py)).
    It requires dependencies, which we need to install (you may have to adapt the
    command to your own environment and flavor of Linux):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We run `im2rec` to build two list files, one for training data and one for
    validation data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 3197  0.000000  cat/cat.1625.jpg
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 15084 1.000000  dog/dog.12322.jpg
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1479  0.000000  cat/cat.11328.jpg
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5262  0.000000  cat/cat.3484.jpg
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 20714 1.000000  dog/dog.6140.jpg
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We move the list files to specific directories. This is required because they
    will be passed to the `Estimator` as two new channels, `train_lst` and `validation_lst`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The dataset now looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we sync this folder to the SageMaker default bucket for future use.
    Please make sure to only sync the four folders, and nothing else:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's move on to using the image format with the object detection algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Converting detection datasets to image format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The general principle is identical. We need to build a file tree representing
    the four channels: `train`, `validation`, `train_annotation`, and `validation_annotation`.'
  prefs: []
  type: TYPE_NORMAL
- en: The main difference lies in how labeling information is stored. Instead of list
    files, we need to build JSON files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of a fictitious picture in an object detection dataset.
    For each object in the picture, we define the coordinates of the top-left corner
    of its bounding box, its height, and its width. We also define the class identifier,
    which points to a category array that also stores class names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We would need to do this for every picture in the dataset, building a JSON file
    for the training set and one for the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's see how to use the image format with the semantic segmentation
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Converting segmentation datasets to image format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image format is the only format supported by the image segmentation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we need to build a file tree representing the four channels: `train`,
    `validation`, `train_annotation`, and `validation_annotation`. The first two channels
    contain the source images, and the last two contain the segmentation mask images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'File naming is critical in matching an image to its mask: the source image
    and the mask image must have the same name in their respective channels. Here''s
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see sample pictures in the following figure. The source image on the
    left would go to the `train` folder and the mask picture on the right would go
    to the `train_annotation` folder. They would have to have exactly the same name
    so that the algorithm could match them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Sample image from the Pascal VOC dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_05_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Sample image from the Pascal VOC dataset
  prefs: []
  type: TYPE_NORMAL
- en: One clever feature of this format is how it matches class identifiers to mask
    colors. Mask images are PNG files with a 256-color palette. Each class in the
    dataset is assigned a specific entry in the color palette. These colors are the
    ones you see in masks for objects belonging to that class.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your labeling tool or your existing dataset doesn''t support this PNG feature,
    you can add your own color mapping file. Please refer to the AWS documentation
    for details: [https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s prepare the **Pascal VOC** dataset. This dataset is frequently
    used to benchmark object detection and semantic segmentation model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first download and extract the 2012 version of the dataset. Again, I recommend
    using an AWS-hosted instance to speed up network transfers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a work directory where we''ll build the four channels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the list of training files defined in the dataset, we copy the corresponding
    images to the `train` folder. I''m using `bash` scripting here; feel free to use
    your tool of choice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then do the same for validation images, training masks, and validation masks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We check that we have the same number of images in the two training channels,
    and in the two validation channels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see 1,464 training files and masks, and 1,449 validation files and masks.
    We''re all set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last step is to sync the file tree to S3 for later use. Again, please make
    sure to sync only the four folders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We know how to prepare classification, detection, and segmentation datasets
    in image format. This is a critical step, and you have to get things exactly right.
  prefs: []
  type: TYPE_NORMAL
- en: Still, I'm sure that you found the steps in this section a little painful. So
    did I! Now imagine doing the same with millions of images. That doesn't sound
    very exciting, does it?
  prefs: []
  type: TYPE_NORMAL
- en: We need an easier way to prepare image datasets. Let's see how we can simplify
    dataset preparation with **RecordIO** files.
  prefs: []
  type: TYPE_NORMAL
- en: Working with RecordIO files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RecordIO files are easier to move around. It's much more efficient for an algorithm
    to read a large sequential file than to read lots of tiny files stored at random
    disk locations.
  prefs: []
  type: TYPE_NORMAL
- en: Converting an image classification dataset to RecordIO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s convert the Dogs vs. Cats dataset to RecordIO:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from a freshly extracted copy of the dataset, we move the images to
    the appropriate class folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We run `im2rec` to generate list files for the training dataset (90%) and the
    validation dataset (10%). There's no need to split the dataset ourselves!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We run `im2rec` once more to generate the RecordIO files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: $ ls dogscats*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dogscats_train.idx dogscats_train.lst dogscats_train.rec
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: dogscats_val.idx dogscats_val.lst dogscats_val.rec
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s store the RecordIO files in S3, as we''ll use them later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This was much simpler, wasn't it? `im2rec` has additional options to resize
    images and more. It can also break the dataset into several chunks, a useful technique
    for **Pipe Mode** and **Distributed Training**. We'll study them in [*Chapter
    9*](B17705_09_Final_JM_ePub.xhtml#_idTextAnchor168), *Scaling Your Training Jobs*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to using RecordIO files for object detection.
  prefs: []
  type: TYPE_NORMAL
- en: Converting an object detection dataset to RecordIO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process is very similar. A major difference is the format of list files.
    Instead of dealing only with class labels, we also need to store bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what this means for the Pascal VOC dataset. The following image
    is taken from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Sample image from the Pascal VOC dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_05_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Sample image from the Pascal VOC dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'It contains three chairs. The labeling information is stored in an individual
    **XML** file, shown in slightly abbreviated form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting this to a list file entry should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s decode each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '`9404` is a unique image identifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2` is the number of columns containing header information, including this
    one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`6` is the number of columns for labeling information. These six columns are
    the class identifier, the four bounding-box coordinates, and a flag telling us
    whether the object is difficult to see (we won''t use it).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is for the first object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) `8` is the class identifier. Here, `8` is the `chair` class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `0.0022 0.6607 0.2612 1.0000` are the relative coordinates of the `0` means
    that the object is not difficult.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the second object, we have the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) `8` is the class identifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `0.9576 0.6429 1.0000 0.8839` are the coordinates of the second object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `1` means that the object is difficult.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The third object has the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) `8` is the class identifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `0.6272 0.4435 0.7076 0.628` are the coordinates of the third object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `1` means that the object is difficult.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`VOC2007/JPEGImages/003988.jpg` is the path to the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, how do we convert thousands of XML files into a couple of list files? Unless
    you enjoy writing parsers, this isn't a very exciting task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, our work has been cut out for us. Apache MXNet includes a Python
    script, `prepare_dataset.py`, that will handle this task. Let''s see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next steps, I recommend using an Amazon Linux 2 EC2 instance with at
    least 10 GB of storage. Here are the setup steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the 2007 and 2012 Pascal VOC datasets with `wget`, and extract them
    in the same directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Clone the Apache MXNet repository ([https://github.com/apache/incubator-mxnet/](https://github.com/apache/incubator-mxnet/)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the `prepare_dataset.py` script to build our training dataset, merging
    the training and validation sets of the 2007 and 2012 versions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s follow similar steps to generate our validation dataset, using the test
    set of the 2007 version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the top-level directory, we see the files generated by the script. Feel
    free to take a look at the list files; they should have the format presented previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s store the RecordIO files in S3 as we''ll use them later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `prepare_dataset.py` script has really made things simple here. It also
    supports the **COCO** dataset ([http://cocodataset.org](http://cocodataset.org)),
    and the workflow is extremely similar.
  prefs: []
  type: TYPE_NORMAL
- en: What about converting other public datasets? Well, your mileage may vary. You'll
    find more examples at [https://gluon-cv.mxnet.io/build/examples_datasets/index.html](https://gluon-cv.mxnet.io/build/examples_datasets/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: RecordIO is definitely a step forward. Still, when working with custom datasets,
    it's very likely that you'll have to write your own list file generator. That's
    not a huge deal, but it's extra work.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets labeled with **Amazon SageMaker Ground Truth** solve these problems
    altogether. Let's see how this works!
  prefs: []
  type: TYPE_NORMAL
- en: Working with SageMaker Ground Truth files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030), *Handling
    Data Preparation Techniques*, you learned about SageMaker Ground Truth workflows
    and their outcome, an **augmented manifest** file. This file is in **JSON Lines**
    format: each JSON object describes a specific annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example from the semantic segmentation job we ran in [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030),
    *Handling Data Preparation Techniques* (the story is the same for other task types).
    We see the paths to the source image and the segmentation mask, as well as color
    map information telling us how to match mask colors to classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following images are the ones referenced in the preceding JSON document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Source image and segmented image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_05_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Source image and segmented image
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly what we would need to train our model. In fact, we can pass
    the augmented manifest to the SageMaker `Estimator` as is. No data processing
    is required whatsoever.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use an **augmented manifest** pointing at labeled images in S3, we would
    simply pass its location and the name of the JSON attributes (highlighted in the
    previous example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: That's it! This is much simpler than anything we've seen before.
  prefs: []
  type: TYPE_NORMAL
- en: You can find more examples of using SageMaker Ground Truth at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to prepare image datasets for training, let's put the CV
    algorithms to work.
  prefs: []
  type: TYPE_NORMAL
- en: Using the built-in CV algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to train and deploy models with all three algorithms
    using public image datasets. We will cover both training from scratch and transfer
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Training an image classification model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this first example, let's use the image classification algorithm to build
    a model classifying the Dogs vs. Cats dataset that we prepared in a previous section.
    We'll first train using image format, and then using RecordIO format.
  prefs: []
  type: TYPE_NORMAL
- en: Training in image format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will begin training using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Jupyter notebook, we define the appropriate data paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We configure the `Estimator` for the image classification algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use a GPU instance called `ml.p3.2xlarge`, which packs more than enough punch
    for this dataset ($4.131/hour in eu-west-1).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'What about hyperparameters ([https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html))?
    We set the number of classes (2) and the number of training samples (22,500).
    Since we''re working with the image format, we need to resize images explicitly,
    setting the smallest dimension to 224 pixels. As we have enough data, we decide
    to train from scratch. In order to keep the training time low, we settle for an
    18-layer **ResNet** model, and we train only for 10 epochs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the four channels, setting their content type to `application/x-image`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We launch the training job as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the training log, we see that data download takes about 3 minutes. Surprise,
    surprise: we also see that the algorithm builds RecordIO files before training.
    This step lasts about 1 minute:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As training starts, we see that an epoch takes approximately 22 seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The job lasts 506 seconds in total (about 8 minutes), costing us (506/3600)*$4.131=$0.58\.
    It reaches a validation accuracy of **91.2%** (hopefully, you see something similar).
    This is pretty good considering that we haven't even tweaked the hyperparameters
    yet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then deploy the model on a small CPU instance as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We download the following test image and send it for prediction in `application/x-image`
    format.![Figure 5.6 – Test picture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_05_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Printing out the probability and the class, our model indicates that this image
    is a dog with 99.997% confidence and that the image belongs to class 1:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we''re done, we delete the endpoint as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now let's run the same training job with the dataset in RecordIO format.
  prefs: []
  type: TYPE_NORMAL
- en: Training in RecordIO format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The only difference is how we define the input channels. We only need two channels
    this time in order to serve the RecordIO files we uploaded to S3\. Accordingly,
    the content type is set to `application/x-recordio`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Training again, we see that data download takes 1 minute and that the file generation
    step has disappeared. Although it's difficult to draw any conclusion from a single
    run, using RecordIO datasets will generally save you time and money, even when
    training on a single instance.
  prefs: []
  type: TYPE_NORMAL
- en: The Dogs vs. Cats dataset has over 10,000 samples per class, which is more than
    enough to train from scratch. Now, let's try a dataset where that's not the case.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning an image classification model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please consider the **Caltech-256** dataset, a popular public dataset of 15,240
    images in 256 classes, plus a clutter class ([http://www.vision.caltech.edu/Image_Datasets/Caltech256/](http://www.vision.caltech.edu/Image_Datasets/Caltech256/)).
    Browsing image categories, we see that all classes have a small number of samples.
    For instance, the "duck" class only has 60 images: it''s doubtful that a deep
    learning algorithm, no matter how sophisticated, could extract the unique visual
    features of ducks with that little data.'
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, training from scratch is simply not an option. Instead, we will
    use a technique called **transfer learning**, where we start from a network that
    has already been trained on a very large and diverse image dataset. **ImageNet**
    ([http://www.image-net.org/](http://www.image-net.org/)) is probably the most
    popular choice for pretraining, with 1,000 classes and millions of images.
  prefs: []
  type: TYPE_NORMAL
- en: The pretrained network has already learned how to extract patterns from complex
    images. Assuming that the images in our dataset are similar enough to those in
    the pretraining dataset, our model should be able to inherit that knowledge. Training
    for only a few more epochs on our dataset, we should be able to **fine-tune**
    the pretrained model on our data and classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we can easily do this with SageMaker. In fact, we''ll reuse
    the code for the previous example with minimal changes. Let''s get into it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We download the Caltech-256 in RecordIO format (if you''d like, you could download
    it in image format, and convert it to RecordIO: practice makes perfect!):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We upload the dataset to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We configure the `Estimator` function for the image classification algorithm.
    The code is strictly identical to *step 2* in the previous example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use `use_pretrained_network` to 1\. The final fully connected layer of the
    pretrained network will be resized to the number of classes present in our dataset,
    and its weights will be assigned random values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We set the correct number of classes (256+1) and training samples as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we're fine-tuning, we only train for 5 epochs, with a smaller learning
    rate of 0.001.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We configure channels and we launch the training job. The code is strictly identical
    to *step 4* in the previous example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After 5 epochs and 272 seconds, we see the following metric in the training
    log:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is quite good for just a few minutes of training. Even with enough data,
    it would have taken much longer to get that result from scratch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To deploy and test the model, we would reuse *steps 7-9* in the previous example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see, transfer learning is a very powerful technique. It can deliver
    excellent results, even when you have little data. You will also train for fewer
    epochs, saving time and money in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the next algorithm, **object detection**.
  prefs: []
  type: TYPE_NORMAL
- en: Training an object detection model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we''ll use the object detection algorithm to build a model
    on the Pascal VOC dataset that we prepared in a previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining data paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We select the object detection algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We configure the `Estimator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set the required hyperparameters. We select a pretrained ResNet-50 network
    for the base network. We set the number of classes and training samples. We settle
    on 30 epochs, which should be enough to start seeing results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then configure the two channels, and we launch the training job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Selecting our job in **SageMaker components and registries** | **Experiments
    and trials**, we can see near-real-time metrics and charts. The next image shows
    the validation **mean average precision metric (mAP)**, a key metric for object
    detection models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Validation mAP'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_05_7.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.7 – Validation mAP
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Please explore the other tabs (**Metrics**, **Parameters**, **Artifacts**, and
    so on). They contain everything you need to know about a particular job. Please
    note the **Stop training job** button in the top-right corner, which you can use
    to terminate a job at any time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Training lasts for 1 hour and 40 minutes. This is a pretty heavy model! We get
    a **mean average precision metric** (**mAP**) of 0.5151\. Production use would
    require more training, but we should be able to test the model already.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Given its complexity, we deploy the model to a larger CPU instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We download a test image from Wikipedia and predict it with our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response contains a list of predictions. Each individual prediction contains
    a class identifier, the confidence score, and the relative coordinates of the
    bounding box. Here are the first predictions in the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using this information, we could plot the bounding boxes on the source image.
    For the sake of brevity, I will not include the code here, but you''ll find it
    in the GitHub repository for this book. The following output shows the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Test image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_05_8.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.8 – Test image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When we''re done, we delete the endpoint as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This concludes our exploration of object detection. We have one more algorithm
    to go: **Semantic Segmentation**.'
  prefs: []
  type: TYPE_NORMAL
- en: Training a semantic segmentation model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we''ll use the semantic segmentation algorithm to build a
    model on the Pascal VOC dataset that we prepared in a previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we define the data paths, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We select the semantic segmentation algorithm, and we configure the `Estimator`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the required hyperparameters. We select a pretrained ResNet-50 network
    for the base network and a pretrained **FCN** for detection. We set the number
    of classes and training samples. Again, we settle on 30 epochs, which should be
    enough to start seeing results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We configure the four channels, setting the content type to `image/jpeg` for
    source images, and `image/png` for mask images. Then, we launch the training job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training lasts about 32 minutes. We get a **mean intersection-over-union metric**
    (**mIOU**) of 0.4874, as shown in the following plot:![Figure 5.9 – Validation
    mIOU
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_05_9.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.9 – Validation mIOU
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We deploy the model to a CPU instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the endpoint is in service, we grab a test image, and we send it for prediction
    as a byte array with the appropriate content type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the **Python Imaging Library** (**PIL**), we process the response mask
    and display it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following images show the source image and the predicted mask. This result
    is promising, and would improve with more training:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Test image and segmented test image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_05_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.10 – Test image and segmented test image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Predicting again with the `application/x-protobuf` accept type, we receive
    class probabilities for all pixels in the source image. The response is a protobuf
    buffer, which we save to a binary file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The buffer contains two tensors: one with the shape of the probability tensor,
    and one with the actual probabilities. We load them using `values` tensor describes
    one image of size 289x337, where each pixel is assigned 21 probabilities, one
    for each of the Pascal VOC classes. You can check that 289*337*21=2,045,253.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Knowing that, we can now reshape the `values` tensor, retrieve the 21 probabilities
    for the (0,0) pixel, and print the class identifier with the highest probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The highest probability is at index 0: the predicted class for pixel (0,0)
    is class 0, the background class.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When we''re done, we delete the endpoint as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, these three algorithms make it easy to train CV models. Even
    with default hyperparameters, we get good results pretty quickly. Still, we start
    feeling the need to scale our training jobs. Don't worry once the relevant features
    have been covered in future chapters, we'll revisit some of our CV examples and
    we'll scale them radically!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned about image classification, object detection, and
    semantic segmentation algorithms. You also learned how to prepare datasets in
    Image, RecordIO, and SageMaker Ground Truth formats. Labeling and preparing data
    is a critical step that takes a lot of work, and we covered it in great detail.
    Finally, you learned how to use the SageMaker SDK to train and deploy models with
    the three algorithms, as well as how to interpret results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to use built-in algorithms for natural
    language processing.
  prefs: []
  type: TYPE_NORMAL

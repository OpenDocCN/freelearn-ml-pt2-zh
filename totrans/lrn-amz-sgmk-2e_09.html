<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer128">
			<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/>Chapter 7: Extending Machine Learning Services Using Built-In Frameworks</h1>
			<p>In the last three chapters, you learned how to use built-in algorithms to train and deploy models without having to write a line of machine learning code. However, these algorithms don't cover the full spectrum of machine learning problems. In a lot of cases, you'll need to write your own code. Thankfully, several open source frameworks make this reasonably easy.</p>
			<p>In this chapter, you will learn how to train and deploy models with the most popular open source frameworks for machine learning and deep learning. We will cover the following topics:</p>
			<ul>
				<li>Discovering the built-in frameworks in Amazon SageMaker</li>
				<li>Running your framework code on Amazon SageMaker</li>
				<li>Using the built-in frameworks</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor131"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you haven't got one already, please point your browser to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create one. You should also familiarize yourself with the AWS Free Tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the AWS command-line interface for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).</p>
			<p>You will need a working Python 3.x environment. Installing the Anaconda distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory but strongly encouraged, as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>You will need a working Docker installation. You can find installation instructions and the necessary documentation at <a href="https://docs.docker.com">https://docs.docker.com</a>. </p>
			<p>The code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor132"/>Discovering the built-in frameworks in Amazon SageMaker</h1>
			<p>SageMaker lets <a id="_idIndexMarker646"/>you train and deploy your models with the following machine learning and deep learning frameworks:</p>
			<ul>
				<li><strong class="bold">Scikit-learn</strong>, <a id="_idIndexMarker647"/>undoubtedly the<a id="_idIndexMarker648"/> most widely used open source library for machine learning. If you're new to this topic, start here: <a href="https://scikit-learn.org">https://scikit-learn.org</a>.</li>
				<li><strong class="bold">XGBoost</strong>, an <a id="_idIndexMarker649"/>extremely popular<a id="_idIndexMarker650"/> and versatile open source algorithm for regression, classification, and ranking problems (<a href="https://xgboost.ai">https://xgboost.ai</a>). It's also available as a built-in algorithm, as presented in <a href="B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Machine Learning Models</em>. Using it in framework mode will give us more flexibility.</li>
				<li><strong class="bold">TensorFlow</strong>, <a id="_idIndexMarker651"/>an extremely popular open source library<a id="_idIndexMarker652"/> for deep learning (<a href="https://www.tensorflow.org">https://www.tensorflow.org</a>). SageMaker also supports the <a id="_idIndexMarker653"/>lovable <strong class="bold">Keras</strong> API (<a href="https://keras.io">https://keras.io</a>).</li>
				<li><strong class="bold">PyTorch</strong>, another<a id="_idIndexMarker654"/> highly popular<a id="_idIndexMarker655"/> open source library for deep learning (<a href="https://pytorch.org">https://pytorch.org</a>). Researchers, in particular, enjoy its flexibility.</li>
				<li><strong class="bold">Apache MXNet</strong>, an interesting<a id="_idIndexMarker656"/> challenger for deep learning. Natively implemented <a id="_idIndexMarker657"/>in C++, it's often faster and more scalable than its competitors. Its <strong class="bold">Gluon</strong> API provides <a id="_idIndexMarker658"/>rich toolkits for computer vision (<a href="https://gluon-cv.mxnet.io">https://gluon-cv.mxnet.io</a>), <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) (<a href="https://gluon-nlp.mxnet.io">https://gluon-nlp.mxnet.io</a>), and time series data (<a href="https://gluon-ts.mxnet.io">https://gluon-ts.mxnet.io</a>).</li>
				<li><strong class="bold">Chainer</strong>, another<a id="_idIndexMarker659"/> worthy challenger <a id="_idIndexMarker660"/>for deep learning (<a href="https://chainer.org">https://chainer.org</a>). </li>
				<li><strong class="bold">Hugging Face</strong>, the most popular collection of state-of-the-art tools and models for NLP (<a href="https://huggingface.co">https://huggingface.co</a>).</li>
				<li>Frameworks for <strong class="bold">reinforcement learning</strong>, such <a id="_idIndexMarker661"/>as <strong class="bold">Intel Coach</strong>, <strong class="bold">Ray RLlib</strong>, and <strong class="bold">Vowpal Wabbit</strong>. I won't <a id="_idIndexMarker662"/>discuss this topic here as it could take up another book!</li>
				<li><strong class="bold">Spark</strong>, thanks<a id="_idIndexMarker663"/> to a dedicated SDK that lets you train and deploy models directly from your <a id="_idIndexMarker664"/>Spark application using<a id="_idIndexMarker665"/> either <strong class="bold">PySpark</strong> or <strong class="bold">Scala</strong> (<a href="https://github.com/aws/sagemaker-spark">https://github.com/aws/sagemaker-spark</a>). </li>
			</ul>
			<p>You'll find plenty of examples of all of these at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk</a>. </p>
			<p>In this chapter, we'll focus on the most popular ones: XGBoost, scikit-learn, TensorFlow, PyTorch, and Spark.</p>
			<p>The best way <a id="_idIndexMarker666"/>to get started is to run a first simple example. As you will see, the workflow is the same as for built-in algorithms. We'll highlight a few differences along the way, which we'll dive into later in this chapter.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>Running a first example with XGBoost</h2>
			<p>In this example, we'll build a binary <a id="_idIndexMarker667"/>classification model with the XGBoost built-in framework. At the time of writing, the latest version supported by SageMaker is 1.3-1.</p>
			<p>We'll use our own training script based on the <strong class="source-inline">xgboost.XGBClassifier</strong> object and the Direct Marketing dataset, which we used in <a href="B17705_03_Final_JM_ePub.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">AutoML with Amazon SageMaker Autopilot</em>:</p>
			<ol>
				<li value="1">First, we download and extract the dataset:<p class="source-code">%%sh</p><p class="source-code">wget -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</p><p class="source-code">unzip -o bank-additional.zip</p></li>
				<li>We import the SageMaker SDK and define an S3 prefix for the job:<p class="source-code">import sagemaker</p><p class="source-code">sess   = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()                     </p><p class="source-code">prefix = 'xgboost-direct-marketing'</p></li>
				<li>We load the dataset and apply very basic processing (as it's not our focus here). Simply one-hot encode the categorical features, move the labels to the first column (an XGBoost requirement), shuffle the dataset, split it for training and validation, and<a id="_idIndexMarker668"/> save<a id="_idIndexMarker669"/> the results in two separate CSV files:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">data = pd.read_csv('./bank-additional/bank-additional-full.csv')</p><p class="source-code">data = pd.get_dummies(data)</p><p class="source-code">data = data.drop(['y_no'], axis=1)</p><p class="source-code">data = pd.concat([data['y_yes'], </p><p class="source-code">                 data.drop(['y_yes'], axis=1)], </p><p class="source-code">                 axis=1)</p><p class="source-code">data = data.sample(frac=1, random_state=123)</p><p class="source-code">train_data, val_data = train_test_split(</p><p class="source-code">    data, test_size=0.05)</p><p class="source-code">train_data.to_csv(</p><p class="source-code">    'training.csv', index=False, header=False)</p><p class="source-code">val_data.to_csv(</p><p class="source-code">    'validation.csv', index=False, header=False)</p></li>
				<li>We <a id="_idIndexMarker670"/>upload <a id="_idIndexMarker671"/>the two files to S3:<p class="source-code">training = sess.upload_data(path='training.csv', </p><p class="source-code">           key_prefix=prefix + '/training')</p><p class="source-code">validation = sess.upload_data(path='validation.csv', </p><p class="source-code">             key_prefix=prefix + "/validation")</p><p class="source-code">output   = 's3://{}/{}/output/'.format(bucket,prefix)</p></li>
				<li>We define two inputs, with data in CSV format:<p class="source-code">from sagemaker import TrainingInput</p><p class="source-code">train_input = TrainingInput(</p><p class="source-code">    training_path, content_type='text/csv')</p><p class="source-code">val_input = TrainingInput(</p><p class="source-code">    validation_path, content_type='text/csv')</p></li>
				<li>Define an <a id="_idIndexMarker672"/>estimator<a id="_idIndexMarker673"/> for the training job. Of course, we could use the generic <strong class="source-inline">Estimator</strong> object and pass the name of the XGBoost container hosted in <strong class="bold">Amazon ECR</strong>. Instead, we use the <strong class="source-inline">XGBoost</strong> estimator, which automatically selects the right container:<p class="source-code">from sagemaker.xgboost import XGBoost</p><p class="source-code">xgb_estimator = XGBoost(</p><p class="source-code">    role= sagemaker.get_execution_role(),</p><p class="source-code">    entry_point='xgb-dm.py',</p><p class="source-code">    instance_count=1, </p><p class="source-code">    instance_type='ml.m5.large',</p><p class="source-code">    framework_version='1.2-2',</p><p class="source-code">    output_path=output,</p><p class="source-code">    hyperparameters={</p><p class="source-code">        'num_round': 100,</p><p class="source-code">        'early_stopping_rounds': 10,</p><p class="source-code">        'max-depth': 5,</p><p class="source-code">        'eval-metric': 'auc'}</p><p class="source-code">)</p><p>Several parameters are familiar here: the role, the infrastructure requirements, and the output path. What about the other ones? <strong class="source-inline">entry_point</strong> is the path of our training script (available in the GitHub repository for this book). <strong class="source-inline">hyperparameters</strong> is passed to the training script. We also have to select a <strong class="source-inline">framework_version</strong> value; this is the version of XGBoost that we want to use. </p></li>
				<li>We train as usual: <p class="source-code">xgb_estimator.fit({'train':training, </p><p class="source-code">                   'validation':validation})</p></li>
				<li>We also<a id="_idIndexMarker674"/> deploy <a id="_idIndexMarker675"/>as usual, creating a unique endpoint name:<p class="source-code">from time import strftime,gmtime</p><p class="source-code">xgb_endpoint_name = </p><p class="source-code">    prefix+strftime("%Y-%m-%d-%H-%M-%S", gmtime())</p><p class="source-code">xgb_predictor = xgb_estimator.deploy(</p><p class="source-code">    endpoint_name=xgb_endpoint_name,</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.t2.medium')</p><p>Then, we load a few samples from the validation set and send them for prediction in CSV format. The response contains a score between 0 and 1 for each sample:</p><p class="source-code">payload = val_data[:10].drop(['y_yes'], axis=1) </p><p class="source-code">payload = payload.to_csv(header=False, </p><p class="source-code">          index=False).rstrip('\n')</p><p class="source-code">xgb_predictor.serializer =</p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">xgb_predictor.deserializer = </p><p class="source-code">    sagemaker.deserializers.CSVDeserializer()</p><p class="source-code">response = xgb_predictor.predict(payload)</p><p class="source-code">print(response)</p><p>This prints out the following probabilities:</p><p class="source-code"><strong class="bold">[['0.07206538'], ['0.02661967'], ['0.16043524'], ['4.026455e-05'], ['0.0002120432'], ['0.52123886'], ['0.50755614'], ['0.00015006188'], ['3.1439096e-05'], ['9.7614546e-05']]</strong></p></li>
				<li>When we're done, we delete the endpoint:<p class="source-code">xgb_predictor.delete_endpoint()</p></li>
			</ol>
			<p>We used XGBoost here, but the workflow would be identical for another framework. This standard way of training and deploying makes it really easy to switch from built-in algorithms to frameworks, or from one framework to the next.</p>
			<p>The points that we <a id="_idIndexMarker676"/>need to<a id="_idIndexMarker677"/> focus on here are as follows:</p>
			<ul>
				<li><strong class="bold">Framework containers</strong>: What are they? Can we see how they're built? Can we customize them? Can we use them to train on our local machine?</li>
				<li><strong class="bold">Training</strong>: How does a SageMaker training script differ from vanilla framework code? How does it receive hyperparameters? How should it read input data? Where should it save the model?</li>
				<li><strong class="bold">Deploying</strong>: How is the model deployed? Should the script provide some code for this? What's the input format for prediction?</li>
				<li><strong class="bold">Managing dependencies</strong>: Can we add additional source files besides the <strong class="source-inline">entry_point</strong> script? Can we add libraries for training and deployment?</li>
			</ul>
			<p>All these questions will be answered now!</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor134"/>Working with framework containers</h2>
			<p>SageMaker contains a <a id="_idIndexMarker678"/>training and inference container for<a id="_idIndexMarker679"/> each built-in<a id="_idIndexMarker680"/> framework, and they are updated regularly to the latest versions. Different containers are also available for CPU and GPU instances. All these containers are collectively <a id="_idIndexMarker681"/>known as <strong class="bold">Deep Learning Containers</strong> (<a href="https://aws.amazon.com/machine-learning/containers">https://aws.amazon.com/machine-learning/containers</a>).</p>
			<p>As we saw in the previous example, they let you use your own code without having to maintain bespoke containers. In most cases, you won't need to look any further, and you can happily forget that these containers even exist. If this topic feels too advanced for now, feel free to skip it for now, and move on to the <em class="italic">Training and deploying locally</em> section.</p>
			<p>If you're curious or have custom requirements, you'll be happy to learn that the code for these containers is open source:</p>
			<ul>
				<li><strong class="bold">Scikit-learn</strong>: <a href="https://github.com/aws/sagemaker-scikit-learn-container">https://github.com/aws/sagemaker-scikit-learn-container</a><span class="hidden"> </span></li>
				<li><strong class="bold">XGBoost</strong>: <a href="https://github.com/aws/sagemaker-xgboost-container">https://github.com/aws/sagemaker-xgboost-container</a> </li>
				<li><strong class="bold">TensorFlow, PyTorch, Apache MXNet, and Hugging Face</strong>: <a href="https://github.com/aws/deep-learning-containers">https://github.com/aws/deep-learning-containers</a></li>
				<li><strong class="bold">Chainer</strong>: <a href="https://github.com/aws/sagemaker-chainer-container">https://github.com/aws/sagemaker-chainer-container</a> </li>
			</ul>
			<p>For starters, this lets you understand how these containers are built and how SageMaker trains and predicts with them. You could also do the following:</p>
			<ul>
				<li>Build and run them on your local machine for local experimentation.</li>
				<li>Build <a id="_idIndexMarker682"/>and run <a id="_idIndexMarker683"/>them on your favorite managed Docker service, such as <strong class="bold">Amazon ECS</strong>, <strong class="bold">Amazon EKS</strong>, or <strong class="bold">Amazon Fargate</strong> (<a href="https://aws.amazon.com/containers">https://aws.amazon.com/containers</a>).</li>
				<li>Customize them, push<a id="_idIndexMarker684"/> them to Amazon ECR, and use them with the estimators present in the SageMaker SDK. We'll demonstrate this in <a href="B17705_08_Final_JM_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 8</em></a>, <em class="italic">Using Your Algorithms and Code</em>.</li>
			</ul>
			<p>These containers have another nice property. You can use them with the SageMaker SDK to train and deploy models on your local machine. Let's see how this works.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor135"/>Training and deploying locally</h2>
			<p><strong class="bold">Local mode</strong> is the<a id="_idIndexMarker685"/> ability to train<a id="_idIndexMarker686"/> and deploy models with the SageMaker SDK without firing up on-demand managed infrastructure in AWS. You use your local machine instead. In this context, "local" means the machine running the notebook: it could be your laptop, a local server, or a small <strong class="bold">notebook instance</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">At the time of writing, local mode is not available in SageMaker Studio.</p>
			<p>This is an excellent way to quickly experiment and iterate on a small dataset. You won't have to wait for instances to come up, and you won't have to pay for them either!</p>
			<p>Let's revisit our previous XGBoost example, highlighting the changes required to use local mode:</p>
			<ol>
				<li value="1">Explicitly set the name of the IAM role. <strong class="source-inline">get_execution_role()</strong> does not work on your local machine (it does on a notebook instance):<p class="source-code">#role = sagemaker.get_execution_role()</p><p class="source-code">role = 'arn:aws:iam::0123456789012:role/Sagemaker-fullaccess'</p></li>
				<li>Load the training and validation datasets from local files. Store the model locally in <strong class="source-inline">/tmp</strong>:<p class="source-code">training = 'file://training.csv'</p><p class="source-code">validation = 'file://validation.csv'</p><p class="source-code">output = 'file:///tmp'</p></li>
				<li>In the <strong class="source-inline">XGBoost</strong> estimator, set <strong class="source-inline">instance_type</strong> to <strong class="source-inline">local</strong>. For local GPU training, we would use <strong class="source-inline">local_gpu</strong>.</li>
				<li>In <strong class="source-inline">xgb_estimator.deploy()</strong>, set <strong class="source-inline">instance_type</strong> to <strong class="source-inline">local</strong>.</li>
			</ol>
			<p>That's all it takes to<a id="_idIndexMarker687"/> train <a id="_idIndexMarker688"/>on your local machine using the same container you would use at scale on AWS. This container will be pulled once to your local machine and you'll be using it from then on. When you're ready to train at scale, just replace the <strong class="source-inline">local</strong> or <strong class="source-inline">local_gpu</strong> instance type with the appropriate AWS instance type and you're good to go.</p>
			<p class="callout-heading">Troubleshooting</p>
			<p class="callout">If you see strange deployment errors, try restarting Docker (<strong class="source-inline">sudo service docker restart</strong>). I found that it doesn't like being interrupted during deployment, which it tends to do a lot when working inside Jupyter Notebooks!</p>
			<p>Now, let's see what it<a id="_idIndexMarker689"/> takes to run our own code inside these containers. This feature is called <strong class="bold">script mode</strong>.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/>Training with script mode</h2>
			<p>Since your <a id="_idIndexMarker690"/>training<a id="_idIndexMarker691"/> code runs inside a SageMaker container, it needs to be able to do the following:</p>
			<ul>
				<li>Receive hyperparameters passed to the estimator.</li>
				<li>Read data available in input channels (training, validation, and more).</li>
				<li>Save the trained model in the right place.</li>
			</ul>
			<p>Script mode is how SageMaker makes this possible. The name comes from the way your code is invoked in the container. Looking at the training log for our XGBoost job, we see this:</p>
			<p class="source-code"><strong class="bold">Invoking script with the following command:</strong></p>
			<p class="source-code"><strong class="bold">/miniconda3/bin/python3 -m xgb-dm --early-stopping-rounds 10 </strong></p>
			<p class="source-code"><strong class="bold">--eval-metric auc --max-depth 5 </strong></p>
			<p>Our code is invoked like a plain Python script (hence the name script mode). We can see that hyperparameters are passed as command-line arguments, which answers the question of what we should use inside the script to read them: <strong class="source-inline">argparse</strong>.</p>
			<p>Here's the corresponding code snippet in our script:</p>
			<p class="source-code">parser = argparse.ArgumentParser()</p>
			<p class="source-code">parser.add_argument('--max-depth', type=int, default=4)</p>
			<p class="source-code">parser.add_argument('--early-stopping-rounds', type=int, </p>
			<p class="source-code">                    default=10)</p>
			<p class="source-code">parser.add_argument('--eval-metric', type=str, </p>
			<p class="source-code">                    default='error')</p>
			<p>What about the location of the input data and the saved model? If we look at the log a little more closely, we'll see this:</p>
			<p class="source-code"><strong class="bold">SM_CHANNEL_TRAIN=/opt/ml/input/data/train</strong></p>
			<p class="source-code"><strong class="bold">SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation</strong></p>
			<p class="source-code"><strong class="bold">SM_MODEL_DIR=/opt/ml/model</strong></p>
			<p>These three environment variables define <strong class="bold">local paths inside the container</strong>, pointing to the respective locations for the training data, validation data, and the saved model. Does this mean we have to manually copy the datasets and the model from and to S3? No! SageMaker<a id="_idIndexMarker692"/> takes <a id="_idIndexMarker693"/>care of all this automatically for us. This is part of the support code present in the container.</p>
			<p>Our script only needs to read these variables. I recommend using <strong class="source-inline">argparse</strong> again, as this will let us pass the paths to our script when we train outside of SageMaker (more on this soon).</p>
			<p>Here's the corresponding code snippet in our script:</p>
			<p class="source-code">parser.add_argument('--model-dir', type=str, </p>
			<p class="source-code">    default=os.environ['SM_MODEL_DIR'])</p>
			<p class="source-code">parser.add_argument('--training-dir', type=str, </p>
			<p class="source-code">    default=os.environ['SM_CHANNEL_TRAIN'])</p>
			<p class="source-code">parser.add_argument('--validation', type=str, </p>
			<p class="source-code">    default=os.environ['SM_CHANNEL_VALIDATION'])</p>
			<p class="callout-heading">Channel names</p>
			<p class="callout">The <strong class="source-inline">SM_CHANNEL_xxx</strong> variables are named according to the channels passed to <strong class="source-inline">fit()</strong>. For instance, if your algorithm required a channel named <strong class="source-inline">foobar</strong>, you'd name it <strong class="source-inline">foobar</strong> in <strong class="source-inline">fit()</strong> and <strong class="source-inline">SM_CHANNEL_FOOBAR</strong> in your script. In your container, the data for that channel would automatically be available in <strong class="source-inline">/opt/ml/input/data/foobar</strong>.</p>
			<p>To sum things up, in <a id="_idIndexMarker694"/>order to train framework code on SageMaker, we only need to do the following:</p>
			<ol>
				<li value="1">Use <strong class="source-inline">argparse</strong> to read hyperparameters passed as command-line arguments. Chances are you're already doing this in your code anyway!</li>
				<li>Read the <strong class="source-inline">SM_CHANNEL_xxx</strong> environment variables and load data from there.</li>
				<li>Read the <strong class="source-inline">SM_MODEL_DIR</strong> environment variable and save the trained model there.</li>
			</ol>
			<p>Now, let's<a id="_idIndexMarker695"/> talk about <a id="_idIndexMarker696"/>deploying models trained in script mode.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor137"/>Understanding model deployment</h2>
			<p>In general, your script <a id="_idIndexMarker697"/>needs to include the following:</p>
			<ul>
				<li>A function to load the model</li>
				<li>A function to process input data before it's passed to the model</li>
				<li>A function to process predictions before they're returned to the caller</li>
			</ul>
			<p>The amount of actual work required depends on the framework and the input format you use. Let's see what this means for TensorFlow, PyTorch, MXNet, XGBoost, and scikit-learn.</p>
			<h3>Deploying with TensorFlow</h3>
			<p>The<a id="_idIndexMarker698"/> TensorFlow inference container<a id="_idIndexMarker699"/> relies on the <strong class="bold">TensorFlow Serving</strong> model server for <a id="_idIndexMarker700"/>model deployment (<a href="https://www.tensorflow.org/tfx/guide/serving">https://www.tensorflow.org/tfx/guide/serving</a>). For this reason, your training code must save the model in this format. Model loading and prediction are available automatically. </p>
			<p>JSON is the default input format for prediction, and it also works for <strong class="source-inline">numpy</strong> arrays thanks to automatic serialization. JSON Lines and CSV are also supported. For other formats, you can implement your own preprocessing and<a id="_idIndexMarker701"/> postprocessing functions, <strong class="source-inline">input_handler()</strong> and <strong class="source-inline">output_handler()</strong>. You'll find more information at <a href="https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-from-an-estimator">https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-from-an-estimator</a>.</p>
			<p>You can also dive <a id="_idIndexMarker702"/>deeper into the TensorFlow inference container at <a href="https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference">https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference</a>.</p>
			<h3>Deploying with PyTorch</h3>
			<p>The <a id="_idIndexMarker703"/>PyTorch<a id="_idIndexMarker704"/> inference container relies on the <strong class="bold">TorchServe</strong> model server (<a href="https://pytorch.org/serve">https://pytorch.org/serve</a>). Models <a id="_idIndexMarker705"/>are loaded automatically. Prediction is automatically available if they implement the <strong class="source-inline">__call__()</strong> method. If not, you should provide a <strong class="source-inline">predict_fn()</strong> function in the inference script.</p>
			<p>For prediction, <strong class="source-inline">numpy</strong> is the default input format. JSON Lines and CSV are also supported. For other formats, you can implement your own preprocessing and postprocessing functions. You'll find more information at <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model">https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model</a>.</p>
			<p>You can dive deeper into the PyTorch inference container at <a href="https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference">https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference</a>.</p>
			<h3>Deploying with Apache MXNet</h3>
			<p>The<a id="_idIndexMarker706"/> Apache MXNet inference container relies on <strong class="bold">Multi-Model Server</strong> (<strong class="bold">MMS</strong>) for <a id="_idIndexMarker707"/>model deployment (<a href="https://github.com/awslabs/multi-model-server">https://github.com/awslabs/multi-model-server</a>). It uses<a id="_idIndexMarker708"/> the default MXNet model format. </p>
			<p>Models based on the <strong class="source-inline">Module</strong> API do not require a model loading function. For prediction, they support data in JSON, CSV, or <strong class="source-inline">numpy</strong> format. </p>
			<p>Gluon models do require a model loading function as parameters need to be explicitly initialized. Data can be sent in JSON or <strong class="source-inline">numpy</strong> format. </p>
			<p>For other data formats, you can implement your own preprocessing, prediction, and postprocessing<a id="_idIndexMarker709"/> functions. You can find more<a id="_idIndexMarker710"/> information at <a href="https://sagemaker.readthedocs.io/en/stable/using_mxnet.html">https://sagemaker.readthedocs.io/en/stable/using_mxnet.html</a>.</p>
			<p>You can dive deeper into the MXNet inference container at <a href="https://github.com/aws/deep-learning-containers/tree/master/mxnet/inference/docker">https://github.com/aws/deep-learning-containers/tree/master/mxnet/inference/docker</a>.</p>
			<h3>Deploying XGBoost and scikit-learn</h3>
			<p>Likewise, XGBoost <a id="_idIndexMarker711"/>and<a id="_idIndexMarker712"/> scikit-learn rely on <a href="https://github.com/aws/sagemaker-xgboost-container">https://github.com/aws/sagemaker-xgboost-container</a> and <a href="https://github.com/aws/sagemaker-scikit-learn-container">https://github.com/aws/sagemaker-scikit-learn-container</a>, respectively.</p>
			<p>Your script needs to provide the following:</p>
			<ul>
				<li>A <strong class="bold">mandatory</strong> <strong class="source-inline">model_fn()</strong> function<a id="_idIndexMarker713"/> to load the model. Just like for training, the location of the model to load is passed in the <strong class="source-inline">SM_MODEL_DIR</strong> environment variable.</li>
				<li>Two optional functions to deserialize and serialize prediction data, named <strong class="source-inline">input_fn()</strong> and <strong class="source-inline">output_fn()</strong>. These functions are only required if you need another input format other than JSON, CSV, or <strong class="source-inline">numpy</strong>.</li>
				<li>An optional <strong class="source-inline">predict_fn()</strong> function passes deserialized data to the model and returns a prediction. This is only required if you need to preprocess data before predicting it, or to postprocess predictions.</li>
			</ul>
			<p>For XGBoost and scikit-learn, the <strong class="source-inline">model_fn()</strong> function is extremely simple and quite generic. Here are a couple<a id="_idIndexMarker714"/> of examples that should work in most cases:</p>
			<p class="source-code"># Scikit-learn</p>
			<p class="source-code">def model_fn(model_dir):</p>
			<p class="source-code">    clf = joblib.load(os.path.join(model_dir, </p>
			<p class="source-code">                                   'model.joblib'))</p>
			<p class="source-code">    return clf</p>
			<p class="source-code"># XGBoost</p>
			<p class="source-code">def model_fn(model_dir):</p>
			<p class="source-code">    model = xgb.Booster()</p>
			<p class="source-code">    model.load_model(os.path.join(model_dir, 'xgb.model'))</p>
			<p class="source-code">    return model</p>
			<p>SageMaker also lets you import and export models. You can upload an existing model to S3 and deploy it directly on SageMaker. Likewise, you can copy a trained model from S3 and deploy <a id="_idIndexMarker715"/>it <a id="_idIndexMarker716"/>elsewhere. We'll look at this in detail in <a href="B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Deploying Machine Learning Models</em>.</p>
			<p>Now, let's talk about training and deployment dependencies.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/>Managing dependencies</h2>
			<p>In many <a id="_idIndexMarker717"/>cases, you'll need to add extra source files and libraries to the framework's containers. Let's see how we can easily do this.</p>
			<h3>Adding source files for training</h3>
			<p>By default, all <a id="_idIndexMarker718"/>estimators<a id="_idIndexMarker719"/> load the entry point script from the current directory. If you need additional source files for training, estimators let you pass a <strong class="source-inline">source_dir</strong> parameter, which points at the directory storing the extra files. Please note that the entry point script must be in the same directory.</p>
			<p>In the following example, <strong class="source-inline">myscript.py</strong> and all additional source files must be placed in the <strong class="source-inline">src</strong> directory. SageMaker will automatically package the directory and copy it inside the training container:</p>
			<p class="source-code">sk = SKLearn(entry_point='myscript.py',</p>
			<p class="source-code">             source_dir='src',</p>
			<p class="source-code">             . . .</p>
			<h3>Adding libraries for training</h3>
			<p>You can use<a id="_idIndexMarker720"/> different techniques to add libraries that are required for training.</p>
			<p>For libraries that can be installed with <strong class="source-inline">pip</strong>, the simplest technique is to add a <strong class="source-inline">requirements.txt</strong> file in the same folder as the entry point script. SageMaker will automatically install these libraries inside the container.</p>
			<p>Alternatively, you can use <strong class="source-inline">pip</strong> to install libraries directly in the training script by issuing a <strong class="source-inline">pip install</strong> command. We used this in <a href="B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 6</em></a>, <em class="italic">Training Natural Language Processing Models</em>, with LDA and NTM. This is useful when you don't want to or cannot modify the SageMaker code that launches the training job:</p>
			<p class="source-code">import subprocess, sys</p>
			<p class="source-code">def install(package):</p>
			<p class="source-code">    subprocess.call([sys.executable, "-m", </p>
			<p class="source-code">                    "pip", "install", package])</p>
			<p class="source-code">if __name__=='__main__':</p>
			<p class="source-code">    install('gensim')</p>
			<p class="source-code">    import gensim</p>
			<p class="source-code">    . . . </p>
			<p>For libraries that can't be installed with <strong class="source-inline">pip</strong>, you should use the <strong class="source-inline">dependencies</strong> parameter. It's available in all estimators, and it lets you list libraries to add to the training job. These libraries need to be present locally, in a virtual environment or a bespoke directory. SageMaker will package them and copy them inside the training container.</p>
			<p>In the following example, <strong class="source-inline">myscript.py</strong> needs the <strong class="source-inline">mylib</strong> library. We install it in the <strong class="source-inline">lib</strong> local directory:</p>
			<p class="source-code">$ mkdir lib</p>
			<p class="source-code">$ pip install mylib -t lib</p>
			<p>Then, we pass its location to the estimator:</p>
			<p class="source-code">sk = SKLearn(entry_point='myscript.py',</p>
			<p class="source-code">             dependencies=['lib/mylib'],</p>
			<p class="source-code">             . . .</p>
			<p>The last technique <a id="_idIndexMarker721"/>is to install libraries in the Dockerfile for the container, rebuild the image, and push it to Amazon ECR. If you also need the libraries at prediction time (say, for preprocessing), this is the best option.</p>
			<h3>Adding libraries for deployment</h3>
			<p>If you need <a id="_idIndexMarker722"/>specific libraries to be available at prediction time, you can use a <strong class="source-inline">requirements.txt</strong> file for libraries that can be installed with <strong class="source-inline">pip</strong>. </p>
			<p>For other libraries, the only option is to customize the framework container. You can pass its name to the estimator with the <strong class="source-inline">image_uri</strong> parameter:</p>
			<p class="source-code">sk = SKLearn(entry_point='myscript.py', image_uri= '123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-sklearn' </p>
			<p class="source-code">. . .</p>
			<p>We covered a lot of technical topics in this section. Now, let's look at the big picture.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor139"/>Putting it all together</h2>
			<p>The typical workflow when working with frameworks looks like this:</p>
			<ol>
				<li value="1">Implement script mode in your code; that is, read the necessary hyperparameters, input data, and output location.</li>
				<li>If required, add a <strong class="source-inline">model_fn()</strong> function to load the model.</li>
				<li>Test your training code locally, outside of any SageMaker container.</li>
				<li>Configure the appropriate estimator (<strong class="source-inline">XGBoost</strong>, <strong class="source-inline">TensorFlow</strong>, and so on).</li>
				<li>Train in local mode using the <a id="_idIndexMarker723"/>estimator, with either the built-in container or a container you've customized.</li>
				<li>Deploy in local mode and test your model.</li>
				<li>Switch to a managed instance type (say, <strong class="source-inline">ml.m5.large</strong>) for training and deployment.</li>
			</ol>
			<p>This logical progression requires little work at each step. It minimizes friction, the risk of mistakes, and frustration. It also optimizes instance time and cost—no need to wait and pay for managed instances if your code crashes immediately because of a silly bug.</p>
			<p>Now, let's put this knowledge to work. In the next section, we're going to run a simple scikit-learn example. The purpose is to make sure we understand the workflow we just discussed.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor140"/>Running your framework code on Amazon SageMaker</h1>
			<p>We will start <a id="_idIndexMarker724"/>from a vanilla scikit-learn program that trains and saves a linear regression model on the Boston Housing dataset, which we used in <a href="B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Machine Learning Models</em>:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">from sklearn.linear_model import LinearRegression</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">from sklearn.metrics import mean_squared_error, r2_score</p>
			<p class="source-code">import joblib</p>
			<p class="source-code">data = pd.read_csv('housing.csv')</p>
			<p class="source-code">labels = data[['medv']]</p>
			<p class="source-code">samples = data.drop(['medv'], axis=1)</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p>
			<p class="source-code">samples, labels, test_size=0.1, random_state=123)</p>
			<p class="source-code">regr = LinearRegression(normalize=True)</p>
			<p class="source-code">regr.fit(X_train, y_train)</p>
			<p class="source-code">y_pred = regr.predict(X_test)</p>
			<p class="source-code">print('Mean squared error: %.2f' </p>
			<p class="source-code">       % mean_squared_error(y_test, y_pred))</p>
			<p class="source-code">print('Coefficient of determination: %.2f' </p>
			<p class="source-code">       % r2_score(y_test, y_pred))</p>
			<p class="source-code">joblib.dump(regr, 'model.joblib')</p>
			<p>Let's<a id="_idIndexMarker725"/> update it so that it runs on SageMaker.</p>
			<h3>Implementing script mode</h3>
			<p>Now, we will use the<a id="_idIndexMarker726"/> framework to implement script mode, as follows:</p>
			<ol>
				<li value="1">First, read the hyperparameters as command-line arguments:<p class="source-code">import argparse</p><p class="source-code">if __name__ == '__main__':</p><p class="source-code">  parser = argparse.ArgumentParser()</p><p class="source-code">  parser.add_argument('--normalize', type=bool, </p><p class="source-code">                      default=False)</p><p class="source-code">  parser.add_argument('--test-size', type=float, </p><p class="source-code">                      default=0.1)</p><p class="source-code">  parser.add_argument('--random-state', type=int, </p><p class="source-code">                      default=123)</p><p class="source-code">  args, _ = parser.parse_known_args()</p><p class="source-code">  normalize = args.normalize</p><p class="source-code">  test_size = args.test_size</p><p class="source-code">  random_state = args.random_state</p><p class="source-code">  data = pd.read_csv('housing.csv')</p><p class="source-code">  labels = data[['medv']]</p><p class="source-code">  samples = data.drop(['medv'], axis=1)</p><p class="source-code">  X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">    samples, labels,test_size=test_size, </p><p class="source-code">    random_state=random_state)</p><p class="source-code">  . . . </p></li>
				<li>Read the input and output paths as command-line arguments. We could decide to remove the splitting code and pass<a id="_idIndexMarker727"/> two input channels instead. Let's stick to one channel, that is, <strong class="source-inline">training</strong>:<p class="source-code">import os</p><p class="source-code">if __name__ == '__main__':</p><p class="source-code">  . . .    </p><p class="source-code">  parser.add_argument('--model-dir', type=str, </p><p class="source-code">    default=os.environ['SM_MODEL_DIR'])</p><p class="source-code">  parser.add_argument('--training', type=str, </p><p class="source-code">    default=os.environ['SM_CHANNEL_TRAINING'])</p><p class="source-code">  . . .</p><p class="source-code">  model_dir = args.model_dir</p><p class="source-code">  training_dir = args.training</p><p class="source-code">  . . . </p><p class="source-code">  filename = os.path.join(training_dir, 'housing.csv')</p><p class="source-code">  data = pd.read_csv(filename)</p><p class="source-code">  . . .</p><p class="source-code">  model = os.path.join(model_dir, 'model.joblib')</p><p class="source-code">  dump(regr, model)</p></li>
				<li>As we're using scikit-learn, we need to add <strong class="source-inline">model_fn()</strong> to load the model at deployment <a id="_idIndexMarker728"/>time:<p class="source-code">def model_fn(model_dir):</p><p class="source-code">  model = joblib.load(os.path.join(model_dir, </p><p class="source-code">                                   'model.joblib'))</p><p class="source-code">  return model</p></li>
			</ol>
			<p>With that, we're done. Time to test!</p>
			<h3>Testing locally</h3>
			<p>First, we test our <a id="_idIndexMarker729"/>script on our local machine in a Python 3 environment, outside of any SageMaker container. We just need to make sure that we have <strong class="source-inline">pandas</strong> and scikit-learn installed. </p>
			<p>We set the environment variables to empty values as we will pass the paths on the command line:</p>
			<p class="source-code">$ source activate python3</p>
			<p class="source-code">$ export SM_CHANNEL_TRAINING=</p>
			<p class="source-code">$ export SM_MODEL_DIR=</p>
			<p class="source-code">$ python sklearn-boston-housing.py --normalize True –test-ration 0.1 --training . --model-dir .</p>
			<p class="source-code">Mean squared error: 41.82</p>
			<p class="source-code">Coefficient of determination: 0.63</p>
			<p>Nice. Our code runs fine with command-line arguments. We can use this for local development<a id="_idIndexMarker730"/> and debugging, until we're ready to move it to SageMaker local mode.</p>
			<h3>Using local mode</h3>
			<p>We'll get started using the following steps:</p>
			<ol>
				<li value="1">Still on our local machine, we configure an <strong class="source-inline">SKLearn</strong> estimator in<a id="_idIndexMarker731"/> local mode, setting the role according to the setup we're using. Use local paths only:<p class="source-code">role = 'arn:aws:iam::0123456789012:role/Sagemaker-fullaccess'</p><p class="source-code">sk = SKLearn(entry_point='sklearn-boston-housing.py',</p><p class="source-code">  role=role,</p><p class="source-code">  framework_version='0.23-1',</p><p class="source-code">  instance_count=1,</p><p class="source-code">  instance_type='local',</p><p class="source-code">  output_path=output_path,</p><p class="source-code">  hyperparameters={'normalize': True, </p><p class="source-code">                   'test-size': 0.1})</p><p class="source-code">sk.fit({'training':training_path})</p></li>
				<li>As expected, we can see <a id="_idIndexMarker732"/>how our code is invoked in the training log. Of course, we get the same outcome:<p class="source-code"><strong class="bold">/miniconda3/bin/python -m sklearn-boston-housing --normalize True --test-size 0.1</strong></p><p class="source-code"><strong class="bold">. . . </strong></p><p class="source-code"><strong class="bold">Mean squared error: 41.82</strong></p><p class="source-code"><strong class="bold">Coefficient of determination: 0.63</strong></p></li>
				<li>We deploy locally and send some CSV samples for prediction:<p class="source-code">sk_predictor = sk.deploy(initial_instance_count=1, </p><p class="source-code">                         instance_type='local')</p><p class="source-code">data = pd.read_csv('housing.csv')</p><p class="source-code">payload = data[:10].drop(['medv'], axis=1) </p><p class="source-code">payload = payload.to_csv(header=False, index=False)</p><p class="source-code">sk_predictor.serializer = </p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">sk_predictor.deserializer =</p><p class="source-code">    sagemaker.deserializers.CSVDeserializer()</p><p class="source-code">response = sk_predictor.predict(payload)</p><p class="source-code">print(response)</p><p>By printing the response, we will see the predicted values:</p><p class="source-code"><strong class="bold">[['29.801388899699845'], ['24.990809475886074'], ['30.7379654455552'], ['28.786967125316544'], ['28.1421501991961'], ['25.301714533101716'], ['22.717977231840184'], ['19.302415613883348'], ['11.369520911229536'], ['18.785593532977657']]</strong></p><p>With local mode, we can quickly iterate on our model. We're only limited by the compute and <a id="_idIndexMarker733"/>storage capabilities of the local machine. When that happens, we can easily move to managed infrastructure.</p></li>
			</ol>
			<h3>Using managed infrastructure</h3>
			<p>When it's time to train<a id="_idIndexMarker734"/> at scale and deploy in production, all we have to do is make sure the input data is in S3 and replace the "local" instance type with an actual instance type:</p>
			<p class="source-code">sess = sagemaker.Session()</p>
			<p class="source-code">bucket = sess.default_bucket()                     </p>
			<p class="source-code">prefix = 'sklearn-boston-housing'</p>
			<p class="source-code">training_path = sess.upload_data(path='housing.csv', </p>
			<p class="source-code">           key_prefix=prefix + "/training")</p>
			<p class="source-code">output_path = 's3://{}/{}/output/'.format(bucket,prefix)</p>
			<p class="source-code">sk = SKLearn(. . ., instance_type='ml.m5.large')</p>
			<p class="source-code">sk.fit({'training':training_path})</p>
			<p class="source-code">. . .</p>
			<p class="source-code">sk_predictor = sk.deploy(initial_instance_count=1, </p>
			<p class="source-code">                         instance_type='ml.t2.medium')</p>
			<p>Since we're using the same container, we can be confident that training and deployment will work as expected. Again, I strongly recommend that you follow this logical progression: local work first, then SageMaker local mode, and finally, SageMaker managed<a id="_idIndexMarker735"/> infrastructure. It will help you focus on what needs to be done and when.</p>
			<p>For the remainder of this chapter, we're going to run additional examples.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor141"/>Using the built-in frameworks</h1>
			<p>We've covered XGBoost and<a id="_idIndexMarker736"/> scikit-learn already. Now, it's time to see how we can use deep learning frameworks. Let's start with TensorFlow and Keras.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/>Working with TensorFlow and Keras</h2>
			<p>In this example, we're<a id="_idIndexMarker737"/> going to use TensorFlow 2.4.1 to train a simple convolutional neural network on the<a id="_idIndexMarker738"/> Fashion-MNIST dataset (<a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a>). </p>
			<p>Our code is split into two source files: one for the entry point script (<strong class="source-inline">fmnist.py</strong>) and one for the model (<strong class="source-inline">model.py</strong>, based on Keras layers). For the sake of brevity, I will only discuss the SageMaker steps. You can find the full code in the GitHub repository for this book:</p>
			<ol>
				<li value="1"><strong class="source-inline">fmnist.py</strong> starts by reading hyperparameters from the command line:<p class="source-code">import tensorflow as tf</p><p class="source-code">import numpy as np</p><p class="source-code">import argparse, os</p><p class="source-code">from model import FMNISTModel</p><p class="source-code">parser = argparse.ArgumentParser()</p><p class="source-code">parser.add_argument('--epochs', type=int, default=10)</p><p class="source-code">parser.add_argument('--learning-rate', type=float,  </p><p class="source-code">                    default=0.01)</p><p class="source-code">parser.add_argument('--batch-size', type=int, </p><p class="source-code">                    default=128)</p></li>
				<li>Next, we read the environment variables, that is, the input paths for the training set and the validation set, the output path for the model, and the number of GPUs available on the instance. It's the first time we're using the latter. It comes in handy to adjust the batch size for multi-GPU training as it's common practice to multiply the <a id="_idIndexMarker739"/>initial batch's size by the number of<a id="_idIndexMarker740"/> GPUs:<p class="source-code">parser.add_argument('--training', type=str, </p><p class="source-code">    default=os.environ['SM_CHANNEL_TRAINING'])</p><p class="source-code">parser.add_argument('--validation', type=str,</p><p class="source-code">    default=os.environ['SM_CHANNEL_VALIDATION'])</p><p class="source-code">parser.add_argument('--model-dir', type=str, </p><p class="source-code">    default=os.environ['SM_MODEL_DIR'])</p><p class="source-code">parser.add_argument('--gpu-count', type=int, </p><p class="source-code">    default=os.environ['SM_NUM_GPUS'])</p></li>
				<li>Store the arguments in local variables. Then, load the dataset. Each channel provides us with a compressed <strong class="source-inline">numpy</strong> array for storing images and labels:<p class="source-code">x_train = np.load(os.path.join(training_dir, </p><p class="source-code">          'training.npz'))['image']</p><p class="source-code">y_train = np.load(os.path.join(training_dir, </p><p class="source-code">          'training.npz'))['label']</p><p class="source-code">x_val = np.load(os.path.join(validation_dir, </p><p class="source-code">        'validation.npz'))['image']</p><p class="source-code">y_val = np.load(os.path.join(validation_dir, </p><p class="source-code">        'validation.npz'))['label']</p></li>
				<li>Then, prepare the data for training by reshaping the image tensors, normalizing the pixel values, one-hot encoding the image labels, and creating the <strong class="source-inline">tf.data.Dataset</strong> objects that will feed data to the model.</li>
				<li>Create the model, compile it, and fit it.</li>
				<li>Once<a id="_idIndexMarker741"/> training is complete, save the model in TensorFlow<a id="_idIndexMarker742"/> Serving format at the appropriate output location. This step is important as this is the model server that SageMaker uses for TensorFlow models:<p class="source-code">model.save(os.path.join(model_dir, '1'))</p></li>
			</ol>
			<p>We train and deploy the model using the usual workflow:</p>
			<ol>
				<li value="1">In a notebook powered by a TensorFlow 2 kernel, we download the dataset and upload it to S3:<p class="source-code">import os</p><p class="source-code">import numpy as np</p><p class="source-code">import keras</p><p class="source-code">from keras.datasets import fashion_mnist</p><p class="source-code">(x_train, y_train), (x_val, y_val) =  </p><p class="source-code">    fashion_mnist.load_data()</p><p class="source-code">os.makedirs("./data", exist_ok = True)</p><p class="source-code">np.savez('./data/training', image=x_train,       </p><p class="source-code">         label=y_train)</p><p class="source-code">np.savez('./data/validation', image=x_val, </p><p class="source-code">         label=y_val)</p><p class="source-code">prefix = 'tf2-fashion-mnist'</p><p class="source-code">training_input_path = sess.upload_data(</p><p class="source-code">    'data/training.npz', </p><p class="source-code">    key_prefix=prefix+'/training')</p><p class="source-code">validation_input_path = sess.upload_data(</p><p class="source-code">    'data/validation.npz',   </p><p class="source-code">    key_prefix=prefix+'/validation')</p></li>
				<li>We configure<a id="_idIndexMarker743"/> the <strong class="source-inline">TensorFlow</strong> estimator. We also set <a id="_idIndexMarker744"/>the <strong class="source-inline">source_dir</strong> parameter so that our model's file is also deployed in the container:<p class="source-code">from sagemaker.tensorflow import TensorFlow</p><p class="source-code">tf_estimator = TensorFlow(entry_point='fmnist.py',</p><p class="source-code">    source_dir='.',</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.p3.2xlarge', </p><p class="source-code">    framework_version='2.4.1',</p><p class="source-code">    py_version='py37',</p><p class="source-code">    hyperparameters={'epochs': 10})</p></li>
				<li>Train and deploy<a id="_idIndexMarker745"/> as usual. We will go straight for managed<a id="_idIndexMarker746"/> infrastructure, but the same code will work fine on your local machine in local mode:<p class="source-code">from time import strftime,gmtime</p><p class="source-code">tf_estimator.fit(</p><p class="source-code">    {'training': training_input_path, </p><p class="source-code">     'validation': validation_input_path})</p><p class="source-code">tf_endpoint_name = 'tf2-fmnist-'+strftime("%Y-%m-%d-%H-%M-%S", gmtime())</p><p class="source-code">tf_predictor = tf_estimator.deploy(</p><p class="source-code">               initial_instance_count=1,</p><p class="source-code">               instance_type='ml.m5.large',</p><p class="source-code">               endpoint_name=tf_endpoint_name)</p></li>
				<li>The validation accuracy should be 91-92%. By loading and displaying a few sample images from the validation dataset, we can predict their labels. The <strong class="source-inline">numpy</strong> payload is <a id="_idIndexMarker747"/>automatically serialized to JSON, which is the default<a id="_idIndexMarker748"/> format for prediction data:<p class="source-code">response = tf_predictor.predict(payload)</p><p class="source-code">prediction = np.array(reponse['predictions'])</p><p class="source-code">predicted_label = prediction.argmax(axis=1)</p><p class="source-code">print('Predicted labels are: </p><p class="source-code">    {}'.format(predicted_label))</p><p>The output should look as follows:</p><div id="_idContainer121" class="IMG---Figure"><img src="Images/B17705_07_1.jpg" alt="Figure 7.1 – Viewing predicted classes&#13;&#10;" width="506" height="120"/></div><p class="figure-caption">Figure 7.1 – Viewing predicted classes</p></li>
				<li>When we're done, we delete the endpoint:<p class="source-code">tf_predictor.delete_endpoint()</p></li>
			</ol>
			<p>As you can see, the combination of script mode and built-in containers makes it easy to run TensorFlow<a id="_idIndexMarker749"/> on SageMaker. Once you get into the routine, you'll be <a id="_idIndexMarker750"/>surprised at how fast you can move your models from your laptop to AWS.</p>
			<p>Now, let's take a look at PyTorch.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/>Working with PyTorch</h2>
			<p>PyTorch is extremely<a id="_idIndexMarker751"/> popular for computer vision, NLP, and more.</p>
			<p>In this example, we're going <a id="_idIndexMarker752"/>to train a <strong class="bold">Graph Neural Network</strong> (<strong class="bold">GNN</strong>). This category of networks works particularly well on graph-structured data, such as social networks, life sciences, and more. In fact, our PyTorch code will use the <strong class="bold">Deep Graph Library</strong> (<strong class="bold">DGL</strong>), an open <a id="_idIndexMarker753"/>source library that makes it easier to build and train GNNs with TensorFlow, PyTorch, and Apache MXNet (<a href="https://www.dgl.ai/">https://www.dgl.ai/</a>). DGL is already installed in these containers, so let's get to work directly.</p>
			<p>We're going to work with <a id="_idIndexMarker754"/>the Zachary Karate Club dataset (<a href="http://konect.cc/networks/ucidata-zachary/">http://konect.cc/networks/ucidata-zachary/</a>). The following is the graph for this:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="Images/B17705_07_2.jpg" alt="Figure 7.2 – The Zachary Karate Club dataset&#13;&#10;" width="550" height="282"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – The Zachary Karate Club dataset</p>
			<p>Nodes 0 and 33 are teachers, while the other nodes are students. Edges represent ties between these people. As the story goes, the two teachers had an argument and the club needs to be split in two. </p>
			<p>The purpose of the training job<a id="_idIndexMarker755"/> is to find the "best" split. This can be defined as a semi-supervision classification task. The first teacher (node 0) is assigned class 0, while the second teacher (node 33) is assigned class 1. All the other nodes are unlabeled, and their classes will be computed <a id="_idIndexMarker756"/>by a <strong class="bold">graph convolutional network</strong>. At the end of the last epoch, we'll retrieve the node classes and split the club accordingly.</p>
			<p>The dataset is stored as a pickled Python list containing edges. Here are the first few edges:</p>
			<p class="source-code"><strong class="bold">[('0', '8'), ('1', '17'), ('24', '31'), . . .</strong></p>
			<p>The SageMaker code is as simple as it gets. We upload the dataset to S3, create a <strong class="source-inline">PyTorch</strong> estimator, and train it: </p>
			<p class="source-code">import sagemaker</p>
			<p class="source-code">from sagemaker.pytorch import PyTorch</p>
			<p class="source-code">sess = sagemaker.Session()</p>
			<p class="source-code">prefix = 'dgl-karate-club'</p>
			<p class="source-code">training_input_path = sess.upload_data('edge_list.pickle', </p>
			<p class="source-code">key_prefix=prefix+'/training')</p>
			<p class="source-code">estimator = PyTorch(role=sagemaker.get_execution_role(),</p>
			<p class="source-code">    entry_point='karate_club_sagemaker.py',</p>
			<p class="source-code">    hyperparameters={'node_count': 34, 'epochs': 30},</p>
			<p class="source-code">    framework_version='1.5.0',</p>
			<p class="source-code">    py_version='py3',</p>
			<p class="source-code">    instance_count=1,</p>
			<p class="source-code">    instance_type='ml.m5.large')</p>
			<p class="source-code">estimator.fit({'training': training_input_path})</p>
			<p>This hardly needs any explaining at all, does it? </p>
			<p>Let's take a look at the<a id="_idIndexMarker757"/> abbreviated training script, where we're using script mode once again. The full version is available in the GitHub repository for this book:</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    parser = argparse.ArgumentParser()</p>
			<p class="source-code">    parser.add_argument('--epochs', type=int, default=30)</p>
			<p class="source-code">    parser.add_argument('--node_count', type=int)</p>
			<p class="source-code">    args, _    = parser.parse_known_args()</p>
			<p class="source-code">    epochs     = args.epochs</p>
			<p class="source-code">    node_count = args.node_count</p>
			<p class="source-code">    training_dir = os.environ['SM_CHANNEL_TRAINING']</p>
			<p class="source-code">    model_dir    = os.environ['SM_MODEL_DIR']</p>
			<p class="source-code">    with open(os.path.join(training_dir, 'edge_list.pickle'), </p>
			<p class="source-code">    'rb') as f:</p>
			<p class="source-code">        edge_list = pickle.load(f)</p>
			<p class="source-code">    # Build the graph and the model</p>
			<p class="source-code">    . . .</p>
			<p class="source-code">    # Train the model</p>
			<p class="source-code">    . . .</p>
			<p class="source-code">    # Print predicted classes</p>
			<p class="source-code">    last_epoch = all_preds[epochs-1].detach().numpy()</p>
			<p class="source-code">    predicted_class = np.argmax(last_epoch, axis=-1)</p>
			<p class="source-code">    print(predicted_class)</p>
			<p class="source-code">    # Save the model</p>
			<p class="source-code">    torch.save(net.state_dict(), os.path.join(model_dir, </p>
			<p class="source-code">    'karate_club.pt'))</p>
			<p>The following classes are <a id="_idIndexMarker758"/>predicted. Nodes 0 and 1 are class 0, node 2 is class 1, and so on:</p>
			<p class="source-code"><strong class="bold">[0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1]</strong></p>
			<p>By plotting them, we can see that the club has been cleanly split:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="Images/B17705_07_3.jpg" alt="Figure 7.3 – Viewing predicted classes&#13;&#10;" width="507" height="214"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Viewing predicted classes</p>
			<p>Once again, the SageMaker code<a id="_idIndexMarker759"/> doesn't stand in your way. The workflow and APIs are consistent from one framework to the next, and you can focus on the machine learning problem itself. Now, let's do another example with Hugging Face, where we'll also see how to deploy a PyTorch model with the built-in PyTorch <a id="_idIndexMarker760"/>container.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/>Working with Hugging Face</h2>
			<p><strong class="bold">Hugging Face</strong> (<a href="https://huggingface.co">https://huggingface.co</a>) has <a id="_idIndexMarker761"/>quickly become <a id="_idIndexMarker762"/>the most popular collection of open source models for NLP. At the time of writing, they host almost 10,000 state-of-the-art models (<a href="https://huggingface.co/models">https://huggingface.co/models</a>), pretrained on datasets (<a href="https://huggingface.co/datasets">https://huggingface.co/datasets</a>) in over 250 languages (<a href="https://huggingface.co/languages">https://huggingface.co/languages</a>).</p>
			<p>To make it easy to quickly build high-quality NLP applications, Hugging Face actively developed <a id="_idIndexMarker763"/>three open source libraries:</p>
			<ul>
				<li><strong class="bold">Transformers</strong>: Train, fine-tune, and predict with Hugging Face models (<a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a>).</li>
				<li><strong class="bold">Datasets</strong>: Download and process Hugging Face datasets (<a href="https://github.com/huggingface/datasets">https://github.com/huggingface/datasets</a>).</li>
				<li><strong class="bold">Tokenizers</strong>: Tokenize text for training and prediction with Hugging Face models (<a href="https://github.com/huggingface/tokenizers">https://github.com/huggingface/tokenizers</a>).<p class="callout-heading">Hugging Face tutorial</p><p class="callout">If you are completely new to Hugging Face, please run through their tutorial first at <a href="https://huggingface.co/transformers/quicktour.html">https://huggingface.co/transformers/quicktour.html</a>.</p></li>
			</ul>
			<p>SageMaker added support for Hugging Face in March 2021, on both TensorFlow and PyTorch. As you would expect, you can use a <strong class="source-inline">HuggingFace</strong> estimator and built-in containers. Let's run an example where we build a sentiment analysis model for English language customer reviews. For<a id="_idIndexMarker764"/> this purpose, we'll fine-tune a <strong class="bold">DistilBERT</strong> model (<a href="https://arxiv.org/abs/1910.01108">https://arxiv.org/abs/1910.01108</a>) implemented with PyTorch and pretrained on two large English language datasets (Wikipedia and the BookCorpus dataset).</p>
			<h3>Preparing the dataset</h3>
			<p>In this<a id="_idIndexMarker765"/> example, we'll use a Hugging Face dataset named <strong class="source-inline">generated_reviews_enth</strong> (<a href="https://huggingface.co/datasets/generated_reviews_enth">https://huggingface.co/datasets/generated_reviews_enth</a>). It includes an English review, its Thai translation, a flag indicating whether the translation is correct or not, and a star rating:</p>
			<p class="source-code"><strong class="bold">{'correct': 0, 'review_star': 4, 'translation': {'en': "I had a hard time finding a case for my new LG Lucid 2 but finally found this one on amazon. The colors are really pretty and it works just as well as, if not better than the otterbox. Hopefully there will be more available by next Xmas season. Overall, very cute case. I love cheetah's. :)", 'th': '</strong><strong class="bold">ฉันมีปัญหาในการหาเคสสำหรับ</strong><strong class="bold"> LG Lucid 2 </strong><strong class="bold">ใหม่ของฉัน</strong><strong class="bold"> </strong><strong class="bold">แต่ในที่สุดก็พบเคสนี้ใน</strong><strong class="bold"> Amazon </strong><strong class="bold">สีสวยมากและใช้งานได้ดีเช่นเดียวกับถ้าไม่ดีกว่านาก</strong><strong class="bold"> </strong><strong class="bold">หวังว่าจะมีให้มากขึ้นในช่วงเทศกาลคริสต์มาสหน้า</strong><strong class="bold"> </strong><strong class="bold">โดยรวมแล้วน่ารักมาก</strong><strong class="bold"> </strong><strong class="bold">ๆ</strong><strong class="bold"> </strong><strong class="bold">ฉันรักเสือชีตาห์</strong><strong class="bold"> :)'}}</strong></p>
			<p>This is the format that the DistilBERT tokenizer expects: a <strong class="source-inline">labels</strong> variable (<strong class="source-inline">0</strong> for negative sentiment, <strong class="source-inline">1</strong> for positive) and a <strong class="source-inline">text</strong> variable with the English language review:</p>
			<p class="source-code"><strong class="bold">{'labels': 1,</strong></p>
			<p class="source-code"><strong class="bold"> 'text': "I had a hard time finding a case for my new LG Lucid 2 but finally found this one on amazon. The colors are really pretty and it works just as well as, if not better than the otterbox. Hopefully there will be more available by next Xmas season. Overall, very cute case. I love cheetah's. :)"}</strong></p>
			<p>Let's get to work! I'll show you the individual steps, and you'll also find a <strong class="bold">SageMaker Processing</strong> version in<a id="_idIndexMarker766"/> the GitHub repository for this book:</p>
			<ol>
				<li value="1">We first install the <strong class="source-inline">transformers</strong> and <strong class="source-inline">datasets</strong> libraries:<p class="source-code">!pip -q install "transformers&gt;=4.4.2" "datasets[s3]==1.5.0" --upgrade</p></li>
				<li>We download the dataset, which is already split for training (141,369 instances) and validation (15,708 instances). All data is in JSON format:<p class="source-code">from datasets import load_dataset</p><p class="source-code">train_dataset, valid_dataset = load_dataset('generated_reviews_enth', </p><p class="source-code">             split=['train', 'validation'])</p></li>
				<li>In each review, we create a new variable named <strong class="source-inline">labels</strong>. We set it to <strong class="source-inline">1</strong> when <strong class="source-inline">review_star</strong> is equal<a id="_idIndexMarker767"/> to or higher than 4, and to <strong class="source-inline">0</strong> otherwise:<p class="source-code">def map_stars_to_sentiment(row):</p><p class="source-code">    return {</p><p class="source-code">        'labels': 1 if row['review_star'] &gt;= 4 else 0</p><p class="source-code">    }</p><p class="source-code">train_dataset = </p><p class="source-code">    train_dataset.map(map_stars_to_sentiment)</p><p class="source-code">valid_dataset = </p><p class="source-code">    valid_dataset.map(map_stars_to_sentiment)</p></li>
				<li>The reviews are nested JSON documents, making it difficult to remove variables we don't need. Let's flatten both datasets:<p class="source-code">train_dataset = train_dataset.flatten()</p><p class="source-code">valid_dataset = valid_dataset.flatten()</p></li>
				<li>We can now easily drop unwanted variables. We also rename the <strong class="source-inline">translation.en</strong> variable to <strong class="source-inline">text</strong>:<p class="source-code">train_dataset = train_dataset.remove_columns(</p><p class="source-code">    ['correct', 'translation.th', 'review_star'])</p><p class="source-code">valid_dataset = valid_dataset.remove_columns(</p><p class="source-code">    ['correct', 'translation.th', 'review_star'])</p><p class="source-code">train_dataset = train_dataset.rename_column(</p><p class="source-code">    'translation.en', 'text')</p><p class="source-code">valid_dataset = valid_dataset.rename_column(</p><p class="source-code">    'translation.en', 'text')</p></li>
			</ol>
			<p>The training and validation instances now have the format expected by the DistilBERT tokenizer. We already covered tokenization in <a href="B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 6</em></a>, <em class="italic">Training Natural Language Processing Models</em>. A significant difference is that we use a tokenizer that was pretrained on the same English language corpus as the model:</p>
			<ol>
				<li value="1">We <a id="_idIndexMarker768"/>download the tokenizer for our pretrained model:<p class="source-code">from transformers import AutoTokenizer</p><p class="source-code">tokenizer = AutoTokenizer.from_pretrained(</p><p class="source-code">    'distilbert-base-uncased')</p><p class="source-code">def tokenize(batch):</p><p class="source-code">    return tokenizer(batch['text'], </p><p class="source-code">    padding='max_length', truncation=True)</p></li>
				<li>We tokenize both datasets. Words and punctuation are replaced with appropriate tokens. If needed, each sequence is padded or truncated to fit the input layer of the model (512 tokens):<p class="source-code">train_dataset = train_dataset.map(tokenize, </p><p class="source-code">    batched=True, batch_size=len(train_dataset))</p><p class="source-code">valid_dataset = valid_dataset.map(tokenize, </p><p class="source-code">    batched=True, batch_size=len(valid_dataset))</p></li>
				<li>We drop the <strong class="source-inline">text</strong> variable, as it's not needed anymore:<p class="source-code">train_dataset = train_dataset.remove_columns(['text'])</p><p class="source-code">valid_dataset = valid_dataset.remove_columns(['text'])</p></li>
				<li>Printing out an instance, we see the attention mask (all ones, meaning no token is masked in the input sequence), the inputs IDs (the sequence of tokens), and the label:<p class="source-code"><strong class="bold">'{"attention_mask": [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,&lt;zero padding&gt;], "input_ids": [101,1045, 2018,1037,2524,2051,4531,1037,2553,2005,2026,2047,1048,2290,12776,3593,1016,2021,2633,2179,2023,2028,2006,9733,1012,1996,6087,2024,2428,3492,1998,2009,2573,2074,2004,2092,2004,1010,2065,2025,2488,2084,1996,22279,8758,1012,11504,2045,2097,2022,2062,2800,2011,2279,1060,9335,2161,1012,3452,1010,2200,10140,2553,1012,1045,2293,18178,12928,2232,1005,1055,1012,1024,1007,102,&lt;zero padding&gt;], "labels": 1}'</strong></p></li>
			</ol>
			<p>Data preparation<a id="_idIndexMarker769"/> is complete. Let's move on to training the model.</p>
			<h3>Fine-tuning a Hugging Face model</h3>
			<p>We're not going to<a id="_idIndexMarker770"/> train from scratch: it would talk far too long, and we probably don't have enough data anyway. Instead, we're going to fine-tune the model. Starting from a model trained on a very large text corpus, we will just train it for one additional epoch on our own data, so that it picks up the particular patterns present in our data:</p>
			<ol>
				<li value="1">We start by uploading both datasets to S3. The <strong class="source-inline">datasets</strong> library provides a convenient API to do this:<p class="source-code">import sagemaker</p><p class="source-code">from datasets.filesystems import S3FileSystem</p><p class="source-code">bucket = sagemaker.Session().default_bucket()</p><p class="source-code">s3_prefix = 'hugging-face/sentiment-analysis'</p><p class="source-code">train_input_path = </p><p class="source-code">    f's3://{bucket}/{s3_prefix}/training'</p><p class="source-code">valid_input_path =  </p><p class="source-code">    f's3://{bucket}/{s3_prefix}/validation'</p><p class="source-code">s3 = S3FileSystem()</p><p class="source-code">train_dataset.save_to_disk(train_input_path, fs=s3)</p><p class="source-code">valid_dataset.save_to_disk(valid_input_path, fs=s3)</p></li>
				<li>We define hyperparameters and configure a <strong class="source-inline">HuggingFace</strong> estimator. Note that we'll<a id="_idIndexMarker771"/> fine-tune the model for just one epoch:<p class="source-code">hyperparameters={</p><p class="source-code">    'epochs': 1,</p><p class="source-code">    'train_batch_size': 32,</p><p class="source-code">    'model_name':'distilbert-base-uncased'</p><p class="source-code">}</p><p class="source-code">from sagemaker.huggingface import HuggingFace</p><p class="source-code">huggingface_estimator = HuggingFace(</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    entry_point='train.py',</p><p class="source-code">    hyperparameters=hyperparameters,</p><p class="source-code">    transformers_version='4.4.2',</p><p class="source-code">    pytorch_version='1.6.0',</p><p class="source-code">    py_version='py36',</p><p class="source-code">    instance_type='ml.p3.2xlarge',</p><p class="source-code">    instance_count=1</p><p class="source-code">)</p><p>For the sake of brevity, I won't discuss the training script (<strong class="source-inline">train.py</strong>), which is available in the GitHub repository for this book. There's nothing particular about it: we use the <strong class="source-inline">Trainer</strong> Hugging Face API, as well as script mode to interface it with SageMaker. As we only train for a single epoch, checkpointing is disabled (<strong class="source-inline">save_strategy='no'</strong>). This helps cuts down on training time (not saving checkpoints) and deployment time (the model artifact is smaller).</p></li>
				<li>It's also worth noting that you can generate boilerplate code for your estimator on the <a id="_idIndexMarker772"/>Hugging Face website. As shown in the following screenshot, you can click on <strong class="bold">Amazon SageMaker</strong>, pick a task type, and copy and paste the generated code:<div id="_idContainer124" class="IMG---Figure"><img src="Images/B17705_07_4.jpg" alt="Figure 7.4 – Viewing our model on the Hugging Face website&#13;&#10;" width="1650" height="307"/></div><p class="figure-caption">Figure 7.4 – Viewing our model on the Hugging Face website</p></li>
				<li>We launch the training job as usual, and it lasts about 42 minutes:<p class="source-code">huggingface_estimator.fit(</p><p class="source-code">    {'train': train_input_path, </p><p class="source-code">     'valid': valid_input_path})</p></li>
			</ol>
			<p>Just like with other frameworks, we could call the <strong class="source-inline">deploy()</strong> API in order to deploy our model to a SageMaker endpoint. You can find an example at <a href="https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/">https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/</a>.</p>
			<p>Instead, let's see how we can deploy our model with the built-in PyTorch container and <strong class="bold">TorchServe</strong>. In fact, this deployment example can generalize to any PyTorch model that you'd like to serve with TorchServe.</p>
			<p>I find this superb blog post by my colleague Todd Escalona extremely helpful in understanding how <a id="_idIndexMarker773"/>to serve PyTorch models with TorchServe: <a href="https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/">https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/</a>.</p>
			<h3>Deploying a Hugging Face model </h3>
			<p>The only<a id="_idIndexMarker774"/> difference compared to previous examples is that we have to use the model artifact in S3 to create a <strong class="source-inline">PyTorchModel</strong> object, and to build a <strong class="source-inline">Predictor</strong> model that we can use <strong class="source-inline">deploy()</strong> and <strong class="source-inline">predict()</strong> on:</p>
			<ol>
				<li value="1">Starting from the model artifact, we define a <strong class="source-inline">Predictor</strong> object, and we create a <strong class="source-inline">PyTorchModel</strong> with it:<p class="source-code">from sagemaker.pytorch import PyTorchModel</p><p class="source-code">from sagemaker.serializers import JSONSerializer</p><p class="source-code">from sagemaker.deserializers import JSONDeserializer</p><p class="source-code">model = PyTorchModel(</p><p class="source-code">    model_data=huggingface_estimator.model_data,</p><p class="source-code">    role=sagemaker.get_execution_role(), </p><p class="source-code">    entry_point='torchserve-predictor.py',</p><p class="source-code">    framework_version='1.6.0',</p><p class="source-code">    py_version='py36')</p></li>
				<li>Zooming in on the inference script (<strong class="source-inline">torchserve-predictor.py</strong>), we write a model loading function to account for Hugging Face peculiarities that the PyTorch container can't handle by default:<p class="source-code">def model_fn(model_dir):</p><p class="source-code">  config_path = '{}/config.json'.format(model_dir)</p><p class="source-code">  model_path ='{}/pytorch_model.bin'.format(model_dir)</p><p class="source-code">  config = AutoConfig.from_pretrained(config_path)</p><p class="source-code">  model = DistilBertForSequenceClassification</p><p class="source-code">          .from_pretrained(model_path, config=config)</p><p class="source-code">  return model</p></li>
				<li>We also add <a id="_idIndexMarker775"/>a prediction function that returns a text label:<p class="source-code">tokenizer = AutoTokenizer.from_pretrained(</p><p class="source-code">            'distilbert-base-uncased')</p><p class="source-code">CLASS_NAMES = ['negative', 'positive']</p><p class="source-code">def predict_fn(input_data, model):</p><p class="source-code">    inputs = tokenizer(input_data['text'],  </p><p class="source-code">                       return_tensors='pt')</p><p class="source-code">    outputs = model(**inputs)</p><p class="source-code">    logits = outputs.logits</p><p class="source-code">    _, prediction = torch.max(logits, dim=1)</p><p class="source-code">    return CLASS_NAMES[prediction]</p></li>
				<li>The inference script also includes basic <strong class="source-inline">input_fn()</strong> and <strong class="source-inline">output_fn()</strong> functions to check that data is in JSON format. You'll find the code in the GitHub repository for the book.</li>
				<li>Coming back to our notebook, we deploy the model as usual:<p class="source-code">predictor = model.deploy(</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.m5.xlarge')</p></li>
				<li>Once the endpoint is up, we predict a text sample and print the result:<p class="source-code">predictor.serializer = JSONSerializer()</p><p class="source-code">predictor.deserializer = JSONDeserializer()</p><p class="source-code">sample = {'text':'This camera is really amazing!}</p><p class="source-code">prediction = predictor.predict(test_data)</p><p class="source-code">print(prediction)</p><p class="source-code"><strong class="bold">['positive']</strong></p></li>
				<li>Finally, we delete the endpoint:<p class="source-code">predictor.delete_endpoint()</p></li>
			</ol>
			<p>As you can see, it's really easy to work with Hugging Face models. It's also a cost-effective way to build high-quality NLP models, as we typically fine-tune them for a very small number of<a id="_idIndexMarker776"/> epochs.</p>
			<p>To close this chapter, let's look at how SageMaker and Apache Spark can work together.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/>Working with Apache Spark</h2>
			<p>In addition to the <a id="_idIndexMarker777"/>Python SageMaker SDK that we've been using so far, SageMaker also includes an SDK for Spark (<a href="https://github.com/aws/sagemaker-spark">https://github.com/aws/sagemaker-spark</a>). This lets you run SageMaker jobs directly from a PySpark or Scala application running on a Spark cluster. </p>
			<h3>Combining Spark and SageMaker</h3>
			<p>First, you <a id="_idIndexMarker778"/>can decouple the <strong class="bold">Extract-Transform-Load</strong> (<strong class="bold">ETL</strong>) step<a id="_idIndexMarker779"/> and the machine learning step. Each usually has different infrastructure requirements (instance type, instance count, storage) that need to be the right size both technically and financially. Setting up your Spark cluster just right for ETL and using on-demand infrastructure in SageMaker for training and prediction is a powerful combination.</p>
			<p>Second, although Spark's MLlib is an amazing library, you may need something else, such as custom algorithms in different languages or deep learning.</p>
			<p>Finally, deploying models for prediction on Spark clusters may not be the best option. SageMaker endpoints should be considered instead, especially since they <a id="_idIndexMarker780"/>support the <strong class="bold">MLeap</strong> format (<a href="https://combust.github.io/mleap-docs/">https://combust.github.io/mleap-docs/</a>). </p>
			<p>In the following example, we'll combine SageMaker and Spark to build a spam detection model. Data will be hosted in S3, with one text file for spam messages and one for non-spam ("ham") messages. We'll use Spark running on an Amazon EMR cluster to preprocess it. Then, we'll train and <a id="_idIndexMarker781"/>deploy a model with the XGBoost algorithm that's available in SageMaker. Finally, we'll predict data with it on our Spark cluster. For the sake of language diversity, we'll code with Scala this time.</p>
			<p>First of all, we need to build a Spark cluster.</p>
			<h3>Creating a Spark cluster</h3>
			<p>We will create the cluster as follows:</p>
			<ol>
				<li value="1">Starting<a id="_idIndexMarker782"/> from the <strong class="bold">Amazon EMR</strong> console at <a href="https://console.aws.amazon.com/elasticmapreduce">https://console.aws.amazon.com/elasticmapreduce</a>, we will create a cluster. First, click on <strong class="bold">Create cluster</strong>, then on <strong class="bold">Go to advanced options</strong>. This lets us select the list of EMR applications present on the cluster: starting from EMR 5.33.0, we install <strong class="bold">JupyterHub</strong>, <strong class="bold">JupyterEnterpriseGateway</strong>, <strong class="bold">Zeppelin</strong>, and <strong class="bold">Spark</strong>, as visible in the following screenshot:<div id="_idContainer125" class="IMG---Figure"><img src="Images/B17705_07_5.jpg" alt="Figure 7.5 – Creating a Spark cluster &#13;&#10;" width="774" height="303"/></div><p class="figure-caption">Figure 7.5 – Creating a Spark cluster </p><p>We then click on <strong class="bold">Next</strong> twice, name the cluster <strong class="source-inline">sagemaker-cluster</strong>, click on <strong class="bold">Next</strong> again, and then click on <strong class="bold">Create cluster</strong>. You can find additional details at <a href="https://docs.aws.amazon.com/emr/">https://docs.aws.amazon.com/emr/</a>.</p></li>
				<li>While the <a id="_idIndexMarker783"/>cluster is being created, we define our Git repository in the <strong class="bold">Notebooks</strong> entry in the left-hand side vertical menu. Then, we click on <strong class="bold">Add repository</strong>:<div id="_idContainer126" class="IMG---Figure"><img src="Images/B17705_07_6.jpg" alt="Figure 7.6 – Adding a Git repository&#13;&#10;" width="621" height="325"/></div><p class="figure-caption">Figure 7.6 – Adding a Git repository</p></li>
				<li>Then, we create a Jupyter notebook connected to the cluster. Starting from the <strong class="bold">Notebooks</strong> entry in the left-hand side vertical menu, as shown in the following screenshot, we give it a name and select both the EMR cluster and the repository we just created. Then, we click on <strong class="bold">Create notebook</strong>:<div id="_idContainer127" class="IMG---Figure"><img src="Images/B17705_07_7.jpg" alt="Figure 7.7 – Creating a Jupyter notebook &#13;&#10;" width="737" height="682"/></div><p class="figure-caption">Figure 7.7 – Creating a Jupyter notebook </p></li>
				<li>Once the cluster and the <a id="_idIndexMarker784"/>notebook are ready, we can click on <strong class="bold">Open in Jupyter</strong>, which takes us to the familiar Jupyter interface.</li>
			</ol>
			<p>Everything is now ready. Let's write a spam classifier!</p>
			<h3>Building a spam classification model with Spark and SageMaker</h3>
			<p>In this<a id="_idIndexMarker785"/> example, we're<a id="_idIndexMarker786"/> going<a id="_idIndexMarker787"/> to<a id="_idIndexMarker788"/> use the combined benefits of Spark and SageMaker to train, deploy, and predict with a spam classification model, thanks to just a few lines of Scala code:</p>
			<ol>
				<li value="1">First, we need to make sure that our dataset is available in S3. On our local machine, upload the<a id="_idIndexMarker789"/> two files to the default<a id="_idIndexMarker790"/> SageMaker bucket (feel free to use another bucket):<p class="source-code"><strong class="bold">$ aws s3 cp ham s3://sagemaker-eu-west-1-123456789012</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp spam s3://sagemaker-eu-west-1-123456789012</strong></p></li>
				<li>Back in <a id="_idIndexMarker791"/>the<a id="_idIndexMarker792"/> Jupyter notebook, make sure it's running the Spark kernel. Then, import the necessary objects from Spark MLlib and the SageMaker SDK.</li>
				<li>Load the data from S3. Convert all the sentences into lowercase. Then, remove all punctuation and numbers and trim any whitespace:<p class="source-code">val spam = sc.textFile(</p><p class="source-code">"s3://sagemaker-eu-west-1-123456789012/spam")</p><p class="source-code">.map(l =&gt; l.toLowerCase())</p><p class="source-code">.map(l =&gt; l.replaceAll("[^ a-z]", ""))</p><p class="source-code">.map(l =&gt; l.trim())</p><p class="source-code">val ham = sc.textFile(</p><p class="source-code">"s3://sagemaker-eu-west-1-123456789012/ham")</p><p class="source-code">.map(l =&gt; l.toLowerCase())</p><p class="source-code">.map(l =&gt; l.replaceAll("[^ a-z]", ""))</p><p class="source-code">.map(l =&gt; l.trim())</p></li>
				<li>Then, split the messages into words and hash these words into 200 buckets. This technique is much less sophisticated than the word vectors we used in <a href="B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 6</em></a>, <em class="italic">Training Natural Language Processing Models</em>, but it should do the trick:<p class="source-code">val tf = new HashingTF(numFeatures = 200)</p><p class="source-code">val spamFeatures = spam.map(</p><p class="source-code">                   m =&gt; tf.transform(m.split(" ")))</p><p class="source-code">val hamFeatures = ham.map(</p><p class="source-code">                  m =&gt; tf.transform(m.split(" ")))</p><p>For example, the <a id="_idIndexMarker793"/>following message has one <a id="_idIndexMarker794"/>occurrence of a word from bucket 15, one <a id="_idIndexMarker795"/>from <a id="_idIndexMarker796"/>bucket 83, two words from bucket 96, and two from bucket 188:</p><p class="source-code"><strong class="bold">Array((200,[15,83,96,188],[1.0,1.0,2.0,2.0]))</strong></p></li>
				<li>We assign a <strong class="source-inline">1</strong> label for spam messages and a <strong class="source-inline">0</strong> label for ham messages:<p class="source-code">val positiveExamples = spamFeatures.map(</p><p class="source-code">    features =&gt; LabeledPoint(1, features))</p><p class="source-code">val negativeExamples = hamFeatures.map(</p><p class="source-code">    features =&gt; LabeledPoint(0, features))</p></li>
				<li>Merge the messages and encode them in <strong class="bold">LIBSVM</strong> format, one of the formats supported by <strong class="bold">XGBoost</strong>:<p class="source-code">val data = positiveExamples.union(negativeExamples)</p><p class="source-code">val data_libsvm =  </p><p class="source-code">    MLUtils.convertVectorColumnsToML(data.toDF)</p><p>The samples now look similar to this:</p><p class="source-code"><strong class="bold">Array([1.0,(200,[2,41,99,146,172,181],[2.0,1.0,1.0,1.0,1.0])])</strong></p></li>
				<li>Split the data for training and validation:<p class="source-code">val Array(trainingData, testData) = </p><p class="source-code">    data_libsvm.randomSplit(Array(0.8, 0.2))</p></li>
				<li>Configure the XGBoost estimator available in the SageMaker SDK. Here, we're going to train and deploy in one single step:<p class="source-code">val roleArn = "arn:aws:iam:YOUR_SAGEMAKER_ROLE"</p><p class="source-code">val xgboost_estimator = new XGBoostSageMakerEstimator(</p><p class="source-code">    trainingInstanceType="ml.m5.large",</p><p class="source-code">    trainingInstanceCount=1,</p><p class="source-code">    endpointInstanceType="ml.t2.medium",</p><p class="source-code">    endpointInitialInstanceCount=1,</p><p class="source-code">    sagemakerRole=IAMRole(roleArn))</p><p class="source-code">xgboost_estimator.setObjective("binary:logistic")</p><p class="source-code">xgboost_estimator.setNumRound(25)</p></li>
				<li>Fire <a id="_idIndexMarker797"/>up<a id="_idIndexMarker798"/> a <a id="_idIndexMarker799"/>training <a id="_idIndexMarker800"/>job and a deployment job on the managed infrastructure, exactly like when we worked with built-in algorithms in <a href="B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Machine Learning Models</em>. The SageMaker SDK automatically passes the Spark DataFrame to the training job, so no work is required from our end:<p class="source-code">val xgboost_model =   </p><p class="source-code">    xgboost_estimator.fit(trainingData_libsvm)</p><p>As you would expect, these activities are visible in SageMaker Studio in the <strong class="bold">Experiments</strong> section.</p></li>
				<li>When the deployment is complete, transform the test set and score the model. This automatically invokes the SageMaker endpoint. Once again, we don't need to worry about data movement:<p class="source-code">val transformedData = </p><p class="source-code">    xgboost_model.transform(testData_libsvm)</p><p class="source-code">val accuracy = 1.0*transformedData.filter(</p><p class="source-code">    $"label"=== $"prediction")</p><p class="source-code">    .count/transformedData.count()</p><p>The accuracy should be around 97%, which is not too bad!</p></li>
				<li>Once done, delete all SageMaker resources created by the job. This will delete the model, the endpoint, and the endpoint configuration (an object we haven't discussed yet):<p class="source-code">val cleanup = new SageMakerResourceCleanup(</p><p class="source-code">                  xgboost_model.sagemakerClient)</p><p class="source-code">cleanup.deleteResources(</p><p class="source-code">    xgboost_model.getCreatedResources)</p></li>
				<li>Don't <a id="_idIndexMarker801"/>forget<a id="_idIndexMarker802"/> to <a id="_idIndexMarker803"/>terminate the notebook<a id="_idIndexMarker804"/> and the EMR cluster too. You can easily do this in the EMR console.</li>
			</ol>
			<p>This example demonstrates how easy it is to combine the respective strengths of Spark and SageMaker. Another way to do this is to build MLlib pipelines with a mix of Spark and <a id="_idIndexMarker805"/>SageMaker stages. You'll find examples of this at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-spark">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-spark</a>.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor146"/>Summary</h1>
			<p>Open source frameworks such as scikit-learn and TensorFlow have made it simple to write machine learning and deep learning code. They've become immensely popular in the developer community and for good reason. However, managing training and deployment infrastructure still requires a lot of effort and skills that data scientists and machine learning engineers typically do not possess. SageMaker simplifies the whole process. You can go quickly from experimentation to production, without ever worrying about infrastructure.</p>
			<p>In this chapter, you learned about the different frameworks available in SageMaker for machine learning and deep learning, as well as how to customize their containers. You also learned how to use script mode and local mode for fast iteration until you're ready to deploy in production. Finally, you ran several examples, including one that combines Apache Spark and SageMaker.</p>
			<p>In the next chapter, you will learn how to use your own custom code on SageMaker, without having to rely on a built-in container.</p>
		</div>
	</div></body></html>
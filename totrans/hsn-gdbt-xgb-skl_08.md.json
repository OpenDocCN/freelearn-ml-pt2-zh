["```py\n    import pandas as pd\n    df = pd.read_csv('heart_disease.csv')\n    df.head()\n    ```", "```py\n    df.info()\n    ```", "```py\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 303 entries, 0 to 302\n    Data columns (total 14 columns):\n     #   Column    Non-Null Count  Dtype  \n    ---  ------    --------------  -----  \n     0   age       303 non-null    int64  \n     1   sex       303 non-null    int64  \n     2   cp        303 non-null    int64  \n     3   trestbps  303 non-null    int64  \n     4   chol      303 non-null    int64  \n     5   fbs       303 non-null    int64  \n     6   restecg   303 non-null    int64  \n     7   thalach   303 non-null    int64  \n     8   exang     303 non-null    int64  \n     9   oldpeak   303 non-null    float64\n     10  slope     303 non-null    int64  \n     11  ca        303 non-null    int64  \n     12  thal      303 non-null    int64  \n     13  target    303 non-null    int64  \n    dtypes: float64(1), int64(13)\n    memory usage: 33.3 KB\n    ```", "```py\n    from xgboost import XGBClassifier\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    X = df.iloc[:, :-1]\n    y = df.iloc[:, -1]\n    ```", "```py\n    model = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2)\n    ```", "```py\n    from sklearn.model_selection import cross_val_score\n    import numpy as np\n    scores = cross_val_score(model, X, y, cv=5)\n    print('Accuracy:', np.round(scores, 2))\n    print('Accuracy mean: %0.2f' % (scores.mean()))\n    ```", "```py\n    Accuracy: [0.85 0.85 0.77 0.78 0.77]\n    Accuracy mean: 0.81\n    ```", "```py\n    from sklearn.model_selection import StratifiedKFold\n    ```", "```py\n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n    ```", "```py\nscores = cross_val_score(model, X, y, cv=kfold)\nprint('Accuracy:', np.round(scores, 2))\nprint('Accuracy mean: %0.2f' % (scores.mean()))\n```", "```py\nAccuracy: [0.72 0.82 0.75 0.8 0.82]\nAccuracy mean: 0.78\n```", "```py\n    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n    ```", "```py\n    def grid_search(params, random=False): \n    ```", "```py\n    xgb = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2)\n    ```", "```py\n        if random:\n            grid = RandomizedSearchCV(xgb, params, cv=kfold, n_iter=20, n_jobs=-1)\n        else:\n            grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1)\n    ```", "```py\n    grid.fit(X, y)\n    ```", "```py\n    best_params = grid.best_params_\n    print(\"Best params:\", best_params)\n    ```", "```py\n    best_score = grid.best_score_\n    print(\"Training score: {:.3f}\".format(best_score))\n    ```", "```py\ngrid_search(params={'n_estimators':[100, 200, 400, 800]})\n```", "```py\nBest params: {'n_estimators': 100}\nBest score: 0.78235\n```", "```py\ngrid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]})\n```", "```py\nBest params: {'learning_rate': 0.05}\nBest score: 0.79585\n```", "```py\ngrid_search(params={'max_depth':[2, 3, 5, 6, 8]})\n```", "```py\nBest params: {'max_depth': 2}\nBest score: 0.79902\n```", "```py\ngrid_search(params={'gamma':[0, 0.1, 0.5, 1, 2, 5]})\n```", "```py\nBest params: {'gamma': 0.5}\nBest score: 0.79574\n```", "```py\ngrid_search(params={'min_child_weight':[1, 2, 3, 4, 5]})\n```", "```py\nBest params: {'min_child_weight': 5}\nBest score: 0.81219\n```", "```py\ngrid_search(params={'subsample':[0.5, 0.7, 0.8, 0.9, 1]})\n```", "```py\nBest params: {'subsample': 0.8}\nBest score: 0.79579\n```", "```py\ngrid_search(params={'colsample_bytree':[0.5, 0.7, 0.8, 0.9, 1]})\n```", "```py\nBest params: {'colsample_bytree': 0.7}\nBest score: 0.79902\n```", "```py\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n    ```", "```py\n    model = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2)\n    ```", "```py\n    eval_set = [(X_test, y_test)]\n    ```", "```py\n    eval_metric = 'error'\n    ```", "```py\n    model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set)\n    ```", "```py\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n    ```", "```py\n    [0]\tvalidation_0-error:0.15790\n    [1]\tvalidation_0-error:0.10526\n    [2]\tvalidation_0-error:0.11842\n    [3]\tvalidation_0-error:0.13158\n    [4]\tvalidation_0-error:0.11842\n    …\n    [96]\tvalidation_0-error:0.17105\n    [97]\tvalidation_0-error:0.17105\n    [98]\tvalidation_0-error:0.17105\n    [99]\tvalidation_0-error:0.17105\n    Accuracy: 82.89%\n    ```", "```py\nmodel = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2)\neval_set = [(X_test, y_test)]\neval_metric='error'\nmodel.fit(X_train, y_train, eval_metric=\"error\", eval_set=eval_set, early_stopping_rounds=10, verbose=True)\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n```", "```py\n[0]\tvalidation_0-error:0.15790\nWill train until validation_0-error hasn't improved in 10 rounds.\n[1]\tvalidation_0-error:0.10526\n[2]\tvalidation_0-error:0.11842\n[3]\tvalidation_0-error:0.13158\n[4]\tvalidation_0-error:0.11842\n[5]\tvalidation_0-error:0.14474\n[6]\tvalidation_0-error:0.14474\n[7]\tvalidation_0-error:0.14474\n[8]\tvalidation_0-error:0.14474\n[9]\tvalidation_0-error:0.14474\n[10]\tvalidation_0-error:0.14474\n[11]\tvalidation_0-error:0.15790\nStopping. Best iteration:\n[1]\tvalidation_0-error:0.10526\nAccuracy: 89.47%\n```", "```py\nmodel = XGBClassifier(random_state=2, n_estimators=5000)\neval_set = [(X_test, y_test)]\neval_metric=\"error\"\nmodel.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=100)\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n```", "```py\n[0]\tvalidation_0-error:0.15790\nWill train until validation_0-error hasn't improved in 100 rounds.\n[1]\tvalidation_0-error:0.10526\n[2]\tvalidation_0-error:0.11842\n[3]\tvalidation_0-error:0.13158\n[4]\tvalidation_0-error:0.11842\n...\n[98]\tvalidation_0-error:0.17105\n[99]\tvalidation_0-error:0.17105\n[100]\tvalidation_0-error:0.17105\n[101]\tvalidation_0-error:0.17105\nStopping. Best iteration:\n[1]\tvalidation_0-error:0.10526\nAccuracy: 89.47%\n```", "```py\ngrid_search(params={'n_estimators':[2, 25, 50, 75, 100]})\n```", "```py\nBest params: {'n_estimators': 50}\nBest score: 0.78907\n```", "```py\ngrid_search(params={'max_depth':[1, 2, 3, 4, 5, 6, 7, 8], 'n_estimators':[50]})\n```", "```py\nBest params: {'max_depth': 1, 'n_estimators': 50}\nBest score: 0.83869\n```", "```py\ngrid_search(params={'max_depth':[1, 2, 3, 4, 6, 7, 8], 'n_estimators':[2, 50, 100]})\n```", "```py\nBest params: {'max_depth': 1, 'n_estimators': 50}\nBest score: 0.83869\n```", "```py\ngrid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5], 'max_depth':[1], 'n_estimators':[50]})\n```", "```py\nBest params: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 50}\nBest score: 0.83869\n```", "```py\ngrid_search(params={'min_child_weight':[1, 2, 3, 4, 5], 'max_depth':[1], 'n_estimators':[50]})\n```", "```py\nBest params: {'max_depth': 1, 'min_child_weight': 1, 'n_estimators': 50}\nBest score: 0.83869\n```", "```py\ngrid_search(params={'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1], 'max_depth':[1], 'n_estimators':[50]})\n```", "```py\nBest params: {'max_depth': 1, 'n_estimators': 50, 'subsample': 1}\nBest score: 0.83869\n```", "```py\ngrid_search(params={'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1], \n                    'min_child_weight':[1, 2, 3, 4, 5], \n                    'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5], \n                    'max_depth':[1, 2, 3, 4, 5],\n                    'n_estimators':[2]})\n```", "```py\nBest params: {'learning_rate': 0.5, 'max_depth': 2, 'min_child_weight': 4, 'n_estimators': 2, 'subsample': 0.9}\nBest score: 0.81224\n```", "```py\ngrid_search(params={'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1], \n                    'min_child_weight':[1, 2, 3, 4, 5], \n                    'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5], \n                    'max_depth':[1, 2, 3, 4, 5, None], \n                    'n_estimators':[2, 25, 50, 75, 100]},\n                    random=True)\n```", "```py\nBest params: {'subsample': 0.6, 'n_estimators': 25, 'min_child_weight': 4, 'max_depth': 4, 'learning_rate': 0.5}\nBest score: 0.82208\n```", "```py\ngrid_search(params={'colsample_bytree':[0.5, 0.6, 0.7, 0.8, 0.9, 1], 'max_depth':[1], 'n_estimators':[50]})\n```", "```py\nBest params: {'colsample_bytree': 1, 'max_depth': 1, 'n_estimators': 50}\nBest score: 0.83869\n```", "```py\ngrid_search(params={'colsample_bylevel':[0.5, 0.6, 0.7, 0.8, 0.9, 1],'max_depth':[1], 'n_estimators':[50]})\n```", "```py\nBest params: {'colsample_bylevel': 1, 'max_depth': 1, 'n_estimators': 50}\nBest score: 0.83869\n```", "```py\ngrid_search(params={'colsample_bynode':[0.5, 0.6, 0.7, 0.8, 0.9, 1], 'colsample_bylevel':[0.5, 0.6, 0.7, 0.8, 0.9, 1], 'colsample_bytree':[0.5, 0.6, 0.7, 0.8, 0.9, 1], 'max_depth':[1], 'n_estimators':[50]})\n```", "```py\nBest params: {'colsample_bylevel': 0.9, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'max_depth': 1, 'n_estimators': 50}\nBest score: 0.84852\n```", "```py\ngrid_search(params={'gamma':[0, 0.01, 0.05, 0.1, 0.5, 1, 2, 3], 'colsample_bylevel':[0.9], 'colsample_bytree':[0.8], 'colsample_bynode':[0.5], 'max_depth':[1], 'n_estimators':[50]})\n```", "```py\nBest params: {'colsample_bylevel': 0.9, 'colsample_bynode': 0.5, 'colsample_bytree': 0.8, 'gamma': 0, 'max_depth': 1, 'n_estimators': 50}\nBest score: 0.84852\n```"]
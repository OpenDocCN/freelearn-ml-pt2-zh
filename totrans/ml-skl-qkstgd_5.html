<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Numeric Outcomes with Linear Regression</h1>
                </header>
            
            <article>
                
<p class="mce-root">graph_from_dot_data() function on the Linear regression is used to predict a continuous numeric value from a set of input features. This machine learning algorithm is fundamental to statisticians when it comes to predicting numeric outcomes. Although advanced algorithms such as neural networks and deep learning have taken the place of linear regression in modern times, the algorithm is still key when it comes to providing you with the foundations for neural networks and deep learning. </p>
<p>The key benefit of building machine learning models with the linear regression algorithm, as opposed to neural networks and deep learning, is that it is highly interpretable. Interpretability helps you, as the machine learning practitioner, to understand how the different input variables behave when it comes to predicting output. </p>
<p>The linear regression algorithm is applied in the financial industry (in order to predict stock prices) and in the real estate industry (in order to predict housing prices). In fact, the linear regression algorithm can be applied in any field where there is a need to predict a numeric value, given a set of input features.</p>
<p>In this chapter, you will learn about the following topics:</p>
<ul>
<li>The inner mechanics of the linear regression algorithm</li>
<li>Building and evaluating your first linear regression algorithm, using scikit-learn</li>
<li>Scaling your data for a potential performance improvement</li>
<li>Optimizing your linear regression model</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You will be required to have <span class="fontstyle0">Python 3.6 or greater, </span><span class="fontstyle0">Pandas ≥ 0.23.4,</span><span class="fontstyle2"> </span><span class="fontstyle0">Scikit-learn ≥ 0.20.0, and </span><span class="fontstyle0">Matplotlib ≥ 3.0.0 </span></span><span class="fontstyle0">installed on your system.</span></p>
<p class="mce-root">The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_05.ipynb">https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_05.ipynb</a></p>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2Ay95cJ">http://bit.ly/2Ay95cJ</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The inner mechanics of the linear regression algorithm</h1>
                </header>
            
            <article>
                
<p>In its most fundamental form, the expression for the linear regression algorithm can be written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em><img class="fm-editor-equation" src="assets/f5050bc0-a8db-43a6-983e-d963e7b87ea5.png" style="width:33.08em;height:1.75em;"/></em></p>
<p>In the preceding equation, the output of the model is a numeric outcome. In order to obtain this numeric outcome, we require that each input feature be multiplied with a parameter called <em>Parameter1</em>, and we add the second parameter, <em>Parameter2</em>, to this result. </p>
<p>So, in other words, our task is to find the values of the two parameters that can predict the value of the numeric outcome as accurately as possible. In visual terms, consider the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f4c43234-60c1-4a0a-a28b-b9c0fba0b94c.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Two-dimensional plot between the target and input feature</div>
<p>The preceding diagram shows a two-dimensional plot between the target that we want to predict on the <em>y </em>axis (numeric output) and the input feature, which is along the <em>x </em>axis. The goal of linear regression is to find the optimal values of the two parameters mentioned in the preceding equation, in order to fit a line through the given set of points.</p>
<p>This line is known as the <strong>line of best fit</strong>. A line of best fit is one that fits the given sets of points very well, so that it can make accurate predictions for us. Therefore, in order to find the optimal values of the parameters that will result in the line of best fit, we need to define a function that can do it for us. </p>
<p>This function is known as the <strong>loss function</strong>. The goal of the loss function, as the name suggests, is to minimize the loss/errors as much as possible, so that we can obtain a line of best fit. In order to understand how this works, consider the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/deeb026a-8eed-4df2-9931-dc9c7990f3a4.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Line of best fit</div>
<p>In the preceding diagram, the line is fit through the set of data points, and the features can be defined as follows:</p>
<ul>
<li>The distance between each point in the plot and the line is known as the <strong>residual</strong>. </li>
<li>The loss/error function is the sum of the squares of these residuals. </li>
<li>The goal of the linear regression algorithm is to minimize this value. The sum of the squares of the residuals is known as <strong>ordinary least squares </strong>(<strong>OLS</strong>).</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing linear regression in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, you will implement your first linear regression algorithm in scikit-learn. To make this easy to follow, the section will be divided into three subsections, in which you will learn about the following topics:</p>
<ul>
<li>Implementing and visualizing a simple linear regression model in two dimensions </li>
<li>Implementing linear regression to predict the mobile transaction amount </li>
<li>Scaling your data for a potential increase in performance </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression in two dimensions </h1>
                </header>
            
            <article>
                
<p>In this subsection, you will learn how to implement your first linear regression algorithm, in order to predict the amount of a mobile transaction by using one input feature: the old balance amount of the account holder. We will be using the same fraudulent mobile transaction dataset that we used in <a href="08e4b04a-e866-4754-9b0b-1486016dce2c.xhtml" target="_blank"><em>Chapter 2</em></a>, <em>Predicting Categories with K-Nearest Neighbors</em>, of this book.</p>
<p>The first step is to read in the dataset and define the feature and target variable. This can be done by using the following code:</p>
<pre>import pandas as pd<br/><br/>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Define the feature and target arrays<br/><br/>feature = df['oldbalanceOrg'].values<br/>target = df['amount'].values</pre>
<p>Next, we will create a simple scatter plot between the amount of the mobile transaction on the <em>y </em>axis (which is the outcome of the linear regression model) and the old balance of the account holder along the <em>x </em>axis (which is the input feature). This can be done by using the following code:</p>
<pre>import matplotlib.pyplot as plt<br/><br/>#Creating a scatter plot<br/><br/>plt.scatter(feature, target)<br/>plt.xlabel('Old Balance of Account Holder')<br/>plt.ylabel('Amount of Transaction')<br/>plt.title('Amount Vs. Old Balance')<br/>plt.show()</pre>
<p>In the preceding code, we use the <kbd>plt.scatter()</kbd> function to create a scatter plot between the feature<em> </em>on the <em>x </em>axis and the target<em> </em>on the <em>y </em>axis. This results in the scatter plot illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/97a7b42f-9f3f-4193-9500-d1f4f02a58c7.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Two-dimensional space of the linear regression model</div>
<p>Now, we will fit a linear regression model into the two-dimensional space illustrated in the preceding diagram. Note that, in the preceding diagram, the data is not entirely linear. In order to do this, we use the following code:</p>
<pre>#Initializing a linear regression model <br/><br/>linear_reg = linear_model.LinearRegression()<br/><br/>#Reshaping the array since we only have a single feature<br/><br/>feature = feature.reshape(-1, 1)<br/>target = target.reshape(-1, 1)<br/><br/>#Fitting the model on the data<br/><br/>linear_reg.fit(feature, target)<br/><br/>#Define the limits of the x axis <br/><br/>x_lim = np.linspace(min(feature), max(feature)).reshape(-1, 1)<br/><br/>#Creating the scatter plot<br/><br/>plt.scatter(feature, target)<br/>plt.xlabel('Old Balance of Account Holder')<br/>plt.ylabel('Amount of Transaction')<br/>plt.title('Amount Vs. Old Balance')<br/><br/>#Creating the prediction line <br/><br/>plt.plot(x_lim, linear_reg.predict(x_lim), color = 'red')<br/><br/>#Show the plot<br/><br/>plt.show()</pre>
<p>This results in a line of best fit, as illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b3b641d3-a7bc-462d-9263-31a9c7bca3c7.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Line of best fit</div>
<p>In the preceding code, first, we initialize a linear regression model and fit the training data into that model. Since we only have a single feature, we need to reshape the feature and target for scikit-learn. Next, we define the upper and lower limits of the <em>x </em>axis, which contains our feature variable. </p>
<p>Finally, we create a scatter plot between the feature and the target variable and include the line of best fit with the color red, as indicated in the preceding diagram.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using linear regression to predict mobile transaction amount</h1>
                </header>
            
            <article>
                
<p>Now that we have visualized how a simple linear regression model works in two dimensions, we can use the linear regression algorithm to predict the total amount of a mobile transaction, using all of the other features in our mobile transaction dataset. </p>
<p>The first step is to import our fraud prediction dataset into our workspace and divide it into training and test sets. This can be done by using the following code:</p>
<pre>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/># Reading in the dataset <br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)</pre>
<p>We can now fit the linear regression model and evaluate the initial accuracy score of the model by using the following code:</p>
<pre>from sklearn import linear_model<br/><br/>#Initializing a linear regression model <br/><br/>linear_reg = linear_model.LinearRegression()<br/><br/>#Fitting the model on the data<br/><br/>linear_reg.fit(X_train, y_train)<br/><br/>#Accuracy of the model<br/><br/>linear_reg.score(X_test, y_test)</pre>
<p>In the preceding code, first, we initialize a linear regression model, which we can then fit into the training data by using the <kbd>.fit()</kbd><em> </em>function. Then, we evaluate the accuracy score on the test data by using the <kbd>.score()</kbd><em> </em>function. This results in an accuracy score of 98%, which is fantastic!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling your data</h1>
                </header>
            
            <article>
                
<p>Scaling your data and providing a level of standardization is a vital step in any linear regression pipeline, as it could offer a way to enhance the performance of your model. In order to scale the data, we use the following code:</p>
<pre>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import Pipeline<br/><br/>#Setting up the scaling pipeline <br/><br/>pipeline_order = [('scaler', StandardScaler()), ('linear_reg', linear_model.LinearRegression())]<br/><br/>pipeline = Pipeline(pipeline_order)<br/><br/>#Fitting the classfier to the scaled dataset <br/><br/>linear_reg_scaled = pipeline.fit(X_train, y_train)<br/><br/>#Extracting the score <br/><br/>linear_reg_scaled.score(X_test, y_test)</pre>
<p>We use the same scaling pipeline that we used in all of the previous chapters. In the preceding code, we replace the model name with the linear regression model and evaluate the scaled accuracy scores on the test data. </p>
<p>In this case, scaling the data did not lead to any improvements in the accuracy score, but it is vital to implement scaling into your linear regression pipeline, as it does lead to an improvement in the accuracy scores in most cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model optimization </h1>
                </header>
            
            <article>
                
<p>The fundamental objective of the linear regression algorithm is to minimize the loss/cost function. In order to do this, the algorithm tries to optimize the values of the coefficients of each feature (<em>Parameter1</em>),<em> </em>such that the loss function is minimized. </p>
<p class="mce-root"/>
<p>Sometimes, this leads to overfitting, as the coefficients of each variable are optimized for the data that the variable is trained on. This means that your linear regression model will not generalize beyond your current training data very well.</p>
<p>The process by which we penalize hyper-optimized coefficients in order to prevent this type of overfitting is called <strong>regularization</strong>. </p>
<p>There are two broad types of regularization methods, as follows:</p>
<ul>
<li>Ridge regression </li>
<li>Lasso regression</li>
</ul>
<p>In the following subsections, the two types of regularization techniques will be discussed in detail, and you will learn about how you can implement them into your model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ridge regression</h1>
                </header>
            
            <article>
                
<p>The equation for ridge regression is as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/2bf0db77-ab6c-43a2-bd5c-6c5afeef7676.png" style="width:32.08em;height:1.75em;"/></div>
<p>In the preceding equation, the ridge loss function is equal to the ordinary least squares loss function, plus the product of the square of <em><span><span>Parameter1</span></span></em> of each feature and <kbd>alpha</kbd>. </p>
<p><kbd>alpha</kbd> is a parameter that we can optimize in order to control the amount by which the ridge loss function penalizes the coefficients, in order to prevent overfitting. Obviously, if <kbd>alpha</kbd> is equal to <kbd>0</kbd>, the ridge loss function is equal to the ordinary least squares loss function, thereby making no difference to the initial overfit model. </p>
<p>Therefore, optimizing this value of <kbd>alpha</kbd> provides the optimal model that can generalize beyond the data that it has trained on. </p>
<p>In order to implement ridge regression into the fraud prediction dataset, we use the following code:</p>
<pre>from sklearn.linear_model import Ridge<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import Ridge<br/><br/># Reading in the dataset <br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)<br/><br/>#Initialize a ridge regression model<br/><br/>ridge_reg = Ridge(alpha = 0, normalize = True)<br/><br/>#Fit the model to the training data <br/><br/>ridge_reg.fit(X_train, y_train)<br/><br/>#Extract the score from the test data<br/><br/>ridge_reg.score(X_test, y_test)<br/><br/></pre>
<p>In the preceding code, first, we read in the dataset and divide it into training and test sets (as usual). Next, we initialize a ridge regression model by using the <kbd>Ridge()</kbd> function, with the parameters of <kbd>alpha</kbd> set to <kbd>0</kbd> and <kbd>normalize</kbd> set to <kbd>True</kbd>, in order to standardize the data. </p>
<p>Next, the ridge model is fit into the training data, and the accuracy score is extracted from the test data. The accuracy of this model is exactly the same as the accuracy of the model that we built without the ridge regression as the parameter that controls how the model is optimized; <kbd>alpha</kbd> is set to <kbd>0</kbd>. </p>
<p>In order to obtain the optimal value of <kbd>alpha</kbd> with the <kbd>GridSearchCV</kbd> algorithm, we use the following code:</p>
<pre>from sklearn.model_selection import GridSearchCV<br/><br/>#Building the model <br/><br/>ridge_regression = Ridge()<br/><br/>#Using GridSearchCV to search for the best parameter<br/><br/>grid = GridSearchCV(ridge_regression, {'alpha':[0.0001, 0.001, 0.01, 0.1, 10]})<br/>grid.fit(X_train, y_train)<br/><br/># Print out the best parameter<br/><br/>print("The most optimal value of alpha is:", grid.best_params_)<br/><br/>#Initializing an ridge regression object<br/><br/>ridge_regression = Ridge(alpha = 0.01)<br/><br/>#Fitting the model to the training and test sets<br/><br/>ridge_regression.fit(X_train, y_train)<br/><br/>#Accuracy score of the ridge regression model<br/><br/>ridge_regression.score(X_test, y_test)</pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we initialize a ridge regression model, and then, we use the <kbd>GridSearchCV</kbd> algorithm to search for the optimal value of <kbd>alpha</kbd>, from a range of values.</li>
<li>After we obtain this optimal value of <kbd>alpha</kbd>, we build a new ridge regression model with this optimal value in the training data, and we evaluate the accuracy score on the test data. </li>
</ol>
<p>Since our initial model was already well optimized, the accuracy score did not increase by an observable amount. However, on datasets with larger dimensions/features, ridge regression holds immense value for providing you with a model that generalizes well, without overfitting. </p>
<p>In order to verify the results that the <kbd>GridSearchCV</kbd> algorithm has provided us with, we will construct a plot between the accuracy scores on the <em>y </em>axis and the different values of <kbd>alpha</kbd> along the <em>x </em>axis, for both the training and test data. In order to do this, we use the following code:</p>
<pre>import matplotlib.pyplot as plt <br/><br/>train_errors = []<br/>test_errors = []<br/><br/>alpha_list = [0.0001, 0.001, 0.01, 0.1, 10]<br/><br/># Evaluate the training and test classification errors for each value of alpha<br/><br/>for value in alpha_list:<br/>   <br/>    # Create Ridge object and fit<br/>    ridge_regression = Ridge(alpha= value)<br/>    ridge_regression.fit(X_train, y_train)<br/>    <br/>    # Evaluate error rates and append to lists<br/>    train_errors.append(ridge_regression.score(X_train, y_train) )<br/>    test_errors.append(ridge_regression.score(X_test, y_test))<br/>    <br/># Plot results<br/>plt.semilogx(alpha_list, train_errors, alpha_list, test_errors)<br/>plt.legend(("train", "test"))<br/>plt.ylabel('Accuracy Score')<br/>plt.xlabel('Alpha')<br/>plt.show()</pre>
<p>This results in the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/15bc0a4c-ab68-41c0-8439-c4ec352b9748.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Accuracy versus alpha </div>
<p>In the preceding plot, it is clear that a value of 0.01 or lower provides the highest value of accuracy for both the training and test data, and therefore, the results from the <kbd>GridSearchCV</kbd> algorithm make logical sense. </p>
<p>In the preceding code, first, we initialize two empty lists, to store the accuracy scores for both the training and test data. We then evaluate the accuracy scores for both the training and test sets for different values of <kbd>alpha</kbd>, and we create the preceding plot. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lasso regression</h1>
                </header>
            
            <article>
                
<p>The equation for lasso regression is as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/d8213a04-3fa1-4d9c-9d65-d4d9f6ff399d.png" style="width:30.83em;height:1.67em;"/></div>
<p>In the preceding equation, the lasso loss function is equal to the ordinary least squares loss function plus the product of the absolute value of the coefficients of each feature and <kbd>alpha</kbd>. </p>
<p><kbd>alpha</kbd> is a parameter that we can optimize to control the amount by which the lasso loss function penalizes the coefficients, in order to prevent overfitting. Once again, if <kbd>alpha</kbd> is equal to <kbd>0</kbd>, the lasso loss function is equal to the ordinary least squares loss function, thereby making no difference to the initial overfit model. </p>
<p>Therefore, optimizing this value of <kbd>alpha</kbd> provides the optimal model that generalizes well beyond the data that it has trained on. </p>
<p>In order to implement lasso regression into the fraud prediction dataset, we use the following code:</p>
<pre>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import Lasso<br/>import warnings<br/><br/># Reading in the dataset <br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)<br/><br/>#Initialize a lasso regression model<br/><br/>lasso_reg = Lasso(alpha = 0, normalize = True)<br/><br/>#Fit the model to the training data <br/><br/>lasso_reg.fit(X_train, y_train)<br/><br/>warnings.filterwarnings('ignore')<br/><br/>#Extract the score from the test data<br/><br/>lasso_reg.score(X_test, y_test)</pre>
<p>The preceding code is very similar to the code that we used to build the ridge regression model; the only difference is the <kbd>Lasso()</kbd><em> </em>function which we use to initialize a lasso regression model. Additionally, the <kbd>warnings</kbd> package is used, in order to suppress the warning that is generated as we set the value of <kbd>alpha</kbd> to <kbd>0</kbd>. </p>
<p>In order to optimize the value of <kbd>alpha</kbd>, we use the <kbd>GridSearchCV</kbd> algorithm. This is done by using the following code:</p>
<pre>from sklearn.model_selection import GridSearchCV<br/><br/>#Building the model <br/><br/>lasso_regression = Lasso()<br/><br/>#Using GridSearchCV to search for the best parameter<br/><br/>grid = GridSearchCV(lasso_regression, {'alpha':[0.0001, 0.001, 0.01, 0.1, 10]})<br/>grid.fit(X_train, y_train)<br/><br/># Print out the best parameter<br/><br/>print("The most optimal value of alpha is:", grid.best_params_)<br/><br/>#Initializing an lasso regression object<br/><br/>lasso_regression = Lasso(alpha = 0.0001)<br/><br/>#Fitting the model to the training and test sets<br/><br/>lasso_regression.fit(X_train, y_train)<br/><br/>#Accuracy score of the lasso regression model<br/><br/>lasso_regression.score(X_test, y_test)</pre>
<p>The preceding code is similar to the <kbd>alpha</kbd> optimization that we implemented for the ridge regression. Here, we use the lasso regression model instead of the ridge regression model.</p>
<p class="mce-root"/>
<p>In order to verify the results of the <kbd>GridSearchCV</kbd> algorithm, we construct a plot between the accuracy scores and the value of <kbd>alpha</kbd> for the training and test sets. This is shown in the following code:</p>
<pre>train_errors = []<br/>test_errors = []<br/><br/>alpha_list = [0.0001, 0.001, 0.01, 0.1, 10]<br/><br/># Evaluate the training and test classification errors for each value of alpha<br/><br/>for value in alpha_list:<br/>    <br/>    # Create Lasso object and fit<br/>    lasso_regression = Lasso(alpha= value)<br/>    lasso_regression.fit(X_train, y_train)<br/>    <br/>    # Evaluate error rates and append to lists<br/>    train_errors.append(ridge_regression.score(X_train, y_train) )<br/>    test_errors.append(ridge_regression.score(X_test, y_test))<br/>    <br/># Plot results<br/>plt.semilogx(alpha_list, train_errors, alpha_list, test_errors)<br/>plt.legend(("train", "test"))<br/>plt.ylabel('Accuracy Score')<br/>plt.xlabel('Alpha')<br/>plt.show()</pre>
<p>This results in the following output:</p>
<div class="CDPAlignCenter CDPAlign"> <img src="assets/f9b6f0c2-31c7-43c4-9f34-ad1679b7e0f3.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Accuracy versus alpha</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>All of the values of <kbd>alpha</kbd> provide the same values of accuracy scores, and we can thus pick the value given to us by the <kbd>GridSearchCV</kbd> algorithm. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned about how the linear regression algorithm works internally, through key concepts such as residuals and ordinary least squares. You also learned how to visualize a simple linear regression model in two dimensions.</p>
<p>We also covered implementing the linear regression model to predict the amount of a mobile transaction, along with scaling your data in an effective pipeline, to bring potential improvements to your performance. </p>
<p class="mce-root"><span>Finally, you learned how to optimize your model by using the concept of regularization, in the form of ridge and lasso regression. </span></p>


            </article>

            
        </section>
    </body></html>
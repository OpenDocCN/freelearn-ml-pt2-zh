- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After working on multiple projects covering important machine learning concepts,
    techniques, and widely used algorithms, you have a broad picture of the machine
    learning ecosystem, as well as solid experience in tackling practical problems
    using machine learning algorithms and Python. However, there will be issues once
    we start working on projects from scratch in the real world. This chapter aims
    to get us ready for it with 21 best practices to follow throughout the entire
    machine learning solution workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning solution workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in the data preparation stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in the training set generation stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in the model training, evaluation, and selection stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in the deployment and monitoring stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning solution workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, the main tasks involved in solving a machine learning problem can
    be summarized into four areas, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training set generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training, evaluation, and selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Starting from data sources and ending with the final machine learning system,
    a machine learning solution basically follows the paradigm shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: The life cycle of a machine learning solution'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will learn about the typical tasks, common challenges,
    and best practices for each of these four stages.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in the data preparation stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No machine learning system can be built without data. Therefore, **data collection**
    should be our first focus.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 1 – Completely understanding the project goal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before starting to collect data, we should make sure that the goal of the project
    and the business problem are completely understood, as this will guide us on what
    data sources to look into, and where sufficient domain knowledge and expertise
    is also required. For example, in a previous chapter, *Chapter 5*, *Predicting
    Stock Prices with Regression Algorithms*, our goal was to predict the future prices
    of the stock index, so we first collected data on its past performance, instead
    of the past performance of an irrelevant European stock. In *Chapter 3*, *Predicting
    Online Ad Click-Through with Tree-Based Algorithms*, for example, the business
    problem was to optimize advertising, targeting efficiency measured by click-through
    rate, so we collected the clickstream data of who clicked or did not click on
    what ad on what page, instead of merely using how many ads were displayed in a
    web domain.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 2 – Collecting all fields that are relevant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With a set goal in mind, we can narrow down potential data sources to investigate.
    Now the question becomes: is it necessary to collect the data of all fields available
    in a data source, or is a subset of attributes enough? It would be perfect if
    we knew in advance which attributes were key indicators or key predictive factors.
    However, it is in fact very difficult to ensure that the attributes hand-picked
    by a domain expert will yield the best prediction results. Hence, for each data
    source, it is recommended to collect all of the fields that are related to the
    project, especially in cases where recollecting the data is time-consuming, or
    even impossible.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the stock price prediction example, we collected the data of
    all fields, including **Open**, **High**, **Low**, and **Volume**, even though
    we were initially not certain of how useful **high** and **low** predictions would
    be. Retrieving the stock data is quick and easy, however. In another example,
    if we ever want to collect data ourselves by scraping online articles for topic
    classification, we should store as much information as possible. Otherwise, if
    any piece of information is not collected but is later found to be valuable, such
    as hyperlinks in an article, the article might already have been removed from
    the web page; if it still exists, rescraping those pages can be costly.
  prefs: []
  type: TYPE_NORMAL
- en: After collecting the datasets that we think are useful, we need to ensure the
    data quality by inspecting its **consistency** and **completeness**. Consistency
    refers to how the distribution of data changes over time. Completeness means how
    much data is present across fields and samples. They are explained in detail in
    the following two practices.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 3 – Maintaining the consistency and normalization of field values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a dataset that already exists, or in one that we collect from scratch, we
    often see different values representing the same meaning. For example, we see
    *American*, *US*, and *U.S.A* in the `Country` field, and *male* and *M* in the
    `Gender` field. It is necessary to unify or standardize the values in a field,
    otherwise, it will mess up the algorithms in later stages as different feature
    values will be treated differently even if they have the same meaning. For example,
    we keep only the three options *M*, *F*, and *gender-diverse* in the `Gender`
    field, and replace other alternative values. It is also a great practice to keep
    track of what values are mapped to the default value of a field.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the format of values in the same field should also be consistent.
    For instance, in the *age* field, there could be true age values, such as *21*
    and *35*, and incorrect age values, such as *1990* and *1978*; in the *rating*
    field, both cardinal numbers and English numerals could be found, such as *1*,
    *2*, and *3*, and *one*, *two*, and *three*. Transformation and reformatting should
    be conducted in order to ensure data consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 4 – Dealing with missing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to various reasons, datasets in the real world are rarely completely clean
    and often contain missing or corrupted values. They are usually presented as blanks,
    *Null*, *-1, 999999*, *unknown*, or any other placeholder. Samples with missing
    data not only provide incomplete predictive information but also confuse the machine
    learning model as it cannot tell whether *-1* or *unknown* holds a meaning. It
    is important to pinpoint and deal with missing data in order to avoid jeopardizing
    the performance of models in the later stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are three basic strategies that we can use to tackle the missing data
    issue:'
  prefs: []
  type: TYPE_NORMAL
- en: Discarding samples containing any missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discarding fields containing missing values in any sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferring the missing values based on the known part of the attribute. This
    process is called **missing data imputation**. Typical imputation methods include
    replacing missing values with the mean or median value of the field across all
    samples, or the most frequent value for categorical data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two strategies are simple to implement; however, they come at the
    expense of the data lost, especially when the original dataset is not large enough.
    The third strategy doesn’t abandon any data but does try to fill in the blanks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how each strategy is applied in an example where we have a dataset
    (age, income) consisting of six samples – (30, 100), (20, 50), (35, *unknown*),
    (25, 80), (30, 70), and (40, 60):'
  prefs: []
  type: TYPE_NORMAL
- en: If we process this dataset using the first strategy, it becomes (30, 100), (20,
    50), (25, 80), (30, 70), and (40, 60).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we employ the second strategy, the dataset becomes (30), (20), (35), (25),
    (30), and (40), where only the first field remains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we decide to complete the unknown value instead of skipping it, the sample
    (35, *unknown*) can be transformed into (35, 72) with the mean of the rest of
    the values in the second field, or (35, 70), with the median value in the second
    field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In scikit-learn, the `SimpleImputer` class ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))
    provides a nicely written imputation transformer. We can use it for the following
    small example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Represent the unknown value with `np.nan` in `numpy`, as detailed in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the imputation transformer with the mean value and obtain the mean
    value from the original data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Complete the missing value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, initialize the imputation transformer with the median value, as
    detailed in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When new samples come in, the missing values (in any attribute) can be imputed
    using the trained transformer, for example, with the mean value, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that `30` in the age field is the mean of those six age values in the original
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have seen how imputation works, as well as its implementation,
    let’s explore how the strategy of imputing missing values and discarding missing
    data affects the prediction results through the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the diabetes dataset, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Simulate a corrupted dataset by adding 25% missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Randomly select the `m_missing` samples, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each missing sample, randomly select 1 out of `n` features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Represent missing values with `nan`, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we deal with this corrupted dataset by discarding the samples containing
    a missing value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Measure the effects of using this strategy by estimating the averaged regression
    score *R*², with a regression forest model in a cross-validation manner. Estimate
    *R*² on the dataset with the missing samples removed, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we approach the corrupted dataset differently by imputing missing values
    with the mean, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, measure the effects of using this strategy by estimating the averaged
    *R*², as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An imputation strategy works better than discarding in this case. So, how far
    is the imputed dataset from the original full one? We can check it again by estimating
    the averaged regression score on the original dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It turns out that little information is compromised in the imputed dataset.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is no guarantee that an imputation strategy always works better,
    and sometimes, dropping samples with missing values can be more effective. Hence,
    it is a great practice to compare the performance of different strategies via
    cross-validation, as we have done previously.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 5 – Storing large-scale data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the ever-growing size of data, oftentimes, we can’t simply fit the data
    on our single local machine and need to store it on the cloud or distributed filesystems.
    As this is mainly a book on machine learning with Python, we will just touch on
    some basic areas that you can look into. The two main strategies for storing big
    data are **scale up** and **scale out**:'
  prefs: []
  type: TYPE_NORMAL
- en: A **scale-up** approach increases storage capacity if data exceeds the current
    system capacity, such as by adding more disks. This is useful in fast-access platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a **scale-out** approach, storage capacity grows incrementally with additional
    nodes in a storage cluster. Hadoop Distributed File System (HDFS) ([https://hadoop.apache.org/](https://hadoop.apache.org/))
    and Spark ([https://spark.apache.org/](https://spark.apache.org/)) are used to
    store and process big data in scale-out clusters, where data is spread across
    hundreds or even thousands of nodes. Also, there are cloud-based distributed file
    services, such as S3 in Amazon Web Services ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)),
    Google Cloud Storage in Google Cloud ([https://cloud.google.com/storage/](https://cloud.google.com/storage/)),
    and Storage in Microsoft Azure ([https://azure.microsoft.com/en-us/services/storage/](https://azure.microsoft.com/en-us/services/storage/)).
    They are massively scalable and are designed for secure and durable storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besides choosing the right storage system to increase capacity, you also need
    to pay attention to the following practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data partitioning**: Divide your data into smaller partitions or shards.
    This distributes the load across multiple servers or nodes, enabling better parallel
    processing and retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data compression and encoding**: Implement data compression techniques to
    reduce storage space and optimize data retrieval times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replication and redundancy**: Replicate data across multiple storage nodes
    or geographical locations to ensure data availability and fault tolerance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and access control**: Implement robust access control mechanisms
    to ensure that only authorized personnel can access sensitive data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With well-prepared data, it is safe to move on to the training set generation
    stage. Let’s see the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in the training set generation stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Typical tasks in this stage can be summarized into two major categories: **data
    preprocessing** and **feature engineering**.'
  prefs: []
  type: TYPE_NORMAL
- en: To begin, data preprocessing usually involves categorical feature encoding,
    feature scaling, feature selection, and dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 6 – Identifying categorical features with numerical values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, categorical features are easy to spot, as they convey qualitative
    information, such as risk level, occupation, and interests. However, it gets tricky
    if the feature takes on a discreet and countable (limited) number of numerical
    values, for instance, 1 to 12 representing months of the year, and 1 and 0 indicating
    true and false.
  prefs: []
  type: TYPE_NORMAL
- en: The key to identifying whether such a feature is categorical or numerical is
    whether it provides a mathematical or ranking implication; if it does, it is a
    numerical feature, such as a product rating from 1 to 5; otherwise, it is categorical,
    such as the month, or day of the week.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 7 – Deciding whether to encode categorical features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a feature is considered categorical, we need to decide whether we should
    encode it. This depends on what prediction algorithm(s) we will use in later stages.
    Naïve Bayes and tree-based algorithms can directly work with categorical features,
    while other algorithms in general cannot, in which case encoding is essential.
  prefs: []
  type: TYPE_NORMAL
- en: As the output of the feature generation stage is the input of the model training
    stage, *steps taken in the feature generation stage should be compatible with
    the prediction algorithm*. Therefore, we should look at the two stages of feature
    generation and predictive model training as a whole, instead of two isolated components.
    The next two practical tips also reinforce this point.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 8 – Deciding whether to select features and, if so, how to do
    so
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have seen, in *Chapter 4*, *Predicting Online Ad Click-Through with Logistic
    Regression*, how feature selection can be performed using L1-based regularized
    logistic regression and random forest. The benefits of feature selection include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the training time of prediction models as redundant or irrelevant features
    are eliminated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing overfitting for the same preceding reason
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Likely improving performance, as prediction models will learn from data with
    more significant features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that we used the word *likely* because there is no absolute certainty
    that feature selection will increase prediction accuracy. It is, therefore, good
    practice to compare the performances of conducting feature selection and not doing
    so via cross-validation. For example, by executing the following steps, we can
    measure the effects of feature selection by estimating the averaged classification
    accuracy with an `SVC` model in a cross-validation manner:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the handwritten digits dataset from `scikit-learn`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, estimate the accuracy of the original dataset, which is 64-dimensional,
    as detailed here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, conduct feature selection based on random forest and sort the features
    based on their importance scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now select a different number of top features to construct a new dataset, and
    estimate the accuracy on each dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we use the top 25 features selected by random forest, the SVM classification
    performance can increase from `0.9` to `0.95`.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 9 – Deciding whether to reduce dimensionality and, if so, how
    to do so
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature selection and dimensionality are different in the sense that the former
    chooses features from the original data space, while the latter does so from a
    projected space from the original space. Dimensionality reduction has the following
    advantages that are similar to feature selection:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the training time of prediction models, as redundant or correlated
    features are merged into new ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing overfitting for the same reason
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Likely improving performance, as prediction models will learn from data with
    less redundant or correlated features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Again, it is not guaranteed that dimensionality reduction will yield better
    prediction results. In order to examine its effects, integrating dimensionality
    reduction in the model training stage is recommended. Reusing the preceding handwritten
    digits example, we can measure the effects of **Principal Component Analysis**
    (**PCA**)-based dimensionality reduction, where we keep a different number of
    top components to construct a new dataset, and estimate the accuracy on each dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If we use the top 15 features generated by PCA, the SVM classification performance
    can increase from `0.9` to `0.95`.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 10 – Deciding whether to rescale features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As seen in *Chapter 5*, *Predicting Stock Prices with Regression Algorithms*,
    and *Chapter 6*, *Predicting Stock Prices with Artificial Neural Networks*, SGD-based
    linear regression, SVR, and the neural network model require features to be standardized
    by removing the mean and scaling to unit variance. So, when is feature scaling
    needed, and when is it not?
  prefs: []
  type: TYPE_NORMAL
- en: In general, Naïve Bayes and tree-based algorithms are not sensitive to features
    at different scales, as they look at each feature independently.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, an algorithm that involves any form of distance (or separation
    in spaces) of samples in learning requires scaled/standardized inputs, such as
    SVC, SVR, k-means clustering, and **k-nearest neighbors** (**KNN**) algorithms.
    Feature scaling is also a must for any algorithm using SGD for optimization, such
    as linear or logistic regression with gradient descent, and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We have so far covered tips regarding data preprocessing and will next discuss
    best practices of feature engineering as another major aspect of training set
    generation. We will do so from two perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 11 – Performing feature engineering with domain expertise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we are lucky enough to possess sufficient domain knowledge, we can apply
    it in creating domain-specific features; we utilize our business experience and
    insights to identify what is in the data and formulate new data that correlates
    to the prediction target. For example, in *Chapter 5*, *Predicting Stock Prices
    with Regression Algorithms*, we designed and constructed feature sets for the
    prediction of stock prices based on factors that investors usually look at when
    making investment decisions.
  prefs: []
  type: TYPE_NORMAL
- en: While particular domain knowledge is required, sometimes we can still apply
    some general tips in this category. For example, in fields related to customer
    analytics, such as marketing and advertising, the time of the day, day of the
    week, and month are usually important signals. Given a data point with the value
    *2020/09/01* in the `Date` column and *14:34:21* in the `Time` column, we can
    create new features including *afternoon*, *Tuesday*, and *September*. In retail,
    information covering a period of time is usually aggregated to provide better
    insights. The number of times a customer visited a store in the past three months,
    or the average number of products purchased weekly in the previous year, for instance,
    can be good predictive indicators for customer behavior prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 12 – Performing feature engineering without domain expertise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If, unfortunately, we have very little domain knowledge, how can we generate
    features? Don’t panic. There are several generic approaches that you can follow,
    such as binarization, discretization, interaction, and polynomial transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Binarization and discretization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Binarization** is the process of converting a numerical feature to a binary
    one with a preset threshold. For example, in spam email detection, for the feature
    (or term) *prize*, we can generate a new feature, `whether_term_prize_occurs`:
    any term frequency value greater than 1 becomes 1; otherwise, it is 0\. The feature
    *number of visits per week* can be used to produce a new feature, `is_frequent_visitor`,
    by judging whether the value is greater than or equal to 3\. We implement such
    binarization using scikit-learn, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Discretization** is the process of converting a numerical feature to a categorical
    feature with limited possible values. Binarization can be viewed as a special
    case of discretization. For example, we can generate an *age group* feature: “*18-24*”
    for ages from 18 to 24, “*25-34*” for ages from 25 to 34, “*34-54*”, and “*55+*”.'
  prefs: []
  type: TYPE_NORMAL
- en: Interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This includes the sum, multiplication, or any operations of two numerical features,
    and the joint condition check of two categorical features. For example, *the number
    of visits per week* and *the number of products purchased per week* can be used
    to generate *the number of products purchased per visit* feature; *interest and
    occupation*, such as *sports* and *engineer*, can form *occupation AND interest*,
    such as *engineer interested in sports*.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the process of generating polynomial and interaction features. For
    two features, *a* and *b*, the two degrees of polynomial features generated are
    *a*², *ab*, and *b*². In scikit-learn, we can use the `PolynomialFeatures` class
    ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html))
    to perform polynomial transformation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note the resulting new features consist of *1* (bias, intercept), *a*, *b*,
    *a*², *ab*, and *b*².
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 13 – Documenting how each feature is generated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have covered the rules of feature engineering with domain knowledge, and
    in general, there is one more thing worth noting: documenting how each feature
    is generated. It sounds trivial, but oftentimes we just forget about how a feature
    is obtained or created. We usually need to go back to this stage after some failed
    trials in the model training stage and attempt to create more features with the
    hope of improving performance. We have to be clear on what and how features are
    generated, in order to remove those that do not quite work out, and to add new
    ones that have more potential.'
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 14 – Extracting features from text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start with a traditional approach to extract features from text, tf,
    and tf-idf. Then, we will continue with a modern approach: word embedding. Specifically,
    we will look at word embedding using `Word2Vec` models, and embedding layers in
    neural network models.'
  prefs: []
  type: TYPE_NORMAL
- en: tf and tf-idf
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have worked intensively with text data in *Chapter 7*, *Mining the 20 Newsgroups
    Dataset with Text Analysis Techniques*, and *Chapter 8*, *Discovering Underlying
    Topics in the Newsgroups Dataset with Clustering and Topic Modeling*, where we
    extracted features from text based on **term frequency** (**tf**) and **term frequency-inverse
    document frequency** (**tf-idf**). Both methods consider each document of words
    (terms) a collection of words, or a **bag of words** (**BoW**), disregarding the
    order of the words but keeping multiplicity. A tf approach simply uses the counts
    of tokens, while tf-idf extends tf by assigning each tf a weighting factor that
    is inversely proportional to the document frequency. With the idf factor incorporated,
    tf-idf diminishes the weight of common terms (such as “get” and “make”) that occur
    frequently, and emphasizes terms that rarely occur but convey important meaning.
    Hence, oftentimes, features extracted from tf-idf are more representative than
    those from tf.
  prefs: []
  type: TYPE_NORMAL
- en: As you may remember, a document is represented by a very sparse vector where
    only present terms have non-zero values. The vector’s dimensionality is usually
    high, which is determined by the size of the vocabulary and the number of unique
    terms. Also, such a one-hot encoding approach treats each term as an independent
    item and does not consider the relationship across words (referred to as “context”
    in linguistics).
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the contrary, another approach, called **word embedding**, is able to capture
    the meanings of words and their context. In this approach, a word is represented
    by a vector of float numbers. Its dimensionality is a lot lower than the size
    of the vocabulary and is usually several hundred only.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding vectors are of real values, where each dimension encodes an aspect
    of meaning for the words in the vocabulary. This helps preserve the semantic information
    of the words, as opposed to discarding it as in the one-hot encoding approach
    using tf or tf-idf. An interesting phenomenon is that vectors from semantically
    similar words are proximate to each other in geometric space. For example, both
    the words *clustering and grouping* refer to unsupervised clustering in the context
    of machine learning, hence their embedding vectors are close together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some popular ways to obtain word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word2Vec**: Train your own Word2Vec embeddings on your specific corpus using
    the Skip-gram or Continuous Bag of Words (CBOW) models. We covered this in *Chapter
    7*, *Mining the 20 Newsgroups Dataset with Text Analysis Techniques*. Libraries
    like Gensim in Python provide easy-to-use interfaces for training Word2Vec embeddings.
    We will present a simple example shortly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-trained embeddings**: Use pre-trained word embeddings that are trained
    on large corpora. We also talked about this in *Chapter 7*. Popular examples include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FastText
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GloVe** (**Global Vectors for Word Representation**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BERT** (**Bidirectional Encoder Representations from Transformers**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT** (**Generative Pre-trained Transformer**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**USE** (**Universal Sentence Encoder**) embeddings'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training custom models with an embedding layer**: If you have a specific
    domain or dataset, you can train your own word embeddings using custom neural
    network models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prior to delving into training a custom model for word embeddings, let’s begin
    with the following example of training a basic `Word2Vec` model using `gensim`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the `gensim` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We define some sample sentences for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In practice, you will need to format the sentences in plain text into a list
    of word lists just like the `sentences` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create a Word2Vec model with various parameters, such as `vector_size`
    (embedding dimension), `window` (context window size), `min_count` (minimum frequency
    of words), and `sg` (training algorithm – `0` for CBOW, `1` for Skip-gram):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: After training, we access word vectors using the model’s `wv` property. Here,
    we display the embedding vector for the word *machine:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that this is a basic example. In practice, you might need to preprocess
    your data more thoroughly, adjust hyperparameters, and train on a larger corpus
    for better embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding layers in custom neural networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a complete deep neural network for NLP tasks, we would typically combine
    an embedding layer with other layers, like fully connected (dense) layers, or
    recurrent layers (we will talk about recurrent layers in *Chapter 12**, Making
    Predictions with Sequences Using Recurrent Neural Networks*) to build a more sophisticated
    model. The embedding layer allows the network to learn meaningful representations
    for words in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simplified example of using an embedding layer for word embeddings.
    In PyTorch, we use the `nn.Embedding` module ([https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html))
    for embedding layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we first import the necessary modules from PyTorch. We define
    some sample input data containing word indices (for example, 1 represents *I*,
    2 represents *love*, 3 represents *machine*, and 4 represents *learning*). Then,
    we define the embedding layer using `nn.Embedding` with `vocab_size` as the total
    number of unique words in the vocabulary, and `embedding_dim` as the desired dimensionality
    of the embeddings. The embedding layer is usually the first layer of a neural
    network model after the input layer. Upon completion of network training, when
    we pass the input data through the embedding layer, it returns embedded vectors
    for each input word index. The shape of output `embedded_data` will be `(sample
    size, sequence length, embedding_dim)`, which is `(2, 4, 3)` in our case.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, this is a simplified example. In practice, the embedding layers
    are involved in more complex architectures with additional layers, in order to
    process and interpret the embeddings for specific tasks, such as classification,
    sentiment analysis, or sequence generation. Keep an eye out for the upcoming *Chapter
    12*.
  prefs: []
  type: TYPE_NORMAL
- en: Curious about the choice between tf-idf and word embeddings? In conventional
    NLP applications, such as simple text classification and topic modeling, tf, or
    tf-idf, remains an exceptional method for feature extraction. In more complicated
    areas, such as text summarization, machine translation, named entity resolution,
    question answering, and information retrieval, word embeddings are extensively
    utilized and yield significantly enhanced features compared to conventional methods.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have reviewed the best practices for data and feature generation,
    let’s look at model training next.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in the model training, evaluation, and selection stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a supervised machine learning problem, the first question many people
    ask is usually *What is the best classification or regression algorithm to solve
    it?* However, there is no one-size-fits-all solution and no free lunch. No one
    could know which algorithm will work best before trying multiple ones and fine-tuning
    the optimal one. We will be looking into best practices around this in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 15 – Choosing the right algorithm(s) to start with
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the fact that there are several parameters to tune for an algorithm,
    exhausting all algorithms and fine-tuning each one can be extremely time-consuming
    and computationally expensive. We should instead shortlist one to three algorithms
    to start with using the general guidelines that follow (note we herein focus on
    classification, but the theory transcends to regression, and there is usually
    a counterpart algorithm in regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several things we need to be clear about before shortlisting potential
    algorithms, as described in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dimensionality of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the data is linearly separable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether features are independent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolerance and trade-off of bias and variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether online learning is required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at how we choose the right algorithm to start with, taking into
    account the aforementioned perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a very simple algorithm. For a relatively small training dataset, if
    features are independent, Naïve Bayes will usually perform well. For a large dataset,
    Naïve Bayes will still work well as feature independence can be assumed in this
    case, regardless of the truth. The training of Naïve Bayes is usually faster than
    any other algorithm due to its computational simplicity. However, this may lead
    to a high bias (but low variance).
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is probably the most widely used classification algorithm, and the first
    algorithm that a machine learning practitioner usually tries when given a classification
    problem. It performs well when data is linearly separable or approximately **linearly
    separable**. Even if it is not linearly separable, it might be possible to convert
    the linearly non-separable features into separable ones and apply logistic regression
    afterward.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following instance, data in the original space is not linearly separable,
    but it becomes separable in a transformed space created from the interaction of
    two features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated with low
    confidence](img/B21047_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Transforming features from linearly non-separable to separable'
  prefs: []
  type: TYPE_NORMAL
- en: Also, logistic regression is extremely scalable to large datasets with SGD optimization,
    which makes it efficient in solving big data problems. Plus, it makes online learning
    feasible. Although logistic regression is a low-bias, high-variance algorithm,
    we overcome the potential overfitting by adding L1, L2, or a mix of the two regularizations.
  prefs: []
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is versatile enough to adapt to the linear separability of data. For a
    separable dataset, SVM with a linear kernel performs comparably to logistic regression.
    Beyond this, SVM also works well for a non-separable dataset if equipped with
    a non-linear kernel, such as RBF. Logistic regression may face challenges in high-dimensional
    datasets, while SVM still performs well. A good example of this can be in news
    classification, where the feature dimensionality is in the tens of thousands.
    In general, very high accuracy can be achieved by SVM with the right kernel and
    parameters. However, this might be at the expense of intense computation and high
    memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest (or decision tree)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear separability of the data does not matter to this algorithm, and it
    works directly with categorical features without encoding, which provides great
    ease of use. Also, the trained model is very easy to interpret and explain to
    non-machine learning practitioners, which cannot be achieved with most other algorithms.
    Additionally, random forest boosts the decision tree algorithm, which can reduce
    overfitting by ensembling a collection of separate trees. Its performance is comparable
    to SVM, while fine-tuning a random forest model is less difficult compared to
    SVM and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are extremely powerful, especially with the development of deep learning.
    However, finding the right topology (layers, nodes, activation functions, and
    so on) is not easy, not to mention the time-consuming model of training and tuning.
    Hence, they are not recommended as an algorithm to start with for general machine
    learning problems. However, for computer vision and many NLP tasks, the neural
    network is still the go-to model. In summary, here are some scenarios where using
    neural networks is particularly beneficial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complex patterns**: When the task involves learning complex patterns or relationships
    within the data that may be difficult for traditional algorithms to capture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large amounts of data**: Neural networks tend to perform well when you have
    a substantial amount of data available for training, as they are capable of learning
    from large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unstructured data**: Neural networks excel in handling unstructured data
    types like images, audio, and text, where traditional methods might struggle to
    extract meaningful features. For NLP tasks like sentiment analysis, machine translation,
    named entity recognition, and text generation, neural networks, especially recurrent
    and transformer models, have shown remarkable performance. In image classification,
    object detection, segmentation, and image generation tasks, deep neural networks
    have revolutionized computer vision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practice 16 – Reducing overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We touched on ways to avoid overfitting when discussing the pros and cons of
    algorithms in the last practice. We herein formally summarize them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**More data, if possible**: Increase the size of your training dataset. More
    data can help the model learn relevant patterns and reduce its tendency to memorize
    noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplification, if possible**: The more complex the model is, the higher
    the chance of overfitting. Complex models include a tree or forest with excessive
    depth, a linear regression with a high degree of polynomial transformation, and
    an SVM with a complicated kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-validation**: A good habit that we have built over all of the chapters
    in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: This adds penalty terms to reduce the error caused by fitting
    the model perfectly on the given training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: Monitor the model’s performance on a validation set during
    training. Stop training when the performance on the validation set starts to degrade,
    indicating that the model is starting to overfit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: In neural networks, apply dropout layers during training. Dropout
    randomly drops out a fraction of neurons during each forward pass, preventing
    reliance on specific neurons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: Select a subset of relevant features. Removing irrelevant
    or redundant features can prevent the model from fitting noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble learning**: This involves combining a collection of weak models
    to form a stronger one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, how can we tell whether a model suffers from overfitting, or the other extreme,
    underfitting? Let’s see the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 17 – Diagnosing overfitting and underfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **learning curve** is usually used to evaluate the bias and variance of a
    model. A learning curve is a graph that compares the cross-validated training
    and testing scores over a given number of training samples.
  prefs: []
  type: TYPE_NORMAL
- en: For a model that fits well on the training samples, the performance of the training
    samples should be beyond what’s desired. Ideally, as the number of training samples
    increases, the model performance on the testing samples will improve; eventually,
    the performance on the testing samples will become close to that of the training
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: When the performance on the testing samples converges at a value much lower
    than that of the training performance, overfitting can be concluded. In this case,
    the model fails to generalize to instances that have not been seen.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a model that does not even fit well on the training samples, underfitting
    is easily spotted: both performances on the training and testing samples are below
    the desired performance in the learning curve.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the learning curve in an ideal case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, line, diagram  Description automatically
    generated](img/B21047_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Ideal learning curve'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of the learning curve for an overfitted model is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of training data  Description automatically generated with low
    confidence](img/B21047_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Overfitting learning curve'
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning curve for an underfitted model may look like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, line, diagram  Description automatically
    generated](img/B21047_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Underfitting learning curve'
  prefs: []
  type: TYPE_NORMAL
- en: To generate the learning curve, you can utilize the `learning_curve` module
    ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve))
    from scikit-learn, and the `plot_learning_curve` function defined at [https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html).
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 18 – Modeling on large-scale datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We gained experience working with large datasets in *Chapter 4*, *Predicting
    Online Ad Click-Through with Logistic Regression*. There are a few tips that can
    help you model on large-scale data more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: First, start with a small subset, for instance, a subset that can fit on your
    local machine. This can help speed up early experimentation. Obviously, you don’t
    want to train on the entire dataset just to find out whether SVM or random forest
    works better. Instead, you can randomly sample data points and quickly run a few
    models on the selected set.
  prefs: []
  type: TYPE_NORMAL
- en: The second tip is choosing scalable algorithms, such as logistic regression,
    linear SVM, and SGD-based optimization. This is quite intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are other best practices for modeling on large-scale datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sampling and subset selection**: When starting model development, work with
    smaller subsets of your data to iterate and experiment quickly. Once your model
    architecture and parameters are tuned, scale up to the full dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed computing**: Utilize distributed computing frameworks like Apache
    Spark to handle large-scale data processing and model training across multiple
    nodes or clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Focus on relevant features and avoid unnecessary dimensions.
    Use dimensionality reduction techniques like PCA or t-SNE to reduce feature space
    if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelization**: Explore techniques to parallelize training, like data
    parallelism or model parallelism, to leverage multiple GPUs or distributed systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory management**: Optimize memory usage by using data generators, streaming
    data from storage, and releasing memory when no longer needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimized libraries**: Choose libraries and frameworks that are optimized
    for large-scale data, such as TensorFlow, PyTorch, scikit-learn, and XGBoost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incremental learning**: For streaming data or dynamic datasets, consider
    incremental learning techniques that update the model as new data arrives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, don’t forget to save the trained model. Training on a large
    dataset takes a long time, which you would want to avoid redoing, if possible.
    We will explore saving and loading models in detail in *Best practice 19 – Saving,
    loading, and reusing models*, which is a part of the deployment and monitoring
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in the deployment and monitoring stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After performing all processes in the previous three stages, we now have a well-established
    data preprocessing pipeline and a correctly trained prediction model. The last
    stage of a machine learning system involves saving those resulting models from
    previous stages and deploying them on new data, as well as monitoring their performance
    and updating the prediction models regularly. We also need to implement monitoring
    and logging to track model performance, training progress, and potential issues
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 19 – Saving, loading, and reusing models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When machine learning is deployed, new data should go through the same data
    preprocessing procedures (scaling, feature engineering, feature selection, dimensionality
    reduction, and so on) as in the previous stages. The preprocessed data is then
    fed into the trained model. We simply cannot rerun the entire process and retrain
    the model every time new data comes in. Instead, we should save the established
    preprocessing models and trained prediction models after the corresponding stages
    have been completed. In deployment mode, these models are loaded in advance and
    are used to produce prediction results from the new data. Let’s explore methods
    for saving and loading models using pickle, TensorFlow, and PyTorch below.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and restoring models using pickle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start with using `pickle`. This can be illustrated via the diabetes example,
    where we standardize the data and employ an `SVR` model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Preprocess the training data with scaling, as shown in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now save the established standardizer, the `scaler` object with `pickle`, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This generates a `scaler.p` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Move on to training an `SVR` model on the scaled data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the trained `regressor` object with `pickle`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This generates a `regressor.p` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the deployment stage, we first load the saved standardizer and the `regressor`
    object from the preceding two files, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we preprocess the new data using the standardizer and make a prediction
    with the `regressor` object just loaded, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Saving and restoring models in TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I will also demonstrate how to save and restore models in TensorFlow. As an
    example, we will train a simple logistic regression model on the cancer dataset,
    save the trained model, and reload it in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary TensorFlow modules and load the cancer dataset from `scikit-learn`
    and rescale the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a simple logistic regression model using the Keras Sequential API, along
    with several specified parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the TensorFlow model against the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the model’s architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will see if we can retrieve the same model later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, the previous steps look familiar to you. If not, feel free to review
    our TensorFlow implementation. Now we save the model to a path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After this, you will see that a folder called `model_tf` is created. The folder
    contains the trained model’s architecture, weights, and training configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we load the model from the previous path and display the loaded model’s
    path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We just loaded back the exact same model.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and restoring models in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, let’s see how to save and restore models in PyTorch. Similarly, we
    will train a simple logistic regression model on the same cancer dataset, save
    the trained model, and reload it in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the data `torch` tensors used for modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a simple logistic regression model using the `nn.sequential` module,
    along with the loss function and optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reuse the `train_step` function we developed previously in *Chapter 6*, *Predicting
    Stock Prices with Artificial Neural Networks*, and train the `PyTorch` model against
    the data for 10 iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the model’s architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will see if we can retrieve the same model later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, the previous steps look familiar to you. If not, feel free to review
    our PyTorch implementation. Now we save the model to a path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After this, you will see that a folder called `model.pth` is created. The folder
    contains the entire trained model’s architecture, weights, and training configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we load the model from the previous path and display the loaded model’s
    path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We just loaded back the exact same model.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 20 – Monitoring model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The machine learning system is now up and running. To make sure everything
    is on the right track, we need to conduct performance checks on a regular basis.
    To do so, besides making a prediction in real time, we should also record the
    ground truth at the same time. Here are some best practices for monitoring model
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define evaluation metrics**: Choose appropriate evaluation metrics that align
    with your problem’s goals. Accuracy, precision, recall, F1-score, AUC-ROC, *R*²,
    and mean squared error are some common metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Baseline performance**: Establish a baseline model or a simple rule-based
    approach to compare your model’s performance. This provides context for understanding
    whether your model is adding value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning curves**: Plot learning curves showing training and validation loss
    or evaluation metrics over epochs. This helps identify overfitting or underfitting
    issues, as mentioned in *Best practice 17 – Diagnosing overfitting and underfitting*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Continuing with the diabetes example from earlier in the chapter, we conduct
    a performance check as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We should log the performance and set up an alert for any decayed performance.
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 21 – Updating models regularly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the performance is getting worse, chances are that the pattern of data has
    changed. We can work around this by updating the model. Depending on whether online
    learning is feasible or not with the model, the model can be modernized with the
    new set of data (online updating) or retrained completely with the most recent
    data. Here are some best practices for the last section of the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitor model performance**: Continuously monitor model performance metrics.
    If there’s a significant drop, it’s a sign that the model needs updating.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduled updates**: Implement a schedule for model updates based on the
    frequency of data changes and business needs. This ensures that the model remains
    relevant, without unnecessary updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online updating**: For models that support online learning, update the model
    incrementally with new data. This applies to models based on gradient descent
    algorithms, or Naïve Bayes. Online updating minimizes the need for retraining
    the entire model and adapts it to changing patterns over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version control**: Maintain version control of models and datasets to track
    changes and facilitate rollback if necessary. This helps in comparing model performance
    over time and reverting to previous versions if updates lead to performance degradation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular auditing**: Periodically review model performance, reevaluate business
    goals, and update your evaluation metrics if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that monitoring should be an ongoing process, starting from model development
    through deployment and maintenance. It ensures that your machine learning models
    remain effective, trustworthy, and aligned with your business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The purpose of this chapter is to prepare you for real-world machine learning
    problems. We started with the general workflow that a machine learning solution
    follows: data preparation, training set generation, algorithm training, evaluation
    and selection, and finally, system deployment and monitoring. We then went, in
    depth, through the typical tasks, common challenges, and best practices for each
    of these four stages.'
  prefs: []
  type: TYPE_NORMAL
- en: Practice makes perfect. The most important best practice is practice itself.
    Get started with a real-world project to deepen your understanding and apply what
    you have learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start our deep learning journey by categorizing
    clothing images using convolutional neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you use word embedding to extract text features and develop a multiclass
    classifier to classify the newsgroup data? (Note that you might not be able to
    get better results with word embedding than tf-idf, but it is good practice.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you find several challenges in Kaggle ([www.kaggle.com](https://www.kaggle.com))
    and practice what you have learned throughout the entire book?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1878468721786989681.png)'
  prefs: []
  type: TYPE_IMG

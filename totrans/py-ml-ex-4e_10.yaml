- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Machine Learning Best Practices
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: After working on multiple projects covering important machine learning concepts,
    techniques, and widely used algorithms, you have a broad picture of the machine
    learning ecosystem, as well as solid experience in tackling practical problems
    using machine learning algorithms and Python. However, there will be issues once
    we start working on projects from scratch in the real world. This chapter aims
    to get us ready for it with 21 best practices to follow throughout the entire
    machine learning solution workflow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了多个涵盖重要机器学习概念、技术和广泛使用的算法的项目之后，你已经对机器学习生态系统有了全面的了解，并且在使用机器学习算法和Python解决实际问题方面积累了扎实的经验。然而，当我们开始从零开始在现实世界中开展项目时，仍会面临一些问题。本章旨在通过21条最佳实践，帮助我们为此做好准备，贯穿整个机器学习解决方案的工作流程。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Machine learning solution workflow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习解决方案工作流程
- en: Best practices in the data preparation stage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备阶段的最佳实践
- en: Best practices in the training set generation stage
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集生成阶段的最佳实践
- en: Best practices in the model training, evaluation, and selection stage
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练、评估和选择阶段的最佳实践
- en: Best practices in the deployment and monitoring stage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署与监控阶段的最佳实践
- en: Machine learning solution workflow
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习解决方案工作流程
- en: 'In general, the main tasks involved in solving a machine learning problem can
    be summarized into four areas, as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，解决机器学习问题的主要任务可以总结为四个方面，如下所示：
- en: Data preparation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Training set generation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集生成
- en: Model training, evaluation, and selection
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练、评估与选择
- en: Deployment and monitoring
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署与监控
- en: 'Starting from data sources and ending with the final machine learning system,
    a machine learning solution basically follows the paradigm shown here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据源开始，到最终的机器学习系统，机器学习解决方案基本上遵循如下模式：
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_10_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_10_01.png)'
- en: 'Figure 10.1: The life cycle of a machine learning solution'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：机器学习解决方案的生命周期
- en: In the following sections, we will learn about the typical tasks, common challenges,
    and best practices for each of these four stages.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将学习这些四个阶段的典型任务、常见挑战以及最佳实践。
- en: Best practices in the data preparation stage
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备阶段的最佳实践
- en: No machine learning system can be built without data. Therefore, **data collection**
    should be our first focus.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数据就无法构建机器学习系统。因此，**数据收集**应当是我们首先关注的重点。
- en: Best practice 1 – Completely understanding the project goal
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 1 – 完全理解项目目标
- en: Before starting to collect data, we should make sure that the goal of the project
    and the business problem are completely understood, as this will guide us on what
    data sources to look into, and where sufficient domain knowledge and expertise
    is also required. For example, in a previous chapter, *Chapter 5*, *Predicting
    Stock Prices with Regression Algorithms*, our goal was to predict the future prices
    of the stock index, so we first collected data on its past performance, instead
    of the past performance of an irrelevant European stock. In *Chapter 3*, *Predicting
    Online Ad Click-Through with Tree-Based Algorithms*, for example, the business
    problem was to optimize advertising, targeting efficiency measured by click-through
    rate, so we collected the clickstream data of who clicked or did not click on
    what ad on what page, instead of merely using how many ads were displayed in a
    web domain.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始收集数据之前，我们应该确保完全理解项目的目标和业务问题，因为这将指导我们选择哪些数据来源，以及在哪些领域需要充足的领域知识和专业技能。例如，在前面的章节中，*第5章*，*使用回归算法预测股价*，我们的目标是预测股票指数的未来价格，因此我们首先收集了其历史表现的数据，而不是收集与之无关的欧洲股票的历史表现数据。在*第3章*，*使用基于树的算法预测在线广告点击率*中，业务问题是优化广告投放，目标是提高点击率，因此我们收集了点击流数据，记录谁在什么页面点击或未点击了哪个广告，而不是仅仅使用广告在网页域名中展示的数量。
- en: Best practice 2 – Collecting all fields that are relevant
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 2 – 收集所有相关字段
- en: 'With a set goal in mind, we can narrow down potential data sources to investigate.
    Now the question becomes: is it necessary to collect the data of all fields available
    in a data source, or is a subset of attributes enough? It would be perfect if
    we knew in advance which attributes were key indicators or key predictive factors.
    However, it is in fact very difficult to ensure that the attributes hand-picked
    by a domain expert will yield the best prediction results. Hence, for each data
    source, it is recommended to collect all of the fields that are related to the
    project, especially in cases where recollecting the data is time-consuming, or
    even impossible.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 确定了目标后，我们可以缩小潜在的数据源进行调查。现在的问题是：是否有必要收集数据源中所有领域的数据，还是仅仅一个属性的子集就足够了？如果我们能事先知道哪些属性是关键指标或预测因素，那将是最理想的。然而，实际上很难确保由领域专家挑选的属性能够产生最佳的预测结果。因此，对于每个数据源，建议收集与项目相关的所有字段，尤其是在重新收集数据既费时又几乎不可能的情况下。
- en: For example, in the stock price prediction example, we collected the data of
    all fields, including **Open**, **High**, **Low**, and **Volume**, even though
    we were initially not certain of how useful **high** and **low** predictions would
    be. Retrieving the stock data is quick and easy, however. In another example,
    if we ever want to collect data ourselves by scraping online articles for topic
    classification, we should store as much information as possible. Otherwise, if
    any piece of information is not collected but is later found to be valuable, such
    as hyperlinks in an article, the article might already have been removed from
    the web page; if it still exists, rescraping those pages can be costly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在股价预测的案例中，我们收集了所有字段的数据，包括**Open**、**High**、**Low**和**Volume**，尽管最初我们不确定**high**和**low**预测的用处。然而，获取股市数据非常迅速且容易。在另一个例子中，如果我们想要通过抓取在线文章进行主题分类并自己收集数据，我们应该尽可能多地存储信息。否则，如果某些信息没有被收集，但后来发现它很有价值，例如文章中的超链接，文章可能已经从网页上删除；如果它仍然存在，重新抓取这些页面的成本可能会很高。
- en: After collecting the datasets that we think are useful, we need to ensure the
    data quality by inspecting its **consistency** and **completeness**. Consistency
    refers to how the distribution of data changes over time. Completeness means how
    much data is present across fields and samples. They are explained in detail in
    the following two practices.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了我们认为有用的数据集之后，我们需要通过检查其**一致性**和**完整性**来确保数据质量。一致性是指数据分布随时间的变化情况。完整性是指在字段和样本中的数据量。这将在以下两个实践中详细解释。
- en: Best practice 3 – Maintaining the consistency and normalization of field values
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 3 – 维护字段值的一致性和标准化
- en: In a dataset that already exists, or in one that we collect from scratch, we
    often see different values representing the same meaning. For example, we see
    *American*, *US*, and *U.S.A* in the `Country` field, and *male* and *M* in the
    `Gender` field. It is necessary to unify or standardize the values in a field,
    otherwise, it will mess up the algorithms in later stages as different feature
    values will be treated differently even if they have the same meaning. For example,
    we keep only the three options *M*, *F*, and *gender-diverse* in the `Gender`
    field, and replace other alternative values. It is also a great practice to keep
    track of what values are mapped to the default value of a field.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个已经存在的数据集或我们从零开始收集的数据集中，我们经常看到不同的值表示相同的含义。例如，在`Country`字段中，我们看到*American*、*US*和*U.S.A*，在`Gender`字段中，我们看到*male*和*M*。有必要统一或标准化字段中的值，否则在后期阶段，算法会混淆处理不同的特征值，即使它们具有相同的含义。例如，我们只保留`Gender`字段中的三种选项：*M*、*F*和*gender-diverse*，并替换其他替代值。记录哪些值被映射到字段的默认值也是一个很好的做法。
- en: In addition, the format of values in the same field should also be consistent.
    For instance, in the *age* field, there could be true age values, such as *21*
    and *35*, and incorrect age values, such as *1990* and *1978*; in the *rating*
    field, both cardinal numbers and English numerals could be found, such as *1*,
    *2*, and *3*, and *one*, *two*, and *three*. Transformation and reformatting should
    be conducted in order to ensure data consistency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，同一字段中值的格式也应保持一致。例如，在*age*字段中，可能会有真实年龄值，如*21*和*35*，也可能会有不正确的年龄值，如*1990*和*1978*；在*rating*字段中，可能会看到基数数字和英文数字，如*1*、*2*、*3*，以及*one*、*two*、*three*。应该进行转换和重新格式化，以确保数据的一致性。
- en: Best practice 4 – Dealing with missing data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 4 – 处理缺失数据
- en: Due to various reasons, datasets in the real world are rarely completely clean
    and often contain missing or corrupted values. They are usually presented as blanks,
    *Null*, *-1, 999999*, *unknown*, or any other placeholder. Samples with missing
    data not only provide incomplete predictive information but also confuse the machine
    learning model as it cannot tell whether *-1* or *unknown* holds a meaning. It
    is important to pinpoint and deal with missing data in order to avoid jeopardizing
    the performance of models in the later stages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各种原因，现实世界中的数据集很少完全干净，通常包含缺失或损坏的值。它们通常被呈现为空白、*Null*、*-1, 999999*、*unknown*
    或其他任何占位符。带有缺失数据的样本不仅提供不完整的预测信息，还会使机器学习模型困惑，因为它无法确定 *-1* 或 *unknown* 是否具有特定含义。准确定位并处理缺失数据是非常重要的，以避免在后期模型的性能受到影响。
- en: 'Here are three basic strategies that we can use to tackle the missing data
    issue:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有三种基本策略，我们可以用来处理缺失数据问题：
- en: Discarding samples containing any missing values.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃包含任何缺失值的样本。
- en: Discarding fields containing missing values in any sample.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃任何样本中包含缺失值的字段。
- en: Inferring the missing values based on the known part of the attribute. This
    process is called **missing data imputation**. Typical imputation methods include
    replacing missing values with the mean or median value of the field across all
    samples, or the most frequent value for categorical data.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据属性的已知部分推断缺失值。这个过程称为**缺失数据填补**。典型的填补方法包括用所有样本中字段的平均值或中位数替换缺失值，或者对于分类数据，用最频繁出现的值替换。
- en: The first two strategies are simple to implement; however, they come at the
    expense of the data lost, especially when the original dataset is not large enough.
    The third strategy doesn’t abandon any data but does try to fill in the blanks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种策略实现简单；然而，它们牺牲了数据的丢失，特别是当原始数据集不够大时。第三种策略并不放弃任何数据，而是试图填补空缺。
- en: 'Let’s look at how each strategy is applied in an example where we have a dataset
    (age, income) consisting of six samples – (30, 100), (20, 50), (35, *unknown*),
    (25, 80), (30, 70), and (40, 60):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每种策略在一个包含六个样本（年龄、收入）的数据集中如何应用 - (30, 100), (20, 50), (35, *unknown*), (25,
    80), (30, 70), 和 (40, 60)。
- en: If we process this dataset using the first strategy, it becomes (30, 100), (20,
    50), (25, 80), (30, 70), and (40, 60).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用第一种策略处理这个数据集，它将变成 (30, 100), (20, 50), (25, 80), (30, 70), 和 (40, 60)。
- en: If we employ the second strategy, the dataset becomes (30), (20), (35), (25),
    (30), and (40), where only the first field remains.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们采用第二种策略，数据集变成了 (30), (20), (35), (25), (30), 和 (40)，只有第一个字段保留了下来。
- en: If we decide to complete the unknown value instead of skipping it, the sample
    (35, *unknown*) can be transformed into (35, 72) with the mean of the rest of
    the values in the second field, or (35, 70), with the median value in the second
    field.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们决定补全未知值而不是跳过它，例如样本 (35, *unknown*) 可以被转换为 (35, 72)，其中第二个字段的值是其余值的平均值，或者转换为
    (35, 70)，其中第二个字段的值是中位数。
- en: 'In scikit-learn, the `SimpleImputer` class ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))
    provides a nicely written imputation transformer. We can use it for the following
    small example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，`SimpleImputer` 类 ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))
    提供了一个写得很好的填补转换器。我们可以用它来做下面这个小例子：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Represent the unknown value with `np.nan` in `numpy`, as detailed in the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `numpy` 中，用 `np.nan` 表示未知值，如下所示：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Initialize the imputation transformer with the mean value and obtain the mean
    value from the original data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用平均值初始化填补转换器，并从原始数据中获取平均值：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Complete the missing value as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 补全缺失值如下：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, initialize the imputation transformer with the median value, as
    detailed in the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，使用中位数初始化填补转换器，如下所示：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When new samples come in, the missing values (in any attribute) can be imputed
    using the trained transformer, for example, with the mean value, as shown here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当新样本出现时，可以使用训练好的转换器填补缺失值（在任何属性中），例如用平均值，如下所示：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that `30` in the age field is the mean of those six age values in the original
    dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，年龄字段中的 `30` 是原始数据集中这六个年龄值的平均值。
- en: 'Now that we have seen how imputation works, as well as its implementation,
    let’s explore how the strategy of imputing missing values and discarding missing
    data affects the prediction results through the following example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了填补工作的方式及其实施，让我们通过以下示例来探讨填充缺失值和丢弃缺失数据策略如何影响预测结果：
- en: 'First, we load the diabetes dataset, as shown here:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载糖尿病数据集，如下所示：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Simulate a corrupted dataset by adding 25% missing values:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加 25% 的缺失值来模拟一个损坏的数据集：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Randomly select the `m_missing` samples, as follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择`m_missing`个样本，如下所示：
- en: '[PRE8]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For each missing sample, randomly select 1 out of `n` features:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个缺失的样本，随机选择 `n` 个特征中的一个：
- en: '[PRE9]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Represent missing values with `nan`, as shown here:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用`nan`表示缺失值，如下所示：
- en: '[PRE10]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we deal with this corrupted dataset by discarding the samples containing
    a missing value:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过丢弃包含缺失值的样本来处理这个损坏的数据集：
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Measure the effects of using this strategy by estimating the averaged regression
    score *R*², with a regression forest model in a cross-validation manner. Estimate
    *R*² on the dataset with the missing samples removed, as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过交叉验证方式，在移除了缺失样本的数据集上，估计使用这种策略的平均回归分数 *R*²，如下所示：
- en: '[PRE12]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we approach the corrupted dataset differently by imputing missing values
    with the mean, as shown here:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们通过使用均值来填补缺失值来处理这个损坏的数据集，如下所示：
- en: '[PRE13]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Similarly, measure the effects of using this strategy by estimating the averaged
    *R*², as follows:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，通过估计平均 *R*² 来衡量使用这种策略的效果，如下所示：
- en: '[PRE14]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'An imputation strategy works better than discarding in this case. So, how far
    is the imputed dataset from the original full one? We can check it again by estimating
    the averaged regression score on the original dataset, as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，填补策略比丢弃策略更有效。那么，填补后的数据集与原始完整数据集相比有多大差距？我们可以通过在原始数据集上估计平均回归分数来再次检查，如下所示：
- en: '[PRE15]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It turns out that little information is compromised in the imputed dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，在补充后的数据集中丢失了少量信息。
- en: However, there is no guarantee that an imputation strategy always works better,
    and sometimes, dropping samples with missing values can be more effective. Hence,
    it is a great practice to compare the performance of different strategies via
    cross-validation, as we have done previously.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不能保证填补策略总是更好，有时，丢弃带有缺失值的样本可能更有效。因此，通过交叉验证比较不同策略的性能是一个很好的实践，正如我们之前所做的。
- en: Best practice 5 – Storing large-scale data
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 5 – 存储大规模数据
- en: 'With the ever-growing size of data, oftentimes, we can’t simply fit the data
    on our single local machine and need to store it on the cloud or distributed filesystems.
    As this is mainly a book on machine learning with Python, we will just touch on
    some basic areas that you can look into. The two main strategies for storing big
    data are **scale up** and **scale out**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据规模的不断增长，我们往往不能简单地将数据安装在我们的单个本地机器上，需要将其存储在云端或分布式文件系统中。由于这主要是一本关于Python机器学习的书籍，我们只会涉及一些你可以深入了解的基本领域。存储大数据的两种主要策略是
    **扩展** 和 **扩展**：
- en: A **scale-up** approach increases storage capacity if data exceeds the current
    system capacity, such as by adding more disks. This is useful in fast-access platforms.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展** 方法通过增加更多磁盘等方式来增加存储容量，如果数据超出了当前系统容量。这在快速访问平台上非常有用。'
- en: In a **scale-out** approach, storage capacity grows incrementally with additional
    nodes in a storage cluster. Hadoop Distributed File System (HDFS) ([https://hadoop.apache.org/](https://hadoop.apache.org/))
    and Spark ([https://spark.apache.org/](https://spark.apache.org/)) are used to
    store and process big data in scale-out clusters, where data is spread across
    hundreds or even thousands of nodes. Also, there are cloud-based distributed file
    services, such as S3 in Amazon Web Services ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)),
    Google Cloud Storage in Google Cloud ([https://cloud.google.com/storage/](https://cloud.google.com/storage/)),
    and Storage in Microsoft Azure ([https://azure.microsoft.com/en-us/services/storage/](https://azure.microsoft.com/en-us/services/storage/)).
    They are massively scalable and are designed for secure and durable storage.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**扩展性**方法中，存储容量随着存储集群中新增节点的加入而逐步增长。Hadoop分布式文件系统（HDFS）（[https://hadoop.apache.org/](https://hadoop.apache.org/)）和Spark（[https://spark.apache.org/](https://spark.apache.org/)）用于在扩展集群中存储和处理大数据，其中数据分布在成百上千个节点上。此外，还有基于云的分布式文件服务，如亚马逊Web
    Services中的S3（[https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)）、Google
    Cloud中的Google Cloud Storage（[https://cloud.google.com/storage/](https://cloud.google.com/storage/)）和Microsoft
    Azure中的Storage（[https://azure.microsoft.com/en-us/services/storage/](https://azure.microsoft.com/en-us/services/storage/)）。它们具有大规模可扩展性，设计用于安全且持久的存储。
- en: 'Besides choosing the right storage system to increase capacity, you also need
    to pay attention to the following practices:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择合适的存储系统以增加容量外，你还需要关注以下做法：
- en: '**Data partitioning**: Divide your data into smaller partitions or shards.
    This distributes the load across multiple servers or nodes, enabling better parallel
    processing and retrieval.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分区**：将数据划分为更小的分区或碎片。这可以将负载分配到多个服务器或节点，从而实现更好的并行处理和检索。'
- en: '**Data compression and encoding**: Implement data compression techniques to
    reduce storage space and optimize data retrieval times.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据压缩和编码**：实施数据压缩技术以减少存储空间，并优化数据检索时间。'
- en: '**Replication and redundancy**: Replicate data across multiple storage nodes
    or geographical locations to ensure data availability and fault tolerance.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制和冗余**：将数据复制到多个存储节点或地理位置，以确保数据的可用性和容错性。'
- en: '**Security and access control**: Implement robust access control mechanisms
    to ensure that only authorized personnel can access sensitive data.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性和访问控制**：实施强大的访问控制机制，确保只有授权人员能够访问敏感数据。'
- en: With well-prepared data, it is safe to move on to the training set generation
    stage. Let’s see the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备充分的情况下，可以安全地进入训练集生成阶段。让我们来看下一部分。
- en: Best practices in the training set generation stage
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集生成阶段的最佳实践
- en: 'Typical tasks in this stage can be summarized into two major categories: **data
    preprocessing** and **feature engineering**.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段的典型任务可以总结为两个主要类别：**数据预处理**和**特征工程**。
- en: To begin, data preprocessing usually involves categorical feature encoding,
    feature scaling, feature selection, and dimensionality reduction.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，数据预处理通常包括类别型特征编码、特征缩放、特征选择和降维。
- en: Best practice 6 – Identifying categorical features with numerical values
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践6 – 识别具有数值值的类别型特征
- en: In general, categorical features are easy to spot, as they convey qualitative
    information, such as risk level, occupation, and interests. However, it gets tricky
    if the feature takes on a discreet and countable (limited) number of numerical
    values, for instance, 1 to 12 representing months of the year, and 1 and 0 indicating
    true and false.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，类别型特性容易辨认，因为它们传递的是定性信息，如风险等级、职业和兴趣。然而，如果特性具有离散且可计数（有限）数量的数值，例如表示月份的1到12，或者表示真假值的1和0时，判断就会变得复杂。
- en: The key to identifying whether such a feature is categorical or numerical is
    whether it provides a mathematical or ranking implication; if it does, it is a
    numerical feature, such as a product rating from 1 to 5; otherwise, it is categorical,
    such as the month, or day of the week.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 判断某个特性是类别型还是数值型的关键在于它是否提供数学或排名的含义；如果有，则是数值型特性，例如从1到5的产品评分；否则，是类别型特性，例如月份或星期几。
- en: Best practice 7 – Deciding whether to encode categorical features
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践7 – 决定是否对类别型特征进行编码
- en: If a feature is considered categorical, we need to decide whether we should
    encode it. This depends on what prediction algorithm(s) we will use in later stages.
    Naïve Bayes and tree-based algorithms can directly work with categorical features,
    while other algorithms in general cannot, in which case encoding is essential.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个特征被视为类别型特征，我们需要决定是否对其进行编码。这取决于我们在后续阶段将使用哪些预测算法。朴素贝叶斯和基于树的算法可以直接处理类别型特征，而其他算法通常不能，在这种情况下，编码是必需的。
- en: As the output of the feature generation stage is the input of the model training
    stage, *steps taken in the feature generation stage should be compatible with
    the prediction algorithm*. Therefore, we should look at the two stages of feature
    generation and predictive model training as a whole, instead of two isolated components.
    The next two practical tips also reinforce this point.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征生成阶段的输出是模型训练阶段的输入，*特征生成阶段的步骤应与预测算法兼容*。因此，我们应将特征生成和预测模型训练这两个阶段作为一个整体来看待，而不是将其视为两个孤立的部分。接下来的两个实用建议也强调了这一点。
- en: Best practice 8 – Deciding whether to select features and, if so, how to do
    so
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 8 – 决定是否选择特征，如果选择，应该如何操作
- en: 'You have seen, in *Chapter 4*, *Predicting Online Ad Click-Through with Logistic
    Regression*, how feature selection can be performed using L1-based regularized
    logistic regression and random forest. The benefits of feature selection include
    the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*《使用逻辑回归预测在线广告点击率》中，你已经看到了如何使用基于L1正则化的逻辑回归和随机森林进行特征选择。特征选择的好处包括以下几点：
- en: Reducing the training time of prediction models as redundant or irrelevant features
    are eliminated
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低预测模型的训练时间，因为冗余或无关特征已被剔除
- en: Reducing overfitting for the same preceding reason
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于前述原因，减少过拟合
- en: Likely improving performance, as prediction models will learn from data with
    more significant features
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能提升性能，因为预测模型将从具有更重要特征的数据中学习
- en: 'Note that we used the word *likely* because there is no absolute certainty
    that feature selection will increase prediction accuracy. It is, therefore, good
    practice to compare the performances of conducting feature selection and not doing
    so via cross-validation. For example, by executing the following steps, we can
    measure the effects of feature selection by estimating the averaged classification
    accuracy with an `SVC` model in a cross-validation manner:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了*可能*这个词，因为无法绝对确定特征选择一定会提高预测精度。因此，最好通过交叉验证比较进行特征选择与不进行特征选择的表现。例如，通过执行以下步骤，我们可以通过`SVC`模型以交叉验证方式估算特征选择的影响，进而衡量其对分类准确率的平均影响：
- en: 'First, we load the handwritten digits dataset from `scikit-learn`, as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从`scikit-learn`加载手写数字数据集，如下所示：
- en: '[PRE16]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, estimate the accuracy of the original dataset, which is 64-dimensional,
    as detailed here:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，估算原始数据集（64维）的准确率，具体如下所示：
- en: '[PRE17]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, conduct feature selection based on random forest and sort the features
    based on their importance scores:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，基于随机森林进行特征选择，并根据特征的重要性分数对其进行排序：
- en: '[PRE18]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now select a different number of top features to construct a new dataset, and
    estimate the accuracy on each dataset, as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在选择不同数量的前几个特征来构建新数据集，并在每个数据集上估算准确率，具体如下：
- en: '[PRE19]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If we use the top 25 features selected by random forest, the SVM classification
    performance can increase from `0.9` to `0.95`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用随机森林选择的前25个特征，SVM分类性能可以从`0.9`提升到`0.95`。
- en: Best practice 9 – Deciding whether to reduce dimensionality and, if so, how
    to do so
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 9 – 决定是否降维，如果降维，应该如何操作
- en: 'Feature selection and dimensionality are different in the sense that the former
    chooses features from the original data space, while the latter does so from a
    projected space from the original space. Dimensionality reduction has the following
    advantages that are similar to feature selection:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择和降维的区别在于，前者是从原始数据空间中选择特征，而后者则是从原始空间的投影空间中选择特征。降维具有与特征选择相似的以下优点：
- en: Reducing the training time of prediction models, as redundant or correlated
    features are merged into new ones
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低预测模型的训练时间，因为冗余或相关特征已被合并为新的特征
- en: Reducing overfitting for the same reason
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于同样的原因，减少过拟合
- en: Likely improving performance, as prediction models will learn from data with
    less redundant or correlated features
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能提高性能，因为预测模型将从具有较少冗余或相关特征的数据中学习
- en: 'Again, it is not guaranteed that dimensionality reduction will yield better
    prediction results. In order to examine its effects, integrating dimensionality
    reduction in the model training stage is recommended. Reusing the preceding handwritten
    digits example, we can measure the effects of **Principal Component Analysis**
    (**PCA**)-based dimensionality reduction, where we keep a different number of
    top components to construct a new dataset, and estimate the accuracy on each dataset:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，降维并不能保证产生更好的预测结果。为了检验其效果，建议在模型训练阶段集成降维方法。以之前的手写数字示例为例，我们可以衡量基于**主成分分析**（**PCA**）的降维效果，其中我们保留不同数量的主要成分来构建新数据集，并估算每个数据集的准确度：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If we use the top 15 features generated by PCA, the SVM classification performance
    can increase from `0.9` to `0.95`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用PCA生成的前15个特征，SVM分类性能可以从`0.9`提高到`0.95`。
- en: Best practice 10 – Deciding whether to rescale features
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践10 – 决定是否重新缩放特征
- en: As seen in *Chapter 5*, *Predicting Stock Prices with Regression Algorithms*,
    and *Chapter 6*, *Predicting Stock Prices with Artificial Neural Networks*, SGD-based
    linear regression, SVR, and the neural network model require features to be standardized
    by removing the mean and scaling to unit variance. So, when is feature scaling
    needed, and when is it not?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*第5章*《使用回归算法预测股票价格》和*第6章*《使用人工神经网络预测股票价格》中所见，基于SGD的线性回归、SVR和神经网络模型要求特征通过去均值并缩放到单位方差进行标准化。那么，什么时候需要特征缩放，什么时候不需要呢？
- en: In general, Naïve Bayes and tree-based algorithms are not sensitive to features
    at different scales, as they look at each feature independently.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，朴素贝叶斯和基于树的算法对不同尺度的特征不敏感，因为它们独立地看待每个特征。
- en: In most cases, an algorithm that involves any form of distance (or separation
    in spaces) of samples in learning requires scaled/standardized inputs, such as
    SVC, SVR, k-means clustering, and **k-nearest neighbors** (**KNN**) algorithms.
    Feature scaling is also a must for any algorithm using SGD for optimization, such
    as linear or logistic regression with gradient descent, and neural networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，涉及任何形式的样本距离（或空间分离）的学习算法都需要缩放/标准化的输入，如SVC、SVR、k-means聚类和**k近邻**（**KNN**）算法。对于任何使用SGD进行优化的算法（例如带有梯度下降的线性回归或逻辑回归，以及神经网络），特征缩放也是必须的。
- en: We have so far covered tips regarding data preprocessing and will next discuss
    best practices of feature engineering as another major aspect of training set
    generation. We will do so from two perspectives.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了数据预处理的技巧，接下来将讨论特征工程的最佳实践，作为训练集生成的另一个主要方面。我们将从两个角度进行探讨。
- en: Best practice 11 – Performing feature engineering with domain expertise
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践11 – 利用领域专业知识进行特征工程
- en: If we are lucky enough to possess sufficient domain knowledge, we can apply
    it in creating domain-specific features; we utilize our business experience and
    insights to identify what is in the data and formulate new data that correlates
    to the prediction target. For example, in *Chapter 5*, *Predicting Stock Prices
    with Regression Algorithms*, we designed and constructed feature sets for the
    prediction of stock prices based on factors that investors usually look at when
    making investment decisions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们幸运地拥有足够的领域知识，我们可以应用它来创建领域特定的特征；我们利用我们的商业经验和洞察力，识别数据中的信息，并制定与预测目标相关的新数据。例如，在*第5章*《使用回归算法预测股票价格》中，我们根据投资者在做出投资决策时通常关注的因素，设计并构建了用于股票价格预测的特征集。
- en: While particular domain knowledge is required, sometimes we can still apply
    some general tips in this category. For example, in fields related to customer
    analytics, such as marketing and advertising, the time of the day, day of the
    week, and month are usually important signals. Given a data point with the value
    *2020/09/01* in the `Date` column and *14:34:21* in the `Time` column, we can
    create new features including *afternoon*, *Tuesday*, and *September*. In retail,
    information covering a period of time is usually aggregated to provide better
    insights. The number of times a customer visited a store in the past three months,
    or the average number of products purchased weekly in the previous year, for instance,
    can be good predictive indicators for customer behavior prediction.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然特定的领域知识是必需的，但有时我们仍然可以应用一些通用的技巧。例如，在与客户分析相关的领域，如营销和广告，一天中的时间、一周中的日子和月份通常是重要的信号。在`Date`列中给定数据点的值为*2020/09/01*，`Time`列中的值为*14:34:21*，我们可以创建新特征，包括*下午*、*星期二*和*九月*。在零售业中，通常会聚合一段时间内的信息以提供更好的洞察力。例如，过去三个月内客户访问店铺的次数，或者上一年每周平均购买产品的次数，可以成为客户行为预测的良好预测指标。
- en: Best practice 12 – Performing feature engineering without domain expertise
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践12 – 在没有领域专业知识的情况下进行特征工程
- en: If, unfortunately, we have very little domain knowledge, how can we generate
    features? Don’t panic. There are several generic approaches that you can follow,
    such as binarization, discretization, interaction, and polynomial transformation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不幸我们缺乏领域知识，我们如何生成特征？不要惊慌。有几种通用方法可以遵循，例如二值化、离散化、交互和多项式转换。
- en: Binarization and discretization
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二值化和离散化
- en: '**Binarization** is the process of converting a numerical feature to a binary
    one with a preset threshold. For example, in spam email detection, for the feature
    (or term) *prize*, we can generate a new feature, `whether_term_prize_occurs`:
    any term frequency value greater than 1 becomes 1; otherwise, it is 0\. The feature
    *number of visits per week* can be used to produce a new feature, `is_frequent_visitor`,
    by judging whether the value is greater than or equal to 3\. We implement such
    binarization using scikit-learn, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**二值化**是将数值特征转换为二进制特征的过程，具有预设阈值。例如，在垃圾邮件检测中，对于特征（或术语）*prize*，我们可以生成新特征`whether_term_prize_occurs`：任何频率大于1的术语值变为1；否则为0。特征*每周访问次数*可用于生成新特征`is_frequent_visitor`，判断其值是否大于或等于3。我们使用scikit-learn来实现这种二值化，如下所示：'
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Discretization** is the process of converting a numerical feature to a categorical
    feature with limited possible values. Binarization can be viewed as a special
    case of discretization. For example, we can generate an *age group* feature: “*18-24*”
    for ages from 18 to 24, “*25-34*” for ages from 25 to 34, “*34-54*”, and “*55+*”.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散化**是将数值特征转换为具有有限可能值的分类特征的过程。二值化可以看作是离散化的一种特殊情况。例如，我们可以生成*年龄组*特征：“*18-24*”适用于18到24岁的年龄，“*25-34*”适用于25到34岁的年龄，“*34-54*”，和“*55+*”。'
- en: Interaction
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交互
- en: This includes the sum, multiplication, or any operations of two numerical features,
    and the joint condition check of two categorical features. For example, *the number
    of visits per week* and *the number of products purchased per week* can be used
    to generate *the number of products purchased per visit* feature; *interest and
    occupation*, such as *sports* and *engineer*, can form *occupation AND interest*,
    such as *engineer interested in sports*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括两个数值特征的求和、乘积或任何操作，以及两个分类特征的联合条件检查。例如，*每周访问次数*和*每周购买产品数量*可用于生成*每次访问购买产品数量*特征；*兴趣和职业*，如*体育*和*工程师*，可以形成*职业和兴趣*，例如*对体育感兴趣的工程师*。
- en: Polynomial transformation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多项式转换
- en: 'This is the process of generating polynomial and interaction features. For
    two features, *a* and *b*, the two degrees of polynomial features generated are
    *a*², *ab*, and *b*². In scikit-learn, we can use the `PolynomialFeatures` class
    ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html))
    to perform polynomial transformation, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成多项式和交互特征的过程。对于两个特征*a*和*b*，生成的二次多项式特征包括*a*²、*ab*和*b*²。在scikit-learn中，我们可以使用`PolynomialFeatures`类（[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)）来执行多项式转换，如下所示：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note the resulting new features consist of *1* (bias, intercept), *a*, *b*,
    *a*², *ab*, and *b*².
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 13 – Documenting how each feature is generated
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have covered the rules of feature engineering with domain knowledge, and
    in general, there is one more thing worth noting: documenting how each feature
    is generated. It sounds trivial, but oftentimes we just forget about how a feature
    is obtained or created. We usually need to go back to this stage after some failed
    trials in the model training stage and attempt to create more features with the
    hope of improving performance. We have to be clear on what and how features are
    generated, in order to remove those that do not quite work out, and to add new
    ones that have more potential.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 14 – Extracting features from text data
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start with a traditional approach to extract features from text, tf,
    and tf-idf. Then, we will continue with a modern approach: word embedding. Specifically,
    we will look at word embedding using `Word2Vec` models, and embedding layers in
    neural network models.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: tf and tf-idf
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have worked intensively with text data in *Chapter 7*, *Mining the 20 Newsgroups
    Dataset with Text Analysis Techniques*, and *Chapter 8*, *Discovering Underlying
    Topics in the Newsgroups Dataset with Clustering and Topic Modeling*, where we
    extracted features from text based on **term frequency** (**tf**) and **term frequency-inverse
    document frequency** (**tf-idf**). Both methods consider each document of words
    (terms) a collection of words, or a **bag of words** (**BoW**), disregarding the
    order of the words but keeping multiplicity. A tf approach simply uses the counts
    of tokens, while tf-idf extends tf by assigning each tf a weighting factor that
    is inversely proportional to the document frequency. With the idf factor incorporated,
    tf-idf diminishes the weight of common terms (such as “get” and “make”) that occur
    frequently, and emphasizes terms that rarely occur but convey important meaning.
    Hence, oftentimes, features extracted from tf-idf are more representative than
    those from tf.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: As you may remember, a document is represented by a very sparse vector where
    only present terms have non-zero values. The vector’s dimensionality is usually
    high, which is determined by the size of the vocabulary and the number of unique
    terms. Also, such a one-hot encoding approach treats each term as an independent
    item and does not consider the relationship across words (referred to as “context”
    in linguistics).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the contrary, another approach, called **word embedding**, is able to capture
    the meanings of words and their context. In this approach, a word is represented
    by a vector of float numbers. Its dimensionality is a lot lower than the size
    of the vocabulary and is usually several hundred only.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The embedding vectors are of real values, where each dimension encodes an aspect
    of meaning for the words in the vocabulary. This helps preserve the semantic information
    of the words, as opposed to discarding it as in the one-hot encoding approach
    using tf or tf-idf. An interesting phenomenon is that vectors from semantically
    similar words are proximate to each other in geometric space. For example, both
    the words *clustering and grouping* refer to unsupervised clustering in the context
    of machine learning, hence their embedding vectors are close together.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some popular ways to obtain word embeddings:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '**Word2Vec**: Train your own Word2Vec embeddings on your specific corpus using
    the Skip-gram or Continuous Bag of Words (CBOW) models. We covered this in *Chapter
    7*, *Mining the 20 Newsgroups Dataset with Text Analysis Techniques*. Libraries
    like Gensim in Python provide easy-to-use interfaces for training Word2Vec embeddings.
    We will present a simple example shortly.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-trained embeddings**: Use pre-trained word embeddings that are trained
    on large corpora. We also talked about this in *Chapter 7*. Popular examples include:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FastText
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GloVe** (**Global Vectors for Word Representation**)'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BERT** (**Bidirectional Encoder Representations from Transformers**)'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT** (**Generative Pre-trained Transformer**)'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**USE** (**Universal Sentence Encoder**) embeddings'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training custom models with an embedding layer**: If you have a specific
    domain or dataset, you can train your own word embeddings using custom neural
    network models.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec embedding
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prior to delving into training a custom model for word embeddings, let’s begin
    with the following example of training a basic `Word2Vec` model using `gensim`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the `gensim` module:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We define some sample sentences for training:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In practice, you will need to format the sentences in plain text into a list
    of word lists just like the `sentences` object.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create a Word2Vec model with various parameters, such as `vector_size`
    (embedding dimension), `window` (context window size), `min_count` (minimum frequency
    of words), and `sg` (training algorithm – `0` for CBOW, `1` for Skip-gram):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After training, we access word vectors using the model’s `wv` property. Here,
    we display the embedding vector for the word *machine:*
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Keep in mind that this is a basic example. In practice, you might need to preprocess
    your data more thoroughly, adjust hyperparameters, and train on a larger corpus
    for better embeddings.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Embedding layers in custom neural networks
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a complete deep neural network for NLP tasks, we would typically combine
    an embedding layer with other layers, like fully connected (dense) layers, or
    recurrent layers (we will talk about recurrent layers in *Chapter 12**, Making
    Predictions with Sequences Using Recurrent Neural Networks*) to build a more sophisticated
    model. The embedding layer allows the network to learn meaningful representations
    for words in the input data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simplified example of using an embedding layer for word embeddings.
    In PyTorch, we use the `nn.Embedding` module ([https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html))
    for embedding layers:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this example, we first import the necessary modules from PyTorch. We define
    some sample input data containing word indices (for example, 1 represents *I*,
    2 represents *love*, 3 represents *machine*, and 4 represents *learning*). Then,
    we define the embedding layer using `nn.Embedding` with `vocab_size` as the total
    number of unique words in the vocabulary, and `embedding_dim` as the desired dimensionality
    of the embeddings. The embedding layer is usually the first layer of a neural
    network model after the input layer. Upon completion of network training, when
    we pass the input data through the embedding layer, it returns embedded vectors
    for each input word index. The shape of output `embedded_data` will be `(sample
    size, sequence length, embedding_dim)`, which is `(2, 4, 3)` in our case.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Once again, this is a simplified example. In practice, the embedding layers
    are involved in more complex architectures with additional layers, in order to
    process and interpret the embeddings for specific tasks, such as classification,
    sentiment analysis, or sequence generation. Keep an eye out for the upcoming *Chapter
    12*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Curious about the choice between tf-idf and word embeddings? In conventional
    NLP applications, such as simple text classification and topic modeling, tf, or
    tf-idf, remains an exceptional method for feature extraction. In more complicated
    areas, such as text summarization, machine translation, named entity resolution,
    question answering, and information retrieval, word embeddings are extensively
    utilized and yield significantly enhanced features compared to conventional methods.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have reviewed the best practices for data and feature generation,
    let’s look at model training next.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in the model training, evaluation, and selection stage
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a supervised machine learning problem, the first question many people
    ask is usually *What is the best classification or regression algorithm to solve
    it?* However, there is no one-size-fits-all solution and no free lunch. No one
    could know which algorithm will work best before trying multiple ones and fine-tuning
    the optimal one. We will be looking into best practices around this in this section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 15 – Choosing the right algorithm(s) to start with
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the fact that there are several parameters to tune for an algorithm,
    exhausting all algorithms and fine-tuning each one can be extremely time-consuming
    and computationally expensive. We should instead shortlist one to three algorithms
    to start with using the general guidelines that follow (note we herein focus on
    classification, but the theory transcends to regression, and there is usually
    a counterpart algorithm in regression).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several things we need to be clear about before shortlisting potential
    algorithms, as described in the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: The size of the training dataset
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dimensionality of the dataset
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the data is linearly separable
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether features are independent
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolerance and trade-off of bias and variance
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether online learning is required
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at how we choose the right algorithm to start with, taking into
    account the aforementioned perspectives.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a very simple algorithm. For a relatively small training dataset, if
    features are independent, Naïve Bayes will usually perform well. For a large dataset,
    Naïve Bayes will still work well as feature independence can be assumed in this
    case, regardless of the truth. The training of Naïve Bayes is usually faster than
    any other algorithm due to its computational simplicity. However, this may lead
    to a high bias (but low variance).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is probably the most widely used classification algorithm, and the first
    algorithm that a machine learning practitioner usually tries when given a classification
    problem. It performs well when data is linearly separable or approximately **linearly
    separable**. Even if it is not linearly separable, it might be possible to convert
    the linearly non-separable features into separable ones and apply logistic regression
    afterward.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following instance, data in the original space is not linearly separable,
    but it becomes separable in a transformed space created from the interaction of
    two features:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated with low
    confidence](img/B21047_10_02.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Transforming features from linearly non-separable to separable'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Also, logistic regression is extremely scalable to large datasets with SGD optimization,
    which makes it efficient in solving big data problems. Plus, it makes online learning
    feasible. Although logistic regression is a low-bias, high-variance algorithm,
    we overcome the potential overfitting by adding L1, L2, or a mix of the two regularizations.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: SVM
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is versatile enough to adapt to the linear separability of data. For a
    separable dataset, SVM with a linear kernel performs comparably to logistic regression.
    Beyond this, SVM also works well for a non-separable dataset if equipped with
    a non-linear kernel, such as RBF. Logistic regression may face challenges in high-dimensional
    datasets, while SVM still performs well. A good example of this can be in news
    classification, where the feature dimensionality is in the tens of thousands.
    In general, very high accuracy can be achieved by SVM with the right kernel and
    parameters. However, this might be at the expense of intense computation and high
    memory consumption.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Random forest (or decision tree)
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear separability of the data does not matter to this algorithm, and it
    works directly with categorical features without encoding, which provides great
    ease of use. Also, the trained model is very easy to interpret and explain to
    non-machine learning practitioners, which cannot be achieved with most other algorithms.
    Additionally, random forest boosts the decision tree algorithm, which can reduce
    overfitting by ensembling a collection of separate trees. Its performance is comparable
    to SVM, while fine-tuning a random forest model is less difficult compared to
    SVM and neural networks.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are extremely powerful, especially with the development of deep learning.
    However, finding the right topology (layers, nodes, activation functions, and
    so on) is not easy, not to mention the time-consuming model of training and tuning.
    Hence, they are not recommended as an algorithm to start with for general machine
    learning problems. However, for computer vision and many NLP tasks, the neural
    network is still the go-to model. In summary, here are some scenarios where using
    neural networks is particularly beneficial:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '**Complex patterns**: When the task involves learning complex patterns or relationships
    within the data that may be difficult for traditional algorithms to capture.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large amounts of data**: Neural networks tend to perform well when you have
    a substantial amount of data available for training, as they are capable of learning
    from large datasets.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unstructured data**: Neural networks excel in handling unstructured data
    types like images, audio, and text, where traditional methods might struggle to
    extract meaningful features. For NLP tasks like sentiment analysis, machine translation,
    named entity recognition, and text generation, neural networks, especially recurrent
    and transformer models, have shown remarkable performance. In image classification,
    object detection, segmentation, and image generation tasks, deep neural networks
    have revolutionized computer vision.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practice 16 – Reducing overfitting
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We touched on ways to avoid overfitting when discussing the pros and cons of
    algorithms in the last practice. We herein formally summarize them, as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '**More data, if possible**: Increase the size of your training dataset. More
    data can help the model learn relevant patterns and reduce its tendency to memorize
    noise.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplification, if possible**: The more complex the model is, the higher
    the chance of overfitting. Complex models include a tree or forest with excessive
    depth, a linear regression with a high degree of polynomial transformation, and
    an SVM with a complicated kernel.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-validation**: A good habit that we have built over all of the chapters
    in this book.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: This adds penalty terms to reduce the error caused by fitting
    the model perfectly on the given training set.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: Monitor the model’s performance on a validation set during
    training. Stop training when the performance on the validation set starts to degrade,
    indicating that the model is starting to overfit.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: In neural networks, apply dropout layers during training. Dropout
    randomly drops out a fraction of neurons during each forward pass, preventing
    reliance on specific neurons.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: Select a subset of relevant features. Removing irrelevant
    or redundant features can prevent the model from fitting noise.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble learning**: This involves combining a collection of weak models
    to form a stronger one.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, how can we tell whether a model suffers from overfitting, or the other extreme,
    underfitting? Let’s see the next section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 17 – Diagnosing overfitting and underfitting
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **learning curve** is usually used to evaluate the bias and variance of a
    model. A learning curve is a graph that compares the cross-validated training
    and testing scores over a given number of training samples.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: For a model that fits well on the training samples, the performance of the training
    samples should be beyond what’s desired. Ideally, as the number of training samples
    increases, the model performance on the testing samples will improve; eventually,
    the performance on the testing samples will become close to that of the training
    samples.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: When the performance on the testing samples converges at a value much lower
    than that of the training performance, overfitting can be concluded. In this case,
    the model fails to generalize to instances that have not been seen.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'For a model that does not even fit well on the training samples, underfitting
    is easily spotted: both performances on the training and testing samples are below
    the desired performance in the learning curve.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the learning curve in an ideal case:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, line, diagram  Description automatically
    generated](img/B21047_10_03.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Ideal learning curve'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of the learning curve for an overfitted model is shown in the following
    diagram:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of training data  Description automatically generated with low
    confidence](img/B21047_10_04.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Overfitting learning curve'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning curve for an underfitted model may look like the following diagram:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, line, diagram  Description automatically
    generated](img/B21047_10_05.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Underfitting learning curve'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: To generate the learning curve, you can utilize the `learning_curve` module
    ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve))
    from scikit-learn, and the `plot_learning_curve` function defined at [https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 18 – Modeling on large-scale datasets
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We gained experience working with large datasets in *Chapter 4*, *Predicting
    Online Ad Click-Through with Logistic Regression*. There are a few tips that can
    help you model on large-scale data more efficiently.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: First, start with a small subset, for instance, a subset that can fit on your
    local machine. This can help speed up early experimentation. Obviously, you don’t
    want to train on the entire dataset just to find out whether SVM or random forest
    works better. Instead, you can randomly sample data points and quickly run a few
    models on the selected set.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The second tip is choosing scalable algorithms, such as logistic regression,
    linear SVM, and SGD-based optimization. This is quite intuitive.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are other best practices for modeling on large-scale datasets:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '**Sampling and subset selection**: When starting model development, work with
    smaller subsets of your data to iterate and experiment quickly. Once your model
    architecture and parameters are tuned, scale up to the full dataset.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed computing**: Utilize distributed computing frameworks like Apache
    Spark to handle large-scale data processing and model training across multiple
    nodes or clusters.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Focus on relevant features and avoid unnecessary dimensions.
    Use dimensionality reduction techniques like PCA or t-SNE to reduce feature space
    if needed.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelization**: Explore techniques to parallelize training, like data
    parallelism or model parallelism, to leverage multiple GPUs or distributed systems.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory management**: Optimize memory usage by using data generators, streaming
    data from storage, and releasing memory when no longer needed.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimized libraries**: Choose libraries and frameworks that are optimized
    for large-scale data, such as TensorFlow, PyTorch, scikit-learn, and XGBoost.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incremental learning**: For streaming data or dynamic datasets, consider
    incremental learning techniques that update the model as new data arrives.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, don’t forget to save the trained model. Training on a large
    dataset takes a long time, which you would want to avoid redoing, if possible.
    We will explore saving and loading models in detail in *Best practice 19 – Saving,
    loading, and reusing models*, which is a part of the deployment and monitoring
    stage.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in the deployment and monitoring stage
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After performing all processes in the previous three stages, we now have a well-established
    data preprocessing pipeline and a correctly trained prediction model. The last
    stage of a machine learning system involves saving those resulting models from
    previous stages and deploying them on new data, as well as monitoring their performance
    and updating the prediction models regularly. We also need to implement monitoring
    and logging to track model performance, training progress, and potential issues
    during training.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 19 – Saving, loading, and reusing models
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When machine learning is deployed, new data should go through the same data
    preprocessing procedures (scaling, feature engineering, feature selection, dimensionality
    reduction, and so on) as in the previous stages. The preprocessed data is then
    fed into the trained model. We simply cannot rerun the entire process and retrain
    the model every time new data comes in. Instead, we should save the established
    preprocessing models and trained prediction models after the corresponding stages
    have been completed. In deployment mode, these models are loaded in advance and
    are used to produce prediction results from the new data. Let’s explore methods
    for saving and loading models using pickle, TensorFlow, and PyTorch below.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Saving and restoring models using pickle
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start with using `pickle`. This can be illustrated via the diabetes example,
    where we standardize the data and employ an `SVR` model, as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Preprocess the training data with scaling, as shown in the following commands:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now save the established standardizer, the `scaler` object with `pickle`, as
    follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This generates a `scaler.p` file.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Move on to training an `SVR` model on the scaled data, as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Save the trained `regressor` object with `pickle`, as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This generates a `regressor.p` file.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'In the deployment stage, we first load the saved standardizer and the `regressor`
    object from the preceding two files, as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, we preprocess the new data using the standardizer and make a prediction
    with the `regressor` object just loaded, as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Saving and restoring models in TensorFlow
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I will also demonstrate how to save and restore models in TensorFlow. As an
    example, we will train a simple logistic regression model on the cancer dataset,
    save the trained model, and reload it in the following steps:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary TensorFlow modules and load the cancer dataset from `scikit-learn`
    and rescale the data:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Build a simple logistic regression model using the Keras Sequential API, along
    with several specified parameters:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Train the TensorFlow model against the data:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Display the model’s architecture:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We will see if we can retrieve the same model later.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, the previous steps look familiar to you. If not, feel free to review
    our TensorFlow implementation. Now we save the model to a path:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: After this, you will see that a folder called `model_tf` is created. The folder
    contains the trained model’s architecture, weights, and training configuration.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we load the model from the previous path and display the loaded model’s
    path:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We just loaded back the exact same model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Saving and restoring models in PyTorch
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, let’s see how to save and restore models in PyTorch. Similarly, we
    will train a simple logistic regression model on the same cancer dataset, save
    the trained model, and reload it in the following steps:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the data `torch` tensors used for modeling:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Build a simple logistic regression model using the `nn.sequential` module,
    along with the loss function and optimizer:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Reuse the `train_step` function we developed previously in *Chapter 6*, *Predicting
    Stock Prices with Artificial Neural Networks*, and train the `PyTorch` model against
    the data for 10 iterations:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Display the model’s architecture:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We will see if we can retrieve the same model later.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, the previous steps look familiar to you. If not, feel free to review
    our PyTorch implementation. Now we save the model to a path:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: After this, you will see that a folder called `model.pth` is created. The folder
    contains the entire trained model’s architecture, weights, and training configuration.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we load the model from the previous path and display the loaded model’s
    path:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We just loaded back the exact same model.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 20 – Monitoring model performance
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The machine learning system is now up and running. To make sure everything
    is on the right track, we need to conduct performance checks on a regular basis.
    To do so, besides making a prediction in real time, we should also record the
    ground truth at the same time. Here are some best practices for monitoring model
    performance:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '**Define evaluation metrics**: Choose appropriate evaluation metrics that align
    with your problem’s goals. Accuracy, precision, recall, F1-score, AUC-ROC, *R*²,
    and mean squared error are some common metrics.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Baseline performance**: Establish a baseline model or a simple rule-based
    approach to compare your model’s performance. This provides context for understanding
    whether your model is adding value.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning curves**: Plot learning curves showing training and validation loss
    or evaluation metrics over epochs. This helps identify overfitting or underfitting
    issues, as mentioned in *Best practice 17 – Diagnosing overfitting and underfitting*.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Continuing with the diabetes example from earlier in the chapter, we conduct
    a performance check as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We should log the performance and set up an alert for any decayed performance.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Best practice 21 – Updating models regularly
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the performance is getting worse, chances are that the pattern of data has
    changed. We can work around this by updating the model. Depending on whether online
    learning is feasible or not with the model, the model can be modernized with the
    new set of data (online updating) or retrained completely with the most recent
    data. Here are some best practices for the last section of the chapter:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitor model performance**: Continuously monitor model performance metrics.
    If there’s a significant drop, it’s a sign that the model needs updating.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduled updates**: Implement a schedule for model updates based on the
    frequency of data changes and business needs. This ensures that the model remains
    relevant, without unnecessary updates.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online updating**: For models that support online learning, update the model
    incrementally with new data. This applies to models based on gradient descent
    algorithms, or Naïve Bayes. Online updating minimizes the need for retraining
    the entire model and adapts it to changing patterns over time.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version control**: Maintain version control of models and datasets to track
    changes and facilitate rollback if necessary. This helps in comparing model performance
    over time and reverting to previous versions if updates lead to performance degradation.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular auditing**: Periodically review model performance, reevaluate business
    goals, and update your evaluation metrics if needed.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that monitoring should be an ongoing process, starting from model development
    through deployment and maintenance. It ensures that your machine learning models
    remain effective, trustworthy, and aligned with your business objectives.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The purpose of this chapter is to prepare you for real-world machine learning
    problems. We started with the general workflow that a machine learning solution
    follows: data preparation, training set generation, algorithm training, evaluation
    and selection, and finally, system deployment and monitoring. We then went, in
    depth, through the typical tasks, common challenges, and best practices for each
    of these four stages.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Practice makes perfect. The most important best practice is practice itself.
    Get started with a real-world project to deepen your understanding and apply what
    you have learned so far.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start our deep learning journey by categorizing
    clothing images using convolutional neural networks.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you use word embedding to extract text features and develop a multiclass
    classifier to classify the newsgroup data? (Note that you might not be able to
    get better results with word embedding than tf-idf, but it is good practice.)
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you find several challenges in Kaggle ([www.kaggle.com](https://www.kaggle.com))
    and practice what you have learned throughout the entire book?
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1878468721786989681.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG

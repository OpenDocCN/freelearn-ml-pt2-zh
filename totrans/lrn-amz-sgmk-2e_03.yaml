- en: 'Chapter 2: Handling Data Preparation Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is the starting point of any machine learning project, and it takes lots
    of work to turn data into a dataset that can be used to train a model. That work
    typically involves annotating datasets, running bespoke scripts to preprocess
    them, and saving processed versions for later use. As you can guess, doing all
    this work manually, or building tools to automate it, is not an exciting prospect
    for machine learning teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about AWS services that help you build and
    process data. We''ll first cover **Amazon SageMaker Ground Truth**, a capability
    of Amazon SageMaker that helps you quickly build accurate training datasets. Then,
    we''ll introduce **Amazon SageMaker Data Wrangler**, a new way to transform your
    data interactively. Next, we''ll talk about **Amazon SageMaker Processing**, another
    capability that helps you run your data processing workloads, such as feature
    engineering, data validation, model evaluation, and model interpretation. Finally,
    we''ll quickly discuss other AWS services that may help with data analytics: **Amazon
    Elastic Map Reduce**, **AWS Glue**, and **Amazon Athena**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter consists of the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Labeling data with Amazon SageMaker Ground Truth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming data with Amazon SageMaker Data Wrangler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running batch jobs with Amazon SageMaker Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS Free Tier , which
    lets you use many AWS services for free within certain usage limits.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to install and to configure the AWS **Command Line Interface**
    (**CLI**) for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory, but
    strongly encouraged as it includes many projects that we will need (Jupyter, `pandas`,
    `numpy`, and more).
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Labeling data with Amazon SageMaker Ground Truth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Added to Amazon SageMaker in late 2018, Amazon SageMaker Ground Truth helps
    you quickly build accurate training datasets. Machine learning practitioners can
    distribute labeling work to public and private workforces of human labelers. Labelers
    can be productive immediately, thanks to built-in workflows and graphical interfaces
    for common image, video, and text tasks. In addition, Ground Truth can enable
    automatic labeling, a technique that trains a machine learning model able to label
    data without additional human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you'll learn how to use Ground Truth to label images and text.
  prefs: []
  type: TYPE_NORMAL
- en: Using workforces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in using Ground Truth is to create a workforce, a group of workers
    in charge of labeling data samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s head out to the SageMaker console: in the left-hand vertical menu, we
    click on **Ground Truth**, then on **Labeling workforces**. Three types of workforces
    are available: **Amazon Mechanical Turk**, **Vendor**, and **Private**. Let''s
    discuss what they are, and when you should use them.'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Mechanical Turk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Amazon Mechanical Turk** ([https://www.mturk.com/](https://www.mturk.com/))
    makes it easy to break down large batch jobs into small work units that can be
    processed by a distributed workforce.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With Mechanical Turk, you can enroll tens or even hundreds of thousands of
    workers located across the globe. This is a great option when you need to label
    extremely large datasets. For example, think about a dataset for autonomous driving,
    made up of 1,000 hours of video: each frame would need to be processed in order
    to identify other vehicles, pedestrians, road signs, and more. If you wanted to
    annotate every single frame, you''d be looking at 1,000 hours x 3,600 seconds
    x 24 frames per second = **86.4 million images**! Clearly, you would have to scale
    out your labeling workforce to get the job done, and Mechanical Turk lets you
    do that.'
  prefs: []
  type: TYPE_NORMAL
- en: Vendor workforce
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As scalable as Mechanical Turk is, sometimes you need more control on who data
    is shared with, and on the quality of annotations, particularly if additional
    domain knowledge is required.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, AWS has vetted a number of data labeling companies, which
    have integrated Ground Truth in their workflows. You can find the list of companies
    on **AWS Marketplace** ([https://aws.amazon.com/marketplace/](https://aws.amazon.com/marketplace/)),
    under **Machine Learning** | **Data Labeling Services** | **Amazon SageMaker Ground
    Truth Services**.
  prefs: []
  type: TYPE_NORMAL
- en: Private workforce
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, data can't be processed by third parties. Maybe it's just too sensitive,
    or maybe it requires expert knowledge that only your company's employees have.
    In this case, you can create a private workforce made up of well-identified individuals
    that will access and label your data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a private workforce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating a private workforce is the quickest and simplest option. Let''s see
    how it''s done:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the **Labeling workforces** entry in the SageMaker console, we
    select the **Private** tab, as seen in the following screenshot. Then, we click
    on **Create private team**:![Figure 2.1 – Creating a private workforce
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_001.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.1 – Creating a private workforce
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We give the team a name, then we have to decide whether we're going to invite
    workers by email, or whether we're going to import users that belong to an existing
    **Amazon Cognito** group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Cognito ([https://aws.amazon.com/cognito/](https://aws.amazon.com/cognito/))
    is a managed service that lets you build and manage user directories at any scale.
    Cognito supports both social identity providers (Google, Facebook, and Amazon),
    and enterprise identity providers (Microsoft Active Directory, SAML).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This makes a lot of sense in an enterprise context, but let''s keep things
    simple and use email instead. Here, I will use some sample email addresses: please
    make sure to use your own, otherwise you won''t be able to join the team!'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we need to enter an organization name, and more importantly a contact
    email that workers can use for questions and feedback on the labeling job. These
    conversations are extremely important in order to fine-tune labeling instructions,
    pinpoint problematic data samples, and more.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, we can set up notifications with **Amazon Simple Notification Service**
    ([https://aws.amazon.com/sns/](https://aws.amazon.com/sns/)) to let workers know
    that they have work to do.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The screen should look like in the following screenshot. Then, we click on **Create
    private team**:![Figure 2.2 – Setting up a private workforce
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.2 – Setting up a private workforce
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A few seconds later, the team has been set up. Invitations have been sent to
    workers, requesting that they join the workforce by logging in to a specific URL.
    The invitation email looks like that shown in the following screenshot:![Figure
    2.3 – Email invitation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.3 – Email invitation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Clicking on the link opens a login window. Once we''ve logged in and defined
    a new password, we''re taken to a new screen showing available jobs, as in the
    following screenshot. As we haven''t defined one yet, it''s obviously empty:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Worker console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_02_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Worker console
  prefs: []
  type: TYPE_NORMAL
- en: Let's keep our workers busy and create an image labeling job.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading data for labeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you would expect, Amazon SageMaker Ground Truth uses Amazon S3 to store
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the AWS CLI, we create an S3 bucket hosted in the same region we''re
    running SageMaker in. Bucket names are globally unique, so please make sure to
    pick your own unique name when you create the bucket. Use the following code (feel
    free to use another AWS Region):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we copy the cat images located in the `chapter2` folder of our GitHub
    repository as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have some data waiting to be labeled, let's create a labeling job.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a labeling job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you would expect, we need to define the location of the data, what type
    of task we want to label it for, and what our instructions are:'
  prefs: []
  type: TYPE_NORMAL
- en: In the left-hand vertical menu of the SageMaker console, we click on **Ground
    Truth**, then on **Labeling jobs**, then on the **Create labeling job** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we give the job a name, say ''`my-cat-job`''. Then, we define the location
    of the data in S3\. Ground Truth expects a **manifest file**: a manifest file
    is a **JSON** file that lets you filter which objects need to be labeled, and
    which ones should be left out. Once the job is complete, a new file, called the
    augmented manifest, will contain labeling information, and we''ll be able to use
    this to feed data to training jobs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we define the location and the type of our input data, just like in the
    following screenshot:![Figure 2.5 – Configuring input data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.5 – Configuring input data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As is visible in the next screenshot, we select the IAM role that we created
    for SageMaker in the first chapter (your name will be different), and we then
    click on the **Complete data setup** button to validate this section:![Figure
    2.6 – Validating input data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.6 – Validating input data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clicking on **View more details**, you can learn about what is happening under
    the hood. SageMaker Ground Truth crawls your data in S3 and creates a JSON file
    called the **manifest file**. You can go and download it from S3 if you're curious.
    This file points at your objects in S3 (images, text files, and so on).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Optionally, we could decide to work either with the full manifest, a random
    sample, or a filtered subset based on a **SQL** query. We could also provide an
    **Amazon KMS** key to encrypt the output of the job. Let's stick to the defaults
    here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Task type** section asks us what kind of job we'd like to run. Please
    take a minute to explore the different task categories that are available (text,
    image, video, point cloud, and custom). As shown in the next screenshot, let's
    select the **Image** task category and the **Semantic segmentation** task, and
    then click **Next**:![Figure 2.7 – Selecting a task type
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.7 – Selecting a task type
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the next screen, visible in the following screenshot, we first select our
    private team of workers:![Figure 2.8 – Selecting a team type
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.8 – Selecting a team type
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we had a lot of samples (say, tens of thousands or more), we should consider
    enabling **automated data labeling**, as this feature would reduce both the duration
    and the cost of the labeling job. Indeed, as workers would start labeling data
    samples, SageMaker Ground Truth would train a machine learning model on these
    samples. It would use them as a dataset for a supervised learning problem. With
    enough worker-labeled data, this model would pretty quickly be able to match and
    exceed human accuracy, at which point it would replace workers and label the rest
    of the dataset. If you'd like to know more about this feature, please read the
    documentation at [https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last step in configuring our training job is to enter instructions for the
    workers. This is an important step, especially if your job is distributed to third-party
    workers. The better our instructions, the higher the quality of the annotations.
    Here, let's explain what the job is about, and enter a "cat" label for workers
    to apply. In a real-life scenario, you should add detailed instructions, provide
    sample images for good and bad examples, explain what your expectations are, and
    so on. The following screenshot shows what our instructions look like:![Figure
    2.9 – Setting up instructions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.9 – Setting up instructions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once we're done with instructions, we click on **Create** to launch the labeling
    job. After a few minutes, the job is ready to be distributed to workers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Labeling images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logging in to the worker URL, we can see from the screen shown in the following
    screenshot that we have work to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Worker console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_02_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 – Worker console
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on **Start working** opens a new window, visible in the next picture.
    It displays instructions as well as a first image to work on:![Figure 2.11 – Labeling
    images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.11 – Labeling images
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the graphical tools in the toolbar, and especially the auto-segment tool,
    we can very quickly produce high-quality annotations. Please take a few minutes
    to practice, and you'll be able to do the same in no time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we''re done with the three images, the job is complete, and we can visualize
    the labeled images under **Labeling jobs** in the SageMaker console. Your screen
    should look like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Labeled images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_02_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 – Labeled images
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, we can find labeling information in the S3 output location.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the `output/my-cat-job/manifests/output/output.manifest`) contains
    annotation information on each data sample, such as the classes present in the
    image, and a link to the segmentation mask.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091)*, Training
    Computer Vision Models*, we'll see how we can feed this information directly to
    the built-in computer vision algorithms implemented in Amazon SageMaker. Of course,
    we could also parse this information, and convert it for whatever framework we
    use to train our computer vision model.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, SageMaker Ground Truth makes it easy to label image datasets.
    You just need to upload your data to S3 and create a workforce. Ground Truth will
    then distribute the work automatically, and store the results in S3.
  prefs: []
  type: TYPE_NORMAL
- en: We just saw how to label images, but what about text tasks? Well, they're equally
    easy to set up and run. Let's go through an example.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a quick example of labeling text for named entity recognition. The dataset
    is made up of text fragments from one of my blog posts, where we'd like to label
    all AWS service names. These are available in our GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start labeling text using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s upload text fragments to S3 with the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Just like in the previous example, we configure a text labeling job, set up
    input data, and select an IAM role, as shown in the following screenshot:![Figure
    2.13 – Creating a text labeling job
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.13 – Creating a text labeling job
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we select **Text** as the category, and **Named entity recognition** as
    the task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, shown in the following screenshot, we simply select our
    private team again, add a label, and enter instructions:![Figure 2.14 – Setting
    up instructions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_014.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.14 – Setting up instructions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the job is ready, we log in to the worker console and start labeling. You
    can see a labeled example in the following screenshot:![Figure 2.15 – Labeling
    text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.15 – Labeling text
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We're done quickly, and we can find the labeling information in our S3 bucket.
    For each sample, we see a start offset, an end offset, and a label for each labeled
    entity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon SageMaker Ground Truth really makes it easy to label datasets at scale.
    It has many nice features including job chaining and custom workflows, which I
    encourage you to explore at [https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to label datasets, let's see how we can easily transform
    data interactively with Amazon SageMaker Data Wrangler.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data with Amazon SageMaker Data Wrangler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Collecting and labeling data samples is only the first step in preparing a
    dataset. Indeed, it''s very likely that you''ll have to pre-process your dataset
    in order to do the following, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert it to the input format expected by the machine learning algorithm you're
    using.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rescale or normalize numerical features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineer higher-level features, for example, one-hot encoding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean and tokenize text for natural language processing applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the early stage of a machine learning project, it's not always obvious which
    transformations are required, or which ones are most efficient. Thus, practioners
    often need to experiment with lots of different combinations, transforming data
    in many different ways, training models, and evaluating results.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we're going to learn about **Amazon SageMaker Data Wrangler**,
    a graphical interface integrated in SageMaker Studio that makes it very easy to
    transform data, and to export results to a variety of Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a dataset in SageMaker Data Wrangler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need a dataset. We'll use the direct marketing dataset published by
    S. Moro, P. Cortez, and P. Rita in "A Data-Driven Approach to Predict the Success
    of Bank Telemarketing", *Decision Support Systems*, Elsevier, 62:22-31, June 2014.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset describes a binary classification problem: will a customer accept
    a marketing offer, yes or no? It contains a little more than 41,000 customer samples,
    and labels are stored in the **y** column.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will get started using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the AWS command line, let''s download the dataset, extract it, and copy
    it to the default SageMaker bucket for the region we''re running in (it should
    have been created automatically). You can run this on your local machine or in
    a Jupyter terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In SageMaker Studio, we create a new Data Wrangler flow with **File** | **New**
    | **Data Wrangler Flow** to create. The following screenshot shows the Data Wrangler
    image being loaded:![Figure 2.16 – Loading Data Wrangler
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_016.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.16 – Loading Data Wrangler
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once Data Wrangler is ready, the **Import** screen opens. We also see the Data
    Wrangler image in the left-hand pane, as shown in the next screenshot:![Figure
    2.17 – Opening Data Wrangler
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_017.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.17 – Opening Data Wrangler
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can import data from S3, Athena or Redshift (by clicking on **Add data source**).
    Here, we click on S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As shown in the following screenshot, we can easily locate the dataset that
    we just uploaded. Let's click on it.![Figure 2.18 – Locating a dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_018.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.18 – Locating a dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This opens a preview of the dataset, as shown in the next screenshot:![Figure
    2.19 – Previewing a dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_019.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.19 – Previewing a dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's just click on **Import**, which opens the **Prepare** view, as shown in
    the next screenshot:![Figure 2.20 – Previewing a dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_020.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.20 – Previewing a dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clicking on the **+** icon, we could add more data sources, joining them or
    concatenating them to our dataset. We could also edit data types for all columns,
    should Data Wrangler have detected them incorrectly. Instead, let's select **Add
    analysis** to visualize properties of our dataset. This opens the **Analyze view**,
    visible in the next screenshot:![Figure 2.21 – Visualizing a dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_021.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.21 – Visualizing a dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next screenshot shows a scatter plot on duration vs. age. See how easy this
    is? You can experiment by selecting different columns, click on **Preview** to
    see results, and click on **Save** to create the analysis and save it for further
    use.![Figure 2.22 – Building a scatter plot
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_022.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.22 – Building a scatter plot
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On top of histograms and scatter plots, we can also build **Table Summary**,
    **Bias Analysis**, and **Target Leakage** reports. Let's build the latter to find
    out if certain columns are either leaking into the prediction, or not helpful
    at all. You can see the report in the next screenshot:![Figure 2.23 – Building
    a target leakage report
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_023.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.23 – Building a target leakage report
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This report tells us that no column is leaking (all scores are lower than 1).
    Several columns are also not useful in predicting the target (some scores are
    0.5 or lower): we should probably drop these columns during data processing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We could also try the **Quick Model** report, which trains a model using a **Random
    Forest** algorithm implemented with Spark, right in SageMaker Studio. Unfortunately,
    an error message pops up, complaining about column names. Indeed, some column
    names include a dot, which is not allowed by Spark. No problem, we can easily
    fix this during data processing, and build the report later.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, let's move on to transforming data with Data Wrangler.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming a dataset in SageMaker Data Wrangler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data Wrangler includes hundreds of built-in transforms, and we can also add
    our own.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the **Prepare** view visible in the next screenshot, we click
    on the **+** icon to add transforms.![Figure 2.24 – Adding a transform
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_024.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.24 – Adding a transform
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This opens the list of transforms, shown in the next screenshot. Take a minute
    to explore them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start by dropping the columns flagged as useless in the `marital`, `day
    of week`, `month`, `housing`, `cons.conf.idx`, `nr.employed`, `cons.price.idx`.
    We click on `marital` column. Your screen should look like the following screenshot:![Figure
    2.25 – Dropping a column
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_025.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.25 – Dropping a column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can preview results and add the transform to our pipeline. We'll repeat the
    same operations for the other columns we want to drop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's remove these annoying dots in column names, replacing them with underscores.
    The easiest way to do this is to use a `df`.![Figure 2.26 – Applying a custom
    transform
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_026.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.26 – Applying a custom transform
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Jumping back to the **Analyze** view, and clicking on **Steps**, we can see
    the list of transforms that we've already applied, as shown in the next screenshot.
    We could also delete each transform by clicking on the icon to the right of it.![Figure
    2.27 – Viewing a pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_027.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.27 – Viewing a pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clicking on the `y` label, as shown in the next screenshot. The F1 score for
    this classification model is 0.881, and the most important features are `duration`,
    `euribor3m`, and `pdays`. By applying more transforms and building a quick model
    again, we can iteratively measure the positive impact (or the lack thereof) of
    our feature engineering steps.![Figure 2.28 – Building a quick model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_028.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.28 – Building a quick model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Coming back to the `job` and `education`. We decide to encode them to help algorithms
    understand that the different values are different dimensions to the problem.
    Starting with `job`, we apply the `job` column is automatically dropped.![Figure
    2.29 – One-hot encoding a column
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_029.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.29 – One-hot encoding a column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `job_admin.` column name contains a dot! We can remove it with the `education`
    column… and remove the dots in column names. We could apply **Process numeric**
    transforms to scale and normalize numerical columns, but let's stop there for
    now. Feel free to explore and experiment!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One last thing: Data Wrangler workflows are stored in `.flow` files, visible
    in the Jupyter file view. These are JSON files that you can (and should) store
    in your Git repositories, in order to reuse them later and share them with other
    team members.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that our pipeline is ready, let's see how we can export it to Python code.
    All it takes is a single click, and we won't have to write a single line of code.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting a SageMaker Data Wrangler pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Wrangler makes it easy to export a pipeline in four ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Plain Python code that you can readily include in your machine learning project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Jupyter notebook running a SageMaker Processing job, which will apply the
    pipeline to your dataset and save results in S3\. The notebook also includes optional
    code to train a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Jupyter notebook storing the processed dataset in SageMaker Feature Store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Jupyter notebook creating a SageMaker Pipelines workflow, with steps to process
    your dataset and train a model on it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OK, let''s go for it:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the **Export** view, we click on Steps and select the steps we'd
    like to export. Here, I selected them all, as shown in the next screenshot:![Figure
    2.30 – Selecting steps to export
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_030.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.30 – Selecting steps to export
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we simply click on **Export step** and select one of the four options.
    Here, I go for **Save to S3** in order to run a SageMaker Processing job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This opens a new notebook. We'll discuss SageMaker Processing in the next section,
    but let's go ahead and run the job. Once the Job Status & S3 Output Location cell
    is complete, our dataset is available in S3, as visible in the next screenshot:![Figure
    2.31 – Locating the processed dataset in S3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_02_031.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.31 – Locating the processed dataset in S3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Downloading and opening the CSV file stored at this location, we see that it
    contains the processed dataset, as shown in the next screenshot. In a typical
    machine learning workflow, we would then use this data directly to train a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.32 – Viewing the processed dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_02_032.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.32 – Viewing the processed dataset
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, SageMaker Data Wrangler makes it very easy (and even fun) to
    apply transforms to your datasets. Once you're done, you can immediately export
    them to Python code, without having to write a single line of code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we're going to learn about Amazon SageMaker Processing,
    a great way run batch jobs for data processing and other machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Running batch jobs with Amazon SageMaker Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous section, datasets usually need quite a bit of work
    to be ready for training. Once training is complete, you may also want to run
    additional jobs to post-process the predicted data and to evaluate your model
    on different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Once the experimentation phase is complete, it's good practice to start automating
    all these jobs, so that you can run them on demand with little effort.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the Amazon SageMaker Processing API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Amazon SageMaker Processing API is part of the SageMaker SDK, which we installed
    in [*Chapter 1*](B17705_01_Final_JM_ePub.xhtml#_idTextAnchor013)*, Introducing
    Amazon SageMaker*.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker Processing jobs run inside Docker containers:'
  prefs: []
  type: TYPE_NORMAL
- en: A built-in container for **scikit-learn** ([https://scikit-learn.org](https://scikit-learn.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A built-in container for **PySpark** ([https://spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)),
    which supports distributed training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your own custom container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs are available in `/aws/sagemaker/ProcessingJobs` log group.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first see how we can use scikit-learn and SageMaker Processing to prepare
    a dataset for training.
  prefs: []
  type: TYPE_NORMAL
- en: Processing a dataset with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s the high-level process:'
  prefs: []
  type: TYPE_NORMAL
- en: Upload your unprocessed dataset to Amazon S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a script with scikit-learn in order to load the dataset, process it, and
    save the processed features and labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run this script with SageMaker Processing on managed infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading the dataset to Amazon S3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We're going to reuse the direct marketing dataset introduced in the previous
    section, and apply our own transforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a new Jupyter notebook, let''s first download and extract the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we load it with `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s display the first five lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the table visible in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.33 – Viewing the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_02_033.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.33 – Viewing the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scrolling to the right, we can see a column named **y**, storing the labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s upload the dataset to Amazon S3\. We''ll use a default bucket automatically
    created by SageMaker in the region we''re running in. We''ll just add a prefix
    to keep things nice and tidy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Writing a processing script with scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As SageMaker Processing takes care of all infrastructure concerns, we can focus
    on the script itself. SageMaker Processing will also automatically copy the input
    dataset from S3 into the container, and the processed datasets from the container
    to S3\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Container paths are provided when we configure the job itself. Here''s what
    we''ll use:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input dataset: `/opt/ml/processing/input`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The processed training set: `/opt/ml/processing/train`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The processed test set: `/opt/ml/processing/test`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our Jupyter environment, let''s start writing a new Python file named `preprocessing.py`.
    As you would expect, this script will load the dataset, perform basic feature
    engineering, and save the processed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we read our single command-line parameter with the `argparse` library
    ([https://docs.python.org/3/library/argparse.html](https://docs.python.org/3/library/argparse.html)):
    the ratio for the training and test datasets. The actual value will be passed
    to the script by the SageMaker Processing SDK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the input dataset using `pandas`. At startup, SageMaker Processing
    automatically copied it from S3 to a user-defined location inside the container,
    `/opt/ml/processing/input`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we remove any line with missing values, as well as duplicate lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we count negative and positive samples, and display the class ratio.
    This will tell us how unbalanced the dataset is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at the dataset, we can see a column named `pdays`, telling us how long
    ago a customer has been contacted. Some lines have a 999 value, and that looks
    pretty suspicious: indeed, this is a placeholder value meaning that a customer
    has never been contacted. To help the model understand this assumption, let''s
    add a new column stating it explicitly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the job column, we can see three categories (`student`, `retired`, and `unemployed`)
    that should probably be grouped to indicate that these customers don''t have a
    full-time job. Let''s add another column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s split the dataset into training and test sets. Scikit-learn has
    a convenient API for this, and we set the split ratio according to a command-line
    argument passed to the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to scale numerical features and to one-hot encode the categorical
    features. We''ll use `StandardScaler` for the former, and `OneHotEncoder` for
    the latter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we process the training and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we save the processed datasets, separating the features and labels.
    They''re saved to user-defined locations in the container, and SageMaker Processing
    will automatically copy the files to S3 before terminating the job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it. As you can see, this code is vanilla scikit-learn, so it shouldn't
    be difficult to adapt your own scripts for SageMaker Processing. Now let's see
    how we can actually run this.
  prefs: []
  type: TYPE_NORMAL
- en: Running a processing script
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Coming back to our Jupyter notebook, we use the `SKLearnProcessor` object from
    the SageMaker SDK to configure the processing job:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define which version of scikit-learn we want to use, and what our
    infrastructure requirements are. Here, we go for an `ml.m5.xlarge` instance, an
    all-round good choice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we simply launch the job, passing the name of the script, the dataset
    input path in S3, the user-defined dataset paths inside the SageMaker Processing
    environment, and the command-line arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As the job starts, SageMaker automatically provisions a managed `ml.m5.xlarge`
    instance, pulls the appropriate container to it, and runs our script inside the
    container. Once the job is complete, the instance is terminated, and we only pay
    for the amount of time we used it. There is zero infrastructure management, and
    we'll never leave idle instances running for no reason.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After a few minutes, the job is complete, and we can see the output of the
    script as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following screenshot shows the same log in **CloudWatch**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.34 – Viewing the log in CloudWatch Logs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_02_034.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.34 – Viewing the log in CloudWatch Logs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we can describe the job and see the location of the processed datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In a terminal, we can use the AWS CLI to fetch the processed training set located
    at the preceding path, and take a look at the first sample and label:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that the dataset has been processed with our own code, we could use it to
    train a machine learning model. In real life, we would also automate these steps
    instead of running them manually inside a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'One last thing: here, our job writes output data to S3\. SageMaker Processing
    also supports writing directly to an existing Feature Group in **SageMaker Feature
    Store** (which we''ll introduce later in the book). API details are available
    at [https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingOutput](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingOutput).'
  prefs: []
  type: TYPE_NORMAL
- en: Processing a dataset with your own code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, we used a built-in container to run our scikit-learn
    code. SageMaker Processing also makes it possible to use your own container. You
    can find an example at [https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, SageMaker Processing makes it really easy to run data processing
    jobs. You can focus on writing and running your script, without having to worry
    about provisioning and managing infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how Amazon SageMaker Ground Truth helps you build
    highly accurate training datasets using image and text labeling workflows. We'll
    see in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091)*,* *Training
    Computer Vision Models*, how to use image datasets labeled with Ground Truth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you learned about Amazon SageMaker Processing, a capability that helps
    you run your own data processing workloads on managed infrastructure: feature
    engineering, data validation, model evaluation, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed three other AWS services (Amazon EMR, AWS Glue, and Amazon
    Athena), and how they could fit into your analytics and machine learning workflows.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll start training models using the built-in machine
    learning models of Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL

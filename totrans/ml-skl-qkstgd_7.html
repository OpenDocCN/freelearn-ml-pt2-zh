<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Clustering Data with Unsupervised Machine Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">Most of the data that you will encounter out in the wild will not come with labels. <span><span>It is impossible to apply supervised machine learning techniques when your data does not come with labels.</span></span> Unsupervised machine learning addresses this issue by grouping data into clusters; we can then assign labels based on those clusters.</p>
<p>Once the data has been clustered into a specific number of groups, we can proceed to give those groups labels. Unsupervised machine learning is the first step that you, as the data scientist, will have to implement, before you can apply supervised machine learning techniques (such as classification) to make meaningful predictions. </p>
<p>A common application of the unsupervised machine learning algorithm is customer data, which can be found across a wide range of industries. As a data scientist, your job is to find groups of customers that you can segment and deliver targeted products and advertisements to. </p>
<p>In this chapter, you will learn about the following topics:</p>
<ul>
<li>The k-means algorithm and how it works internally, in order to cluster unlabeled data</li>
<li>Implementing the k-means algorithm in scikit-learn</li>
<li>Using feature engineering to optimize unsupervised machine learning </li>
<li>Cluster visualization </li>
<li>Going from unsupervised to supervised machine learning </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You will be required to have <span class="fontstyle0">Python 3.6 or greater, </span><span class="fontstyle0">Pandas  ≥ 0.23.4,</span><span class="fontstyle2"> </span><span class="fontstyle0">Scikit-learn ≥ 0.20.0, </span><span class="fontstyle0">NumPy ≥ 1.15.1, </span><span class="fontstyle0">Matplotlib ≥ 3.0.0, </span><span class="fontstyle0">Pydotplus ≥ 2.0.2, </span><span class="fontstyle0">Image ≥ 3.1.2, </span><span class="fontstyle0">Seaborn ≥ 0.9.0, and </span><span class="fontstyle0">SciPy ≥ 1.1.0 </span></span><span class="fontstyle0">installed on your system.</span></p>
<p class="mce-root">The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb">https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb</a><a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb">.</a></p>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p class="mce-root"><a href="http://bit.ly/2qeEJpI">http://bit.ly/2qeEJpI</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The k-means algorithm</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn about how the k-means algorithm works under the hood, in order to cluster data into groups that make logical sense. </p>
<p>Let's consider a set of points, as illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9d2492a0-3cb0-4ab3-8685-d523c395d4dd.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A random set of points </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assignment of centroids</h1>
                </header>
            
            <article>
                
<p>The first step that the algorithm takes is to assign a set of random centroids. Assuming that we want to find two distinct clusters or groups, the algorithm can assign two centroids, as shown in the following diagram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/a4f79e48-a120-4fdb-a6c9-ca18b6d3d209.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Centroids, represented by stars</div>
<p>In the preceding diagram, the stars represent the centroids of the algorithm. Note that in this case, the clusters' centers perfectly fit the two distinct groups. This is the most ideal case. In reality, the means (or centroids) are assigned randomly, and, with every iteration, the cluster centroids move closer to the center of the two groups. </p>
<p>The algorithm is known as the k-means algorithm, as we try to find the mean of a group of points as the centroid. Since the mean can only be computed for a set of numeric points, such clustering algorithms only work with numerical data. </p>
<p>In reality, the process of grouping these points into two distinct clusters is not this straightforward. A visual representation of the process can be illustrated as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/97902875-2328-417d-89d6-f0a4c32d7342.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The process of assigning centroids in the k-means algorithm</div>
<p><span>In the preceding diagram, the process of assigning the random centroids begins in the upper-left corner. As we go down and toward the upper-right corner, note how the centroids move closer to the center of the two distinct groups. In reality, the algorithm does not have an optimal endpoint at which it stops the iteration.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When does the algorithm stop iterating?</h1>
                </header>
            
            <article>
                
<p class="mce-root">Typically, the algorithm looks for two metrics, in order to stop the iteration process:</p>
<ul>
<li>The distance between the distinct groups (or clusters) that are formed</li>
<li>The distance between each point and the centroid of a cluster</li>
</ul>
<p class="mce-root">The optimal case of cluster formation is when the distance between the distinct groups or clusters are as large as possible, while the distances between each point and the centroid of a cluster are as small as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the k-means algorithm in scikit-learn</h1>
                </header>
            
            <article>
                
<p>Now that you understand how the k-means algorithm works internally, we can proceed to implement it in scikit-learn. We are going to work with the same fraud detection dataset that we used in all of the previous chapters. The key difference is that we are going to drop the target feature, which contains the labels, and identify the two clusters that are used to detect fraud.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the base k-means model</h1>
                </header>
            
            <article>
                
<p>In order to load the dataset into our workspace and drop the target feature with the labels, we use the following code:</p>
<pre class="mce-root">import pandas as pd<br/>#Reading in the dataset<br/>df = pd.read_csv('fraud_prediction.csv')<br/>#Dropping the target feature &amp; the index<br/>df = df.drop(['Unnamed: 0', 'isFraud'], axis = 1)</pre>
<p>Next, we can implement the k-means algorithm with two cluster means. The choice of using two cluster means is arbitrary in nature, since we know that there should be two distinct clusters as a result of two labels: fraud and not fraud transactions. We can do this by using the following code:</p>
<pre>from sklearn.cluster import KMeans<br/>#Initializing K-means with 2 clusters<br/>k_means = KMeans(n_clusters = 2)<br/>#Fitting the model on the data<br/>k_means.fit(df)</pre>
<p>In the preceding code, first, we import the <kbd>KMeans</kbd> package from scikit-learn and initialize a model with two clusters. We then fit this model to the data by using the <kbd>.fit()</kbd><em> </em>function. This results in a set of labels as the output. We can extract the labels by using the following code:</p>
<pre>#Extracting labels <br/>target_labels = k_means.predict(df)<br/>#Printing the labels<br/>target_labels</pre>
<p>The output produced by the preceding code is an array of labels for each mobile transaction, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9d6d2097-7765-4adb-8135-6402afc75127.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Array of labels</div>
<p>Now that we have a set of labels, we know which cluster each transaction falls into. Mobile transactions that have a label of <kbd>0</kbd> fall into one group, while transactions that have a label of <kbd>1</kbd> fall into the second group. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The optimal number of clusters</h1>
                </header>
            
            <article>
                
<p>While explaining how the k-means algorithm works, we mentioned how the algorithm terminates once it finds the optimal number of clusters. When picking clusters arbitrarily using scikit-learn, this is not always the case. We need to find the optimal number of clusters, in this case. </p>
<p>One way that we can do this is by a measure known as<strong> inertia. </strong>Inertia measures how close the data points in a cluster are to its centroid. Obviously, a lower inertia signifies that the groups or clusters are tightly packed, which is good. </p>
<p>In order to compute the inertia for the model, we use the following code:</p>
<pre># Inertia of present model<br/>k_means.inertia_</pre>
<p>The preceding code produced an inertia value of <em>4.99 × 10 ^ 17</em>, which is extremely large with respect to the other values of inertia produced by different numbers of clusters (explained as follows), and is not a good value of inertia. This suggests that the individual data points are spread out, and are not tightly packed together.</p>
<p>In most cases, we do not really know what the optimal numbers of clusters are, so we need to plot the inertia scores for different numbers of clusters. We can do this by using the following code:</p>
<pre>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>#Initialize a list of clusters from 1 to 10 clusters <br/><br/>clusters = [1,2,3,4,5,6,7,8,9,10]<br/><br/>#Create an empty list in order to store the inertia values <br/><br/>inertia_values = []<br/><br/>for cluster in clusters:<br/>    <br/>    #Build a k-means model for each cluster value<br/>    <br/>    k_means = KMeans(n_clusters = cluster)<br/>    <br/>    #Fit the model to the data<br/>    <br/>    k_means.fit(df)<br/>    <br/>    # Store inertia value of each model into the empty list <br/>    <br/>    inertia_values.append(k_means.inertia_)<br/>    <br/># Plot the result<br/><br/><span><span>plt</span></span>.lineplot(x = clusters, y = inertia_values)<br/>plt.xlabel('Number of Clusters')<br/>plt.ylabel('Inertia Value')<br/>plt.title('Number of Clusters Vs. Inertia Values')<br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c094e7ab-092b-4c7f-8b34-2e0508722eaf.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Inertia as a function of the number of clusters</div>
<p>In the preceding code, first, we create a list of clusters that have values from 1 to 10. Each value denotes the number of clusters that will be used in the machine learning model. Next, we create an empty list that will store all of the inertia values that each model will produce. </p>
<p>Next, we loop over the list of clusters and build and evaluate a k-means model for each cluster value in the list. Each model now produces an inertia, which is stored in the list that we initialized at the start of the code block. A simple line plot is then constructed by using the list of clusters along the x<em> </em>axis and the corresponding inertia values along the y<em> </em>axis, using <kbd>matplotlib</kbd>. </p>
<p>The plot tells us that the inertia values are the lowest when the number of clusters is equal to 10. However, having a large number of clusters is also something that we must aim at avoiding, as having too many groups does not help us to generalize well, and the characteristics about each group become very specific. </p>
<p>Therefore, the ideal way to choose the best number of clusters for a problem, given that we do not have prior information about the number of groups that we want beforehand, is to identify the <strong>elbow point</strong> of the plot.</p>
<p>The elbow point is the point at which the rate of decrease in inertia values slows down. The elbow point is illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fa54e78e-83a7-4135-80b2-98fa7b8ebe95.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Elbow point of the graph</div>
<p>In the preceding plot, it is clear that the elbow point corresponds to four clusters. This could mean that there are four distinct types of fraudulent transactions, apart from the standard categorizations of fraud and not fraud. However, since we know beforehand that the dataset has a binary target feature with two categories, we will dig too deeply into why four is the ideal number of groups/clusters for this dataset. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature engineering for optimization</h1>
                </header>
            
            <article>
                
<p>Engineering the features in your dataset is a concept that is fundamentally used to improve the performance of your model. Fine-tuning the features to the algorithm's design is beneficial, because it can lead to an improvement in accuracy, while reducing the generalization errors at the same time. The different kinds of feature engineering techniques for optimizing your dataset that you will learn are as follows:</p>
<ul>
<li>Scaling</li>
<li>Principal component analysis </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling</h1>
                </header>
            
            <article>
                
<p>Scaling is the process of standardizing your data so that the values under every feature fall within a certain range, such as -1 to +1. In order to scale the data, we subtract each value of a particular feature with the mean of that feature, and divide it by the variance of that feature. In order to scale the features in our fraud detection dataset, we use the following code:</p>
<pre>from sklearn.preprocessing import StandardScaler<br/><br/>#Setting up the standard scaler <br/><br/>scale_data = StandardScaler()<br/><br/>#Scaling the data<br/><br/>scale_data.fit(df)<br/><br/>df_scaled = scale_data.transform(df)<br/><br/>#Applying the K-Means algorithm on the scaled data<br/><br/>#Initializing K-means with 2 clusters<br/><br/>k_means = KMeans(n_clusters = 2)<br/><br/>#Fitting the model on the data<br/><br/>k_means.fit(df_scaled)<br/><br/># Inertia of present model<br/><br/>k_means.inertia_</pre>
<p>In the preceding code, we use the <kbd>StandardScalar()</kbd><em> </em>function to scale our dataframe, and then we build a k-means model with two clusters on the scaled data. After evaluating the inertia of the model, the value output is 295,000, which is substantially better than the value of <em>4.99 × 10<sup>17</sup></em>, produced by the model without scaling. </p>
<p>We can then create a new plot of the number of clusters versus the inertia values, using the same code that we did earlier, with the only difference being replacing the original dataframe with the scaled dataframe:</p>
<pre>#Initialize a list of clusters from 1 to 10 clusters <br/><br/>clusters = [1,2,3,4,5,6,7,8,9,10]<br/><br/>#Create an empty list in order to store the inertia values <br/><br/>inertia_values = []<br/><br/>for cluster in clusters:<br/>    <br/>    #Build a k-means model for each cluster value<br/>    <br/>    k_means = KMeans(n_clusters = cluster)<br/>    <br/>    #Fit the model to the data<br/>    <br/>    k_means.fit(df_scaled)<br/>    <br/>    # Store inertia value of each model into the empty list <br/>    <br/>    inertia_values.append(k_means.inertia_)<br/>    <br/># Plot the result<br/><br/>sns.lineplot(x = clusters, y = inertia_values)<br/>plt.xlabel('Number of Clusters')<br/>plt.ylabel('Inertia Value')<br/>plt.title('Number of Clusters Vs. Inertia Values')<br/>plt.show()</pre>
<p>This produces the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7945b2c1-962a-4a74-a2f6-0d301b06041c.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Optimal number of clusters, post-scaling</div>
<p>We notice that the preceding plot does not have a very clear elbow point, where the rate of decrease in the inertia values is lower. However, if we look closely, we can find this point at <span class="packt_screen">8</span> clusters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principal component analysis</h1>
                </header>
            
            <article>
                
<p>The <strong>principal component analysis</strong> (<strong>PCA</strong>) is a subset of dimensionality reduction. <strong>Dimensionality reduction</strong> is the process of reducing the number of features that provide no predictive value to a predictive model. We also optimize and improve the computational efficiency of processing the algorithms. This is because a dataset with a smaller number of features will make it easier for the algorithm to detect patterns more quickly.</p>
<p>The first step in PCA is called <strong>decorrelation</strong>. Features that are highly correlated with each other provide no value to the predictive model. Therefore, in the decorrelation step, the PCA takes two highly correlated features and spreads their data points such that it's aligned across the axis, and is not correlated anymore. This process can be illustrated as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2ca670b5-80b4-42eb-9604-968e02b5a01e.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The process of decorrelation</div>
<p>Once the features are decorrelated, the principal components (or features) are extracted from the data. These features are the ones that have high variance, and, in turn, provide the most value to a predictive model. The features with low variance are discarded, and thus, the number of dimensions in the dataset is reduced.</p>
<p>In order to perform dimensionality reduction using PCA, we use the following code:</p>
<pre>from sklearn.decomposition import PCA<br/><br/>#Initialize a PCA model with 5 features <br/><br/>pca_model = PCA(n_components = 5)<br/><br/>#Fit the model to the scaled dataframe<br/><br/>pca_model.fit(df_scaled)<br/><br/>#Transform the features so that it is de-correlated<br/><br/>pca_transform = pca_model.transform(df_scaled)<br/><br/>#Check to see if there are only 5 features<br/><br/>pca_transform.shape</pre>
<p>In the preceding code, first, we import the <kbd>PCA</kbd><em> </em>method from scikit-learn. Next, we initialize a PCA<em> </em>model with five components. Here, we are specifying that we want the PCA to reduce the dataset to only the five most important features.</p>
<p>We then fit the PCA model to the dataframe and transform it, in order to obtain the decorrelated features. Checking the shape of the final array of features, we can see that it only has five features. Finally, we create a new k-means model with only the principal component features, as shown in the following code:</p>
<pre>#Applying the K-Means algorithm on the scaled data<br/><br/>#Initializing K-means with 2 clusters<br/><br/>k_means = KMeans(n_clusters = 2)#Fitting the model on the data<br/><br/>k_means.fit(pca_transform)<br/><br/># Inertia of present model<br/><br/>k_means.inertia_</pre>
<p>Evaluating the inertia of the new model improved its performance. We obtained a lower value of inertia than in the case of the scaled model. Now, let's evaluate the inertia scores for different numbers of principal components or features. In order to this, we use the following code:</p>
<pre>#Initialize a list of principal components<br/><br/>components = [1,2,3,4,5,6,7,8,9,10]<br/><br/>#Create an empty list in order to store the inertia values <br/><br/>inertia_values = []<br/><br/>for comp in components:<br/>    <br/>    #Initialize a PCA model<br/><br/>    pca_model = PCA(n_components = comp)<br/>    <br/>    #Fit the model to the dataframe<br/><br/>    pca_model.fit(df_scaled)<br/>    <br/>    #Transform the features so that it is de-correlated<br/><br/>    pca_transform = pca_model.transform(df_scaled)<br/>    <br/>    #Build a k-means model <br/>    <br/>    k_means = KMeans(n_clusters = 2)<br/>    <br/>    #Fit the model to the data<br/>    <br/>    k_means.fit(pca_transform)<br/>    <br/>    # Store inertia value of each model into the empty list <br/>    <br/>    inertia_values.append(k_means.inertia_)<br/>    <br/># Plot the result<br/><br/>sns.lineplot(x = components, y = inertia_values)<br/>plt.xlabel('Number of Principal Components')<br/>plt.ylabel('Inertia Value')<br/>plt.title('Number of Components Vs. Inertia Values')<br/>plt.show()</pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we initialize a list to store the different principal component values that we want to build our models with. These values are from 1 to 10.</li>
<li>Next, we initialize an empty list, in order to store the inertia values from each and every model. </li>
<li>Using each principal component value, we build a new k-means model and append the inertia value for that model into the empty list. </li>
<li>Finally, a plot is constructed between the inertia values and the different values of components.</li>
</ol>
<p>This plot is illustrated as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/eef35bb7-907b-437c-8a0a-5a282fe3ea82.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Inertia values versus the numbers of principal components</div>
<p>In the preceding plot, it is clear that the inertia value is lowest for one component. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster visualization</h1>
                </header>
            
            <article>
                
<p>Visualizing how your clusters are formed is no easy task when the number of variables/dimensions in your dataset is very large. There are two main methods that you can use in order to visualize how the clusters are distributed, as follows:</p>
<ul>
<li><strong>t-SNE</strong>: This <span>creates a map of the dataset in two-dimensional space</span></li>
<li><strong>Hierarchical clustering</strong>: This <span>uses a tree-based visualization, known as a <strong>dendrogram</strong>, in order to create hierarchies</span></li>
</ul>
<p>In this section, you will learn how to implement these visualization techniques, in order to create compelling cluster visuals. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">t-SNE</h1>
                </header>
            
            <article>
                
<p>The <strong>t-SNE</strong> is an abbreviation that stands for <strong>t-distributed stochastic neighbor embedding</strong>. The fundamental concept behind the t-SNE is to map a higher dimension to a two-dimensional space. In simple terms, if your dataset has more than two features, the t-SNE does a great job at showing you how your entire dataset can be visualized on your computer screen!</p>
<p>The first step is to implement the k-means algorithm and create a set of prediction labels that we can merge into the unlabeled dataset. We can do this by using the following code:</p>
<pre>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the target feature &amp; the index<br/><br/>df = df.drop(['Unnamed: 0', 'isFraud'], axis = 1)<br/><br/>#Initializing K-means with 2 clusters<br/><br/>k_means = KMeans(n_clusters = 2)<br/><br/>#Fitting the model on the data<br/><br/>k_means.fit(df)<br/><br/>#Extracting labels <br/><br/>target_labels = k_means.predict(df)<br/><br/>#Converting the labels to a series <br/><br/>target_labels = pd.Series(target_labels)<br/><br/>#Merging the labels to the dataset<br/><br/>df = pd.merge(df, pd.DataFrame(target_labels), left_index=True, right_index=True)<br/><br/>#Renaming the target <br/><br/>df['fraud'] = df[0]<br/>df = df.drop([0], axis = 1)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Don't worry about how the preceding segment of code works for the moment, as this will be explained in detail in a later section within this chapter, when we deal with converting an unsupervised machine learning problem into a supervised learning one. </p>
<p>Next, we will create a t-SNE object and fit that into our array of data points that consists of only the features. We will then transform the features at the same time so that we can view all the features on a two-dimensional space. This is done in the following code segment:</p>
<pre>from sklearn.manifold import TSNE<br/><br/>#Creating the features<br/><br/>features = df.drop('fraud', axis = 1).values<br/><br/>target = df['fraud'].values<br/><br/>#Initialize a TSNE object<br/><br/>tsne_object = TSNE()<br/><br/>#Fit and transform the features using the TSNE object<br/><br/>transformed = tsne_object.fit_transform(features)</pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we initialize the t-SNE object by using the <kbd>TSNE()</kbd><em> </em>function.</li>
<li>Using the t-SNE object, we fit and transform the data in our features, using the <kbd>fit_transform()</kbd><em> </em>method. </li>
</ol>
<p>Next, we create the t-SNE visualization by using the following code:</p>
<pre>#Creating a t-SNE visualization<br/><br/>x_axis = transformed[:,0]<br/><br/><br/>y_axis = transformed[:,1]<br/><br/><br/>plt.scatter(x_axis, y_axis, c = target)<br/><br/>plt.show()</pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>We extract the first and second features from the set of transformed features for the x axis and <em>y </em>axis, respectively.</li>
<li>We then plot a scatter plot and color it by the target labels, which were generated earlier, using the k-means algorithm. This generates the following plot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3e1e748d-08c5-4dce-9d61-07117f8f82af.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">t-SNE visualization</div>
<p>In the preceding plot, the yellow color represents the transactions that have been assigned the fraud label, while the purple color represents the transactions that have been assigned the non-fraudulent label. (Please refer to the color version of the image.)</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hierarchical clustering</h1>
                </header>
            
            <article>
                
<p>As discussed initially, the hierarchical clustering technique uses the dendrogram to visualize clusters or groups. In order to explain how the dendrogram works, we will consider a dataset with four features. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – Individual features as individual clusters</h1>
                </header>
            
            <article>
                
<p>In the first step, each feature in the dataset is considered to be its own cluster. This is illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5d56db01-84a4-4320-b24a-d491ae54b4f7.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Each feature as a single cluster in the dendrogram</div>
<p>Each feature in the preceding diagram is one single cluster, at this point in time. The algorithm now searches to find the two features that are closest to each other, and merges them into a single cluster. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – The merge</h1>
                </header>
            
            <article>
                
<p>In this step, the algorithm merges the data points in the two closest features together, into one single cluster. This is illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e0cffef7-2d65-4b61-b6ef-e0dd0683c8a7.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The process in which features merge into a single cluster</div>
<p>In the preceding diagram, it is clear that the algorithm has now chosen <strong>Feature 2</strong> and <strong>Feature 3</strong>, and has decided that the data under these two features are the closest to each other. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 – Iteration</h1>
                </header>
            
            <article>
                
<p>The algorithm now continues the process of merging features together iteratively, until no more clusters can be formed. The final dendrogram that is formed is as follows:</p>
<p>In the preceding diagram, <strong>Feature 2</strong> and <strong>Feature 3</strong> were grouped into a single cluster. The algorithm then decided that <strong>Feature 1</strong> and the cluster of <strong>Feature 2</strong> and <strong>3</strong> were closest to each other. Therefore, these three features were clustered into one group. Finally, <strong>Feature 4</strong> was grouped together with <strong>Feature 3</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing hierarchical clustering</h1>
                </header>
            
            <article>
                
<p>Now that you have learned how hierarchical clustering works, we can implement this concept. In order to create a hierarchical cluster, we use the following code:</p>
<pre>from scipy.cluster.hierarchy import linkage<br/>from scipy.cluster.hierarchy import dendrogram<br/>import numpy as np<br/>import matplotlib.pyplot as plt <br/><br/>#Creating an array of 4 features<br/><br/>array = np.array([[1,2,3,4], [5,6,7,8], [2,3,4,5], [5,6,4,3]])<br/><br/>feature_names = ['a', 'b', 'c', 'd']<br/><br/>#Creating clusters<br/><br/>clusters = linkage(array, method = 'complete')<br/><br/>#Creating a dendrogram<br/><br/>dendrogram(clusters, labels = feature_names, leaf_rotation = 90)<br/><br/>plt.show()</pre>
<p>The preceding code results in a dendrogram, as illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ab00c657-6b5d-4608-b2d6-a64f8123ed7f.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Dendrogram</div>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we create an array with four columns.</li>
<li>We then use the <kbd>linkage</kbd> function to create the clusters. Within the function, we specify the <kbd>method</kbd> argument as complete, in order to indicate that we want the entire dendrogram. </li>
<li>Finally, we use the <kbd>dendrogram</kbd><em> </em>function to create the dendrogram with the clusters. We set the label names to the list of feature names that was created earlier in the code.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Going from unsupervised to supervised learning</h1>
                </header>
            
            <article>
                
<p>The eventual goal of unsupervised learning is to take a dataset with no labels and assign labels to each row of the dataset, so that we can run a supervised learning algorithm through it. This allows us to create predictions that make use of the labels. </p>
<p>In this section, you will learn how to convert the labels generated by the unsupervised machine learning algorithm into a decision tree that makes use of those labels. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a labeled dataset </h1>
                </header>
            
            <article>
                
<p>The first step is to convert the labels generated by an unsupervised machine learning algorithm, such as the k-means algorithm, and append it to the dataset. We can do this by using the following code:</p>
<pre>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the target feature &amp; the index<br/><br/>df = df.drop(['Unnamed: 0', 'isFraud'], axis = 1)</pre>
<p>In the preceding code, we read in the fraud detection dataset and drop the target and index columns:</p>
<pre>#Initializing K-means with 2 clusters<br/><br/>k_means = KMeans(n_clusters = 2)<br/><br/>#Fitting the model on the data<br/><br/>k_means.fit(df)</pre>
<p>Next, in the preceding code we initialize and fit a k-means model with two clusters:</p>
<pre>#Extracting labels <br/><br/>target_labels = k_means.predict(df)<br/><br/>#Converting the labels to a series <br/><br/>target_labels = pd.Series(target_labels)<br/><br/>#Merging the labels to the dataset<br/><br/>df = pd.merge(df, pd.DataFrame(target_labels), left_index=True, right_index=True)</pre>
<p>Finally, we create the target labels by using the <kbd>predict()</kbd><em> </em>method, and convert it into a <kbd>pandas</kbd> series. We then merge this series into the dataframe, in order to create our labeled dataset. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the decision tree</h1>
                </header>
            
            <article>
                
<p>Now that we have the labeled dataset, we can create a decision tree, in order to convert the unsupervised machine learning problem into a supervised machine learning one. </p>
<p>In order to do this, we start with all of the necessary package imports, as shown in the following code:</p>
<pre>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.externals.six import StringIO<br/>from IPython.display import Image<br/>from sklearn.tree import export_graphviz<br/>import pydotplus<br/>from sklearn import tree</pre>
<p>Next, we rename the target column to a name that is appropriate (when we merged the target labels created by the k-means algorithm, it produced <kbd>0</kbd> as the default name). We can do this by using the following code:</p>
<pre>#Renaming the target <br/><br/>df['fraud'] = df[0]<br/>df = df.drop([0], axis = 1)</pre>
<p>Next, we build the decision tree classification algorithm, using the following code:</p>
<pre>#Creating the features<br/><br/>features = df.drop('fraud', axis = 1).values<br/><br/>target = df['fraud'].values<br/><br/>#Initializing an empty DT classifier with a random state value of 42<br/><br/>dt_classifier = DecisionTreeClassifier(criterion = 'gini', random_state = 42)<br/><br/>#Fitting the classifier on the training data <br/><br/>dt_classifier.fit(features, target)<br/><br/></pre>
<p>In the preceding code, first, we create the features and target variables and initialize a decision tree classifier. We then fit the classifier onto the features and target.</p>
<p>Finally, we want to visualize the decision tree. We can do this by using the following code: </p>
<pre>#Creating a data frame with the features only<br/><br/>features = df.drop('fraud', axis = 1)<br/><br/>dot_data = tree.export_graphviz(dt_classifier, out_file=None, feature_names= features.columns)<br/><br/># Draw graph<br/><br/>graph = pydotplus.graph_from_dot_data(dot_data)<br/><br/>#Show graph <br/><br/>Image(graph.create_png())</pre>
<p>This results in the decision tree shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d761508f-2bf5-4d69-b782-b2a616cc3b09.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A part of the decision tree that was created </div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned about how the k-means algorithm works, in order to cluster unlabeled data points into clusters or groups. You then learned how to implement the same using scikit-learn, and we expanded upon the feature engineering aspect of the implementation. </p>
<p>Having learned how to visualize clusters using hierarchical clustering and t-SNE, you then learned how to map a multi-dimensional dataset into a two-dimensional space. Finally, you learned how to convert an unsupervised machine learning problem into a supervised learning one, using decision trees. </p>
<p>In the next (and final) chapter, you will learn how to formally evaluate the performance of all of the machine learning algorithms that you have built so far!</p>


            </article>

            
        </section>
    </body></html>
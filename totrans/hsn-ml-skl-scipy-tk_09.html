<html><head></head><body>
    <section>
      <header class="header-title chapter-title">
                    Neural Networks – Here Comes Deep Learning
                </header>
      <article>
        <p>It is not uncommon to read news articles or encounter people who misuse the term <em>deep learning</em> in place of <em>machine learning</em>. This is due to the fact that this particular sub-field of machine learning has become very successful at solving plenty of previously unsolvable image processing and natural language processing problems. This success has caused many to confuse the child field with its parent.</p>
        <p>The term <em>deep learning</em> refers to deep <strong>Artificial Neural Networks</strong> (<strong>ANNs</strong>). The latter concept comes in different forms and shapes. In this chapter, we are going to cover one subset of <strong>feedforward neural networks</strong> known as the <strong>Multilayer Perceptron</strong> (<strong>MLP</strong>). It is one of the most commonly used types and is implemented by scikit-learn. As its name suggests, it is composed of multiple layers, and it is a feedforward network as there are no cyclic connections between its layers. The more layers there are, the deeper the network is. These deep networks can exist in multiple forms, such as <strong>MLP</strong>, <strong>Convolutional Neural Networks </strong>(<strong>CNNs</strong>), or <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>). The latter two are not implemented by scikit-learn, yet this will not stop us from discussing the main concepts behind CNNs and manually mimicking them using the tools from the scientific Python ecosystem. </p>
        <p>In this chapter, we are going to cover the following topics:</p>
        <ul>
          <li>Getting to know MLP</li>
          <li>Classifying items of clothing</li>
          <li>Untangling convolutions</li>
          <li>MLP regressors</li>
        </ul>
        <h1 id="uuid-af54ec06-acd7-4824-8c7c-a5026963ba65">Getting to know MLP</h1>
        <p>When learning a new algorithm, you can get discouraged by the number of hyperparameters and find it hard to decide where to start. Therefore, I suggest we start by answering the following two questions:</p>
        <ul>
          <li>How has the algorithm been architected?</li>
          <li>How does the algorithm train?</li>
        </ul>
        <p>In the following sections, we are going to answer both of these questions and learn about the corresponding hyperparameters one by one. </p>
        <h2 id="uuid-7febdd83-102e-4dcd-8745-88e8d880247f">Understanding the algorithm's architecture</h2>
        <p>Luckily, the knowledge we gained about linear models in<a href="f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml">Chapter 3</a>, <em>Making Decisions with Linear Equations</em>, will give us a good headstart here. In brief, linear models can be outlined in the following diagram:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/360de1e7-1de9-4902-b4f3-5f7ed0243feb.png" style="width:26.75em;"/>
        </p>
        <p>Each of the input features (<span class="packt_screen">x<sub>i</sub></span>) is multiplied by a weight (<span class="packt_screen">w<sub>i</sub></span>), and the sum of these products is the output of the model (<span class="packt_screen">y</span>). Additionally, we sometimes add an extra bias (threshold), along with its weight. Nevertheless, one main problem with linear models is that they are in fact linear (duh!). Furthermore, each feature gets its own weight, regardless of its neighbors. This simple architecture prevents the model from capturing any interactions between its features. So, you can stack more layers next to each other, as follows: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/61f146e5-1b11-4dec-8157-9f62fd99efb5.png" style="width:32.92em;"/>
        </p>
        <p>This sounds like a potential solution; however, based on simple mathematical derivations, these combinations of multiplications and summations can still be reduced into a single linear equation. It is as if all these layers have no effect at all. Therefore, to get to the desired effect, we want to apply non-linear transformations after each summation. These non-linear transformations are known as activation functions, and they turn the model to a non-linear one. Let's see where they fit into the model, then I will explain further: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/3468d483-682c-4cbe-b87a-60d41ef47070.png" style="width:36.08em;"/>
        </p>
        <p>This model has a single hidden layer with two hidden nodes, which are shown inside the box. In practice, you may have multiple hidden layers with a number of nodes. The aforementioned activation functions are applied at the outputs of the hidden nodes. Here, we used a <strong>R</strong><strong>ectified Linear Unit</strong> (<strong>ReLU</strong>), it is an activation function; for the negative values, it returns <kbd>0</kbd>, and it keeps the positive values unchanged. In addition to the <kbd>relu</kbd>function, <kbd>identity</kbd>, <kbd>logistic</kbd>, and <kbd>tanh</kbd> activation functions are also supported for the hidden layers, and they are set using the <kbd>activation</kbd> hyperparameter. Here is how each of these four activation functions look:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/d9b355fc-cc60-4a52-bf77-ce13c309b88d.png" style="width:51.67em;"/>
        </p>
        <div class="packt_tip">As mentioned earlier, since the <kbd>identity</kbd> function keeps its inputs untouched without any non-linear transformations, it is seldom used as it will end up reducing the model into a simple linear one. It is also cursed by having a constant gradient, which is not very helpful for the gradient descent algorithm used for training. Therefore, the <kbd>relu</kbd> function is usually a good non-linear alternative. It is the current default setting and is a good first choice; the <kbd>logistic</kbd> or the <kbd>tanh</kbd> activation functions are the next alternative options.</div>
        <p>The output layer also has its own activation function, but it serves a different purpose. If you recall from <a href="f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml">Chapter 3</a>,<em> Making Decisions with Linear Equations,</em> we used the <kbd>logistic</kbd> function to turn a linear regression into a classifier—that is, logistic regression. The output's activation function serves the exact same purpose here as well. This list has the possible output activation functions and their corresponding use cases:</p>
        <ul>
          <li><strong>Identity function</strong>: Set when doing regression using <kbd>MLPRegressor</kbd></li>
          <li><strong>Logistic function</strong>: Set when performing a binary classification using <kbd>MLPClassifier</kbd></li>
          <li><strong>Softmax function</strong>:<strong/>Set when using <kbd>MLPClassifier</kbd>to differentiate between three or more classes </li>
        </ul>
        <p>We do not set the output activation functions by hand; they are automatically chosen based on whether<kbd>MLPRegressor</kbd> or<kbd>MLPClassifier</kbd>is<em/>used and on the number of classes available for the latter to classify.</p>
        <p>If we look at the network architecture, it is clear that another important hyperparameter to set is the number of hidden layers and the number of nodes in each layer. This is set using the <kbd>hidden_layer_sizes</kbd><em/>hyperparameter, which accepts tuples. To achieve the architecture in the previous figure—that is, one hidden layer with two nodes—we will set <kbd>hidden_layer_sizes</kbd> to <kbd>2</kbd>. Setting it to <kbd>(10, 10, 5)</kbd> gives us three hidden layers; the first two have 10 nodes each, while the third one has 5 nodes. </p>
        <h2 id="uuid-33316efe-4ffc-4934-afd9-76c1548f4b40">Training the neural network</h2>
        <div class="packt_quote">"Psychologists tell us that in order to learn from experience, two ingredients are necessary: frequent practice and immediate feedback."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– <span class="authorOrTitle">Richard Thaler</span></div>
        <p>A big chunk of researchers' time is spent on improving how their neural networks train. This is also reflected in the number of hyperparameters related to the training algorithms used. To better understand these hyperparameters, we need to examine the following training workflow: </p>
        <ol>
          <li>Get a subset of the training samples.</li>
          <li>Run them through the network and make predictions.</li>
          <li>Calculate the training loss by comparing the actual values and predictions.</li>
          <li>Use the calculated loss to update the network weights. </li>
          <li>Return to <em>step 1</em> to get more samples, and if all the samples are already used, go through the training data over and over until the training process converges. </li>
        </ol>
        <p>Going through these steps one by one, you can seethe need to set the size of the training subset at the first stage. This is what the <kbd>batch_size</kbd>parameter sets. As we will see in a bit, you can go from using one sample at a time to using the entire training set all at once to anything in between. The first and second steps are straightforward, but the third step dictates that we should know which loss function to use. As for the available loss functions, we do not have much choice when working with scikit-learn. A<strong>log loss function</strong>is selected for us when performing classifications and<strong>mean squared error</strong>is what is available for regression. The fourth step is the trickiest part with the most of hyperparameters to set. We calculate the gradient of the loss function with respect to the network weights.</p>
        <p>This gradient tells us the direction to move toward to decrease the loss function. In other words, we use the gradient to update the weights in the hope that we can iteratively decrease the loss function to its minimum. The logic responsible for this operation is known as the solver. Solvers deserve their own separate section, though, which will come in a bit. Finally, the number of times we go through the training data over and over is called epochs and is set using the <kbd>max_iter</kbd>hyperparameter. We also may decide to stop earlier (<kbd>early_stopping</kbd>) if the model is not learning any more. The<kbd>validation_fraction</kbd>, <kbd>n_iter_no_change</kbd>, and <kbd>tol</kbd>hyperparameters help us decide when to stop. More on how they work in the next section.</p>
        <h3 id="uuid-4ea886e7-a323-4614-984f-30632539bfb0">Configuring the solvers </h3>
        <p>After calculating the loss function (also known as the cost or objective function), we need to find the optimum network weights that minimize the loss function. In the linear models from <a href="f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml">Chapter 3</a>, <em>Making Decisions with Linear Equations</em>, the loss functions were chosen to be convex. A convex function, as seen in the following figure, has one minimum, which is both its global minimum as well as its local one. This simplifies the solvers' job when trying to optimize this function. In the case of non-linear neural networks, the loss function is typically non-convex, which requires extra care during training, hence more attention is given to the solvers here:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/a6203d0d-adb7-4a27-a8d4-b9b8619b9ce4.png" style="width:57.83em;"/>
        </p>
        <p>The supported solvers for MLP can be grouped into <strong>Limited Memory</strong><strong>Broyden–Fletcher–Goldfarb–Shanno</strong> (<strong>LBFGS</strong>) and <strong>gradient descent</strong> (<strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>) and<strong/>Adam). In both variants, we want to pick a random point on the loss function, calculate its slope (gradient), and use it to figure out in which direction we should move next. Remember that in reality, we are dealing with way higher dimensions than the two-dimensional graphs shown here. Furthermore, we cannot usually see the entire graph as we can now:</p>
        <ul>
          <li>The <strong>LBFGS </strong>algorithm uses both the slope (first derivative) and the rate of change of the slope (second derivative), which helps in providing better coverage; however, it doesn't scale well with the size of the training data. It can be very slow to train, and so this algorithm is recommended for smaller datasets, unless more powerful concurrent machines come to the rescue.</li>
          <li>The <strong>gradient descent</strong><em><strong/></em>algorithm relies on the first derivative only. So, more effort is needed to help it move effectively. The calculated gradient is combined with <kbd>learning_rate</kbd>. This controls how much it moves each time after calculating the gradient. Moving too quickly may result in overshooting and missing the local minimum, while moving too slowly may cause the algorithm not to converge soon enough. We start our quest with a rate defined by <kbd>learning_rate_init</kbd>. If we set <kbd>learning_rate='constant'</kbd>, the initial rate is kept unchanged throughout the training process. Otherwise, we can set the rate to decrease with each step (in scaling) or to only decrease whenever the model is not able to learn enough anymore (adaptive).</li>
          <li><strong>Gradient descent</strong>can use the entire training data to calculate the gradient, use a single sample at a time (<kbd>sgd</kbd>), or  consume the data in small subsets (mini-batch gradient descent). These choices are controlled by <kbd>batch_size</kbd>. Having a dataset that cannot fit into memory may prevent us from using the entire dataset at once, while using small batches may cause the loss function to fluctuate. We will see this effect in practice in the next section. </li>
          <li>The problem with the learning rate is that it doesn't adapt to the shape of the curve, especially as we are only using the first derivative here. We want to control the learning speed, depending on how steep the curve at our feet is. One notable adjustment to make the learning process smarter is the concept of <kbd>momentum</kbd>. This adapts the learning process based on current and previous updates. The <kbd>momentum</kbd> is enabled for the <kbd>sgd</kbd> solver by default, and its magnitude can be set using the <kbd>momentum</kbd>hyperparameter. The<kbd>adam</kbd>solver incorporates this concept and combines it with the ability to compute separate learning rates for each one of the network weights. It is parameterized by <kbd>beta_1</kbd>and<kbd>beta_2</kbd>. They are usually kept at their default values of <kbd>0.9</kbd> and <kbd>0.999</kbd>, respectively. The <kbd>adam</kbd>solver is the default solver since it requires fewer tuning efforts compared to the <kbd>sgd</kbd> solver. Nevertheless, the <kbd>sgd</kbd> solver can converge to better solutions if tuned correctly. </li>
          <li>Finally, deciding when to stop the training process is another essential decision to make. We loop over the data more than once, bounded by the <kbd>max_iter</kbd> setting. Yet, we can stop before <kbd>max_iter</kbd> is reached if we feel that we aren't learning enough. We define how much learning is enough using <kbd>tol</kbd>, then we can stop the training process right away or give it a few more chances (<kbd>n_iter_no_change</kbd>) before we decide to stop it. Furthermore, we can set a separate fraction of the training set aside (<kbd>validation_fraction</kbd>) and use it to evaluate our learning process better. Then, if we set <kbd>early_stopping =True</kbd>, the training process will stop once the improvement for the validation set does not meet the <kbd>tol</kbd>threshold for<strong/><kbd>n_iter_no_change</kbd>epochs.</li>
        </ul>
        <p>Now that we have a good high-level picture of how things work, I feel the best way forward is to put all these hyperparameters into practice and see their effect on real data. In the next section, we will load an image dataset and use it to learn more about the aforementioned hyperparameters. </p>
        <h1 id="uuid-e375efb0-0894-4fed-b378-1ec10387402a">Classifying items of clothing </h1>
        <p>In this section, we are going to classify clothing items based on their images. We are going to use a dataset release by Zalando. Zalando is an e-commerce website based in Berlin. They released a dataset of 70,000 pictures of clothing items, along with their labels. Each item belongs to one of the following 10 labels:</p>
        <pre>{ 0: 'T-shirt/top ', 1: 'Trouser  ', 2: 'Pullover  ', 3: 'Dress  ', 4: 'Coat  ', 5: 'Sandal  ', 6: 'Shirt  ', 7: 'Sneaker  ', 8: 'Bag  ', 9: 'Ankle boot' }</pre>
        <p>The data is published on the OpenML platform, so we can easily download it using the built-in downloader in scikit-learn.</p>
        <h2 id="uuid-155f053d-2415-43e3-a224-50d894983519">Downloading the Fashion-MNIST dataset</h2>
        <p>Each dataset on the OpenML platform has a specific ID. We can give this ID to<kbd>fetch_openml()</kbd>to download the required dataset, as follows: </p>
        <pre>from sklearn.datasets import fetch_openml<br/>fashion_mnist = fetch_openml(data_id=40996) </pre>
        <p>The class labels are given as numbers. To extract their names, we can parse the following line from the description, as follows:</p>
        <pre>labels_s = '0 T-shirt/top \n1 Trouser \n2 Pullover \n3 Dress \n4 Coat \n5 Sandal \n6 Shirt \n7 Sneaker \n8 Bag \n9 Ankle boot'<br/><br/>fashion_label_translation = {<br/>    int(k): v for k, v in [<br/>        item.split(maxsplit=1) for item in labels_s.split('\n')<br/>    ]<br/>}<br/><br/>def translate_label(y, translation=fashion_label_translation):<br/>    return pd.Series(y).apply(lambda y: translation[int(y)]).values</pre>
        <p>We can also create a function similar to the one we created in <a href="b95b628d-5913-477e-8897-989ce2afb974.xhtml">Chapter 5</a>, <em>Image Processing with Nearest Neighbors,</em> to display the images in the dataset: </p>
        <pre>def display_fashion(img, target, ax):<br/><br/>    if len(img.shape):<br/>        w = int(np.sqrt(img.shape[0]))<br/>        img = img.reshape((w, w))<br/><br/>    ax.imshow(img, cmap='Greys')<br/>    ax.set_title(f'{target}')<br/>    ax.grid(False)<br/><br/></pre>
        <p>The previous function expects an image and a target label in addition to the <kbd>matplotlib</kbd> axis to display the image on. We are going to see how to use it in the upcoming sections.</p>
        <h2 id="uuid-5dc12a85-2678-41ad-8472-649bdf270387">Preparing the data for classification</h2>
        <p>When developing a model and optimizing its hyperparameters, you will need to run it over and over multiple times. Therefore, it is advised that you start working with a smaller dataset to minimize the training time. Once you reach an acceptable model, you can then add more data and do your final hyperparameter-tuning. Later on, we will see how to tell whether the data at hand is enough and whether more samples are needed; but for now, let's stick to a subset of 10,000 images.</p>
        <div class="packt_tip">I deliberately avoided setting any random states when sampling from the original dataset and when splitting the sampled data into a training and a test set. By not setting a random state, you should expect the final results to vary from one run to the other. I made this choice since my main objective here is to focus on the underlying concepts, and I did not want you to obsess over the final results. In the end, the data you will deal with in real-life scenarios will vary from one problem to the other, and we have already learned in previous chapters how to better understand the boundaries of our model's performance via cross-validation. So, in this chapter, as in many other chapters in this book, don't worry too much if the mentioned model's accuracy numbers, coefficients, or learning behavior vary slightly from yours.</div>
        <p>We will use the<kbd>train_test_split()</kbd> function twice. Initially, we will use it for sampling. Afterward, we will reuse it for its designated purpose of splitting the data into training and test sets:</p>
        <pre>from sklearn.model_selection import train_test_split<br/><br/>fashion_mnist_sample = {}<br/><br/>fashion_mnist_sample['data'], _, fashion_mnist_sample['target'], _ = train_test_split(<br/>    fashion_mnist['data'], fashion_mnist['target'], train_size=10000<br/>)<br/><br/>x, y = fashion_mnist_sample['data'], fashion_mnist_sample['target']<br/>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)</pre>
        <p>The pixels here take values between <kbd>0</kbd> and <kbd>255</kbd>. Usually, this is fine; however, the solvers we will use converge better when the data is put into tighter ranges.<kbd>MinMaxScaler</kbd> is going to help us achieve this, as can be seen in the following code, whereas <kbd>StandardScaler</kbd> is also an option:</p>
        <pre>from sklearn.preprocessing import MinMaxScaler<br/><br/>scaler = MinMaxScaler()<br/><br/>x_train = scaler.fit_transform(x_train)<br/>x_test = scaler.transform(x_test)</pre>
        <p>We can now translate the numerical labels into names using the function we created in the previous section:</p>
        <pre>translation = fashion_label_translation<br/>y_train_translated = translate_label(y_train, translation=translation)<br/>y_test_translated = translate_label(y_test, translation=translation)</pre>
        <p>If your original labels came in as strings, you can use <kbd>LabelEncoder</kbd> to convert them into numerical values:</p>
        <pre>from sklearn.preprocessing import LabelEncoder<br/><br/>le = LabelEncoder()<br/>y_train_encoded = le.fit_transform(y_train_translated)<br/>y_test_encoded = le.transform(y_test_translated)</pre>
        <p>Finally, let's use the following code to see how the images look:</p>
        <pre>import random <br/><br/>fig, axs = plt.subplots(1, 10, figsize=(16, 12))<br/><br/>for i in range(10):<br/>    rand = random.choice(range(x_train.shape[0]))<br/>    display_fashion(x_train[rand], y_train_translated[rand], axs[i])<br/><br/>fig.show()</pre>
        <p>Here, we see 10 random images alongside their labels. We loop over 10 random images and use the display function we created earlier to display them next to each other:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/80f63e13-3b73-4a80-acc6-045da971f865.png" style="width:56.92em;"/>
        </p>
        <p>Now that the data is ready, it is time to see the effect of the hyperparameters in practice.</p>
        <h2 id="uuid-b6173adc-9c45-4ba9-8f16-a7166744673d">Experiencing the effects of the hyperparameters </h2>
        <p>After the neural network is trained, you can check its weights (<kbd>coefs_</kbd>), intercepts (<kbd>intercepts_</kbd>), and the final value of the loss function (<kbd>loss_</kbd>). One additional piece of information is the computed loss after each epoch (<kbd>loss_curve_</kbd>). This trace of calculated losses is very useful for the learning process. </p>
        <p>Here, we train a neural network with two hidden layers of 100 nodes each, and we set the maximum number of epoch to <kbd>500</kbd>. We leave all the other hyperparameters to their default values for now: </p>
        <pre>from sklearn.neural_network import MLPClassifier<br/>clf = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=500)<br/>clf.fit(x_train, y_train_encoded)<br/>y_test_pred = clf.predict(x_test)</pre>
        <p>After the network is trained, we can plot the loss curve using the following line of code:</p>
        <pre>pd.Series(clf.loss_curve_).plot(<br/>    title=f'Loss Curve; stopped after {clf.n_iter_} epochs'<br/>)</pre>
        <p>This gives us the following graph: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/546e0a22-d910-4e30-920a-e1bd13a308ef.png" style="width:26.17em;"/>
        </p>
        <p>Despite the fact that the algorithm was told to continue learning for up to <kbd>500</kbd> epochs, it stopped after the 107<sup>th</sup> epoch. The default value for <kbd>n_iter_no_change</kbd> is <kbd>10</kbd> epochs. This means that the learning rate was not improving enough since the 97<sup>th</sup> epoch, and so the network came to a halt 10 epochs later. Keep in mind that <kbd>early_stopping</kbd> is set to <kbd>False</kbd> by default, which means that this decision was made regardless of the <kbd>10%</kbd> validation set that was set aside by default. If we want to use the validation set for the early stopping decision, we should set <kbd>early_stopping=True</kbd>. </p>
        <h3 id="uuid-63dafa95-8c08-4535-9480-015587feb13c">Learning not too quickly and not too slowly</h3>
        <p>As mentioned earlier, the gradient of the loss function (<em>J</em>) with respect to the weights (<em>w</em>) is used to update the network's weights. The updates are done according to the following equation, where <em>lr</em> is the learning rate:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/2443e935-17a8-4772-b7b0-f37c0e36e430.png" style="width:12.83em;"/>
        </p>
        <p>You might wonder about the need for a learning rate; why don't we just use the gradient as it is by setting <em>lr = 1</em>? In this section, we are going to answer this question by witnessing the effect of the learning rate on the training process.</p>
        <div class="packt_infobox">Another hidden gem in the MLP estimator is<kbd>validation_scores_</kbd>. Like <kbd>loss_curve_</kbd>, this one is also not documented, and its interface may change with future releases. In the case of <kbd>MLPClassifier</kbd>, <kbd>validation_scores_</kbd> keeps track of the classifier's accuracy on the validation set, whereas for<kbd>MLPRegressor</kbd>, it keeps track of the regressor's R<sup>2</sup> score instead.</div>
        <p>We are going to use the validation score (<kbd>validation_scores_</kbd>) to see the effect of the different learning rates. Since these scores are stored only when <kbd>early_stopping</kbd> is set to <kbd>True</kbd> and we do not want to stop early, we will also set <kbd>n_iter_no_change</kbd>to be the same value as<kbd>max_iter</kbd>to cancel the early stopping effect. </p>
        <p>The default learning rate is<kbd>0.001</kbd>, and it stays constant during the training process by default. Here, we are going to take an even smaller subset of the training data—1,000 samples—and try different learning rates from<kbd>0.0001</kbd>to<kbd>1</kbd>:</p>
        <pre>from sklearn.neural_network import MLPClassifier<br/><br/>learning_rate_init_options = [1, 0.1, 0.01, 0.001, 0.0001]<br/><br/>fig, axs = plt.subplots(1, len(learning_rate_init_options), figsize=(15, 5), sharex=True, sharey=True)<br/><br/>for i, learning_rate_init in enumerate(learning_rate_init_options):<br/><br/>    print(f'{learning_rate_init} ', end='')<br/><br/>    clf = MLPClassifier(<br/>        hidden_layer_sizes=(500, ), <br/>        learning_rate='constant',<br/>        learning_rate_init=learning_rate_init,<br/>        validation_fraction=0.2,<br/>        early_stopping=True, <br/>        n_iter_no_change=120,<br/>        max_iter=120, <br/>        solver='sgd',<br/>        batch_size=25,<br/>        verbose=0,<br/>    )<br/><br/>    clf.fit(x_train[:1000,:], y_train_encoded[:1000])<br/><br/>    pd.Series(clf.validation_scores_).plot(<br/>        title=f'learning_rate={learning_rate_init}', <br/>        kind='line', <br/>        color='k',<br/>        ax=axs[i]<br/>    )<br/><br/>fig.show()</pre>
        <p>The following graphs compare the progress of the validation scores for the different learning rates. The code used for formatting the axes was omitted for brevity:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/2c4f4fc2-45f2-4095-9a26-86b990738610.png" style="width:60.58em;"/>
        </p>
        <p>As we can see, when setting the learning rate to <kbd>1</kbd>, the network wasn't able to learn and the accuracy score was stuck around 10%. This is because the bigger steps taken to update the weights caused the gradient descent to overshoot and miss the local minima. Ideally, we want the gradient descent to move wisely over the curve; it shouldn't rush and miss the optimum solutions. On the other hand, we can see that a very slow learning rate, <kbd>0.0001</kbd>, caused the network to take forever to train. It's clear that <kbd>120</kbd> epochs wasn't enough, then, and more epochs were needed. For this example, a learning rate of <kbd>0.01</kbd> looks like a good balance.</p>
        <div class="packt_infobox">The concept of learning rate is commonly used in iterative methods to prevent overshooting. It might have different names and different justifications, but in essence, it serves the same purpose. For example, in the <strong>reinforcement learning</strong> field, the <strong>discount factor</strong> in the <strong>Bellman equation</strong> might resemble the learning rate here.</div>
        <h3 id="uuid-c2d1ecdf-b77b-4b09-a81b-84944941f2cb">Picking a suitable batch size</h3>
        <p>When dealing with massive training data, you don't want to use it all at once when calculating the gradient, especially when it is not possible to fit this data in memory. Using the data in small subsets is something we can configure. Here, we are going to try different batch sizes while keeping everything else constant. Keep in mind that with<kbd>batch_size</kbd>set to <kbd>1</kbd>, the model is going to be very slow as it updates its weights after each training instance: </p>
        <pre>from sklearn.neural_network import MLPClassifier<br/><br/>batch_sizes = [1, 10, 100, 1500]<br/><br/>fig, axs = plt.subplots(1, len(batch_sizes), figsize=(15, 5), sharex=True, sharey=True)<br/><br/>for i, batch_size in enumerate(batch_sizes):<br/><br/>    print(f'{batch_size} ', end='')<br/><br/>    clf = MLPClassifier(<br/>        hidden_layer_sizes=(500, ), <br/>        learning_rate='constant',<br/>        learning_rate_init=0.001, <br/>        momentum=0,<br/>        max_iter=250, <br/>        early_stopping=True,<br/>        n_iter_no_change=250,<br/>        solver='sgd',<br/>        batch_size=batch_size,<br/>        verbose=0,<br/>    )<br/><br/>    clf.fit(x_train[:1500,:], y_train_encoded[:1500])<br/><br/>    pd.Series(clf.validation_scores_).plot( <br/>        title=f'batch_size={batch_size}',<br/>        color='k',<br/>        kind='line', <br/>        ax=axs[i]<br/>    )<br/><br/>fig.show()</pre>
        <p>This figure gives us a visual comparison between the four batch size settings and their effects. Parts of the formatting code were omitted for brevity:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/6ade3c82-da3d-46fa-9ecb-48a266a7efa1.png" style="width:52.92em;"/>
        </p>
        <p>You can see why the use of mini-batch gradient descent is becoming the norm among practitioners, not only because of the memory constraints but also because smaller batches helped our model here learn better. This final outcome was achieved despite the higher fluctuations in the validation scores for the smaller batch sizes. On the other hand, setting <kbd>batch_size</kbd> to <kbd>1</kbd> slows down the learning process. </p>
        <p>So far, we have tweaked multiple hyperparameters and witnessed their effects on the training process. In addition to these hyperparameters, two additional questions are still waiting for answers:</p>
        <ul>
          <li>How many training samples are enough?</li>
          <li>How many epochs are enough?</li>
        </ul>
        <h3 id="uuid-0adb179e-c798-4de5-9e9d-3f62f9e1c5d0">Checking whether more training samples are needed</h3>
        <p>We want to compare when the entire training sample (100%) is used when 75%, 50%, 25%, 10%, and 5% of it is used. The<kbd>learning_curve</kbd>function is useful for this comparison. It uses cross-validation to calculate the average training and test scores for the different sample sizes. Here, we are going to define the different sampling ratios and specify that three-fold cross-validation is needed:</p>
        <pre>from sklearn.model_selection import learning_curve<br/><br/>train_sizes = [1, 0.75, 0.5, 0.25, 0.1, 0.05]<br/><br/>train_sizes, train_scores, test_scores = learning_curve(<br/>    MLPClassifier(<br/>        hidden_layer_sizes=(100, 100), <br/>        solver='adam',<br/>        early_stopping=False<br/>    ), <br/>    x_train, y_train_encoded,<br/>    train_sizes=train_sizes,<br/>    scoring="precision_macro",<br/>    cv=3,<br/>    verbose=2,<br/>    n_jobs=-1<br/>)</pre>
        <p>When done, we can use the following code to plot the progress of the training and test scores with an increase in the sample size:</p>
        <pre>df_learning_curve = pd.DataFrame(<br/>    {<br/>        'train_sizes': train_sizes,<br/>        'train_scores': train_scores.mean(axis=1),<br/>        'test_scores': test_scores.mean(axis=1)<br/>    }<br/>).set_index('train_sizes')<br/><br/>df_learning_curve['train_scores'].plot(<br/>    title='Learning Curves', ls=':',<br/>)<br/><br/>df_learning_curve['test_scores'].plot(<br/>    title='Learning Curves', ls='-',<br/>)</pre>
        <p>The resulting graphs show the increase in the classifier's accuracy with more training data. Notice how the training score is constant, while the test score is what we really care about, and it seems to saturate after a certain amount of data:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/556e220c-917d-4989-8626-32b5f245f67a.png" style="width:49.00em;"/>
        </p>
        <p>Earlier in this chapter, we took a sample of 10,000 images out of the original 70,000 images. We then split it into 8,000 for training and 2,000 for testing. From the learning curve graph, we can see that it is possible to settle for an even smaller training set. Somewhere after 2,000 the additional samples don't add much value. </p>
        <div class="packt_tip">Usually, we want to use as many data samples as we have to train our models. Nevertheless, when tuning the model's hyperparameters, you need to compromise and use a smaller sample to speed up the development process. Once that's done, it is then advised to train the final model on the entire dataset.</div>
        <h3 id="uuid-d3c153e3-122d-4aa2-b0b3-fed64b8ff9f5">Checking whether more epochs are needed</h3>
        <p>This time, we are going to use the <kbd>validation_curve</kbd> function. It works in a similar fashion to the <kbd>learning_curve</kbd> function, but rather than comparing the different training sample sizes, it compares the different hyperparameter settings. Here, we will see the effect of using different values for <kbd>max_iter</kbd>:</p>
        <pre>from sklearn.model_selection import validation_curve<br/><br/>max_iter_range = [5, 10, 25, 50, 75, 100, 150]<br/><br/>train_scores, test_scores = validation_curve(<br/>    MLPClassifier(<br/>        hidden_layer_sizes=(100, 100), <br/>        solver='adam',<br/>        early_stopping=False<br/>    ), <br/>    x_train, y_train_encoded,<br/>    param_name="max_iter", param_range=max_iter_range,<br/>    scoring="precision_macro",<br/>    cv=3,<br/>    verbose=2,<br/>    n_jobs=-1<br/>)</pre>
        <p>With the training and test scores, we can plot them as we did in the previous section to get the following graph:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/58a6a67e-9f20-48a1-b3d2-e96329b83132.png" style="width:53.75em;"/>
        </p>
        <p>In this example, we can see that the test score stopped improving roughly after <kbd>25</kbd> epochs. The training score continued to improve beyond that until it reached 100%, which is a symptom of overfitting. In practice, we may not need this graph as we use the <kbd>early_stopping</kbd>, <kbd>tol</kbd>, and <kbd>n_iter_no_change</kbd> hyperparameters to stop the training process once we have learned enough and before we overfit. </p>
        <h2 id="uuid-5bcb1e75-b94c-4a8e-b57f-00eee86e2397">Choosing the optimum architecture and hyperparameters </h2>
        <p>So far, we haven't talked about the network architecture. How many layers should we have and how many nodes should we put in each layer? We also haven't compared the different activation functions. As you can see, there are plenty of hyperparameters to choose from. Previously in this book, we mentioned tools such as <kbd>GridSearchCV</kbd> and <kbd>RandomizedSearchCV</kbd> that help you pick the best hyperparameters. These are still good tools to use, but they can be too slow if we decide to use them to tune every possible value for every parameter we have. They can also become too slow when used with too many training samples or for too many epochs.</p>
        <p>The tools we have seen in the previous sections should help us find our needle in a slightly smaller haystack by ruling out some hyperparameter ranges. They will also allow us to stick to smaller datasets and for shorter training times. Then, we can effectively use <kbd>GridSearchCV</kbd> and<kbd>RandomizedSearchCV</kbd>to fine-tune our neural network. </p>
        <p>Parallelism is also advised where possible. <kbd>GridSearchCV</kbd> and<em><strong/></em><kbd>RandomizedSearchCV</kbd> allow us to use the different processors on our machines to train multiple models at the same time. We can achieve that via the <kbd>n_jobs</kbd> setting. This means that you can significantly speed up the hyperparameter-tuning process by using machines with a high number of processors. As for the data size, since we are going to perform k-fold cross-validation and the training data will be split further down, we should add more data than the amount estimated in the previous section. Now, without further ado, let's use<kbd>GridSearchCV</kbd> to tune our network: </p>
        <pre>from sklearn.model_selection import GridSearchCV<br/><br/>param_grid = {<br/>    'hidden_layer_sizes': [(50,), (50, 50), (100, 50), (100, 100), (500, 100), (500, 100, 100)],<br/>    'activation': ['logistic', 'tanh', 'relu'],<br/>    'learning_rate_init': [0.01, 0.001],<br/>    'solver': ['sgd', 'adam'],<br/>}<br/><br/>gs = GridSearchCV(<br/>    estimator=MLPClassifier(<br/>        max_iter=50,<br/>        batch_size=50,<br/>        early_stopping=True,<br/>    ), <br/>    param_grid=param_grid,<br/>    cv=4,<br/>    verbose=2,<br/>    n_jobs=-1<br/>)<br/><br/>gs.fit(x_train[:2500,:], y_train_encoded[:2500])</pre>
        <p>It ran for 14 minutes on four CPUs, and the following hyperparameters were picked:</p>
        <ul>
          <li><strong>Activation</strong>: <kbd>relu</kbd></li>
          <li><strong>Hidden layer sizes</strong>: <kbd>(500, 100)</kbd></li>
          <li><strong>Initial learning rate</strong>: <kbd>0.01</kbd></li>
          <li><strong>Solver</strong>: <kbd>adam</kbd></li>
        </ul>
        <div class="cell code_cell rendered unselected">
          <div class="input">
            <div class="prompt_container">
              <p class="prompt input_prompt">The selected model achieved a <strong>micro F-score</strong> of <strong>85.6%</strong> on the test set. By using the <kbd>precision_recall_fscore_support</kbd> function, you can see in more detail which classes were easier to predict than others:</p>
            </div>
          </div>
        </div>
        <div class="CDPAlignCenter CDPAlign">
          <img src="assets/f4e18add-f143-458a-893f-b185ce43422a.png" style="width:34.25em;"/>
        </div>
        <div>
          <p>Ideally, we should retrain again using the entire training set, but I've left this for now. In the end, developing an optimum neural network is usually seen as a mixture of art and science. Nevertheless, knowing your hyperparameters and how to measure their effects should make it a straightforward endeavor. Then, tools such as <kbd>GridSearchCV</kbd> and <kbd>RandomizedSearchCV</kbd> are at your disposal for automating parts of the process. Automation trumps dexterity many times.</p>
          <p>Before moving on to the next topic, I'd like to digress a bit and show you how to build your own activation function.</p>
        </div>
        <h2 id="uuid-f92ad773-2135-4c6b-8de1-957ed662ac32">Adding your own activation function</h2>
        <p>One common problem with many activation functions is the vanishing gradient problem. If you look at the curves for the <kbd>logistic</kbd> and <kbd>tanh</kbd> activation functions, you can see that for high positive and negative values, the curve is almost horizontal. This means that the gradient of the curve is almost constant for these high values. This hinders the learning process. The <kbd>relu</kbd> activation function tried to solve this problem for one part but failed to deal with it for the negative values. This drove the researchers to keep proposing different activations functions. Here, we are going to compare the <strong>ReLU</strong> activation to a modified version of it, <strong>Leaky ReLU</strong>:</p>
        <div class="CDPAlignCenter CDPAlign">
          <img src="assets/18e43923-6e2d-415a-b2a7-bc78953f2730.png" style="width:56.42em;"/>
        </div>
        <div class="cell code_cell rendered unselected">
          <p>As you can see in the <strong>Leaky ReLU</strong> example, the line is not constant for the negative values anymore, but rather, decreasing at a small rate. To add <strong>Leaky ReLU</strong>, I had to look for how the <kbd>relu</kbd> function is built in scikit-learn and shamelessly modify the code for my needs. There are basically two methods to build. The methods are used in the forward path and just apply the activation function on its inputs, while the second method applies the derivative of the activation function to the calculated error. Here are the two existing methods for <kbd>relu</kbd>, after I slightly modified the code for brevity:</p>
          <pre>def relu(X):<br/>    return np.clip(X, 0, np.finfo(X.dtype).max)<br/><br/><span class="pl-k">def</span><span class="pl-en">inplace_relu_derivative</span>(<span class="pl-v">Z</span>, <span class="pl-s1">delta</span>):<br/><span class="pl-s1">    delta</span>[<span class="pl-v">Z</span><span class="pl-c1">==</span><span class="pl-c1">0</span>] <span class="pl-c1">=</span><span class="pl-c1">0</span></pre>
          <p>In the first method, NumPy's <kbd>clip()</kbd> method is used to set the negative values to <kbd>0</kbd>. Since the <kbd>clip</kbd> method requires both the lower- and upper-bounds, the cryptic part of the code just gets the maximum values of this data type to set it as the upper-bound. The second method takes the output of the activation function (<kbd>Z</kbd>), as well as the calculated error (<kbd>delta</kbd>). It is supposed to multiply the error by the gradient of the activation's output. Nevertheless, for this particular activation function, the gradient is <kbd>1</kbd> for positive values and <kbd>0</kbd> for the negative values. So, the error was set to <kbd>0</kbd> for the negative values—that is, it was set to <kbd>0</kbd> whenever <kbd>relu</kbd> returned <kbd>0</kbd>.</p>
        </div>
        <p><kbd>leaky_relu</kbd> keeps the positive values unchanged and multiplies the negative values by a small value, <kbd>0.01</kbd>. Now, all we need to do is to build out new methods using this information: </p>
        <pre>leaky_relu_slope = 0.01<br/><br/>def leaky_relu(X):<br/>    X_min = leaky_relu_slope * np.array(X)<br/>    return np.clip(X, X_min, np.finfo(X.dtype).max)<br/><br/>def inplace_leaky_relu_derivative(Z, delta):<br/>    delta[Z &lt; 0] = leaky_relu_slope * delta[Z &lt; 0]</pre>
        <p>Recall that the slope for <kbd>leaky_relu</kbd> is <kbd>1</kbd> for the positive values and is equal to the <kbd>leaky_relu_slope</kbd> constant for the negative values. That's why we multiplied the deltas where <kbd>Z</kbd> is negative by <kbd>leaky_relu_slope</kbd>. Now, before using our new methods, we have to inject them into the scikit-learn's code base, as follows:</p>
        <pre>from sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES<br/><br/>ACTIVATIONS['leaky_relu'] = leaky_relu<br/>DERIVATIVES['leaky_relu'] = inplace_leaky_relu_derivative</pre>
        <p>Then, you can just use <kbd>MLPClassifier</kbd> as if it were there from the beginning:</p>
        <pre>clf = MLPClassifier(activation='leaky_relu')</pre>
        <p>Hacking libraries like these forces us to read its source code and understand it better. It also shows the value of open source, where you are not bounded by what is already there. In the next section, we are going to continue hacking and build our own convolutional layers. </p>
        <h1 id="uuid-8a166263-9b7e-47ae-818c-8b3da553cd29">Untangling the convolutions</h1>
        <div class="packt_quote">"Look deep into nature, and then you will understand everything better"</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– Albert Einstein</div>
        <p>No chapter about the use of neural networks to classify images is allowed to end without touching on CNNs. Despite the fact that scikit-learn does not implement convolutional layers, we can still understand the concept and see how it works. </p>
        <p>Let's start with the following <em>5 </em>x <em>5</em> image and see how to apply a convolutional layer to it:</p>
        <pre>x_example = array(<br/>    [[0, 0, 0, 0, 0],
     [0, 0, 0, 0, 0],
     [0, 0, 1, 1, 0],
     [0, 0, 1, 1, 0],
     [0, 0, 0, 0, 0]]<br/>)</pre>
        <p>In natural language processing, words usually serve as a middle ground between characters and entire sentences when it comes to feature extraction. In this image, maybe smaller patches serve as better units of information than a separate pixel. The objective of this section is to find ways to represent these small <em>2 x 2</em>, <em>3 x 3</em>, or <em>N x N</em> patches in an image. We can start with averages as summaries. We can basically take the average of each <em>3 x 3</em> patch by multiplying each pixel in it by 1, and then dividing the total by 9; there are 9 pixels in the patch. For the pixels on the edges, as they don't have neighbors in all directions, we can pretend that there is an extra 1-pixel border around the image where all pixels are set to 0. By doing so, we get another <em>5 x 5</em> array.</p>
        <p>This kind of operation is known as <strong>convolutions</strong>, and <strong>SciPy</strong> provides a way of doing it. The <em>3 x 3</em> all-ones matrix used and is also known as the kernel or weights. Here, we specify the all-ones kernel and divide by 9 later. We also specify the need for an all-zeros border by setting<kbd>mode</kbd> to <kbd>constant</kbd> and <kbd>cval</kbd> to <kbd>0</kbd>, as you can see in the following code:</p>
        <pre>from scipy import ndimage<br/><br/>kernel = [[1,1,1],[1,1,1],[1,1,1]] <br/>x_example_convolve = ndimage.convolve(x_example, kernel, mode='constant', cval=0)<br/>x_example_convolve = x_example_convolve / 9 </pre>
        <p>Here is a comparison between the original image and the output of the convolution: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/0d901206-8df7-41d1-86c1-e1d2b9dc94ee.png" style="width:52.08em;"/>
        </p>
        <p>Calculating the mean gave us a blurred version of the original image, so next time you need to blur an image, you know what to do. Multiplying each pixel by a certain weight and calculating the sum of these products sounds like a linear model. Furthermore, we can think of averages as linear models where all weights are set to <img class="fm-editor-equation" src="assets/f1e2a388-dfb2-4f7a-ba68-5bd41656920e.png" style="width:1.50em;"/>. So, you may say that we are building mini-linear models for each patch of the image. Keep this analogy in mind, but for now, we have to set the model's weights by hand.</p>
        <p>While each patch gets the exact same linear model as the others, there is nothing stopping the pixels within each patch from being multiplied by different weights. In fact, different kernels with different weights give different effects. In the next section, we are going to witness the effect of the different kernels on our <strong>Fashion-MNIST</strong> dataset.</p>
        <h2 id="uuid-452aa131-cc7b-4d61-a4b5-9ede2fd4db96">Extracting features by convolving</h2>
        <p>Rather than dealing with the images one by one, we can tweak the code to convolve over multiple images at once. The images in our Fashion-MNIST dataset are flattened, so we need to reshape them into <em>28 </em>x <em>28</em> pixels each. Then, we convolve using the given kernel, and finally, make sure that all pixel values are between <kbd>0</kbd> and <kbd>1</kbd> using our favorite <kbd>MinMaxScaler</kbd> parameter:</p>
        <pre>from scipy import ndimage<br/>from sklearn.preprocessing import MinMaxScaler<br/><br/>def convolve(x, kernel=[[1,1,1],[1,1,1],[1,1,1]]):<br/>    w = int(np.sqrt(x.shape[1]))<br/>    x = ndimage.convolve(<br/>        x.reshape((x.shape[0], w, w)), [kernel], <br/>        mode='constant', cval=0.0<br/>    ) <br/>    x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]) <br/>    return MinMaxScaler().fit_transform(x)</pre>
        <p>Next, we can use it as our training and test data, as follows:</p>
        <pre>sharpen_kernel = [[0,-1,0], [-1,5,-1], [0,-1,0]]<br/>x_train_conv = convolve(x_train, sharpen_kernel)<br/>x_test_conv = convolve(x_test, sharpen_kernel)</pre>
        <p>Here are few kernels: the first one is used to sharpen an image, then comes a kernel to emphasize vertical edges, while the last one emphasizes the horizontal ones: </p>
        <ul>
          <li><strong>Sharpen</strong>: <kbd>[[0,-1,0], [-1,5,-1], [0,-1,0]]</kbd></li>
          <li><strong>V-edge</strong>: <kbd>[[-1,0,1], [-2,0,2], [-1,0,1]]</kbd></li>
          <li><strong>H-edge</strong>: <kbd>[[-1,-2,-1], [0,0,0], [1,2,1]]</kbd></li>
        </ul>
        <p>Giving those kernels to the convolve function we have just created will give us the following effects:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/19a88a69-803a-46a5-870e-471a223e3b9b.png" style="width:54.08em;"/>
        </p>
        <p>You can find more kernels on the internet, or you can try your own and see the effects they make. The kernels are also intuitive; the sharpening kernel clearly gives more weight to the central pixels versus its surroundings.</p>
        <p>Each of the different convolutional transformations captures certain information from our images. We can thussee them as a feature engineering layer, where we extract features to feed to our classifier. Nevertheless, the size of our data will grow with each additional convolutional transformation we append to our data. In the next section, we will see how to deal with this issue. </p>
        <h2 id="uuid-c263273c-3285-472b-8aa7-b731de2168bf">Reducing the dimensionality of the data via max pooling</h2>
        <p>Ideally, we would like to feed the outputs from more than one of the previous convolutional transformations into our neural network. Nevertheless, if our images are made of 784 pixels, then concatenating the outputs of just three convolutional functions will result in 2,352 features, <em>784 x 3</em>. This will slow down our training process, and as we have learned earlier in this book, the more features are not always the merrier.</p>
        <p>To shrink an image to one-quarter of its size—that is, half its width and half its height—you can divide it into multiple <em>2 x 2</em> patches, then take the maximum value in each of these patches to represent the entire patch. This is exactly what <strong>max pooling</strong> does. To implement it, we need to install another library called <kbd>scikit-image</kbd><span class="n"> using <kbd>pip</kbd> in your computer terminal:</span></p>
        <pre><span class="n">pip</span><span class="n">install</span><span class="n">sciki</span>t-image</pre>
        <p>Then, we can create our max pooling function, as follows:</p>
        <pre><br/>from skimage.measure import block_reduce<br/>from sklearn.preprocessing import MinMaxScaler<br/><br/>def maxpool(x, size=(2,2)):<br/>    w = int(np.sqrt(x.shape[1]))<br/>    x = np.array([block_reduce(img.reshape((w, w)), block_size=(size[0], size[1]), func=np.max) for img in x])<br/>    x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]) <br/>    return MinMaxScaler().fit_transform(x)</pre>
        <p>We can then apply it to the outputs of one of the convolutions, as follows:</p>
        <pre>x_train_maxpool = maxpool(x_train_conv, size=(5,5))<br/>x_test_maxpool = maxpool(x_test_conv, size=(5,5))</pre>
        <p>Applying max pooling on <em>5 x 5</em> patches will reduce the size of our data from <em>28 x 28</em> to <em>6 x 6</em>, which is less than 5% of its original size.</p>
        <h2 id="uuid-2af2d09c-36a7-41ae-933a-1343dab501d5">Putting it all together</h2>
        <p>The <kbd>FeatureUnion</kbd> pipeline in scikit-learn can combine the output of multiple transformers. In other words, if scikit-learn had transformers that could convolve over images and max pool the output of these convolutions, you would have been able to combine the outputs of more than one of these transformers, each with its specific kernel. Luckily, we can build this transformer ourselves and combine their outputs via <kbd>FeatureUnion</kbd>. We just need them to provide the fit, transform, and fit_transform methods, as follows:  </p>
        <pre>class ConvolutionTransformer:<br/><br/>    def __init__(self, kernel=[], max_pool=False, max_pool_size=(2,2)):<br/>        self.kernel = kernel<br/>        self.max_pool = max_pool<br/>        self.max_pool_size = max_pool_size<br/><br/>    def fit(self, x):<br/>        return x<br/><br/>    def transform(self, x, y=None):<br/>        x = convolve(x, self.kernel)<br/>        if self.max_pool:<br/>            x = maxpool(x, self.max_pool_size)<br/>        return x<br/><br/>    def fit_transform(self, x, y=None):<br/>        x = self.fit(x)<br/>        return self.transform(x)</pre>
        <p>You can specify the kernel to use at the initialization step. You can also skip the max pooling part by setting <kbd>max_pool</kbd> to <kbd>False</kbd>. Here, we define our three kernels and combine their outputs while pooling each <em>4 x 4</em> patch in our images:</p>
        <pre>kernels = [<br/>    ('Sharpen', [[0,-1,0], [-1,5,-1], [0,-1,0]]),<br/>    ('V-Edge', [[-1,0,1], [-2,0,2], [-1,0,1]]),<br/>    ('H-Edge', [[-1,-2,-1], [0,0,0], [1,2,1]]),<br/>]<br/><br/>from sklearn.pipeline import FeatureUnion<br/><br/>funion = FeatureUnion(<br/>    [<br/>        (kernel[0], ConvolutionTransformer(kernel=kernel[1], max_pool=True, max_pool_size=(4,4)))<br/>        for kernel in kernels<br/>    ]<br/>)<br/><br/>x_train_convs = funion.fit_transform(x_train)<br/>x_test_convs = funion.fit_transform(x_test)</pre>
        <p>Then, we can use the output of the <kbd>FeatureUnion</kbd> pipeline into our neural network, as follows:</p>
        <pre>from sklearn.neural_network import MLPClassifier<br/><br/>mlp = MLPClassifier(<br/>    hidden_layer_sizes=(500, 300),<br/>    activation='relu',<br/>    learning_rate_init=0.01,<br/>    solver='adam',<br/>    max_iter=80,<br/>    batch_size=50,<br/>    early_stopping=True,<br/>)<br/><br/>mlp.fit(x_train_convs, y_train)<br/>y_test_predict = mlp.predict(x_test_convs)</pre>
        <p>This network achieved a <strong>micro F-score</strong> of <strong>79%</strong>. You may try adding more kernel and tune the network's hyperparameters and see whether we can achieve a better score than the one we got without the convolutions.    </p>
        <div class="packt_infobox">We had to set the kernel weights of the convolutions by hand. We then displayed their outputs to see whether they make intuitive sense and hope they will improve our model's performance when used. This doesn't sound like a real data-driven approach. You would ideally want the weights to be learned from the data. That's exactly what the real CNNs do. I would suggest you look into TensorFlow and PyTorch for their CNN implementations. It would be nice if you could compare their accuracy to the model we have built here. </div>
        <h1 id="uuid-ffc0052a-62ea-457e-8147-646e9fdc99be">MLP regressors</h1>
        <p>As well as <kbd>MLPClassifier</kbd>, there is its regressor sibling,<kbd>MLPRegressor</kbd>. The two share an almost identical interface.The main difference between the two is the loss functions used by each of them and the activation functions of the output layer. The regressor optimizes a squared loss, and the last layer is activated by an identity function. All other hyperparameters are the same, including the four activation options for the hidden layers. </p>
        <p>Both estimators have a <kbd>partial_fit()</kbd>method. You can use it to update the model once you get a hold of additional training data after the estimator has already been fitted.<kbd>score()</kbd>in <kbd>MLPRegressor</kbd>calculates the regressor's<em><strong/></em>R<em><sup>2</sup>,</em> as opposed to the classifier's accuracy, which is calculated by<kbd>MLPClassifier</kbd><em>.<strong/></em></p>
        <h1 id="uuid-6f74585b-aca0-4798-a436-0b9875c3bb74">Summary</h1>
        <p>We have now developed a good understanding of ANNs and their underlying technologies. I'd recommend libraries suchas TensorFlow and PyTorch for more complex architecture and for scaling up the training process on GPUs. However, you have a good headstart already. Most of the concepts discussed here are transferable to any other library. You will be using more or less the same activation functions and the same solvers, as well as most of the other hyperparameters discussed here. scikit-learn's implementation is still good for prototyping and for cases where we want to move beyond linear models without the need for too many hidden layers. </p>
        <p>Furthermore, the solvers discussed here, such as gradient descent, are so ubiquitous in the field of machine learning, and so understanding their concepts is also helpful for understanding other algorithms that aren't neural networks. We saw earlier how gradient descent is used in training linear and logistic regressors as well as support vector machines. We are also going to use them with the gradient boosting algorithms we will look at in the next chapter.</p>
        <p>Concepts such as the learning rate and how to estimate the amount of training data needed are good to have at your disposal, regardless of what algorithm you are using. These concepts were easily applied here, thanks to the helpful tools provided by scikit-learn. I sometimes find myself using scikit-learn's tools even when I'm not building a machine learning solution. </p>
        <p class="mce-root">If ANNs and deep learning are the opium of the media, then ensemble algorithms are the bread and butter for most practitioners when solving any business problem or when competing for a $10,000 prize on Kaggle.</p>
        <p class="mce-root">In the next chapter, we are going to learn about the different ensemble methods and their theoretical background, and then get our hands dirty fine-tuning their hyperparameters. </p>
      </article>
    </section>
  </body></html>
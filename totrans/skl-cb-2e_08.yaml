- en: Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover these recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying data with a linear SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing an SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiclass classification with SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will start by using a **support vector machine** (**SVM**)
    with a linear kernel to get a rough idea of how SVMs work. They create a hyperplane,
    or linear surface in several dimensions, which best separates the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In two dimensions, this is easy to see: the hyperplane is a line that separates
    the data. We will see the array of coefficients and intercept of the SVM. Together
    they uniquely describe a scikit-learn linear SVC predictor.'
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of the chapter, the SVMs have a **radial basis function** (**RBF**)
    kernel. They are nonlinear, but with smooth separating surfaces. In practice,
    SVMs work well with many datasets and thus are an integral part of the `scikit-learn`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying data with a linear SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapter, we saw some examples of classification with SVMs. We focused
    on SVMs' slightly superior classification performance compared to logistic regression,
    but for the most part, we left SVMs alone.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will focus on them more closely. While SVMs do not have an easy probabilistic
    interpretation, they do have an easy visual-geometric one. The main idea behind
    linear SVMs is to separate two classes with the best possible plane.
  prefs: []
  type: TYPE_NORMAL
- en: Let's linearly separate two classes with an SVM.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us start by loading and visualizing the iris dataset available in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load part of the iris dataset. This will allow for easy comparison with the
    first chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will use a NumPy mask to focus on the first two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the two classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Plot the classes `0` and `1` with matplotlib. Recall that the notation `X_0[:,0]`
    refers to the first column of a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, `X_0` refers to the subset of inputs `X` that correspond
    to the target `y` being `0` and `X_1` is a subset with a matching target value
    of `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e3e10b08-ae0c-441c-a69c-3b72d5ab0389.png)'
  prefs: []
  type: TYPE_IMG
- en: From the graph, it is clear that we could find a straight line to separate these
    two classes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process of finding the SVM line is straightforward. It is the same process
    as with any scikit-learn supervised learning estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: Create training and testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an SVM model instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the SVM model to the loaded data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict with the SVM model and measure the performance of the model in preparation
    for predictions of unseen data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the dataset of the first two features of the first two classes. Stratify
    the target set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an SVM model instance. Set the kernel to be linear, as we want a line
    to separate the two classes that are involved in this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model (train the model):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict using the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Measure the performance of the SVM on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It did perfectly on the test set. This is not surprising, because when we visualized
    each class, they were easy to visually separate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualize the decision boundary, the line separating the classes, by using
    the estimator on a two-dimensional grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the grid by coloring the predictions. Note that we have amended the previous
    visualization to include SVM predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fe389784-e987-4481-9a99-a48e372f59b2.png)'
  prefs: []
  type: TYPE_IMG
- en: We fleshed out the SVM linear decision boundary by predicting on a two-dimensional
    grid.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At times, it could be computationally expensive to predict on a whole grid of
    points, especially if the SVM is predicting many classes in many dimensions. In
    these cases, you will need access to the geometric information of the SVM decision
    boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear decision boundary, a hyperplane, is uniquely specified by a vector
    normal to the hyperplane and an intercept. The normal vectors are contained in
    the SVM instance''s `coef_ data` attribute. The intercepts are contained in the
    SVM instance''s `intercept_ data` attribute. View these two attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You might be able to quickly see that the `coef_[0]` vector is perpendicular
    to the line we drew to separate both of the iris classes we have been viewing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every time, these two NumPy arrays, `svm_inst.coef_` and `svm_inst.intercept_`,
    will have the same number of rows. Each row corresponds to each plane separating
    the classes involved. In the example, there are two classes linearly separated
    by one hyperplane. The particular SVM type, SVC in this case, implements a one-versus-one
    classifier: it will draw a unique plane separating every pair of classes involved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were trying to separate three classes, there would be three possible
    combinations, 3 x 2/2 = 3\. For *n* classes, the number of planes provided by
    SVC is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85dc7ab5-c651-4f01-8127-93b16db2445c.png)'
  prefs: []
  type: TYPE_IMG
- en: The number of columns in the `coef_ data` attribute is the number of features
    in the data, which in this case is two.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the decision in regards to a point in space, solve the following equation
    for zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a977e6ac-9fac-498b-8618-c28efc373378.png)'
  prefs: []
  type: TYPE_IMG
- en: If you only desire the uniqueness of the plane, store the tuple `(coef_, intercept_)`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Additionally, you can view the the parameters of the instance to learn more
    about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Traditionally, the SVC prediction performance is optimized over the following
    parameters: C, gamma, and the shape of the kernel. C describes the margin of the
    SVM and is set to one by default. The margin is the empty space on either side
    of the hyperplane with no class examples. If your dataset has many noisy observations,
    try higher Cs with cross-validation. C is proportional to error on the margin,
    and as C gets higher in value, the SVM will try to make the margin smaller.'
  prefs: []
  type: TYPE_NORMAL
- en: A final note on SVMs is that we could re-scale the data and test that scaling
    with cross-validation. Conveniently, the iris dataset has units of cms for all
    of the inputs so re-scaling is not necessary but for an arbitrary dataset you
    should look into it.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing an SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example we will continue with the iris dataset, but will use two classes
    that are harder to tell apart, the Versicolour and Virginica iris species.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section we will focus on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting up a scikit-learn pipeline**: A chain of transformations with a predictive
    model at the end'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A grid search**: A performance scan of several versions of SVMs with varying
    parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load two classes and two features of the iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Begin by splitting the data into training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Construct a pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Then construct a pipeline with two steps: a scaling step and an SVM step. It
    is best to scale the data before passing it to an SVM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the pipeline, the scaling step has the name `scaler` and the SVM
    has the name `svc`. The names will be crucial in the next step. Note that the
    default SVM is an RBF SVM, which is nonlinear.
  prefs: []
  type: TYPE_NORMAL
- en: Construct a parameter grid for a pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vary the relevant RBF parameters, C and gamma, logarithmically, varying by
    one order of magnitude at a time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, construct the parameter grid by making it into a dictionary. The SVM
    parameter dictionary key names begin with `svc__`, taking the pipeline SVM name
    and adding two underscores. This is followed by the parameter name within the
    SVM estimator, `C` and `gamma`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Provide a cross-validation scheme
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a stratified and shuffled split. The `n_splits` parameter
    refers to the number of splits, or tries, the dataset will be split into. The
    `test_size` parameter is how much data will be left out for testing within the
    fold. The estimator will be scored using the test set on each fold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The most important element of the stratified shuffle is that each fold preserves
    the proportion of samples for each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a plain cross-validation scheme, set `cv` to an integer representing the
    number of folds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Perform a grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three required elements for a grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: An estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A parameter grid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cross-validation scheme
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have those three elements. Set up the grid search. Run it on the training
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Look up the best parameters found with the grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Look up the best score, that pertains to the best estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us look at additional perspectives of SVM for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Randomized grid search alternative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn's `GridSearchCV` performs a full scan for the best set of parameters
    for the estimator. In this case, it searches the 5 x 5 = 25 (C, gamma) pairs specified
    by the `param_grid` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative would have been using `RandomizedSearchCV`, by using the following
    line instead of the one used with `GridSearchCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It yields the same `C` and `gamma`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the nonlinear RBF decision boundary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Visualize the RBF decision boundary with code similar to the previous recipe.
    First, create a grid and predict to which class each point on the grid corresponds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now visualize the grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in the resulting graph, the RBF curve looks quite straight, but it
    really corresponds to a slight curve. This is an SVM with gamma = 0.1 and C =
    0.001:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc24acfb-8367-4673-86d1-c8af1c559b6e.png)'
  prefs: []
  type: TYPE_IMG
- en: More meaning behind C and gamma
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More intuitively, the gamma parameter determines how influential a single example
    can be per units of distance. If gamma is low, examples have an influence at long
    distances. If gamma is high, their influence is only over short distances. The
    SVM selects support vectors in its implementation, and gamma is inversely proportional
    to the radius of influence of these vectors.
  prefs: []
  type: TYPE_NORMAL
- en: With regard to C, a low C makes the decision surface smoother, while a high
    C makes the SVM try to classify all the examples correctly and leads to surfaces
    that are not as smooth.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification with SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin expanding the previous recipe to classify all iris flower types based
    on two features. This is not a binary classification problem, but a multiclass
    classification problem. These steps expand on the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SVC classifier (scikit's SVC) can be changed slightly in the case of multiclass
    classifications. For this, we will use all three classes of the iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load two features for each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OneVsRestClassifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load `OneVsRestClassifier` within a pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up a parameter grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Construct the parameter grid. Note the very special syntax to denote the `OneVsRestClassifier`
    SVC. The parameter key names within the dictionary start with `svc__estimator__`
    when named `svc` within the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Load a randomized hyperparameter search. Fit it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Look up the best parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Visualize it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to predict the category of every point in a two-dimensional grid
    by calling the trained SVM to predict along the grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/efceac4a-81bf-44db-ba2e-15889c0a8e6c.png)'
  prefs: []
  type: TYPE_IMG
- en: The boundaries generated by SVM tend to be  smooth curves, very different from
    the tree-based boundaries we will see in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `OneVsRestClassifier` creates many binary SVC classifiers: one for each
    class versus the rest of the classes. In this case, three decision boundaries
    will be computed because there are three classes. This type of classifier is easy
    to conceptualize because there are fewer decision boundaries and surfaces.'
  prefs: []
  type: TYPE_NORMAL
- en: If there were 10 classes, there would be 10 x 9/2 = 45 surfaces if SVC was the
    default `OneVsOneClassifier`. On the other hand, there would be 10 surfaces for
    the `OneVsAllClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will capitalize on the SVM classification recipes by performing support vector
    regression on scikit-learn's diabetes dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the diabetes dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data in training and testing sets. There is no stratification for
    regression in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a `OneVsRestClassifier` within a pipeline and **s****upport vector regression**
    (**SVR**) from `sklearn.svm`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a parameter grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform a randomized search of the best hyperparameters, C and gamma:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the best parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the best score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The score does not seem very good. Try different algorithms with different score
    setups and see which one performs best.
  prefs: []
  type: TYPE_NORMAL

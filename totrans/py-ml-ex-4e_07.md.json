["```py\nsudo pip install -U nltk \n```", "```py\nconda install nltk \n```", "```py\npip install -U spacy \n```", "```py\nconda install -c conda-forge spacy \n```", "```py\npip install --upgrade gensim \n```", "```py\nconda install -c conda-forge gensim \n```", "```py\npip install -U textblob \n```", "```py\nconda install -c conda-forge textblob \n```", "```py\n>>> import nltk\n>>> nltk.download() \n```", "```py\n>>> from nltk.corpus import names \n```", "```py\n>>> print(names.words()[:10])\n['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie',\n'Abby', 'Abigael', 'Abigail', 'Abigale'] \n```", "```py\n>>> print(len(names.words()))\n7944 \n```", "```py\n>>> from nltk.tokenize import word_tokenize\n>>> sent = '''I am reading a book.\n...           It is Python Machine Learning By Example,\n...           4th edition.'''\n>>> print(word_tokenize(sent))\n['I', 'am', 'reading', 'a', 'book', '.', 'It', 'is', 'Python', 'Machine', 'Learning', 'By', 'Example', ',', '3rd', 'edition', '.'] \n```", "```py\n>>> sent2 = 'I have been to U.K. and U.S.A.'\n>>> print(word_tokenize(sent2))\n['I', 'have', 'been', 'to', 'U.K.', 'and', 'U.S.A', '.'] \n```", "```py\npython -m spacy download en_core_web_sm \n```", "```py\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_sm')\n>>> tokens2 = nlp(sent2)\n>>> print([token.text for token in tokens2])\n['I', 'have', 'been', 'to', 'U.K.', 'and', 'U.S.A.'] \n```", "```py\n>>> from nltk.tokenize import sent_tokenize\n>>> print(sent_tokenize(sent))\n['I am reading a book.',\n'It's Python Machine Learning By Example,\\n          4th edition.'] \n```", "```py\n>>> import nltk\n>>> tokens = word_tokenize(sent)\n>>> print(nltk.pos_tag(tokens))\n[('I', 'PRP'), ('am', 'VBP'), ('reading', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('Python', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('By', 'IN'), ('Example', 'NNP'), (',', ','), ('4th', 'CD'), ('edition', 'NN'), ('.', '.')] \n```", "```py\n>>> nltk.help.upenn_tagset('PRP')\nPRP: pronoun, personal\n   hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us\n>>> nltk.help.upenn_tagset('VBP')\nVBP: verb, present tense, not 3rd person singular\n   predominate wrap resort sue twist spill cure lengthen brush terminate appear tend stray glisten obtain comprise detest tease attract emphasize mold postpone sever return wag ... \n```", "```py\n>>> print([(token.text, token.pos_) for token in tokens2])\n[('I', 'PRON'), ('have', 'VERB'), ('been', 'VERB'), ('to', 'ADP'), ('U.K.', 'PROPN'), ('and', 'CCONJ'), ('U.S.A.', 'PROPN')] \n```", "```py\n>>> tokens3 = nlp('The book written by Hayden Liu in 2024 was sold at $30 in America') \n```", "```py\n>>> print([(token_ent.text, token_ent.label_) for token_ent in tokens3.ents])\n[('Hayden Liu', 'PERSON'), ('2024', 'DATE'), ('30', 'MONEY'), ('America', 'GPE')] \n```", "```py\n    >>> from nltk.stem.porter import PorterStemmer\n    >>> porter_stemmer = PorterStemmer() \n    ```", "```py\n    >>> porter_stemmer.stem('machines')\n    'machin'\n    >>> porter_stemmer.stem('learning')\n    'learn' \n    ```", "```py\n    >>> from nltk.stem import WordNetLemmatizer\n    >>> lemmatizer = WordNetLemmatizer() \n    ```", "```py\n>>> lemmatizer.lemmatize('machines')\n'machine'\n>>> lemmatizer.lemmatize('learning')\n'learning' \n```", "```py\n>>> from sklearn.datasets import fetch_20newsgroups \n```", "```py\n>>> groups = fetch_20newsgroups()\nDownloading 20news dataset. This may take a few minutes.\nDownloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB) \n```", "```py\n>>> groups.keys()\ndict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR']) \n```", "```py\n>>> groups['target_names']\n   ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] \n```", "```py\n>>> groups.target\narray([7, 4, 4, ..., 3, 1, 8]) \n```", "```py\n>>> import numpy as np\n>>> np.unique(groups.target)\narray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) \n```", "```py\npython -m pip install -U matplotlib\npip install seaborn \n```", "```py\nconda install -c conda-forge matplotlib\nconda install seaborn \n```", "```py\n>>> import seaborn as sns\n>>> import matplotlib.pyplot as plt\n>>> sns.histplot(groups.target, bins=20)\n>>> plt.xticks(range(0, 20, 1))\n>>> plt.show() \n```", "```py\n>>> groups.data[0]\n\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\"\n>>> groups.target[0]\n7\n>>> groups.target_names[groups.target[0]]\n'rec.autos' \n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer \n```", "```py\n>>>  count_vector = CountVectorizer(max_features=500) \n```", "```py\n>>> data_count = count_vector.fit_transform(groups.data) \n```", "```py\n>>> data_count\n<11314x500 sparse matrix of type '<class 'numpy.int64'>'\n      with 798221 stored elements in Compressed Sparse Row format>\n>>> data_count[0]\n<1x500 sparse matrix of type '<class 'numpy.int64'>'\n      with 53 stored elements in Compressed Sparse Row format> \n```", "```py\n>>> data_count.toarray() \n```", "```py\n>>> data_count.toarray()[0] \n```", "```py\n>>> print(count_vector. get_feature_names_out())\n['00' '000' '10' '100' '11' '12' '13' '14' '145' '15' '16' '17' '18' '19' '1993' '20' '21' '22' '23' '24' '25' '26' '27' '30' '32' '34' '40' '50' '93' 'a86' 'able' 'about' 'above' 'ac' 'access' 'actually' 'address' 'after'\n……\n……\n……\n 'well' 'were' 'what' 'when' 'where' 'whether' 'which' 'while' 'who' 'whole' 'why' 'will' 'win' 'window' 'windows' 'with' 'without' 'won' 'word' 'work' 'works' 'world' 'would' 'writes' 'wrong' 'wrote' 'year' 'years' 'yes' 'yet' 'you' 'your'] \n```", "```py\n>>> data_cleaned = []\n>>> for doc in groups.data:\n...     doc_cleaned = ' '.join(word for word in doc.split()\n                                             if word.isalpha())\n...     data_cleaned.append(doc_cleaned) \n```", "```py\n>>> from sklearn.feature_extraction import _stop_words\n>>> print(_stop_words.ENGLISH_STOP_WORDS)\nfrozenset({latter', 'somewhere', 'further', 'full', 'de', 'under', 'beyond', 'than', 'must', 'has', 'him', 'hereafter', 'they', 'third', 'few', 'most', 'con', 'thereby', 'ltd', 'take', 'five', 'alone', 'yours', 'above', 'hereupon', 'seeming', 'least', 'over', 'amongst', 'everyone', 'anywhere', 'yourself', 'these', 'name', 'even', 'in', 'forty', 'part', 'perhaps', 'sometimes', 'seems', 'down', 'among', 'still', 'own', 'wherever', 'same', 'about', 'because', 'four', 'none', 'nothing', 'could'\n……\n……\n'myself', 'except', 'whom', 'up', 'six', 'get', 'sixty', 'those', 'whither', 'once', 'something', 'elsewhere', 'my', 'both', 'another', 'one', 'a', 'hasnt', 'everywhere', 'thin', 'not', 'eg', 'someone', 'seem', 'detail', 'either', 'being'}) \n```", "```py\n>>> count_vector_sw = CountVectorizer(stop_words=\"english\", max_features=500) \n```", "```py\n>>> all_names = set(names.words())\n>>> def get_cleaned_data(groups, lemmatizer, remove_words):\n        data_cleaned = []\n        for doc in groups.data:\n...         doc = doc.lower()\n...         doc_cleaned = ' '.join(lemmatizer.lemmatize(word)\n                                  for word in doc.split()\n                                  if word.isalpha() and\n                                  word not in remove_words)\n...         data_cleaned.append(doc_cleaned)\n        return data_cleaned\n>>> data_cleaned = get_cleaned_data(groups, lemmatizer, all_names)\n>>> data_cleaned_count = count_vector_sw.fit_transform(data_cleaned) \n```", "```py\n>>> print(count_vector_sw.get_feature_names_out())\n['able', 'accept', 'access', 'according', 'act', 'action', 'actually', 'add', 'address', 'ago', 'agree', 'algorithm', 'allow', 'american', 'anonymous', 'answer', 'anybody', 'apple', 'application', 'apr', 'april', 'arab', 'area', 'argument', 'armenian', 'article', 'ask', 'asked',\n……\n……\n'video', 'view', 'wa', 'want', 'wanted', 'war', 'water', 'way', 'weapon', 'week', 'went', 'western', 'white', 'widget', 'win', 'window', 'woman', 'word', 'work', 'working', 'world', 'worth', 'write', 'written', 'wrong', 'year', 'york', 'young'] \n```", "```py\n>>> from sklearn.manifold import TSNE \n```", "```py\n>>> categories_3 = ['talk.religion.misc', 'comp.graphics', 'sci.space']\n>>> groups_3 = fetch_20newsgroups(categories=categories_3)\n>>> data_cleaned = get_cleaned_data(groups_3, lemmatizer, all_names)\n>>> data_cleaned_count_3 = count_vector_sw.fit_transform(data_cleaned) \n```", "```py\n>>> tsne_model = TSNE(n_components=2, perplexity=40,\n                     random_state=42, learning_rate=500)\n>>> data_tsne = tsne_model.fit_transform(data_cleaned_count_3.toarray()) \n```", "```py\n>>> plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=groups_3.target)\n>>> plt.show() \n```", "```py\n>>> categories_5 = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x']\n>>> groups_5 = fetch_20newsgroups(categories=categories_5) \n```", "```py\n>>> import gensim.downloader as api\n>>> model = api.load(\"glove-twitter-25\")\n[==================================================] 100.0%\n104.8/104.8MB downloaded \n```", "```py\n>>> vector = model['computer']\n>>> print('Word computer is embedded into:\\n', vector)\nWord computer is embedded into:\n[ 0.64005 -0.019514 0.70148 -0.66123 1.1723 -0.58859 0.25917\n-0.81541 1.1708 1.1413 -0.15405 -0.11369 -3.8414 -0.87233\n  0.47489 1.1541 0.97678 1.1107 -0.14572 -0.52013 -0.52234\n -0.92349 0.34651 0.061939 -0.57375 ] \n```", "```py\n>>> similar_words = model.most_similar(\"computer\")\n>>> print('Top ten words most contextually relevant to computer:\\n',\n           similar_words)\nTop ten words most contextually relevant to computer:\n [('camera', 0.907833456993103), ('cell', 0.891890287399292), ('server', 0.8744666576385498), ('device', 0.869352400302887), ('wifi', 0.8631256818771362), ('screen', 0.8621907234191895), ('app', 0.8615544438362122), ('case', 0.8587921857833862), ('remote', 0.8583616018295288), ('file', 0.8575270771980286)] \n```", "```py\n>>> doc_sample = ['i', 'love', 'reading', 'python', 'machine',\n                 'learning', 'by', 'example']\n>>> doc_vector = np.mean([model[word] for word in doc_sample],\n                                                           axis=0)\n>>> print('The document sample is embedded into:\\n', doc_vector)\nThe document sample is embedded into:\n [-0.17100249 0.1388764 0.10616798 0.200275 0.1159925 -0.1515975\n  1.1621187 -0.4241785 0.2912 -0.28199488 -0.31453252 0.43692702\n -3.95395 -0.35544625 0.073975 0.1408525 0.20736426 0.17444688\n  0.10602863 -0.04121475 -0.34942 -0.2736689 -0.47526264 -0.11842456\n -0.16284864] \n```"]
["```py\nimport numpy as np\nfrom numpy import linalg\n\nx1 = np.array([2500, 2250, 3500, 4000, 1890, 1200, 2630])\nx2 = np.array([3, 2, 4, 3, 2, 1, 2])\ny = np.array([260000, 285000, 425000, 482500, 205000, 220000, 320000])\n```", "```py\n# solve for the values of theta\nfrom numpy import linalg\nX = np.hstack((x1.reshape(x1.shape[0], 1), x2.reshape(x2.shape[0], 1)))\nlinalg.lstsq(X, y)[0]\n\n# get the estimated y values\nX.dot(linalg.lstsq(X,y)[0])\n```", "```py\narray([   142.58050018, -23629.43252307])\n\narray([285562.95287566, 273547.26035425, 404514.02053054, 499433.70314259,\n       222218.28029018, 147467.16769047, 327727.85042187])\n```", "```py\nX_means = X.mean(axis=0) # compute column (variable) name\nX_center = X - X_means  # center our data\ntheta = linalg.lstsq(X_center, y)[0]  # solve lstsq\nprint (\"Theta: %r\" % theta)\n\nintercept = y.mean() -np.dot(X_means, theta.T) # find the intercept\nprint(\"Intercept: %.2f\" % intercept)\nprint(\"Preds: %r\" % (X.dot(theta.T) + intercept))\n```", "```py\nTheta: array([ 128.90596161, -28362.07260241])\nIntercept: 51887.87\nPreds: array([289066.55823365, 285202.14043457, 389610.44723722, 482425.50064261,\n 238795.99425642, 178212.9533507 , 334186.40584484])\n```", "```py\nclass BaseSimpleEstimator(six.with_metaclass(ABCMeta)):\n \"\"\"Base class for packt estimators.\n The estimators in the Packt package do not behave exactly like \n scikit-learn\n estimators (by design). They are made to perform the model fit \n immediately upon class instantiation. Moreover, many of the hyper\n parameter options are limited to promote readability and avoid\n confusion.\n The constructor of every Packt estimator should resemble the \n following::\n def __init__(self, X, y, *args, **kwargs):\n ...\n where ``X`` is the training matrix, ``y`` is the training target\n variable,\n and ``*args`` and ``**kwargs`` are varargs that will differ for each\n estimator.\n \"\"\"\n @abstractmethod\n def predict(self, X):\n \"\"\"Form predictions based on new data.\n This function must be implemented by subclasses to generate\n predictions given the model fit.\n Parameters\n ----------\n X : array-like, shape=(n_samples, n_features)\n The test array. Should be only finite values.\n \"\"\"\n```", "```py\nfrom __future__ import absolute_import\n\nfrom sklearn.utils.validation import check_X_y, check_array\n\nimport numpy as np\nfrom numpy.linalg import lstsq\n\nfrom packtml.base import BaseSimpleEstimator\n\n__all__ = [\n 'SimpleLinearRegression'\n]\n\nclass SimpleLinearRegression(BaseSimpleEstimator):\n \"\"\"Simple linear regression.\n\n This class provides a very simple example of straight forward OLS\n regression with an intercept. There are no tunable parameters, and\n the model fit happens directly on class instantiation.\n\n Parameters\n ----------\n X : array-like, shape=(n_samples, n_features)\n The array of predictor variables. This is the array we will use\n to regress on ``y``.\n```", "```py\n@abstractmethod\n   def predict(self, X):\n       \"\"\"Form predictions based on new data.\n       This function must be implemented by subclasses to generate\n       predictions given the model fit.\n       Parameters\n       ----------\n       X : array-like, shape=(n_samples, n_features)\n       The test array. Should be only finite values.\n      \"\"\"\n```", "```py\nfrom __future__ import absolute_import\n\nfrom sklearn.utils.validation import check_X_y, check_array\n\nimport numpy as np\nfrom numpy.linalg import lstsq\n\nfrom ..base import BaseSimpleEstimator\n\n__all__ = [\n    'SimpleLinearRegression'\n]\n\nclass SimpleLinearRegression(BaseSimpleEstimator):\n    \"\"\"Simple linear regression.\n\n    This class provides a very simple example of straight forward OLS\n    regression with an intercept. There are no tunable parameters, and\n    the model fit happens directly on class instantiation.\n\n    Parameters\n    ----------\n    X : array-like, shape=(n_samples, n_features)\n        The array of predictor variables. This is the array we will use\n        to regress on ``y``.\n```", "```py\nParameters\n    ----------\n    X : array-like, shape=(n_samples, n_features)\n        The array of predictor variables. This is the array we will use\n        to regress on ``y``.\n\n    y : array-like, shape=(n_samples,)\n        This is the target array on which we will regress to build\n        our model.\n    Attributes\n    ----------\n    theta : array-like, shape=(n_features,)\n        The least-squares solution (the coefficients)\n\n    rank : int\n        The rank of the predictor matrix, ``X``\n\n    singular_values : array-like, shape=(n_features,)\n        The singular values of ``X``\n\n    X_means : array-like, shape=(n_features,)\n        The column means of the predictor matrix, ``X``\n\n    y_mean : float\n        The mean of the target variable, ``y``\n\n    intercept : float\n        The intercept term\n    \"\"\"\n    def __init__(self, X, y):\n        # First check X, y and make sure they are of equal length, no\n        NaNs\n        # and that they are numeric\n        X, y = check_X_y(X, y, y_numeric=True,\n                         accept_sparse=False) # keep it simple\n```", "```py\n\n# We will do the same with our target variable, y\nX_means = np.average(X, axis=0)\ny_mean = y.mean(axis=0)\n\n# don't do in place, so we get a copy\nX = X - X_means\ny = y - y_mean\n\n# Let's compute the least squares on X wrt y\n# Least squares solves the equation `a x = b` by computing a\n# vector `x` that minimizes the Euclidean 2-norm `|| b - a x ||^2`.\ntheta, _, rank, singular_values = lstsq(X, y, rcond=None)\n\n# finally, we compute the intercept values as the mean of the target\n# variable MINUS the inner product of the X_means and the coefficients\nintercept = y_mean - np.dot(X_means, theta.T)\n\n# ... and set everything as an instance attribute\nself.theta = theta\nself.rank = rank\nself.singular_values = singular_values\n\n# we have to retain some of the statistics around the data too\nself.X_means = X_means\nself.y_mean = y_mean\nself.intercept = intercept\n```", "```py\ndef predict(self, X):\n        \"\"\"Compute new predictions for X\"\"\"\n        # copy, make sure numeric, etc...\n        X = check_array(X, accept_sparse=False, copy=False) # type: np.ndarray\n\n        # make sure dims match\n        theta = self.theta\n        if theta.shape[0] != X.shape[1]:\n            raise ValueError(\"Dim mismatch in predictors!\")\n\n        # creates a copy\n        return np.dot(X, theta.T) + self.intercept\n\n```", "```py\nfrom packtml.regression import SimpleLinearRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport sys\n\n# #############################################################################\n# Create a data-set that perfectly models the linear relationship:\n# y = 2a + 1.5b + 0\nrandom_state = np.random.RandomState(42)\nX = random_state.rand(500, 2)\ny = 2\\. * X[:, 0] + 1.5 * X[:, 1]\n\n```", "```py\n# split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n\n                                                   random_state=random_state)\n```", "```py\n# Fit a simple linear regression, produce predictions\nlm = SimpleLinearRegression(X_train, y_train)\npredictions = lm.predict(X_test)\nprint(\"Test sum of residuals: %.3f\" % (y_test - predictions).sum())\nassert np.allclose(lm.theta, [2., 1.5])\n\n```", "```py\n# Show that our solution is similar to scikit-learn's\n\nlr = LinearRegression(fit_intercept=True)\nlr.fit(X_train, y_train)\nassert np.allclose(lm.theta, lr.coef_)\nassert np.allclose(predictions, lr.predict(X_test))\n```", "```py\n# Fit another on ONE feature so we can show the plot\nX_train = X_train[:, np.newaxis, 0]\nX_test = X_test[:, np.newaxis, 0]\nlm = SimpleLinearRegression(X_train, y_train)\n\n# create the predictions & plot them as the line\npreds = lm.predict(X_test)\nplt.scatter(X_test[:, 0], y_test, color='black')\nplt.plot(X_test[:, 0], preds, linewidth=3)\n\n# if we're supposed to save it, do so INSTEAD OF showing it\nif len(sys.argv) > 1:\n    plt.savefig(sys.argv[1])\nelse:\n    plt.show()\n```", "```py\nsource activate packt-sml\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nx = np.linspace(-10, 10, 10000)\ny = 1\\. / (1 + np.exp(-x)) # sigmoid transformation\nplt.plot(x, y)\n```", "```py\nsigmoid = (lambda x: 1\\. / (1 + np.exp(-x)))\nlog_odds = np.array([-5.6, 8.9, 3.7, 0.6, 0.])\nprobas = sigmoid(log_odds)\nclasses = np.round(probas).astype(int)\nprint(\"Log odds: %r\\nProbas: %r\\nClasses: %r\"\n      % (log_odds, probas, classes))\n```", "```py\nLog odds: array([-5.6, 8.9, 3.7, 0.6, 0\\. ])\nProbas: array([0.00368424, 0.99986363, 0.97587298, 0.64565631, 0.5 ])\nClasses: array([0, 1, 1, 1, 0])\n```", "```py\ndef log_likelihood(X, y, w):\n    \"\"\"Compute the log-likelihood function.\n\n    Computes the log-likelihood function over the training data.\n    The key to the log-likelihood is that the log of the product of\n    likelihoods becomes the sum of logs. That is (in pseudo-code),\n\n        np.log(np.product([f(i) for i in range(N)]))\n\n    is equivalent to:\n\n        np.sum([np.log(f(i)) for i in range(N)])\n\n    The log-likelihood function is used in computing the gradient for\n    our loss function since the derivative of the sum (of logs) is equivalent\n    to the sum of derivatives, which simplifies all of our math.\n```", "```py\n weighted = X.dot(w)\n return (y * weighted - np.log(1\\. + np.exp(weighted))).sum()\n```", "```py\ndef logistic_sigmoid(x):\n    \"\"\"The logistic function.\n\n    Compute the logistic (sigmoid) function over a vector, ``x``.\n\n    Parameters\n    ----------\n    x : np.ndarray, shape=(n_samples,)\n        A vector to transform.\n    \"\"\"\n    return 1\\. / (1\\. + np.exp(-x))\n```", "```py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\n\nfrom sklearn.utils.validation import check_X_y, check_array\n\nimport numpy as np\nfrom packtml.utils.extmath import log_likelihood, logistic_sigmoid\nfrom packtml.utils.validation import assert_is_binary\nfrom packtml.base import BaseSimpleEstimator\n\n__all__ = [\n    'SimpleLogisticRegression'\n]\n\ntry:\n    xrange\nexcept NameError: # py 3 doesn't have an xrange\n    xrange = range\n\nclass SimpleLogisticRegression(BaseSimpleEstimator):\n    \"\"\"Simple logistic regression.\n\n    This class provides a very simple example of straight forward logistic\n    regression with an intercept. There are few tunable parameters aside from\n    the number of iterations, & learning rate, and the model is fit upon\n    class initialization.\n```", "```py\nParameters\n    ----------\nX : array-like, shape=(n_samples, n_features)\n        The array of predictor variables. This is the array we will use\n        to regress on ``y``.\n\ny : array-like, shape=(n_samples,)\n        This is the target array on which we will regress to build\n        our model. It should be binary (0, 1).\n\nn_steps : int, optional (default=100)\n        The number of iterations to perform.\n\nlearning_rate : float, optional (default=0.001)\n        The learning rate.\n\nloglik_interval : int, optional (default=5)\n        How frequently to compute the log likelihood. This is an expensive\n        operation--computing too frequently will be very expensive.\n```", "```py\ndef __init__(self, X, y, n_steps=100, learning_rate=0.001,\n                 loglik_interval=5):\n        X, y = check_X_y(X, y, accept_sparse=False, # keep dense for example\n                         y_numeric=True)\n\n        # we want to make sure y is binary since that's all our example covers\n        assert_is_binary(y)\n\n        # X should be centered/scaled for logistic regression, much like\n        # with linear regression\n        means, stds = X.mean(axis=0), X.std(axis=0)\n        X = (X - means) / stds\n```", "```py\n# since we're going to learn an intercept, we can cheat and set the\n# intercept to be a new feature that we'll learn with everything else\nX_w_intercept = np.hstack((np.ones((X.shape[0], 1)), X))\n```", "```py\n # initialize the coefficients as zeros\n theta = np.zeros(X_w_intercept.shape[1])\n```", "```py\n # now for each step, we compute the inner product of X and the\n # coefficients, transform the predictions with the sigmoid function,\n # and adjust the weights by the gradient\n ll = []\n for iteration in xrange(n_steps):\n     preds = logistic_sigmoid(X_w_intercept.dot(theta))\n     residuals = y - preds # The error term\n     gradient = X_w_intercept.T.dot(residuals)\n```", "```py\n# update the coefficients\ntheta += learning_rate * gradient\n\n# you may not always want to do this, since it's expensive. Tune\n# the error_interval to increase/reduce this\nif (iteration + 1) % loglik_interval == 0:\n    ll.append(log_likelihood(X_w_intercept, y, theta))\n```", "```py\n# recall that our theta includes the intercept, so we need to  pop\n# that off and store it\nself.intercept = theta[0]\nself.theta = theta[1:]\nself.log_likelihood = ll\nself.column_means = means\nself.column_std = stds\n```", "```py\n# scale the data appropriately\nX = (X - self.column_means) / self.column_std\n\n# creates a copy\nreturn logistic_sigmoid(np.dot(X, theta.T) + self.intercept)\n```", "```py\n def predict(self, X):\n     return np.round(self.predict_proba(X)).astype(int)\n```", "```py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\n\nfrom packtml.regression import SimpleLogisticRegression\nfrom packtml.utils.plotting import add_decision_boundary_to_axis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\nimport sys\n\n# #############################################################################\n# Create an almost perfectly linearly-separable classification set\nX, y = make_classification(n_samples=100, n_features=2, random_state=42,\n                           n_redundant=0, n_repeated=0, n_classes=2,\n                           class_sep=1.0)\n\n# split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# #############################################################################\n# Fit a simple logistic regression, produce predictions\nlm = SimpleLogisticRegression(X_train, y_train, n_steps=50)\n\npredictions = lm.predict(X_test)\nacc = accuracy_score(y_test, predictions)\nprint(\"Test accuracy: %.3f\" % acc)\n```", "```py\n# Show that our solution is similar to scikit-learn's\nlr = LogisticRegression(fit_intercept=True, C=1e16) # almost no regularization\nlr.fit(X_train, y_train)\nprint(\"Sklearn test accuracy: %.3f\" % accuracy_score(y_test, \n                                                   lr.predict(X_test)))\n```"]
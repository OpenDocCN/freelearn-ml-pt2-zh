["```py\n    >>> import os\n    >>> from PIL import Image\n    >>> import torch\n    >>> from torch.utils.data import Dataset, DataLoader\n    >>> import torchvision.transforms as transforms \n    ```", "```py\n    >>> image_dir = \"flickr8k/Flicker8k_Dataset\"\n    >>> caption_file = \"flickr8k/captions.txt\" \n    ```", "```py\n    >>> from transformers import DistilBertTokenizer\n    >>> tokenizer = DistilBertTokenizer.from_pretrained(\n                                             'distilbert-base-uncased') \n    ```", "```py\n    >>> class Flickr8kDataset(Dataset):\n            def __init__(self, image_dir, caption_file):\n                self.image_dir = image_dir\n                self.transform = transforms.Compose([\n                                    transforms.Resize((224, 224)),\n                                    transforms.ToTensor(),\n                                 ])\n                self.image_paths, self.captions = \n                self.read_caption_file(caption_file)\n            def read_caption_file(self, caption_file):\n                image_paths = []\n                captions = []\n                with open(caption_file, \"r\") as file:\n                    lines = file.readlines()\n                    for line in lines[1:]:\n                        parts = line.strip().split(\",\")\n                         image_paths.append(os.path.join(self.image_dir,\n                                                         parts[0]))\n                         captions.append(parts[1])\n                self.caption_encodings = tokenizer(captions, truncation=True,\n                                                   padding=True,\n                                                   max_length=200)\n                return image_paths, captions\n            def __len__(self):\n                return len(self.image_paths)\n\n            def __getitem__(self, idx):\n                item = {key: torch.tensor(val[idx]) for key, val in\n                                                self.caption_encodings.items()}\n                caption = self.captions[idx]\n                item[\"caption\"] = caption\n                img_path = self.image_paths[idx]\n                img = Image.open(img_path).convert(\"RGB\")\n                img = self.transform(img)\n                item['image'] = img\n                return item \n    ```", "```py\n    >>> flickr8k_dataset = Flickr8kDataset(image_dir=image_dir,\n                                           caption_file=caption_file) \n    ```", "```py\n>>> item_sample = next(iter(flickr8k_dataset))\n{'input_ids': tensor([ 101, 1037, 2775, 1999, 1037, 5061, 4377, 2003, \n        8218, 2039, 1037, 2275, 1997, 5108, 1999, 2019, 4443, 2126, 1012, \n         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, \n           0,    0,    0,    0,    0,    0,    0,    0,    0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n           1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0]),\n 'caption': 'A child in a pink dress is climbing up a set of stairs in an entry way.',\n 'image': tensor([[[0.3216, 0.4353, 0.4549,  ..., 0.0157, 0.0235, 0.0235],\n          [0.3098, 0.4431, 0.4667,  ..., 0.0314, 0.0275, 0.0471],\n          [0.3020, 0.4588, 0.4745,  ..., 0.0314, 0.0275, 0.0392],\n          ...,\n          [0.7294, 0.5882, 0.6706,  ..., 0.8314, 0.6471, 0.6471],\n          [0.6902, 0.6941, 0.8627,  ..., 0.8235, 0.6588, 0.6588],\n          [0.8118, 0.8196, 0.7333,  ..., 0.8039, 0.6549, 0.6627]],\n         [[0.3412, 0.5020, 0.5255,  ..., 0.0118, 0.0235, 0.0314],\n          [0.3294, 0.5059, 0.5412,  ..., 0.0353, 0.0392, 0.0824],\n          [0.3098, 0.5176, 0.5529,  ..., 0.0353, 0.0510, 0.0863],\n          ...,\n          [0.4235, 0.3137, 0.4784,  ..., 0.8667, 0.7255, 0.7216],\n          [0.3765, 0.5059, 0.6627,  ..., 0.8549, 0.7216, 0.7216],\n          [0.4941, 0.5804, 0.4784,  ..., 0.8392, 0.7216, 0.7216]],\n         [[0.3804, 0.4902, 0.4980,  ..., 0.0118, 0.0157, 0.0196],\n          [0.3608, 0.5059, 0.5176,  ..., 0.0275, 0.0235, 0.0235],\n          [0.3647, 0.5255, 0.5333,  ..., 0.0196, 0.0235, 0.0275],\n          ...,\n          [0.1216, 0.1098, 0.2549,  ..., 0.9176, 0.8235, 0.7961],\n          [0.0784, 0.1804, 0.2902,  ..., 0.9137, 0.8118, 0.7843],\n          [0.1843, 0.2588, 0.2824,  ..., 0.9176, 0.8039, 0.7686]]])} \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> npimg = item_sample['image'].numpy()\n>>> plt.imshow(np.transpose(npimg, (1, 2, 0))) \n```", "```py\n    >>> batch_size = 32\n    >>> data_loader = DataLoader(flickr8k_dataset, batch_size=batch_size, shuffle=True) \n    ```", "```py\n>>> import torch.nn as nn\n>>> from torchvision.models import resnet50\n>>> class VisionEncoder(nn.Module):\n        def __init__(self):\n            super().__init__()\n            pretrained_resnet50 = resnet50(pretrained=True)\n            self.model = nn.Sequential(*list(\n                                       pretrained_resnet50.children())[:-1])\n            for param in self.model.parameters():\n                param.requires_grad = False\n        def forward(self, x):\n            x= self.model(x)\n            x = x.view(x.size(0), -1)\n            return x \n```", "```py\n>>> from transformers import DistilBertModel\n>>> class TextEncoder(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.model = DistilBertModel.from_pretrained(\n                                              'distilbert-base-uncased')\n            for param in self.model.parameters():\n                param.requires_grad = False\n        def forward(self, input_ids, attention_mask=None):\n            outputs = self.model(input_ids=input_ids,\n                                 attention_mask=attention_mask)\n            return outputs.last_hidden_state[:, 0, :] \n```", "```py\n>>> class ProjectionHead(nn.Module):\n        def __init__(self, embedding_dim, projection_dim=256, dropout=0.1):\n            super().__init__()\n            self.projection = nn.Linear(embedding_dim, projection_dim)\n            self.gelu = nn.GELU()\n            self.fc = nn.Linear(projection_dim, projection_dim)\n            self.dropout = nn.Dropout(dropout)\n            self.layer_norm = nn.LayerNorm(projection_dim)\n        def forward(self, x):\n            projection = self.projection(x)\n            x = self.gelu(projection)\n            x = self.fc(x)\n            x = self.dropout(x)\n            x = projection + x\n            x = self.layer_norm(x)\n            return x \n```", "```py\n>>> import torch.nn.functional as F\n>>> class CLIPModel(nn.Module):\n        def __init__(self, image_embedding=2048, text_embedding=768):\n            super().__init__()\n            self.vision_encoder = VisionEncoder()\n            self.text_encoder = TextEncoder()\n            self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n            self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n        def forward(self, batch):\n            image_features = self.vision_encoder(batch[\"image\"])\n            text_features = self.text_encoder(\n                input_ids=batch[\"input_ids\"], \n                attention_mask=batch[\"attention_mask\"]\n            )\n            image_embeddings = self.image_projection(image_features)\n            text_embeddings = self.text_projection(text_features)\n            logits = text_embeddings @ image_embeddings.T\n            images_similarity = image_embeddings @ image_embeddings.T\n            texts_similarity = text_embeddings @ text_embeddings.T\n            targets = F.softmax((images_similarity + texts_similarity)/2 , dim=-1)\n            texts_loss = F.cross_entropy(logits, targets)\n            images_loss = F.cross_entropy(logits.T, targets.T)\n            loss = (images_loss + texts_loss) / 2\n            return loss.mean() \n```", "```py\n    >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    >>> model = CLIPModel().to(device) \n    ```", "```py\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n    ```", "```py\n    >>> def train(model, dataloader, optimizer):\n            model.train()\n            total_loss = 0\n            b = 0\n            for batch in dataloader:\n                optimizer.zero_grad()\n                batch = {k: v.to(device) for k, v in batch.items()\n                                             if k != \"caption\"}\n                loss = model(batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()*len(batch)\n\n            return total_loss/len(dataloader.dataset) \n    ```", "```py\n    >>> num_epochs = 3\n    >>> for epoch in range(num_epochs):\n            train_loss = train(model, data_loader, optimizer)\n            print(f'Epoch {epoch+1} - loss: {train_loss:.4f}')\n    Epoch 1 - loss: 0.2551\n    Epoch 2 - loss: 0.1504\n    Epoch 3 - loss: 0.1274 \n    ```", "```py\n    >>> torch.manual_seed(0)\n    >>> data_loader = DataLoader(flickr8k_dataset, batch_size=batch_size,\n                                 shuffle=True)\n    >>> sample_batch = next(iter(data_loader)) \n    ```", "```py\n    >>> batch_image_features = model.vision_encoder(sample_batch[\"image\"].to(device))\n    >>> batch_image_embeddings = model.image_projection(batch_image_features) \n    ```", "```py\n    >>> def search_top_images(model, image_embeddings, query, n=1):\n            encoded_query = tokenizer([query])\n            batch = {\n                key: torch.tensor(values).to(device)\n                for key, values in encoded_query.items()\n            }\n            model.eval()\n            with torch.no_grad():\n                text_features = model.text_encoder(\n                    input_ids=batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"])\n                text_embeddings = model.text_projection(text_features)\n            dot_similarity = text_embeddings @ image_embeddings.T\n            values, indices = torch.topk(dot_similarity.squeeze(0), n)\n            return indices \n    ```", "```py\n    >>> query = \"a running dog\"\n    >>> top_image_ids = search_top_images(model, batch_image_embeddings, query, 2)\n    >>> print(\"Query:\", query)\n    >>> for id in top_image_ids:\n            image = sample_batch[\"image\"][id]\n            npimg = image.numpy()\n            plt.imshow(np.transpose(npimg, (1, 2, 0)))\n            plt.title(f\"Query: {query}\")\n            plt.show() \n    ```", "```py\n    >>> query = \" kids jumping into a pool \"\n    >>> top_image_ids = search_top_images(model, batch_image_embeddings, query)\n    >>> print(\"Query:\", query)\n    >>> for id in top_image_ids:\n            image = sample_batch[\"image\"][id]\n            npimg = image.numpy()\n            plt.imshow(np.transpose(npimg, (1, 2, 0)))\n            plt.title(f\"Query: {query}\")\n            plt.show() \n    ```", "```py\n    pip install -U sentence-transformers \n    ```", "```py\nconda install -c conda-forge sentence-transformers \n```", "```py\n    >>> from sentence_transformers import SentenceTransformer, util\n    >>> model = SentenceTransformer('clip-ViT-B-32') \n    ```", "```py\n    >>> import glob\n    >>> image_paths = list(glob.glob('flickr8k/Flicker8k_Dataset/*.jpg'))\n    >>> all_image_embeddings = []\n    >>> for img_path in image_paths:\n            img = Image.open(img_path)\n            all_image_embeddings.append(model.encode(img, convert_to_tensor=True)) \n    ```", "```py\n    >>> def search_top_images(model, image_embeddings, query, top_k=1):\n            query_embeddings = model.encode([query], convert_to_tensor=True,\n                                            show_progress_bar=False)\n            hits = util.semantic_search(query_embeddings,  image_embeddings,\n                                        top_k=top_k)[0]\n            return hits \n    ```", "```py\n    >>> query = \"a swimming dog\"\n    >>> hits = search_top_images(model, all_image_embeddings, query)\n    >>> for hit in hits:\n            img_path = image_paths[hit['corpus_id']]\n            image = Image.open(img_path)\n            plt.imshow(image)\n            plt.title(f\"Query: {query}\")\n            plt.show() \n    ```", "```py\n    >>> image_query =\n           Image.open(\"flickr8k/Flicker8k_Dataset/240696675_7d05193aa0.jpg\") \n    ```", "```py\n>>> hits = search_top_images(model, all_image_embeddings, image_query, 3)[1:]\n>>> plt.imshow(image_query)\n>>> plt.title(f\"Query image\")\n>>> plt.show()\n>>> for hit in hits:\n        img_path = image_paths[hit['corpus_id']]\n        image = Image.open(img_path)\n        plt.imshow(image)\n        plt.title(f\"Similar image\")       \n        plt.show() \n```", "```py\n    >>> from torchvision.datasets import CIFAR100\n    >>> cifar100 = CIFAR100(root=\"CIFAR100\", download=True, train=False) \n    ```", "```py\n    >>> print(cifar100.classes)\n    >>> print(\"Number of classes in CIFAR100 dataset:\", len(cifar100.classes))\n    ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n    Number of classes in CIFAR100 dataset: 100 \n    ```", "```py\n    >>> sample_index = 0\n    >>> img, class_id = cifar100[index]\n    >>> print(f\"Class of the sample image: {class_id} - {cifar100.classes[class_id]}\")\n    Class of the sample image: 49 - mountain \n    ```", "```py\n    >>> sample_image_embeddings = model.encode(img, convert_to_tensor=True) \n    ```", "```py\n    >>> class_text = model.encode(cifar100.classes, convert_to_tensor=True) \n    ```", "```py\n    >>> hits = util.semantic_search(sample_image_embeddings,  class_text, top_k=1)[0]\n    >>> pred = hits[0]['corpus_id']\n    >>> print(f\"Predicted class of the sample image: {pred}\")  \n    Predicted class of the sample image: 49 \n    ```", "```py\n    >>> all_image_embeddings = []\n    >>> class_true = []\n    >>> for img, class_id in cifar100:\n            class_true.append(class_id)\n            all_image_embeddings.append(model.encode(img, convert_to_tensor=True)) \n    ```", "```py\n    >>> class_pred = []\n    >>> for hit in util.semantic_search(all_image_embeddings,  class_text, top_k=1):\n            class_pred.append(hit[0]['corpus_id']) \n    ```", "```py\n>>> from sklearn.metrics import accuracy_score\n>>> acc = accuracy_score(class_true, class_pred)\n>>> print(f\"Accuracy of zero-shot classification: {acc * 100}%\")\nAccuracy of zero-shot classification: 55.15% \n```"]
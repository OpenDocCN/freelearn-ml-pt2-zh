<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 7. Regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Regression</h1></div></div></div><p>You probably learned about regression in your high school mathematics class. The specific method you learned was probably what is called <a id="id385" class="indexterm"/>
<span class="strong"><strong>ordinary least squares</strong></span> (<span class="strong"><strong>OLS</strong></span>) regression. This 200-year-old technique is computationally fast and can be used for many real-world problems. This chapter will start by reviewing it and showing you how it is available in scikit-learn.</p><p>For some problems, however, this method is insufficient. This is particularly true when we have many features, and it completely fails when we have more features than datapoints. For those cases, we need more advanced methods. These methods are very modern, with major developments happening in the last decade. They go by names such as Lasso, Ridge, or ElasticNets. We will go into these in detail. They are also available in scikit-learn.</p><div class="section" title="Predicting house prices with regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec47"/>Predicting house prices with regression</h1></div></div></div><p>Let's start<a id="id386" class="indexterm"/> with a simple problem, predicting house prices in Boston; a problem for which we can use a publicly available dataset. We are given several demographic and geographical attributes, such as the crime rate or the pupil-teacher ratio in the neighborhood. The goal is to predict the median value of a house in a particular area. As usual, we have some training data, where the answer is known to us.</p><p>This is one of the built-in datasets that scikit-learn comes with, so it is very easy to load the data into memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.datasets import load_boston</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; boston = load_boston()</strong></span>
</pre></div><p>The <code class="literal">boston</code> object contains several attributes; in particular, <code class="literal">boston.data</code> contains the input data and <code class="literal">boston.target</code> contains the price of houses.</p><p>We will start with a simple <a id="id387" class="indexterm"/>one-dimensional regression, trying to regress the price on a single attribute, the average number of rooms per dwelling in the neighborhood, which is stored at position <code class="literal">5</code> (you can consult <code class="literal">boston.DESCR</code> and <code class="literal">boston.feature_names</code> for detailed information on the data):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from matplotlib import pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.scatter(boston.data[:,5], boston.target, color='r')</strong></span>
</pre></div><p>The <code class="literal">boston.target</code> attribute <a id="id388" class="indexterm"/>contains the average house price (our target variable). We can use the standard least squares regression you probably first saw in high-school. Our first attempt looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; lr = LinearRegression()</strong></span>
</pre></div><p>We import <code class="literal">LinearRegression</code> from the <code class="literal">sklearn.linear_model</code> module and construct a <code class="literal">LinearRegression</code> object. This object will behave analogously to the classifier objects from scikit-learn that we used earlier.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; x = boston.data[:,5]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; y = boston.target</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x = np.transpose(np.atleast_2d(x))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; lr.fit(x, y)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; y_predicted = lr.predict(x)</strong></span>
</pre></div><p>The only nonobvious line in this code block is the call to <code class="literal">np.atleast_2d</code>, which converts <code class="literal">x</code> from a one-dimensional to a two-dimensional array. This conversion is necessary as the <code class="literal">fit</code> method expects a two-dimensional array as its first argument. Finally, for the dimensions to work out correctly, we need to transpose this array.</p><p>Note that we are calling methods named <code class="literal">fit</code> and predict on the <code class="literal">LinearRegression</code> object, just as we did with classifier objects, even though we are now performing regression. This regularity in the API is one of the nicer features of scikit-learn.</p><div class="mediaobject"><img src="images/2772OS_07_02.jpg" alt="Predicting house prices with regression"/></div><p>The preceding <a id="id389" class="indexterm"/>graph shows all the points (as dots) and our fit (the solid line). We can see that visually it looks good, except for a few outliers.</p><p>Ideally, though, we would like to measure how good of a fit this is quantitatively. This will be critical in order to be able to compare alternative methods. To do so, we can measure how close our prediction is to the true values. For this task, we can use the <code class="literal">mean_squared_error</code> function from the <code class="literal">sklearn.metrics</code> module:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.metrics import mean_squared_error</strong></span>
</pre></div><p>This function takes two arguments, the true value and the predictions, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; mse = mean_squared_error(y, lr.predict(x))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print("Mean squared error (of training data): {:.3}".format(mse))</strong></span>
<span class="strong"><strong>Mean squared error (of training data): 58.4</strong></span>
</pre></div><p>This value can sometimes be hard to interpret, and it's better to take the square root, to obtain the <a id="id390" class="indexterm"/>
<span class="strong"><strong>root mean square error</strong></span> (<span class="strong"><strong>RMSE</strong></span>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; rmse = np.sqrt(mse)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print("RMSE (of training data): {:.3}".format(rmse))</strong></span>
<span class="strong"><strong>RMSE (of training data): 6.6</strong></span>
</pre></div><p>One <a id="id391" class="indexterm"/>advantage<a id="id392" class="indexterm"/> of using RMSE is that we can quickly obtain a very rough estimate of the error by multiplying it by two. In our case, we can expect the estimated price to be different from the real price by, at most, 13 thousand dollars.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip12"/>Tip</h3><p>
<span class="strong"><strong>Root mean squared error and prediction</strong></span>
</p><p>Root mean squared error corresponds approximately to an estimate of the standard deviation. Since most data is at most two standard deviations from the mean, we can double our RMSE to obtain a rough confident interval. This is only completely valid if the errors are normally distributed, but it is often roughly correct even if they are not.</p></div></div><p>A number such as 6.6 is still hard to immediately intuit. Is this a good prediction? One possible way to answer this question is to compare it with the most simple baseline, the constant model. If we knew nothing of the input, the best we could do is predict that the output will always be the average value of <code class="literal">y</code>. We can then compare the mean-squared error of this model with the mean-squared error of the null model. This idea is formalized in the <a id="id393" class="indexterm"/>
<span class="strong"><strong>coefficient of determination</strong></span>, which is defined as follows:</p><div class="mediaobject"><img src="images/2772OS_07_09.jpg" alt="Predicting house prices with regression"/></div><p>In this formula, <span class="emphasis"><em>y<sub>i</sub></em></span> represents the value of the element with index <span class="emphasis"><em>i</em></span>, while <span class="inlinemediaobject"><img src="images/2772OS_07_19.jpg" alt="Predicting house prices with regression"/></span> is the estimate for the same element obtained by the regression model. Finally, <span class="inlinemediaobject"><img src="images/2772OS_07_20.jpg" alt="Predicting house prices with regression"/></span> is the mean value of <span class="emphasis"><em>y</em></span>, which represents the <span class="emphasis"><em>null model</em></span> that always returns the same value. This is roughly the same as first computing the ratio of the mean squared error with the variance of the output and, finally, considering one minus this ratio. This way, a perfect model obtains a score of one, while the null model obtains a score of zero. Note that it is possible to obtain a negative score, which means that the model is so poor that one is better off using the mean as a prediction.</p><p>The coefficient of determination can be obtained using <code class="literal">r2_score</code> of the <code class="literal">sklearn.metrics</code> module:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.metrics import r2_score</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; r2 = r2_score(y, lr.predict(x))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print("R2 (on training data): {:.2}".format(r2))</strong></span>
<span class="strong"><strong>R2 (on training data): 0.31</strong></span>
</pre></div><p>This measure<a id="id394" class="indexterm"/> is also called the R² score. If you are using linear regression and evaluating the error on the training data, then it does correspond to the square of the correlation coefficient, R. However, this measure is more general, and as we discussed, may even return a negative value.</p><p>An alternative way to compute the coefficient of determination is to use the <code class="literal">score</code> method of the <code class="literal">LinearRegression</code> object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; r2 = lr.score(x,y)</strong></span>
</pre></div><div class="section" title="Multidimensional regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec59"/>Multidimensional regression</h2></div></div></div><p>So far, we <a id="id395" class="indexterm"/>have only used a <a id="id396" class="indexterm"/>single variable for prediction, the number of rooms per dwelling. We will now use all the data we have to fit a model, using <a id="id397" class="indexterm"/>multidimensional regression. We now try to predict a single output (the average house price) based on multiple inputs.</p><p>The code looks very much like before. In fact, it's even simpler as we can now pass the value of <code class="literal">boston.data</code> directly to the <code class="literal">fit</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; x = boston.data</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; y = boston.target</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; lr.fit(x, y)</strong></span>
</pre></div><p>Using all the input variables, the root mean squared error is only 4.7, which corresponds to a coefficient of determination of 0.74. This is better than what we had before, which indicates that the extra variables did help. We can no longer easily display the regression line as we did, because we have a 14-dimensional regression hyperplane instead of a single line.</p><p>We can, however, plot the prediction versus the actual value. The code is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; p = lr.predict(x)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.scatter(p, y)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.xlabel('Predicted price')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.ylabel('Actual price')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.plot([y.min(), y.max()], [[y.min()], [y.max()]])</strong></span>
</pre></div><p>The last line plots a diagonal line that corresponds to perfect agreement. This aids with visualization. The<a id="id398" class="indexterm"/> results are shown in the following plot, where the solid line shows the diagonal (where all the points would lie if there was perfect agreement between the prediction and the underlying value):</p><div class="mediaobject"><img src="images/2772OS_07_04.jpg" alt="Multidimensional regression"/></div></div><div class="section" title="Cross-validation for regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec60"/>Cross-validation for regression</h2></div></div></div><p>If you <a id="id399" class="indexterm"/>remember <a id="id400" class="indexterm"/>when we first introduced classification, we stressed the importance of cross-validation for checking the quality of our predictions. In regression, this is not always done. In fact, we discussed only the training error in this chapter so far. This is a mistake if you want to confidently infer the generalization ability. Since ordinary least squares is a very simple model, this is often not a very serious mistake. In other words, the amount of overfitting is slight. However, we should still test this empirically, which we can easily do with scikit-learn.</p><p>We will use the <code class="literal">Kfold</code> class<a id="id401" class="indexterm"/> to build a 5 fold cross-validation loop and test the generalization ability of linear regression:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.cross_validation import Kfold</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; kf = KFold(len(x), n_folds=5)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; p = np.zeros_like(y)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for train,test in kf:</strong></span>
<span class="strong"><strong>...    lr.fit(x[train], y[train])</strong></span>
<span class="strong"><strong>...    p[test] = lr.predict(x[test])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; rmse_cv = np.sqrt(mean_squared_error(p, y))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('RMSE on 5-fold CV: {:.2}'.format(rmse_cv))</strong></span>
<span class="strong"><strong>RMSE on 5-fold CV: 5.6</strong></span>
</pre></div><p>With cross-validation, we obtain a more conservative estimate (that is, the error is larger): <code class="literal">5.6</code>. As in the case of classification, the cross-validation estimate is a better estimate of how well we could generalize to predict on unseen data.</p><p>Ordinary least <a id="id402" class="indexterm"/>squares is fast at learning time and returns a simple model, which is fast at prediction time. For these reasons, it should often be the first model that you try in a regression problem. However, we are now going to see more advanced methods and why they are sometimes preferable.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Penalized or regularized regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec48"/>Penalized or regularized regression</h1></div></div></div><p>This section introduces penalized regression, also called <span class="strong"><strong>regularized regression</strong></span>, an important class of<a id="id403" class="indexterm"/> regression models.</p><p>In ordinary regression, the returned fit is the best fit on the training data. This can lead to over-fitting. Penalizing means that we add a penalty for over-confidence in the parameter values. Thus, we accept a slightly worse fit in order to have a simpler model.</p><p>Another way to think about it is to consider that the default is that there is no relationship between the input variables and the output prediction. When we have data, we change this opinion, but adding a penalty means that we require more data to convince us that this is a strong relationship.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip13"/>Tip</h3><p>
<span class="strong"><strong>Penalized regression is about tradeoffs</strong></span>
</p><p>Penalized regression is another example of the bias-variance tradeoff. When using a penalty, we get a worse fit in the training data, as we are adding bias. On the other hand, we reduce the variance and tend to avoid over-fitting. Therefore, the overall result might generalize better to unseen (test) data.</p></div></div><div class="section" title="L1 and L2 penalties"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec61"/>L1 and L2 penalties</h2></div></div></div><p>We now<a id="id404" class="indexterm"/> explore<a id="id405" class="indexterm"/> these ideas in detail. Readers who do not care about some of the mathematical aspects should feel free to skip directly to the next section on how to use regularized regression in scikit-learn.</p><p>The problem, in general, is that we are given a matrix <span class="emphasis"><em>X</em></span> of training data (rows are observations and each column is a different feature), and a vector <span class="emphasis"><em>y</em></span> of output values. The goal is to obtain a vector of weights, which we will call <span class="emphasis"><em>b*</em></span>. The ordinary least squares regression is given by the following formula:</p><div class="mediaobject"><img src="images/2772OS_07_13.jpg" alt="L1 and L2 penalties"/></div><p>That is, we find vector <span class="emphasis"><em>b</em></span> that minimizes the squared distance to the target <span class="emphasis"><em>y</em></span>. In these equations, we ignore the issue of setting an intercept by assuming that the training data has been preprocessed so that the mean of <span class="emphasis"><em>y</em></span> is zero.</p><p>Adding a penalty or a regularization means that we do not simply consider the best fit on the training data, but also how vector <span class="inlinemediaobject"><img src="images/2772OS_07_21.jpg" alt="L1 and L2 penalties"/></span> is composed. There are two types of penalties that are typically used for regression: L1 and L2 penalties. An L1 penalty means that we penalize the regression by the sum of the absolute values of the coefficients, while an L2 penalty penalizes by the sum of squares.</p><p>When we add an L1 penalty, instead of the preceding equation, we instead optimize the following:</p><div class="mediaobject"><img src="images/2772OS_07_14.jpg" alt="L1 and L2 penalties"/></div><p>Here, we are trying to simultaneously make the error small, but also make the values of the coefficients small (in absolute terms). Using an L2 penalty, means that we use the following formula:</p><div class="mediaobject"><img src="images/2772OS_07_15.jpg" alt="L1 and L2 penalties"/></div><p>The difference is rather subtle: we now penalize by the square of the coefficient rather than their absolute value. However, the difference in the results is dramatic.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip14"/>Tip</h3><p>
<span class="strong"><strong>Ridge, Lasso, and ElasticNets</strong></span>
</p><p>These penalized models often go by rather interesting names. The L1 penalized model is often called the <a id="id406" class="indexterm"/>
<span class="strong"><strong>Lasso</strong></span>, while an L2 penalized one is known as <a id="id407" class="indexterm"/>
<span class="strong"><strong>Ridge Regression</strong></span>. When using both, we call this an <a id="id408" class="indexterm"/>
<span class="strong"><strong>ElasticNet</strong></span> model.</p></div></div><p>Both the Lasso<a id="id409" class="indexterm"/> and the Ridge result in smaller coefficients than unpenalized regression (smaller in absolute value, ignoring the sign). However, the Lasso has the additional property that it results in many coefficients being set to exactly zero! This means that the final model does not even use some of its input features, the model is <a id="id410" class="indexterm"/>
<span class="strong"><strong>sparse</strong></span>. This is often a very desirable property as the model performs both feature selection and <span class="strong"><strong>regression</strong></span><a id="id411" class="indexterm"/> in a single step.</p><p>You will notice <a id="id412" class="indexterm"/>that whenever we add a penalty, we also add a weight <span class="emphasis"><em>α</em></span>, which governs how much penalization we want. When <span class="emphasis"><em>α</em></span> is close to zero, we are very close to unpenalized regression (in fact, if you set <span class="emphasis"><em>α</em></span> to zero, you will simply perform OLS), and when <span class="emphasis"><em>α</em></span> is large, we have a model that is very different from the unpenalized one.</p><p>The Ridge model is older as the Lasso is hard to compute with pen and paper. However, with modern computers, we can use the Lasso as easily as Ridge, or even combine them to form ElasticNets. An ElasticNet has two penalties, one for the absolute value and the other for the squares and it solves the following equation:</p><div class="mediaobject"><img src="images/2772OS_07_16.jpg" alt="L1 and L2 penalties"/></div><p>This formula is a combination of the two previous ones, with two parameters, <span class="emphasis"><em>α<sub>1</sub></em></span> and <span class="emphasis"><em>α<sub>2</sub></em></span>. Later in this chapter, we will discuss how to choose a good value for parameters.</p></div><div class="section" title="Using Lasso or ElasticNet in scikit-learn"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec62"/>Using Lasso or ElasticNet in scikit-learn</h2></div></div></div><p>Let's adapt the<a id="id413" class="indexterm"/> preceding example to use <a id="id414" class="indexterm"/>ElasticNets. Using scikit-learn, it is very easy to swap in the ElasticNet regressor for the least squares one that we had before:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.linear_model import ElasticNet, Lasso</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; en = ElasticNet(alpha=0.5)</strong></span>
</pre></div><p>Now, we <a id="id415" class="indexterm"/>use <code class="literal">en</code>, whereas <a id="id416" class="indexterm"/>earlier we had used <code class="literal">lr</code>. This is the only change that is needed. The results are exactly what we would have expected. The training error increases to 5.0 (it was 4.6 before), but the cross-validation error decreases to 5.4 (it was 5.6 before). We trade a larger error on the training data in order to gain better generalization. We could have tried an L1 penalty using the <code class="literal">Lasso</code> class or L2 using the <code class="literal">Ridge</code> class with the same code.</p></div><div class="section" title="Visualizing the Lasso path"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec63"/>Visualizing the Lasso path</h2></div></div></div><p>Using <a id="id417" class="indexterm"/>scikit-learn, we can easily visualize what happens as the value of the regularization parameter (alpha) changes. We will again use the Boston data, but now we will use the <code class="literal">Lasso</code> regression object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; las = Lasso(normalize=1)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; alphas = np.logspace(-5, 2, 1000)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; alphas, coefs, _= las.path(x, y, alphas=alphas)</strong></span>
</pre></div><p>For each value in alphas, the <code class="literal">path</code> method on the <code class="literal">Lasso</code> object returns the coefficients that solve the lasso problem with that parameter value. Because the result changes smoothly with alpha, this can be computed very efficiently.</p><p>A typical way to visualize this path is to plot the value of the coefficients as alpha decreases. You can do so as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; fig,ax = plt.subplots()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; ax.plot(alphas, coefs.T)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; # Set log scale</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; ax.set_xscale('log')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; # Make alpha decrease from left to right</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; ax.set_xlim(alphas.max(), alphas.min())</strong></span>
</pre></div><p>This results in the following plot (we left out the trivial code that adds axis labels and the title):</p><div class="mediaobject"><img src="images/2772OS_07_10.jpg" alt="Visualizing the Lasso path"/></div><p>In this plot, the <span class="emphasis"><em>x</em></span> axis shows decreasing amounts of regularization from left to right (alpha is decreasing). Each line shows how a different coefficient varies as alpha changes. The plot<a id="id418" class="indexterm"/> shows that when using very strong regularization (left side, very high alpha), the best solution is to have all values be exactly zero. As the regularization becomes weaker, one by one, the values of the different coefficients first shoot up, then stabilize. At some point, they all plateau as we are probably already close to the unpenalized solution.</p></div><div class="section" title="P-greater-than-N scenarios"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec64"/>P-greater-than-N scenarios</h2></div></div></div><p>The<a id="id419" class="indexterm"/> title of this section is a bit of inside jargon, which you will learn now. Starting in the 1990s, first in the biomedical domain, and then on the Web, problems started to appear where P was greater than N. What this means is that the number of features, P, was greater than the number of examples, N (these letters were the conventional statistical shorthand for these concepts). These became known as <span class="emphasis"><em>P greater than N</em></span> problems.</p><p>For example, if <a id="id420" class="indexterm"/>your input is a set of written documents, a simple way to approach it is to consider each possible word in the dictionary as a feature and regress on those (we will later work on one such problem ourselves). In the English language, you have over 20,000 words (this is if you perform some stemming and only consider common words; it is more than ten times that if you skip this preprocessing step). If you only have a few hundred or a few thousand examples, you will have more features than examples.</p><p>In this case, as the number of features is greater than the number of examples, it is possible to have a perfect fit on the training data. This is a mathematical fact, which is independent of your data. You are, in effect, solving a system of linear equations with fewer equations than variables. You can find a set of regression coefficients with zero training error (in fact, you can find more than one perfect solution, infinitely many).</p><p>However, and this is a major problem, <span class="emphasis"><em>zero training error does not mean that your solution will generalize well</em></span>. In fact, it may generalize very poorly. Whereas earlier regularization could give you a little extra boost, it is now absolutely required for a meaningful result.</p></div><div class="section" title="An example based on text documents"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec65"/>An example based on text documents</h2></div></div></div><p>We will now <a id="id421" class="indexterm"/>turn to an example that comes from a study performed at Carnegie Mellon University by Prof. Noah Smith's research group. The study was based on mining the so-called 10-K reports that companies file with the <span class="strong"><strong>Securities and Exchange Commission</strong></span> (<span class="strong"><strong>SEC</strong></span>)<a id="id422" class="indexterm"/> in the United States. This filing is mandated by law for all publicly traded companies. The goal of their study was to predict, based on this piece of public information, what the future volatility of the company's stock will be. In the training data, we are actually using historical data for which we already know what happened.</p><p>There are 16,087 examples available. The features, which have already been preprocessed for us, correspond to different words, 150,360 in total. Thus, we have many more features than examples, almost ten times as much. In the introduction, it was stated that ordinary least regression fails in these cases and we will now see why by attempting to blindly apply it.</p><p>The dataset is available in SVMLight format from multiple sources, including the book's companion website. This is a format that scikit-learn can read. SVMLight is, as the name says, a support vector machine implementation, which is also available through scikit-learn; right now, we are only interested in the file format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.datasets import load_svmlight_file</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; data,target = load_svmlight_file('E2006.train')</strong></span>
</pre></div><p>In the preceding code, data is a sparse matrix (that is, most of its entries are zeros and, therefore, only the nonzero entries are saved in memory), while the target is a simple one-dimensional vector. We can start by looking at some attributes of the target:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print('Min target value: {}'.format(target.min()))</strong></span>
<span class="strong"><strong>Min target value: -7.89957807347</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Max target value: {}'.format(target.max()))</strong></span>
<span class="strong"><strong>Max target value: -0.51940952694</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Mean target value: {}'.format(target.mean()))</strong></span>
<span class="strong"><strong>Mean target value: -3.51405313669</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Std. dev. target: {}'.format(target.std()))</strong></span>
<span class="strong"><strong>Std. dev. target: 0.632278353911</strong></span>
</pre></div><p>So, we can see <a id="id423" class="indexterm"/>that the data lies between -7.9 and -0.5. Now that we have a feel for the data, we can check what happens when we use OLS to predict. Note that we can use exactly the same classes and methods as we did earlier:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.linear_model import LinearRegression</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; lr = LinearRegression()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; lr.fit(data,target)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pred = lr.predict(data)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; rmse_train = np.sqrt(mean_squared_error(target, pred))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('RMSE on training: {:.2}'.format(rmse_train))</strong></span>
<span class="strong"><strong>RMSE on training: 0.0025</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('R2 on training: {:.2}'.format(r2_score(target, pred)))</strong></span>
<span class="strong"><strong>R2 on training: 1.0</strong></span>
</pre></div><p>The root mean squared error is not exactly zero because of rounding errors, but it is very close. The coefficient of determination is <code class="literal">1.0</code>. That is, the linear model is reporting a perfect prediction on its training data.</p><p>When we use cross-validation (the code is very similar to what we used earlier in the Boston example), we get something very different: RMSE of 0.75, which corresponds to a negative coefficient of determination of -0.42. This means that if we always "predict" the mean value of -3.5, we do better than when using the regression model!</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip15"/>Tip</h3><p>
<span class="strong"><strong>Training and generalization error</strong></span>
</p><p>When the number of features is greater than the number of examples, you always get zero training errors with OLS, except perhaps for issues due to rounding off. However, this is rarely a sign that your model will do well in terms of generalization. In fact, you may get zero training error and have a completely useless model.</p></div></div><p>The natural solution is to use regularization to counteract the overfitting. We can try the same cross-validation loop <a id="id424" class="indexterm"/>with an ElasticNet learner, having set the penalty parameter to <code class="literal">0.1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.linear_model import ElasticNet</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; met = ElasticNet(alpha=0.1)</strong></span>

<span class="strong"><strong>&gt;&gt;&gt; kf = KFold(len(target), n_folds=5)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pred = np.zeros_like(target)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for train, test in kf:</strong></span>
<span class="strong"><strong>...    met.fit(data[train], target[train])</strong></span>
<span class="strong"><strong>...    pred[test] = met.predict(data[test])</strong></span>

<span class="strong"><strong>&gt;&gt;&gt; # Compute RMSE</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; rmse = np.sqrt(mean_squared_error(target, pred))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('[EN 0.1] RMSE on testing (5 fold): {:.2}'.format(rmse))</strong></span>
<span class="strong"><strong>[EN 0.1] RMSE on testing (5 fold): 0.4</strong></span>

<span class="strong"><strong>&gt;&gt;&gt; # Compute Coefficient of determination</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; r2 = r2_score(target, pred)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('[EN 0.1] R2 on testing (5 fold): {:.2}'.format(r2))</strong></span>
<span class="strong"><strong>[EN 0.1] R2 on testing (5 fold): 0.61</strong></span>
</pre></div><p>Now, we get <code class="literal">0.4</code> RMSE and an R2 of <code class="literal">0.61</code>, much better than just predicting the mean. There is one problem with this solution, though, which is the choice of alpha. When using the default value (<code class="literal">1.0</code>), the result is very different (and worse).</p><p>In this case, we cheated as the author had previously tried a few values to see which ones would give a good result. This is not effective and can lead to over estimates of confidence (we are looking at the test data to decide which parameter values to use and which we should never use). The next section explains how to do it properly and how this is supported by scikit-learn.</p></div><div class="section" title="Setting hyperparameters in a principled way"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec66"/>Setting hyperparameters in a principled way</h2></div></div></div><p>In the <a id="id425" class="indexterm"/>preceding example, we set the penalty parameter to <code class="literal">0.1</code>. We could just as well have set it to 0.7 or 23.9. Naturally, the results vary each time. If we pick an overly large value, we get underfitting. In the extreme case, the learning system will just return every coefficient equal to zero. If we pick a value that is too small, we are very close to OLS, which overfits and generalizes poorly (as we saw earlier).</p><p>How do we choose a good value? This is a general problem in machine learning: setting parameters for our learning methods. A generic solution is to use cross-validation. We pick a set of possible values, and then use cross-validation to choose which one is best. This performs more computation (five times more if we use five folds), but is always applicable and unbiased.</p><p>We must be careful, though. In<a id="id426" class="indexterm"/> order to obtain an estimate of generalization, we have to use <a id="id427" class="indexterm"/>
<span class="strong"><strong>two-levels of cross-validation</strong></span>: one level is to estimate the generalization, while the second level is to get good parameters. That is, we split the data in, for example, five folds. We start by holding out the first fold and will learn on the other four. Now, we split these again into 5 folds in order to choose the parameters. Once we have set our parameters, we test on the first fold. Now, we repeat this four other times:</p><div class="mediaobject"><img src="images/2772OS_07_12.jpg" alt="Setting hyperparameters in a principled way"/></div><p>The preceding figure shows how you break up a single training fold into subfolds. We would need to repeat it for all the other folds. In this case, we are looking at five outer folds and five inner folds, but there is no reason to use the same number of outer and inner folds, you can use any number you want as long as you keep the folds separate.</p><p>This leads to a lot of computation, but it is necessary in order to do things correctly. The problem is that if you use a piece of data to make any decisions about your model (including which parameters to set), you have contaminated it and you can no longer use it to test the generalization ability of your model. This is a subtle point and it may not be immediately obvious. In fact, it is still the case that many users of machine learning get this wrong and overestimate how well their systems are doing, because they do not perform cross-validation correctly!</p><p>Fortunately, scikit-learn makes it very easy to do the right thing; it provides classes named <code class="literal">LassoCV</code>, <code class="literal">RidgeCV</code>, and <code class="literal">ElasticNetCV</code>, all of which encapsulate an inner cross-validation loop to optimize for the necessary parameter. The code is almost exactly like the previous one, except that we do not need to specify any value for alpha:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.linear_model import ElasticNetCV</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; met = ElasticNetCV()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; kf = KFold(len(target), n_folds=5)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; p = np.zeros_like(target)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for train,test in kf:</strong></span>
<span class="strong"><strong>...    met.fit(data[train],target[train])</strong></span>
<span class="strong"><strong>...    p[test] = met.predict(data[test])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; r2_cv = r2_score(target, p)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print("R2 ElasticNetCV: {:.2}".format(r2_cv))</strong></span>
<span class="strong"><strong>R2 ElasticNetCV: 0.65</strong></span>
</pre></div><p>This results in a <a id="id428" class="indexterm"/>lot of computation, so you may want to get some coffee while you are waiting (depending on how fast your computer is). You might get better performance by taking advantage of multiple processors. This is a built-in feature of scikit-learn, which can be accessed quite trivially by using the <code class="literal">n_jobs</code> parameter to the <code class="literal">ElasticNetCV</code> constructor. To use four CPUs, make use of the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; met = ElasticNetCV(n_jobs=4)</strong></span>
</pre></div><p>Set the <code class="literal">n_jobs</code> parameter to <code class="literal">-1</code> to use all the available CPUs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; met = ElasticNetCV(n_jobs=-1)</strong></span>
</pre></div><p>You may have wondered why, if ElasticNets have two penalties, the L1 and the L2 penalty, we only need to set a single value for alpha. In fact, the two values are specified by separately specifying alpha and the <code class="literal">l1_ratio</code> variable (that is spelled <span class="emphasis"><em>ell-1-underscore-ratio</em></span>). Then, α1 and α2 are set as follows (where <span class="emphasis"><em>ρ</em></span> stands for <code class="literal">l1_ratio</code>):</p><div class="mediaobject"><img src="images/2772OS_07_18.jpg" alt="Setting hyperparameters in a principled way"/></div><div class="mediaobject"><img src="images/2772OS_07_17.jpg" alt="Setting hyperparameters in a principled way"/></div><p>In an intuitive sense, alpha sets the overall amount of regularization while <code class="literal">l1_ratio</code> sets the tradeoff between the different types of regularization, L1 and L2.</p><p>We can request that the <code class="literal">ElasticNetCV</code> object tests different values of <code class="literal">l1_ratio</code>, as is shown in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; l1_ratio=[.01, .05, .25, .5, .75, .95, .99]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; met = ElasticNetCV(</strong></span>
<span class="strong"><strong>                l1_ratio=l1_ratio,</strong></span>
<span class="strong"><strong>                n_jobs=-1)</strong></span>
</pre></div><p>This set of <code class="literal">l1_ratio</code> values is recommended in the documentation. It will test models that are almost like Ridge (when <code class="literal">l1_ratio</code> is 0.01 or 0.05) as well as models that are almost like Lasso (when <code class="literal">l1_ratio</code> is 0.95 or 0.99). Thus, we explore a full range of different options.</p><p>Because of its <a id="id429" class="indexterm"/>flexibility and the ability to use multiple CPUs, <code class="literal">ElasticNetCV</code> is an excellent default solution for regression problems when you don't have any particular reason to prefer one type of model over the rest.</p><p>Putting all this together, we can now visualize the prediction versus real fit on this large dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; l1_ratio = [.01, .05, .25, .5, .75, .95, .99]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; met = ElasticNetCV(</strong></span>
<span class="strong"><strong>                l1_ratio=l1_ratio,</strong></span>
<span class="strong"><strong>                n_jobs=-1)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; p = np.zeros_like(target)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for train,test in kf:</strong></span>
<span class="strong"><strong>...     met.fit(data[train],target[train])</strong></span>
<span class="strong"><strong>...    p[test] = met.predict(data[test])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.scatter(p, y)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; # Add diagonal line for reference</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; # (represents perfect agreement)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.plot([p.min(), p.max()], [p.min(), p.max()])</strong></span>
</pre></div><p>This results in the following plot:</p><div class="mediaobject"><img src="images/2772OS_07_11.jpg" alt="Setting hyperparameters in a principled way"/></div><p>We can see that the <a id="id430" class="indexterm"/>predictions do not match very well on the bottom end of the value range. This is perhaps because there are so many fewer elements on this end of the target range (which also implies that this affects only a small minority of datapoints).</p><p>One last note: the approach of using an inner cross-validation loop to set a parameter is also available in scikit-learn using a grid search. In fact, we already used it in the previous chapter.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Summary</h1></div></div></div><p>In this chapter, we started with the oldest trick in the book, ordinary least squares regression. Although centuries old, it is still often the best solution for regression. However, we also saw more modern approaches that avoid overfitting and can give us better results especially when we have a large number of features. We used Ridge, Lasso, and ElasticNets; these are the state-of-the-art methods for regression.</p><p>We saw, once again, the danger of relying on training error to estimate generalization: it can be an overly optimistic estimate to the point where our model has zero training error, but we know that it is completely useless. When thinking through these issues, we were led into two-level cross-validation, an important point that many in the field still have not completely internalized.</p><p>Throughout this chapter, we were able to rely on scikit-learn to support all the operations we wanted to perform, including an easy way to achieve correct cross-validation. ElasticNets with an inner cross-validation loop for parameter optimization (as implemented in scikit-learn by <code class="literal">ElasticNetCV</code>) should probably become your default method for regression.</p><p>One reason to use an alternative is when you are interested in a sparse solution. In this case, a pure Lasso solution is more appropriate as it will set many coefficients to zero. It will also allow you to discover from the data a small number of variables, which are important to the output. Knowing the identity of these may be interesting in and of itself, in addition to having a good regression model.</p><p>In the next chapter, we will look at recommendations, another machine learning problem. Our first approach will be to use regression to predict consumer product ratings. We will then see alternative models to generate recommendations.</p></div></div>
</body></html>
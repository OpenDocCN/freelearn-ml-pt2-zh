- en: High-Performance Machine Learning – NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the iris dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing the iris dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing the iris dataset with pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting with NumPy and matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A minimal machine learning recipe – SVM classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning overview – classification versus regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll learn how to make predictions with scikit-learn. Machine
    learning emphasizes on measuring the ability to predict, and with scikit-learn
    we will predict accurately and quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will examine the `iris` dataset, which consists of measurements of three
    types of Iris flowers: *Iris S**etosa*, *Iris V**ersicolor*, and *Iris Virginica*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the strength of the predictions, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Save some data for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a model using only training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measure the predictive power on the test set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prediction—one of three flower types is categorical. This type of problem
    is called a **classification problem**.
  prefs: []
  type: TYPE_NORMAL
- en: Informally, classification asks, *Is it an apple or an orange?* Contrast this
    with machine learning regression, which asks, *How many apples?* By the way, the
    answer can be *4.5 apples* for regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the evolution of its design, scikit-learn addresses machine learning mainly
    via four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-text classification, like the Iris flowers example
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science deals in part with structured tables of data. The `scikit-learn`
    library requires input tables of two-dimensional NumPy arrays. In this section,
    you will learn about the `numpy` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will try a few operations on NumPy arrays. NumPy arrays have a single type
    for all of their elements and a predefined shape. Let us look first at their shape.
  prefs: []
  type: TYPE_NORMAL
- en: The shape and dimension of NumPy arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start by importing NumPy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Produce a NumPy array of 10 digits, similar to Python''s `range(10)` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The array looks like a Python list with only one pair of brackets. This means
    it is of one dimension. Store the array and find out the shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The array has a data attribute, `shape`. The type of `array_1.shape` is a tuple `(10L,)`,
    which has length `1`, in this case. The number of dimensions is the same as the
    length of the tuple—a dimension of `1`, in this case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The array has 10 elements. Reshape the array by calling the `reshape` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This reshapes the array into 5 x 2 data object that resembles a list of lists
    (a three dimensional NumPy array looks like a list of lists of lists). You did
    not save the changes. Save the reshaped array as follows::'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `array_1` is now two-dimensional. This is expected, as its shape
    has two numbers and it looks like a Python list of lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: NumPy broadcasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add `1` to every element of the array by broadcasting. Note that changes to
    the array are not saved:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The term **broadcasting** refers to the smaller array being stretched or broadcast
    across the larger array. In the first example, the scalar `1` was stretched to
    a 5 x 2 shape and then added to `array_1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new `array_2` array. Observe what occurs when you multiply the array
    by itself (this is not matrix multiplication; it is element-wise multiplication
    of arrays):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Every element has been squared. Here, element-wise multiplication has occurred.
    Here is a more complicated example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Change `array_1` as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now add `array_1` and `array_2` element-wise by simply placing a plus sign
    between the arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The formal broadcasting rules require that whenever you are comparing the shapes
    of both arrays from right to left, all the numbers have to either match or be
    one. The shapes **5 X 2** and **5 X 2** match for both entries from right to left.
    However, the shape **5 X 2 X 1** does not match **5 X 2**, as the second values
    from the right, **2** and **5** respectively, are mismatched:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/862cc6cc-1c37-433c-870d-70f656a4da57.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Initializing NumPy arrays and dtypes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several ways to initialize NumPy arrays besides `np.arange`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize an array of zeros with `np.zeros`. The `np.zeros((5,2))` command creates
    a 5 x 2 array of zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Initialize an array of ones using `np.ones`. Introduce a `dtype` argument, set
    to `np.int`, to ensure that the ones are of NumPy integer type. Note that scikit-learn
    expects `np.float` arguments in arrays. The `dtype` refers to the type of every
    element in a NumPy array. It remains the same throughout the array. Every single
    element of the array below has a np.int integer type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `np.empty` to allocate memory for an array of a specific size and `dtype`,
    but no particular initialized values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Use `np.zeros`, `np.ones`, and `np.empty` to allocate memory for NumPy arrays
    with different initial values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Look up the values of the two-dimensional arrays with indexing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'View the first row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then view the first column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'View specific values along both axes. Also view the second to the fourth rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'View the second to the fourth rows only along the first column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Boolean arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Additionally, NumPy handles indexing with Boolean logic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First produce a Boolean array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Place brackets around the Boolean array to filter by the Boolean array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Arithmetic operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add all the elements of the array with the `sum` method. Go back to `array_1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Find all the sums by row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Find all the sums by column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the mean of each column in a similar way. Note that the `dtype` of the
    array of averages is `np.float`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: NaN values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn will not accept `np.nan` values. Take `array_3` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the NaN values with a special Boolean array created by the `np.isnan`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter the NaN values by negating the Boolean array with the symbol ~ and placing
    brackets around the expression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, set the NaN values to zero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data, in the present and minimal sense, is about 2D tables of numbers, which
    NumPy handles very well. Keep this in mind in case you forget the NumPy syntax
    specifics. Scikit-learn accepts only 2D NumPy arrays of real numbers with no missing `np.nan` values.
  prefs: []
  type: TYPE_NORMAL
- en: From experience, it tends to be best to change `np.nan` to some value instead
    of throwing away data. Personally, I like to keep track of Boolean masks and keep
    the data shape roughly the same, as this leads to fewer coding errors and more
    coding flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform machine learning with scikit-learn, we need some data to start with.
    We will load the `iris` dataset, one of the several datasets available in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A scikit-learn program begins with several imports. Within Python, preferably
    in Jupyter Notebook, load the `numpy`, `pandas`, and `pyplot` libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are within a Jupyter Notebook, type the following to see a graphical
    output instantly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the scikit-learn `datasets` module, access the `iris` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similarly, you could have imported the `diabetes` dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'There! You''ve loaded `diabetes` using the `load_diabetes()` function of the
    `datasets` module. To check which datasets are available, type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you try that, you might observe that there is a dataset named `datasets.load_digits`.
    To access it, type the `load_digits()` function, analogous to the other loading
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: To view information about the dataset, type `digits.DESCR`.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've loaded the dataset, let's examine what is in it. The `iris` dataset
    pertains to a supervised classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To access the observation variables, type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s examine the NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the data is 150 rows by 4 columns. Let''s look at the first
    row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The NumPy array for the first row has four numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine what they mean, type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The feature or column names name the data. They are strings, and in this case,
    they correspond to dimensions in different types of flowers. Putting it all together,
    we have 150 examples of flowers with four measurements per flower in centimeters.
    For example, the first flower has measurements of 5.1 cm for sepal length, 3.5
    cm for sepal width, 1.4 cm for petal length, and 0.2 cm for petal width. Now,
    let''s look at the output variable in a similar manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields an array of outputs: `0`, `1`, and `2`. There are only three outputs.
    Type this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You get a shape of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This refers to an array of length 150 (150 x 1). Let''s look at what the numbers
    refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `iris.target_names` variable gives the English names for
    the numbers in the `iris.target` variable. The number zero corresponds to the
    `setosa` flower, number one corresponds to the `versicolor` flower, and number
    two corresponds to the `virginica` flower. Look at the first row of `iris.target`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This produces zero, and thus the first row of observations we examined before
    correspond to the `setosa` flower.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, we often deal with data tables and two-dimensional arrays
    corresponding to examples. In the `iris` set, we have 150 observations of flowers
    of three types. With new observations, we would like to predict which type of
    flower those observations correspond to. The observations in this case are measurements
    in centimeters. It is important to look at the data pertaining to real objects.
    Quoting my high school physics teacher, <q>"*Do not forget the units!*"</q>
  prefs: []
  type: TYPE_NORMAL
- en: The `iris` dataset is intended to be for a supervised machine learning task
    because it has a target array, which is the variable we desire to predict from
    the observation variables. Additionally, it is a classification problem, as there
    are three numbers we can predict from the observations, one for each type of flower.
    In a classification problem, we are trying to distinguish between categories.
    The simplest case is binary classification. The `iris` dataset, with three flower
    categories, is a multi-class classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the same data, we can rephrase the problem in many ways, or formulate new
    problems. What if we want to determine relationships between the observations?
    We can define the petal width as the target variable. We can rephrase the problem
    as a regression problem and try to predict the target variable as a real number,
    not just three categories. Fundamentally, it comes down to what we intend to predict.
    Here, we desire to predict a type of flower.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the iris dataset with Pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we will use the handy `pandas` data analysis library to view
    and visualize the `iris` dataset. It contains the notion o, a dataframe which
    might be familiar to you if you use the language R's dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can view the `iris` dataset with Pandas, a library built on top of NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a dataframe with the observation variables `iris.data`, and column names `columns`,
    as arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The dataframe is more user-friendly than the NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at a quick histogram of the values in the dataframe for `sepal length`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/bc06d071-510d-464e-8d01-1b6917d17b23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also color the histogram by the `target` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, iterate through the target numbers for each flower and draw a color histogram
    for each. Consider this line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'It finds the NumPy index location for each class of flower:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc479efd-c396-4ea0-8faa-2d699d0f84c5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Observe that the histograms overlap. This encourages us to model the three histograms
    as three normal distributions. This is possible in a machine learning manner if
    we model the training data only as three normal distributions, not the whole set.
    Then we use the test set to test the three normal distribution models we just
    made up. Finally, we test the accuracy of our predictions on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataframe data object is a 2D NumPy array with column names and row names.
    In data science, the fundamental data object looks like a 2D table, possibly because
    of SQL's long history. NumPy allows for 3D arrays, cubes, 4D arrays, and so on.
    These also come up often.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting with NumPy and matplotlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple way to make visualizations with NumPy is by using the library `matplotlib`.
    Let's make some visualizations quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start by importing `numpy` and `matplotlib`. You can view visualizations within
    an IPython Notebook using the `%matplotlib inline` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main command in matplotlib, in pseudo code, is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot a straight line by placing two NumPy arrays of the same length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/287e1a2e-a723-403f-912a-dc2ab10cfd2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot an exponential:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/df166a79-ba13-48c5-adfa-765233d0aec8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Place the two graphs side by side:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Or top to bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/490c815b-e265-454c-967b-e7dc4e617bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first two numbers in the subplot command refer to the grid size in the
    figure instantiated by `plt.figure()`. The grid size referred to in `plt.subplot(221)`
    is 2 x 2, the first two digits. The last digit refers to traversing the grid in
    reading order: left to right and then up to down.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot in a 2 x 2 grid traversing in reading order from one to four:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b6546b85-8074-4d63-9b2f-66bdadc52305.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, with real data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The `c` parameter takes an array of colors—in this case, the colors `0`, `1`,
    and `2` in the `iris` target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/975ccab7-6564-4a71-82cd-daee92065546.png)'
  prefs: []
  type: TYPE_IMG
- en: A minimal machine learning recipe – SVM classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning is all about making predictions. To make predictions, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: State the problem to be solved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose a model to solve the problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measure how well the model performed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Back to the iris example, we now store the first two features (columns) of
    the observations as `X` and the target as `y`, a convention in the machine learning
    community:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we state the problem. We are trying to determine the flower-type category
    from a set of new observations. This is a classification task. The data available
    includes a target variable, which we have named `y`. This is a supervised classification
    problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The task of supervised learning involves predicting values of an output variable
    with a model that trains using input variables and an output variable.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we choose a model to solve the supervised classification. For now, we
    will use a support vector classifier. Because of its simplicity and interpretability,
    it is a commonly used algorithm (*interpretable* means easy to read into and understand).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To measure the performance of prediction, we will split the dataset into training
    and test sets. The training set refers to data we will learn from. The test set
    is the data we hold out and pretend not to know as we would like to measure the
    performance of our learning procedure. So, import a function that will split the
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the function to both the observation and target data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The test size is 0.25 or 25% of the whole dataset. A random state of one fixes
    the random seed of the function so that you get the same results every time you
    call the function, which is important for now to reproduce the same results consistently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now load a regularly used estimator, a support vector machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'You have imported a support vector classifier from the `svm` module. Now create
    an instance of a linear SVC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The random state is fixed to reproduce the same results with the same code later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The supervised models in scikit-learn implement a `fit(X, y)` method, which
    trains the model and returns the trained model. `X` is a subset of the observations,
    and each element of `y` corresponds to the target of each observation in `X`.
    Here, we fit a model on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Now, the `clf` variable is the fitted, or trained, model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The estimator also has a `predict(X)` method that returns predictions for several
    unlabeled observations, `X_test`, and returns the predicted values, `y_pred`.
    Note that the function does not return the estimator. It returns a set of predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, you have done all but the last step. To examine the model performance,
    load a scorer from the metrics module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'With the scorer, compare the predictions with the held-out test targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Without knowing very much about the details of support vector machines, we have
    implemented a predictive model. To perform machine learning, we held out one-fourth
    of the data and examined how the SVC performed on that data. In the end, we obtained
    a number that measures accuracy, or how the model performed.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To summarize, we will do all the steps with a different algorithm, logistic
    regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import `LogisticRegression`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Then write a program with the modeling steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training and testing sets.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the logistic regression model.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict using the test observations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Measure the accuracy of the predictions with `y_test` versus `y_pred`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This number is lower; yet we cannot make any conclusions comparing the two models,
    SVC and logistic regression classification. We cannot compare them, because we
    were not supposed to look at the test set for our model. If we made a choice between
    SVC and logistic regression, the choice would be part of our model as well, so
    the test set cannot be involved in the choice. Cross-validation, which we will
    look at next, is a way to choose between models.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are thankful for the `iris` dataset, but as you might recall, it has only
    150 observations. To make the most out of the set, we will employ cross-validation.
    Additionally, in the last section, we wanted to compare the performance of two
    different classifiers, support vector classifier and logistic regression. Cross-validation
    will help us with this comparison issue as well.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose we wanted to choose between the support vector classifier and the logistic
    regression classifier. We cannot measure their performance on the unavailable
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if, instead, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Forgot about the test set for now?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the training set into two parts, one to train on and one to test the training?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Split the training set into two parts using the train_test_split function used
    in previous sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '`X_train_2` consists of 75% of the `X_train` data, while `X_test_2` is the
    remaining 25%. `y_train_2` is 75% of the target data, and matches the observations
    of `X_train_2`. `y_test_2` is 25% of the target data present in `y_train`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might have expected, you have to use these new splits to choose between
    the two models: SVC and logistic regression. Do so by writing a predictive program.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start with all the imports and load the `iris` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an instance of an SVC classifier and fit it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Do the same for logistic regression (both lines for logistic regression are
    compressed into one):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now predict and examine the SVC and logistic regression''s performance on `X_test_2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The SVC performs better, but we have not yet seen the original test data. Choose
    SVC over logistic regression and try it on the original test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In comparing the SVC and logistic regression classifier, you might wonder (and
    be a little suspicious) about a lot of scores being very different. The final
    test on SVC scored lower than logistic regression. To help with this situation,
    we can do cross-validation in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-validation involves splitting the training set into parts, as we did
    before. To match the preceding example, we split the training set into four parts,
    or folds. We are going to design a cross-validation iteration by taking turns
    with one of the four folds for testing and the other three for training. It is
    the same split as done before four times over with the same set, thereby rotating,
    in a sense, the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1e7c4be-773e-49cb-9291-1b06a0b75050.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With scikit-learn, this is relatively easy to accomplish:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with an import:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we produce an accuracy score on four folds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We can find the mean for average performance and standard deviation for a measure
    of spread of all scores relative to the mean:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, with the logistic regression instance, we compute four scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Now we have many scores, which confirms our selection of SVC over logistic regression.
    Thanks to cross-validation, we used the training multiple times and had four small
    test sets within it to score our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that our model is a bigger model that consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: Training an SVM through cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a logistic regression through cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing between SVM and logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice at the end is part of the model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite our hard work and the elegance of the scikit-learn syntax, the score
    on the test set at the very end remains suspicious. The reason for this is that
    the test and train split are not necessarily balanced; the train and test sets
    do not necessarily have similar proportions of all the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is easily remedied by using a stratified test-train split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: By selecting the target set as the stratified argument, the target classes are
    balanced. This brings the SVC scores closer together.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, note that in the preceding example, the cross-validation procedure
    produces stratified folds by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we are going to perform the same procedure as before, except that we will
    reset, regroup, and try a new algorithm: **K-Nearest Neighbors** (**KNN**).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start by importing the model from `sklearn`, followed by a balanced split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: The `random_state` parameter fixes the `random_seed` in the function `train_test_split`.
    In the preceding example, the `random_state` is set to zero and can be set to
    any integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Construct two different KNN models by varying the `n_neighbors` parameter.
    Observe that the number of folds is now 10\. Tenfold cross-validation is common
    in the machine learning community, particularly in data science competitions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Score and print out the scores for selection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Both nearest neighbor types score similarly, yet the KNN with parameter `n_neighbors
    = 5` is a bit more stable. This is an example of *hyperparameter optimization* which
    we will examine closely throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You could have just as easily run a simple loop to score the function more
    quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Its output suggests that `n_neighbors = 4` is a good choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Machine learning overview – classification versus regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we will examine how regression can be viewed as being very similar
    to classification. This is done by reconsidering the categorical labels of regression
    as real numbers. In this section we will also look at at several aspects of machine
    learning from a very broad perspective including the purpose of scikit-learn.
    scikit-learn allows us to find models that work well incredibly quickly. We do
    not have to work out all the details of the model, or optimize, until we found
    one that works well. Consequently, your company saves precious development time
    and computational resources thanks to scikit-learn giving us the ability to develop
    models relatively quickly.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen before, scikit-learn allowed us to find a model that works fairly
    quickly. We tried SVC, logistic regression, and a few KNN classifiers. Through
    cross-validation, we selected models that performed better than others. In industry,
    after trying SVMs and logistic regression, we might focus on SVMs and optimize
    them further. Thanks to scikit-learn, we saved a lot of time and resources, including
    mental energy. After optimizing the SVM at work on a realistic dataset, we might
    re-implement it for speed in Java or C and gather more data.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised versus unsupervised
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification and regression are supervised, as we know the target variables
    for the observations. Clustering—creating regions in space for each category without
    being given any labels is unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In classification, the target variable is one of several categories, and there
    must be more than one instance of every category. In regression, there can be
    only one instance of every target variable, as the only requirement is that the
    target is a real number.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of logistic regression, we saw previously that the algorithm first
    performs a regression and estimates a real number for the target. Then the target
    class is estimated by using thresholds. In scikit-learn, there are `predict_proba`
    methods that yield probabilistic estimates, which relate regression-like real
    number estimates with classification classes in the style of logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Any regression can be turned into classification by using thresholds. A binary
    classification can be viewed as a regression problem by using a regressor. The
    target variables produced will be real numbers, not the original class variables.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quick SVC – a classifier and regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load `iris` from the `datasets` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity, consider only targets `0` and `1`, corresponding to Setosa
    and Versicolor. Use the Boolean array `iris.target < 2` to filter out target `2`.
    Place it within brackets to use it as a filter in defining the observation set
    `X` and the target set `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Now import `train_test_split` and apply it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare and run an SVC by importing it and scoring it with cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'As done in previous sections, view the average of the scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the same with support vector regression by importing `SVR` from `sklearn.svm`,
    the same module that contains SVC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Then write the necessary syntax to fit the model. It is almost identical to
    the syntax for SVC, just replacing some `c` keywords with `r`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Making a scorer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make a scorer, you need:'
  prefs: []
  type: TYPE_NORMAL
- en: A scoring function that compares `y_test`, the ground truth, with `y_pred`,
    the predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To determine whether a high score is good or bad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before passing the SVR regressor to the cross-validation, make a scorer by
    supplying two elements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, begin by importing the `make_scorer` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Use this sample scoring function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: The `np.rint` function rounds off the prediction to the nearest integer, hopefully
    one of the targets, `0` or `1`. The `astype` method changes the type of the prediction
    to integer type, as the original target is in integer type and consistency is
    preferred with regard to types. After the rounding occurs, the scoring function
    uses the old `accuracy_score` function, which you are familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, determine whether a higher score is better. Higher accuracy is better,
    so for this situation, a higher score is better. In scikit code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, run the cross-validation with a new parameter, the scoring parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the mean:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy scores are similar for the SVR regressor-based classifier and the
    traditional SVC classifier.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might ask, why did we take out class `2` out of the target set?
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason is that, to use a regressor, our intent has to be to predict a real
    number. The categories had to have real number properties: that they are ordered
    (informally, if we have three ordered categories *x*, *y*, *z* and *x* < *y* and
    *y* < *z* then *x* < *z*). By eliminating the third category, the remaining flowers
    (Setosa and Versicolor) became ordered by a property we invented: Setosaness or
    Versicolorness.'
  prefs: []
  type: TYPE_NORMAL
- en: The next time you encounter categories, you can consider whether they can be
    ordered. For example, if the dataset consists of shoe sizes, they can be ordered
    and a regressor can be applied, even though no one has a shoe size of 12.125.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear versus nonlinear
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear algorithms involve lines or hyperplanes. Hyperplanes are flat surfaces
    in any *n*-dimensional space. They tend to be easy to understand and explain,
    as they involve ratios (with an offset). Some functions that consistently and
    monotonically increase or decrease can be mapped to a linear function with a transformation.
    For example, exponential growth can be mapped to a line with the log transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear algorithms tend to be tougher to explain to colleagues and investors,
    yet ensembles of decision trees that are nonlinear tend to perform very well.
    KNN, which we examined earlier, is nonlinear. In some cases, functions not increasing
    or decreasing in a familiar manner are acceptable for the sake of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try a simple SVC with a polynomial kernel, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The polynomial kernel of degree 3 looks like a cubic curve in two dimensions.
    It leads to a slightly better fit, but note that it can be harder to explain to
    others than a linear kernel with consistent behavior throughout all of the Euclidean
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Black box versus not
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the sake of efficiency, we did not examine the classification algorithms
    used very closely. When we compared SVC and logistic regression, we chose SVMs.
    At that point, both algorithms were black boxes, as we did not know any internal
    details. Once we decided to focus on SVMs, we could proceed to compute coefficients
    of the separating hyperplanes involved, optimize the hyperparameters of the SVM,
    use the SVM for big data, and do other processes. The SVMs have earned our time
    investment because of their superior performance.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some machine learning algorithms are easier to understand than others. These
    are usually easier to explain to others as well. For example, linear regression
    is well known and easy to understand and explain to potential investors of your
    company. SVMs are more difficult to entirely understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'My general advice: if SVMs are highly effective for a particular dataset, try
    to increase your personal interpretability of SVMs in the particular problem context.
    Also, consider merging algorithms somehow, using linear regression as an input
    to SVMs, for example. This way, you have the best of both worlds.'
  prefs: []
  type: TYPE_NORMAL
- en: This is really context-specific, however. Linear SVMs are relatively simple
    to visualize and understand. Merging linear regression with SVM could complicate
    things. You can start by comparing them side by side.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you cannot understand every detail of the math and practice of SVMs,
    be kind to yourself, as machine learning is focused more on prediction performance
    rather than traditional statistics.
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In programming, a pipeline is a set of procedures connected in series, one
    after the other, where the output of one process is the input to the next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc22816a-cefb-4269-9c4d-04900308b55e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can replace any procedure in the process with a different one, perhaps
    better in some way, without compromising the whole system. For the model in the
    middle step, you can use an SVC or logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2a52812-d38e-45ae-b047-3c96e13f0d62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One can also keep track of the classifier itself and build a flow diagram from
    the classifier. Here is a pipeline keeping track of the SVC classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8267f221-917f-41ea-b55a-d9185d3fbb01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the upcoming chapters, we will see how scikit-learn uses the intuitive notion
    of a pipeline. So far, we have used a simple one: train, predict, test.'
  prefs: []
  type: TYPE_NORMAL

- en: 2\. Exploratory Data Analysis and Visualization
  id: totrans-0
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2. 探索性数据分析与可视化
- en: Overview
  id: totrans-1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter takes us through how to perform exploration and analysis on a new
    dataset. By the end of this chapter, you will be able to explain the importance
    of data exploration and communicate the summary statistics of a dataset. You will
    visualize patterns in missing values in data and be able to replace null values
    appropriately. You will be equipped to identify continuous features, categorical
    features and visualize distributions of values across individual variables. You
    will also be able to describe and analyze relationships between different types
    of variables using correlation and visualizations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章带领我们了解如何对一个新的数据集进行探索和分析。到本章结束时，你将能够解释数据探索的重要性，并能够传达数据集的汇总统计信息。你将能够可视化数据中缺失值的模式，并能够适当地替换空值。你将学会识别连续特征、分类特征，并可视化各个变量的值分布。你还将能够使用相关性和可视化来描述和分析不同类型变量之间的关系。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 介绍
- en: Say we have a problem statement that involves predicting whether a particular
    earthquake caused a tsunami. How do we decide what model to use? What do we know
    about the data we have? Nothing! But if we don't know and understand our data,
    chances are we'll end up building a model that's not very interpretable or reliable.
    When it comes to data science, it's important to have a thorough understanding
    of the data we're dealing with, in order to generate features that are highly
    informative and, consequently, to build accurate and powerful models. To acquire
    this understanding, we perform an exploratory analysis of the data to see what
    the data can tell us about the relationships between the features and the target
    variable (the value that you are trying to predict using the other variables).
    Getting to know our data will even help us interpret the model we build and identify
    ways we can improve its accuracy. The approach we take to achieve this is to allow
    the data to reveal its structure or model, which helps us gain some new, often
    unsuspected, insight into the data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个问题陈述，涉及预测某次地震是否引发了海啸。我们如何决定使用什么模型？我们对现有的数据了解多少？什么都不知道！但如果我们不了解数据，最终可能会建立一个不太可解释或不可靠的模型。在数据科学中，彻底理解我们所处理的数据非常重要，以便生成高度信息化的特征，并因此构建准确而强大的模型。为了获得这种理解，我们对数据进行探索性分析，看看数据能告诉我们关于特征和目标变量（你试图通过其他变量预测的值）之间关系的信息。了解数据甚至有助于我们解释所构建的模型，并找出改进其准确性的方法。我们采取的做法是让数据揭示其结构或模型，这有助于我们获得一些新的、往往是意想不到的见解。
- en: We will first begin with a brief introduction to exploratory data analysis and
    then progress to explaining summary statistics and central values. This chapter
    also teaches you how to find and visualize missing values and then describes the
    various imputation strategies for addressing the problem of missing values. The
    remainder of the chapter then focuses on visualizations. Specifically, the chapter
    teaches you how to create various plots such as scatter plot, histograms, pie
    charts, heatmaps, pairplots and more. Let us begin with exploratory data analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先简要介绍探索性数据分析，然后逐步解释汇总统计和中心值。本章还将教你如何查找和可视化缺失值，并描述处理缺失值问题的各种填充策略。接下来的部分将专注于可视化。具体来说，本章教你如何创建各种图表，如散点图、直方图、饼图、热图、配对图等。让我们从探索性数据分析开始。
- en: Exploratory Data Analysis (EDA)
  id: totrans-6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）
- en: Exploratory data analysis (EDA) is defined as a method to analyze datasets and
    sum up their main characteristics to derive useful conclusions, often with visual
    methods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）被定义为一种分析数据集并总结其主要特征的方法，通过这种方法得出有用的结论，通常采用可视化方法。
- en: 'The purpose of EDA is to:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: EDA的目的是：
- en: Discover patterns within a dataset
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 发现数据集中的模式
- en: Spot anomalies
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 发现异常值
- en: Form hypotheses regarding the behavior of data
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据行为形成假设
- en: Validate assumptions
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 验证假设
- en: Everything from basic summary statistics to complex visualizations helps us
    gain an intuitive understanding of the data itself, which is highly important
    when it comes to forming new hypotheses about the data and uncovering what parameters
    affect the target variable. Often, discovering how the target variable varies
    across a single feature gives us an indication of how important a feature might
    be, and a variation across a combination of several features helps us to come
    up with ideas for new informative features to engineer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从基本的摘要统计到复杂的可视化帮助我们直观地理解数据本身，这在形成关于数据的新假设并揭示哪些参数影响目标变量时极为重要。通常，发现目标变量如何在单一特征上变化，会给我们提供该特征可能有多重要的指示，而多个特征组合的变化有助于我们提出新的有信息量的特征工程思路。
- en: Most explorations and visualizations are intended to understand the relationship
    between the features and the target variable. This is because we want to find
    out what relationships exist (or don't exist) between the data we have and the
    values we want to predict.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数探索性分析和可视化的目的是理解特征与目标变量之间的关系。这是因为我们希望找出我们所拥有的数据与我们要预测的值之间存在（或不存在）什么关系。
- en: 'EDA can tell us about:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: EDA可以告诉我们：
- en: Features that are unclean, have missing values, or have outliers
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有缺失值、脏数据或异常值的特征
- en: Features that are informative and are a good indicator of the target
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有助于目标识别的具有信息性的特征
- en: The kind of relationships features have with the target
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 特征与目标之间的关系类型
- en: Further features that the data might need that we don't already have
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能需要的进一步特征，我们当前并没有
- en: Edge cases you might need to account for separately
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要单独考虑的边缘情况
- en: Filters you might need to apply to the dataset
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要应用于数据集的过滤条件
- en: The presence of incorrect or fake data points
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 存在不正确或虚假数据点
- en: Now that we've looked at why EDA is important and what it can tell us, let's
    talk about what exactly EDA involves. EDA can involve anything from looking at
    basic summary statistics to visualizing complex trends over multiple variables.
    However, even simple statistics and plots can be powerful tools, as they may reveal
    important facts about the data that could change our modeling perspective. When
    we see plots representing data, we are able to easily detect trends and patterns,
    compared to just raw data and numbers. These visualizations further allow us to
    ask questions such as "How?" and "Why?", and form hypotheses about the dataset
    that can be validated by further visualizations. This is a continuous process
    that leads to a deeper understanding of the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了EDA的重要性以及它能告诉我们什么，接下来让我们讨论EDA到底涉及什么。EDA可以包括从查看基本的摘要统计数据到可视化多变量之间的复杂趋势。尽管如此，即便是简单的统计数据和图表也可以是强大的工具，因为它们可能揭示出关于数据的重要事实，这些事实可能会改变我们建模的视角。当我们看到表示数据的图表时，相比于仅仅是原始数据和数字，我们能更容易地检测到趋势和模式。这些可视化还可以让我们提出“如何？”和“为什么？”这样的问题，并对数据集形成可以通过进一步可视化验证的假设。这是一个持续的过程，最终会加深我们对数据的理解。
- en: 'The dataset that we will use for our exploratory analysis and visualizations
    has been taken from the Significant Earthquake Database from NOAA, available as
    a public dataset on Google BigQuery (table ID: ''bigquery-public-data.noaa_significant_earthquakes.earthquakes'').
    We will be using a subset of the columns available, the metadata for which is
    available at https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table,
    and will load it into a pandas DataFrame to perform the exploration. We''ll primarily
    be using Matplotlib for most of our visualizations, along with the Seaborn and
    Missingno libraries for some. It is to be noted, however, that Seaborn merely
    provides a wrapper over Matplotlib''s functionalities, so anything that is plotted
    using Seaborn can also be plotted using Matplotlib. We''ll try to keep things
    interesting by using visualizations from both libraries.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于探索性分析和可视化的数据集来自NOAA的显著地震数据库，该数据库作为公共数据集可以在Google BigQuery上访问（表ID：'bigquery-public-data.noaa_significant_earthquakes.earthquakes'）。我们将使用其中一部分列，相关元数据可以在[https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table](https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table)获取，并将其加载到pandas
    DataFrame中进行探索。我们主要将使用Matplotlib进行大多数可视化，同时也会使用Seaborn和Missingno库进行部分可视化。然而需要注意的是，Seaborn只是Matplotlib功能的封装，因此，使用Seaborn绘制的图形也可以通过Matplotlib绘制。我们将通过这两种库的可视化来保持内容的趣味性。
- en: 'The exploration and analysis will be conducted keeping in mind a sample problem
    statement: Given the data we have, we want to predict whether an earthquake caused
    a tsunami. This will be a classification problem (more on this in Chapter 5, Classification
    Techniques) where the target variable is the flag_tsunami column.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 探索和分析将基于一个示例问题：根据我们拥有的数据，我们想预测地震是否引发了海啸。这将是一个分类问题（更多内容请参见第 5 章，分类技术），目标变量是 flag_tsunami
    列。
- en: Before we begin, let's first import the required libraries, which we will be
    using for most of our data manipulations and visualizations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，首先导入所需的库，这些库将用于我们大多数的数据操作和可视化。
- en: 'In a Jupyter notebook, import the following libraries:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中，导入以下库：
- en: import json
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: import json
- en: import pandas as pd
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import numpy as np
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import missingno as msno
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: import missingno as msno
- en: from sklearn.impute import SimpleImputer
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.impute import SimpleImputer
- en: import matplotlib.pyplot as plt
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: import seaborn as sns
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: import seaborn as sns
- en: 'We can also read in the metadata containing the data types for each column,
    which are stored in the form of a JSON file. Do this using the following command.
    This command opens the file in a readable format and uses the json library to
    read the file into a dictionary:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以读取包含每列数据类型的元数据，这些数据以 JSON 文件的形式存储。使用以下命令来读取。此命令以可读格式打开文件，并使用 json 库将文件读取为字典：
- en: 'with open(''..\dtypes.json'', ''r'') as jsonfile:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''..\dtypes.json'', ''r'') as jsonfile:'
- en: dtyp = json.load(jsonfile)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: dtyp = json.load(jsonfile)
- en: Note
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The output of the preceding command can be found here: https://packt.live/3a4Zjhm'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令的输出可以在这里找到：https://packt.live/3a4Zjhm
- en: Summary Statistics and Central Values
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概要统计和中心值
- en: In order to find out what our data really looks like, we use a technique known
    as data profiling. This is defined as the process of examining the data available
    from an existing information source (for example, a database or a file) and collecting
    statistics or informative summaries about that data. The goal is to make sure
    that you understand your data well and are able to identify any challenges that
    the data may pose early on in the project, which is done by summarizing the dataset
    and assessing its structure, content, and quality.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们的数据真实情况，我们使用一种称为数据概况的技术。数据概况被定义为检查现有信息来源（例如，数据库或文件）中的数据，并收集该数据的统计信息或信息性摘要的过程。其目的是确保你充分理解数据，并能够及早识别数据可能在项目中带来的挑战，这通常通过总结数据集并评估其结构、内容和质量来实现。
- en: Data profiling includes collecting descriptive statistics and data types. Common
    data profile commands include those you have seen previously, including data.describe(),
    data.head(), and data.tail(). You can also use data.info(), which tells you how
    many non-null values there are in each column, along with the data type of the
    values (non-numeric types are represented as object types).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据概况包括收集描述性统计信息和数据类型。常见的数据概况命令包括你之前见过的命令，例如 data.describe()、data.head() 和 data.tail()。你还可以使用
    data.info()，它会告诉你每列中有多少非空值，以及这些值的数据类型（非数字类型表示为对象类型）。
- en: 'Exercise 2.01: Summarizing the Statistics of Our Dataset'
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.01：总结我们的数据集统计信息
- en: 'In this exercise, we will use the summary statistics functions we read about
    previously to get a basic idea of our dataset:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用之前学到的概要统计函数，初步了解我们的数据集：
- en: Note
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be found on our GitHub repository here: https://packt.live/2TjU9aj'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以在我们的 GitHub 仓库中找到，链接如下：https://packt.live/2TjU9aj
- en: 'Read the earthquakes data into a data pandas DataFrame and use the dtyp dictionary
    we read using the json library in the previous section, to specify the data types
    of each column in the CSV. Begin by loading the requisite libraries and the JSON
    file we have prepared with the data types. You can inspect the data types before
    reading the data:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将地震数据读取到 pandas DataFrame 中，并使用我们在前一节中使用 json 库读取的 dtyp 字典，指定 CSV 中每列的数据类型。首先加载所需的库和我们已经准备好的包含数据类型的
    JSON 文件。你可以在读取数据之前检查数据类型：
- en: import json
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: import json
- en: import pandas as pd
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import numpy as np
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import missingno as msno
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: import missingno as msno
- en: from sklearn.impute import SimpleImputer
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.impute import SimpleImputer
- en: import matplotlib.pyplot as plt
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: import seaborn as sns
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: import seaborn as sns
- en: 'with open(''../dtypes.json'', ''r'') as jsonfile:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''../dtypes.json'', ''r'') as jsonfile:'
- en: dtyp = json.load(jsonfile)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: dtyp = json.load(jsonfile)
- en: dtyp
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: dtyp
- en: 'The output will be as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.1: Inspecting data types'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1：检查数据类型'
- en: '](img/image-J7XDCDBE.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-J7XDCDBE.jpg)'
- en: 'Figure 2.1: Inspecting data types'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：检查数据类型
- en: 'Use the data.info() function to get an overview of the dataset:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 data.info() 函数获取数据集的概览：
- en: data = pd.read_csv('../Datasets/earthquake_data.csv', dtype = dtyp)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: data = pd.read_csv('../Datasets/earthquake_data.csv', dtype = dtyp)
- en: data.info()
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: data.info()
- en: 'The output will be as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.2: Overview of the dataset'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2：数据集概览'
- en: '](img/image-R6JIB68X.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-R6JIB68X.jpg)'
- en: 'Figure 2.2: Overview of the dataset'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：数据集概览
- en: 'Print the first five and the last five rows of the dataset. The first five
    rows are printed as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 打印数据集的前五行和最后五行。前五行打印如下：
- en: data.head()
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: data.head()
- en: data.tail()
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: data.tail()
- en: 'The output will be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.3: The first and last five rows'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3：前五行和最后五行'
- en: '](img/image-2B3IJSFX.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-2B3IJSFX.jpg)'
- en: 'Figure 2.3: The first and last five rows'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：前五行和最后五行
- en: We can see in these outputs that there are 28 columns, but not all of them are
    displayed. Only the first 10 and last 10 columns are displayed, with the ellipses
    representing the fact that there are columns in between that are not displayed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这些输出中看到，数据集有28列，但并不是所有的列都显示出来。只显示了前10列和最后10列，省略号表示其中有未显示的列。
- en: 'Use data.describe() to find the summary statistics of the dataset. Run data.describe().T:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 data.describe() 查找数据集的汇总统计信息。运行 data.describe().T：
- en: data.describe().T
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: data.describe().T
- en: Here, .T indicates that we're taking a transpose of the DataFrame to which it
    is applied, that is, turning the columns into rows and vice versa. Applying it
    to the describe() function allows us to see the output more easily with each row
    in the transposed DataFrame now corresponding to the statistics for a single feature.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，.T 表示我们正在对应用它的 DataFrame 进行转置，即将列变成行，反之亦然。将其应用于 describe() 函数，使我们能更容易地查看输出，每一行在转置后的
    DataFrame 中对应一个特征的统计信息。
- en: 'We should get an output like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到类似这样的输出：
- en: '![Figure 2.4: Summary statistics'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4：汇总统计信息'
- en: '](img/image-1BKB9Y9V.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-1BKB9Y9V.jpg)'
- en: 'Figure 2.4: Summary statistics'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：汇总统计信息
- en: Note
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2Yl5qer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考 [https://packt.live/2Yl5qer](https://packt.live/2Yl5qer)。
- en: You can also run this example online at https://packt.live/2V3I76D. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 [https://packt.live/2V3I76D](https://packt.live/2V3I76D)。你必须执行整个
    Notebook 才能得到期望的结果。
- en: Notice here that the describe() function only shows the statistics for columns
    with numerical values. This is because we cannot calculate the statistics for
    the columns having non-numerical values (although we can visualize their values,
    as we will see later).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里 describe() 函数只显示数值型列的统计信息。这是因为我们无法对非数值型列计算统计数据（尽管我们可以像稍后所见那样可视化它们的值）。
- en: Missing Values
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺失值
- en: When there is no value (that is, a null value) recorded for a particular feature
    in a data point, we say that the data is missing. Having missing values in a real
    dataset is inevitable; no dataset is ever perfect. However, it is important to
    understand why the data is missing, and whether there is a factor that has affected
    the loss of data. Appreciating and recognizing this allows us to handle the remaining
    data in an appropriate manner. For example, if the data is missing randomly, then
    it's highly likely that the remaining data is still representative of the population.
    However, if the missing data is not random in nature and we assume that it is,
    it could bias our analysis and subsequent modeling.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据点的某个特征没有记录值（即为空值）时，我们称数据为缺失数据。在实际数据集中出现缺失值是不可避免的；没有数据集是完美的。然而，理解数据缺失的原因以及是否有某些因素影响了数据丢失非常重要。理解并识别这一点可以帮助我们以合适的方式处理其余数据。例如，如果数据是随机缺失的，那么剩余的数据很可能仍然能够代表整个数据集。但如果缺失的数据并非随机缺失，而我们假设它是随机的，这可能会导致我们的分析和后续建模出现偏差。
- en: 'Let''s look at the common reasons (or mechanisms) for missing data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下缺失数据的常见原因（或机制）：
- en: 'Missing Completely at Random (MCAR): Values in a dataset are said to be MCAR
    if there is no correlation whatsoever between the value missing and any other
    recorded variable or external parameter. This means that the remaining data is
    still representative of the population, though this is rarely the case and taking
    missing data to be completely random is usually an unrealistic assumption.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 完全随机缺失（MCAR）：如果数据集中缺失的值与任何其他记录的变量或外部参数之间没有任何相关性，那么这些缺失值被称为 MCAR。这意味着其余数据仍然能够代表整个群体，尽管这种情况很少发生，且假设缺失数据是完全随机的通常是不现实的。
- en: For example, in a study that involves determining the reason for obesity among
    K12 children, MCAR is when the parents forgot to take their children to the clinic
    for the study.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一项研究中，如果要确定 K12 学生肥胖的原因，MCAR 的情况是父母忘记带孩子去诊所参加研究。
- en: 'Missing at Random (MAR): If the case where the data is missing is related to
    the data that was recorded rather than the data that was not, then the data is
    said to be MAR. Since it''s unfeasible to statistically verify whether data is
    MAR, we''d have to depend on whether it''s a reasonable possibility.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随机缺失（MAR）：如果数据缺失的原因与已记录的数据相关，而与未记录的数据无关，那么这些数据就被称为 MAR。由于无法通过统计方法验证数据是否为 MAR，我们只能依赖是否存在合理的可能性来判断。
- en: Using the K12 study, missing data in this case is due to parents moving to a
    different city, hence the children had to leave the study; missingness has nothing
    to do with the study itself.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以 K12 研究为例，缺失数据的原因是父母搬到了其他城市，导致孩子不得不退出研究；缺失与研究本身无关。
- en: 'Missing Not at Random (MNAR): Data that is neither MAR nor MCAR is said to
    be MNAR. This is the case of a non-ignorable non-response, that is, the value
    of the variable that''s missing is related to the reason it is missing.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 非随机缺失（MNAR）：既不是 MAR 也不是 MCAR 的数据被称为 MNAR。这种情况通常是不可忽略的非响应情况，也就是说，缺失的变量值与其缺失的原因有关。
- en: Continuing with the example of the case study, data would be MNAR if the parents
    were offended by the nature of the study and did not want their children to be
    bullied, so they withdrew their children from the study.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用案例研究的例子，如果父母因为研究的性质感到不悦，不希望孩子被欺负，因此将孩子从研究中撤出，那么数据就是 MNAR（非随机缺失）。
- en: Finding Missing Values
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查找缺失值
- en: So, now that we know why it's important to familiarize ourselves with the reasons
    behind why our data is missing, let's talk about how we can find these missing
    values in a dataset. For a pandas DataFrame, this is most commonly executed using
    the .isnull() method on a DataFrame to create a mask of the null values (that
    is, a DataFrame of Boolean values) indicating where the null values exist—a True
    value at any position indicates a null value, while a False value indicates the
    existence of a valid value at that position.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们知道了为什么了解数据缺失背后的原因如此重要，接下来让我们讨论如何在数据集中找到这些缺失值。对于一个 pandas DataFrame，最常见的做法是使用
    `.isnull()` 方法，这个方法会在 DataFrame 上创建一个缺失值的掩码（即一个布尔值的 DataFrame），用来指示缺失值的位置——任何位置上为
    True 的值表示该位置是缺失值，而 False 则表示该位置有有效值。
- en: Note
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The .isnull() method can be used interchangeably with the .isna() method for
    pandas DataFrames. Both these methods do exactly the same thing—the reason there
    are two methods to do the same thing is pandas DataFrames were originally based
    on R DataFrames, and hence have reproduced much of the syntax and ideas of the
    latter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`.isnull()` 方法与 `.isna()` 方法可以互换使用，在 pandas DataFrame 中，两者做的事情完全相同——之所以有两个方法做同一件事，是因为
    pandas DataFrame 最初是基于 R DataFrame 开发的，因此复用了很多 R DataFrame 的语法和思想。'
- en: 'It may not be immediately obvious whether the missing data is random or not.
    Discovering the nature of missing values across features in a dataset is possible
    through two common visualization techniques:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据是否随机可能一开始并不明显。通过两种常见的可视化技术，我们可以发现数据集中特征的缺失值性质：
- en: 'Nullity matrix: This is a data-dense display that lets us quickly visualize
    the patterns in data completion. It gives us a quick glance at how the null values
    within a feature (and across features) are distributed, how many there are, and
    how often they appear with other features.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失矩阵：这是一种数据密集型的显示方式，可以让我们快速可视化数据完成情况的模式。它可以帮助我们快速查看一个特征（以及多个特征）中的缺失值是如何分布的，缺失值的数量是多少，以及它们与其他特征的关联频率。
- en: 'Nullity-correlation heatmap: This heatmap visually describes the nullity relationship
    (or a data completeness relationship) between each pair of features; that is,
    it measures how strongly the presence or absence of one variable affects the presence
    of another.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 空值相关性热图：该热图直观地描述了每对特征之间的空值关系（或数据完整性关系）；即它衡量了一个变量的存在或缺失如何影响另一个变量的存在。
- en: Akin to regular correlation, nullity correlation values range from -1 to 1,
    the former indicating that one variable appears when the other definitely does
    not, and the latter indicating the simultaneous presence of both variables. A
    value of 0 implies that one variable having a null value has no effect on the
    other being null.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于常规的相关性分析，空值相关性值的范围从 -1 到 1，前者表示一个变量出现时，另一个变量肯定不出现，后者则表示两个变量同时存在。值为 0 表示一个变量的空值对另一个变量的空值没有影响。
- en: 'Exercise 2.02: Visualizing Missing Values'
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.02：可视化缺失值
- en: Let's analyze the nature of the missing values by first looking at the count
    and percentage of missing values for each feature, and then plotting a nullity
    matrix and correlation heatmap using the missingno library in Python. We will
    be using the same dataset from the previous exercises.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过查看每个特征的缺失值数量和百分比，来分析缺失值的性质，然后使用 Python 中的 missingno 库绘制空值矩阵和相关性热图。我们将继续使用前面练习中的相同数据集。
- en: 'Please note that this exercise is a continuation of Exercise 2.01: Summarizing
    the Statistics of Our Dataset.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本练习是练习 2.01：总结我们的数据集统计信息的延续。
- en: 'The following steps will help you complete this exercise to visualize the missing
    values in the dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习，以可视化数据集中的缺失值：
- en: Calculate the count and percentage of missing values in each column and arrange
    these in decreasing order. We will use the .isnull() function on the DataFrame
    to get a mask. The count of null values in each column can then be found using
    the .sum() function over the DataFrame mask. Similarly, the fraction of null values
    can be found using .mean() over the DataFrame mask and multiplied by 100 to convert
    it to a percentage.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每列缺失值的数量和百分比，并按降序排列。我们将使用 `.isnull()` 函数对 DataFrame 进行掩码操作。然后，可以使用 `.sum()`
    函数计算每列的空值数量。类似地，空值的比例可以通过对 DataFrame 掩码使用 `.mean()` 并乘以 100 来转换为百分比。
- en: 'Then, we combine the total and percentage of null values into a single DataFrame
    using the pd.concat() function, and subsequently sort the rows by percentage of
    missing values and print the DataFrame:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 `pd.concat()` 函数将空值的总数和百分比合并到一个 DataFrame 中，接着按照缺失值的百分比对行进行排序，并打印出该
    DataFrame：
- en: mask = data.isnull()
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: mask = data.isnull()
- en: total = mask.sum()
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: total = mask.sum()
- en: percent = 100*mask.mean()
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: percent = 100*mask.mean()
- en: missing_data = pd.concat([total, percent], axis=1,join='outer', \
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: missing_data = pd.concat([total, percent], axis=1,join='outer', \
- en: keys=['count_missing', 'perc_missing'])
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: keys=['count_missing', 'perc_missing'])
- en: missing_data.sort_values(by='perc_missing', ascending=False, \
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: missing_data.sort_values(by='perc_missing', ascending=False, \
- en: inplace=True)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: inplace=True)
- en: missing_data
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: missing_data
- en: 'The output will be as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.5: The count and percentage of missing values in each column'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5：每列缺失值的数量和百分比'
- en: '](img/image-Z70NA5T0.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-Z70NA5T0.jpg)'
- en: 'Figure 2.5: The count and percentage of missing values in each column'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：每列缺失值的数量和百分比
- en: Here, we can see that the state, total_damage_millions_dollars, and damage_millions_dollars
    columns have over 90% missing values, which means that data for fewer than 10%
    of the data points in the dataset are available for these columns. On the other
    hand, year, flag_tsunami, country, and region_code have no missing values.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到 state、total_damage_millions_dollars 和 damage_millions_dollars 列的缺失值超过
    90%，意味着数据集中这些列的可用数据点不到 10%。另一方面，year、flag_tsunami、country 和 region_code 列没有缺失值。
- en: 'Plot the nullity matrix. First, we find the list of columns that have any null
    values in them using the .any() function on the DataFrame mask from the previous
    step. Then, we use the missingno library to plot the nullity matrix for a random
    sample of 500 data points from our dataset, for only those columns that have missing
    values:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制空值矩阵。首先，我们使用 `.any()` 函数从前一步骤的 DataFrame mask 中找出包含空值的列。然后，我们使用 missingno
    库绘制一个空值矩阵，针对数据集中仅包含缺失值的列，从 500 条随机数据中取样：
- en: nullable_columns = data.columns[mask.any()].tolist()
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: nullable_columns = data.columns[mask.any()].tolist()
- en: msno.matrix(data[nullable_columns].sample(500))
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: msno.matrix(data[nullable_columns].sample(500))
- en: plt.show()
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.6: The nullity matrix'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6：缺失值矩阵'
- en: '](img/image-G34KOPOB.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-G34KOPOB.jpg)'
- en: 'Figure 2.6: The nullity matrix'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：缺失值矩阵
- en: Here, black lines represent non-nullity while the white lines indicate the presence
    of a null value in that column. At a glance, location_name appears to be completely
    populated (we know from the previous step that there is, in fact, only one missing
    value in this column), while latitude and longitude seem mostly complete, but
    spottier.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，黑色线条表示非空值，而白色线条表示该列中存在缺失值。总体来看，location_name列似乎完全填充（我们从前面的步骤中得知，这一列实际上只有一个缺失值），而latitude和longitude列似乎大部分完整，但有些地方较为稀疏。
- en: The spark line on the right summarizes the general shape of the data completeness
    and points out the rows with the maximum and minimum nullity in the dataset. Note
    that this is only for the sample of 500 points.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的火花图总结了数据完整性的整体形态，并指出了数据集中最大和最小缺失值所在的行。请注意，这仅适用于500个数据点的样本。
- en: 'Plot the nullity correlation heatmap. We will plot the nullity correlation
    heatmap using the missingno library for our dataset, for only those columns that
    have missing values:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制缺失值相关性热图。我们将使用missingno库绘制缺失值相关性热图，仅针对那些存在缺失值的列：
- en: msno.heatmap(data[nullable_columns], figsize=(18,18))
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: msno.heatmap(data[nullable_columns], figsize=(18,18))
- en: plt.show()
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.7: The nullity correlation heatmap'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.7：缺失值相关性热图'
- en: '](img/image-UT14Y4SX.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-UT14Y4SX.jpg)'
- en: 'Figure 2.7: The nullity correlation heatmap'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：缺失值相关性热图
- en: 'Here, we can also see some boxes labeled <1: this just means that the correlation
    values in those cases are all close to 1.0, but still not quite perfectly so.
    We can see a value of <1 between injuries and total_injuries, which means that
    the missing values in each category are correlated. We would need to dig deeper
    to understand whether the missing values are correlated because they are based
    upon the same or similar information, or for some other reason.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还可以看到一些标有<1的框：这仅意味着这些情况下的相关性值都接近1.0，但仍然不是完全一致。我们可以看到在injuries和total_injuries之间有<1的值，这意味着每个类别中的缺失值是相关的。我们需要深入挖掘，了解这些缺失值是否因为基于相同或类似的信息而相关，或者是出于其他原因。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2YSXq3k.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考https://packt.live/2YSXq3k。
- en: You can also run this example online at https://packt.live/2Yn3Us7\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在https://packt.live/2Yn3Us7上在线运行此示例。您必须执行整个Notebook才能获得所需的结果。
- en: Imputation Strategies for Missing Values
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺失值的填充策略
- en: 'There are multiple ways of dealing with missing values in a column. The simplest
    way is to simply delete rows having missing values; however, this can result in
    the loss of valuable information from other columns. Another option is to impute
    the data, that is, replace the missing values with a valid value inferred from
    the known part of the data. The common ways in which this can be done are listed
    here:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 处理列中缺失值有多种方法。最简单的方法是直接删除包含缺失值的行；然而，这可能会导致丧失其他列中的有价值信息。另一种选择是对数据进行填充，即用从已知数据部分推断出的有效值替换缺失值。常见的填充方法如下：
- en: Create a new value that is distinct from the other values to replace the missing
    values in the column so as to differentiate those rows altogether. Then, use a
    non-linear machine learning algorithm (such as ensemble models or support vectors)
    that can separate the values out.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个与其他值不同的新值，以替换列中的缺失值，从而区分这些行。然后，使用非线性机器学习算法（如集成模型或支持向量机），将这些值分离出来。
- en: Use an appropriate central value from the column (mean, median, or mode) to
    replace the missing values.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列中的适当中心值（均值、中位数或众数）来替换缺失值。
- en: Use a model (such as a K-nearest neighbors or a Gaussian mixture model) to learn
    the best value with which to replace the missing values.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型（例如K近邻或高斯混合模型）来学习最佳值，以替换缺失值。
- en: 'Python has a few functions that are useful for replacing null values in a column
    with a static value. One way to do this is to use the inherent pandas .fillna(0)
    function: there is no ambiguity in imputation here—the static value with which
    to substitute the null data point in the column is the argument being passed to
    the function (the value in the brackets).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Python有一些函数可以用来将列中的空值替换为静态值。实现这一点的一种方式是使用pandas本身的.fillna(0)函数：这里没有歧义——替换空数据点的静态值就是传递给函数的参数（括号中的值）。
- en: 'However, if the number of null values in a column is significant and it''s
    not immediately obvious what the appropriate central value is that can be used
    to replace each null value, then we can either delete the rows having null values
    or delete the column altogether from the modeling perspective, as it may not add
    any significant value. This can be done by using the .dropna() function on the
    DataFrame. The parameters that can be passed to the function are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果列中空值的数量较大，并且无法立即确定可以用来替换每个空值的合适中心值，那么我们可以从建模的角度出发，选择删除含有空值的行，或者直接删除整个列，因为它可能不会增加任何重要的价值。这可以通过在DataFrame上使用.dropna()函数来完成。可以传递给该函数的参数如下：
- en: 'axis: This defines whether to drop rows or columns, which is determined by
    assigning the parameter a value of 0 or 1, respectively.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 'axis: 这定义了是删除行还是列，具体取决于为参数分配0或1的值。'
- en: 'how: A value of all or any can be assigned to this parameter to indicate whether
    the row/column should contain all null values to drop the column, or whether to
    drop the column if there is at least one null value.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'how: 可以为此参数分配“all”或“any”值，表示行/列是否应包含所有空值才能删除该列，或者是否应在至少有一个空值时删除该列。'
- en: 'thresh: This defines the minimum number of null values the row/column should
    have in order to be dropped.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 'thresh: 这定义了行/列应具有的最小空值数量，才会被删除。'
- en: Additionally, if an appropriate replacement for a null value for a categorical
    feature cannot be determined, a possible alternative to deleting the column is
    to create a new category in the feature that can represent the null values.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果无法确定一个适当的替代值来填充分类特征的空值，那么删除该列的一个可能替代方案是为该特征创建一个新的类别，用来表示空值。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: If it is immediately obvious how a null value for a column can be replaced from
    an intuitive understanding or domain knowledge, then we can replace the value
    on the spot. Keep in mind that any such data changes should be made in your code
    and never directly on the raw data. One reason for this is that it allows the
    strategy to be updated easily in the future. Another reason is that it makes it
    visible to others who may later be reviewing the work where changes were made.
    Directly changing raw data can lead to data versioning problems and make it impossible
    for others to reproduce your work. In many cases, inferences become more obvious
    at later stages in the exploration process. In these cases, we can substitute
    null values as and when we find an appropriate way to do so.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以通过直观理解或领域知识立即得出一个适合的值来替换列中的空值，那么我们可以当场进行替换。请记住，任何此类数据更改应在代码中进行，而绝不是直接在原始数据上操作。这样做的一个原因是它使得将来可以轻松地更新策略；另一个原因是它使得其他人如果以后审查这项工作时，可以清楚地看到在哪里进行了更改。直接更改原始数据可能会导致数据版本控制问题，并使得其他人无法重现你的工作。在很多情况下，推断会在后续的探索阶段变得更加明显。在这种情况下，我们可以在找到合适的方法时，随时替换空值。
- en: 'Exercise 2.03: Performing Imputation Using Pandas'
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习2.03：使用Pandas进行填充
- en: 'Let''s look at missing values and replace them with zeros in time-based (continuous)
    features having at least one null value (month, day, hour, minute, and second).
    We do this because, for cases where we do not have recorded values, it would be
    safe to assume that the events take place at the beginning of the time duration.
    This exercise is a continuation of Exercise 2.02: Visualizing Missing Values:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下缺失值，并在具有至少一个空值的基于时间（连续）的特征中将它们替换为零（如月份、日期、小时、分钟和秒）。我们这么做是因为对于那些没有记录值的情况，可以安全地假设事件发生在时间段的开始。这项操作是练习2.02：可视化缺失值的延续：
- en: 'Create a list containing the names of the columns whose values we want to impute:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含我们想要填充的列名的列表：
- en: time_features = ['month', 'day', 'hour', 'minute', 'second']
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: time_features = ['month', 'day', 'hour', 'minute', 'second']
- en: 'Impute the null values using .fillna(). We will replace the missing values
    in these columns with 0 using the inherent pandas .fillna() function and pass
    0 as an argument to the function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: data[time_features] = data[time_features].fillna(0)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the .info() function to view null value counts for the imputed columns:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: data[time_features].info()
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: Null value counts'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-Z1ZGRYK5.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8: Null value counts'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: As we can now see, all values for our features in the DataFrame are now non-null.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2V9nMx3.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/2BqoZZM. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.04: Performing Imputation Using Scikit-Learn'
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this exercise, you will replace the null values in the description-related
    categorical features using scikit-learn''s SimpleImputer class. In Exercise 2.02:
    Visualizing Missing Values, we saw that almost all of these features comprised
    more than 50% of null values in the data. Replacing these null values with a central
    value might bias any model we try to build using the features, deeming them irrelevant.
    Let''s instead replace the null values with a separate category, having the value
    NA. This exercise is a continuation of Exercise 2.02: Visualizing Missing Values:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a list containing the names of the columns whose values we want to impute:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: description_features = ['injuries_description', \
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '''damage_description'', \'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '''total_injuries_description'', \'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '''total_damage_description'']'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an object of the SimpleImputer class. Here, we first create an imp object
    of the SimpleImputer class and initialize it with parameters that represent how
    we want to impute the data. The parameters we will pass to initialize the object
    are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'missing_values: This is the placeholder for the missing values, that is, all
    occurrences of the values in the missing_values parameter will be imputed.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'strategy: This is the imputation strategy, which can be one of mean, median,
    most_frequent (that is, the mode), or constant. While the first three can only
    be used with numeric data and will replace missing values using the specified
    central value along each column, the last one will replace missing values with
    a constant as per the fill_value parameter.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'fill_value: This specifies the value with which to replace all occurrences
    of missing_values. If left to the default, the imputed value will be 0 when imputing
    numerical data and the missing_value string for strings or object data types:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: imp = SimpleImputer(missing_values=np.nan, \
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: strategy='constant', \
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: fill_value='NA')
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the imputation. We will use imp.fit_transform() to actually perform
    the imputation. It takes the DataFrame with null values as input and returns the
    imputed DataFrame:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: data[description_features] = \
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: imp.fit_transform(data[description_features])
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the .info() function to view null value counts for the imputed columns:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: data[description_features].info()
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: The null value counts'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-PSVVIOTZ.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.9: The null value counts'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/3ervLgk.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/3doEX3G. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In the last two exercises, we looked at two ways to use pandas and scikit-learn
    methods to impute missing values. These methods are very basic methods we can
    use if we have little or no information about the underlying data. Next, we'll
    look at more advanced techniques we can use to fill in missing data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.05: Performing Imputation Using Inferred Values'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s replace the null values in the continuous damage_millions_dollars feature
    with information from the categorical damage_description feature. Although we
    may not know the exact dollar amount that was incurred, the categorical feature
    gives us information on the range of the amount that was incurred due to damage
    from the earthquake. This exercise is a continuation of Exercise 2.04: Performing
    Imputation Using scikit-learn:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Find how many rows have null damage_millions_dollars values, and how many of
    those have non-null damage_description values:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: print(data[pd.isnull(data.damage_millions_dollars)].shape[0])
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: print(data[pd.isnull(data.damage_millions_dollars) \
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '& (data.damage_description != ''NA'')].shape[0])'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '5594'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '3849'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, 3,849 of 5,594 null values can be easily substituted with the
    help of another variable. For example, we know that all variables having column
    names ending with _description are a descriptor field containing estimates for
    data that may not be available in the original numerical column. For deaths, injuries,
    and total_injuries, the corresponding categorical values represent the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 0 = None
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 1 = Few (~1 to 50 deaths)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 2 = Some (~51 to 100 deaths)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 3 = Many (~101 to 1,000 deaths)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 4 = Very Many (~1,001 or more deaths)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'As regards damage_millions_dollars, the corresponding categorical values represent
    the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 0 = None
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 1 = Limited (roughly corresponding to less than 1 million dollars)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 2 = Moderate (~1 to 5 million dollars)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 3 = Severe (~>5 to 24 million dollars)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 4 = Extreme (~25 million dollars or more)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the mean damage_millions_dollars value for each category. Since each of
    the categories in damage_description represents a range of values, we find the
    mean damage_millions_dollars value for each category from the non-null values
    already available. These provide a reasonable estimate for the most likely value
    for that category:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: category_means = data[['damage_description', \
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '''damage_millions_dollars'']]\'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: .groupby('damage_description').mean()
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: category_means
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10: The mean damage_millions_dollars value for each category'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-QMB0CEWJ.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.10: The mean damage_millions_dollars value for each category'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the first three values make intuitive sense given the preceding definitions:
    0.42 is between 0 and 1, 3.1 is between 1 and 5, and 13.8 is between 5 and 24\.
    The last category is defined as 25 million or more; it transpires that the mean
    of these extreme cases is very high (3,575!).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Store the mean values as a dictionary. In this step, we will convert the DataFrame
    containing the mean values to a dictionary (a Python dict object), so that accessing
    them is convenient.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, since the value for the newly created NA category (the imputed
    value in the previous exercise) was NaN, and the value for the 0 category was
    absent (no rows had damage_description equal to 0 in the dataset), we explicitly
    added these values to the dictionary as well:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: replacement_values = category_means\
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: .damage_millions_dollars.to_dict()
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: replacement_values['NA'] = -1
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: replacement_values['0'] = 0
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: replacement_values
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11: The dictionary of mean values'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-M7BYA4VU.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.11: The dictionary of mean values'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a series of replacement values. For each value in the damage_description
    column, we map the categorical value onto the mean value using the map function.
    The .map() function is used to map the keys in the column to the corresponding
    values for each element from the replacement_values dictionary:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: imputed_values = data.damage_description.map(replacement_values)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Replace null values in the column. We do this by using np.where as a ternary
    operator: the first argument is the mask, the second is the series from which
    to take the value if the mask is positive, and the third is the series from which
    to take the value if the mask is negative.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'This ensures that the array returned by np.where only replaces the null values
    in damage_millions_dollars with values from the imputed_values series:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: data['damage_millions_dollars'] = \
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: np.where(data.damage_millions_dollars.isnull(), \
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: data.damage_description.map(replacement_values), \
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: data.damage_millions_dollars)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the .info() function to view null value counts for the imputed columns:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: data[['damage_millions_dollars']].info()
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: The null value counts'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-H5UW1GNA.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12: The null value counts'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: We can see that, after replacement, there are no null values in the damage_millions_dollars
    column.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/3fMRqQo.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/2YkBgYC. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have looked at replacing missing values in more than one
    way. In one case, we replaced values with zeros; in another case, we looked at
    more information about the dataset to reason that we could replace missing values
    with a combination of information from a descriptive field and the means of values
    we did have. These sorts of decisions and steps are extremely common when working
    with real data. We also noted that, occasionally, when we have sufficient data
    and the instances with missing values are few, we can just drop them. In the following
    activity, we'll use a different dataset for you to practice and reinforce these
    methods.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2.01: Summary Statistics and Missing Values'
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this activity, we''ll revise some of the summary statistics and missing
    value exploration we have looked at thus far in this chapter. We will be using
    a new dataset, House Prices: Advanced Regression Techniques, available on Kaggle.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The original dataset is available at https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
    or on our GitHub repository at https://packt.live/2TjU9aj.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: While the Earthquakes dataset used in the exercises is aimed at solving a classification
    problem (when the target variable has only discrete values), the dataset we will
    use in the activities will be aimed at solving a regression problem (when the
    target variable takes on a range of continuous values). We will use pandas functions
    to generate summary statistics and visualize missing values using a nullity matrix
    and nullity correlation heatmap.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Read the data (house_prices.csv).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Use pandas' .info() and .describe() methods to view the summary statistics of
    the dataset.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the info() method will be as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13: The output of the info() method (abbreviated)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-Q9SXD9EK.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.13: The output of the info() method (abbreviated)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the describe() method will be as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14: The output of the describe() method (abbreviated)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-99VP2HBT.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.14: The output of the describe() method (abbreviated)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The outputs of the info() and describe() methods have been truncated for presentation
    purposes. You can find the outputs in their entirety here: https://packt.live/2TjZSgi'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Find the total count and total percentage of missing values in each column of
    the DataFrame and display them for columns having at least one null value, in
    descending order of missing percentages.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Plot the nullity matrix and nullity correlation heatmap.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'The nullity matrix will be as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15: Nullity matrix'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-TTRK4BON.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.15: Nullity matrix'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'The nullity correlation heatmap will be as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16: Nullity correlation heatmap'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-QT4NJDMU.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.16: Nullity correlation heatmap'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Delete the columns having more than 80% of their values missing.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Replace null values in the FireplaceQu column with NA values.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found via this link.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: You should now be comfortable using the approaches we've learned to investigate
    missing values in any type of tabular data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Distribution of Values
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we'll look at how individual variables behave—what kind of
    values they take, what the distribution across those values is, and how those
    distributions can be represented visually.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Target Variable
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The target variable can either have values that are continuous (in the case
    of a regression problem) or discrete (as in the case of a classification problem).
    The problem statement we're looking at in this chapter involves predicting whether
    an earthquake caused a tsunami, that is, the flag_tsunami variable, which takes
    on two discrete values only—making it a classification problem.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: One way of visualizing how many earthquakes resulted in tsunamis and how many
    didn't involves the use of a bar chart, where each bar represents a single discrete
    value of the variable, and the height of the bars is equal to the count of the
    data points having the corresponding discrete value. This gives us a good comparison
    of the absolute counts of each category.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.06: Plotting a Bar Chart'
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s look at how many of the earthquakes in our dataset resulted in a tsunami.
    We will do this by using the value_counts() method over the column and using the
    .plot(kind=''bar'') function directly on the returned pandas series. This exercise
    is a continuation of Exercise 2.05: Performing Imputation Using Inferred Values:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'Use plt.figure() to initiate the plotting:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: plt.figure(figsize=(8,6))
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, type in our primary plotting command:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: data.flag_tsunami.value_counts().plot(kind='bar', \
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: color = ('grey', \
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '''black''))'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the display parameters and display the plot:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel('Number of data points')
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel('flag_tsunami')
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17: Bar chart showing how many earthquakes resulted in a tsunami'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-2DJC2J7X.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.17: Bar chart showing how many earthquakes resulted in a tsunami'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: From this bar plot, we can see that most of the earthquakes did not result in
    tsunamis and that fewer than one-third of the earthquakes actually did. This shows
    us that the dataset is slightly imbalanced.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2Yn4UfR.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/37QvoJI. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look more closely at what these Matplotlib commands do:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'plt.figure(figsize=(8,6)): This command defines how big our plot should be,
    by providing width and height values. This is always the first command before
    any plotting command is written.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'plt.xlabel() and plt.ylabel(): These commands take a string as input and allow
    us to specify what the labels for the X and Y axes on the plot should be.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'plt.show(): This is the final command that is written when plotting a visualization
    that displays the plot inline within the Jupyter notebook.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Categorical Data
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Categorical variables are ones that take discrete values representing different
    categories or levels of observation that can either be string objects or integer
    values. For example, our target variable, flag_tsunami, is a categorical variable
    with two categories, Tsu and No.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorical variables can be of two types:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Nominal variables: Variables in which the categories are labeled without any
    order of precedence are called nominal variables. An example of a nominal variable
    from our dataset would be location_name. The values that this variable takes cannot
    be said to be ordered, that is, one location is not greater than the other. Similarly,
    more examples of such a variable would be color, types of footwear, ethnicity
    type, and so on.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Ordinal variables: Variables that have some order associated with them are
    called ordinal variables. An example from our dataset would be damage_description
    since each value represents an increasing value of damage incurred. Another example
    could be days of the week, which would have values from Monday to Sunday, which
    have some order associated with them and we know that Thursday comes after Wednesday
    but before Friday.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Although ordinal variables can be represented by object data types, they are
    often represented as numerical data types as well, often making it difficult to
    differentiate between them and continuous variables.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: One of the major challenges faced when dealing with categorical variables in
    a dataset is high cardinality, that is, a large number of categories or distinct
    values with each value appearing a relatively small number of times. For example,
    location_name has a large number of unique values, with each value occurring a
    small number of times in the dataset.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, non-numerical categorical variables will always require some form
    of preprocessing to be converted into a numerical format so that they can be ingested
    for training by a machine learning model. It can be a challenge to encode categorical
    variables numerically without losing out on contextual information that, despite
    being easy for humans to interpret (due to domain knowledge or otherwise just
    plain common sense), would be hard for a computer to automatically understand.
    For example, a geographical feature such as country or location name by itself
    would give no indication of the geographical proximity of different values, but
    that might just be an important feature—what if earthquakes that occur at locations
    in South East Asia trigger more tsunamis than those that occur in Europe? There
    would be no way of capturing that information by merely encoding the feature numerically.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.07: Identifying Data Types for Categorical Variables'
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s establish which variables in our Earthquake dataset are categorical
    and which are continuous. As we now know, categorical variables can also have
    numerical values, so having a numeric data type doesn''t guarantee that a variable
    is continuous. This exercise is a continuation of Exercise 2.05: Performing Imputation
    Using Inferred Values:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 'Find all the columns that are numerical and object types. We use the .select_dtypes()
    method on the DataFrame to create a subset DataFrame having numeric (np.number)
    and categorical (np.object) columns, and then print the column names for each.
    For numeric columns, use this command:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: numeric_variables = data.select_dtypes(include=[np.number])
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: numeric_variables.columns
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18: All columns that are numerical'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-76AZUMB9.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.18: All columns that are numerical'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'For categorical columns, use this command:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: object_variables = data.select_dtypes(include=[np.object])
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: object_variables.columns
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19: All columns that are object types'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-81ULCRD2.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.19: All columns that are object types'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Here, it is evident that the columns that are object types are categorical variables.
    To differentiate between the categorical and continuous variables from the numeric
    columns, let's see how many unique values there are for each of these features.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the number of unique values for numeric features. We use the select_dtypes
    method on the DataFrame to find the number of unique values in each column and
    sort the resulting series in ascending order. For numeric columns, use this command:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: numeric_variables.nunique().sort_values()
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20: Number of unique values for numeric features'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-AUNV0ACJ.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.20: Number of unique values for numeric features'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'For categorical columns, use this command:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: object_variables.nunique().sort_values()
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21: Number of unique values for categorical columns'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-YC1T58GJ.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.21: Number of unique values for categorical columns'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2YlSmFt.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/31hnuIr. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: For the numeric variables, we can see that the top nine have significantly fewer
    unique values than the remaining rows, and it's likely that these are categorical
    variables. However, we must keep in mind that it is possible that some of them
    might just be continuous variables with a low range of rounded-up values. Also,
    month and day would not be considered categorical variables here.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.08: Calculating Category Value Counts'
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For columns with categorical values, it would be useful to see what the unique
    values (categories) of the feature are, along with what the frequencies of these
    categories are, that is, how often does each distinct value occur in the dataset.
    Let''s find the number of occurrences of each 0 to 4 label and NaN values for
    the injuries_description categorical variable. This exercise is a continuation
    of Exercise 2.07: Identifying Data Types for Categorical Variables:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the value_counts() function on the injuries_description column to find
    the frequency of each category. Using value_counts gives us the frequencies of
    each value in decreasing order in the form of a pandas series:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: counts = data.injuries_description.value_counts(dropna=False)
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: counts
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'The output should be as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.22: Frequency of each category'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-UW9LQJF8.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.22: Frequency of each category'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Sort the values in increasing order of the ordinal variable. If we want the
    frequencies in the order of the values themselves, we can reset the index to give
    us a DataFrame and sort values by the index (that is, the ordinal variable):'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: counts.reset_index().sort_values(by='index')
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23: Sorted values'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-TGI2TTM4.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.23: Sorted values'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2Yn5URj.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/314dYIr. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.09: Plotting a Pie Chart'
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since our target variable in our sample data is categorical, the example in
    Exercise 2.06: Plotting a Bar Chart, showed us one way of visualizing how the
    categorical values are distributed (using a bar chart). Another plot that can
    make it easy to see how each category functions as a fraction of the overall dataset
    is a pie chart. Let''s plot a pie chart to visualize the distribution of the discrete
    values of the damage_description variable. This exercise is a continuation of
    Exercise 2.08, Calculating Category Value Counts:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'Format the data into the form that needs to be plotted. Here, we run value_counts()
    over the column and sort the series by index:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: counts = data.damage_description.value_counts()
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: counts = counts.sort_index()
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the pie chart. The plt.pie() category plots the pie chart using the count
    data. We will use the same three steps for plotting as described in Exercise 2.06:
    Plotting a Bar Chart:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: fig, ax = plt.subplots(figsize=(10,10))
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: slices = ax.pie(counts, \
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: labels=counts.index, \
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: colors = ['white'], \
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'wedgeprops = {''edgecolor'': ''black''})'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: patches = slices[0]
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: hatches = ['/', '\\', '|', '-', '+', 'x', 'o', 'O', '\.', '*']
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'for patch in range(len(patches)):'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: patches[patch].set_hatch(hatches[patch])
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: plt.title('Pie chart showing counts for\ndamage_description '\
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '''categories'')'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.24: Pie chart showing counts for damage_description categories'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-5PCI6LVT.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.24: Pie chart showing counts for damage_description categories'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-405
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/37Ovj9s.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/37OvotM. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.24 tells us the relative number of items in each of the five damage
    description categories. Note that it would be good practice to do the extra work
    to change the uninformative labels to the categories—recall from the EDA discussion
    that:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 0 = NONE
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 1 = LIMITED (roughly corresponding to less than $1 million)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 2 = MODERATE (~$1 to $5 million)
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 3 = SEVERE (~>$5 to $24 million)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 4 = EXTREME (~$25 million or more)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: In addition, while the pie chart gives us a quick visual impression of which
    are the largest and smallest categories, we get no idea of the actual quantities,
    so adding those labels would increase the value of the chart. You can use the
    code in the repository for this book to update the chart.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Data
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Continuous variables can take any number of values and are usually integer
    (for example, number of deaths) or float data types (for example, the height of
    a mountain). It''s useful to get an idea of the basic statistics of the values
    in the feature: the minimum, maximum, and percentile values we see from the output
    of the describe() function gives us a fair estimate of this.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: However, for continuous variables, it is also very useful to see how the values
    are distributed in the range they operate in. Since we cannot simply find the
    counts of individual values, instead, we order the values in ascending order,
    group them into evenly-sized intervals, and find the counts for each interval.
    This gives us the underlying frequency distribution and plotting this gives us
    a histogram, which allows us to examine the shape, central values, and amount
    of variability in the data.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Histograms give us an easy view of the data that we're looking at. They tell
    us about the behavior of the values at a glance in terms of the underlying distribution
    (for example, a normal or exponential distribution), the presence of outliers,
    skewness, and more.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is easy to get confused between a bar chart and a histogram. The major difference
    is that a histogram is used to plot continuous data that has been binned to visualize
    the frequency distribution, while bar charts can be used for a variety of other
    use cases, including to represent categorical variables as we have done. Additionally,
    with histograms, the number of bins is something we can vary, so the range of
    values in a bin is determined by the number of bins, as is the height of the bars
    in the histogram. In a bar chart, the width of the bars does not generally convey
    meaning, and the height is usually a property of the category, like a count.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common frequency distributions is a Gaussian (or normal) distribution.
    This is a symmetric distribution that has a bell-shaped curve, which indicates
    that the values near the middle of the range have the highest occurrences in the
    dataset with a symmetrically decreasing frequency of occurrences as we move away
    from the middle. You almost certainly have seen examples of Gaussian distributions,
    because many natural and man-made processes generate values that vary nearly like
    the Gaussian distribution. Thus, it is extremely common to see data compared to
    the Gaussian distribution.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a probability distribution and the area under the curve equals one, as
    shown in Figure 2.25:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.25: Gaussian (normal) distribution'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-5FZD4VW6.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.25: Gaussian (normal) distribution'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: A symmetric distribution like normal distribution can be characterized entirely
    by two parameters—the mean (µ) and the standard deviation (σ). In Figure 2.25,
    the mean is at 7.5, for example. However, there are significant amounts of real
    data that do not follow a normal distribution and may be asymmetric. The asymmetry
    of data is often referred to as a skew.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Skewness
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A distribution is said to be skewed if it is not symmetric in nature, and skewness
    measures the asymmetry of a variable about its mean. The value can be positive
    or negative (or undefined). In the former case, the tail is on the right-hand
    side of the distribution, while the latter indicates that the tail is on the left-hand
    side.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: However, it must be noted that a thick and short tail would have the same effect
    on the value of skewness as a long, thin tail.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Kurtosis
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kurtosis is a measure of the tailedness of the distribution of a variable and
    is used to measure the presence of outliers in one tail versus the other. A high
    value of kurtosis indicates a fatter tail and the presence of outliers. In a similar
    way to the concept of skewness, kurtosis also describes the shape of the distribution.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.10: Plotting a Histogram'
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s plot the histogram for the eq_primary feature using the Seaborn library.
    This exercise is a continuation of Exercise 2.09, Plotting a Pie Chart:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'Use plt.figure() to initiate the plotting:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: plt.figure(figsize=(10,7))
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'sns.distplot() is the primary command that we will use to plot the histogram.
    The first parameter is the one-dimensional data over which to plot the histogram,
    while the bins parameter defines the number and size of the bins. Use this as
    follows:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: sns.distplot(data.eq_primary.dropna(), \
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: bins=np.linspace(0,10,21))
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: 'Display the plot using plt.show():'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.26: Histogram for the example primary feature'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-DME453BH.jpg)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.26: Histogram for the example primary feature'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: The plot gives us a normed (or normalized) histogram, which means that the area
    under the bars of the histogram equals unity. Additionally, the line over the
    histogram is the kernel density estimate, which gives us an idea of what the probability
    distribution for the variable would look like.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2BwZrdj.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/3fMSxj2\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: From the plot, we can see that the values of eq_primary lie mostly between 5
    and 8, which means that most earthquakes had a magnitude with a moderate to high
    value, with barely any earthquakes having a low or very high magnitude.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.11: Computing Skew and Kurtosis'
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s calculate the skew and kurtosis values for all of the features in the
    dataset using the core pandas functions available to us. This exercise is a continuation
    of Exercise 2.10, Plotting a Histogram:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the .skew() DataFrame method to calculate the skew for all features and
    then sort the values in ascending order:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: data.skew().sort_values()
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.27: Skew values for all the features in the dataset'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-V0JJI54Q.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.27: Skew values for all the features in the dataset'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the .kurt() DataFrame method to calculate the kurtosis for all features:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: data.kurt()
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.28: Kurtosis values for all the features in the dataset'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-WUUXAE1Y.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.28: Kurtosis values for all the features in the dataset'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the kurtosis values for some variables deviate significantly
    from 0\. This means that these columns have a long tail. But the values that are
    at the tail end of these variables (which indicate the number of people dead,
    injured, and the monetary value of damage), in our case, may be outliers that
    we may need to pay special attention to. Larger values might, in fact, indicate
    an additional force that added to the devastation caused by an earthquake, that
    is, a tsunami.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2Yklmh0.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/37PcMdj. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2.02: Representing the Distribution of Values Visually'
  id: totrans-468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this activity, we will implement what we learned in the previous section
    by creating different plots such as histograms and pie charts. Furthermore, we
    will calculate the skew and kurtosis for the features of the dataset. Here, will
    use the same dataset we used in Activity 2.01: Summary Statistics and Missing
    Values, that is, House Prices: Advanced Regression Techniques. We''ll use different
    types of plots to visually represent the distribution of values for this dataset.
    This activity is a continuation of Activity 2.01: Summary Statistics and Missing
    Values:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: Plot a histogram using Matplotlib for the target variable, SalePrice.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.29: Histogram for the target variable'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-5YLC5971.jpg)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.29: Histogram for the target variable'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: Find the number of unique values within each column having an object type.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: Create a DataFrame representing the number of occurrences for each categorical
    value in the HouseStyle column.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Plot a pie chart representing these counts.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.30: Pie chart representing the counts'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-IYKYADF1.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.30: Pie chart representing the counts'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Find the number of unique values within each column having a number type.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Plot a histogram using seaborn for the LotArea variable.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.31: Histogram for the LotArea variable'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-JBM6D62T.jpg)'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.31: Histogram for the LotArea variable'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the skew and kurtosis values for the values in each column.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for skew values will be:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.32: Skew values for each column'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-8QWHUC43.jpg)'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.32: Skew values for each column'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for kurtosis values will be:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.33: Kurtosis values for each column'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-XPECYU25.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.33: Kurtosis values for each column'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found via this link.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to look into the nature of data in more detail, in particular,
    by beginning to understand the distribution of the data using histograms or density
    plots, relative counts of data using pie charts, as well as inspecting the skew
    and kurtosis of the variables as a first step to finding potentially problematic
    data, outliers, and so on.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should have a comfort level handling various statistical measures
    of data such as summary statistics, counts, and the distribution of values. Using
    tools such as histograms and density plots, you can explore the shape of datasets,
    and augment that understanding by calculating statistics such as skew and kurtosis.
    You should be developing some intuition for some flags that warrant further investigation,
    such as large skew or kurtosis values.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Relationships within the Data
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two reasons why it is important to find relationships between variables
    in the data:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: Establishing which features are potentially important can be deemed essential,
    since finding ones that have a strong relationship with the target variable will
    aid in the feature selection process.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Finding relationships between different features themselves can be useful since
    variables in the dataset are usually never completely independent of every other
    variable and this can affect our modeling in a number of ways.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: Now, there are a number of ways in which we can visualize these relationships,
    and this really depends on the types of variable we are trying to find the relationship
    between, and how many we are considering as part of the equation or comparison.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: Relationship between Two Continuous Variables
  id: totrans-507
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Establishing a relationship between two continuous variables is basically seeing
    how one varies as the value of the other is increased. The most common way to
    visualize this would be to use a scatter plot, in which we take each variable
    along a single axis (the X and Y axes in a two-dimensional plane when we have
    two variables) and plot each data point using a marker in the X-Y plane. This
    visualization gives us a good idea of whether any kind of relationship exists
    between the two variables at all.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: If we want to quantize the relationship between the two variables, however,
    the most common method is to find the correlation between them. If the target
    variable is continuous and it has a high degree of correlation with another variable,
    this is an indication that the feature would be an important part of the model.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: Pearson's Coefficient of Correlation
  id: totrans-510
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pearson''s Coefficient of Correlation is a correlation coefficient that is
    commonly used to show the linear relationship between a pair of variables. The
    formula returns a value between -1 and +1, where:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: +1 indicates a strong positive relationship
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: -1 indicates a strong negative relationship
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: 0 indicates no relationship at all
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: It's also useful to find correlations between pairs of features themselves.
    In some models, highly correlated features can cause issues, including coefficients
    that vary strongly with small changes in data or modal parameters. In the extreme
    case, perfectly correlated features (such as X2 = 2.5 * X1) cause some models,
    including linear regression, to return undefined coefficients (values of Inf).
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-516
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When fitting a linear model, having features that are highly correlated to one
    another can result in an unpredictable and widely varying model. This is because
    the coefficients of each feature in a linear model can be interpreted as the unit
    change in the target variable, keeping all other features constant. When a set
    of features is not independent (that is, are correlated), however, we cannot determine
    the effect of the independent changes on the target variable due to each feature,
    resulting in widely varying coefficients.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: To find the pairwise correlation for every numeric feature in a DataFrame with
    every other feature, we can use the .corr() function on the DataFrame.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.12: Plotting a Scatter Plot'
  id: totrans-519
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s plot a scatter plot between the primary earthquake magnitude on the
    X axis and the corresponding number of injuries on the Y axis. This exercise is
    a continuation of Exercise 2.11, Computing Skew and Kurtosis:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'Filter out the null values. Since we know that there are null values in both
    columns, let''s first filter the data to include only the non-null rows:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: data_to_plot = data[~pd.isnull(data.injuries) \
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: '& ~pd.isnull(data.eq_primary)]'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: 'Create and display the scatter plot. We will use Matplotlib''s plt.scatter(x=...,
    y=...) command as the primary command for plotting the data. The x and y parameters
    state which feature is to be considered along which axis. They take a single-dimensional
    data structure such as a list, a tuple, or a pandas series. We can also send the
    scatter function more parameters that define, say, the icon to use to plot an
    individual data point. For example, to use a red cross as the icon, we would need
    to send the parameters marker=''x'', c=''r'':'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: plt.figure(figsize=(12,9))
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: plt.scatter(x=data_to_plot.eq_primary, y=data_to_plot.injuries)
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel('Primary earthquake magnitude')
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel('No. of injuries')
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.34: Scatter plot'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-OA11WG07.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.34: Scatter plot'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: From the plot, we can infer that although there doesn't appear to be a trend
    between the number of people who were injured and the earthquake magnitude, there
    is an increasing number of earthquakes with large injury counts as the magnitude
    increases. However, for the majority of earthquakes, there does not seem to be
    a relationship.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/314eupR.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/2YWtbsm. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.13: Plotting a Correlation Heatmap'
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let's plot a correlation heatmap between all the numeric variables in our dataset
    using seaborn's sns.heatmap() function on the inter-feature correlation values
    in the dataset. This exercise is a continuation of Exercise 2.12, Plotting a Scatter
    Plot.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: 'The optional parameters passed to the sns.heatmap() function are square and
    cmap, which indicate that the plot should be such that each pixel is square and
    specify which color scheme to use, respectively:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot a basic heatmap with all the features:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: plt.figure(figsize = (12,10))
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: sns.heatmap(data.corr(), square=True, cmap="YlGnBu")
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.35: Correlation heatmap'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-KJ0HRKHR.jpg)'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.35: Correlation heatmap'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: We can see from the color bar on the right of the plot that the minimum value,
    around -0.2, is the lightest shade, which is a misrepresentation of the correlation
    values, which vary from -1 to 1.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot a subset of features in a more customized heatmap. We will specify the
    upper and lower limits using the vmin and vmax parameters and plot the heatmap
    again with annotations specifying the pairwise correlation values on a subset
    of features. We will also change the color scheme to one that can be better interpreted—while
    the neutral white will represent no correlation, increasingly darker shades of
    blue and red will represent higher positive and negative correlation values, respectively:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: feature_subset = ['focal_depth', 'eq_primary', 'eq_mag_mw', \
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: '''eq_mag_ms'', ''eq_mag_mb'', ''intensity'', \'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '''latitude'', ''longitude'', ''injuries'', \'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: '''damage_millions_dollars'',''total_injuries'', \'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: '''total_damage_millions_dollars'']'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: plt.figure(figsize = (12,10))
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: sns.heatmap(data[feature_subset].corr(), square=True, \
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: annot=True, cmap="RdBu", vmin=-1, vmax=1)
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.36: Customized correlation heatmap'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-Y1Y2KFEB.jpg)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.36: Customized correlation heatmap'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-564
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2Z1lPUB.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/2YntBc8\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Now, while we can calculate the value of correlation, this only gives us an
    indication of a linear relationship. To better judge whether there's a possible
    dependency, we could plot a scatter plot between pairs of features, which is mostly
    useful when the relationship between the two variables is not known, and visualizing
    how the data points are scattered or distributed could give us an idea of whether
    (and how) the two may be related.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: Using Pairplots
  id: totrans-568
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A pairplot is useful for visualizing multiple relationships between pairs of
    features at once and can be plotted using Seaborn's .pairplot() function. In the
    following exercise, we will create a pairplot and visualize relations between
    the features in a dataset.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.14: Implementing a Pairplot'
  id: totrans-570
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this exercise, we will look at a pairplot between the features having the
    highest pairwise correlation in the dataset. This exercise is a continuation of
    Exercise 2.13, Plotting a Correlation Heatmap:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a list having the subset of features on which to create the pairplot:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: feature_subset = ['focal_depth', 'eq_primary', 'eq_mag_mw', \
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '''eq_mag_ms'', ''eq_mag_mb'', ''intensity'',]'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: Create the pairplot using seaborn. The arguments sent to the plotting function
    are kind='scatter', which indicates that we want each individual plot between
    the pair of variables in the grid to be represented as a scatter plot, and diag_kind='kde',
    which indicates that we want the plots along the diagonal (where both the features
    in the pair are the same) to be a kernel density estimate.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: 'It should also be noted here that the plots symmetrically across the diagonal
    from one another will essentially be the same, just with the axes reversed:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: sns.pairplot(data[feature_subset].dropna(), kind ='scatter', \
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: diag_kind='kde')
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.37: Pairplot between the features having the highest pairwise correlation'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-8A67TY8S.jpg)'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.37: Pairplot between the features having the highest pairwise correlation'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully visualized a pairplot to look at the features that have
    high correlation between them within a dataset.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-585
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2Ni11T0.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/3eol7aj. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: Relationship between a Continuous and a Categorical Variable
  id: totrans-588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A common way to view the relationship between two variables when one is categorical
    and the other is continuous is to use a bar plot or a box plot:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: A bar plot helps compare the value of a variable for a discrete set of parameters
    and is one of the most common types of plots. Each bar represents a categorical
    value and the height of the bar usually represents an aggregated value of the
    continuous variable over that category (such as average, sum, or count of the
    values of the continuous variable in that category).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: A box plot is a rectangle drawn to represent the distribution of the continuous
    variable for each discrete value of the categorical variable. It not only allows
    us to visualize outliers efficiently but also allows us to compare the distribution
    of the continuous variable across categories of the categorical variable. The
    lower and upper edges of the rectangle represent the first and third quartiles,
    respectively, the line down through the middle represents the median value, and
    the points (or fliers) above and below the rectangle represent outlier values.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.15: Plotting a Bar Chart'
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s visualize the total number of tsunamis created by earthquakes of each
    intensity level using a bar chart. This exercise is a continuation of Exercise
    2.14, Implementing a Pairplot:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: 'Preprocess the flag_tsunami variable. Before we can use the flag_tsunami variable,
    we need to preprocess it to convert the No values to zeros and the Tsu values
    to ones. This will give us the binary target variable. To do this, we set the
    values in the column using the .loc operator, with : indicating that values need
    to be set for all rows, and the second parameter specifying the name of the column
    for which values are to be set:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: data.loc[:,'flag_tsunami'] = data.flag_tsunami\
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '.apply(lambda t: int(str(t) == ''Tsu''))'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: 'Remove all rows having null intensity values from the data we want to plot:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: subset = data[~pd.isnull(data.intensity)][['intensity',\
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: '''flag_tsunami'']]'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the total number of tsunamis for each intensity level and display the
    DataFrame. To get the data in a format by means of which a bar plot can be visualized,
    we will need to group the rows by each intensity level, and then sum over the
    flag_tsunami values to get the total number of tsunamis for each intensity level:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: data_to_plot = subset.groupby('intensity').sum()
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: data_to_plot
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.38: Total number of tsunamis for each intensity level'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-TZAVF3XT.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.38: Total number of tsunamis for each intensity level'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the bar chart, using Matplotlib''s plt.bar(x=..., height=...) method,
    which takes two arguments, one specifying the x values at which bars need to be
    drawn, and the second specifying the height of each bar. Both of these are one-dimensional
    data structures that must have the same length:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: plt.figure(figsize=(12,9))
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: plt.bar(x=data_to_plot.index, height=data_to_plot.flag_tsunami)
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel('Earthquake intensity')
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel('No. of tsunamis')
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.39: Bar chart'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-R4RR092D.jpg)'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.39: Bar chart'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: From this plot, we can see that as the earthquake intensity increases, the number
    of tsunamis caused also increases, but beyond an intensity of 9, the number of
    tsunamis seems to suddenly drop.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: Think about why this could be happening. Perhaps it's just that there are fewer
    earthquakes with an intensity that high, and hence fewer tsunamis. Or it could
    be an entirely independent factor; maybe high-intensity earthquakes have historically
    occurred on land and couldn't trigger a tsunami. Explore the data to find out.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-619
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/3enFjsZ.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/2V5apxV. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.16: Visualizing a Box Plot'
  id: totrans-622
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll plot a box plot that represents the variation in eq_primary
    over those countries with at least 100 earthquakes. This exercise is a continuation
    of Exercise 2.15, Plotting a Bar Chart:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: 'Find countries with over 100 earthquakes. We will find the value counts for
    all the countries in the dataset. Then, we''ll create a series comprising only
    those countries having a count greater than 100:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: country_counts = data.country.value_counts()
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: top_countries = country_counts[country_counts > 100]
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: top_countries
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.40: Countries with over 100 earthquakes'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-85T9MVBS.jpg)'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.40: Countries with over 100 earthquakes'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: 'Subset the DataFrame to filter in only those rows having countries in the preceding
    set. To filter the rows, we use the .isin() method on the pandas series to select
    those rows containing a value in the array-like object passed as a parameter:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: subset = data[data.country.isin(top_countries.index)]
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: 'Create and display the box plot. The primary command for plotting the data
    is sns.boxplot(x=..., y=..., data=..., order=). The x and y parameters are the
    names of the columns in the DataFrame to be plotted on each axis—the former is
    assumed to be the categorical variable and the latter the continuous. The data
    parameter takes the DataFrame from which to take the data and order takes a list
    of category names that indicates the order in which to display the categories
    on the X axis:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: plt.figure(figsize=(15, 15))
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: sns.boxplot(x='country', y="eq_primary", data=subset, \
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: order=top_countries.index)
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.41: Box plot'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-Y7EBC9P1.jpg)'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.41: Box plot'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-643
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/2zQHPZw.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/3hPAzhN. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: Relationship Between Two Categorical Variables
  id: totrans-646
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When we are looking at only a pair of categorical variables to find a relationship
    between them, the most intuitive way to do this is to divide the data on the basis
    of the first category, and then subdivide it further on the basis of the second
    categorical variable and look at the resultant counts to find the distribution
    of data points. While this might seem confusing, a popular way to visualize this
    is to use stacked bar charts. As in a regular bar chart, each bar would represent
    a categorical value. But each bar would again be subdivided into color-coded categories
    that would provide an indication of what fraction of the data points in the primary
    category fall into each subcategory (that is, the second category). The variable
    with a larger number of categories is usually considered the primary category.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.17: Plotting a Stacked Bar Chart'
  id: totrans-648
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll plot a stacked bar chart that represents the number
    of tsunamis that occurred for each intensity level. This exercise is a continuation
    of Exercise 2.16, Visualizing a Box Plot :'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the number of data points that fall into each grouped value of intensity
    and flag_tsunami:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: grouped_data = data.groupby(['intensity', \
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: '''flag_tsunami'']).size()'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: grouped_data
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.42: Data points falling into each grouped value of intensity and
    flag_tsunami'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-G6BXFRIT.jpg)'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.42: Data points falling into each grouped value of intensity and flag_tsunami'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the .unstack() method on the resultant DataFrame to get the level-1 index
    (flag_tsunami) as a column:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: data_to_plot = grouped_data.unstack()
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: data_to_plot
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.43: The level-1 index'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-43W4BPAB.jpg)'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.43: The level-1 index'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the stacked bar chart. We first use the sns.set() function to indicate
    that we want to use seaborn as our visualization library. Then, we can easily
    use the native .plot() function in pandas to plot a stacked bar chart by passing
    the kind=''bar'' and stacked=True arguments:'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: sns.set()
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: data_to_plot.plot(kind='bar', stacked=True, figsize=(12,8))
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.44: A stacked bar chart'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-JSR92G61.jpg)'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.44: A stacked bar chart'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-673
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/37SnqA8.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/3dllvVx. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot now lets us visualize and interpret the fraction of earthquakes that
    caused tsunamis at each intensity level. In Exercise 2.15: Plotting a Bar Chart,
    we saw the number of tsunamis drop for earthquakes having an intensity of greater
    than 9\. From this plot, we can now confirm that this was primarily because the
    number of earthquakes themselves dropped beyond level 10; the fraction of tsunamis
    even increased for level 11.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2.03: Relationships within the Data'
  id: totrans-677
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this activity, we will revise what we learned in the previous section about
    relationships between data. We will use the same dataset we used in Activity 2.01:
    Summary Statistics and Missing Values, that is, House Prices: Advanced Regression
    Techniques. We''ll use different plots to highlight relationships between values
    in this dataset. This activity is a continuation of Activity 2.01: Summary Statistics
    and Missing Values:'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: Plot the correlation heatmap for the dataset.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
- en: 'The output should be similar to the following:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.45: Correlation Heatmap for the Housing dataset'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-KZM5Q1AT.jpg)'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.45: Correlation Heatmap for the Housing dataset'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot a more compact heatmap having annotations for correlation values using
    the following subset of features:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: feature_subset = ['GarageArea','GarageCars','GarageCond', \
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: '''GarageFinish'',''GarageQual'',''GarageType'', \'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: '''GarageYrBlt'',''GrLivArea'',''LotArea'', \'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: '''MasVnrArea'',''SalePrice'']'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: 'The output should be similar to the following:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.46: Correlation heatmap for selected variables of the Housing dataset'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-R3PJ6CKC.jpg)'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.46: Correlation heatmap for selected variables of the Housing dataset'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: Display the pairplot for the same subset of features, with the KDE plot on the
    diagonals and the scatter plot elsewhere.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.47: Pairplot for the same subset of features'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-SYRP2X30.jpg)'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.47: Pairplot for the same subset of features'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a boxplot to show the variation in SalePrice for each category of GarageCars:'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.48: Boxplot showing variation in SalePrice for each category of
    GarageCars'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-RV8XI78X.jpg)'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.48: Boxplot showing variation in SalePrice for each category of GarageCars'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot a line graph using seaborn to show the variation in SalePrice for older
    and more recently built homes:'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.49: Line graph showing the variation in SalePrice'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: for older to more recently built homes
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-0972CWFE.jpg)'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.49: Line graph showing the variation in SalePrice for older to more
    recently built homes'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-710
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found via this link.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: You have learned how to use more advanced methods from the seaborn package to
    visualize large numbers of variables at once, using charts such as the correlation
    heatmap, pairplot, and boxplots. With boxplots, you learned how to visualize the
    range of one variable segmented across another, categorical variable. The boxplot
    further directly visualizes the quantiles and outliers, making it a powerful tool
    in your EDA toolkit. You have also created some preliminary line and scatter plots
    that are helpful in visualizing continuous data that trends over time or some
    other variable.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-713
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this chapter, we started by talking about why data exploration is an important
    part of the modeling process and how it can help in not only preprocessing the
    dataset for the modeling process but also help us engineer informative features
    and improve model accuracy. This chapter focused on not only gaining a basic overview
    of the dataset and its features but also gaining insights by creating visualizations
    that combine several features. We looked at how to find the summary statistics
    of a dataset using core functionality from pandas. We looked at how to find missing
    values and talked about why they're important while learning how to use the Missingno
    library to analyze them and the pandas and scikit-learn libraries to impute the
    missing values. Then, we looked at how to study the univariate distributions of
    variables in the dataset and visualize them for both categorical and continuous
    variables using bar charts, pie charts, and histograms. Lastly, we learned how
    to explore relationships between variables, and about how they can be represented
    using scatter plots, heatmaps, box plots, and stacked bar charts, to name but
    a few.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will start exploring supervised machine learning
    algorithms. Now that we have an idea of how to explore a dataset that we have,
    we can proceed to the modeling phase. The next chapter will introduce regression,
    a class of algorithms that are primarily used to build models for continuous target variables.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL

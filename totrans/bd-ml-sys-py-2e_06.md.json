["```py\n>>> X, Y = load_sanders_data()\n>>> classes = np.unique(Y)\n>>> for c in classes: print(\"#%s: %i\" % (c, sum(Y==c)))\n#irrelevant: 490\n#negative: 487\n#neutral: 1952\n#positive: 433\n\n```", "```py\n>>> import numpy as np\n>>> np.set_printoptions(precision=20) # tell numpy to print out more digits (default is 8)\n>>> np.array([2.48E-324])\narray([ 4.94065645841246544177e-324])\n>>> np.array([2.47E-324])\narray([ 0.])\n\n```", "```py\n>>> x = 0.00001\n>>> x**64 # still fine\n1e-320\n>>> x**65 # ouch\n0.0\n\n```", "```py\n>>> import sys\n>>> sys.float_info\nsys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)\n\n```", "```py\n>>> # first create a Boolean list having true for tweets\n>>> # that are either positive or negative\n>>> pos_neg_idx = np.logical_or(Y==\"positive\", Y==\"negative\")\n\n>>> # now use that index to filter the data and the labels\n>>> X = X[pos_neg_idx]\n>>> Y = Y[pos_neg_idx]\n\n>>> # finally convert the labels themselves into Boolean\n>>> Y = Y==\"positive\"\n\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\ndef create_ngram_model():\n tfidf_ngrams = TfidfVectorizer(ngram_range=(1, 3), analyzer=\"word\", binary=False)\n clf = MultinomialNB()\n return Pipeline([('vect', tfidf_ngrams), ('clf', clf)])\n\n```", "```py\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom sklearn.cross_validation import ShuffleSplit\n\ndef train_model(clf_factory, X, Y):\n # setting random_state to get deterministic behavior\n cv = ShuffleSplit(n=len(X), n_iter=10, test_size=0.3, random_state=0)\n\n scores = []\n pr_scores = []\n\n for train, test in cv:\n X_train, y_train = X[train], Y[train]\n X_test, y_test = X[test], Y[test]\n\n clf = clf_factory()\n clf.fit(X_train, y_train)\n\n train_score = clf.score(X_train, y_train)\n test_score = clf.score(X_test, y_test)\n\n scores.append(test_score)\n proba = clf.predict_proba(X_test)\n\n precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:,1])\n\n pr_scores.append(auc(recall, precision))\n\n summary = (np.mean(scores), np.std(scores),\nnp.mean(pr_scores), np.std(pr_scores))\n print(\"%.3f\\t%.3f\\t%.3f\\t%.3f\" % summary)\n\n```", "```py\n>>> X, Y = load_sanders_data()\n>>> pos_neg_idx = np.logical_or(Y==\"positive\", Y==\"negative\")\n>>> X = X[pos_neg_idx]\n>>> Y = Y[pos_neg_idx]\n>>> Y = Y==\"positive\"\n>>> train_model(create_ngram_model, X, Y)\n0.788   0.024   0.882   0.036\n\n```", "```py\ndef tweak_labels(Y, pos_sent_list):\n pos = Y==pos_sent_list[0]\n for sent_label in pos_sent_list[1:]:\n pos |= Y==sent_label\n\n Y = np.zeros(Y.shape[0])\n Y[pos] = 1\n Y = Y.astype(int)\n\nreturn Y\n\n```", "```py\n>>> Y = tweak_labels(Y, [\"positive\", \"negative\"])\n\n```", "```py\n>>> train_model(create_ngram_model, X, Y, plot=True)\n0.750   0.012   0.659   0.023\n\n```", "```py\n== Pos vs. rest ==\n0.873   0.009   0.305   0.026\n== Neg vs. rest ==\n0.861   0.006   0.497   0.026\n```", "```py\n<estimator>__<subestimator>__...__<param_name>\n```", "```py\nparam_grid={\"vect__ngram_range\"=[(1, 1), (1, 2), (1, 3)]}\n\n```", "```py\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import f1_score\n\ndef grid_search_model(clf_factory, X, Y):\n cv = ShuffleSplit(\n n=len(X), n_iter=10, test_size=0.3,random_state=0)\n\n param_grid = dict(vect__ngram_range=[(1, 1), (1, 2), (1, 3)],\n vect__min_df=[1, 2],\n vect__stop_words=[None, \"english\"],\n vect__smooth_idf=[False, True],\n vect__use_idf=[False, True],\n vect__sublinear_tf=[False, True],\n vect__binary=[False, True],\n clf__alpha=[0, 0.01, 0.05, 0.1, 0.5, 1],\n )\n\n grid_search = GridSearchCV(clf_factory(),\n param_grid=param_grid,\n cv=cv,\n score_func=f1_score,\n verbose=10)\n grid_search.fit(X, Y) \n\n return grid_search.best_estimator_\n\n```", "```py\nclf = grid_search_model(create_ngram_model, X, Y)\nprint(clf)\n\n```", "```py\n... waiting some hours  ...\nPipeline(clf=MultinomialNB(\nalpha=0.01, class_weight=None, fit_prior=True),\nclf__alpha=0.01, \nclf__class_weight=None, \nclf__fit_prior=True,\nvect=TfidfVectorizer(\nanalyzer=word, binary=False,\n charset=utf-8, charset_error=strict, \ndtype=<type 'long'>,input=content,\nlowercase=True, max_df=1.0,\nmax_features=None, max_n=None,\nmin_df=1, min_n=None, ngram_range=(1, 2),\nnorm=l2, preprocessor=None, smooth_idf=False,\nstop_words=None,strip_accents=None, \nsublinear_tf=True,token_pattern=(?u)\\b\\w\\w+\\b,\ntoken_processor=None, tokenizer=None, \nuse_idf=False, vocabulary=None),\nvect__analyzer=word, vect__binary=False, \nvect__charset=utf-8,\nvect__charset_error=strict, \nvect__dtype=<type 'long'>,\nvect__input=content, vect__lowercase=True, \nvect__max_df=1.0,vect__max_features=None, \nvect__max_n=None, vect__min_df=1,\nvect__min_n=None, vect__ngram_range=(1, 2), \nvect__norm=l2, vect__preprocessor=None, \nvect__smooth_idf=False, vect__stop_words=None, \nvect__strip_accents=None, vect__sublinear_tf=True,\nvect__token_pattern=(?u)\\b\\w\\w+\\b,\nvect__token_processor=None, vect__tokenizer=None,\nvect__use_idf=False, vect__vocabulary=None)\n0.795  0.007  0.702  0.028\n\n```", "```py\n== Pos vs. rest ==\n0.889   0.010   0.509   0.041\n== Neg vs. rest ==\n0.886   0.007   0.615   0.035\n\n```", "```py\nemo_repl = {\n # positive emoticons\n \"&lt;3\": \" good \",\n \":d\": \" good \", # :D in lower case\n \":dd\": \" good \", # :DD in lower case\n \"8)\": \" good \",\n \":-)\": \" good \",\n \":)\": \" good \",\n \";)\": \" good \",\n \"(-:\": \" good \",\n \"(:\": \" good \",\n\n # negative emoticons:\n \":/\": \" bad \",\n \":&gt;\": \" sad \",\n \":')\": \" sad \",\n \":-(\": \" bad \",\n \":(\": \" bad \",\n \":S\": \" bad \",\n \":-S\": \" bad \",\n }\n\n# make sure that e.g. :dd is replaced before :d\nemo_repl_order = [k for (k_len,k) in reversed(sorted([(len(k),k) for k in emo_repl.keys()]))]\n\n```", "```py\nre_repl = {\nr\"\\br\\b\": \"are\",\nr\"\\bu\\b\": \"you\",\nr\"\\bhaha\\b\": \"ha\",\nr\"\\bhahaha\\b\": \"ha\",\nr\"\\bdon't\\b\": \"do not\",\nr\"\\bdoesn't\\b\": \"does not\",\nr\"\\bdidn't\\b\": \"did not\",\nr\"\\bhasn't\\b\": \"has not\",\nr\"\\bhaven't\\b\": \"have not\",\nr\"\\bhadn't\\b\": \"had not\",\nr\"\\bwon't\\b\": \"will not\",\nr\"\\bwouldn't\\b\": \"would not\",\nr\"\\bcan't\\b\": \"can not\",\nr\"\\bcannot\\b\": \"can not\",\n }\n\ndef create_ngram_model(params=None):\n def preprocessor(tweet):\n tweet = tweet.lower()\n for k in emo_repl_order:\n tweet = tweet.replace(k, emo_repl[k])\n for r, repl in re_repl.items():\n tweet = re.sub(r, repl, tweet)\n\n return tweet\n\n tfidf_ngrams = TfidfVectorizer(preprocessor=preprocessor,\nanalyzer=\"word\")\n # ...\n\n```", "```py\n== Pos vs. neg ==\n0.808   0.024   0.885   0.029\n== Pos/neg vs. irrelevant/neutral ==\n0.793   0.010   0.685   0.024\n== Pos vs. rest ==\n0.890   0.011   0.517   0.041\n== Neg vs. rest ==\n0.886   0.006   0.624   0.033\n\n```", "```py\n>>> import nltk\n>>> nltk.pos_tag(nltk.word_tokenize(\"This is a good book.\"))\n[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('book', 'NN'), ('.', '.')]\n>>> nltk.pos_tag(nltk.word_tokenize(\"Could you please book the flight?\"))\n[('Could', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('book', 'NN'), ('the', 'DT'), ('flight', 'NN'), ('?', '.')]\n\n```", "```py\nimport csv, collections\n\ndef load_sent_word_net():\n # making our life easier by using a dictionary that\n # automatically creates an empty list whenever we access\n # a not yet existing key\n sent_scores = collections.defaultdict(list)\n\n with open(os.path.join(DATA_DIR, SentiWordNet_3.0.0_20130122.txt\"), \"r\") as csvfile:\n reader = csv.reader(csvfile, delimiter='\\t',\nquotechar='\"')\n for line in reader:\n if line[0].startswith(\"#\"):\n continue\n if len(line)==1:\n continue\n\n POS, ID, PosScore, NegScore, SynsetTerms, Gloss = line\n if len(POS)==0 or len(ID)==0:\n continue\n for term in SynsetTerms.split(\" \"):\n # drop number at the end of every term\n term = term.split(\"#\")[0] \n term = term.replace(\"-\", \" \").replace(\"_\", \" \")\n key = \"%s/%s\"%(POS, term.split(\"#\")[0])\n sent_scores[key].append((float(PosScore), \nfloat(NegScore)))\n\n for key, value in sent_scores.items():\n sent_scores[key] = np.mean(value, axis=0)\n\n return sent_scores\n\n```", "```py\nsent_word_net = load_sent_word_net()\n\nclass LinguisticVectorizer(BaseEstimator):\n def get_feature_names(self):\n return np.array(['sent_neut', 'sent_pos', 'sent_neg',\n 'nouns', 'adjectives', 'verbs', 'adverbs',\n 'allcaps', 'exclamation', 'question', 'hashtag', 'mentioning'])\n\n # we don't fit here but need to return the reference\n # so that it can be used like fit(d).transform(d)\n def fit(self, documents, y=None):\n return self\n\n def _get_sentiments(self, d):\n sent = tuple(d.split())\n tagged = nltk.pos_tag(sent)\n\n pos_vals = []\n neg_vals = []\n\n nouns = 0.\n adjectives = 0.\n verbs = 0.\n adverbs = 0.\n\n for w,t in tagged:\n p, n = 0,0\n sent_pos_type = None\n if t.startswith(\"NN\"):\n sent_pos_type = \"n\"\n nouns += 1\n elif t.startswith(\"JJ\"):\n sent_pos_type = \"a\"\n adjectives += 1\n elif t.startswith(\"VB\"):\n sent_pos_type = \"v\"\n verbs += 1\n elif t.startswith(\"RB\"):\n sent_pos_type = \"r\"\n adverbs += 1\n\n if sent_pos_type is not None:\n sent_word = \"%s/%s\" % (sent_pos_type, w)\n\n if sent_word in sent_word_net:\n p,n = sent_word_net[sent_word]\n\n pos_vals.append(p)\n neg_vals.append(n)\n\n l = len(sent)\n avg_pos_val = np.mean(pos_vals)\n avg_neg_val = np.mean(neg_vals)\n return [1-avg_pos_val-avg_neg_val, avg_pos_val, avg_neg_val,\nnouns/l, adjectives/l, verbs/l, adverbs/l]\n\n def transform(self, documents):\n obj_val, pos_val, neg_val, nouns, adjectives, \\\nverbs, adverbs = np.array([self._get_sentiments(d) \\\nfor d in documents]).T\n\n allcaps = []\n exclamation = []\n question = []\n hashtag = []\n mentioning = []\n\n for d in documents:\n allcaps.append(np.sum([t.isupper() \\\n for t in d.split() if len(t)>2]))\n\n exclamation.append(d.count(\"!\"))\n question.append(d.count(\"?\"))\n hashtag.append(d.count(\"#\"))\n mentioning.append(d.count(\"@\"))\n\n result = np.array([obj_val, pos_val, neg_val, nouns, adjectives, verbs, adverbs, allcaps, exclamation, question, \nhashtag, mentioning]).T\n\n return result\n\n```", "```py\ndef create_union_model(params=None):\n def preprocessor(tweet):\n tweet = tweet.lower()\n\n for k in emo_repl_order:\n tweet = tweet.replace(k, emo_repl[k])\n for r, repl in re_repl.items():\n tweet = re.sub(r, repl, tweet)\n\n return tweet.replace(\"-\", \" \").replace(\"_\", \" \")\n\n tfidf_ngrams = TfidfVectorizer(preprocessor=preprocessor, analyzer=\"word\")\n ling_stats = LinguisticVectorizer()\n all_features = FeatureUnion([('ling', ling_stats), ('tfidf', tfidf_ngrams)])\n clf = MultinomialNB()\n pipeline = Pipeline([('all', all_features), ('clf', clf)])\n\n if params:\n pipeline.set_params(**params)\n\n return pipeline\n\n```", "```py\n== Pos vs. neg ==\n0.810   0.023   0.890   0.025\n== Pos/neg vs. irrelevant/neutral ==\n0.791   0.007   0.691   0.022\n== Pos vs. rest ==\n0.890   0.011   0.529   0.035\n== Neg vs. rest ==\n0.883   0.007   0.617   0.033\ntime spent: 214.12578797340393\n\n```"]
<html><head></head><body>
		<div class="Content" id="_idContainer132">
			<h1 id="_idParaDest-97"><em class="italics"><a id="_idTextAnchor106"/>Chapter 5</em></h1>
		</div>
		<div class="Content" id="_idContainer133">
			<h1 id="_idParaDest-98"><a id="_idTextAnchor107"/>Autoencoders</h1>
		</div>
		<div class="Content" id="_idContainer134">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to do the following:</p>
			<ul>
				<li class="bullets">Explain where autoencoders can be applied and their use cases</li>
				<li class="bullets">Understand how artificial neural networks are implemented and used</li>
				<li class="bullets">Implement an artificial neural network using the Keras framework</li>
				<li class="bullets">Explain how autoencoders are used in dimensionality reduction and denoising</li>
				<li class="bullets">Implement an autoencoder using the Keras framework</li>
				<li class="bullets">Explain and implement an autoencoder model using convolutional neural networks</li>
			</ul>
			<p>In this chapter, we will take a look at autoencoders and their applications.</p>
		</div>
		<div class="Content" id="_idContainer175">
			<h2 id="_idParaDest-99"><a id="_idTextAnchor108"/>Introduction</h2>
			<p>This chapter continues our discussion of dimensionality reduction techniques as we turn our attention to autoencoders. Autoencoders are a particularly interesting area of focus as they provide a means of using supervised learning based on artificial neural networks, but in an unsupervised context. Being based on artificial neural networks, autoencoders are an extremely effective means of dimensionality reduction, but also provide additional benefits. With recent increases in the availability of data, processing power, and network connectivity, autoencoders are experiencing a resurgence in usage and study from their origins in the late 1980s. This is also consistent with the study of artificial neural networks, which was first described and implemented as a concept in the 1960s. Presently, you would only need to conduct a cursory internet search to discover the popularity and power of neural nets.</p>
			<p>Autoencoders can be used for de-noising images and generating artificial data samples in combination with other methods, such as recurrent or <strong class="keyword">Long Short-Term Memory</strong> (<strong class="keyword">LSTM</strong>) architectures, to predict sequences of data. The flexibility and power that arises from the use of artificial neural networks also enables autoencoders to form very efficient representations of the data, which can then be used either directly as an extremely efficient search method, or as a feature vector for later processing.</p>
			<p>Consider the use of an autoencoder in an image de-noising application, where we are presented with the image on the left in <a href="C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor109"><em class="italics">Figure 5.1</em></a>. We can see that the image is affected by the addition of some random noise. We can use a specially trained autoencoder to remove this noise, as represented by the image on the right in <em class="italics">Figure 5.1</em>. In learning how to remove this noise, the autoencoder has also learned to encode the important information that composes the image and how to reconstruct (or decode) this information into a clearer version of the original image.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer135">
					<img alt="Figure 5.1: Autoencoder de-noising" src="image/C12626_05_01.jpg"/>
				</div>
			</div>
			<h6>F<a id="_idTextAnchor109"/>igure 5.1: Autoencoder de-noising</h6>
			<h4>Note</h4>
			<p class="callout">This image is modified from <a href="http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/">http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/</a> under CC0.</p>
			<p>This example demonstrates one aspect of autoencoders that makes them useful for unsupervised learning (the encoding stage), and one that is useful in generating new images (decoding). Throughout this chapter, we will delve further into these two useful stages of autoencoders and apply the output of the autoencoder to clustering the CIFAR-10 dataset.</p>
			<p>Here is a representation of an encoder and decoder:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer136">
					<img alt="Figure 5.2: Encoder/decoder representation" src="image/C12626_05_02.jpg"/>
				</div>
			</div>
			<h6>Figure 5.2: Encoder/decoder representation</h6>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor110"/>Fundamentals of Artificial Neural Networks</h2>
			<p>Given that autoencoders are based on artificial neural networks, an understanding of how neural networks is also critical for understanding autoencoders. This section of the chapter will briefly review the fundamentals of artificial neural networks. It is important to note that there are many aspects of neural nets that are outside of the scope of this book. The topic of neural networks could easily, and has, filled many books on its own, and this section is not to be considered an exhaustive discussion of the topic.</p>
			<p>As described earlier, artificial neural networks are primarily used in supervised learning problems, where we have a set of input information, say a series of images, and we are training an algorithm to map the information to a desired output, such as a class or category. Consider the CIFAR-10 dataset (<a href="C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor111"><em class="italics">Figure 5.3</em></a>) as an example, which contains images of 10 different categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck), with 6,000 images per category. When neural nets are used in a supervised learning context, the images are fed to the network with a representation of the corresponding category labels being the desired output of the network. </p>
			<p>The network is then trained to maximize its ability to infer or predict the correct label for a given image.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer137">
					<img alt="Figure 5.3: CIFAR-10 dataset" src="image/C12626_05_03.jpg"/>
				</div>
			</div>
			<h6>F<a id="_idTextAnchor111"/>igure 5.3: CIFAR-10 dataset</h6>
			<h4>Note</h4>
			<p class="callout">This image is taken from <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a> from Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.</p>
			<h3 id="_idParaDest-101"><a id="_idTextAnchor112"/>The Neuron</h3>
			<p>The artificial neural network derives its name from the biological neural networks commonly found in the brain.  While the accuracy of the analogy can certainly be questioned, it is a useful metaphor to break down the concept of artificial neural networks and facilitate understanding. As with their biological counterparts, the neuron is the building block on which all neural networks are constructed, connecting a number of neurons in different configurations to form more powerful structures. Each neuron (<a href="C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor113"><em class="italics">Figure 5.4</em></a>) is composed of four individual parts: an input value, a tunable weight (theta), an activation function that operates on the input value, and the resulting output value:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer138">
					<img alt="Figure 5.4: Anatomy of a neuron" src="image/C12626_05_04.jpg"/>
				</div>
			</div>
			<h6>Fi<a id="_idTextAnchor113"/>gure 5.4: Anatomy of a neuron</h6>
			<p>The activation function is specifically chosen depending upon the objective of the neural network being designed, and there are a number of common functions, including <strong class="inline">tanh</strong>, <strong class="inline">sigmoid</strong>, <strong class="inline">linear</strong>, <strong class="inline">sigmoid</strong>, and <strong class="inline">ReLU</strong> (rectified linear unit). Throughout this chapter, we will use both the <strong class="inline">sigmoid</strong> and <strong class="inline">ReLU</strong> activation functions, so let's look at them in a little more detail.</p>
			<h3 id="_idParaDest-102"><a id="_idTextAnchor114"/>Sigmoid Function</h3>
			<p>The sigmoid activation function is very commonly used as an output in the classification of neural networks due to its ability to shift the input values to approximate a binary output. The sigmoid function produces the following output:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer139">
					<img alt="" src="image/C12626_05_05.jpg"/>
				</div>
			</div>
			<h6>Fig<a id="_idTextAnchor115"/>ure 5.5: Output of the sigmoid function</h6>
			<p>We can see in <a href="C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor115"><em class="italics">Figure 5.5</em></a> that the output of the sigmoid function asymptotes (approaches but never reaches) 1 as <em class="italics">x</em> increases and asymptotes 0 as <em class="italics">x</em> moves further away from 0 in the negative direction. This function is used in classification tasks as it provides close to a binary output and is not a member of class (0) or is a member of the class (1).</p>
			<h3 id="_idParaDest-103"><a id="_idTextAnchor116"/>Rectified Linear Unit (ReLU)</h3>
			<p>The rectified linear unit is a very useful activation function that's commonly used at intermediary stages of neural networks. Simply put, the value 0 is assigned to values less than 0, and the value is returned for greater than 0.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer140">
					<img alt="Figure 5.6: Output of ReLU" src="image/C12626_05_06.jpg"/>
				</div>
			</div>
			<h6>Figure 5.6: Output of ReLU</h6>
			<h3 id="_idParaDest-104"><a id="_idTextAnchor117"/>Exercise 18: Modeling the Neurons of an Artificial Neural Network</h3>
			<p>In this exercise, we will practically introduce a programmatic representation of the neuron in NumPy using the sigmoid function. We will keep the inputs fixed and adjust the tunable weights to investigate the effect on the neuron. Interestingly, this model is also very close to the supervised learning method of logistic regression. Perform the following steps:</p>
			<ol>
				<li>Import the <strong class="inline">numpy</strong> and matplotlib packages:<p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Configure matplotlib to enable the use of Latex to render mathematical symbols in the images:<p class="snippet">plt.rc('text', usetex=True)</p></li>
				<li>Define the <strong class="inline">sigmoid</strong> function as a Python function:<p class="snippet">def sigmoid(z):</p><p class="snippet">    return np.exp(z) / (np.exp(z) + 1)</p><h4>Note</h4><p class="callout">Here, we're using the sigmoid function. You could also use the ReLU function. The ReLU activation function, while being powerful in artificial neural networks, is easy to define. It simply needs to return the input value if greater than 0; otherwise, it returns 0:</p><p class="callout"><strong class="inline">def relu(x):</strong></p><p class="callout"><strong class="inline">    return np.max(0, x)</strong></p></li>
				<li>Define the inputs (<strong class="inline">x</strong>) and tunable weights (<strong class="inline">theta</strong>) for the neuron. In this example, the inputs (<strong class="inline">x</strong>) will be 100 numbers linearly spaced between <strong class="inline">-5</strong> and <strong class="inline">5</strong>. Set <strong class="inline">theta= 1</strong>:<p class="snippet">theta = 1</p><p class="snippet">x = np.linspace(-5, 5, 100)</p><p class="snippet">x</p><p>A section of the output is as follows:</p><div class="IMG---Figure" id="_idContainer141"><img alt="Figure 5.7: Printing the inputs" src="image/C12626_05_07.jpg"/></div><h6>Figure 5.7: Printing the inputs</h6></li>
				<li>Compute the outputs (<strong class="inline">y</strong>) of the neuron:<p class="snippet">y = sigmoid(x * theta)</p></li>
				<li>Plot the output of the neuron versus the input:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_subplot(111)</p><p class="snippet">ax.plot(x, y)</p><p class="snippet">ax.set_xlabel('$x$', fontsize=22);</p><p class="snippet">ax.set_ylabel('$h(x\Theta)$', fontsize=22);</p><p class="snippet">ax.spines['left'].set_position(('data', 0));</p><p class="snippet">ax.spines['top'].set_visible(False);</p><p class="snippet">ax.spines['right'].set_visible(False);</p><p class="snippet">ax.tick_params(axis='both', which='major', labelsize=22)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer142"><img alt="Figure 5.8: Plot of neurons versus inputs" src="image/C12626_05_08.jpg"/></div><h6>Figure 5.8: Plot of neurons versus inputs</h6></li>
				<li>Set the tunable parameter, <strong class="inline">theta</strong>, to <strong class="inline">5</strong>, and recompute and store the output of the neuron:<p class="snippet">theta = 5</p><p class="snippet">y_2 = sigmoid(x * theta)</p></li>
				<li>Change the tunable parameter, <strong class="inline">theta</strong>, to <strong class="inline">0.2</strong>, and recompute and store the output of the neuron:<p class="snippet">theta = 0.2</p><p class="snippet">y_3 = sigmoid(x * theta)</p></li>
				<li>Plot the three different output curves of the neuron (<strong class="inline">theta = 1</strong>, <strong class="inline">theta = 5</strong>, <strong class="inline">theta = 0.2</strong>) on one graph:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_subplot(111)</p><p class="snippet">ax.plot(x, y, label='$\Theta=1$');</p><p class="snippet">ax.plot(x, y_2, label='$\Theta=5$', linestyle=':');</p><p class="snippet">ax.plot(x, y_3, label='$\Theta=0.2$', linestyle='--');</p><p class="snippet">ax.set_xlabel('$x\Theta$', fontsize=22);</p><p class="snippet">ax.set_ylabel('$h(x\Theta)$', fontsize=22);</p><p class="snippet">ax.spines['left'].set_position(('data', 0));</p><p class="snippet">ax.spines['top'].set_visible(False);</p><p class="snippet">ax.spines['right'].set_visible(False);</p><p class="snippet">ax.tick_params(axis='both', which='major', labelsize=22);</p><p class="snippet">ax.legend(fontsize=22);</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer143">
					<img alt="Figure 5.9: Output curves of neurons" src="image/C12626_05_09.jpg"/>
				</div>
			</div>
			<h6>Figure 5.9: Output curves of neurons</h6>
			<p>In this exercise, we modeled the basic building block of an artificial neural network with a sigmoid activation function. We can see that using the sigmoid function increases the steepness of the gradient and means that only small values of x will push the output to either close to 1 or 0. Similarly, reducing <strong class="inline">theta</strong> reduces the sensitivity of the neuron to non-zero values and results in much extreme input values being required to push the result of the output to either 0 or 1, tuning the output of the neuron.</p>
			<h3 id="_idParaDest-105"><a id="_idTextAnchor118"/>Activity 8: Modeling Neurons with a ReLU Activation Function</h3>
			<p>In this activity, we will investigate the ReLU activation function and the effect tunable weights have in modifying the output of ReLU units:</p>
			<ol>
				<li value="1">Import <strong class="inline">numpy</strong> and matplotlib.</li>
				<li>Define the ReLU activation function as a Python function.</li>
				<li>Define the inputs (<strong class="inline">x</strong>) and tunable weights (<strong class="inline">theta</strong>) for the neuron. In this example, the inputs (<strong class="inline">x</strong>) will be 100 numbers linearly spaced between <strong class="inline">-5</strong> and <strong class="inline">5</strong>.  Set <strong class="inline">theta = 1</strong>.</li>
				<li>Compute the output (<strong class="inline">y</strong>).</li>
				<li>Plot the output of the neuron versus the input.</li>
				<li>Now, set <strong class="inline">theta = 5</strong>, and recompute and store the output of the neuron.</li>
				<li>Now, set <strong class="inline">theta = 0.2</strong>, and recompute and store the output of the neuron.</li>
				<li>Plot the three different output curves of the neuron (<strong class="inline">theta = 1</strong>, <strong class="inline">theta = 5</strong>, and <strong class="inline">theta = 0.2</strong>) on one graph.</li>
			</ol>
			<p>By the end of this activity, you will have developed a range of response curves for the ReLU activated neuron.  You will also be able to describe the effect of changing the value of theta on the output of the neuron. The output will look as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer144">
					<img alt="Figure 5.10: Expected output curves" src="image/C12626_05_10.jpg"/>
				</div>
			</div>
			<h6>Figure 5.10: Expected output curves</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 333.</p>
			<h3 id="_idParaDest-106"><a id="_idTextAnchor119"/>Neural Networks: Architecture Definition</h3>
			<p>Individual neurons aren't particularly useful in isolation; they provide an activation function and a means of tuning the output, but a single neuron would have an limited learning ability. Neurons become much more powerful when many of them are combined and connected together in a network structure. By using a number of different neurons and combining the outputs of individual neurons, more complex relationships can be established and more powerful learning algorithms can be built. In this section, we will briefly discuss the structure of a neural network and implement a simple neural network using the Keras machine learning framework (<a href="https://keras.io/">https://keras.io/</a>).</p>
			<div>
				<div class="IMG---Figure" id="_idContainer145">
					<img alt="Figure 5.11: Simplified representation of a neural network" src="image/C12626_05_11.jpg"/>
				</div>
			</div>
			<h6>Figure 5.11: Simplified representation of a neural network</h6>
			<p><em class="italics">Figure 5.11</em> illustrates the structure of a two-layered, fully-connected neural network. One of the first observations we can make is that there is a lot of information contained within this structure, with a high degree of connectivity as represented by the arrows that point to and from each of the nodes. Working from the left-hand side of the image, we can see the input values to the neural network, as represented by the (<em class="italics">x</em>) values. In this example, we have <em class="italics">m</em> input values per sample, and only the first sample is being fed into the network, hence, values from <img alt="A close up of a stool&#10;&#10;Description automatically generated" src="image/C12626_05_Formula_01.png"/> to <img alt="A close up of a sign&#10;&#10;Description automatically generated" src="image/C12626_05_Formula_02.png"/>. These values are then multiplied by the corresponding weights of the first layer of the neural network (<img alt="" src="image/C12626_05_Formula_03.png"/>) before being passed into the activation function of the corresponding neuron. This is known as a <strong class="keyword">feedforward</strong> neural network. The notation used in <em class="italics">Figure 5.11</em> to identify the weights is <img alt="A close up of a logo&#10;&#10;Description automatically generated" src="image/C12626_05_Formula_04.png"/>, where <em class="italics">i</em> is the layer the weight belongs to, <em class="italics">j</em> is the input node number (starting with 1 at the top), and <em class="italics">k</em> is the node in the subsequent layer that the weight feeds into to. </p>
			<p>Looking at the inter-connectivity between the outputs of layer 1 (also known as the <strong class="keyword">hidden layer</strong>) and the inputs to the output layer, we can see that there is a large number of trainable parameters (weights) that can be used to map the input to the desired output. The network of <em class="italics">Figure 5.11</em> represents an <em class="italics">n</em> class neural network classifier, where the output for each of the <em class="italics">n</em> nodes represents the probability of the input belonging to the corresponding class.</p>
			<p>Each layer is able to use a different activation function as described by <img alt="" src="image/C12626_05_Formula_05.png"/> and <img alt="" src="image/C12626_05_Formula_06.png"/>, thus allowing different activation functions to be mixed, in which the first layer could use ReLU, the second could use tanh, and the third could use sigmoid, for example. The final output is calculated by taking the sum of the product of the output of the previous layer with the corresponding weights. </p>
			<p>If we consider the output of the first node of layer 1, it can be calculated by multiplying the inputs by the corresponding weights, adding the result, and passing it through the activation function:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer152">
					<img alt="" src="image/C12626_05_12.jpg"/>
				</div>
			</div>
			<h6>Figure 5.12: Calculating the output of the last node</h6>
			<p>As we increase the number of layers between the input and output of the network, we increase the depth of the network. An increase in the depth is also an increase in the number of trainable parameters, as well as the complexity of the relationships within the data, as described by the network. It is, typically, harder to train networks with increased depth because the types of features selected for the input become more critical. Additionally, as we add more neurons to each layer, we increase the height of the neural network. By adding more neurons, the ability of the network to describe the dataset increases as we add more trainable parameters. If too many neurons are added, the network can memorize the dataset but fails to generalize new samples. The trick in constructing neural networks is to find the balance between sufficient complexity to be able to describe the relationships within the data and not be so complicated as to memorize the training samples.</p>
			<h3 id="_idParaDest-107"><a id="_idTextAnchor120"/>Exercise 19: Defining a Keras Model</h3>
			<p>In this exercise, we will define a neural network architecture (similar to <em class="italics">Figure 5.11</em>) using the Keras machine learning framework to classify images for the CIFAR-10 dataset. As each input image is 32 x 32 pixels in size, the input vector will comprise 32*32 = 1,024 values. With 10 individual classes in CIFAR-10, the output of the neural network will be composed of 10 individual values, with each value representing the probability of the input data belonging to the corresponding class.</p>
			<ol>
				<li value="1">For this exercise, we will require the Keras machine learning framework. Keras is a high-level neural network API that is used on top of an existing library, such as TensorFlow or Theano. Keras makes it easy to switch between lower-level frameworks because the high-level interface it provides remains the same irrespective of the underlying library. In this book, we will be using TensorFlow as the underlying library. If you have yet to install Keras and TensorFlow, do so using <strong class="inline">conda</strong>:<p class="snippet">!conda install tensforflow keras</p><p>Alternatively, you can install it using <strong class="inline">pip</strong>:</p><p class="snippet">!pip install tensorflow keras</p></li>
				<li>We will require the <strong class="inline">Sequential</strong> and <strong class="inline">Dense</strong> classes from <strong class="inline">keras.models</strong> and <strong class="inline">keras.layers</strong>, respectively. Import these classes:<p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Dense</p></li>
				<li>As described earlier, the input layer will receive 1,024 values. The second layer (Layer 1) will have 500 units and, because the network is to classify one of 10 different classes, the output layer will have 10 units. In Keras, a model is defined by passing an ordered list of layers to the <strong class="inline">Sequential</strong> model class. This example uses the <strong class="inline">Dense</strong> layer class, which is a fully-connected neural network layer. The first layer will use a ReLU activation function, while the output will use the <strong class="inline">softmax</strong> function to determine the probability of each class. Define the model:<p class="snippet">model = Sequential([</p><p class="snippet">    Dense(500, input_shape=(1024,), activation='relu'),</p><p class="snippet">    Dense(10, activation='softmax')</p><p class="snippet">])</p></li>
				<li>With the model defined, we can use the <strong class="inline">summary</strong> method to confirm the structure and the number of trainable parameters (or weights) within the model:<p class="snippet">model.summary()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer153">
					<img alt="Figure 5.13: Structure and count of trainable parameters in the model" src="image/C12626_05_13.jpg"/>
				</div>
			</div>
			<h6>Figure 5.13: Structure and count of trainable parameters in the model</h6>
			<p>This table summarizes the structure of the neural network. We can see that there are the two layers that we specified, with 500 units in the first layer and 10 output units in the second layer. The <strong class="inline">Param #</strong> column tells us how many trainable weights are available in that specific layer. The table also tells us that there are 517,510 trainable weights in total within the network.</p>
			<p>In this exercise, we created a neural network model in Keras that contains a network of over 500,000 weights that can be used to classify the images of CIFAR-10. In the next section, we will train the model.</p>
			<h3 id="_idParaDest-108"><a id="_idTextAnchor121"/>Neural Networks: Training</h3>
			<p>With the neural network model defined, we can begin the training process; at this stage, we will be training the model in a supervised fashion to develop some familiarity with the Keras framework before moving on to training autoencoders. Supervised learning models are trained by providing the model with both the input information as well as the known output; the goal of training is to construct a network that takes the input information and returns the known output using only the parameters of the model.</p>
			<p>In a supervised classification example such as CIFAR-10, the input information is an image and the known output is the class that the image belongs to. During training, for each sample prediction, the errors in the feedforward network predictions are calculated using a specified error function. Each of the weights within the model is then tuned in an attempt to reduce the error. This tuning process is known as <strong class="keyword">back-propagation</strong> because the error is propagated backward through the network from the output to the start of the network.</p>
			<p>During back-propagation, each trainable weight is adjusted in proportion to its contribution to the overall error multiplied by a value known as the <strong class="keyword">learning rate</strong>, which controls the rate of change in the trainable weights.  Looking at <em class="italics">Figure 5.14</em>, we can see that increasing the value of the learning rate can increase the speed at which the error is reduced, but risks not converging on a minimum error as we step over the values. A learning rate that's too small may lead to us running out of patience or simply not having sufficient time to find the global minimum. Thus, finding the correct learning rate is a trial and error process, though starting with a larger learning rate and reducing it can often be a productive method. The following figure represents the selection of the learning rate:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer154">
					<img alt="Figure 5.14: Selecting the correct learning rate (one epoch is one learning step)" src="image/C12626_05_14.jpg"/>
				</div>
			</div>
			<h6>Figure 5.14: Selecting the correct learning rate (one epoch is one learning step)</h6>
			<p>Training is repeated until the error in the predictions stop reducing or the developer runs out of patience waiting for a result. In order to complete the training process, we first need to make some design decisions, the first being the most appropriate error function. There are a range of error functions available for use, from a simple mean squared difference to more complex options. Categorical cross entropy (which is used in the following exercise) is a very useful error function for classifying more than one class.</p>
			<p>With the error function defined, we need to choose the method of updating the trainable parameters using the error function. One of the most memory-efficient and effective update methods is stochastic gradient descent (SGD); there are a number of variants of SGD, all of which involve adjusting each of the weights in accordance with their individual contribution to the calculated error. The final training design decision to be made is the performance metric by which the model is evaluated and the best architecture selected; in a classification problem, this may be the classification accuracy of the model or perhaps the model that produces the lowest error score in a regression problem. These comparisons are generally made using a method of cross-validation.</p>
			<h3 id="_idParaDest-109"><a id="_idTextAnchor122"/>Exercise 20: Training a Keras Neural Network Model</h3>
			<p>Thankfully, we don't need to worry about manually programming the components of the neural network, such as backpropagation, because the Keras framework manages this for us. In this exercise, we will use Keras to train a neural network to classify a small subset of the CIFAR-10 dataset using the model architecture defined in the previous exercise. As with all machine learning problems, the first and the most important step is to understand as much as possible about the dataset, and this will be the initial focus of the exercise:</p>
			<h4>Note</h4>
			<p class="callout">You can download the <strong class="inline">data_batch_1</strong> and <strong class="inline">batches.meta</strong> files from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20</a>.</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, <strong class="inline">matplotlib</strong> and the <strong class="inline">Sequential</strong> class from <strong class="inline">keras.models</strong>, and import <strong class="inline">Dense</strong> from <strong class="inline">keras.layers</strong>:<p class="snippet">import pickle</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Dense</p></li>
				<li>Load the sample of the CIFAR-10 dataset that is provided with the accompanying source code in the <strong class="inline">data_batch_1</strong> file:<p class="snippet">with open('data_batch_1', 'rb') as f:</p><p class="snippet">    dat = pickle.load(f, encoding='bytes')</p></li>
				<li>The data is loaded as a dictionary. Display the keys of the dictionary:<p class="snippet">dat.keys()</p><p>The output is as follows:</p><p class="snippet">dict_keys([b'batch_label', b'labels', b'data', b'filenames'])</p></li>
				<li>Note that the keys are stored as binary strings as denoted by <strong class="inline">b'</strong>. We are interested in the contents of data and labels. Let's look at labels first:<p class="snippet">labels = dat[b'labels']</p><p class="snippet">labels</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer155"><img alt="" src="image/C12626_05_15.jpg"/></div><h6>Figure 5.15: Displaying the labels</h6></li>
				<li>We can see that the labels are a list of values 0 – 9, indicating which class each sample belongs to.  Now, look at the contents of the <strong class="inline">data</strong> key:<p class="snippet">dat[b'data']</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer156"><img alt="Figure 5.16: Content of the data key" src="image/C12626_05_16.jpg"/></div><h6>Figure 5.16: Content of the data key</h6></li>
				<li>The data key provides a NumPy array with all the image data stored within the array. What is the shape of the image data?<p class="snippet">dat[b'data'].shape</p><p>The output is as follows:</p><p class="snippet">(1000, 3072)</p></li>
				<li>We can see that we have 1000 samples, but each sample is a single dimension of 3,072 samples.  Aren't the images supposed to be 32 x 32 pixels? Yes, they are, but because the images are color or RGB images, they contain three channels (red, green, and blue), which means the images are 32 x 32 x 3. They are also flattened, providing 3,072 length vectors. So, we can reshape the array and then visualize a sample of images. According to the CIFAR-10 documentation, the first 1,024 samples are red, the second 1,024 are green, and the third 1,024 are blue:<p class="snippet">images = np.zeros((10000, 32, 32, 3), dtype='uint8')</p><p class="snippet">for idx, img in enumerate(dat[b'data']):</p><p class="snippet">    images[idx, :, :, 0] = img[:1024].reshape((32, 32)) # Red</p><p class="snippet">    images[idx, :, :, 1] = img[1024:2048].reshape((32, 32)) # Green</p><p class="snippet">    images[idx, :, :, 2] = img[2048:].reshape((32, 32)) # Blue</p></li>
				<li>Display the first 12 images, along with their labels:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for i in range(12):</p><p class="snippet">    plt.subplot(3, 4, i + 1)</p><p class="snippet">    plt.imshow(images[i])</p><p class="snippet">    plt.title(labels[i])</p><p class="snippet">    plt.axis('off')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer157"><img alt="Figure 5.17: The first 12 images" src="image/C12626_05_17.jpg"/></div><h6>Figure 5.17: The first 12 images</h6></li>
				<li>What is the actual meaning of the labels? To find out, load the <strong class="inline">batches.meta</strong> file:<p class="snippet">with open('batches.meta', 'rb') as f:</p><p class="snippet">    label_strings = pickle.load(f, encoding='bytes')</p><p class="snippet">    </p><p class="snippet">label_strings</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer158"><img alt="" src="image/C12626_05_18.jpg"/></div><h6>Figure 5.18: Meaning of the labels</h6></li>
				<li>Decode the binary strings to get the actual labels:<p class="snippet">actual_labels = [label.decode() for label in label_strings[b'label_names']]</p><p class="snippet">actual_labels</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer159"><img alt="" src="image/C12626_05_19.jpg"/></div><h6>Figure 5.19: Printing the actual labels</h6></li>
				<li>Print the labels for the first 12 images:<p class="snippet">for lab in labels[:12]:</p><p class="snippet">    print(actual_labels[lab], end=', ')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer160"><img alt="Figure 5.20: Labels of the first 12 images" src="image/C12626_05_20.jpg"/></div><h6>Figure 5.20: Labels of the first 12 images</h6></li>
				<li>Now we need to prepare the data for training the model. The first step is to prepare the output. Currently, the output is a list of numbers 0 – 9, but we need each sample to be represented as a vector of 10 units as per the previous model. The encoded output will be a NumPy array with a shape of 10000 x 10:<h4>Note</h4><p class="callout">This is known as one hot encoding, where for each sample, there are as many columns as the possible classes, and the identified class is indicated by a 1 in the appropriate column. As an example, say we had the labels [3, 2, 1, 3, 1] with 4 possible classes; the corresponding one hot encoded value would be as follows:</p><p class="callout"><strong class="inline">array([[0., 0., 0., 1.],</strong></p><p class="callout"><strong class="inline">       [0., 0., 1., 0.],</strong></p><p class="callout"><strong class="inline">       [0., 1., 0., 0.],</strong></p><p class="callout"><strong class="inline">       [0., 0., 0., 1 ],</strong></p><p class="callout"><strong class="inline">       [0., 0., 1., 0.]])</strong></p><p class="snippet">one_hot_labels = np.zeros((images.shape[0], 10))</p><p class="snippet">for idx, lab in enumerate(labels):</p><p class="snippet">    one_hot_labels[idx, lab] = 1</p></li>
				<li>Display the one hot encoding values for the first 12 samples:<p class="snippet">one_hot_labels[:12]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer161"><img alt="Figure 5.21: One hot encoding values for first 12 samples" src="image/C12626_05_21.jpg"/></div><h6>Figure 5.21: One hot encoding values for first 12 samples</h6></li>
				<li>The model has 1,024 inputs because it expects a 32 x 32 grayscale image. Take the average of the three channels for each image to convert it to RGB:<p class="snippet">images = images.mean(axis=-1)</p></li>
				<li>Display the first 12 images again:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for i in range(12):</p><p class="snippet">    plt.subplot(3, 4, i + 1)</p><p class="snippet">    plt.imshow(images[i], cmap='gray')</p><p class="snippet">    plt.title(labels[i])</p><p class="snippet">    plt.axis('off')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer162"><img alt="Figure 5.22: Displaying the first 12 images again." src="image/C12626_05_22.jpg"/></div><h6>Figure 5.22: Displaying the first 12 images again.</h6></li>
				<li>Finally, scale the images to be between 0 and 1, which is required for all inputs to a neural network. As the maximum value in an image is 255, we will simply divide by 255:<p class="snippet">images /= 255.</p></li>
				<li>We also need the images to be in the shape 10,000 x 1,024:<p class="snippet">images = images.reshape((-1, 32 ** 2))</p></li>
				<li>Redefine the model with the same architecture as <em class="italics">Exercise 19</em>, <em class="italics">Defining a Keras Model</em>:<p class="snippet">model = Sequential([</p><p class="snippet">    Dense(500, input_shape=(1024,), activation='relu'),</p><p class="snippet">    Dense(10, activation='softmax')</p><p class="snippet">    </p><p class="snippet">])</p></li>
				<li>Now we can train the model in Keras. We first need to compile the method to specify the training parameters. We will be using categorical cross-entropy, with stochastic gradient descent and a performance metric of classification accuracy:<p class="snippet">model.compile(loss='categorical_crossentropy',</p><p class="snippet">              optimizer='sgd',</p><p class="snippet">              metrics=['accuracy'])</p></li>
				<li>Train the model using back-propagation for 100 epochs and the <strong class="inline">fit</strong> method of the model:<p class="snippet">model.fit(images, one_hot_labels, epochs=100)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer163"><img alt="Figure 5.23: Training the model" src="image/C12626_05_23.jpg"/></div><h6>Figure 5.23: Training the model</h6></li>
				<li>We achieved approximately 90% classification accuracy for the 1,000 samples using this network. Examine the predictions made for the first 12 samples again:<p class="snippet">predictions = model.predict(images[:12])</p><p class="snippet">predictions</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer164"><img alt="Figure 5.24: Printing the predictions" src="image/C12626_05_24.jpg"/></div><h6>Figure 5.24: Printing the predictions</h6></li>
				<li>We can use the <strong class="inline">argmax</strong> method to determine the most likely class for each sample:<p class="snippet">np.argmax(predictions, axis=1)</p><p>The output is as follows:</p><p class="snippet">array([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 2, 7])</p></li>
				<li>Compare with the labels:<p class="snippet">labels[:12]</p><p>The output is as follows:</p><p class="snippet">[6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7]</p></li>
			</ol>
			<p>The network made one error in these samples, that is, it classified the second last samples as a 2 (bird) instead of a 4 (deer). Congratulations! You have just successfully trained a neural network model in Keras. Complete the next activity to further reinforce your skills in training neural networks.</p>
			<h3 id="_idParaDest-110">Activity 9: MNIST Neural Net<a id="_idTextAnchor123"/>work</h3>
			<p>In this activity, you will train a neural network to identify images in the MNIST dataset and reinforce your skills in training neural networks. This activity forms the basis of many neural network architectures in different classification problems, particularly in computer vision. From object detection and identification to classification, this general structure is used in a variety of applications.</p>
			<p>These steps will help you complete the activity:</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, <strong class="inline">matplotlib</strong>, and the <strong class="inline">Sequential</strong> and <strong class="inline">Dense</strong> classes from Keras.</li>
				<li>Load the <strong class="inline">mnist.pkl</strong> file that contains the first 10,000 images and the corresponding labels from the MNIST dataset that are available in the accompanying source code. The MNIST dataset is a series of 28 x 28 grayscale images of handwritten digits, 0 through 9. Extract the images and labels.<h4>Note</h4><p class="callout">You can find the <strong class="inline">mnist.pkl</strong> file at <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09</a>.</p></li>
				<li>Plot the first 10 samples along with the corresponding labels.</li>
				<li>Encode the labels using one hot encoding.</li>
				<li>Prepare the images for input into a neural network. As a hint, there are <strong class="bold">two</strong> separate steps in this process.</li>
				<li>Construct a neural network model in Keras that accepts the prepared images and has a hidden layer of 600 units with a ReLU activation function and an output of the same number of units as classes. The output layer uses a <strong class="inline">softmax</strong> activation function.</li>
				<li>Compile the model using multiclass cross-entropy, stochastic gradient descent, and an accuracy performance metric.</li>
				<li>Train the model. How many epochs are required to achieve at least 95% classification accuracy on the training data?</li>
			</ol>
			<p>By completing this activity, you have trained a simple neural network to identify handwritten digits 0 through 9.  You have also developed a general framework for building neural networks for classification problems. With this framework, you can extend upon and modify the network for a range of other tasks.</p>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 335.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor124"/>Autoencoders</h2>
			<p>Now that we are comfortable developing supervised neural network models in Keras, we can return our attention to unsupervised learning and the main subject of this chapter—autoencoders. Autoencoders are a specifically designed neural network architecture that aims to compress the input information into lower dimensional space in an efficient yet descriptive manner. Autoencoder networks can be decomposed into two individual sub-networks or stages: an <strong class="keyword">encoding</strong> stage and a <strong class="keyword">decoding</strong> stage. The first, or encoding, stage takes the input information and compresses it through a subsequent layer that has fewer units than the size of the input sample. The latter stage, that is, the decoding stage, then expands the compressed form of the image and aims to return the compressed data to its original form. As such, the inputs and desired outputs of the network are the same; the network takes, say, an image in the CIFAR-10 dataset and tries to return the same image. This network architecture is shown in <em class="italics">Figure 5.25</em>; in this image, we can see that the encoding stage of the autoencoder reduces the number of neurons to represent the information, while the decoding stage takes the compressed format and returns it to its original state. The use of the decoding stage helps to ensure that the encoder has correctly represented the information, because the compressed representation is all that is provided to restore the image in its original state. We will now work through a simplified autoencoder model using the CIFAR-10 dataset:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer165">
					<img alt="Figure 5.25: Simple autoencoder network architecture" src="image/C12626_05_25.jpg"/>
				</div>
			</div>
			<h6>Figure 5.25: Simple autoencod<a id="_idTextAnchor125"/>er network architecture</h6>
			<h3 id="_idParaDest-112"><a id="_idTextAnchor126"/>Exercise 21: Simple Autoencoder</h3>
			<p>In this exercise, we will construct a simple autoencoder for the sample of the CIFAR-10 dataset, compressing the information stored within the images for later use.</p>
			<h4>Note</h4>
			<p class="callout">You can download the <strong class="inline">data_batch_1</strong> file from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21</a>.</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong>, as well as the <strong class="inline">Model</strong> class from <strong class="inline">keras.models</strong>, and import <strong class="inline">Input</strong> and <strong class="inline">Dense</strong> from <strong class="inline">keras.layers</strong>:<p class="snippet">import pickle</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from keras.models import Model</p><p class="snippet">from keras.layers import Input, Dense</p></li>
				<li>Load the data:<p class="snippet">with open('data_batch_1', 'rb') as f:</p><p class="snippet">    dat = pickle.load(f, encoding='bytes')</p></li>
				<li>As this is an unsupervised learning method, we are only interested in the image data. Load the image data as per the previous exercise:<p class="snippet">images = np.zeros((10000, 32, 32, 3), dtype='uint8')</p><p class="snippet">for idx, img in enumerate(dat[b'data']):</p><p class="snippet">    images[idx, :, :, 0] = img[:1024].reshape((32, 32)) # Red</p><p class="snippet">    images[idx, :, :, 1] = img[1024:2048].reshape((32, 32)) # Green</p><p class="snippet">    images[idx, :, :, 2] = img[2048:].reshape((32, 32)) # Blue</p></li>
				<li>Convert the image to grayscale, scale between 0 and 1, and flatten each to a single 1,024 length vector:<p class="snippet">images = images.mean(axis=-1)</p><p class="snippet">images = images / 255.0</p><p class="snippet">images = images.reshape((-1, 32 ** 2))</p><p class="snippet">images</p></li>
				<li>Define the autoencoder model. As we need access to the output of the encoder stage, we will need to define the model using a slightly different method to that previously used. Define an input layer of <strong class="inline">1024</strong> units:<p class="snippet">input_layer = Input(shape=(1024,))</p></li>
				<li>Define a subsequent <strong class="inline">Dense</strong> layer of <strong class="inline">256</strong> units (a compression ratio of 1024/256 = 4) and a ReLU activation function as the encoding stage. Note that we have assigned the layer to a variable and passed the previous layer to a call method for the class:<p class="snippet">encoding_stage = Dense(256, activation='relu')(input_layer)</p></li>
				<li>Define a subsequent decoder layer using the sigmoid function as an activation function and the same shape as the input layer. The sigmoid function has been selected because the input values to the network are only between 0 and 1:<p class="snippet">decoding_stage = Dense(1024, activation='sigmoid')(encoding_stage)</p></li>
				<li>Construct the model by passing the first and last layers of the network to the <strong class="inline">Model</strong> class:<p class="snippet">autoencoder = Model(input_layer, decoding_stage)</p></li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and adadelta gradient descent:<p class="snippet">autoencoder.compile(loss='binary_crossentropy',</p><p class="snippet">              optimizer='adadelta')</p><h4>Note</h4><p class="callout"><strong class="inline">adadelta</strong> is a more sophisticated version of stochastic gradient descent where the learning rate is adjusted on the basis of a window of recent gradient updates. Compared to the other methods of modifying the learning rate, this prevents the gradient of very old epochs from influencing the learning rate.</p></li>
				<li>Now, let's fit the model; again, we pass the images as the training data and as the desired output.  Train for 100 epochs:<p class="snippet">autoencoder.fit(images, images, epochs=100)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer166"><img alt="Figure 5.26: Training the model" src="image/C12626_05_26.jpg"/></div><h6>Figure 5.26: Training the model</h6></li>
				<li>Calculate and store the output of the encoding stage for the first five samples:<p class="snippet">encoder_output = Model(input_layer, encoding_stage).predict(images[:5])</p></li>
				<li>Reshape the encoder output to 16 x 16 (16 x 16 = 256) pixels and multiply by 255:<p class="snippet">encoder_output = encoder_output.reshape((-1, 16, 16)) * 255</p></li>
				<li>Calculate and store the output of the decoding stage for the first five samples:<p class="snippet">decoder_output = autoencoder.predict(images[:5])</p></li>
				<li>Reshape the output of the decoder to 32 x 32 and multiply by 255:<p class="snippet">decoder_output = decoder_output.reshape((-1, 32,32)) * 255</p></li>
				<li>Reshape the original images:<p class="snippet">images = images.reshape((-1, 32, 32))</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for i in range(5):</p><p class="snippet">    plt.subplot(3, 5, i + 1)</p><p class="snippet">    plt.imshow(images[i], cmap='gray')</p><p class="snippet">    plt.axis('off')</p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 6)</p><p class="snippet">    plt.imshow(encoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')   </p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 11)</p><p class="snippet">    plt.imshow(decoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')        </p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer167">
					<img alt="" src="image/C12626_05_27.jpg"/>
				</div>
			</div>
			<h6>Figure 5.27: Output of simple a<a id="_idTextAnchor127"/>utoencoder</h6>
			<p>In <em class="italics">Figure 5.27</em>, we can see three rows of images. The first row is the original grayscale image, the second row is the corresponding autoencoder output for the original image, and finally, the third row is the reconstruction of the original image from the encoded input. We can see that the decoded images in the third row contain information about the basic shape of the image; we can see the main body of the frog and the deer, as well as the outline of the trucks and cars in the sample. Given that we only trained the model for 100 samples, this exercise would also benefit from an increase in the number of training epochs to further improve the performance of both the encoder and decoder. Now that we have the output of the autoencoder stage trained, we can use it as the feature vector for other unsupervised algorithms, such as K-means or K nearest neighbors.</p>
			<h3 id="_idParaDest-113">Activity 10: Simple MNIST Autoe<a id="_idTextAnchor128"/>ncoder</h3>
			<p>In this activity, you will create an autoencoder network for the MNIST dataset contained within the accompanying source code. An autoencoder network such as the one built in this activity can be an extremely useful in the pre-processing stage of unsupervised learning. The encoded information produced by the network can be used in clustering or segmentation analysis, such as image-based web searches:</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong>, and the <strong class="inline">Model</strong>, <strong class="inline">Input</strong>, and <strong class="inline">Dense</strong> classes from Keras.</li>
				<li>Load the images from the supplied sample of the MNIST dataset that is provided with the accompanying source code (<strong class="inline">mnist.pkl</strong>).<h4>Note</h4><p class="callout">You can download the <strong class="inline">mnist.pklP-code</strong> file from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10</a>.</p></li>
				<li>Prepare the images for input into a neural network. As a hint, there are <strong class="bold">two</strong> separate steps in this process.</li>
				<li>Construct a simple autoencoder network that reduces the image size to 10 x 10 after the encoding stage.</li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and <strong class="inline">adadelta</strong> gradient descent.</li>
				<li>Fit the encoder model.</li>
				<li>Calculate and store the output of the encoding stage for the first five samples.</li>
				<li>Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by 255.</li>
				<li>Calculate and store the output of the decoding stage for the first five samples.</li>
				<li>Reshape the output of the decoder to 28 x 28 and multiply by 255.</li>
				<li>Plot the original image, the encoder output, and the decoder.</li>
			</ol>
			<p>In completing this activity, you will have successfully trained an autoencoder network that extracts the critical information from the dataset, preparing it for later processing. The output will be similar to the following:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer168">
					<img alt="" src="image/C12626_05_28.jpg"/>
				</div>
			</div>
			<h6>Figure 5.28: Expected plot of original image, the encoder output, and the decoder</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 338.</p>
			<h3 id="_idParaDest-114"><a id="_idTextAnchor129"/>Exercise 22: Multi-Layer Autoencoder</h3>
			<p>In this exercise, we will construct a multi-layer autoencoder for the sample of the CIFAR-10 dataset, compressing the information stored within the images for later use:</p>
			<h4>Note</h4>
			<p class="callout">You can download the <strong class="inline">data_batch_1</strong> file from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22</a>.</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong>, as well as the <strong class="inline">Model</strong> class from <strong class="inline">keras.models</strong>, and import <strong class="inline">Input</strong> and <strong class="inline">Dense</strong> from <strong class="inline">keras.layers</strong>:<p class="snippet">import pickle</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from keras.models import Model</p><p class="snippet">from keras.layers import Input, Dense</p></li>
				<li>Load the data:<p class="snippet">with open('data_batch_1', 'rb') as f:</p><p class="snippet">    dat = pickle.load(f, encoding='bytes')</p></li>
				<li>As this is an unsupervised learning method, we are only interested in the image data. Load the image data as per the previous exercise:<p class="snippet">images = np.zeros((10000, 32, 32, 3), dtype='uint8')</p><p class="snippet">for idx, img in enumerate(dat[b'data']):</p><p class="snippet">    images[idx, :, :, 0] = img[:1024].reshape((32, 32)) # Red</p><p class="snippet">    images[idx, :, :, 1] = img[1024:2048].reshape((32, 32)) # Green</p><p class="snippet">    images[idx, :, :, 2] = img[2048:].reshape((32, 32)) # Blue</p></li>
				<li>Convert the image to grayscale, scale between 0 and 1, and flatten each to a single 1,024 length vector:<p class="snippet">images = images.mean(axis=-1)</p><p class="snippet">images = images / 255.0</p><p class="snippet">images = images.reshape((-1, 32 ** 2))</p><p class="snippet">images</p></li>
				<li>Define the multi-layer autoencoder model. We will use the same shape input as the simple autoencoder model:<p class="snippet">input_layer = Input(shape=(1024,))</p></li>
				<li>We will add another layer before the 256 autoencoder stage, this time with 512 neurons:<p class="snippet">hidden_encoding = Dense(512, activation='relu')(input_layer)</p></li>
				<li>Using the same size autoencoder as the previous exercise, but the input to the layer is the <strong class="inline">hidden_encoding</strong> layer this time:<p class="snippet">encoding_stage = Dense(256, activation='relu')(hidden_encoding)</p></li>
				<li>Add a decoding hidden layer:<p class="snippet">hidden_decoding = Dense(512, activation='relu')(encoding_stage)</p></li>
				<li>Use the same output stage as in the previous exercise, this time connected to the hidden decoding stage:<p class="snippet">decoding_stage = Dense(1024, activation='sigmoid')(hidden_decoding)</p></li>
				<li>Construct the model by passing the first and last layers of the network to the <strong class="inline">Model</strong> class:<p class="snippet">autoencoder = Model(input_layer, decoding_stage)</p></li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and <strong class="inline">adadelta</strong> gradient descent:<p class="snippet">autoencoder.compile(loss='binary_crossentropy',</p><p class="snippet">              optimizer='adadelta')</p></li>
				<li>Now, let's fit the model; again, we pass the images as the training data and as the desired output.  Train for 100 epochs:<p class="snippet">autoencoder.fit(images, images, epochs=100)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer169"><img alt="Figure 5.29: Training the model" src="image/C12626_05_29.jpg"/></div><h6>Figure 5.29: Training the model</h6></li>
				<li>Calculate and store the output of the encoding stage for the first five samples:<p class="snippet">encoder_output = Model(input_stage, encoding_stage).predict(images[:5])</p></li>
				<li>Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by 255:<p class="snippet">encoder_output = encoder_output.reshape((-1, 10, 10)) * 255</p></li>
				<li>Calculate and store the output of the decoding stage for the first five samples:<p class="snippet">decoder_output = autoencoder.predict(images[:5])</p></li>
				<li>Reshape the output of the decoder to 28 x 28 and multiply by 255:<p class="snippet">decoder_output = decoder_output.reshape((-1, 28, 28)) * 255</p></li>
				<li>Plot the original image, the encoder output, and the decoder:<p class="snippet">images = images.reshape((-1, 28, 28))</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for i in range(5):</p><p class="snippet">    plt.subplot(3, 5, i + 1)</p><p class="snippet">    plt.imshow(images[i], cmap='gray')</p><p class="snippet">    plt.axis('off')</p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 6)</p><p class="snippet">    plt.imshow(encoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')   </p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 11)</p><p class="snippet">    plt.imshow(decoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')      </p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer170">
					<img alt="Figure 5.30: Output of multi-layer autoencoder" src="image/C12626_05_30.jpg"/>
				</div>
			</div>
			<h6>Figure 5.30: Output of multi-layer<a id="_idTextAnchor130"/> autoencoder</h6>
			<p>By looking at the error score produced by both the simple and multilayer autoencoders and by comparing <em class="italics">Figure 5.27</em><a href="C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor127"/> and <em class="italics">Figure 5.30</em>, we can see that there is little difference between the output of the two encoder structures. The middle row of both figures show that the features learned by the two models are, in fact, different. There are a number of options we can use to improve both of these models, such as training for more epochs, using a different number of units or neurons in the layers, or using varying numbers of layers. This exercise was constructed to demonstrate how to build and use an autoencoder, but optimization is often a process of systematic trial and error. We encourage you to adjust some of the parameters of the model and investigate the different results for yourself.</p>
			<h3 id="_idParaDest-115"><a id="_idTextAnchor131"/>Convolutional Neural Networks</h3>
			<p>In constructing all of our previous neural network models, you would have noticed that we removed all the color information when converting the image to grayscale, and then flattened each image into a single vector of length 1,024. In doing so, we essentially threw out a lot of information that may be of use to us. The colors in the images may be specific to the class or objects in the image; additionally, we lost a lot of our spatial information about the image, for example, the position of the trailer in the truck image relative to the cab or the legs of the deer relative to the head. Convolutional neural networks do not suffer from this information loss. This is because rather than using a flat structure of trainable parameters, they store the weights in a grid or matrix, which means that each group of parameters can have many layers in their structure. By organizing the weights in a grid, we prevent the loss of spatial information because the weights are applied in a sliding fashion across the image. Also, by having many layers, we can retain the color channels associated with the image.</p>
			<p>In developing convolutional neural-network-based autoencoders, the MaxPooling2D and Upsampling2D layers are very important. The MaxPooling 2D layer downsamples or reduces the size of an input matrix in two dimensions by selecting the maximum value within a window of the input. Say we had a 2 x 2 matrix, where three cells have a value of 1 and one single cell has a value of 2:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer171">
					<img alt="Figure 5.31: Demonstration of sample matrix" src="image/C12626_05_31.jpg"/>
				</div>
			</div>
			<h6>Figure 5.31: Demonstration of sample matrix</h6>
			<p>If provided to the MaxPooling2D layer, this matrix would return a single value of 2, thus reducing the size of the input in both directions by one half.</p>
			<p>The UpSampling2D layer has the opposite effect as that of the MaxPooling2D layer, increasing the size of the input rather than reducing it. The upsampling process repeats the rows and columns of the data, thus doubling the size of the input matrix.</p>
			<h3 id="_idParaDest-116"><a id="_idTextAnchor132"/>Exercise 23: Convolutional Autoencoder</h3>
			<p>In this exercise, we will develop a convolutional neural-network-based autoencoder and compare the performance to the previous fully-connected neural network autoencoder:</p>
			<h4>Note</h4>
			<p class="callout">You can download the <strong class="inline">data_batch_1</strong> file from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23</a>.</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong>, as well as the <strong class="inline">Model</strong> class from <strong class="inline">keras.models</strong>, and import <strong class="inline">Input</strong>, <strong class="inline">Conv2D</strong>, <strong class="inline">MaxPooling2D</strong>, and <strong class="inline">UpSampling2D</strong> from <strong class="inline">keras.layers</strong>:<p class="snippet">import pickle</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from keras.models import Model</p><p class="snippet">from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D</p></li>
				<li>Load the data:<p class="snippet">with open('data_batch_1', 'rb') as f:</p><p class="snippet">    dat = pickle.load(f, encoding='bytes')</p></li>
				<li>As this is an unsupervised learning method, we are only interested in the image data. Load the image data as per the previous exercise:<p class="snippet">images = np.zeros((10000, 32, 32, 3), dtype='uint8')</p><p class="snippet">for idx, img in enumerate(dat[b'data']):</p><p class="snippet">    images[idx, :, :, 0] = img[:1024].reshape((32, 32)) # Red</p><p class="snippet">    images[idx, :, :, 1] = img[1024:2048].reshape((32, 32)) # Green</p><p class="snippet">    images[idx, :, :, 2] = img[2048:].reshape((32, 32)) # Blue</p></li>
				<li>As we are using a convolutional network, we can use the images with only rescaling:<p class="snippet">images = images / 255.</p></li>
				<li>Define the convolutional autoencoder model. We will use the same shape input as an image:<p class="snippet">input_layer = Input(shape=(32, 32, 3,))</p></li>
				<li>Add a convolutional stage with 32 layers or filters, a 3 x 3 weight matrix, a ReLU activation function, and using the same padding, which means the output has the same length as the input image:<h4>Note</h4><p class="callout">Conv2D convolutional layers are the two-dimensional equivalent of weights in a fully-connected neural network. The weights exist in a series of 2D weight filters or layers, which are then convolved with the input of the layer.</p><p class="snippet">hidden_encoding = Conv2D(</p><p class="snippet">    32, # Number of layers or filters in the weight matrix</p><p class="snippet">    (3, 3), # Shape of the weight matrix</p><p class="snippet">    activation='relu',</p><p class="snippet">    padding='same', # How to apply the weights to the images</p><p class="snippet">)(input_layer)</p></li>
				<li>Add a max pooling layer to the encoder with a 2 x 2 kernel. <strong class="inline">MaxPooling</strong> looks at all the values in an image, scanning through with a 2 x 2 matrix. The maximum value in each 2 x 2 area is returned, thus reducing the size of the encoded layer by a half:<p class="snippet">encoded = MaxPooling2D((2, 2))(hidden_encoding)</p></li>
				<li>Add a decoding convolutional layer (this layer should be identical to the previous convolutional layer):<p class="snippet">hidden_decoding = Conv2D(</p><p class="snippet">    32, # Number of layers or filters in the weight matrix</p><p class="snippet">    (3, 3), # Shape of the weight matrix</p><p class="snippet">    activation='relu',</p><p class="snippet">    padding='same', # How to apply the weights to the images</p><p class="snippet">)(encoded)</p></li>
				<li>Now we need to return the image to its original size, for which we will upsample by the same size as <strong class="inline">MaxPooling2D</strong>:<p class="snippet">upsample_decoding = UpSampling2D((2, 2))(hidden_decoding)</p></li>
				<li>Add the final convolutional stage using three layers for the RGB channels of the images:<p class="snippet">decoded = Conv2D(</p><p class="snippet">    3, # Number of layers or filters in the weight matrix</p><p class="snippet">    (3, 3), # Shape of the weight matrix</p><p class="snippet">    activation='sigmoid',</p><p class="snippet">    padding='same', # How to apply the weights to the images</p><p class="snippet">)(upsample_decoding)</p></li>
				<li>Construct the model by passing the first and last layers of the network to the <strong class="inline">Model</strong> class:<p class="snippet">autoencoder = Model(input_layer, decoded)</p></li>
				<li>Display the structure of the model:<p class="snippet">autoencoder.summary()</p><p>Note that we have far fewer trainable parameters as compared to the previous autoencoder examples.  This has been a specific design decision to ensure that the example runs on a wide variety of hardware.  Convolutional networks typically require a lot more processing power and often special hardware such as Graphical Processing Units (GPUs).</p></li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and <strong class="inline">adadelta</strong> gradient descent:<p class="snippet">autoencoder.compile(loss='binary_crossentropy',</p><p class="snippet">              optimizer='adadelta')</p></li>
				<li>Now, let's fit the model; again, we pass the images as the training data and as the desired output.  Train for 20 epochs, because convolutional networks take a lot longer to compute:<p class="snippet">autoencoder.fit(images, images, epochs=20)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer172"><img alt="Figure 5.32: Training the model" src="image/C12626_05_32.jpg"/></div><h6>Figure 5.32: Training the model</h6><p>Note that the error was already less than in the previous autoencoder exercise after the second epoch, suggesting a better encoding/decoding model. This reduced error can be mostly attributed to the fact that the convolutional neural network did not discard a lot of data, and the encoded images are 16 x 16 x 32, which is significantly larger than the previous 16 x 16 size. Additionally, we have not compressed the images per se as they now contain fewer pixels (16 x 16 x 32 = 8,192), but with more depth (32 x 32 x 3,072) than before. This information has been rearranged to allow more effective encoding/decoding processes.</p></li>
				<li>Calculate and store the output of the encoding stage for the first five samples:<p class="snippet">encoder_output = Model(input_layer, encoded).predict(images[:5])</p></li>
				<li>Each encoded image has a shape of 16 x 16 x 32 due to the number of filters selected for the convolutional stage. As such, we cannot visualize them without modification. We will reshape them to be 256 x 32 in size for visualization:<p class="snippet">encoder_output = encoder_output.reshape((-1, 256, 32))</p></li>
				<li>Get the output of the decoder for the first five images:<p class="snippet">decoder_output = autoencoder.predict(images[:5])</p></li>
				<li>Plot the original image, the mean encoder output, and the decoder:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for i in range(5):</p><p class="snippet">    plt.subplot(3, 5, i + 1)</p><p class="snippet">    plt.imshow(images[i], cmap='gray')</p><p class="snippet">    plt.axis('off')</p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 6)</p><p class="snippet">    plt.imshow(encoder_output[i], cmap='gray')</p><p class="snippet">    plt.axis('off')   </p><p class="snippet">    </p><p class="snippet">    plt.subplot(3, 5, i + 11)</p><p class="snippet">    plt.imshow(decoder_output[i])</p><p class="snippet">    plt.axis('off')    </p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer173">
					<img alt="Figure 5.33: The original image, the encoder output, and the decoder" src="image/C12626_05_33.jpg"/>
				</div>
			</div>
			<h6>Figure 5.33: The original image, the encoder output, and the decoder</h6>
			<h3 id="_idParaDest-117">Activity 11: MNIST Convolutional Auto<a id="_idTextAnchor133"/>encoder</h3>
			<p>In this activity, we will reinforce our knowledge of convolutional autoencoders using the MNIST dataset. Convolutional autoencoders typically achieve significantly improved performance when working with image-based datasets of a reasonable size. This is particularly useful when using autoencoders to generate artificial image samples:</p>
			<ol>
				<li value="1">Import <strong class="inline">pickle</strong>, <strong class="inline">numpy</strong>, and <strong class="inline">matplotlib</strong>, as well as the <strong class="inline">Model</strong> class from <strong class="inline">keras.models</strong>, and import <strong class="inline">Input</strong>, <strong class="inline">Conv2D</strong>, <strong class="inline">MaxPooling2D</strong>, and <strong class="inline">UpSampling2D</strong> from <strong class="inline">keras.layers</strong>.</li>
				<li>Load the <strong class="inline">mnist.pkl</strong> file, which contains the first 10,000 images and corresponding labels from the MNIST dataset, which are available in the accompanying source code.<h4>Note</h4><p class="callout">You can download the <strong class="inline">mnist.pkl</strong> file from <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11</a>.</p></li>
				<li>Rescale the images to have values between 0 and 1.</li>
				<li>We need to reshape the images to add a single depth channel for use with convolutional stages. Reshape the images to have a shape of 28 x 28 x 1.</li>
				<li>Define an input layer. We will use the same shape input as an image.</li>
				<li>Add a convolutional stage, with 16 layers or filters, a 3 x 3 weight matrix, a ReLU activation function, and using same padding, which means the output has the same length as the input image.</li>
				<li>Add a max pooling layer to the encoder with a 2 x 2 kernel.</li>
				<li>Add a decoding convolutional layer.</li>
				<li>Add an upsampling layer.</li>
				<li>Add the final convolutional stage using 1 layer as per the initial image depth.</li>
				<li>Construct the model by passing the first and last layers of the network to the <strong class="inline">Model</strong> class.</li>
				<li>Display the structure of the model.</li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and <strong class="inline">adadelta</strong> gradient descent.</li>
				<li>Now, let's fit the model; again, we pass the images as the training data and as the desired output.  Train for 20 epochs as convolutional networks take a lot longer to compute.</li>
				<li>Calculate and store the output of the encoding stage for the first five samples.</li>
				<li>Reshape the encoder output for visualization, where each image is X*Y in size.</li>
				<li>Get the output of the decoder for the first five images.</li>
				<li>Reshape the decoder output to be 28 x 28 in size.</li>
				<li>Reshape the original images back to be 28 x 28 in size.</li>
				<li>Plot the original image, the mean encoder output, and the decoder.</li>
			</ol>
			<p>At the end of this activity, you will have developed an autoencoder comprising convolutional layers within the neural network. Note the improvements made in the decoder representations. The output will be similar to the following:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer174">
					<img alt="" src="image/C12626_05_34.jpg"/>
				</div>
			</div>
			<h6>Figure 5.34: Expected original image, the encoder output, and the decoder</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 340.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor134"/>Summary</h2>
			<p>In this chapter, we started with an introduction to artificial neural networks, how they are structured, and the processes by which they learn to complete a particular task. Starting with a supervised learning example, we built an artificial neural network classifier to identify objects within the CIFAR-10 dataset. We then progressed to the autoencoder architecture of neural networks and learned how we can use these networks to prepare a dataset for use in an unsupervised learning problem. Finally, we completed this investigation with autoencoders, looking at convolutional neural networks and the benefits these additional layers can provide. This chapter prepared us well for the final instalment in dimensionality reduction, as we look at using and visualizing the encoded data with t-distributed nearest neighbors (t-SNE). T-distributed nearest neighbors provides an extremely effective method of visualizing high-dimensional data even after applying reduction techniques such as PCA. T-SNE is particularly useful method for unsupervised learning.</p>
		</div>
	</body></html>
<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Linear Models – Logistic Regression</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Loading data from the UCI repository</li>
<li>Viewing the Pima Indians diabetes dataset with pandas</li>
<li>Looking at the UCI Pima Indians dataset web page</li>
<li>Machine learning with logistic regression</li>
<li>Examining logistic regression errors with a confusion matrix</li>
<li>Varying the classification threshold in logistic regression</li>
<li>Receiver operating characteristic – ROC analysis</li>
<li>Plotting an ROC curve without context</li>
<li>Putting it all together – UCI<span> b</span><span class="heading">reast cancer dataset</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Linear regression is a very old method and part of traditional statistics. <em>Machine learning linear regression</em> involves a training and testing set. This way, it can be compared by utilizing <em>cross-validation</em> with other models and algorithms. <em>Traditional linear regression</em> trains and tests on the whole dataset. This is still a common practice, possibly because linear regression tends to underfit rather than overfit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using linear methods for classification – logistic regression</h1>
                </header>
            
            <article>
                
<p>As seen in <a href="9a5af114-e518-47ef-ac63-edf9ae69384c.xhtml" target="_blank">Chapter 1</a>, <em>High-Performance Machine Learning – NumPy</em>, logistic regression is a classification method. In some contexts, it is a regressor as it computes the real number probability of a class before assigning a categorical classification prediction. With this in mind, let's explore the Pima Indians diabetes dataset provided by the <span class="st"><strong>University of California, Irvine</strong> (</span><strong>UCI</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading data from the UCI repository</h1>
                </header>
            
            <article>
                
<p>The first dataset we will load is the Pima Indians diabetes dataset. This will require access to the internet. The dataset is available thanks to Sigillito V. (1990), UCI machine learning repository (<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data</a>), Laurel, MD at Johns Hopkins University, applied physics laboratory.</p>
<div class="packt_infobox">The first thing in your mind if you are an open source veteran is, what is the license/permission to this database? This is a very important issue. The UCI repository has a use policy that requires citation of the database whenever we are using it. We are allowed to use it but we must give them proper credit for their great help and provide a citation.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Go to IPython and import <kbd>pandas</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>import pandas as pd</strong></pre>
<ol start="2">
<li>Type the web location of the Pima Indians diabetes dataset as a string as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>data_web_address = "https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"</strong></pre>
<ol start="3">
<li>Type the column names of the data in a list:</li>
</ol>
<pre style="padding-left: 60px"><strong>column_names = ['pregnancy_x', </strong><br/><strong>                'plasma_con', </strong><br/><strong>                'blood_pressure', </strong><br/><strong>                'skin_mm', </strong><br/><strong>                'insulin', </strong><br/><strong>                'bmi', </strong><br/><strong>                'pedigree_func', </strong><br/><strong>                'age', </strong><br/><strong>                'target']</strong></pre>
<ol start="4">
<li>Store the feature names as a list. Exclude the <kbd>target</kbd> column, the last column name in <kbd>column_names</kbd>, because it is not a feature:</li>
</ol>
<pre style="padding-left: 60px"><strong>feature_names = column_names[:-1]</strong></pre>
<ol start="5">
<li>Make a pandas dataframe to store the input data:</li>
</ol>
<pre style="padding-left: 60px"><strong>all_data = pd.read_csv(data_web_address , names=column_names)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Viewing the Pima Indians diabetes dataset with pandas</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>You can view the data in various ways. View the top of the dataframe:</li>
</ol>
<pre style="padding-left: 60px"><strong>all_data.head()</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img height="147" width="558" src="assets/85474218-fbe0-41de-9f83-1595cc9f4845.png"/></div>
<ol start="2">
<li>Nothing seems amiss here, except possibly an insulin level of zero. Is this possible? What about the <kbd>skin_mm</kbd><span> variable? C</span><span>an that be zero? Make a note about it as a comment in your IPython:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>#Is an insulin level of 0 possible? Is a skin_mm of 0 possible?</strong></pre>
<ol start="3">
<li>Get a rough overview of the dataframe with the <kbd>describe()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px"><strong>all_data.describe()</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/225b593a-a7f2-463b-a074-b2e258260e52.png"/></div>
<ol start="4">
<li>Make a note again in your notebook about additional zeros:</li>
</ol>
<pre style="padding-left: 60px"><strong>#The features plasma_con, blood_pressure, skin_mm, insulin, bmi have 0s as values. These values could be physically impossible.</strong></pre>
<ol start="5">
<li>Draw a histogram of the <kbd>pregnancy_x</kbd> variable. Set the <kbd>hist()</kbd> method variable bins equal to 50 for more bins and a higher resolution in the image; otherwise, the image is hard to read:</li>
</ol>
<pre style="padding-left: 60px"><strong>#If within a notebook, include this line to visualize within the notebook.</strong><br/><strong>%matplotlib inline</strong><br/><br/><strong>#The default is bins=10 which is hard to read in the visualization.</strong><br/><strong>all_data.pregnancy_x.hist(bins=50)</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3cd07e63-b559-4998-9f18-3b7f3c682de7.png"/></div>
<ol start="6">
<li>Make histograms for all columns of the dataframe. Change <kbd>figsize</kbd> within the method to the tuple <kbd>(15,9)</kbd> and bins to <kbd>50</kbd> again; otherwise, it is hard to read the image:</li>
</ol>
<pre style="padding-left: 60px"><strong>all_data.hist(figsize=(15,9),bins=50)</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5aaa8355-0f01-49a8-aa4a-df447d30b05c.png"/></div>
<div class="packt_infobox"><kbd>blood_pressure</kbd> and <kbd>bmi</kbd> look like normal distributions aside from the anomalous zeros.</div>
<p style="padding-left: 60px">The <kbd>pedigree_func</kbd> and <kbd>plasma_con</kbd> <span>variables </span><span>are skewed-normal (possibly log-normal). The <kbd>age</kbd> and <kbd>pregnancy_x</kbd> </span><span>variables </span><span>are decaying in some way. The insulin and <kbd>skin_mm</kbd> </span><span>variables</span><span> </span><span>look like they could be normally distributed except for the many values of zero.</span></p>
<ol start="7">
<li>Finally, note the class imbalance in the <kbd>target</kbd> variable. Reexamine that imbalance with the <kbd>value_counts()</kbd> pandas series method:</li>
</ol>
<pre style="padding-left: 60px"><strong>all_data.target.value_counts()</strong><br/><br/><strong>0    500</strong><br/><strong>1    268</strong><br/><strong>Name: target, dtype: int64</strong></pre>
<p>There are more cases where the person is described by category zero instead of category one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at the UCI Pima Indians dataset web page</h1>
                </header>
            
            <article>
                
<p>We did some exploratory analysis to get a rough understanding of the data. Now we will read the UCI Pima Indians dataset documentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">View the citation policy</h1>
                </header>
            
            <article>
                
<ol>
<li>Go to <a href="https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes">https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes</a>.</li>
<li>Here is all the information about the UCI Pima Indians diabetes dataset. First, scroll down to the bottom of the page and look at their citation policy. The diabetes dataset has the general UCI citation policy available at; <a href="https://archive.ics.uci.edu/ml/citation_policy.html">https://archive.ics.uci.edu/ml/citation_policy.html</a>.</li>
</ol>
<ol start="3">
<li>The general policy says that to publish material using the dataset, please cite the UCI repository.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Read about missing values and context</h1>
                </header>
            
            <article>
                
<ol start="4">
<li>The top of the page has important links and an abstract of the dataset. The abstract mentions there are missing values in the dataset:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="175" width="619" src="assets/d6af13eb-bcc7-471d-b8ad-f10be76a4503.png"/></div>
<ol start="5">
<li>Below the abstract, there is a description of the attributes (this is how I came up with the names for the columns at the beginning):</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="174" width="383" src="assets/20fa3278-8e3b-4085-ae7b-0c97cb891847.png"/></div>
<ol start="6">
<li>What do the class variables mean anyway? What does the zero or one mean in the target? To figure this out, click on the <span class="packt_screen">Data Set Description</span> link above the abstract. Scroll to point nine on the page, which yields the desired information:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="84" width="538" src="assets/67a2dc7c-d858-437a-815d-dcaa29a1f815.png"/></div>
<p style="padding-left: 60px">This means that a <span class="packt_screen">1</span> refers to a positive for diabetes. This is important information and with regard to this data analysis, provides a context.</p>
<ol start="7">
<li>Finally, there is a disclaimer noting that: <span class="packt_screen">As pointed out by a repository user, this cannot be true: there are zeros in places where they are biologically impossible, such as the blood pressure attribute. It seems very likely that zero values encode missing data.</span></li>
</ol>
<p>Thus, we were right in suspecting some of the impossible zeros in the data exploration phase. Many datasets have corrupt or missing values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning with logistic regression</h1>
                </header>
            
            <article>
                
<p>You are familiar with the steps of training and testing a classifier. With logistic regression, we will do the following:</p>
<ul>
<li>Load data into feature and target arrays, <kbd>X</kbd> and <kbd>y</kbd>, respectively</li>
<li>Split the data into training and testing sets</li>
<li>Train the logistic regression classifier on the training set</li>
<li>Test the performance of the classifier on the test set</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Define X, y – the feature and target arrays</h1>
                </header>
            
            <article>
                
<p>Let's start predicting with scikit-learn's logistic regression. Perform the necessary imports and set the input variables X and the target variable <kbd>y</kbd>:</p>
<pre><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><br/><strong>X = all_data[feature_names]</strong><br/><strong>y = all_data['target']</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Provide training and testing sets</h1>
                </header>
            
            <article>
                
<ol>
<li>Import <kbd>train_test_split</kbd> to create testing and training sets for both <kbd>X</kbd> and <kbd>y</kbd>: the inputs and target. Note the <kbd>stratify=y</kbd>, which stratifies the categorical variable <kbd>y</kbd>. This means that there are the same proportions of zeros and ones in both <kbd>y_train</kbd> and <kbd>y_test</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Train the logistic regression</h1>
                </header>
            
            <article>
                
<ol start="2">
<li>Now import the <kbd>LogisticRegression</kbd> and fit it to the training data:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.linear_model import LogisticRegression</strong><br/><strong>lr = LogisticRegression()</strong><br/><strong>lr.fit(X_train,y_train)</strong></pre>
<ol start="3">
<li>Make a prediction on the test set and store it as <kbd>y_pred</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>y_pred = lr.predict(X_test)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Score the logistic regression</h1>
                </header>
            
            <article>
                
<ol start="4">
<li>Check the accuracy of the prediction with <kbd>accuracy_score</kbd>, the percentage of classifications that are correct:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.metrics import accuracy_score</strong><br/><strong>accuracy_score(y_test,y_pred)</strong><br/><br/><strong>0.74675324675324672</strong></pre>
<p>So, we have obtained a score, but is this score the best measure we can use under these circumstances? Can we do better? Perhaps yes. Look at a confusion matrix of the results as follows.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examining logistic regression errors with a confusion matrix</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Import and view the confusion matrix for the logistic regression we constructed:</p>
<pre><strong>from sklearn.metrics import confusion_matrix</strong><br/><strong>confusion_matrix(y_test, y_pred,labels = [1,0])</strong><br/><br/><strong>array([[27, 27],</strong><br/><strong>       [12, 88]])</strong></pre>
<p>I passed three arguments to the confusion matrix:</p>
<ul>
<li><kbd>y_test</kbd>: The test target set</li>
<li><kbd>y_pred</kbd>: Our logistic regression predictions</li>
<li><kbd>labels</kbd>: References to a positive class</li>
</ul>
<p>The <kbd>labels = [1,0]</kbd> means that the positive class is <kbd>1</kbd> and the negative class is <kbd>0</kbd>. In the medical context, we found while exploring the Pima Indians diabetes dataset that class <kbd>1</kbd> tested positive for diabetes.</p>
<p>Here is the confusion matrix, again in pandas dataframe form:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e53803a3-fc60-4560-93a4-ac3b7e6cfbad.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading the confusion matrix</h1>
                </header>
            
            <article>
                
<p>The small array of numbers has the following meaning:</p>
<div class="CDPAlignCenter CDPAlign"><img height="218" width="303" src="assets/9f6802ef-d273-48e3-8bd3-1503d8a707db.png"/></div>
<p>The confusion matrix tells us a bit more about what occurred during classification, not only the accuracy score. The diagonal elements from upper-left to lower-right are correct classifications. There were 27 + 88 = 115 correct classifications. Off that diagonal, 27 + 12 = 39 mistakes were made in classification. Note that 115 / (115 + 39) yields the classifier accuracy again, of about 0.75.</p>
<p>Let us focus on the errors again. In the confusion matrix, 27 people were labelled as not having diabetes although they do. In a real-life context, this is a worse error than those who were thought to have diabetes but did not. The first set might go home in real-life and forget while the second set might be retested.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General confusion matrix in context</h1>
                </header>
            
            <article>
                
<p>A general confusion matrix where the positive class refers to identifying a condition (diabetes in this case) thereby having a medical diagnosis context:</p>
<div class="CDPAlignCenter CDPAlign"><img height="126" width="283" src="assets/f51675e3-8550-44cc-a45b-398e68a110ba.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Varying the classification threshold in logistic regression</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will use the fact that underlying the logistic regression classification, there is regression to minimize the number of times people were sent home for not having diabetes although they do. Do so by calling the <kbd>predict_proba()</kbd> method of the estimator:</p>
<pre><strong>y_pred_proba = lr.predict_proba(X_test)</strong></pre>
<p>This yields an array of probabilities. View the array:</p>
<pre><strong>y_pred_proba</strong><br/><br/><strong>array([[ 0.87110309,  0.12889691],</strong><br/><strong>       [ 0.83996356,  0.16003644],</strong><br/><strong>       [ 0.81821721,  0.18178279],</strong><br/><strong>       [ 0.73973464,  0.26026536],</strong><br/><strong>       [ 0.80392034,  0.19607966], ...</strong></pre>
<p>In the first row, a probability of about 0.87 is assigned to class <kbd>0</kbd> and a probability of 0.13 is assigned to <kbd>1</kbd>. Note that, as probabilities, these numbers add up to 1. Because there are only two categories, this result can be viewed as a regressor, a real number that talks about the probability of the class being <kbd>1</kbd> (or <kbd>0</kbd>). Visualize the probabilities of the class being <kbd>1</kbd> with a histogram.</p>
<p>Take the second column of the array, turn it into a pandas series, and draw a histogram:</p>
<pre><strong>pd.Series(y_pred_proba[:,1]).hist()</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img height="198" width="288" src="assets/910ab5af-85bb-40e2-8268-f2e3af798570.png"/></div>
<p>In the probability histogram, high probabilities in regards to selecting 1 are fewer than low probabilities. For example, often the probability of selecting 1 is from 0.1 to 0.2. Within logistic regression, the algorithm will pick 1 <span>by default </span><span>only if the probability is greater than 0.5 or half. Now, contrast this with the target histogram at the beginning:</span></p>
<pre><strong>all_data['target'].hist()</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img height="199" width="296" src="assets/5d263257-e2ab-46a9-86e9-bd06f35db1ef.png"/></div>
<p>In the following recipe, we will:</p>
<ul>
<li>Call the class method <kbd>y_pred_proba()</kbd></li>
<li>Use the <kbd>binarize</kbd> function with a specific threshold</li>
<li>Look at the confusion matrix that is generated by the threshold</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol start="1">
<li>To select the classification class based on a threshold, use <kbd>binarize</kbd> from the <kbd>preprocessing</kbd> module. First import it:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.preprocessing import binarize</strong></pre>
<ol start="2">
<li>Look at the first two columns of <kbd>y_pred_proba</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>array([[ 0.87110309,  0.12889691],</strong><br/><strong>       [ 0.83996356,  0.16003644]</strong></pre>
<ol start="3">
<li>Then try <kbd>binarize</kbd> function on <kbd>y_pred_proba</kbd> with a threshold of <kbd>0.5</kbd>. View the result:</li>
</ol>
<pre style="padding-left: 60px"><strong>y_pred_default = binarize(y_pred_proba, threshold=0.5)</strong><br/><strong>y_pred_default</strong><br/><br/><strong>array([[ 1.,  0.],</strong><br/><strong>       [ 1.,  0.],</strong><br/><strong>       [ 1.,  0.],</strong><br/><strong>       [ 1.,  0.],</strong><br/><strong>       [ 1.,  0.],</strong><br/><strong>       [ 1.,  0.]</strong></pre>
<ol start="4">
<li>The <kbd>binarize</kbd> function fills the array with <kbd>1</kbd> if values in <kbd>y_pred_proba</kbd> are greater than 0.5; otherwise it places a <kbd>0</kbd>. In the first row, 0.87 is greater than 0.5 while 0.13 is not. So to binarize, replace the 0.87 with a <kbd>1</kbd> and the 0.13 with a <kbd>0</kbd>. Now, take the first column of <kbd>y_pred_default</kbd>. View it:</li>
</ol>
<pre style="padding-left: 60px"><strong>y_pred_default[:,1]</strong><br/><br/><strong>array([ 0.,  0.,  0.,  0.,  0.,  0 ...</strong></pre>
<p style="padding-left: 60px">This recovers the decisions made by the default logistic regression classifier with threshold <kbd>0.5</kbd>.</p>
<ol start="5">
<li>Trying the confusion matrix function on the NumPy array yields the first confusion matrix we encountered (note that the labels are chosen to be <kbd>[1,0]</kbd> again):</li>
</ol>
<pre style="padding-left: 60px"><strong>confusion_matrix(y_test, y_pred_default[:,1],labels = [1,0])</strong><br/><br/><strong>array([[27, 27],</strong><br/><strong>       [12, 88]])</strong></pre>
<ol start="6">
<li>Try a different threshold so that class <kbd>1</kbd> has a better chance of being selected. View its confusion matrix:</li>
</ol>
<pre style="padding-left: 60px"><strong>y_pred_low = binarize(y_pred_proba, threshold=0.2)</strong><br/><strong>confusion_matrix(y_test, y_pred_low[:,1],labels=[1,0]) #positive class is 1 again</strong><br/><br/><strong>array([[50,  4],</strong><br/><strong>       [48, 52]])</strong></pre>
<p>By changing the threshold, we increased the likelihood of predicting class <kbd>1</kbd>—increasing the size of the numbers in the first column of the confusion matrix. The first column now adds up to 50 + 48 = 98. Before, the first column was 27 + 12 = 39, a much lower number. Now only four people were classified to not have diabetes although they do. Note that this is a good thing in some contexts.</p>
<p>When the algorithm predicts zero, it is likely to be correct. When it predicts one, it tends to not work. Suppose you run a hospital. You might like this test because you rarely send someone home believing they do not have diabetes although they do. Whenever you send people home who do have diabetes, they cannot be treated earlier and incur greater costs to the hospital, insurance companies, and themselves.</p>
<p>You can measure the accuracy of the test when it predicts zero. Observe the second column of the confusion matrix, [4, 52]. In this situation, it is 52 / (52 + 4) or about 0.93 accurate. This is called the <strong>negative predictive value</strong> (<strong>NPV</strong>). You can write a function to calculate NPV-based on the threshold:</p>
<pre><strong>from __future__ import division #In Python 2.x</strong><br/><strong>import matplotlib.pyplot as plt</strong><br/><br/><strong>def npv_func(th):</strong><br/><strong>    y_pred_low = binarize(y_pred_proba, threshold=th)</strong><br/><br/><strong>    second_column = confusion_matrix(y_test, y_pred_low[:,1],labels=[1,0])[:,1]</strong><br/><strong>    npv = second_column[1]/second_column.sum()</strong><br/><strong>    return npv</strong><br/><br/><strong>npv_func(0.2)</strong><br/><br/><strong>0.9285714285714286</strong></pre>
<p>Then plot it:</p>
<pre><strong>ths = np.arange(0,1,0.05)</strong><br/><br/><strong>npvs = []</strong><br/><strong>for th in np.arange(0,1.00,0.05):</strong><br/><strong>    npvs.append(npv_func(th)) </strong><br/>    <br/><strong>plt.plot(ths,npvs)</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/be4025da-47f3-4060-9590-8b888bb16644.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Receiver operating characteristic – ROC analysis</h1>
                </header>
            
            <article>
                
<p>Along the same lines of examining NPV, there are standard measures that examine cells within a confusion matrix.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sensitivity</h1>
                </header>
            
            <article>
                
<p>Sensitivity, like NPV in the previous section, is a mathematical function of the confusion matrix cells. Sensitivity is the proportion of people who took the test with a condition and were correctly labeled as having the condition, diabetes in this case:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="34" width="349" class="fm-editor-equation" src="assets/29bbe2b5-5c8d-4b78-bc12-bbc736ff415d.png"/></div>
<p>Mathematically, it is the ratio of patients correctly labeled as having a condition (TP) divided by the total number of people who actually have the condition (TP + FN). First, recall the confusion matrix cells. Focus on the <strong>Truth</strong> row, which corresponds to <em>all people who have diabetes</em>:<strong><br/></strong></p>
<div class="CDPAlignCenter CDPAlign"><img height="133" width="292" src="assets/43d62022-e905-4a75-94aa-8f392ffee870.png"/></div>
<p>Consequently:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="38" width="329" class="fm-editor-equation" src="assets/65760563-8d44-4d1b-8d30-ef81a9bf88f1.png"/></div>
<p>Another name for sensitivity is <strong>true positive rate</strong> (<strong>TPR</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A visual perspective</h1>
                </header>
            
            <article>
                
<p>Another perspective on the confusion matrix is very visual. Let us visualize both the positive class having diabetes and the negative class having no diabetes with histograms (on the left column in the next diagram). Each class roughly looks like a normal distribution. With SciPy, I can find the best fit normal distributions. Note on the bottom right that the threshold is set to 0.5, the default setting in the logistic regression. Observe how the threshold causes us to select, imperfectly, <strong>false negatives</strong> (<strong>FN</strong>) and <strong>false positives</strong> (<strong>FP</strong>).</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c6f1aa07-1abf-470c-b7ac-36d7e76b09e0.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating TPR in scikit-learn</h1>
                </header>
            
            <article>
                
<ol>
<li>scikit-learn has convenient functions for calculating the sensitivity or TPR for the logistic regression given a vector of probabilities of the positive class, <kbd>y_pred_proba[:,1]</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.metrics import roc_curve</strong><br/><br/><strong>fpr, tpr, ths = roc_curve(y_test, y_pred_proba[:,1])</strong></pre>
<p>Here, given the positive class vector, the <kbd>roc_curve</kbd> function in scikit-learn yielded a tuple of three arrays:</p>
<ul>
<li>The TPR array (denoted by <kbd>tpr</kbd>)</li>
<li>The FPR array (denoted by <kbd>fpr</kbd>)</li>
<li>A custom set of thresholds to calculate TPR and FPR (denoted by <kbd>ths</kbd>)</li>
</ul>
<p>To elaborate on the <strong>false positive rate</strong> (<strong>FPR</strong>), it describes the rate of false alarms. It is the number of people incorrectly thought to have diabetes although they do not:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="35" width="335" class="fm-editor-equation" src="assets/1fe1f564-fd47-475b-bf6a-95d64b006750.png"/></div>
<p>It is a statement of people who do not have diabetes. Mathematically, with the cells of the confusion matrix:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="32" width="239" class="fm-editor-equation" src="assets/2a8a70d8-c8ea-4270-a7b2-c61494829ef9.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting sensitivity</h1>
                </header>
            
            <article>
                
<ol start="2">
<li>Plot sensitivity in the <em>y</em> axis and the thresholds in the <em>x</em> axis:</li>
</ol>
<pre style="padding-left: 60px"><strong>plt.plot(ths,tpr)</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img height="247" width="374" src="assets/a026c031-1967-402d-81a1-4c8e7d51c7be.png"/></div>
<ol start="3">
<li>Thus, the lower the threshold, the better the sensitivity. Looking at the confusion matrix for the threshold at <kbd>0.1</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>y_pred_th= binarize(y_pred_proba, threshold=0.1)</strong><br/><strong>confusion_matrix(y_test, y_pred_th[:,1],labels=[1,0])</strong><br/><br/><strong>array([[54,  0],</strong><br/><strong>       [81, 19]])</strong></pre>
<ol start="4">
<li>In this case, no one went home believing they had diabetes when they did not. Yet, like our computation with NPV, when the test predicts that someone has diabetes, it is very inaccurate. The best scenario of this type is when the threshold is <kbd>0.146</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>y_pred_th = binarize(y_pred_proba, threshold=0.146)</strong><br/><strong>confusion_matrix(y_test, y_pred_th[:,1],labels=[1,0])</strong><br/><br/><strong>array([[54,  0],</strong><br/><strong>       [67, 33]])</strong></pre>
<p>Even then, the test does not work when the person is predicted to have diabetes. It works 33 / (33 + 121) = 0.21, or 21% of the time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The confusion matrix in a non-medical context</h1>
                </header>
            
            <article>
                
<p>Suppose you are a banker and want to determine whether a customer deserves a mortgage loan to buy a house. Up next is a possible confusion matrix showing whether to give a person a mortgage loan based on customer data available to the bank.</p>
<p>The task is to classify people and determine whether they should receive a mortgage loan or not. In this context, numbers can be assigned to every scenario. When every cell in the confusion matrix has a clear cost, it is easier to find the best classifier:</p>
<div class="CDPAlignCenter CDPAlign"><img height="146" width="280" src="assets/fc33a826-1ae3-4fcd-b6f0-fe46c11304ff.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting an ROC curve without context</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>An ROC curve is a diagnostic tool for any classifier without any context. No context means that we do not know yet which error type (FP or FN) is less desirable yet. Let us plot it right away using a vector of probabilities, <kbd>y_pred_proba[:,1]</kbd>:</p>
<pre><strong>from sklearn.metrics import roc_curve</strong><br/><br/><strong>fpr, tpr, ths = roc_curve(y_test, y_pred_proba[:,1])</strong><br/><strong>plt.plot(fpr,tpr)</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aad4b839-f44c-4922-ac95-c0edb0c51ab1.png"/></div>
<p>The ROC is a plot of the FPR (false alarms) in the <em>x</em> axis and TPR (finding everyone with the condition who really has it) in the <em>y</em> axis. Without context, it is a tool to measure classifier performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Perfect classifier</h1>
                </header>
            
            <article>
                
<p>A perfect classifier would have a TPR of 1 regardless of the <strong>false alarm rate</strong> (<strong>FAR</strong>):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8f83c56a-c8ed-4bc7-b11b-5897cb3d9be6.png"/></div>
<p>In the preceding graph, FN is very small; so the TPR, TP / (TP + FN), is close to 1. Its ROC curve has an L-shape:</p>
<div class="CDPAlignCenter CDPAlign"><img height="228" width="331" src="assets/6f7fffa4-8f06-451e-a5a7-0b59e9b1a72f.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Imperfect classifier</h1>
                </header>
            
            <article>
                
<p>In the following images, the distributions overlap and the categories cannot be distinguished from one another:</p>
<div class="CDPAlignCenter CDPAlign"><img height="616" width="291" src="assets/843b64d1-18f8-4b71-b71a-9056f62473bf.png"/></div>
<p>In the imperfect classifier, FN and TN are nearly equal and so are FP and TP. Thus, by substitution, the TPR TP/ (TP + FP) is nearly equal to the <strong>false negative rate</strong> (<strong>FNR</strong>) FP/ (FP + TN). This is true even if you vary the threshold. Consequently, we obtain an ROC curve that is a straight line with a slope of about 1:</p>
<div class="CDPAlignCenter CDPAlign"><img height="217" width="307" src="assets/a1a7e274-4ccc-4c4d-aa07-4fed11a65f7a.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AUC – the area under the ROC curve</h1>
                </header>
            
            <article>
                
<p>The area of the L-shaped perfect classifier is 1 x 1 = 1. The area of the bad classifier is 0.5. To measure classifier performance, scikit-learn has a handy <strong>area under the ROC curve</strong> (<strong>AUC</strong>) calculating function:</p>
<pre><strong>from sklearn.metrics import auc</strong><br/><br/><strong>auc(fpr,tpr)</strong><br/><br/><strong>0.825185185185</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together – UCI breast cancer dataset</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The dataset is provided thanks to Street, N (1990), UCI machine learning repository (<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data</a>), Madison, WI: University of Wisconsin, computer sciences department:</p>
<ol>
<li>After reading the citation/license information, load the dataset from UCI:</li>
</ol>
<pre style="padding-left: 60px"><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><strong>data_web_address = data_web_address = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</strong><br/><strong>column_names = ['radius',</strong><br/><strong>                'texture',</strong><br/><strong>                'perimeter',</strong><br/><strong>                'area',</strong><br/><strong>                'smoothness' </strong><br/><strong>                ,'compactness',</strong><br/><strong>                'concavity',</strong><br/><strong>                'concave points', </strong><br/><strong>                'symmetry',</strong><br/><strong>                'malignant']</strong><br/><br/><strong>feature_names = column_names[:-1]</strong><br/><strong>all_data = pd.read_csv(data_web_address , names=column_names)</strong></pre>
<ol start="2">
<li>Look at the data types:</li>
</ol>
<pre style="padding-left: 60px"><strong>all_data.dtypes</strong><br/><br/><strong>radius             int64</strong><br/><strong>texture            int64</strong><br/><strong>perimeter          int64</strong><br/><strong>area               int64</strong><br/><strong>smoothness         int64</strong><br/><strong>compactness       object</strong><br/><strong>concavity          int64</strong><br/><strong>concave points     int64</strong><br/><strong>symmetry           int64</strong><br/><strong>malignant          int64</strong><br/><strong>dtype: object</strong></pre>
<p style="padding-left: 60px">It turns out that the feature compactness has characters like <kbd>?</kbd>. For now, we do not use this feature.</p>
<ol start="3">
<li>Now, reading the documentation, the target variable is set to <kbd>2</kbd> (not having cancer) and <kbd>4</kbd> (having cancer). Change the variables to <kbd>0</kbd> for not having cancer and <kbd>1</kbd> for having cancer:</li>
</ol>
<pre style="padding-left: 60px"><strong>#changing the state of having cancer to 1, not having cancer to 0</strong><br/><strong>all_data['malignant'] = all_data['malignant'].astype(np.int)</strong><br/><strong>all_data['malignant'] = np.where(all_data['malignant'] == 4, 1,0) #4, and now 1 means malignant</strong><br/><strong>all_data['malignant'].value_counts()</strong><br/><br/><strong>0    458</strong><br/><strong>1    241</strong><br/><strong>Name: malignant, dtype: int64</strong></pre>
<ol start="4">
<li>Define <kbd>X</kbd> and <kbd>y</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>X = all_data[[col for col in feature_names if col != 'compactness']]</strong><br/><strong>y = all_data.malignant</strong></pre>
<ol start="5">
<li>Split <kbd>X</kbd> and <kbd>y</kbd> into training and testing sets:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)</strong><br/><br/><strong>#Train and test the Logistic Regression. Use the method #predict_proba().</strong><br/><strong>from sklearn.linear_model import LogisticRegression</strong><br/><br/><strong>lr = LogisticRegression()</strong><br/><strong>lr.fit(X_train,y_train)</strong><br/><strong>y_pred_proba = lr.predict_proba(X_test)</strong></pre>
<ol start="6">
<li>Draw the ROC curve and compute the AUC score:</li>
</ol>
<pre style="padding-left: 60px"><strong>import matplotlib.pyplot as plt</strong><br/><strong>from sklearn.metrics import roc_curve, auc, roc_auc_score</strong><br/><br/><strong>fpr, tpr, ths = roc_curve(y_test, y_pred_proba[:,1])</strong><br/><br/><strong>auc_score = auc(fpr,tpr)</strong><br/><strong>plt.plot(fpr,tpr,label="AUC Score:" + str(auc_score))</strong><br/><strong>plt.xlabel('fpr',fontsize='15')</strong><br/><strong>plt.ylabel('tpr',fontsize='15')</strong><br/><strong>plt.legend(loc='best')</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img height="213" width="294" src="assets/d754055f-d6fa-4430-acbc-b5c4eaaa6831.png"/></div>
<p>This classifier performs fairly well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Outline for future projects</h1>
                </header>
            
            <article>
                
<p>Overall, for future classification projects, you can do the following:</p>
<ol>
<li>Load the best data you can find for a particular problem.</li>
<li>Determine whether there is a classification context: is FP better or worse than FN?</li>
<li>Perform training and testing of the data without context using ROC-AUC scores. If several algorithms perform poorly at this step, you might want to go back to step 1.</li>
<li>If the context is important, explore it with confusion matrices.</li>
</ol>
<p>Logistic regression is particularly well suited for all these steps, though any algorithm with a <kbd>predict_proba()</kbd> method will work very similarly. As an exercise, you can generalize this process for other algorithms or even general regression if you are ambitious. The main point here is that not all errors are the same, a point easily emphasized with health datasets, wherein it is very important to treat all patients that have a condition.</p>
<p class="mce-root">A final note on the breast cancer dataset: observe that the data consists of cell measurements. You can gather these measurements by automating looking at the pictures with computers and finding the measurements with a traditional program or machine learning.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>
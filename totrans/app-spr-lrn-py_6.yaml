- en: '*Chapter 6*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain the importance of evaluating models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate regression and classification models using a number of metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the right metric for evaluating and tuning a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the importance of hold-out datasets and types of sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform hyperparameter tuning to find the best model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate feature importance and explain why they are important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter introduces us to how we can improve a model's performance by using
    hyperparameters and model evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous three chapters, we discussed the two types of supervised learning
    problems, regression and classification, followed by ensemble models, which were
    built from a combination of base models. We built several models and discussed
    how and why they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, that is not enough to take a model to production. Model development
    is an iterative process, and the model training step is followed by validation
    and updating steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Machine learning model development process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12622_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.1: Machine learning model development process'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This chapter will explain the peripheral steps in the process shown in the preceding
    flowchart; we will discuss how to select the appropriate hyperparameters and how
    to perform model validation using the appropriate error metrics. Improving a model's
    performance happens by iteratively performing these two tasks.
  prefs: []
  type: TYPE_NORMAL
- en: But why is it important to evaluate your model? Say you've trained your model
    and provided some hyperparameters, made predictions, and found its accuracy. That's
    the gist of it, but how do you make sure that your model is performing to the
    best of its ability? We need to ensure that the performance measure that you've
    come up with is actually representative of the model and that it will indeed perform
    well on an unseen test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The essential part about making sure that the model is the best version of
    itself comes after the initial training: the process of evaluating and improving
    the performance of the model. This chapter will take you through the essential
    techniques required when it comes to this.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first discuss why model evaluation is important, and
    introduce several evaluation metrics for both regression tasks and classification
    tasks that can be used to quantify the predictive performance of a model. This
    will be followed by a discussion on hold-out datasets and k-fold cross-validation
    and why it is imperative to have a test set that is independent of the validation
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, we''ll look at tactics we can use to boost the performance of the
    model. In the previous chapter, we talked about how having a model with a high
    bias or a high variance can result in suboptimal performance, and how building
    an ensemble of models can help us build a robust system that makes more accurate
    predictions without increasing the overall variance. We also mentioned the following
    as techniques to avoid overfitting our model to the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**To get more data**: A highly complex model can easily overfit to a small
    dataset but may not be able to as easily on a larger dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: Reducing the number of features can help make
    the model less complex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: A new term is added to the cost function in order to adjust
    the coefficients (especially the high-degree coefficients in linear regression)
    toward a small value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll introduce learning curves and validation curves as a
    way to see how variations in training and validation errors allow us to see whether
    the model needs more data, and where the appropriate level of complexity is. This
    will be followed by a section on hyperparameter tuning in an effort to boost performance,
    and a brief introduction to feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 49: Importing the Modules and Preparing Our Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will load the data and models that we trained as part
    of *Chapter 5*, *Ensemble Modeling*. We will use the stacked linear regression
    model from *Activity 14: Stacking with Standalone and Ensemble Algorithms*, and
    the random forest classification model to predict the survival of passengers from
    *Exercise 45: Building the Ensemble Model Using Random Forest*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the processed data files from *Chapter 5*, *Ensemble Modeling*. We will
    use pandas'' `read_csv()` method to read in our prepared datasets, which we will
    use in the exercises in this chapter. First, we''ll read the house price data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.2: First five rows of house_prices](img/C12622_06_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.2: First five rows of house_prices'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we''ll read in the Titanic data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.3: First five rows of Titanic](img/C12622_06_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.3: First five rows of Titanic'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, load the model files that we will use for the exercises in this chapter
    by using the `pickle` library to load them from a binary file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluating a machine learning model is an essential part of any project: once
    we have allowed our model to learn from the training data, the next step is to
    measure the performance of the model. We need to find a metric that can not only
    tell us how accurate the predictions made by the model are, but also allow us
    to compare the performance of a number of models so that we can select the one
    best suited for our use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a metric is usually one of the first things we should do when defining
    our problem statement and before we begin the EDA, since it's a good idea to plan
    ahead and think about how we intend to evaluate the performance of any model we
    build and how to judge whether it is performing optimally or not. Eventually,
    calculating the performance evaluation metric will fit into the machine learning
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, evaluation metrics will be different for regression tasks and
    classification tasks, since the output values in the former are continuous while
    the outputs in the latter are categorical. In this section, we'll look at the
    different metrics we can use to quantify the predictive performance of a model.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For an input variable, *X*, a regression model gives us a predicted value,![](img/C12622_06_Eq1.png)
    , that can take on a range of values. The ideal scenario would be to have the
    model predict ![](img/C12622_06_Eq11.png) values that are as close as possible
    to the actual value of *y*. Therefore, the smaller the difference between the
    two, the better the model performs. Regression metrics mostly involve looking
    at the numerical difference between the predicted value and actual value (that
    is, the residual or error value) for each data point, and subsequently aggregating
    these differences in some way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following plot, which plots the actual and predicted values
    for every point *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: Residuals between actual and predicted outputs in a linear regression
    problem](img/C12622_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Residuals between actual and predicted outputs in a linear regression
    problem'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: However, we can't just find the mean value of ![](img/C12622_06_Eq2.png) over
    all data points, since there could be data points that have a prediction error
    that is positive or negative, and the aggregate would ultimately end up canceling
    out a lot of the errors and severely overestimate the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we can consider the absolute error for each data point and find the
    **Mean Absolute Error** (**MAE**), which is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: Mean Absolute Error](img/C12622_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Mean Absolute Error'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, ![](img/C12622_06_Eq3.png) and ![](img/C12622_06_Eq4.png) are the actual
    and predicted values, respectively, for the *i**th* data point.
  prefs: []
  type: TYPE_NORMAL
- en: MAE is a **linear scoring function**, which means that it gives each residual
    an equal weight when it aggregates the errors. The MAE can take on any value from
    zero to infinity and is indifferent to the direction (positive or negative) of
    errors. Since these are error metrics, a lower value (as close to zero as possible)
    is usually desirable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to not let the direction of the error affect the performance estimate,
    we can also take the square of the error terms. Taking the mean of the squared
    errors gives us the **Mean Squared Error** (**MSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6: Mean Squared Error](img/C12622_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Mean Squared Error'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While the MAE has the same units as the target variable, *y*, the units for
    the MSE will be the squared unit of *y*, which may make the MSE slightly less
    interpretable while judging the model in real-world terms. However, if we take
    the square root of the MSE, we get the **Root Mean Squared Error** (**RMSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: Root Mean Squared Error](img/C12622_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Root Mean Squared Error'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since the errors are squared before they are averaged, having even a few error
    values that are high can cause the RMSE value to significantly increase. This
    means that the RMSE is more useful than MAE for judging models in which we want
    to penalize large errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since MAE and RMSE have the same units as the target variable, it can be hard
    to judge whether a particular value of the MAE or RMSE is good or bad, since there
    is no scale to refer to. A metric that is commonly used to overcome this problem
    is the **R****2** **Score**, or the **R-Squared Score**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8: R-Squared score](img/C12622_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: R-Squared score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The R2 score has a lower limit of *-∞* and an upper limit of 1\. The base model
    predicts the target variable to be equal to the mean of the target values in the
    training dataset, that is, where ![](img/C12622_06_Eq41.png) is equal to ![](img/C12622_06_Eq5.png)
    for all values of *i*. Keeping this in mind, a negative value of R2 would be one
    where the trained model makes a prediction that is worse than the mean, and a
    value close to 1 would be achieved if the MSE of the model is close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 50: Regression Metrics'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use the same model and processed dataset that we
    trained in *Activity 14: Stacking with Standalone and Ensemble Algorithms* in
    *Chapter 5*, *Ensemble Modeling*, to calculate regression metrics. We will use
    scikit-learn''s implementation of MAE and MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the metric functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the loaded model to predict the output on the given data. We will use the
    same features as we did in *Activity 14: Stacking with Standalone and Ensemble
    Algorithms* in *Chapter 5*, *Ensemble Modeling*, and use the model to make a prediction
    on the loaded dataset. The column we saved as *y* is the target variable and we
    will create *X* and *y* accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the MAE, RMSE, and R2 scores. Let''s print the values of the MAE
    and the RMSE from the predicted values. Also print the R2 score for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9: Scores](img/C12622_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Scores'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that the RMSE is much higher than the MAE. This shows that there
    are some data points where the residuals are particularly high, which is being
    highlighted by the larger RMSE value. But the R2 score is very close to 1, indicating
    that the model actually has close to ideal performance compared to a base model,
    which would predict a mean value.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For an input variable, *X*, a classification task gives us a predicted value,
    ![](img/C12622_06_Eq12.png), which can take on a limited set of values (two in
    the case of binary classification problems). Since the ideal scenario would be
    to predict a class for each data point that is the same as the actual class, there
    is no measure of how *close* or *far* the predicted class is from the actual class.
    Therefore, to judge the model's performance, it would be as simple as determining
    whether or not the model predicted the class correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Judging a classification model''s performance can be done in two ways: using
    numerical metrics, or by plotting a curve and looking at the shape of the curve.
    Let''s explore both of these in greater detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical Metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest and most basic way to judge the performance of the model is to
    calculate the proportion of the correct predictions to the total number of predictions,
    which gives us the **accuracy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10: Accuracy](img/C12622_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Accuracy'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although the accuracy metric is appropriate no matter the number of classes,
    the next few metrics are discussed keeping in mind a binary classification problem.
    Additionally, accuracy may not be the best metric to judge the performance of
    a classification task in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of fraud detection: say the problem statement is
    to detect whether a particular email is fraudulent or not. Our dataset in this
    case is highly skewed (or imbalanced, that is, there are many more data points
    belonging to one class compared to the other class), with 100 out of 10,000 emails
    (1% of the total) having been classified as fraudulent (having class 1). Say we
    build two models:'
  prefs: []
  type: TYPE_NORMAL
- en: The first model simply predicts each email as not being fraud, that is, each
    of the 10,000 emails is classified with the class 0\. In this case, 9,900 of the
    10,000 were classified correctly, which means the model has 99% accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second model predicts the 100 fraud emails as being fraud, but also predicts
    another 100 emails incorrectly as fraud. In this case as well, 100 data points
    were misclassified out of 10,000, and the model has an accuracy of 99%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How do we compare these two models? The purpose of building a fraud detection
    model is to allow us to know *how well the fraud was detected*: it matters more
    that the fraudulent emails were correctly classified than if non-fraud emails
    were classified as fraudulent. Although both the models were equally high in accuracy,
    the second was actually more effective than the first.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this cannot be captured using accuracy, we need the **confusion matrix**,
    a table with four different combinations of predicted and actual values that essentially
    gives us a summary of the prediction results of a classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11: Confusion matrix](img/C12622_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Confusion matrix'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here''s what the terms used in the matrix mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** and **true negatives**: These are the counts of the correctly
    predicted data points in the positive and negative classes respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives**: These are also known as **Type 1 errors** and refer to
    the count of the data points that actually belong to the negative class but were
    predicted to be positive. Continuing from the previous example, a false positive
    case would be if a normal email is classified as a fraudulent email.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negatives**: These are also known as **Type 2 errors** and refer to
    the count of the data points that actually belong to the positive class but were
    predicted to be negative. An example of a false negative case would be if a fraudulent
    email was classified as not being one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two extremely important metrics can be derived from a confusion matrix: **precision**
    and **recall**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12: Precision](img/C12622_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Precision'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 6.13: Recall](img/C12622_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Recall'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While precision tells us how many of the actual positives were correctly predicted
    to be positive (from the results the model says are relevant, how many are actually
    relevant?), recall tells us how many of the predicted positives were actually
    positive (from the real relevant results, how many are included in the model's
    list of relevant results?). These two metrics are especially useful when there
    is an imbalance between the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is usually a trade-off between the precision and recall of a model: if
    you have to recall all the relevant results, the model will generate more results
    that are not accurate, hence lowering the precision. On the other hand, having
    a higher percentage of relevant results from the generated results would involve
    including as few results as possible. In most cases, you would give a higher priority
    to either the precision or the recall, and this entirely depends on the problem
    statement. For example, since it matters more that all the fraudulent emails are
    correctly classified, recall would be an important metric that would need to be
    maximized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question that arises is how we take both precision and recall to evaluate
    our model using a single number instead of balancing two separate metrics. The
    **F****1** **score** combines the two into a single number that can be used as
    a fair judge of the model and is equal to the harmonic mean of precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14: F1 Score](img/C12622_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: F1 Score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The value of the F1 score will always lie between 0 (if either precision or
    recall is zero) and 1 (if both precision and recall are 1). The higher the score,
    the better the model''s performance is said to be. The F1 score gives equal weight
    to both measures and is a specific example of the general Fβ metric, where β can
    be adjusted to give more weight to either recall or precision using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15: F beta score](img/C12622_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: F beta score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A value of *β* *< 1* focuses more on precision, while taking *β* *> 1* focuses
    more on recall. The F1 score takes *β* *= 1* to give both equal weight.
  prefs: []
  type: TYPE_NORMAL
- en: '**Curve Plots**'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, instead of predicting the class, we have the class probabilities
    at our disposal. Say, in a binary classification task, the class probabilities
    of both the positive (class 1) and negative (class 0) classes will always add
    up to unity (or 1), which means that if we take the classification probability
    as equal to the probability of class 1 and apply a threshold, we can essentially
    use it as a cut-off value to either round up (to 1) or down (to 0), which will
    give the output class.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, by varying the threshold, we can get data points that have classification
    probabilities closer to 0.5 from one class to another. For example, with a threshold
    of 0.5, a data point having a probability of 0.4 would be assigned class 0 and
    a data point having probability 0.6 would be assigned class 1\. But if we change
    the threshold to 0.35 or 0.65, both those data points would be classified as 1
    or 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, varying the probability changes the precision and recall values
    and this can be captured by plotting the **precision-recall curve**. The plot
    has precision on the *Y* axis and recall on the *X* axis, and for a range of thresholds
    starting from 0 to 1 plots each *(recall, precision)* point. Connecting these
    points gives us the curve. The following graph shows an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16: Precision-recall curve](img/C12622_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Precision-recall curve'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We know that in an ideal case, the values of precision and recall will be unity.
    This means that upon increasing the threshold from 0 to 1, the precision would
    stay constant at 1, but the recall would increase from 0 to 1 as more and more
    (relevant) data points would be classified correctly. Thus, in an ideal case,
    the precision-recall curve would essentially just be a square and the **area under
    the curve** (**AUC**) would be equal to one.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can see that, as with the F1 score, the AUC is another metric derived
    from the precision and recall behavior that uses a combination of their values
    to evaluate the performance of the model. We want the model to achieve an AUC
    as high and close to 1 as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other main visualization technique for showing the performance of a classification
    model is the **Receiver Operating Characteristic** (**ROC**) curve. The ROC curve
    plots the relationship between the **True Positive Rate** (**TPR**) on the *Y*
    axis and the **False Positive Rate** (**FPR**) on the *X* axis across a varying
    classification probability threshold. TPR is exactly the same as the recall (and
    is also known as the **sensitivity** of the model), and FPR is an equal complement
    of the **specificity** (that is, *1 - FPR = Sensitivity*); both can be derived
    from the confusion matrix using these formulae:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17: True positive rate](img/C12622_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: True positive rate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 6.18: False positive rate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12622_06_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.18: False positive rate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following diagram shows an example of an ROC curve, plotted in the same
    way as the precision-recall curve: by varying the probability threshold such that
    each point on the curve represents a *(TPR, FPR)* data point corresponding to
    a specific probability threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19: ROC curve](img/C12622_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: ROC curve'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ROC curves are more useful when the classes are fairly balanced, since they
    tend to present an overly optimistic picture of the model on datasets with a class
    imbalance via their use of true negatives in the false positive rate in the ROC
    curve (which is not present in the precision-recall curve).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 51: Classification Metrics'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use the random forest model we trained in *Chapter
    5*, *Ensemble Modeling,* and use its predictions to generate the confusion matrix
    and calculate the precision, recall, and F1 scores, as a way of rating our model.
    We will use scikit-learn''s implementations to calculate these metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the model to predict classes for all data points. We will use the same
    features as we did earlier and use the random forest classifier to make a prediction
    on the loaded dataset. Every classifier in scikit-learn has a `.predict_proba()`
    function, which we will use here along with the standard `.predict()` function
    to give us the class probabilities and the classes respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.20: Accuracy score](img/C12622_06_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.20: Accuracy score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.21: Confusion matrix](img/C12622_06_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.21: Confusion matrix'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we can see that the model seems to have a high number of false negatives,
    which means that we can expect the recall value for this model to be extremely
    low. Similarly, since the count of the false positives is just one, we can expect
    the model to have high precision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the precision and recall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.22: Precision and recall scores](img/C12622_06_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.22: Precision and recall scores'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Calculate the F1 score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.23: F1 score](img/C12622_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: F1 score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that, since the recall is extremely low, this is affecting the F1
    score as well, making it close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have talked about the metrics we can use to measure the predictive
    performance of the model, let's talk about validation strategies, in which we
    will use a metric to evaluate the performance of the model in different cases
    and situations.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common mistake made when determining how well a model is performing is to
    calculate the prediction error on the data that the model was trained on and conclude
    that a model performs really well on the basis of a high prediction accuracy on
    the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This means that we are trying to test the model on data that the model has already
    *seen*, that is, the model has already learned the behavior of the training data
    because it was exposed to it—if asked to predict the behavior of the training
    data again, it would undoubtedly perform well. And the better the performance
    on the training data, the higher the chances that the model knows the data *too
    well*, so much so that it has even learned the noise and behavior of outliers
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, high training accuracy results in a model having high variance, as we saw
    in the previous chapter. In order to get an unbiased estimate of the model's performance,
    we need to find its prediction accuracy on data it has not already been exposed
    to during training. This is where the hold-out dataset comes into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: Hold-out Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **hold-out dataset** refers to a sample of the dataset that has been held
    back from training the model on and is essentially *unseen* by the model. The
    hold-out data points will likely contain outliers and noisy data points that behave
    differently from those in the training dataset, given that noise is random. Thus,
    calculating the performance on the hold-out dataset would allow us to validate
    whether the model is overfitting or not, as well as giving us an unbiased view
    of the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We began our previous chapter by splitting the Titanic dataset into training
    and validation sets. What is this validation dataset, and how is it different
    from a test dataset? We often see the terms validation set and test set used interchangeably—although
    they both characterize a hold-out dataset, there are some differences in purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Validation data**: After the model learns from the training data, its performance
    is evaluated on the validation dataset. However, in order to get the model to
    perform the best it can, we need to fine-tune the model and iteratively evaluate
    the updated model''s performance repeatedly, and this is done on the validation
    dataset. The fine-tuned version of the model that performs best on the validation
    dataset is usually chosen to be the final model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model, thus, is exposed to the validation dataset multiple times, at each
    iteration of improvement, although does not essentially *learn* from the data.
    It can be said that the validation set does affect the model, although indirectly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Test data**: The final model that was chosen is now evaluated on the test
    dataset. The performance measured on this dataset will be an unbiased measure
    that is reported as the final performance metric of the model. This final evaluation
    is done once the model has been completely trained on the combined training and
    validation datasets. There is no training or updating of the model performed after
    this metric has been calculated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that the model is exposed to the test dataset only once, when calculating
    the final performance metric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It should be kept in mind that the validation dataset should never be used
    to evaluate the final performance of the model: our estimate of the true performance
    of a model will be positively biased if the model has seen and been modified subsequently
    in an effort to specifically improve the performance on the validation set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a single hold-out validation dataset does have some limitations, however:'
  prefs: []
  type: TYPE_NORMAL
- en: Since the model is only validated once in each iteration of improvement, it
    might be difficult to capture the uncertainty in prediction using this single
    evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing the data into training and validation sets decreases the size of the
    data upon which the model is trained, and this can lead to the model having high
    variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final model may *overfit* to this validation set since it was tuned in order
    to maximize performance on this dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These challenges can be overcome if we use a validation technique called K-fold
    cross-validation instead of using a single validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: K-Fold Cross-Validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'K-fold cross-validation is a validation technique that helps us get an unbiased
    estimate of the model''s performance by essentially rotating the validation set
    in *k* folds. This is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we choose the value of *k* and divide the data into *k* subsets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we set aside the first subset as the validation set and use the remaining
    data to train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We measure the performance of the model on the validation subset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we set aside the second subset as the validation subset and repeat the
    process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have done this *k* times, we aggregate the performance metric values
    over all the folds and present the final metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure explains this visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24: K-fold cross-validation](img/C12622_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.24: K-fold cross-validation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although this method of validation is more computationally expensive, the benefits
    outweigh the costs. This approach makes sure that the model is validated on each
    example in the training dataset exactly once and that the performance estimate
    we achieve in the end is not biased in favor of a validation dataset, especially
    in the case of small datasets. A special case is **leave-one-out** cross-validation,
    where the value of *k* is equal to the number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we''ve looked at the strategies for splitting the dataset for training
    and validating the model, let''s discuss how to allocate data points to these
    splits. There are two ways we can sample the data into the splits, and these are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random sampling**: This is as simple as allocating random samples from the
    overall dataset into the training, validation, and/or test datasets. Randomly
    splitting the data only works when all the data points are independent of each
    other. For example, random splitting would not be the way to go if the data was
    in the form of a time-series, since the data points are ordered, and each depends
    on the previous one. Randomly splitting the data would destroy that order and
    not take into account this dependence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stratified sampling**: This is a way to ensure that each subset has the same
    distribution of values of the target variable as the original dataset. For example,
    if the original dataset has two classes in the ratio 3:7, stratified sampling
    ensures that each subset will also contain the two classes in the ratio 3:7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stratified sampling is important since testing our model on a dataset with a
    different distribution of target values from the dataset on which the model was
    trained can give us a performance estimate that is not representative of the model's
    actual performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The size of the train, validation, and test samples also plays an important
    role in the model evaluation process. Keeping aside a large dataset to test the
    final performance of the model on will help us get an unbiased estimate of the
    model's performance and reduce the variance in prediction, but if the test set
    is so large that it compromises the model's ability to train due to a lack of
    training data, this will severely affect the model as well. This is a consideration
    that is especially relevant for smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 52: K-Fold Cross-Validation with Stratified Sampling'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll implement K-fold cross-validation with stratified
    sampling on scikit-learn''s random forest classifier. The `StratifiedKFold` class
    in scikit-learn implements a combination of the cross-validation and sampling
    together in one class, and we will use this in our exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant classes. We will import scikit-learn''s `StratifiedKFold`
    class, which is a variation of `KFold` that returns stratified folds, along with
    the `RandomForestClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare data for training and initialize the k-fold object. Here, we will use
    five folds to evaluate the model, and hence will give the `n_splits` parameter
    a value of `5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train a classifier for each fold and record the score. The functioning of the
    `StratifiedKFold` class is similar to the `KFold` class that we used in the previous
    chapter, in *Exercise 48: Building a Stacked Model*: for each of the five folds,
    we will train on other four folds and predict on the fifth fold, and find the
    accuracy score for the predictions on the fifth fold. As we saw in the last chapter,
    the `skf.split()` function takes the dataset to split as input and returns an
    iterator comprising the index values used to subdivide the training data for training
    and validation for each row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.25: Scores using random forest classifier](img/C12622_06_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.25: Scores using random forest classifier'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the aggregated accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.26: Mean accuracy score](img/C12622_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.26: Mean accuracy score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Performance Improvement Tactics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Performance improvement for supervised machine learning models is an iterative
    process, and a continuous cycle of updating and evaluation is usually required
    to get the perfect model. While the previous sections in this chapter dealt with
    the evaluation strategies, this section will talk about model updating: we will
    discuss some ways we can determine what our model needs to give it that performance
    boost, and how to make that change in our model.'
  prefs: []
  type: TYPE_NORMAL
- en: Variation in Train and Test Error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced the concepts of underfitting and overfitting,
    and mentioned a few ways to overcome them, later introducing ensemble models.
    But we didn't talk about how to identify whether our model was underfitting or
    overfitting to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: It's usually useful to look at the learning and validation curves.
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Curve**'
  prefs: []
  type: TYPE_NORMAL
- en: The learning curve shows the variation in the training and validation error
    with the training data increasing in size. By looking at the shape of the curves,
    we can get a good idea of whether or not more data will benefit the modeling and
    possibly improve the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following figure: the dotted curve represents the validation
    error and the solid curve represents the training error. The plot on the left
    shows the two curves converging to an error value that is quite high. This means
    that the model has a high bias and adding more data isn''t likely to affect the
    model performance. So instead of wasting time and money collecting more data,
    all we need to do is increase model complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the plot on the right shows a high difference between the
    training and test errors, even with an increasing number of data points in the
    training set. The wide gap indicates a high variance in the system, which means
    the model is overfitting. In this case, adding more data points will probably
    help the model generalize better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.27: Learning curve for increasing data size](img/C12622_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.27: Learning curve for increasing data size'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'But how will we recognize the perfect learning curve? When we have a model
    with low bias and low variance, we will see a curve like the one shown in the
    following figure. It shows a low training error (low bias) as well as a low gap
    between the validation and training curves (low variance) as they converge. In
    practice, the best possible learning curves we can see are those that converge
    to the value of some irreducible error value (which exists due to noise and outliers
    in the dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.28: Variation in training and validation error with an increasing
    training data size for a low bias and variance model](img/C12622_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.28: Variation in training and validation error with an increasing
    training data size for a low bias and variance model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Validation Curve**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have discussed previously, the goal of a machine learning model is to
    be able to generalize to unseen data. Validation curves allow us to find the ideal
    point between an underfitted and an overfitted model where the model would generalize
    well. In the previous chapter, we talked a bit about how model complexity affects
    prediction performance: we said that as we move from an overly simplistic to an
    overly complex model, we go from having an underfitted model with high bias and
    low variance to an overfitted model with a low bias and high variance.'
  prefs: []
  type: TYPE_NORMAL
- en: A validation curve shows the variation in training and validation error with
    a varying value of a model parameter that has some degree of control over the
    model's complexity—this could be the degree of the polynomial in linear regression,
    or the depth of a decision tree classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.29: Variation in training and validation with increasing model complexity](img/C12622_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.29: Variation in training and validation with increasing model complexity'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The preceding figure shows how the validation and training error will vary with
    model complexity (of which the model parameter is an indicator). We can also see
    how the point in between the shaded regions is where the total error would be
    at a minimum, at the sweet spot between underfitting and overfitting. Finding
    this point will help us find the ideal value of the model's parameters that will
    help build a model with low bias as well as low variance.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ve talked about hyperparameter tuning several times before this; now let''s
    discuss why it''s so important. First, it should be noted that model parameters
    are different from model hyperparameters: while the former are internal to the
    model and are learned from the data, the latter define the architecture of the
    model itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of hyperparameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The degree of polynomial features to be used for a linear regressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum depth allowed for a decision tree classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of trees to be included in a random forest classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate used for the gradient descent algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The design choices that define the architecture of the model can make a huge
    difference in how well the model performs. Usually, the default values for the
    hyperparameters work, but getting the perfect combination of values for the hyperparameters
    can really give the predictive power of the model a boost as the default values
    may be completely inappropriate for the problem we are trying to model. In the
    following figure, we see how varying the values of two hyperparameters can cause
    such a difference in the model score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.30: Variation in model score (Z axis) across values of two model
    parameters (the X and Y axes)](img/C12622_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.30: Variation in model score (Z axis) across values of two model parameters
    (the X and Y axes)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finding that perfect combination by exploring a range of possible values is
    what is referred to as **hyperparameter tuning**. Since there is no loss function
    we can use to maximize the model performance, tuning the hyperparameters generally
    just involves experimenting with different combinations and choosing the one that
    performs best during validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few ways in which we can go about tuning our model''s hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hand-tuning**: When we manually choose the values of our hyperparameters,
    this is known as hand-tuning. It is usually inefficient, since solving a high-dimensional
    optimization problem by hand can not only be slow, but also would not allow the
    model to reach its peak performance as we probably wouldn''t try out every single
    combination of hyperparameter values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grid search**: Grid search involves training and evaluating a model for each
    combination of the hyperparameter values provided and selecting the combination
    that produces the best performing model. Since this involves performing an exhaustive
    sampling of the hyperparameter space, it is quite computationally expensive and
    hence inefficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random search**: While the first method was deemed inefficient because too
    few combinations were tried, the second one was deemed so because too many combinations
    were tried. Random search aims to solve this by selecting a random subset of hyperparameter
    combinations from the grid (specified previously), and training and evaluating
    a model only for those. Alternatively, we can also provide a statistical distribution
    for each hyperparameter from which the values can be randomly sampled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The logic behind random search was proved by Bergstra and Bengio: if at least
    5% of the points on the grid yield a close-to-optimal solution, then random search
    with 60 trials will find that region with a high probability.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can read the paper by Bergstra and Bengio at [http://www.jmlr.org/papers/v13/bergstra12a.html](http://www.jmlr.org/papers/v13/bergstra12a.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Bayesian optimization**: The previous two methods involved independently
    experimenting with combinations of hyperparameter values and recording the model
    performance for each. However, Bayesian optimization iterates over experiments
    sequentially and allows us to use the results of a previous experiment to improve
    the sampling method for the next experiment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 53: Hyperparameter Tuning with Random Search'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using scikit-learn''s `RandomizedSearchCV` method, we can define a grid of
    hyperparameter ranges and randomly sample from the grid, performing K-fold cross-validation
    with each combination of values. In this exercise, we''ll perform hyperparameter
    tuning with the random search method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the class for random search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare data for training and initialize the classifier. Here, we will initialize
    our random forest classifier without passing any arguments, since this is just
    a base object that will be instantiated for each grid point on which to perform
    the random search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the parameters to sample from. Here, we will list down the different
    values for each hyperparameter that we would like to have in the grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run a randomized search. We initialize the random search object with the total
    number of trials we want to run, the parameter values dictionary, the scoring
    function, and the number of folds in the K-fold cross-validation. Then, we call
    the `.fit()` function to perform the search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print scores and hyperparameters for the top five models. Convert the `results`
    dictionary into a pandas DataFrame and sort the values by `rank_test_score`. Then,
    for the first five rows, print the rank, mean validation score, and the hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.31: Top five models’ scores and hyperparameters](img/C12622_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.31: Top five models'' scores and hyperparameters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that the model that performs best has only 70 trees, compared to
    the 160+ trees in the models ranked 2 to 4\. Also, the model ranked 5 only has
    10 trees and still has a performance comparable to that of the more complex models.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While it is essential to focus on model performance, it''s also important to
    understand how the features in our model contribute to the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to be able to explain the model and how different variables affect the
    prediction to the relevant stakeholders who might demand insight into why our
    model is successful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data might be biased and training a model on this data could hurt the model's
    performance and result in biased model evaluation, in which case the ability to
    interpret the model by finding the important features and analyzing them will
    help debug the performance of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the previous point, it must be noted that some model biases might
    just be socially or legally unacceptable. For example, if a model works well because
    it implicitly places high importance on a feature based on ethnicity, this might
    cause issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides these points, finding feature importance can also help in feature selection.
    If the data has high dimensionality and the trained model has high variance, removing
    features that have low importance is one way to achieve lowered variance through
    dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 54: Feature Importance Using Random Forest'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will find the feature importance from the random forest
    model we loaded earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find feature importance. Let''s find the feature importance and save it in
    a pandas DataFrame with index equal to the column names, and sort this DataFrame
    in descending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the feature importance as a bar plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.32: Histogram of features](img/C12622_06_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.32: Histogram of features'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we can see that the `Sex`, `Fare`, and `Pclass` features seem to have
    the highest importance, that is, they have the most effect on the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 15: Final Test Project'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we''ll use the *IBM HR Analytics Employee Attrition & Performance*
    dataset (available at [https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset),
    and the accompanying source code at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python))
    to solve a classification problem wherein we have to predict whether or not an
    employee will leave the company given the features. In the employee attrition
    problem, we want to maximize our recall, that is, we want to be able to identify
    all employees that will leave, even at the cost of predicting that a good employee
    will leave: this will help HR take the appropriate action for these employees
    so that they don''t leave.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each row in the dataset represents a single employee, and the target variable
    we have here is `Attrition`, which has two values: `1` and `0`, representing a
    *Yes* and *No* with respect to whether the corresponding employee left. We will
    use a gradient boosting classifier from scikit-learn to train the model. This
    activity is meant as a final project that will help consolidate the practical
    aspects of the concepts learned in this book, and particularly in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: We will find the most optimal set of hyperparameters for the model by using
    random search with cross-validation. Then, we will build the final classifier
    using the gradient boosting algorithm on a portion of the dataset and evaluate
    its performance using the classification metrics we have learned about on the
    remaining portion of the dataset. We will use the mean absolute error as the evaluation
    metric for this activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the relevant libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the `attrition_train.csv` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the `categorical_variable_values.json` file, which has details of categorical
    variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the dataset to convert all features to numerical values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a base model and define the range of hyperparameter values corresponding
    to the model to be searched over for hyperparameter tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the parameters with which to initialize the `RandomizedSearchCV` object
    and use K-fold cross-validation to find the best model hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into training and validation sets and train a new model using
    the final hyperparameters on the training dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the accuracy, precision, and recall for predictions on the validation
    set, and print the confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with varying thresholds to find the optimal point with high recall.
    Plot the precision-recall curve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finalize a threshold that will be used for predictions on the test dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read and process the test dataset to convert all features to numerical values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the final values on the test dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 373.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter discussed why model evaluation is important in supervised machine
    learning and looked at several important metrics that are used to evaluate regression
    and classification tasks. We saw that while regression models were fairly straightforward
    to evaluate, the performance of classification models could be measured in a number
    of ways, depending on what we want the model to prioritize. Besides numerical
    metrics, we also looked at how to plot precision-recall and ROC curves to better
    interpret and evaluate model performance.
  prefs: []
  type: TYPE_NORMAL
- en: After this, we talked about why evaluating a model by calculating the prediction
    error on the data that the model was trained on was a bad idea, and how testing
    a model on data that it has already *seen* would lead to the model having a high
    variance. With this, we introduced the concept of having a hold-out dataset and
    why K-fold cross-validation is a useful strategy to have, along with sampling
    techniques that ensure that the model training and evaluation process remains
    unbiased.
  prefs: []
  type: TYPE_NORMAL
- en: The last section on performance improvement tactics started with a discussion
    on learning and validation curves, and how they can be interpreted to drive the
    model development process towards a better-performing model. This was followed
    by a section on hyperparameter tuning as an effort to boost performance, and a
    brief introduction to feature importance.
  prefs: []
  type: TYPE_NORMAL

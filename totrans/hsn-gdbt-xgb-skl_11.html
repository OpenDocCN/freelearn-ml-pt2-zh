<html><head></head><body>
		<div id="_idContainer145">
			<h1 id="_idParaDest-176"><em class="italic"><a id="_idTextAnchor189"/>Chapter 8</em>: XGBoost Alternative Base Learners</h1>
			<p>In this chapter, you will analyze and apply different <strong class="bold">base learners</strong> in XGBoost. In XGBoost, base learners are the individual models, most commonly trees, that are iterated upon for each boosting round. Along with the default decision tree, which XGBoost defines as <strong class="source-inline">gbtree</strong>, additional options for base learners include <strong class="source-inline">gblinear</strong> and <strong class="source-inline">dart</strong>. Furthermore, XGBoost has its own implementations of random forests as base learners and as tree ensemble algorithms that you will experiment with in this chapter.</p>
			<p>By learning how to apply alternative base learners, you will greatly extend your range with XGBoost. You will have the capacity to build many more models and you will learn new approaches to developing linear, tree-based, and random forest machine learning algorithms. The goal of the chapter is to give you proficiency in building XGBoost models with alternative base learners so that you can leverage advanced XGBoost options to find the best possible model for a range of situations.</p>
			<p>In this chapter, we cover the following main topics:</p>
			<ul>
				<li><p>Exploring alternative base learners</p></li>
				<li><p>Applying <strong class="source-inline">gblinear</strong></p></li>
				<li><p>Comparing <strong class="source-inline">dart</strong> </p></li>
				<li><p>Finding XGBoost random forests</p></li>
			</ul>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor190"/>Technical requirements</h1>
			<p>The code and datasets for this chapter may be found at <a href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08">https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08</a>.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor191"/>Exploring alternative base learners</h1>
			<p>The base learner is the <a id="_idIndexMarker498"/>machine learning model that XGBoost uses to build the first model in its ensemble. The word <em class="italic">base</em> is used because it's the model that comes first, and the word <em class="italic">learner</em> is used because the model iterates upon itself after learning from the errors.</p>
			<p>Decision trees have emerged as the preferred base learners for XGBoost on account of the excellent scores that boosted trees consistently produce. The popularity of decision trees extends beyond XGBoost to other ensemble algorithms such as random forests and <strong class="bold">extremely randomized trees</strong>, which you can preview in the scikit-learn documentation under <strong class="source-inline">ExtraTreesClassifier</strong> and <strong class="source-inline">ExtraTreesRegressor</strong> (<a href="https://scikit-learn.org/stable/modules/ensemble.html">https://scikit-learn.org/stable/modules/ensemble.html</a>).</p>
			<p>In XGBoost, the default base learner, known as <strong class="source-inline">gbtree</strong>, is one of <a id="_idIndexMarker499"/>several base learners. There is also <strong class="source-inline">gblinear</strong>, a gradient boosted linear model, and <strong class="source-inline">dart</strong>, a variation of decision trees that includes a dropout technique based on neural networks. Furthermore, there are XGBoost random forests. In the next section, we will explore the differences between these base learners before applying them in subsequent sections.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor192"/>gblinear</h2>
			<p>Decision trees <a id="_idIndexMarker500"/>are optimal for <strong class="bold">non-linear data</strong> as they can easily access points by splitting the data as many times as needed. Decision trees are often <a id="_idIndexMarker501"/>preferable as base learners because real data is usually non-linear.</p>
			<p>There may be cases, however, where a <strong class="bold">linear model</strong> is ideal. If the real data has a linear relationship, a decision tree is probably not the best option. For this scenario, XGBoost provides <strong class="source-inline">gblinear</strong> as an option for a <strong class="bold">linear base learner</strong>.</p>
			<p>The general idea behind boosted linear models is the same as boosted tree models. A base model is built, and each subsequent model is trained upon the residuals. At the end, the individual models are summed for the final result. The primary distinction with linear base learners is that each model in the ensemble is linear.</p>
			<p>Like <strong class="bold">Lasso</strong> and <strong class="bold">Ridge</strong>, variations of linear regression that add regularization terms (see <a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Landscape</em>), <strong class="source-inline">gblinear</strong> also adds regularization terms to linear regression. Tianqi Chin, the founder and developer of XGBoost commented on GitHub that multiple rounds of boosting <strong class="source-inline">gblinear</strong> may be used <em class="italic">to get back a single lasso regression</em> (<a href="https://github.com/dmlc/xgboost/issues/332">https://github.com/dmlc/xgboost/issues/332</a>).</p>
			<p><strong class="source-inline">gblinear</strong> may also be used for classification problems via <strong class="bold">logistic regression</strong>. This works because logistic regression is also built by finding optimal coefficients (weighted inputs), as in <strong class="bold">linear regression</strong>, and summed via the <strong class="bold">sigmoid equation</strong> (see <a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Landscape</em>).</p>
			<p>We will explore the details and applications of <strong class="source-inline">gblinear</strong> in the <em class="italic">Applying gblinear</em> section in this chapter. For now, let's learn about <strong class="source-inline">dart</strong>.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor193"/>DART</h2>
			<p><strong class="bold">Dropouts meet Multiple Additive Regression Trees</strong>, simply <a id="_idIndexMarker502"/>known as <strong class="bold">DART</strong>, was introduced <a id="_idIndexMarker503"/>in 2015 by K. V. Rashmi from UC Berkeley and Ran Gilad-Bachrach from Microsoft in the following paper: <a href="http://proceedings.mlr.press/v38/korlakaivinayak15.pdf">http://proceedings.mlr.press/v38/korlakaivinayak15.pdf</a>.</p>
			<p>Rashmi <a id="_idIndexMarker504"/>and Gilad-Bachrach highlight <strong class="bold">Multiple Additive Regression Trees</strong> (<strong class="bold">MART</strong>) as a successful model that suffers from too much dependency on earlier trees. Instead of focusing on <strong class="bold">shrinkage</strong>, a standard penalization term, they use the <strong class="bold">dropout</strong> technique from <strong class="bold">neural networks</strong>. Simply put, the dropout technique eliminates nodes (mathematical points) from each layer of learning in a neural network, thereby reducing overfitting. In other words, the dropout technique slows down the learning process by eliminating information from each round.</p>
			<p>In DART, in each new round of boosting, instead of summing the residuals from all previous trees to build a new model, DART selects a random sample of previous trees and normalizes the leaves by a scaling factor <img src="image/Formula_08_001.png" alt=""/> where <img src="image/Formula_08_002.png" alt=""/> is the number of trees dropped.</p>
			<p>DART is a variation of decision trees. The XGBoost implementation of DART is similar to <strong class="source-inline">gbtree</strong> with additional hyperparameters to accommodate dropouts.</p>
			<p>For the mathematical details of DART, reference the original paper highlighted in the first paragraph of this section.</p>
			<p>You will practice building machine learning models with <strong class="source-inline">DART</strong> base learners in the <em class="italic">Comparing dart</em> section later in this chapter.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor194"/>XGBoost random forests</h2>
			<p>The final option <a id="_idIndexMarker505"/>that we'll explore in <a id="_idIndexMarker506"/>this section is XGBoost random forests. Random forests may be implemented as base learners by setting <strong class="source-inline">num_parallel_trees</strong> equal to an integer greater than <strong class="source-inline">1</strong>, and as class options within XGBoost defined as <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong>.</p>
			<p>Keep in mind that gradient boosting was designed to improve upon the errors of relatively weak base learners, not strong base learners like random forests. Nevertheless, there may be fringe cases where random forest base learners can be advantageous so it's a nice option to have.</p>
			<p>As an additional bonus, XGBoost provides <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> as random forest machine learning algorithms that are not base learners, but algorithms in their own right. These algorithms work in a similar manner as scikit-learn's random forests (see <a href="B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070"><em class="italic">Chapter 3</em></a>, <em class="italic">Bagging with Random Forests</em>). The primary difference is that XGBoost includes default hyperparameters to counteract overfitting and their own methods for building individual trees. XGBoost random forests have been in the experimental stage, but they are starting to outperform scikit-learn's random forests as of late 2020 as you willwill see in this chapter.</p>
			<p>In the final section of this chapter, we will experiment with XGBoost's random forests, both as base learners and as models in their own right.</p>
			<p>Now that you have an overview of XGBoost base learners, let's apply them one at a time.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor195"/>Applying gblinear</h1>
			<p>It's challenging <a id="_idIndexMarker507"/>to find real-world datasets that work best with linear models. It's often the case that real data is messy and more complex models like tree ensembles produce better scores. In other cases, linear models may generalize better.</p>
			<p>The success of machine learning algorithms depends on how they perform with real-world data. In the next <a id="_idIndexMarker508"/>section, we will apply <strong class="source-inline">gblinear</strong> to the Diabetes dataset first and then to a linear dataset by construction.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor196"/>Applying gblinear to the Diabetes dataset</h2>
			<p>The Diabetes dataset is <a id="_idIndexMarker509"/>a regression dataset of 442 diabetes patients provided by scikit-learn. The prediction columns include age, sex, <strong class="bold">BMI</strong> (<strong class="bold">body mass index</strong>), <strong class="bold">BP</strong> (<strong class="bold">blood pressure</strong>), and five serum measurements. The target column is the progression of the disease after 1 year. You can read about the dataset in the original paper here: <a href="http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf">http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf</a>.</p>
			<p>Scikit-learn's datasets are already split into predictor and target columns for you. They are preprocessed for machine learning with <strong class="source-inline">X</strong>, the predictor columns, and <strong class="source-inline">y</strong>, the target column, loaded separately. </p>
			<p>Here is the full list of imports that you will need to work with this dataset and the rest of this chapter:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from sklearn.datasets import load_diabetes</p>
			<p class="source-code">from sklearn.model_selection import cross_val_score</p>
			<p class="source-code">from xgboost import XGBRegressor, XGBClassifier, XGBRFRegressor, XGBRFClassifier</p>
			<p class="source-code">from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier</p>
			<p class="source-code">from sklearn.linear_model import LinearRegression, LogisticRegression</p>
			<p class="source-code">from sklearn.linear_model import Lasso, Ridge</p>
			<p class="source-code">from sklearn.model_selection import GridSearchCV</p>
			<p class="source-code">from sklearn.model_selection import KFold</p>
			<p class="source-code">from sklearn.metrics import mean_squared_error as MSE</p>
			<p>Let's begin! To use the Diabetes dataset, do the following:</p>
			<ol>
				<li value="1"><p>You first need to define <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> using <strong class="source-inline">load_diabetes</strong> with the <strong class="source-inline">return_X_y</strong> parameter set equal to <strong class="source-inline">True</strong>:</p><p class="source-code">X, y = load_diabetes(return_X_y=True)</p><p>The plan is to use <strong class="source-inline">cross_val_score</strong> and <strong class="source-inline">GridSearchCV</strong>, so let's create folds in advance to obtain consistent scores. In <a href="B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136"><em class="italic">Chapter 6</em></a>, <em class="italic">XGBoost Hyperparameters</em>, we used <strong class="source-inline">StratifiedKFold</strong>, which stratifies the target column, ensuring that each test set includes the same number of classes. </p><p>This approach works for classification, but not for regression, where the target column takes on continuous values and classes are not involved. <strong class="source-inline">KFold</strong> achieves a similar <a id="_idIndexMarker510"/>goal without stratification by creating consistent splits in the data.</p></li>
				<li><p>Now, shuffle the data and use <strong class="source-inline">5</strong> splits with <strong class="source-inline">KFold</strong> using the following parameters: </p><p class="source-code">kfold = KFold(n_splits=5, shuffle=True, random_state=2)  </p></li>
				<li><p>Build a function with <strong class="source-inline">cross_val_score</strong> that takes a machine learning model as input and returns the mean score of <strong class="source-inline">5</strong> folds as the output, making sure to set <strong class="source-inline">cv=kfold</strong>:</p><p class="source-code">def regression_model(model):</p><p class="source-code">    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kfold)</p><p class="source-code">    rmse = (-scores)**0.5</p><p class="source-code">    return rmse.mean()</p></li>
				<li><p>To use <strong class="source-inline">gblinear</strong> as the base model, just set <strong class="source-inline">booster='gblinear'</strong> for <strong class="source-inline">XGBRegressor</strong> inside the regression function: </p><p class="source-code">regression_model(XGBRegressor(booster='gblinear'))</p><p>The score is as follows:</p><p class="source-code">55.4968907398679</p></li>
				<li><p>Let's check this score against other linear models including <strong class="source-inline">LinearRegression</strong>, <strong class="source-inline">Lasso</strong>, which uses <strong class="bold">L1</strong> or <strong class="bold">absolute value regularization</strong>, and <strong class="source-inline">Ridge</strong>, which uses <strong class="bold">L2</strong> or <strong class="bold">Euclidean distance regularization</strong>:</p><p>a) <strong class="source-inline">LinearRegression</strong> is as follows:</p><p class="source-code">regression_model(LinearRegression())</p><p>The score is as follows:</p><p class="source-code">55.50927267834351</p><p>b) <strong class="source-inline">Lasso</strong> is as follows:</p><p class="source-code">regression_model(Lasso())</p><p>The score is as follows:</p><p class="source-code">62.64900771743497</p><p>c) <strong class="source-inline">Ridge</strong> is as follows:</p><p class="source-code">regression_model(Ridge())</p><p>The score is as follows:</p><p class="source-code">58.83525077919004</p><p>As you <a id="_idIndexMarker511"/>can see, <strong class="source-inline">XGBRegressor</strong> with <strong class="source-inline">gblinear</strong> as the base learner performs the best, along with <strong class="source-inline">LinearRegression</strong>.</p></li>
				<li><p>Now place <strong class="source-inline">booster='gbtree'</strong> inside <strong class="source-inline">XGBRegressor</strong>, which is the default base learner:</p><p class="source-code">regression_model(XGBRegressor(booster='gbtree'))</p><p>The score is as follows:</p><p class="source-code">65.96608419624594</p></li>
			</ol>
			<p>As you can see, the <strong class="source-inline">gbtree</strong> base learner does not perform nearly as well as the <strong class="source-inline">gblinear</strong> base learner in this case indicating that a linear model is ideal.</p>
			<p>Let's see if we can modify hyperparameters to make some gains with <strong class="source-inline">gblinear</strong> as the base learner.</p>
			<h3>gblinear hyperparameters</h3>
			<p>It's important to understand <a id="_idIndexMarker512"/>the differences between <strong class="source-inline">gblinear</strong> and <strong class="source-inline">gbtree</strong> when adjusting hyperparameters. Many of the XGBoost hyperparameters presented in <a href="B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136"><em class="italic">Chapter 6</em></a><em class="italic">, XGBoost Hyperparameters</em>, are tree hyperparameters and do not apply to <strong class="source-inline">gblinear</strong>. For instance, <strong class="source-inline">max_depth</strong> and <strong class="source-inline">min_child_weight</strong> are hyperparameters specifically designed for trees.</p>
			<p>The following list is a summary of XGBoost <strong class="source-inline">gblinear</strong> hyperparameters that are designed for linear models. </p>
			<h4>reg_lambda</h4>
			<p>Scikit-learn uses <strong class="source-inline">reg_lambda</strong> instead of <strong class="source-inline">lambda</strong>, which is a reserved keyword for lambda <a id="_idIndexMarker513"/>functions in Python. This is the standard L2 regularization used by <strong class="source-inline">Ridge</strong>. Values close to <strong class="source-inline">0</strong> tend to work best:</p>
			<ul>
				<li><p><em class="italic">Default: 0</em></p></li>
				<li><p><em class="italic">Range: [0, inf)</em></p></li>
				<li><p><em class="italic">Increasing prevents overfitting</em></p></li>
				<li><p><em class="italic">Alias: lambda</em></p></li>
			</ul>
			<h4>reg_alpha</h4>
			<p>Scikit-learn <a id="_idIndexMarker514"/>accepts both <strong class="source-inline">reg_alpha</strong> and <strong class="source-inline">alpha</strong>. This is the standard L1 regularization used by <strong class="source-inline">Lasso</strong>. Values close to <strong class="source-inline">0</strong> tend to work best:</p>
			<ul>
				<li><p><em class="italic">Default: 0</em></p></li>
				<li><p><em class="italic">Range: [0, inf)</em></p></li>
				<li><p><em class="italic">Increasing prevents overfitting</em></p></li>
				<li><p><em class="italic">Alias: alpha</em></p></li>
			</ul>
			<h4>updater</h4>
			<p>This is the algorithm that XGBoost uses <a id="_idIndexMarker515"/>to build the linear model during each round of boosting. <strong class="source-inline">shotgun</strong> uses <strong class="source-inline">hogwild</strong> parallelism with coordinate descent to produce a non-deterministic solution. By contrast, <strong class="source-inline">coord_descent</strong> is ordinary coordinate descent with a deterministic solution:</p>
			<ul>
				<li><p><em class="italic">Default: shotgun</em></p></li>
				<li><p><em class="italic">Range: shotgun, coord_descent</em></p><p class="callout-heading">Note </p><p class="callout"><em class="italic">Coordinate descent</em> is a machine learning term defined as minimizing the error by finding the gradient one coordinate at a time.</p></li>
			</ul>
			<h4>feature_selector</h4>
			<p><strong class="source-inline">feature_selector</strong> determines how <a id="_idIndexMarker516"/>the weights are selected with the following options:</p>
			<p>a) <strong class="source-inline">cyclic</strong> – cycles through features iteratively</p>
			<p>b) <strong class="source-inline">shuffle</strong> – cyclic with random feature-shuffling in each round</p>
			<p>c) <strong class="source-inline">random</strong> – the coordinate selector during coordinate descent is random</p>
			<p>d) <strong class="source-inline">greedy</strong> – time-consuming; selects the coordinate with the greatest gradient magnitude</p>
			<p>e) <strong class="source-inline">thrifty</strong> – approximately greedy, reorders features according to weight changes</p>
			<ul>
				<li><p><em class="italic">Default: cyclic</em></p></li>
				<li><p><em class="italic">Range must be used in conjunction with updater as follows: </em></p><p>a) <strong class="source-inline">shotgun</strong>: <strong class="source-inline">cyclic</strong>, <strong class="source-inline">shuffle</strong></p><p>b) <strong class="source-inline">coord_descent</strong>: <strong class="source-inline">random</strong>, <strong class="source-inline">greedy</strong>, <strong class="source-inline">thrifty</strong></p><p class="callout-heading">Note </p><p class="callout"><strong class="source-inline">greedy</strong> will be computationally expensive for large datasets, but the number of features that <strong class="source-inline">greedy</strong> considers may be reduced by changing the parameter <strong class="source-inline">top_k</strong> (see the following).</p></li>
			</ul>
			<h4>top_k</h4>
			<p><strong class="source-inline">top_k</strong> is the number of <a id="_idIndexMarker517"/>features that <strong class="source-inline">greedy</strong> and <strong class="source-inline">thrifty</strong> select from during coordinate descent: </p>
			<ul>
				<li><p><em class="italic">Default: 0 (all features)</em></p></li>
				<li><p><em class="italic">Range: [0, max number of features]</em></p><p class="callout-heading">Note</p><p class="callout">For more information on XGBoost <strong class="source-inline">gblinear</strong> hyperparameters consult the official XGBoost <a id="_idIndexMarker518"/>documentation page at <a href="https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear">https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear</a>.</p></li>
			</ul>
			<h3>gblinear grid search</h3>
			<p>Now that you are familiar <a id="_idIndexMarker519"/>with the range of hyperparameters that <strong class="source-inline">gblinear</strong> may use, let's use <strong class="source-inline">GridSearchCV</strong> in a customized <strong class="source-inline">grid_search</strong> function to find the best ones:</p>
			<ol>
				<li value="1"><p>Here is a version of our <strong class="source-inline">grid_search</strong> function from <a href="B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136"><em class="italic">Chapter 6</em></a>, <em class="italic">XGBoost Hyperparameters</em>:</p><p class="source-code">def grid_search(params, reg=XGBRegressor(booster='gblinear')):</p><p class="source-code">    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=kfold)</p><p class="source-code">    grid_reg.fit(X, y)</p><p class="source-code">    best_params = grid_reg.best_params_</p><p class="source-code">    print("Best params:", best_params)</p><p class="source-code">    best_score = np.sqrt(-grid_reg.best_score_)</p><p class="source-code">    print("Best score:", best_score)</p></li>
				<li><p>Let's start by <a id="_idIndexMarker520"/>modifying <strong class="source-inline">alpha</strong> with a standard range:</p><p class="source-code">grid_search(params={'reg_alpha':[0.001, 0.01, 0.1, 0.5, 1, 5]})</p><p>The output is as follows:</p><p class="source-code">Best params: {'reg_alpha': 0.01}</p><p class="source-code">Best score: 55.485310447306425</p><p>The score is about the same, with a very slight improvement.</p></li>
				<li><p>Next, let's modify <strong class="source-inline">reg_lambda</strong> with the same range:</p><p class="source-code">grid_search(params={'reg_lambda':[0.001, 0.01, 0.1, 0.5, 1, 5]})</p><p>The output is as follows:</p><p class="source-code">Best params: {'reg_lambda': 0.001}</p><p class="source-code">Best score: 56.17163554152289</p><p>This score here is very similar but slightly worse.</p></li>
				<li><p>Now let's use <strong class="source-inline">feature_selector</strong> in tandem with <strong class="source-inline">updater</strong>. By default, <strong class="source-inline">updater=shotgun</strong> and <strong class="source-inline">feature_selector=cyclic</strong>. When<strong class="source-inline"> updater=shotgun</strong>, the only other option for <strong class="source-inline">feature_selector</strong> is <strong class="source-inline">shuffle</strong>. </p><p>Let's see if <strong class="source-inline">shuffle</strong> can perform better than <strong class="source-inline">cyclic</strong>:</p><p class="source-code">grid_search(params={'feature_selector':['shuffle']})</p><p>The output is as follows:</p><p class="source-code">Best params: {'feature_selector': 'shuffle'}</p><p class="source-code">Best score: 55.531684115240594</p><p>In this case, <strong class="source-inline">shuffle</strong> does not perform better.</p></li>
				<li><p>Now let's change <strong class="source-inline">updater</strong> to <strong class="source-inline">coord_descent</strong>. As a result, <strong class="source-inline">feature_selector</strong> may <a id="_idIndexMarker521"/>take on <strong class="source-inline">random</strong>, <strong class="source-inline">greedy</strong>, or <strong class="source-inline">thrifty</strong>. Try all <strong class="source-inline">feature_selector</strong> alternatives in <strong class="source-inline">grid_search</strong> by entering the following code:</p><p class="source-code">grid_search(params={'feature_selector':['random', 'greedy', 'thrifty'], 'updater':['coord_descent'] })</p><p>The output is as follows:</p><p class="source-code">Best params: {'feature_selector': 'thrifty', 'updater': 'coord_descent'}</p><p class="source-code">Best score: 55.48798105805444</p><p class="source-code">This is a slight improvement from the base score.</p><p>The final hyperparameter to check is <strong class="source-inline">top_k</strong>, which defines the number of features that <strong class="source-inline">greedy</strong> and <strong class="source-inline">thrifty</strong> check during coordinate descent. A range from <strong class="source-inline">2</strong> to <strong class="source-inline">9</strong> is acceptable since there are 10 features in total.</p></li>
				<li><p>Enter a range for <strong class="source-inline">top_k</strong> inside <strong class="source-inline">grid_search</strong> for <strong class="source-inline">greedy</strong> and <strong class="source-inline">thrifty</strong> to find the best option:</p><p class="source-code">grid_search(params={'feature_selector':['greedy', 'thrifty'], 'updater':['coord_descent'], 'top_k':[3, 5, 7, 9]})</p><p>The output is as follows:</p><p class="source-code">Best params: {'feature_selector': 'thrifty', 'top_k': 3, 'updater': 'coord_descent'}</p><p class="source-code">Best score: 55.478623763746256</p></li>
			</ol>
			<p>This is the best score yet.</p>
			<p>Before moving on, note that <a id="_idIndexMarker522"/>additional hyperparameters that are not limited to trees, such as <strong class="source-inline">n_estimators</strong> and <strong class="source-inline">learning_rate</strong>, may be used as well.</p>
			<p>Now let's see how <strong class="source-inline">gblinear</strong> works on a dataset that is linear by construction.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor197"/>Linear datasets</h2>
			<p>One way to ensure that a <a id="_idIndexMarker523"/>dataset is linear is by construction. We can choose a range of <strong class="source-inline">X</strong> values, say <strong class="source-inline">1</strong> to <strong class="source-inline">99</strong>, and multiply them by a scaling factor with some randomness involved. </p>
			<p>Here are the steps to construct a linear dataset:</p>
			<ol>
				<li value="1"><p>Set the range of <strong class="source-inline">X</strong> values from <strong class="source-inline">1</strong> to <strong class="source-inline">100</strong>:</p><p class="source-code">X = np.arange(1,100)</p></li>
				<li><p>Declare a random seed using NumPy to ensure the consistency of the results:</p><p class="source-code">np.random.seed(2) </p></li>
				<li><p>Create an empty list defined as <strong class="source-inline">y</strong>:</p><p class="source-code">y = []</p></li>
				<li><p>Loop through <strong class="source-inline">X</strong>, multiplying each entry by a random number from <strong class="source-inline">-0.2</strong> to <strong class="source-inline">0.2</strong>:</p><p class="source-code">for i in X:</p><p class="source-code">       y.append(i * np.random.uniform(-0.2, 0.2))</p></li>
				<li><p>Transform <strong class="source-inline">y</strong> to a <strong class="source-inline">numpy</strong> array for machine learning:</p><p class="source-code">y = np.array(y)</p></li>
				<li><p>Reshape <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> so that they contain as many rows as members in the array and one column since columns are expected as machine learning inputs with scikit-learn:</p><p class="source-code">X = X.reshape(X.shape[0], 1)</p><p class="source-code">y = y.reshape(y.shape[0], 1)</p><p>We now have a linear dataset that includes randomness in terms of <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong>.</p></li>
			</ol>
			<p>Let's run the <strong class="source-inline">regression_model</strong> function again with <strong class="source-inline">gblinear</strong> as the base learner: </p>
			<p class="source-code">regression_model(XGBRegressor(booster='gblinear', objective='reg:squarederror'))</p>
			<p>The score is as follows:</p>
			<p class="source-code">6.214946302686011</p>
			<p>Now <a id="_idIndexMarker524"/>run the <strong class="source-inline">regression_model</strong> function with <strong class="source-inline">gbtree</strong> as the base learner:</p>
			<p class="source-code">regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror'))</p>
			<p>The score is as follows:</p>
			<p class="source-code">9.37235946501318</p>
			<p>As you can see, <strong class="source-inline">gblinear</strong> performs much better in our constructed linear dataset.</p>
			<p>For good measure, let's try <strong class="source-inline">LinearRegression</strong> on the same dataset:</p>
			<p class="source-code">regression_model(LinearRegression())</p>
			<p>The score is as follows:</p>
			<p class="source-code">6.214962315808842</p>
			<p>In this case, <strong class="source-inline">gblinear</strong> performs slightly better, perhaps negligibly, scoring <strong class="source-inline">0.00002</strong> points lower than <strong class="source-inline">LinearRegression</strong>.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor198"/>Analyzing gblinear</h2>
			<p><strong class="source-inline">gblinear</strong> is a compelling option, but it should only be used when you have reason to believe that a linear model <a id="_idIndexMarker525"/>may perform better than a tree-based model. <strong class="source-inline">gblinear</strong> did outperform <strong class="source-inline">LinearRegression</strong> in the real and constructed datasets by a very slight margin. Within XGBoost, <strong class="source-inline">gblinear</strong> is a strong option for a base learner when datasets are large and linear. <strong class="source-inline">gblinear</strong> is an option for classification datasets as well, an option that you will apply in the next section.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor199"/>Comparing dart</h1>
			<p>The base learner <strong class="source-inline">dart</strong> is similar to <strong class="source-inline">gbtree</strong> in the sense that both are gradient boosted trees. The primary difference is that <strong class="source-inline">dart</strong> removes trees (called dropout) during each round of boosting.</p>
			<p>In this section, we will apply and compare the base learner <strong class="source-inline">dart</strong> to other base learners in regression and classification problems.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor200"/>DART with XGBRegressor</h2>
			<p>Let's <a id="_idIndexMarker526"/>see how <strong class="source-inline">dart</strong> performs <a id="_idIndexMarker527"/>on the Diabetes dataset: </p>
			<ol>
				<li value="1"><p>First, redefine <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> using <strong class="source-inline">load_diabetes</strong> as before:</p><p class="source-code">X, y = load_diabetes(return_X_y=True)</p></li>
				<li><p>To use <strong class="source-inline">dart</strong> as the XGBoost base learner, set the <strong class="source-inline">XGBRegressor</strong> parameter <strong class="source-inline">booster='dart'</strong> inside the <strong class="source-inline">regression_model</strong> function:</p><p class="source-code">regression_model(XGBRegressor(booster='dart', objective='reg:squarederror'))</p><p>The score is as follows:</p><p class="source-code">65.96444746130739</p></li>
			</ol>
			<p>The <strong class="source-inline">dart</strong> base learner gives the same result as the <strong class="source-inline">gbtree</strong> base learner down to two decimal places. The similarity of results is on account of the small dataset and the success of the <strong class="source-inline">gbtree</strong> default hyperparameters to prevent overfitting without requiring the dropout technique.</p>
			<p>Let's see how <strong class="source-inline">dart</strong> performs compared to <strong class="source-inline">gbtree</strong> on a larger dataset with classification.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor201"/>dart with XGBClassifier</h2>
			<p>You have used <a id="_idIndexMarker528"/>the Census dataset in multiple chapters throughout this book. A clean <a id="_idIndexMarker529"/>version of the dataset that we modified in <a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Landscape</em>, has been pre-loaded for you along with the code for <a href="B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189"><em class="italic">Chapter 8</em></a>, <em class="italic">XGBoost Alternative Base Learners</em>, at <a href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08">https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08</a>. Let's now begin to test how <strong class="source-inline">dart</strong> performs on a larger dataset:</p>
			<ol>
				<li value="1"><p>Load the Census dataset into a DataFrame and split the predictor and target columns into <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> using the last index (<strong class="source-inline">-1</strong>) as the target column:</p><p class="source-code">df_census = pd.read_csv('census_cleaned.csv')</p><p class="source-code">X_census = df_census.iloc[:, :-1]</p><p class="source-code">y_census = df_census.iloc[:, -1]</p></li>
				<li><p>Define a new classification function that uses <strong class="source-inline">cross_val_score</strong> with the machine learning model as input and the mean score as output similar to the regression function defined earlier in this chapter:</p><p class="source-code">def classification_model(model):</p><p class="source-code">    scores = cross_val_score(model, X_census, y_census, scoring='accuracy', cv=kfold)</p><p class="source-code">    return scores.mean()</p></li>
				<li><p>Now call the function twice using <strong class="source-inline">XGBClassifier</strong> with <strong class="source-inline">booster='gbtree'</strong> and <strong class="source-inline">booster='dart'</strong> to compare results. Note that the run time will be longer since the dataset is larger:</p><p>a) Let's first call <strong class="source-inline">XGBClassifier</strong> with <strong class="source-inline">booster='gbtree'</strong>:</p><p class="source-code">classification_model(XGBClassifier(booster='gbtree'))</p><p>The score is as follows:</p><p class="source-code">0.8701208195968675</p><p>b) Now, let's call <strong class="source-inline">XGBClassifier</strong> with <strong class="source-inline">booster='dart'</strong>:</p><p class="source-code">classification_model(XGBClassifier(booster='dart')</p><p>The score is as follows:</p><p class="source-code">0.8701208195968675</p></li>
			</ol>
			<p>This is surprising. <strong class="source-inline">dart</strong> gives the exact same result as <strong class="source-inline">gbtree</strong> for all 16 decimal places! It's unclear whether trees have been dropped or the dropping of trees has had zero effect. </p>
			<p>We can adjust hyperparameters to ensure that trees are dropped, but first, let's see how <strong class="source-inline">dart</strong> compares to <strong class="source-inline">gblinear</strong>. Recall that <strong class="source-inline">gblinear</strong> also works for classification by using <a id="_idIndexMarker530"/>the sigmoid function <a id="_idIndexMarker531"/>to scale weights as with logistic regression:</p>
			<ol>
				<li value="1"><p>Call the <strong class="source-inline">classification_model</strong> function with <strong class="source-inline">XGBClassifier</strong> and set <strong class="source-inline">booster='gblinear'</strong>:</p><p class="source-code">classification_model(XGBClassifier(booster='gblinear'))</p><p>The score is as follows:</p><p class="source-code">0.8501275704120015</p><p>This linear base learner does not perform as well as the tree base learners. </p></li>
				<li><p>Let's see how <strong class="source-inline">gblinear</strong> compares with logistic regression. Since the dataset is large, it's best to adjust logistic regression's <strong class="source-inline">max_iter</strong> hyperparameter from <strong class="source-inline">100</strong> to <strong class="source-inline">1000</strong> to allow more time for convergence and to silence warnings. Note that increasing <strong class="source-inline">max_iter</strong> increases the accuracy in this case: </p><p class="source-code">classification_model(LogisticRegression(max_iter=1000))</p><p>The score is as follows:</p><p class="source-code">0.8008968643699182</p><p><strong class="source-inline">gblinear</strong> maintains a clear edge over logistic regression in this case. It's worth underscoring that XGBoost's <strong class="source-inline">gblinear</strong> option in classification provides a viable alternative to logistic regression.</p></li>
			</ol>
			<p>Now that <a id="_idIndexMarker532"/>you have seen how <strong class="source-inline">dart</strong> compares with <strong class="source-inline">gbtree</strong> and <strong class="source-inline">gblinear</strong> as a base learner, let's <a id="_idIndexMarker533"/>modify <strong class="source-inline">dart</strong>'s hyperparameters.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor202"/>DART hyperparameters</h2>
			<p><strong class="source-inline">dart</strong> includes all <strong class="source-inline">gbtree</strong> hyperparameters <a id="_idIndexMarker534"/>along with its own set of additional hyperparameters designed to adjust the percentage, frequency, and probability of dropout trees. See the XGBoost documentation at <a href="https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart">https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart</a> for detailed information.</p>
			<p>The following sections are a summary of XGBoost hyperparameters that are unique to <strong class="source-inline">dart</strong>.</p>
			<h4>sample_type</h4>
			<p>The options for <strong class="source-inline">sample_type</strong> include <strong class="source-inline">uniform</strong>, which <a id="_idIndexMarker535"/>drops trees uniformly, and <strong class="source-inline">weighted</strong>, which drops trees in proportion to their weights:</p>
			<ul>
				<li><p><em class="italic">Default: "uniform"</em></p></li>
				<li><p><em class="italic">Range: ["uniform", "weighted"]</em></p></li>
				<li><p><em class="italic">Determines how dropped trees are selected</em></p></li>
			</ul>
			<h4>normalize_type</h4>
			<p>The options for <strong class="source-inline">normalize_type</strong> include <strong class="source-inline">tree</strong>, where new trees have the same weight as <a id="_idIndexMarker536"/>dropped trees, and <strong class="source-inline">forest</strong>, where new trees have the same weight as the sum of dropped trees:</p>
			<ul>
				<li><p><em class="italic">Default: "tree"</em></p></li>
				<li><p><em class="italic">Range: ["tree", "forest"]</em></p></li>
				<li><p><em class="italic">Calculates weights of trees in terms of dropped trees</em></p></li>
			</ul>
			<h4>rate_drop</h4>
			<p><strong class="source-inline">rate_drop</strong> allows the user to <a id="_idIndexMarker537"/>set exactly how many trees are dropped percentage-wise:</p>
			<ul>
				<li><p><em class="italic">Default: 0.0</em></p></li>
				<li><p><em class="italic">Range: [0.0, 1.0]</em></p></li>
				<li><p><em class="italic">Percentage of trees that are dropped</em></p></li>
			</ul>
			<h4>one_drop</h4>
			<p>When set to <strong class="source-inline">1</strong>, <strong class="source-inline">one_drop</strong> ensures <a id="_idIndexMarker538"/>that at least one tree is always dropped during the boosting round:</p>
			<ul>
				<li><p><em class="italic">Default: 0</em></p></li>
				<li><p><em class="italic">Range: [0, 1]</em></p></li>
				<li><p><em class="italic">Used to ensure drops</em></p></li>
			</ul>
			<h4>skip_drop</h4>
			<p><strong class="source-inline">skip_drop</strong> gives the <a id="_idIndexMarker539"/>probability of skipping the dropout entirely. In the official documentation, XGBoost says that <strong class="source-inline">skip_drop</strong> has a higher priority than <strong class="source-inline">rate_drop</strong> or <strong class="source-inline">one_drop</strong>. By default, each tree is dropped with the same probability so there is a probability that no trees are dropped for a given boosting round. <strong class="source-inline">skip_drop</strong> allows this probability to be updated to control the number of dropout rounds:</p>
			<ul>
				<li><p><em class="italic">Default: 0.0</em></p></li>
				<li><p><em class="italic">Range: [0.0, 1.0]</em></p></li>
				<li><p><em class="italic">Probability of skipping the dropout</em></p></li>
			</ul>
			<p>Now let's modify <strong class="source-inline">dart</strong> hyperparameters to differentiate scores.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor203"/>Modifying dart hyperparameters</h2>
			<p>To ensure that at least <a id="_idIndexMarker540"/>one tree in each boosting round is dropped, we can set <strong class="source-inline">one_drop=1</strong>. Do this with the Census dataset via the <strong class="source-inline">classification_model</strong> function now:</p>
			<p class="source-code">classification_model(XGBClassifier(booster='dart', one_drop=1))</p>
			<p>The result is as follows:</p>
			<p class="source-code">0.8718714338474818</p>
			<p>This is an improvement by a tenth of a percentage point, indicating that dropping at least one tree per boosting round can be advantageous.</p>
			<p>Now that we are dropping trees to change scores, let's return to the smaller and faster Diabetes dataset to modify the remaining hyperparameters:</p>
			<ol>
				<li value="1"><p>Using the <strong class="source-inline">regression_model</strong> function, change <strong class="source-inline">sample_type</strong> from <strong class="source-inline">uniform</strong> to <strong class="source-inline">weighted</strong>:</p><p class="source-code">regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', sample_type='weighted'))</p><p>The score is as follows:</p><p class="source-code">65.96444746130739</p><p>This is 0.002 points better than the <strong class="source-inline">gbtree</strong> model scored earlier.</p></li>
				<li><p>Change <strong class="source-inline">normalize_type</strong> to <strong class="source-inline">forest</strong> to include the sum of trees when updating weights:</p><p class="source-code">regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', normalize_type='forest'))</p><p>The score is as follows:</p><p class="source-code">65.96444746130739</p><p>There is no change in the score, which may happen with a shallow dataset.</p></li>
				<li><p>Change <strong class="source-inline">one_drop</strong> to <strong class="source-inline">1</strong> guaranteeing that at least one tree is dropped each boosting round:</p><p class="source-code">regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))</p><p>The score is as follows:</p><p class="source-code">61.81275131335009</p><p>This is a <a id="_idIndexMarker541"/>clear improvement, gaining four full points.</p></li>
			</ol>
			<p>When it comes to <strong class="source-inline">rate_drop</strong>, the percentage of trees that will be dropped, a range of percentages may be used with the <strong class="source-inline">grid_search</strong> function as follows:</p>
			<p class="source-code">grid_search(params={'rate_drop':[0.01, 0.1, 0.2, 0.4]}, reg=XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))</p>
			<p>The results are as follows:</p>
			<p class="source-code">Best params: {'rate_drop': 0.2}</p>
			<p class="source-code">Best score: 61.07249602732062</p>
			<p>This is the best result yet.</p>
			<p>We can implement a similar range with <strong class="source-inline">skip_drop</strong>, which gives the probability that a given tree is <em class="italic">not</em> dropped:</p>
			<p class="source-code">grid_search(params={'skip_drop':[0.01, 0.1, 0.2, 0.4]}, reg=XGBRegressor(booster='dart', objective='reg:squarederror'))</p>
			<p>The results are as follows:</p>
			<p class="source-code">Best params: {'skip_drop': 0.1}</p>
			<p class="source-code">Best score: 62.879753748627635</p>
			<p>This is a <a id="_idIndexMarker542"/>good score, but <strong class="source-inline">skip_drop</strong> has resulted in no substantial gains.</p>
			<p>Now that you see how <strong class="source-inline">dart</strong> works in action, let's analyze the results.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor204"/>Analyzing dart</h2>
			<p><strong class="source-inline">dart</strong> provides a compelling <a id="_idIndexMarker543"/>option within the XGBoost framework. Since <strong class="source-inline">dart</strong> accepts all <strong class="source-inline">gbtree</strong> hyperparameters, it's easy to change the base learner from <strong class="source-inline">gbtree</strong> to <strong class="source-inline">dart</strong> when modifying hyperparameters. In effect, the advantage is that you can experiment with new hyperparameters including <strong class="source-inline">one_drop</strong>, <strong class="source-inline">rate_drop</strong>, <strong class="source-inline">normalize</strong>, and others to see if you can make additional gains. <strong class="source-inline">dart</strong> is definitely worth trying as a base learner in your research and model-building with XGBoost.</p>
			<p>Now that you have a good understanding of <strong class="source-inline">dart</strong>, it's time to move on to random forests.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor205"/>Finding XGBoost random forests</h1>
			<p>There are two <a id="_idIndexMarker544"/>strategies to implement random forests within XGBoost. The first is to use random forests as the base learner, the second is to use XGBoost's original random forests, <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong>. We start with our original theme, random forests as alternative base learners.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor206"/>Random forests as base learners</h2>
			<p>There is not an <a id="_idIndexMarker545"/>option to set the booster hyperparameter to a random forest. Instead, the hyperparameter <strong class="source-inline">num_parallel_tree</strong> may be increased from its default value of <strong class="source-inline">1</strong> to transform <strong class="source-inline">gbtree</strong> (or <strong class="source-inline">dart</strong>) into a boosted random forest. The idea here is that each boosting round will no longer consist of one tree, but a number of parallel trees, which in turn make up a forest.</p>
			<p>The following is a quick summary of the XGBoost hyperparameter <strong class="source-inline">num_parallel_tree</strong>.</p>
			<h4>num_parallel_tree</h4>
			<p><strong class="source-inline">num_parallel_tree</strong> gives the number <a id="_idIndexMarker546"/>of trees, potentially more than 1, that are built during each boosting round: </p>
			<ul>
				<li><p><em class="italic">Default: 1</em></p></li>
				<li><p><em class="italic">Range: [1, inf)</em></p></li>
				<li><p><em class="italic">Gives number of trees boosted in parallel</em></p></li>
				<li><p><em class="italic">Value greater than 1 turns booster into random forest</em></p></li>
			</ul>
			<p>By including multiple trees per round, the base learner is no longer a tree, but a forest. Since XGBoost includes the same hyperparameters as random forests, the base learner is appropriately classified as a random forest when <strong class="source-inline">num_parallel_tree</strong> exceeds 1.</p>
			<p>Let's see how XGBoost random forest base learners work in practice:</p>
			<ol>
				<li value="1"><p>Call <strong class="source-inline">regression_model</strong> with <strong class="source-inline">XGBRegressor</strong> and set <strong class="source-inline">booster='gbtree'</strong>. Additionally, set <strong class="source-inline">num_parallel_tree=25</strong> meaning that each boosted round consists of a forest of <strong class="source-inline">25</strong> trees:</p><p class="source-code">regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror', num_parallel_tree=25))</p><p>The score is as follows:</p><p class="source-code">65.96604877151103</p><p>The score is respectable, and in this case, it's nearly the same as boosting a single <strong class="source-inline">gbtree</strong>. The reason is that gradient boosting is designed to learn from the mistakes of the previous trees. By starting with a robust random forest, there is little to be learned and the gains are minimal at best.</p><p>Understanding the fundamental point that gradient boosting's strength as an algorithm comes from the learning process is essential. It makes sense, therefore, to try a much smaller value for <strong class="source-inline">num_parallel_tree</strong>, such as <strong class="source-inline">5</strong>.</p></li>
				<li><p>Set <strong class="source-inline">num_parallel_tree=5</strong> inside the same regression model:</p><p class="source-code">regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror', num_parallel_tree=5))</p><p>The score is as follows:</p><p class="source-code">65.96445649315855</p><p>Technically, this <a id="_idIndexMarker547"/>score is 0.002 points better than the score produced by a forest of 25 trees. Although the improvement is not much, generally speaking, when building XGBoost random forests, low values of <strong class="source-inline">num_parallel_tree</strong> are better.</p></li>
			</ol>
			<p>Now that you have seen how random forests may be implemented as base learners within XGBoost, it's time to build random forests as original XGBoost models.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor207"/>Random forests as XGBoost models</h2>
			<p>In <a id="_idIndexMarker548"/>addition to <strong class="source-inline">XGBRegressor</strong> and <strong class="source-inline">XGBClassifier</strong>, <strong class="source-inline">XGBoost</strong> also comes with <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> to build random forests.</p>
			<p>According to the official XGBoost documentation at <a href="https://xgboost.readthedocs.io/en/latest/tutorials/rf.html">https://xgboost.readthedocs.io/en/latest/tutorials/rf.html</a>, the random forest scikit-learn wrapper is still in the experimentation stage and the defaults may be changed at any time. At the time of writing, in 2020, the following <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> defaults are included.</p>
			<h4>n_estimators</h4>
			<p>Use <strong class="source-inline">n_estimators</strong> and not <strong class="source-inline">num_parallel_tree</strong> when using <strong class="source-inline">XGBRFRegressor</strong> or <strong class="source-inline">XGBRFClassifier</strong> to build a random forest. Keep in mind that when using <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong>, you are not gradient boosting but bagging trees in one round only as is the case with a traditional random forest:</p>
			<ul>
				<li><p><em class="italic">Default: 100</em></p></li>
				<li><p><em class="italic">Range: [1, inf)</em></p></li>
				<li><p><em class="italic">Automatically converted to num_parallel_tree for random forests</em></p></li>
			</ul>
			<h4>learning_rate</h4>
			<p><strong class="source-inline">learning_rate</strong> is generally designed for <a id="_idIndexMarker549"/>models that learn, including boosters, not <strong class="source-inline">XGBRFRegressor</strong> or <strong class="source-inline">XGBRFClassifier</strong> since they consist of one round of trees. Nevertheless, changing <strong class="source-inline">learning_rate</strong> from 1 will change the scores, so modifying this hyperparameter is generally not advised:</p>
			<ul>
				<li><p><em class="italic">Default: 1</em></p></li>
				<li><p><em class="italic">Range: [0, 1]</em></p></li>
			</ul>
			<h4>subsample, colsample_by_node</h4>
			<p>Scikit-learn's <a id="_idIndexMarker550"/>random forest keeps these defaults at <strong class="source-inline">1</strong>, making the default <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> less prone to overfitting. This is the primary difference between the XGBoost and scikit-learn <a id="_idIndexMarker551"/>random forest default implementations:</p>
			<ul>
				<li><p><em class="italic">Defaults: 0.8</em></p></li>
				<li><p><em class="italic">Range: [0, 1]</em></p></li>
				<li><p><em class="italic">Decreasing helps prevent overfitting</em></p></li>
			</ul>
			<p>Now, let's see how XGBoost's random forests work in practice:</p>
			<ol>
				<li value="1"><p>First, place <strong class="source-inline">XGBRFRegressor</strong> inside of the <strong class="source-inline">regression_model</strong> function:</p><p class="source-code">regression_model(XGBRFRegressor(objective='reg:squarederror'))</p><p>The score is as follows:</p><p class="source-code">59.447250741400595</p><p>This score is a little better than the <strong class="source-inline">gbtree</strong> model presented earlier, and a little worse than the best linear models presented in this chapter.</p></li>
				<li><p>As a comparison, let's see how <strong class="source-inline">RandomForestRegressor</strong> performs by placing it inside the same function:</p><p class="source-code">regression_model(RandomForestRegressor())</p><p>The score is as follows:</p><p class="source-code">59.46563031802505</p><p>This score is slightly worse than <strong class="source-inline">XGBRFRegressor</strong>.</p></li>
			</ol>
			<p>Now let's compare <a id="_idIndexMarker552"/>the XGBoost random <a id="_idIndexMarker553"/>forest with scikit-learn's standard random forest using the larger Census dataset for classification:</p>
			<ol>
				<li value="1"><p>Place <strong class="source-inline">XGBRFClassifier</strong> inside of the <strong class="source-inline">classification_model</strong> function to see how well it predicts user income:</p><p class="source-code">classification_model(XGBRFClassifier())</p><p>The score is as follows:</p><p class="source-code">0.856085650471878</p><p>This is a good score, a little off the mark from <strong class="source-inline">gbtree</strong>, which previously gave 87%.</p></li>
				<li><p>Now place <strong class="source-inline">RandomForestClassifier</strong> inside the same function to compare results:</p><p class="source-code">classification_model(RandomForestClassifier())</p><p>The score is as follows:</p><p class="source-code">0.8555328202034789</p><p>This is slightly worse than XGBoost's implementation.</p></li>
			</ol>
			<p>Since XGBoost's random forests are still in the developmental stage, we'll stop here and analyze the results.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor208"/>Analyzing XGBoost random forests</h2>
			<p>You can try a random forest as your XGBoost base learner anytime by increasing <strong class="source-inline">num_parallel_tree</strong> to a value <a id="_idIndexMarker554"/>greater than <strong class="source-inline">1</strong>. Although, as you have seen in this section, bo<a id="_idTextAnchor209"/>osting is designed to learn from weak models, not strong models, so values for <strong class="source-inline">num_parallel_tree</strong> should remain close to <strong class="source-inline">1</strong>. Trying random forests as base learners should be used sparingly. If boosting single trees fails to produce optimal scores, random forest base learners are an option.</p>
			<p>Alternatively, the XGBoost random forest's <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> may be implemented as alternatives to scikit-learn's random forests. XGBoost's new <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> outperformed scikit-learn's <strong class="source-inline">RandomForestRegressor</strong> and <strong class="source-inline">RandomForestClassifier</strong>, although the comparison was very close. Given the overall success of XGBoost in the machine learning community, it's definitely worth using <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> as viable options going forward.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor210"/>Summary</h1>
			<p>In this chapter, you greatly extended your range of XGBoost by applying all XGBoost base learners, including <strong class="source-inline">gbtree</strong>, <strong class="source-inline">dart</strong>, <strong class="source-inline">gblinear</strong>, and random forests, to regression and classification datasets. You previewed, applied, and tuned hyperparameters unique to base learners to improve scores. Furthermore, you experimented with <strong class="source-inline">gblinear</strong> using a linearly constructed dataset and with <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> to build XGBoost random forests without any boosting whatsoever. Now that you have worked with all base learners, your comprehension of the range of XGBoost is at an advanced level.</p>
			<p>In the next chapter, you will analyze tips and tricks from Kaggle masters to advance your XGBoost skills even further!</p>
		</div>
	</body></html>
<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Create a Simple Estimator</h1>
                </header>
            
            <article>
                
<p>In this chapter we will cover the following recipes:</p>
<ul>
<li>
<p>Creating a simple estimator</p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>We are going to make a custom estimator with scikit-learn. We will take traditional statistical math and programming and turn it into machine learning. You are able to turn any statistics into machine learning by using scikit-learn's powerful cross-validation capabilities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Create a simple estimator</h1>
                </header>
            
            <article>
                
<p>We are going to do some work towards building our own scikit-learn estimator. The custom scikit-learn estimator consists of at least three methods:</p>
<ul>
<li>An <kbd>__init__</kbd> initialization method: This method takes as input the estimator's parameters</li>
<li>A <kbd>fit</kbd> method: This trains the estimator</li>
<li>A <kbd>predict</kbd> method: This method performs a prediction on unseen data</li>
</ul>
<p>Schematically, the class looks like this:</p>
<pre><strong>#Inherit from the classes BaseEstimator, ClassifierMixin</strong><br/><strong>class RidgeClassifier(BaseEstimator, ClassifierMixin):</strong><br/> <br/><strong>     def __init__(self,param1,param2):</strong><br/><strong>          self.param1 = param1</strong><br/><strong>          self.param2 = param2</strong><br/><br/><strong>     def fit(self, X, y = None):</strong><br/><strong>          #do as much work as possible in this method</strong><br/><strong>          return self</strong><br/> <br/><strong>     def predict(self, X_test):</strong><br/><strong>          #do some work here and return the predictions, y_pred</strong><br/><strong>          return y_pred</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Load the breast cancer dataset from scikit learn:</p>
<pre><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><br/><strong>from sklearn.datasets import load_breast_cancer</strong><br/><br/><strong>bc = load_breast_cancer() </strong><br/><br/><strong>new_feature_names = ['_'.join(ele.split()) for ele in bc.feature_names]</strong><br/><br/><strong>X = pd.DataFrame(bc.data,columns = new_feature_names)</strong><br/><strong>y = bc.target</strong></pre>
<p>Split the data into training and testing sets:</p>
<pre><strong>from sklearn.model_selection import train_test_split</strong><br/><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7, stratify = y)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>A scikit estimator should have a <kbd>fit</kbd> method, that returns the class itself, and a <kbd>predict</kbd> method, that returns the predictions:</p>
<ol>
<li>The following is a classifier we call <kbd>RidgeClassifier</kbd>. Import <kbd>BaseEstimator</kbd> and <kbd>ClassifierMixin</kbd> from <kbd>sklearn.base</kbd> and pass them along as arguments to your new classifier:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.base import BaseEstimator, ClassifierMixin</strong><br/><strong>from sklearn.linear_model import Ridge</strong><br/> <br/><strong>class RidgeClassifier(BaseEstimator, ClassifierMixin):</strong><br/>    <br/><strong>    """A Classifier made from Ridge Regression"""</strong><br/>    <br/><strong>    def __init__(self,alpha=0):</strong><br/><strong>        self.alpha = alpha</strong><br/>        <br/><strong>    def fit(self, X, y = None):</strong><br/><strong>        #pass along the alpha parameter to the internal ridge estimator and perform a fit using it</strong><br/><strong>        self.ridge_regressor = Ridge(alpha = self.alpha) </strong><br/><strong>        self.ridge_regressor.fit(X, y)</strong><br/>        <br/><strong>        #save the seen class labels</strong><br/><strong>        self.class_labels = np.unique(y)</strong><br/>        <br/><strong>        return self</strong><br/>    <br/><strong>    def predict(self, X_test):</strong><br/><strong>        #store the results of the internal ridge regressor estimator</strong><br/><strong>        results = self.ridge_regressor.predict(X_test)</strong><br/>        <br/><strong>        #find the nearest class label</strong><br/><strong>        return np.array([self.class_labels[np.abs(self.class_labels - x).argmin()] for x in results])</strong></pre>
<p style="padding-left: 60px">Let's focus on the <kbd>__init__</kbd> method. There, we input a single parameter; it corresponds to the regularization parameter in the underlying ridge regressor.</p>
<p style="padding-left: 60px">In the <kbd>fit</kbd> method, we perform all of the work. The work consists of using an internal ridge regressor and storing the class labels within the data. We might want to throw an error if there are more than two classes, as many classes usually do not map well to a set of real numbers. In this example, there are two possible targets: malignant cancer or benign cancer. They  map to real numbers as the degree of malignancy, which can be viewed as diametrically opposed to benignness. In the iris dataset, there are Setosa, Versicolor, and Virginica flowers. The Setosaness quality does not have a guaranteed diametric opposite except looking at the classifier in a one-versus-rest manner.</p>
<p style="padding-left: 60px">In the <kbd>predict</kbd> method, you find the class label that is closest to what the ridge regressor predicts.</p>
<ol start="2">
<li>Now write a few lines applying your new ridge classifier:</li>
</ol>
<pre style="padding-left: 60px"><strong>r_classifier = RidgeClassifier(1.5)     </strong><br/><strong>r_classifier.fit(X_train, y_train)</strong><br/><strong>r_classifier.score(X_test, y_test)</strong><br/><br/><strong>0.95744680851063835</strong></pre>
<ol start="3">
<li>It scores pretty well on the test set. You can perform a grid search on it as well:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import GridSearchCV</strong><br/><br/><strong>param_grid = {'alpha': [0,0.5,1.0,1.5,2.0]}</strong><br/><strong>gs_rc = GridSearchCV(RidgeClassifier(), param_grid, cv = 3).fit(X_train, y_train)</strong><br/><br/><strong>gs_rc.grid_scores_</strong><br/><br/><strong>[mean: 0.94751, std: 0.00399, params: {'alpha': 0},
 mean: 0.95801, std: 0.01010, params: {'alpha': 0.5},
 mean: 0.96063, std: 0.01140, params: {'alpha': 1.0},
 mean: 0.96063, std: 0.01140, params: {'alpha': 1.5},
 mean: 0.96063, std: 0.01140, params: {'alpha': 2.0}]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The point of making your own estimator is that the estimator inherits properties from the scikit-learn base estimator and classifier classes. In the line:</p>
<pre><strong>r_classifier.score(X_test, y_test)</strong></pre>
<p>Your classifier looked at the default accuracy score for all scikit-learn classifiers. Conveniently, you did not have to look it up or implement it. Besides, when it came to using your classifier, the procedure was very similar to using any scikit classifier.</p>
<p>In the following example, we use a logistic regression classifier:</p>
<pre><strong>from sklearn.linear_model import LogisticRegression</strong><br/> <br/><strong>lr = LogisticRegression()</strong><br/><strong>lr.fit(X_train,y_train)</strong><br/><strong>lr.score(X_test,y_test)</strong><br/><br/><strong>0.9521276595744681</strong></pre>
<div class="packt_tip">Your new classifier did slightly better than logistic regression.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>At times, statistical packages such as Python's <kbd>statsmodels</kbd> or <kbd>rpy</kbd> (an interface to R within Python) contain very interesting statistical methods and you would want to pass them through scikit's cross-validation. Alternatively, you could have written the method and would like to cross-validate it.</p>
<p>The following is a custom estimator constructed using the <kbd>statsmodels</kbd> <strong>general estimating equation</strong> (<strong>GEE</strong>) available at <a href="http://www.statsmodels.org/dev/gee.html" target="_blank">http://www.statsmodels.org/dev/gee.html</a>.</p>
<p>The GEEs use general linear models (that borrow from R) and we can choose a group-like variable where observations are possibly correlated within a cluster but uncorrelated across clusters—in the words of the documentation. Thus we can group, or cluster, by some variable and see within-group correlations.</p>
<p>Here, we create a model from the breast cancer data based on the R-style formula:</p>
<pre><strong>'y ~ mean_radius + mean_texture + mean_perimeter + mean_area + mean_smoothness + mean_compactness + mean_concavity + mean_concave_points + mean_symmetry + mean_fractal_dimension + radius_error + texture_error + perimeter_error + area_error + smoothness_error + compactness_error + concavity_error + concave_points_error + symmetry_error + fractal_dimension_error + worst_radius + worst_texture + worst_perimeter + worst_area + worst_smoothness + worst_compactness + worst_concavity + worst_concave_points + worst_symmetry + worst_fractal_dimension'</strong></pre>
<p>We cluster by the feature <kbd>mean_concavity</kbd> (the variable <kbd>mean_concavity</kbd> is not included in the R-style formula). Start by importing the <kbd>statsmodels</kbd> module's libraries. The example is as follows:</p>
<pre><strong>import statsmodels.api as sm</strong><br/><strong>import statsmodels.formula.api as smf</strong><br/><br/><strong>from sklearn.base import BaseEstimator, ClassifierMixin</strong><br/><br/><br/><strong>from sklearn.linear_model import Ridge</strong><br/><br/><strong>class GEEClassifier(BaseEstimator, ClassifierMixin):</strong><br/>    <br/><strong>    """A Classifier made from statsmodels' Generalized Estimating Equations </strong><strong>documentation available at: http://www.statsmodels.org/dev/gee.html</strong><br/>  <strong>  """</strong><br/>    <br/><strong>    def __init__(self,group_by_feature):</strong><br/><strong>        self.group_by_feature = group_by_feature</strong><br/>          <br/><strong>    def fit(self, X, y = None):</strong><br/><strong>        #Same settings as the documentation's example: </strong><br/><strong>        self.fam = sm.families.Poisson()</strong><br/><strong>        self.ind = sm.cov_struct.Exchangeable()</strong><br/>        <br/><strong>        #Auxiliary function: only used in this method within the class</strong><br/><strong>        def expand_X(X, y, desired_group): </strong><br/><strong>            X_plus = X.copy()</strong><br/><strong>            X_plus['y'] = y</strong><br/>    <br/><strong>            #roughly make ten groups</strong><br/><strong>            X_plus[desired_group + '_group'] = (X_plus[desired_group] * 10)//10</strong><br/>    <br/><strong>            return X_plus</strong><br/>        <br/><strong>        #save the seen class labels</strong><br/><strong>        self.class_labels = np.unique(y)</strong><br/>        <br/><strong>        dataframe_feature_names = X.columns</strong><br/><strong>        not_group_by_features = [x for x in dataframe_feature_names if x != self.group_by_feature]</strong><br/>        <br/><strong>        formula_in = 'y ~ ' + ' + '.join(not_group_by_features)</strong><br/>        <br/><strong>        data = expand_X(X,y,self.group_by_feature)</strong><br/><strong>        self.mod = smf.gee(formula_in, </strong><br/><strong>                           self.group_by_feature + "_group", </strong><br/><strong>                           data, </strong><br/><strong>                           cov_struct=self.ind, </strong><br/><strong>                           family=self.fam)</strong><br/>        <br/><strong>        self.res = self.mod.fit()</strong><br/>        <br/><strong>        return self</strong><br/>    <br/><strong>    def predict(self, X_test):</strong><br/><strong>        #store the results of the internal GEE regressor estimator</strong><br/><strong>        results = self.res.predict(X_test)</strong><br/>        <br/><strong>        #find the nearest class label</strong><br/><strong>        return np.array([self.class_labels[np.abs(self.class_labels - x).argmin()] for x in results])</strong><br/>        <br/><strong>    def print_fit_summary(self):</strong><br/><strong>        print res.summary()</strong><br/><strong>        return self</strong></pre>
<p>The code within the <kbd>fit</kbd> method is similar to the code within the GEE documentation. You can work it out for your particular situation or statistical method. The code within the <kbd>predict</kbd> method is similar to the ridge classifier you created.</p>
<p>If you run the code like you did for the ridge estimator:</p>
<pre><strong>gee_classifier = GEEClassifier('mean_concavity')     </strong><br/><strong>gee_classifier.fit(X_train, y_train)</strong><br/><strong>gee_classifier.score(X_test, y_test)</strong><br/><br/><strong>0.94680851063829785</strong></pre>
<p>The point is that you turned a traditional statistical method into a machine learning method using scikit-learn's cross-validation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Trying the new GEE classifier on the Pima diabetes dataset</h1>
                </header>
            
            <article>
                
<p>Try the GEE classifier on the Pima diabetes dataset. Load the dataset:</p>
<pre><strong>import pandas as pd</strong><br/><br/><strong>data_web_address = "https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"</strong><br/><br/><strong>column_names = ['pregnancy_x', </strong><br/><strong>                'plasma_con', </strong><br/><strong>                'blood_pressure', </strong><br/><strong>                'skin_mm', </strong><br/><strong>                'insulin', </strong><br/><strong>                'bmi', </strong><br/><strong>                'pedigree_func', </strong><br/><strong>                'age', </strong><br/><strong>                'target']</strong><br/><br/><strong>feature_names = column_names[:-1]</strong><br/><strong>all_data = pd.read_csv(data_web_address , names=column_names)</strong><br/><br/><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><br/><strong>X = all_data[feature_names]</strong><br/><strong>y = all_data['target']</strong></pre>
<p>Split the dataset into training and testing:</p>
<pre><strong>from sklearn.model_selection import train_test_split</strong><br/><strong> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)</strong></pre>
<p>Predict by using the GEE classifier. We will use the <kbd>blood_pressure</kbd> column as the column to group by:</p>
<pre><strong>gee_classifier = GEEClassifier('blood_pressure')     </strong><br/><strong>gee_classifier.fit(X_train, y_train)</strong><br/><strong>gee_classifier.score(X_test, y_test)</strong><br/><br/><strong>0.80519480519480524</strong></pre>
<p>You can also try the ridge classifier:</p>
<pre><strong>r_classifier = RidgeClassifier()     </strong><br/><strong>r_classifier.fit(X_train, y_train)</strong><br/><strong>r_classifier.score(X_test, y_test)</strong><br/><br/><strong>0.76623376623376627</strong></pre>
<p>You can compare these—the ridge classifier and GEE classifier—with logistic regression in the <a href="d2473ebe-f050-4e72-bbf9-fabe5d62d441.xhtml" target="_blank">Chapter 5</a>, <em>Linear Models – Logistic Regression</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving your trained estimator</h1>
                </header>
            
            <article>
                
<p>Saving your custom estimator is the same as saving any scikit-learn estimator. Save the trained ridge classifier in the file <kbd>rc_inst.save</kbd> as follows:</p>
<pre><strong>import pickle</strong><br/><br/><strong>f = open('rc_inst.save','wb')</strong><br/><strong>pickle.dump(r_classifier, f, protocol = pickle.HIGHEST_PROTOCOL)</strong><br/><strong>f.close()</strong></pre>
<p>To retrieve the trained classifier and use it, do this:</p>
<pre><strong>import pickle</strong><br/><br/><strong>f = open('rc_inst.save','rb')</strong><br/><strong>r_classifier = pickle.load(f)</strong><br/><strong>f.close()</strong></pre>
<p>It is very simple to save a trained custom estimator in scikit-learn.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>
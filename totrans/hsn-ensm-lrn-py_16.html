<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Evaluating Sentiment on Twitter</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Twitter is a highly popular social network with over 300 million monthly active users. The platform has been developed around short posts (limited to a number of characters; currently, the limit is 280 characters). The posts themselves are called tweets. On average, 6000 tweets are tweeted every second, which equates to around 200 billion tweets per year. This constitutes a huge amount of data that contains an equal amount of information. As is obvious, it is not possible to analyze this volume of data by hand. Thus, automated solutions have been employed, both by Twitter and third parties. One of the hottest topics involves a tweet's sentiment, or how the user feels about the topic that they tweets. Sentiment analysis comes in many flavors. The most common approach is a positive or negative classification of each tweet. Other approaches involve a more complex analysis of positive and negative emotions, such as anger, disgust, fear, happiness, sadness, and surprise. In this chapter, we will briefly present some sentiment analysis tools and practices. Following this, we will cover the basics of building a classifier that leverages ensemble learning techniques in order to classify tweets. Finally, we will see how we can classify tweets in real time by using Twitter's API.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Sentiment analysis tools</li>
<li>Getting Twitter data</li>
<li>Creating a model</li>
<li>Classifying tweets in real time</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter11">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter11</a></p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/2XSLQ5U">http://bit.ly/2XSLQ5U</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis tools </h1>
                </header>
            
            <article>
                
<p>Sentiment analysis can be implemented in a number of ways. The easiest to both implement and understand are lexicon-based approaches. These methods leverage the use of lists (lexicons) of polarized words and expressions. Given a sentence, these methods count the number of positive and negative words and expressions. If there are more positive words/expressions, the sentence is labeled as positive. If there are more negative than positive words/expressions, the sentence is labeled as negative. If the number of positive and negative words/expressions are equal, the sentence is labeled as neutral. Although this approach is relatively easy to code and does not require any training, it has two major disadvantages. First, it does not take into account interactions between words. For example, <em>not bad</em>, which is actually a positive expression, can be classified as negative, as it is composed of two negative words. Even if the expression is included in the lexicon under positive, the expression <em>not that bad</em> may not be included. The second disadvantage is that the whole process relies on good and complete lexicons. If the lexicon omits certain words, the results can be very poor.</p>
<p>Another approach is to train a machine learning model in order to classify sentences. In order to do so, a training dataset has to be created, where a number of sentences are labeled as positive or negative by human experts. This process indirectly uncovers a hidden problem in (and also indicates the difficulty of) sentiment analysis. Human analysts agree on 80% to 85% of the cases. This is partly due to the subjective nature of many expressions. For example, the sentence <em>Today the weather is nice, yesterday it was bad</em>, can be either positive, negative, or neutral. This depends on intonation. Assuming that the bold word is intonated, <em>Today the weather is <strong>nice</strong>, yesterday it was bad</em> is positive. <em>Today the weather is nice, yesterday it was <strong>bad</strong> </em>is negative, while <em>Today the weather is nice, yesterday it was bad</em> is actually neutral (a simple observation of a change in the weather).</p>
<div class="packt_tip">
<p>You can read more about the problem of disagreement between human analysts in sentiment classification at: <a href="https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview">https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview</a>.<a href="https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview"/></p>
</div>
<p>In order to create machine learning features from text data, usually, n-grams are created. N-grams are sequences of<span> </span><em>n</em><span> </span>words extracted from each sentence. For example, the sentence "Hello there, kids" contains the following:</p>
<ul>
<li>1-grams: "Hello", "there,", "kids"</li>
<li>2-grams: "Hello there,”, "there, kids"</li>
<li>3-grams: "Hello there, kids"</li>
</ul>
<p>In order to create numeric features for a dataset, a single feature is created for each unique N-gram. For each instance, the feature's value depends on the number of times it appears in the sentence. For example, consider the following toy dataset:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 50.8264%">
<p><strong>Sentence</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 48.3471%">
<p><strong>Polarity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 50.8264%">
<p>My head hurts</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 48.3471%">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 50.8264%">
<p>The food was good food</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 48.3471%">
<p>Negative</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 50.8264%">
<p>The sting hurts</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 48.3471%">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 50.8264%">
<p>That was a good time</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 48.3471%">
<p>Negative</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>A sentiment toy dataset</span></div>
<p>Assume that we will only use 1-grams (unigrams). The unique unigrams contained in the dataset are: "My", "head", "hurts", "The", "food", "was", "good", "sting", "That", "a", and "time". Thus, each instance has 11 features. Each feature corresponds to a single <em>n</em>-gram (in our case, a unigram). Each feature’s value equals the number of appearances of the corresponding <em>n</em>-gram in the instance. The final dataset is depicted in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 5%">
<p><strong>My</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p><strong>Head</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 9%">
<p><strong>Hurts</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6%">
<p><strong>The</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p><strong>Food</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p><strong>Was</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p><strong>Good</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p><strong>Sting</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p><strong>That</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6.51515%">
<p><strong>A</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 10%">
<p><strong>Time</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 14%">
<p><strong>Polarity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 5%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 9%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6.51515%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 10%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 14%">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 5%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 9%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6.51515%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 10%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 14%">
<p>Negative</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 5%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 9%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6.51515%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 10%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 14%">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 5%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 9%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 8%">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 6.51515%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 10%">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 14%">
<p>Negative</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The extracted features dataset</span></div>
<p>Usually, each instance is normalized, so each feature represents the relative frequency, rather than the absolute frequency (count), of each <em>n</em>-gram. This method is called <strong>Term Frequency</strong> (<strong>TF</strong>). The TF dataset is depicted as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>My</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Head</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Hurts</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>The</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Food</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Was</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Good</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sting</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>That</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>A</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Time</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Polarity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0.33</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.33</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.33</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Negative</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.33</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.33</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.33</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Negative</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The TF dataset</span></div>
<p>In the English language, some terms exhibit a really high frequency, while contributing little towards the expression’s sentiment. In order to account for this fact, <strong>Inverse Document Frequency</strong> (<strong>IDF</strong>) is employed. IDF puts more emphasis on infrequent terms. For <em>N</em> instances with <em>K</em> unique unigrams, the IDF of unigram <em>u</em>, which is present in <em>M</em> instances, is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cfb7ea6d-9315-4036-8d83-4203240902dd.png" style="width:8.92em;height:2.50em;"/></p>
<p>The following table depicts the IDF-transformed dataset:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>My</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Head</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Hurts</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>The</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Food</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Was</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Good</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sting</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>That</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>A</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Time</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Polarity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Negative</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Negative</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The IDF dataset</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stemming</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Stemming is another practice usually utilized in sentiment analysis. It is the process of reducing words to their root. This lets us handle words that originate from the same root as a single unigram. For example, <em>love</em>, <em>loving</em>, and <em>loved</em> will be all handled as the same unigram, <em>love</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting Twitter data</h1>
                </header>
            
            <article>
                
<p>There are a number of ways to gather Twitter data. From web scraping to using custom libraries, each one has different advantages and disadvantages. For our implementation, as we also need sentiment labeling, we will utilize the <kbd>Sentiment140</kbd> dataset (<a href="http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip">http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip</a>). The reason that we do not collect our own data is mostly due to the time we would need to label it. In the last section of this chapter, we will see how we can collect our own data and analyze it in real time. The dataset consists of 1.6 million tweets, containing the following 6 fields:</p>
<ul>
<li>The tweet's polarity</li>
<li>A numeric ID</li>
<li>The date it was tweeted</li>
<li>The query used to record the tweet</li>
<li>The user's name</li>
<li>The tweet's text content</li>
</ul>
<p>For our models, we will only need the tweet's text and polarity. As can be seen in the following graph, there are 800,000 positive (with a polarity 4) and 800,000 negative (with a polarity 0) tweets:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-678 image-border" src="assets/3b782be9-bd85-4c47-a9ef-d13a8eaf4a44.png" style="width:34.08em;height:25.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Polarity distribution</div>
<p>Here, we can also verify the statement we made earlier about word frequencies. The following graph depicts the 30 most common words in the dataset. As is evident, none of them bears any sentiment. Thus, an IDF transform would be more beneficial to our models:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-679 image-border" src="assets/58641b42-29bc-42aa-b6f0-89246483f46d.png" style="width:40.25em;height:30.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The 30 most common words in the dataset and the number of occurrences of each</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a model</h1>
                </header>
            
            <article>
                
<p>The most important step in sentiment analysis (as is the case with most machine learning problems) is the preprocessing of our data. The following table contains 10 tweets, randomly sampled from the dataset:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>id</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>text</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>44</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>@JonathanRKnight Awww I soo wish I was there to see...</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>143873</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Shaking stomach flipping........god i hate thi...</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>466449</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>why do they refuse to put nice things in our v...</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1035127</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>@KrisAllenmusic visit here</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>680337</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Rafa out of Wimbledon Love Drunk by BLG out S...</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>31250</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>It's official, printers hate me Going to sul...</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1078430</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>@_Enigma__ Good to hear</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1436972</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Dear Photoshop CS2. i love you. and i miss you!</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>401990</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>my boyfriend got in a car accident today !</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1053169</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Happy birthday, Wisconsin! 161 years ago, you ...</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>An outline of 10 random samples from the dataset</span></div>
<p>We can immediately make the following observations. First, there are references to other users, for example, <kbd>@KrisAllenmusic</kbd>. These references do not provide any information about the tweet's sentiment. Thus, during preprocessing, we will remove them. Second, there are numbers and punctuation. These also do not contribute to the tweet’s sentiment, so they must also be removed. Third, some letters are capitalized while others are not. As capitalization does not alter the word’s sentiment, we can choose to either convert all letters to lowercase or to convert them to uppercase. This ensures that words such as <em>LOVE</em>, <em>love</em>, and <em>Love</em> will be handled as the same unigram. If we sample more tweets, we can identify more problems. There are hashtags (such as <kbd>#summer</kbd>), which also do not contribute to the tweet’s sentiment. Furthermore, there are URL links (for example <a href="https://www.packtpub.com/eu/">https://www.packtpub.com/eu/</a>) and HTML attributes (such as <kbd>&amp;amp</kbd> which corresponds to <kbd>&amp;</kbd>). These will also be removed during our preprocessing.</p>
<p>In order to preprocess our data, first, we must import the required libraries. We will use pandas; Python's built-in regular expressions library, <kbd>re</kbd>; <kbd>punctuation</kbd> from <kbd>string</kbd>; and the <strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>). The <kbd>nltk</kbd> library can be easily installed either through <kbd>pip</kbd> or <kbd>conda</kbd> as follows:</p>
<pre class="CodePACKT">import pandas as pd<br/>import re<br/>from nltk.corpus import stopwords<br/>from nltk.stem import PorterStemmer<br/>from string import punctuation</pre>
<p class="CodePACKT">After loading the libraries, we load the data, change the polarity from <em>[0, 4]</em> to <em>[0, 1]</em>, and discard all fields except for the text content and the polarity:</p>
<pre class="CodePACKT"># Read the data and assign labels<br/>labels = ['polarity', 'id', 'date', 'query', 'user', 'text']<br/>data = pd.read_csv("sent140.csv", names=labels)<br/> <br/># Keep only text and polarity, change polarity to 0-1<br/>data = data[['text', 'polarity']]<br/>data.polarity.replace(4, 1, inplace=True)</pre>
<p class="CodePACKT">As we saw earlier, many words do not contribute to a tweet's sentiment, although they frequently appear in text. Search engines handle this by removing such words, which are called stop words. NLTK has a list of the most common stop words that we are going to utilize. Furthermore, as there are a number of stop words that are contractions (such as "you're" and "don't") and tweets frequently omit single quotes in contractions, we expand the list in order to include contractions without single quotes (such as "dont"):</p>
<pre># Create a list of stopwords<br/>stops = stopwords.words("english")<br/># Add stop variants without single quotes<br/>no_quotes = []<br/>for word in stops:<br/>    if "'" in word:<br/>        no_quotes.append(re.sub(r'\'', '', word))<br/>stops.extend(no_quotes)</pre>
<p>We then define two distinct functions. The first function, <kbd>clean_string</kbd>, cleans the tweet by removing all elements we discussed earlier (such as references, hashtags, and so on). The second function removes all punctuation or stop word and stems each word, by utilizing NLTK's <kbd>PorterStemmer</kbd>:</p>
<pre>def clean_string(string):<br/>    # Remove HTML entities<br/>    tmp = re.sub(r'\&amp;\w*;', '', string)<br/>    # Remove @user<br/>    tmp = re.sub(r'@(\w+)', '', tmp)<br/>    # Remove links<br/>    tmp = re.sub(r'(http|https|ftp)://[a-zA-Z0-9\\./]+', '', tmp)<br/>    # Lowercase<br/>    tmp = tmp.lower()<br/>    # Remove Hashtags<br/>    tmp = re.sub(r'#(\w+)', '', tmp)<br/>    # Remove repeating chars<br/>    tmp = re.sub(r'(.)\1{1,}', r'\1\1', tmp)<br/>    # Remove anything that is not letters<br/>    tmp = re.sub("[^a-zA-Z]", " ", tmp)<br/>    # Remove anything that is less than two characters<br/>    tmp = re.sub(r'\b\w{1,2}\b', '', tmp)<br/>    # Remove multiple spaces<br/>    tmp = re.sub(r'\s\s+', ' ', tmp)<br/>    return tmp<br/><br/>def preprocess(string):<br/>    stemmer = PorterStemmer()<br/>    # Remove any punctuation character<br/>    removed_punc = ''.join([char for char in string if char not in punctuation])<br/>    cleaned = []<br/>    # Remove any stopword<br/>    for word in removed_punc.split(' '):<br/>        if word not in stops:<br/>            cleaned.append(stemmer.stem(word.lower()))<br/>    return ' '.join(cleaned)</pre>
<p>As we would like to compare the performance of the ensemble with the base learners themselves, we will define a function that will evaluate any given classifier. The two most important factors that will define our dataset are the n-grams we will use and the number of features. Scikit-learn has an implementation of an IDF feature extractor, the <kbd>TfidfVectorizer</kbd> class. This allows us to only utilize the top <em>M</em> most frequent features, as well as define the n-gram range we will use, through the <kbd>max_features</kbd> and <kbd>ngram_range</kbd> parameters. It creates sparse arrays of features, which saves a great deal of memory, but the results must be converted to normal arrays before they can be processed by scikit-learn's classifiers. This is achieved by calling the <kbd>toarray()</kbd> function. Our <kbd>check_features_ngrams</kbd> function accepts the number of features, a tuple of minimum and maximum n-grams, and a list of named classifiers (a name, classifier tuple). It extracts the required features from the dataset and passes them to the nested <kbd>check_classifier</kbd>. This function trains and evaluates each classifier, as well as exports the results to the specified file, <kbd>outs.txt</kbd>:</p>
<pre>def check_features_ngrams(features, n_grams, classifiers):<br/>    print(features, n_grams)<br/><br/>    # Create the IDF feature extractor<br/>    tf = TfidfVectorizer(max_features=features, ngram_range=n_grams,<br/>                         stop_words='english')<br/><br/>    # Create the IDF features<br/>    tf.fit(data.text)<br/>    transformed = tf.transform(data.text)<br/>    np.random.seed(123456)<br/><br/>    def check_classifier(name, classifier):<br/>        print('--'+name+'--')<br/><br/>        # Train the classifier<br/>        x_data = transformed[:train_size].toarray()<br/>        y_data = data.polarity[:train_size].values<br/>        classifier.fit(x_data, y_data)<br/>        i_s = metrics.accuracy_score(y_data, classifier.predict(x_data))<br/><br/>        # Evaluate on the test set<br/>        x_data = transformed[test_start:test_end].toarray()<br/>        y_data = data.polarity[test_start:test_end].values<br/>        oos = metrics.accuracy_score(y_data, classifier.predict(x_data))<br/><br/>        # Export the results<br/>        with open("outs.txt","a") as f:<br/>            f.write(str(features)+',')<br/>            f.write(str(n_grams[-1])+',')<br/>            f.write(name+',')<br/>            f.write('%.4f'%i_s+',')<br/>            f.write('%.4f'%oos+'\n')<br/><br/>    for name, classifier in classifiers:<br/>        check_classifier(name, classifier)<br/>Finally, we test for n-grams in the range of [1, 3] and for the top 500, 1000, 5000, 10000, 20000, and 30000 features.<br/><br/># Create csv header<br/>with open("outs.txt","a") as f:<br/>    f.write('features,ngram_range,classifier,train_acc,test_acc')<br/># Test all features and n-grams combinations<br/>for features in [500, 1000, 5000, 10000, 20000, 30000]:<br/>    for n_grams in [(1, 1), (1, 2), (1, 3)]:<br/>    # Create the ensemble<br/>        voting = VotingClassifier([('LR', LogisticRegression()),<br/>                                   ('NB', MultinomialNB()),<br/>                                    ('Ridge', RidgeClassifier())])<br/>    # Create the named classifiers<br/>    classifiers = [('LR', LogisticRegression()),<br/>                    ('NB', MultinomialNB()),<br/>                    ('Ridge', RidgeClassifier()),<br/>                    ('Voting', voting)]<br/>     # Evaluate them<br/>     check_features_ngrams(features, n_grams, classifiers)</pre>
<p>The results are depicted in the following diagram. As is evident, as we increase the number of features, the accuracy increases for all classifiers. Furthermore, if the number of features is relatively small, unigrams outperform combinations of unigrams and bigrams/trigrams. This is due to the fact that the most frequent expressions are not sentimental. Finally, although voting exhibits a relatively satisfactory performance, it is not able to outperform logistic regression:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-680 image-border" src="assets/0300f491-87fe-4535-8f10-53273d2ff456.png" style="width:35.83em;height:27.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Results of voting and base learners</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying tweets in real time</h1>
                </header>
            
            <article>
                
<div>
<p>We can use our model in order to classify tweets in real time using Twitter’s API. In order to simplify things, we will make use of a very popular wrapper library for the API, <kbd>tweepy</kbd> (<a href="https://github.com/tweepy/tweepy">https://github.com/tweepy/tweepy</a>). Installation is easily achieved with <kbd>pip install tweepy</kbd>. The first step to accessing Twitter programmatically is to generate relevant credentials. This is achieved by navigating to <a href="https://apps.twitter.com/" target="_blank">https://apps.twitter.com/</a> and selecting <span class="packt_screen">Create an app</span>. The application process is straightforward and should be accepted quickly.</p>
<p>Using tweepy's <kbd>StreamListener</kbd>, we will define a class that listens for incoming tweets, and as soon as they arrive, it classifies them and prints the original text and predicted polarity. First, we will load the required libraries. As a classifier, we will utilize the voting ensemble we trained earlier. First, we load the required libraries. We need the <kbd>json</kbd> library, as tweets are received in the JSON format; parts of the <kbd>tweepy</kbd> library; as well as the scikit-learn components we utilized earlier. Furthermore, we store our API keys in variables:</p>
</div>
<pre class="CodePACKT">import pandas as pd<br/>import json<br/>from sklearn.ensemble import VotingClassifier<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.linear_model import LogisticRegression, RidgeClassifier<br/>from sklearn.naive_bayes import MultinomialNB<br/>from tweepy import OAuthHandler, Stream, StreamListener<br/># Please fill your API keys as strings<br/>consumer_key="HERE,"<br/>consumer_secret="HERE,"<br/><br/>access_token="HERE,"<br/>access_token_secret="AND HERE"</pre>
<p>We then proceed to create and train our <kbd>TfidfVectorizer</kbd> and <kbd>VotingClassifier</kbd> with 30,000 features and n-grams in the <em>[1, 3]</em> range:</p>
<pre># Load the data<br/>data = pd.read_csv('sent140_preprocessed.csv')<br/>data = data.dropna()<br/># Replicate our voting classifier for 30.000 features and 1-3 n-grams<br/>train_size = 10000<br/>tf = TfidfVectorizer(max_features=30000, ngram_range=(1, 3),<br/>                         stop_words='english')<br/>tf.fit(data.text)<br/>transformed = tf.transform(data.text)<br/>x_data = transformed[:train_size].toarray()<br/>y_data = data.polarity[:train_size].values<br/>voting = VotingClassifier([('LR', LogisticRegression()),<br/>                           ('NB', MultinomialNB()),<br/>                           ('Ridge', RidgeClassifier())])<br/>voting.fit(x_data, y_data)</pre>
<p>We then proceed with defining our <kbd>StreamClassifier</kbd> class, responsible for listening for incoming tweets and classifying them as they arrive. It inherits the <kbd>StreamListener</kbd> class from <kbd>tweepy</kbd>. By overriding the <kbd>on_data</kbd> function, we are able to process tweets as they arrive through the stream. The tweets arrive in JSON format, so we first parse them with <kbd>json.loads(data)</kbd>, which returns a dictionary, and then extract the text using the <kbd>"text"</kbd> key. We can then extract the features using the fitted <kbd>vectorizer</kbd> and utilize the features in order to predict its polarity:</p>
<pre># Define the streaming classifier<br/>class StreamClassifier(StreamListener):<br/>    def __init__(self, classifier, vectorizer, api=None):<br/>        super().__init__(api)<br/>        self.clf = classifier<br/>        self.vec = vectorizer<br/>    # What to do when a tweet arrives<br/>    def on_data(self, data):<br/>        # Create a json object<br/>        json_format = json.loads(data)<br/>        # Get the tweet's text<br/>        text = json_format['text']<br/>        features = self.vec.transform([text]).toarray()<br/>        print(text, self.clf.predict(features))<br/>        return True<br/>    # If an error occurs, print the status<br/>    def on_error(self, status):<br/>        print(status)</pre>
<p>Finally, we instantiate <kbd>StreamClassifier</kbd>, passing as arguments, the trained voting ensemble and <kbd>TfidfVectorizer</kbd> and authenticate using the <kbd>OAuthHandler</kbd>. In order to start the stream, we instantiate a <kbd>Stream</kbd> object with the <kbd>OAuthHandler</kbd> and <kbd>StreamClassifier</kbd> objects as parameters and define the keywords we want to track with <kbd>filter(track=['Trump'])</kbd>. In this case, we track tweets that contain the keyword <kbd>'Trump'</kbd> as shown here:</p>
<pre class="CodePACKT"># Create the classifier and authentication handlers<br/>classifier = StreamClassifier(classifier=voting, vectorizer=tf)<br/>auth = OAuthHandler(consumer_key, consumer_secret)<br/>auth.set_access_token(access_token, access_token_secret)<br/><br/># Listen for specific hashtags<br/>stream = Stream(auth, classifier)<br/>stream.filter(track=['Trump'])</pre>
<p>That's it! The preceding code now tracks any tweet containing the keyword Trump and predicts its sentiment in real time. The following table depicts some simple tweets that were classified:</p>
<table border="1" style="border-collapse: collapse;width: 100.677%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 92.6428%">
<p><strong>Text</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7.55082%">
<p><strong>Polarity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 92.6428%">
<p><strong>RT @BillyBaldwin</strong>: Only two things funnier than my brothers impersonation of Trump. Your daughters impersonation of being an honest, decent…</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7.55082%">
<p>Negative</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 92.6428%">
<p><strong>RT @danpfeiffer</strong>: This is a really important article for Democrats to read. Media reports of Trump’s malfeasance is only the start. It's the…</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7.55082%">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 92.6428%">
<p><strong>RT @BillKristol</strong>: "In other words, Trump had backed himself, not Mexico, into a corner. They had him. He had to cave. And cave he did. He go…</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7.55082%">
<p>Positive</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 92.6428%">
<p><strong>RT @SenJeffMerkley</strong>: That Ken Cuccinelli started today despite not being nominated is unacceptable. Trump is doing an end run around the Sen…</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 7.55082%">
<p>Negative</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Example of tweets being classified</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the possibility of using ensemble learning in order to classify tweets. Although a simple logistic regression can outperform ensemble learning techniques, it is an interesting introduction to the realm of natural language processing and the techniques that are used in order to preprocess the data and extract useful features. In summary, we introduced the concepts of n-grams, IDF feature extraction, stemming, and stop word removal. We discussed the process of cleaning the data, as well as training a voting classifier and using it to classify tweets in real time using Twitter's API.</p>
<p>In the next chapter, we will see how ensemble learning can be utilized in the design of recommender systems, with the aim of recommending movies to a specific user.</p>


            </article>

            
        </section>
    </body></html>
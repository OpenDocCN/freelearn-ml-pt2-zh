["```py\n    import langdetect\n    import matplotlib.pyplot\n    import nltk\n    import numpy\n    import pandas\n    import pyLDAvis\n    import pyLDAvis.sklearn\n    import regex\n    import sklearn\n    ```", "```py\n    nltk.download('wordnet')\n    nltk.download('stopwords')\n    ```", "```py\n    %matplotlib inline\n    ```", "```py\n    path = \"News_Final.csv\"\n    df = pandas.read_csv(path, header=0)\n    ```", "```py\n    def dataframe_quick_look(df, nrows):\n        print(\"SHAPE:\\n{shape}\\n\".format(shape=df.shape))\n        print(\"COLUMN NAMES:\\n{names}\\n\".format(names=df.columns))\n        print(\"HEAD:\\n{head}\\n\".format(head=df.head(nrows)))\n    dataframe_quick_look(df, nrows=2)\n    ```", "```py\n    print(\"TOPICS:\\n{topics}\\n\".format(topics=df[\"Topic\"].value_counts()))\n    ```", "```py\n    TOPICS:\n    economy      33928\n    obama        28610\n    microsoft    21858\n    palestine     8843\n    Name: Topic, dtype: int64\n    ```", "```py\n    raw = df[\"Headline\"].tolist()\n    print(\"HEADLINES:\\n{lines}\\n\".format(lines=raw[:5]))\n    print(\"LENGTH:\\n{length}\\n\".format(length=len(raw)))\n    ```", "```py\n    example = raw[5]\n    print(example)\n    ```", "```py\n    def do_language_identifying(txt):\n        try: the_language = langdetect.detect(txt)\n        except: the_language = 'none'\n        return the_language\n    print(\"DETECTED LANGUAGE:\\n{lang}\\n\".format(lang=do_language_identifying(example)\n    ))\n    ```", "```py\n    example = example.split(\" \")\n    print(example)\n    ```", "```py\n    example = ['URL' if bool(regex.search(\"http[s]?://\", i))else i for i in example]\n    print(example)\n    ```", "```py\n    example = [regex.sub(\"[^\\\\w\\\\s]|\\n\", \"\", i) for i in example]\n    print(example)\n    ```", "```py\n    example = [regex.sub(\"^[0-9]*$\", \"\", i) for i in example]\n    print(example)\n    ```", "```py\n    example = [i.lower() if i not in \"URL\" else i for i in example]\n    print(example)\n    ```", "```py\n    example = [i for i in example if i not in \"URL\"]\n    print(example)\n    ```", "```py\n    list_stop_words = nltk.corpus.stopwords.words(\"English\")\n    list_stop_words = [regex.sub(\"[^\\\\w\\\\s]\", \"\", i) for i in list_stop_words]\n    print(list_stop_words)\n    ```", "```py\n    example = [i for i in example if i not in list_stop_words]\n    print(example)\n    ```", "```py\n    def do_lemmatizing(wrd):\n        out = nltk.corpus.wordnet.morphy(wrd)\n        return (wrd if out is None else out)\n    example = [do_lemmatizing(i) for i in example]\n    print(example)\n    ```", "```py\n    example = [i for i in example if len(i) >= 5]\n    print(example)\n    ```", "```py\n    def do_headline_cleaning(txt):\n          # identify language of tweet\n          # return null if language not English\n        lg = do_language_identifying(txt)\n        if lg != 'en': \n            return None\n          # split the string on whitespace\n        out = txt.split(\" \")\n          # identify urls\n          # replace with URL\n        out = ['URL' if bool(regex.search(\"http[s]?://\", i)) else i for i in out]\n          # remove all punctuation\n        out = [regex.sub(\"[^\\\\w\\\\s]|\\n\", \"\", i) for i in out]\n          # remove all numerics\n        out = [regex.sub(\"^[0-9]*$\", \"\", i) for i in out]\n          # make all non-keywords lowercase\n        out = [i.lower() if i not in \"URL\" else i for i in out]\n          # remove URL\n        out = [i for i in out if i not in \"URL\"]\n          # remove stopwords\n        list_stop_words = nltk.corpus.stopwords.words(\"English\")\n        list_stop_words = [regex.sub(\"[^\\\\w\\\\s]\", \"\", i) for i in list_stop_words]\n        out = [i for i in out if i not in list_stop_words]\n          # lemmatizing\n        out = [do_lemmatizing(i) for i in out]\n          # keep words 5 or more characters long\n        out = [i for i in out if len(i) >= 5]\n        return out\n    ```", "```py\n    clean = list(map(do_headline_cleaning, raw))\n    ```", "```py\n    clean = list(filter(None.__ne__, clean))\n    print(\"HEADLINES:\\n{lines}\\n\".format(lines=clean[:5]))\n    print(\"LENGTH:\\n{length}\\n\".format(length=len(clean)))\n    ```", "```py\n    clean_sentences = [\" \".join(i) for i in clean]\n    print(clean_sentences[0:10])\n    ```", "```py\n    number_words = 10\n    number_docs = 10\n    number_features = 1000\n    ```", "```py\n    vectorizer1 = sklearn.feature_extraction.text.CountVectorizer(\n        analyzer=\"word\",\n        max_df=0.5, \n        min_df=20, \n        max_features=number_features\n    )\n    clean_vec1 = vectorizer1.fit_transform(clean_sentences)\n    print(clean_vec1[0])\n    ```", "```py\n    feature_names_vec1 = vectorizer1.get_feature_names()\n    ```", "```py\n    def perplexity_by_ntopic(data, ntopics):\n        output_dict = {\n            \"Number Of Topics\": [], \n            \"Perplexity Score\": []\n        }\n\n        for t in ntopics:\n            lda = sklearn.decomposition.LatentDirichletAllocation(\n                n_components=t,\n                learning_method=\"online\",\n                random_state=0\n            )\n            lda.fit(data)\n\n            output_dict[\"Number Of Topics\"].append(t)\n            output_dict[\"Perplexity Score\"].append(lda.perplexity(data))\n\n        output_df = pandas.DataFrame(output_dict)\n\n        index_min_perplexity = output_df[\"Perplexity Score\"].idxmin()\n        output_num_topics = output_df.loc[\n            index_min_perplexity,  # index\n            \"Number Of Topics\"  # column\n        ]\n\n        return (output_df, output_num_topics)\n    ```", "```py\n    df_perplexity, optimal_num_topics = perplexity_by_ntopic(\n        clean_vec1, \n        ntopics=[1, 2, 3, 4, 6, 8, 10]\n    )\n    print(df_perplexity)\n    ```", "```py\n    df_perplexity.plot.line(\"Number Of Topics\", \"Perplexity Score\")\n    ```", "```py\n    lda = sklearn.decomposition.LatentDirichletAllocation(\n        n_components=optimal_num_topics,\n        learning_method=\"online\",\n        random_state=0\n    )\n    lda.fit(clean_vec1)\n    ```", "```py\n    lda_transform = lda.transform(clean_vec1)\n    print(lda_transform.shape)\n    print(lda_transform)\n    ```", "```py\n    lda_components = lda.components_\n    print(lda_components.shape)\n    print(lda_components)\n    ```", "```py\n    def get_topics(mod, vec, names, docs, ndocs, nwords):\n        # word to topic matrix\n        W = mod.components_\n        W_norm = W / W.sum(axis=1)[:, numpy.newaxis]\n        # topic to document matrix\n        H = mod.transform(vec)\n        W_dict = {}\n        H_dict = {}\n        for tpc_idx, tpc_val in enumerate(W_norm):\n            topic = \"Topic{}\".format(tpc_idx)\n            # formatting w\n            W_indices = tpc_val.argsort()[::-1][:nwords]\n            W_names_values = [\n                (round(tpc_val[j], 4), names[j]) \n                for j in W_indices\n            ]\n            W_dict[topic] = W_names_values\n            # formatting h\n            H_indices = H[:, tpc_idx].argsort()[::-1][:ndocs]\n            H_names_values = [\n                (round(H[:, tpc_idx][j], 4), docs[j]) \n                for j in H_indices\n            ]\n            H_dict[topic] = H_names_values\n        W_df = pandas.DataFrame(\n            W_dict, \n            index=[\"Word\" + str(i) for i in range(nwords)]\n        )\n        H_df = pandas.DataFrame(\n            H_dict,\n            index=[\"Doc\" + str(i) for i in range(ndocs)]\n        )\n        return (W_df, H_df)\n    ```", "```py\n    W_df, H_df = get_topics(\n        mod=lda,\n        vec=clean_vec1,\n        names=feature_names_vec1,\n        docs=raw,\n        ndocs=number_docs, \n        nwords=number_words\n    )\n    ```", "```py\n    print(W_df)\n    ```", "```py\n    print(H_df)\n    ```", "```py\n    lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, vectorizer1, R=10)\n    pyLDAvis.display(lda_plot)\n    ```", "```py\n    def plot_tsne(data, threshold):\n        # filter data according to threshold\n        index_meet_threshold = numpy.amax(data, axis=1) >= threshold\n        lda_transform_filt = data[index_meet_threshold]\n        # fit tsne model\n        # x-d -> 2-d, x = number of topics\n        tsne = sklearn.manifold.TSNE(\n            n_components=2, \n            verbose=0, \n            random_state=0, \n            angle=0.5, \n            init='pca'\n        )\n        tsne_fit = tsne.fit_transform(lda_transform_filt)\n        # most probable topic for each headline\n        most_prob_topic = []\n        for i in range(tsne_fit.shape[0]):\n            most_prob_topic.append(lda_transform_filt[i].argmax())\n        print(\"LENGTH:\\n{}\\n\".format(len(most_prob_topic)))\n        unique, counts = numpy.unique(\n            numpy.array(most_prob_topic), \n            return_counts=True\n        )\n        print(\"COUNTS:\\n{}\\n\".format(numpy.asarray((unique, counts)).T))\n        # make plot\n        color_list = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n        for i in list(set(most_prob_topic)):\n            indices = [idx for idx, val in enumerate(most_prob_topic) if val == i]\n            matplotlib.pyplot.scatter(\n                x=tsne_fit[indices, 0], \n                y=tsne_fit[indices, 1], \n                s=0.5, \n                c=color_list[i], \n                label='Topic' + str(i),\n                alpha=0.25\n            )\n        matplotlib.pyplot.xlabel('x-tsne')\n        matplotlib.pyplot.ylabel('y-tsne')\n        matplotlib.pyplot.legend(markerscale=10)\n    ```", "```py\n    plot_tsne(data=lda_transform, threshold=0.75)\n    ```", "```py\n    lda4 = sklearn.decomposition.LatentDirichletAllocation(\n        n_components=4,  # number of topics data suggests\n        learning_method=\"online\",\n        random_state=0\n    )\n    lda4.fit(clean_vec1)\n    ```", "```py\n    W_df4, H_df4 = get_topics(\n        mod=lda4,\n        vec=clean_vec1,\n        names=feature_names_vec1,\n        docs=raw,\n        ndocs=number_docs, \n        nwords=number_words\n    )\n    ```", "```py\n    print(W_df4)\n    ```", "```py\n    print(H_df4)\n    ```", "```py\n    lda4_plot = pyLDAvis.sklearn.prepare(lda4, clean_vec1, vectorizer1, R=10)\n    pyLDAvis.display(lda4_plot)\n    ```", "```py\n    vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer(\n        analyzer=\"word\",\n        max_df=0.5, \n        min_df=20, \n        max_features=number_features,\n        smooth_idf=False\n    )\n    clean_vec2 = vectorizer2.fit_transform(clean_sentences)\n    print(clean_vec2[0])\n    ```", "```py\n    feature_names_vec2 = vectorizer2.get_feature_names()\n    feature_names_vec2\n    ```", "```py\n    ['abbas',\n     'ability',\n     'accelerate',\n     'accept',\n     'access',\n     'accord',\n     'account',\n     'accused',\n     'achieve',\n     'acknowledge',\n     'acquire',\n     'acquisition',\n     'across',\n     'action',\n     'activist',\n     'activity',\n     'actually',\n    ```", "```py\n    nmf = sklearn.decomposition.NMF(\n        n_components=4,\n        init=\"nndsvda\",\n        solver=\"mu\",\n        beta_loss=\"frobenius\",\n        random_state=0, \n        alpha=0.1, \n        l1_ratio=0.5\n    )\n    nmf.fit(clean_vec2)\n    ```", "```py\n    W_df, H_df = get_topics(\n        mod=nmf,\n        vec=clean_vec2,\n        names=feature_names_vec2,\n        docs=raw,\n        ndocs=number_docs, \n        nwords=number_words\n    )\n    ```", "```py\n    print(W_df)\n    ```", "```py\n    print(H_df)\n    ```", "```py\n    nmf_transform = nmf.transform(clean_vec2)\n    print(nmf_transform.shape)\n    print(nmf_transform)\n    ```", "```py\n    plot_tsne(data=nmf_transform, threshold=0)\n    ```"]
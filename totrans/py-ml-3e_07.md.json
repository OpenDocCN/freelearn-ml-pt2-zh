["```py\n>>> from scipy.special import comb\n>>> import math\n>>> def ensemble_error(n_classifier, error):\n...     k_start = int(math.ceil(n_classifier / 2.))\n...     probs = [comb(n_classifier, k) *\n...              error**k *\n...              (1-error)**(n_classifier - k)\n...              for k in range(k_start, n_classifier + 1)]\n...     return sum(probs)\n>>> ensemble_error(n_classifier=11, error=0.25)\n0.03432750701904297 \n```", "```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> error_range = np.arange(0.0, 1.01, 0.01)\n>>> ens_errors = [ensemble_error(n_classifier=11, error=error)\n...               for error in error_range]\n>>> plt.plot(error_range, ens_errors,\n...          label='Ensemble error',\n...          linewidth=2)\n>>> plt.plot(error_range, error_range,\n...          linestyle='--', label='Base error',\n...          linewidth=2)\n>>> plt.xlabel('Base error')\n>>> plt.ylabel('Base/Ensemble error')\n>>> plt.legend(loc='upper left')\n>>> plt.grid(alpha=0.5)\n>>> plt.show() \n```", "```py\n>>> import numpy as np\n>>> np.argmax(np.bincount([0, 0, 1],\n...           weights=[0.2, 0.2, 0.6]))\n1 \n```", "```py\n>>> ex = np.array([[0.9, 0.1],\n...                [0.8, 0.2],\n...                [0.4, 0.6]])\n>>> p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])\n>>> p\narray([0.58, 0.42])\n>>> np.argmax(p)\n0 \n```", "```py\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.pipeline import _name_estimators\nimport numpy as np\nimport operator\nclass MajorityVoteClassifier(BaseEstimator,\n                             ClassifierMixin):\n    \"\"\" A majority vote ensemble classifier\n\n    Parameters\n    ----------\n    classifiers : array-like, shape = [n_classifiers]\n      Different classifiers for the ensemble\n\n    vote : str, {'classlabel', 'probability'}\n      Default: 'classlabel'\n      If 'classlabel' the prediction is based on\n      the argmax of class labels. Else if\n      'probability', the argmax of the sum of\n      probabilities is used to predict the class label\n      (recommended for calibrated classifiers).\n\n    weights : array-like, shape = [n_classifiers]\n      Optional, default: None\n      If a list of `int` or `float` values are\n      provided, the classifiers are weighted by\n      importance; Uses uniform weights if `weights=None`.\n\n    \"\"\"\n    def __init__(self, classifiers,\n                 vote='classlabel', weights=None):\n\n        self.classifiers = classifiers\n        self.named_classifiers = {key: value for\n                                  key, value in\n                                  _name_estimators(classifiers)}\n        self.vote = vote\n        self.weights = weights\n\n    def fit(self, X, y):\n        \"\"\" Fit classifiers.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix},\n            shape = [n_examples, n_features]\n            Matrix of training examples.\n\n        y : array-like, shape = [n_examples]\n            Vector of target class labels.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        if self.vote not in ('probability', 'classlabel'):\n            raise ValueError(\"vote must be 'probability'\"\n                             \"or 'classlabel'; got (vote=%r)\"\n                             % self.vote)\n        if self.weights and\n        len(self.weights) != len(self.classifiers):\n            raise ValueError(\"Number of classifiers and weights\"\n                             \"must be equal; got %d weights,\"\n                             \"%d classifiers\"\n                             % (len(self.weights),\n                             len(self.classifiers)))\n        # Use LabelEncoder to ensure class labels start\n        # with 0, which is important for np.argmax\n        # call in self.predict\n        self.lablenc_ = LabelEncoder()\n        self.lablenc_.fit(y)\n        self.classes_ = self.lablenc_.classes_\n        self.classifiers_ = []\n        for clf in self.classifiers:\n            fitted_clf = clone(clf).fit(X,\n                               self.lablenc_.transform(y))\n            self.classifiers_.append(fitted_clf)\n        return self \n```", "```py\n def predict(self, X):\n        \"\"\" Predict class labels for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix},\n            Shape = [n_examples, n_features]\n            Matrix of training examples.\n\n        Returns\n        ----------\n        maj_vote : array-like, shape = [n_examples]\n            Predicted class labels.\n\n        \"\"\"\n        if self.vote == 'probability':\n            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n        else: # 'classlabel' vote\n\n            # Collect results from clf.predict calls\n            predictions = np.asarray([clf.predict(X)\n                                      for clf in\n                                      self.classifiers_]).T\n\n            maj_vote = np.apply_along_axis(lambda x: np.argmax(\n                                           np.bincount(x,\n                                           weights=self.weights)),\n                                           axis=1,\n                                           arr=predictions)\n        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n        return maj_vote\n\n    def predict_proba(self, X):\n        \"\"\" Predict class probabilities for X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, \n            shape = [n_examples, n_features]\n            Training vectors, where\n            n_examples is the number of examples and\n            n_features is the number of features.\n\n        Returns\n        ----------\n        avg_proba : array-like,\n            shape = [n_examples, n_classes]\n            Weighted average probability for\n            each class per example.\n\n        \"\"\"\n        probas = np.asarray([clf.predict_proba(X)\n                             for clf in self.classifiers_])\n        avg_proba = np.average(probas, axis=0,\n                               weights=self.weights)\n        return avg_proba\n\n    def get_params(self, deep=True):\n        \"\"\" Get classifier parameter names for GridSearch\"\"\"\n        if not deep:\n            return super(MajorityVoteClassifier, \n                           self).get_params(deep=False)\n        else:\n            out = self.named_classifiers.copy()\n            for name, step in self.named_classifiers.items():\n                for key, value in step.get_params(\n                        deep=True).items():\n                    out['%s__%s' % (name, key)] = value\n            return out \n```", "```py\n>>> from sklearn import datasets\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.preprocessing import LabelEncoder\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data[50:, [1, 2]], iris.target[50:]\n>>> le = LabelEncoder()\n>>> y = le.fit_transform(y) \n```", "```py\n>>> X_train, X_test, y_train, y_test =\\\n...     train_test_split(X, y,\n...                      test_size=0.5,\n...                      random_state=1,\n...                      stratify=y) \n```", "```py\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.pipeline import Pipeline\n>>> import numpy as np\n>>> clf1 = LogisticRegression(penalty='l2',\n...                           C=0.001,\n...                           solver='lbfgs',\n...                           random_state=1)\n>>> clf2 = DecisionTreeClassifier(max_depth=1,\n...                               criterion='entropy',\n...                               random_state=0)\n>>> clf3 = KNeighborsClassifier(n_neighbors=1,\n...                             p=2,\n...                             metric='minkowski')\n>>> pipe1 = Pipeline([['sc', StandardScaler()],\n...                   ['clf', clf1]])\n>>> pipe3 = Pipeline([['sc', StandardScaler()],\n...                   ['clf', clf3]])\n>>> clf_labels = ['Logistic regression', 'Decision tree', 'KNN']\n>>> print('10-fold cross validation:\\n')\n>>> for clf, label in zip([pipe1, clf2, pipe3], clf_labels):\n...     scores = cross_val_score(estimator=clf,\n...                              X=X_train,\n...                              y=y_train,\n...                              cv=10,\n...                              scoring='roc_auc')\n...     print(\"ROC AUC: %0.2f (+/- %0.2f) [%s]\"\n...           % (scores.mean(), scores.std(), label)) \n```", "```py\n10-fold cross validation:\nROC AUC: 0.92 (+/- 0.15) [Logistic regression]\nROC AUC: 0.87 (+/- 0.18) [Decision tree]\nROC AUC: 0.85 (+/- 0.13) [KNN] \n```", "```py\n>>> mv_clf = MajorityVoteClassifier(\n...                  classifiers=[pipe1, clf2, pipe3])\n>>> clf_labels += ['Majority voting']\n>>> all_clf = [pipe1, clf2, pipe3, mv_clf]\n>>> for clf, label in zip(all_clf, clf_labels):\n...     scores = cross_val_score(estimator=clf,\n...                              X=X_train,\n...                              y=y_train,\n...                              cv=10,\n...                              scoring='roc_auc')\n...     print(\"ROC AUC: %0.2f (+/- %0.2f) [%s]\"\n...           % (scores.mean(), scores.std(), label))\nROC AUC: 0.92 (+/- 0.15) [Logistic regression]\nROC AUC: 0.87 (+/- 0.18) [Decision tree]\nROC AUC: 0.85 (+/- 0.13) [KNN]\nROC AUC: 0.98 (+/- 0.05) [Majority voting] \n```", "```py\n>>> from sklearn.metrics import roc_curve\n>>> from sklearn.metrics import auc\n>>> colors = ['black', 'orange', 'blue', 'green']\n>>> linestyles = [':', '--', '-.', '-']\n>>> for clf, label, clr, ls \\\n...     in zip(all_clf, clf_labels, colors, linestyles):\n...     # assuming the label of the positive class is 1\n...     y_pred = clf.fit(X_train,\n...                      y_train).predict_proba(X_test)[:, 1]\n...     fpr, tpr, thresholds = roc_curve(y_true=y_test,\n...                                      y_score=y_pred)\n...     roc_auc = auc(x=fpr, y=tpr)\n...     plt.plot(fpr, tpr,\n...              color=clr,\n...              linestyle=ls,\n...              label='%s (auc = %0.2f)' % (label, roc_auc))\n>>> plt.legend(loc='lower right')\n>>> plt.plot([0, 1], [0, 1],\n...          linestyle='--',\n...          color='gray',\n...          linewidth=2)\n>>> plt.xlim([-0.1, 1.1])\n>>> plt.ylim([-0.1, 1.1])\n>>> plt.grid(alpha=0.5)\n>>> plt.xlabel('False positive rate (FPR)')\n>>> plt.ylabel('True positive rate (TPR)')\n>>> plt.show() \n```", "```py\n>>> sc = StandardScaler()\n>>> X_train_std = sc.fit_transform(X_train)\n>>> from itertools import product\n>>> x_min = X_train_std[:, 0].min() - 1\n>>> x_max = X_train_std[:, 0].max() + 1\n>>> y_min = X_train_std[:, 1].min() - 1\n>>>\n>>> y_max = X_train_std[:, 1].max() + 1\n>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n...                      np.arange(y_min, y_max, 0.1))\n>>> f, axarr = plt.subplots(nrows=2, ncols=2,\n...                         sharex='col',\n...                         sharey='row',\n...                         figsize=(7, 5))\n>>> for idx, clf, tt in zip(product([0, 1], [0, 1]),\n...                         all_clf, clf_labels):\n...     clf.fit(X_train_std, y_train)\n...     Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n...     Z = Z.reshape(xx.shape)\n...     axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)\n...     axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0],\n...                                   X_train_std[y_train==0, 1],\n...                                   c='blue',\n...                                   marker='^',\n...                                   s=50)\n...     axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0],\n...                                   X_train_std[y_train==1, 1],\n...                                   c='green',\n...                                   marker='o',\n...                                   s=50)\n...     axarr[idx[0], idx[1]].set_title(tt)\n>>> plt.text(-3.5, -5.,\n...          s='Sepal width [standardized]',\t\n...          ha='center', va='center', fontsize=12)\n>>> plt.text(-12.5, 4.5,\n...          s='Petal length [standardized]',\n...          ha='center', va='center',\n...          fontsize=12, rotation=90)\n>>> plt.show() \n```", "```py\n>>> mv_clf.get_params()\n{'decisiontreeclassifier':\n DecisionTreeClassifier(class_weight=None, criterion='entropy',\n                        max_depth=1, max_features=None,\n                        max_leaf_nodes=None, min_samples_leaf=1,\n                        min_samples_split=2,\n                        min_weight_fraction_leaf=0.0,\n                        random_state=0, splitter='best'),\n 'decisiontreeclassifier__class_weight': None,\n 'decisiontreeclassifier__criterion': 'entropy',\n [...]\n 'decisiontreeclassifier__random_state': 0,\n 'decisiontreeclassifier__splitter': 'best',\n 'pipeline-1':\n Pipeline(steps=[('sc', StandardScaler(copy=True, with_mean=True,\n                                       with_std=True)),\n                 ('clf', LogisticRegression(C=0.001,\n                                            class_weight=None,\n                                            dual=False,\n                                            fit_intercept=True,\n                                            intercept_scaling=1,\n                                            max_iter=100,\n                                            multi_class='ovr',\n                                            penalty='l2',\n                                            random_state=0,\n                                            solver='liblinear',\n                                            tol=0.0001,\n                                            verbose=0))]),\n 'pipeline-1__clf':\n LogisticRegression(C=0.001, class_weight=None, dual=False,\n                    fit_intercept=True, intercept_scaling=1,\n                    max_iter=100, multi_class='ovr',\n                    penalty='l2', random_state=0,\n                    solver='liblinear', tol=0.0001, verbose=0),\n 'pipeline-1__clf__C': 0.001,\n 'pipeline-1__clf__class_weight': None,\n 'pipeline-1__clf__dual': False,\n [...]\n 'pipeline-1__sc__with_std': True,\n 'pipeline-2':\n Pipeline(steps=[('sc', StandardScaler(copy=True, with_mean=True,\n                                       with_std=True)),\n                 ('clf', KNeighborsClassifier(algorithm='auto',\n                                            leaf_size=30,\n                                            metric='minkowski',\n                                            metric_params=None,\n                                            n_neighbors=1,\n                                            p=2,\n                                            weights='uniform'))]),\n 'pipeline-2__clf':\n KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                      metric='minkowski', metric_params=None,\n                      n_neighbors=1, p=2, weights='uniform'),\n 'pipeline-2__clf__algorithm': 'auto',\n [...]\n 'pipeline-2__sc__with_std': True} \n```", "```py\n>>> from sklearn.model_selection import GridSearchCV\n>>> params = {'decisiontreeclassifier__max_depth': [1, 2],\n...           'pipeline-1__clf__C': [0.001, 0.1, 100.0]}\n>>> grid = GridSearchCV(estimator=mv_clf,\n...                     param_grid=params,\n...                     cv=10,\n...                     iid=False,\n...                     scoring='roc_auc')\n>>> grid.fit(X_train, y_train) \n```", "```py\n>>> for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n...     print(\"%0.3f +/- %0.2f %r\"\n...           % (grid.cv_results_['mean_test_score'][r],\n...              grid.cv_results_['std_test_score'][r] / 2.0,\n...              grid.cv_results_['params'][r]))\n0.944 +/- 0.07 {'decisiontreeclassifier__max_depth': 1,\n                'pipeline-1__clf__C': 0.001}\n0.956 +/- 0.07 {'decisiontreeclassifier__max_depth': 1,\n                'pipeline-1__clf__C': 0.1}\n0.978 +/- 0.03 {'decisiontreeclassifier__max_depth': 1,\n                'pipeline-1__clf__C': 100.0}\n0.956 +/- 0.07 {'decisiontreeclassifier__max_depth': 2,\n                'pipeline-1__clf__C': 0.001}\n0.956 +/- 0.07 {'decisiontreeclassifier__max_depth': 2,\n                'pipeline-1__clf__C': 0.1}\n0.978 +/- 0.03 {'decisiontreeclassifier__max_depth': 2,\n                'pipeline-1__clf__C': 100.0}\n>>> print('Best parameters: %s' % grid.best_params_)\nBest parameters: {'decisiontreeclassifier__max_depth': 1,\n                  'pipeline-1__clf__C': 0.001}\n>>> print('Accuracy: %.2f' % grid.best_score_)\nAccuracy: 0.98 \n```", "```py\n>>> import pandas as pd\n>>> df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n...               'machine-learning-databases/wine/wine.data',\n...               header=None)\n>>> df_wine.columns = ['Class label', 'Alcohol',\n...                    'Malic acid', 'Ash',\n...                    'Alcalinity of ash',\n...                    'Magnesium', 'Total phenols',\n...                    'Flavanoids', 'Nonflavanoid phenols',\n...                    'Proanthocyanins',\n...                    'Color intensity', 'Hue',\n...                    'OD280/OD315 of diluted wines',\n...                    'Proline']\n>>> # drop 1 class\n>>> df_wine = df_wine[df_wine['Class label'] != 1]\n>>> y = df_wine['Class label'].values\n>>> X = df_wine[['Alcohol',\n...              'OD280/OD315 of diluted wines']].values \n```", "```py\n>>> from sklearn.preprocessing import LabelEncoder\n>>> from sklearn.model_selection import train_test_split\n>>> le = LabelEncoder()\n>>> y = le.fit_transform(y)\n>>> X_train, X_test, y_train, y_test =\\\n...            train_test_split(X, y,\n...                             test_size=0.2,\n...                             random_state=1,\n...                             stratify=y) \n```", "```py\ndf = pd.read_csv(\n         'https://archive.ics.uci.edu/ml/'\n         'machine-learning-databases'\n         '/wine/wine.data', header=None) \n```", "```py\ndf = pd.read_csv(\n         'your/local/path/to/wine.data',\n         header=None) \n```", "```py\n>>> from sklearn.ensemble import BaggingClassifier\n>>> tree = DecisionTreeClassifier(criterion='entropy',\n...                               random_state=1,\n...                               max_depth=None)\n>>> bag = BaggingClassifier(base_estimator=tree,\n...                         n_estimators=500,\n...                         max_samples=1.0,\n...                         max_features=1.0,\n...                         bootstrap=True,\n...                         bootstrap_features=False,\n...                         n_jobs=1,\n...                         random_state=1) \n```", "```py\n>>> from sklearn.metrics import accuracy_score\n>>> tree = tree.fit(X_train, y_train)\n>>> y_train_pred = tree.predict(X_train)\n>>> y_test_pred = tree.predict(X_test)\n>>> tree_train = accuracy_score(y_train, y_train_pred)\n>>> tree_test = accuracy_score(y_test, y_test_pred)\n>>> print('Decision tree train/test accuracies %.3f/%.3f'\n...       % (tree_train, tree_test))\nDecision tree train/test accuracies 1.000/0.833 \n```", "```py\n>>> bag = bag.fit(X_train, y_train)\n>>> y_train_pred = bag.predict(X_train)\n>>> y_test_pred = bag.predict(X_test)\n>>> bag_train = accuracy_score(y_train, y_train_pred)\n>>> bag_test = accuracy_score(y_test, y_test_pred)\n>>> print('Bagging train/test accuracies %.3f/%.3f'\n...       % (bag_train, bag_test))\nBagging train/test accuracies 1.000/0.917 \n```", "```py\n>>> x_min = X_train[:, 0].min() - 1\n>>> x_max = X_train[:, 0].max() + 1\n>>> y_min = X_train[:, 1].min() - 1\n>>> y_max = X_train[:, 1].max() + 1\n>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n...                      np.arange(y_min, y_max, 0.1))\n>>> f, axarr = plt.subplots(nrows=1, ncols=2,\n...                         sharex='col',\n...                         sharey='row',\n...                         figsize=(8, 3))\n>>> for idx, clf, tt in zip([0, 1],\n...                         [tree, bag],\n...                         ['Decision tree', 'Bagging']):\n...     clf.fit(X_train, y_train)\n...\n...     Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n...     Z = Z.reshape(xx.shape)\n...     axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n...     axarr[idx].scatter(X_train[y_train==0, 0],\n...                        X_train[y_train==0, 1],\n...                        c='blue', marker='^')\n...     axarr[idx].scatter(X_train[y_train==1, 0],\n...                        X_train[y_train==1, 1],\n...                        c='green', marker='o')\n...     axarr[idx].set_title(tt)\n>>> axarr[0].set_ylabel('Alcohol', fontsize=12)\n>>> plt.tight_layout()\n>>> plt.text(0, -0.2,\n...          s='OD280/OD315 of diluted wines',\n...          ha='center',\n...          va='center',\n...          fontsize=12,\n...          transform=axarr[1].transAxes)\n>>> plt.show() \n```", "```py\n>>> from sklearn.ensemble import AdaBoostClassifier\n>>> tree = DecisionTreeClassifier(criterion='entropy',\n...                               random_state=1,\n...                               max_depth=1)\n>>> ada = AdaBoostClassifier(base_estimator=tree,\n...                          n_estimators=500,\n...                          learning_rate=0.1,\n...                          random_state=1)\n>>> tree = tree.fit(X_train, y_train)\n>>> y_train_pred = tree.predict(X_train)\n>>> y_test_pred = tree.predict(X_test)\n>>> tree_train = accuracy_score(y_train, y_train_pred)\n>>> tree_test = accuracy_score(y_test, y_test_pred)\n>>> print('Decision tree train/test accuracies %.3f/%.3f'\n...       % (tree_train, tree_test))\nDecision tree train/test accuracies 0.916/0.875 \n```", "```py\n>>> ada = ada.fit(X_train, y_train)\n>>> y_train_pred = ada.predict(X_train)\n>>> y_test_pred = ada.predict(X_test)\n>>> ada_train = accuracy_score(y_train, y_train_pred)\n>>> ada_test = accuracy_score(y_test, y_test_pred)\n>>> print('AdaBoost train/test accuracies %.3f/%.3f'\n...       % (ada_train, ada_test))\nAdaBoost train/test accuracies 1.000/0.917 \n```", "```py\n>>> x_min = X_train[:, 0].min() - 1\n>>> x_max = X_train[:, 0].max() + 1\n>>> y_min = X_train[:, 1].min() - 1\n>>> y_max = X_train[:, 1].max() + 1\n>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n...                      np.arange(y_min, y_max, 0.1))\n>>> f, axarr = plt.subplots(1, 2,\n...                         sharex='col',\n...                         sharey='row',\n...                         figsize=(8, 3))\n>>> for idx, clf, tt in zip([0, 1],\n...                         [tree, ada],\n...                         ['Decision Tree', 'AdaBoost']):\n...     clf.fit(X_train, y_train)\n...     Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n...     Z = Z.reshape(xx.shape)\n...     axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n...     axarr[idx].scatter(X_train[y_train==0, 0],\n...                        X_train[y_train==0, 1],\n...                        c='blue',\n...                        marker='^')\n...     axarr[idx].scatter(X_train[y_train==1, 0],\n...                        X_train[y_train==1, 1],\n...                        c='green',\n...                        marker='o')\n...     axarr[idx].set_title(tt)\n...     axarr[0].set_ylabel('Alcohol', fontsize=12)\n>>> plt.tight_layout()\n>>> plt.text(0, -0.2,\n...          s='OD280/OD315 of diluted wines',\n...          ha='center',\n...          va='center',\n...          fontsize=12,\n...          transform=axarr[1].transAxes)\n>>> plt.show() \n```"]
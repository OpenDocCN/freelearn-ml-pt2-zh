- en: 3\. Neighborhood Approaches and DBSCAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see how neighborhood approaches to clustering work
    from start to end and implement the **Density-Based Spatial Clustering of Applications
    with Noise** (**DBSCAN**) algorithm from scratch by using packages. We will also
    identify the most suitable algorithm to solve your problem from k-means, hierarchical
    clustering, and DBSCAN. By the end of this chapter, we will see how the DBSCAN
    clustering approach will serve us best in the sphere of highly complex data.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we evaluated a number of different approaches to data
    clustering, including k-means and hierarchical clustering. While k-means is the
    simplest form of clustering, it is still extremely powerful in the right scenarios.
    In situations where k-means can't capture the complexity of the dataset, hierarchical
    clustering proves to be a strong alternative.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key challenges in unsupervised learning is that you will be presented
    with a collection of feature data but no complementary labels telling you what
    a target state will be. While you may not get a discrete view of what the target
    labels are, you can get some semblance of structure out of the data by clustering
    similar groups together and seeing what is similar within groups. The first approach
    we covered to achieve this goal of clustering similar data points is k-means.
    K-means clustering works best for simple data challenges where speed is paramount.
    Simply looking at the closest data point (cluster centroid) does not require a
    lot of computational overhead; however, there is also a greater challenge posed
    when it comes to higher-dimensional datasets. K-means clustering is also not ideal
    if you are unaware of the potential number of clusters you are looking for. An
    example we worked with in *Chapter 2*, *Hierarchical Clustering*, entailed looking
    at chemical profiles to determine which wines belonged together in a disorganized
    shipment. This exercise only worked well because we knew that three wine types
    were ordered; however, k-means would have been less successful if you had no idea
    regarding what the original order constituted.
  prefs: []
  type: TYPE_NORMAL
- en: The second clustering approach we explored was hierarchical clustering. This
    method can work in two ways – either agglomerative or divisive. Agglomerative
    clustering works with a bottom-up approach, treating each data point as its own
    cluster and recursively grouping them together with linkage criteria. Divisive
    clustering works in the opposite way by treating all data points as one large
    class and recursively breaking them down into smaller clusters. This approach
    has the benefit of fully understanding the entire data distribution, as it calculates
    splitting potential; however, it is typically not implemented in practice due
    to its greater complexity. Hierarchical clustering is a strong contender for your
    clustering needs when it comes to not knowing anything about the data. Using a
    dendrogram, you can visualize all the splits in your data and consider what number
    of clusters makes sense after the fact. This can be really helpful in your specific
    use case; however, it also comes at a higher computational cost than is associated
    with k-means.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover a clustering approach that will serve us best
    in the sphere of highly complex data: **Density-Based Spatial Clustering of Applications
    with Noise** (**DBSCAN**). Canonically, this method has always been seen as a
    high performer in datasets that have a lot of densely interspersed data. Let''s
    walk through why it does so well in these use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Clusters as Neighborhoods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until now, we have explored the concept of likeness being described as a function
    of Euclidean distance – data points that are closer to any one point can be seen
    as similar, while those that are further away in Euclidean space can be seen as
    dissimilar. This notion is seen once again in the DBSCAN algorithm. As alluded
    to by the lengthy name, the DBSCAN approach expands upon basic distance metric
    evaluation by also incorporating the notion of density. If there are clumps of
    data points that all exist in the same area as one another, they can be seen as
    members of the same cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Neighbors have a direct connection to clusters](img/B15923_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Neighbors have a direct connection to clusters'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we can see four neighborhoods. The density-based approach
    has a number of benefits when compared to the past approaches we've covered that
    focus exclusively on distance. If you were just focusing on distance as a clustering
    threshold, then you may find your clustering makes little sense if faced with
    a sparse feature space with outliers. Both k-means and hierarchical clustering
    will automatically group together all data points in the space until no points
    are left.
  prefs: []
  type: TYPE_NORMAL
- en: 'While hierarchical clustering does provide a path around this issue somewhat,
    since you can dictate where clusters are formed using a dendrogram post-clustering
    run, k-means is the most susceptible to failure as it is the simplest approach
    to clustering. These pitfalls are less evident when we begin evaluating neighborhood
    approaches to clustering. In the following dendrogram, you can see an example
    of the pitfall where all data points are grouped together. Clearly, as you travel
    down the dendrogram, there is a lot of potential variation that gets grouped together
    since every point needs to be a member of a cluster. This is less of an issue
    with neighborhood-based clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Example dendrogram](img/B15923_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Example dendrogram'
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating the notion of neighbor density in DBSCAN, we can leave outliers
    out of clusters if we choose to, based on the hyperparameters we choose at runtime.
    Only the data points that have close neighbors will be seen as members within
    the same cluster, and those that are farther away can be left as unclustered outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to DBSCAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In DBSCAN, density is evaluated as a combination of neighborhood radius and
    minimum points found in a neighborhood deemed a cluster. This concept can be driven
    home if we reconsider the scenario where you are tasked with organizing an unlabeled
    shipment of wine for your store. In the previous example, it was made clear that
    we can find similar wines based on their features, such as chemical traits. Knowing
    this information, we can more easily group together similar wines and efficiently
    have our products organized for sale in no time. In the real world, however, the
    products that you order to stock your store will reflect real-world purchase patterns.
    To promote variety in your inventory, but still have sufficient stock of the most
    popular wines, there is a highly uneven distribution of product types that you
    have available. Most people love the classic wines, such as white and red; however,
    you may still carry more exotic wines for your customers who love expensive varieties.
    This makes clustering more difficult, since there are uneven class distributions
    (you don't order 10 bottles of every wine available, for example).
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN differs from k-means and hierarchical clustering because you can build
    this intuition into how we evaluate the clusters of customers we are interested
    in forming. It can cut through the noise in an easier fashion and only point out
    customers who have the highest potential for remarketing in a campaign.
  prefs: []
  type: TYPE_NORMAL
- en: By clustering through the concept of a neighborhood, we can separate out the
    one-off customers who can be seen as random noise, relative to the more valuable
    customers who come back to our store time and time again. This approach calls
    into question how we establish the best numbers when it comes to neighborhood
    radius and minimum points per neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: As a high-level heuristic, we want our neighborhood radius to be small, but
    not too small. At one end of the extreme, you can have the neighborhood radius
    quite high – this can max out at treating all points in the feature space as one
    massive cluster. At the opposite end of the extreme, you can have a very small
    neighborhood radius. Overly small neighborhood radii can result in no points being
    clustered together and having a large collection of single-member clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Similar logic applies when it comes to the minimum number of points that can
    make up a cluster. Minimum points can be seen as a secondary threshold that tunes
    the neighborhood radius a bit, depending on what data you have available in your
    space. If all of the data in your feature space is extremely sparse, minimum points
    become extremely valuable, in tandem with the neighborhood radius, to make sure
    you don't just have a large number of uncorrelated data points. When you have
    very dense data, the minimum points threshold becomes less of a driving factor
    than neighborhood radius.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from these two hyperparameter rules, the best options are, as
    usual, dependent on what your dataset looks like. Oftentimes, you will want to
    find the perfect "goldilocks" zone of not being too small in your hyperparameters,
    but also not too large.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN in Detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To see how DBSCAN works, we can trace the path of a simple toy program as it
    merges together to form a variety of clusters and noise-labeled data points:'
  prefs: []
  type: TYPE_NORMAL
- en: Out of *n* unvisited sample data points, we'll first move through each point
    in a loop and mark each one as visited.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From each point, we'll look at the distance to every other point in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All points that fall within the neighborhood radius hyperparameter should be
    considered as neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of neighbors should be at least as many as the minimum points required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the minimum point threshold is reached, the points should be grouped together
    as a cluster, or else marked as noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process should be repeated until all data points are categorized in clusters
    or as noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DBSCAN is fairly straightforward in some senses – while there are the new concepts
    of density through neighborhood radius and minimum points, at its core, it is
    still just evaluating using a distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of the DBSCAN Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will walk you through this path in slightly more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given six sample data points, view each point as its own cluster [ (1,3) ],
    [ (-8,6) ], [ (-6,4) ] , [ (4,-2) ], ] (2,5) ], [ (-2,0) ]:![Figure 3.3: Plot
    of sample data points](img/B15923_03_03.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.3: Plot of sample data points'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the pairwise Euclidean distance between each of the points:![Figure
    3.4: Point distances](img/B15923_03_04.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.4: Point distances'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From each point, expand a neighborhood size outward and form clusters. For the
    purpose of this example, imagine you pass through a neighborhood radius of five.
    This means that any two points will be neighbors if the distance between them
    is less than five units. For example, point (1,3) has points (2,5) and (-2,0)
    as neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Depending on the number of points in the neighborhood of a given point, the
    point can be classified into the following three categories:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Core Point**: If the point under observation has data points greater than
    the minimum number of points in its neighborhood that make up a cluster, then
    that point is called a core point of the cluster. All core points within the neighborhood
    of other core points are part of the same cluster. However, all the core points
    that are not in same neighborhood are part of another cluster.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Boundary Point**: If the point under observation does not have sufficient
    neighbors (data points) of its own, but it has at least one core point (in its
    neighborhood), then that point represents the boundary point of the cluster. Boundary
    points belong to the same cluster of their nearest core point.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Noise Point**: A data point is treated as a noise point if it does not have
    the required minimum number of data points in its neighborhood and is not associated
    with a core point. This point is treated as pure noise and is excluded from clustering.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Points that have neighbors are then evaluated to see whether they pass the minimum
    points threshold. In this example, if we had passed through a minimum points threshold
    of two, then points (1,3), (2,5), and (-2,0) could formally be grouped together
    as a cluster. If we had a minimum points threshold of four, then these three data
    points would be considered superfluous noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Points that have fewer neighbors than the minimum number of neighboring points
    required and whose neighborhood does not contain a core point are marked as noise
    and remain unclustered. Thus, points (-6,4), (4,-2), and (-8,6) fall under this
    category. However, points such as (2,5) and (2,0), though don't satisfy the criteria
    of the minimum number of points in neighborhood, do contain a core point as their
    neighbor, and are therefore marked as boundary points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following table summarizes the neighbors of a particular point and classifies
    them as core, boundary, and noise data points (mentioned in the preceding step)
    for a neighborhood radius of 5 and a minimum-neighbor criterion of 2.![Figure
    3.5: Table showing details of neighbors for given points](img/B15923_03_05.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.5: Table showing details of neighbors for given points'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Repeat this process on any remaining unvisited data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of this process, you will have sorted your entire dataset into either
    clusters or unrelated noise. DBSCAN performance is highly dependent on the threshold
    hyperparameters you choose. This means that you may have to run DBSCAN a couple
    of times with different hyperparameter options to get an understanding of how
    they influence overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note that DBSCAN does not require the centroids that we saw in both k-means
    and centroid-focused implementation of hierarchical clustering. This feature allows
    DBSCAN to work better for complex datasets, since most data is not shaped like
    clean blobs. DBSCAN is also more effective against outliers and noise than k-means
    or hierarchical clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how the performance of DBSCAN changes with varying neighborhood
    radius sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.01: Evaluating the Impact of Neighborhood Radius Size'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this exercise, we will work in reverse of what we have typically seen in
    previous examples by first seeing the packaged implementation of DBSCAN in scikit-learn,
    and then implementing it on our own. This is done on purpose to fully explore
    how different neighborhood radius sizes drastically impact DBSCAN performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'By completing this exercise, you will become familiar with how tuning neighborhood
    radius size can change how well DBSCAN performs. It is important to understand
    these facets of DBSCAN, as they can save you time in the future by troubleshooting
    your clustering algorithms efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the packages from scikit-learn and matplotlib that are necessary for
    this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a random cluster dataset to experiment on; X = coordinate points,
    and y = cluster labels (not needed):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6: Visualized toy data example](img/B15923_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.6: Visualized toy data example'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After plotting the dummy data for this toy problem, you will see that the dataset
    has two features and approximately seven to eight clusters. To implement DBSCAN
    using scikit-learn, you will need to instantiate a new scikit-learn class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our example DBSCAN instance is stored in the `db` variable, and our hyperparameters
    are passed through on creation. For the sake of this example, you can see that
    the neighborhood radius (`eps`) is set to `0.5`, while the minimum number of points
    is set to `10`. To keep in line with our past chapters, we will once again be
    using Euclidean distance as our distance metric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`eps` stands for epsilon and is the radius of the neighborhood that your algorithm
    will look within when searching for neighbors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s set up a loop that allows us to explore potential neighborhood radius
    size options interactively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following plots:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.7: Resulting plots](img/B15923_03_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.7: Resulting plots'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the plots, setting our neighborhood size too small will
    cause everything to be seen as random noise (purple points). Bumping our neighborhood
    size up a little bit allows us to form clusters that make more sense. A larger
    epsilon value would again convert the entire dataset into a single cluster (purple
    data points). Try recreating the preceding plots and experiment with varying `eps`
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3gEijGC](https://packt.live/3gEijGC).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2ZPBfeJ](https://packt.live/2ZPBfeJ).
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Attributes – Neighborhood Radius
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding exercise, you saw how impactful setting the proper neighborhood
    radius is on the performance of your DBSCAN implementation. If your neighborhood
    is too small, then you will run into issues where all the data will be treated
    as noise and is left unclustered. If you set your neighborhood too large, then
    all of the data will similarly be grouped together into one cluster and not provide
    any value. If you explored the preceding exercise further with your own `eps`
    sizes, you may have noticed that it is very difficult to perform effective clustering
    using only the neighborhood size. This is where a minimum points threshold comes
    in handy. We will visit that topic later.
  prefs: []
  type: TYPE_NORMAL
- en: 'To go deeper into the neighborhood concept of DBSCAN, let''s take a deeper
    look at the `eps` hyperparameter you pass at instantiation time. This epsilon
    value is converted to a radius that sweeps around any given data point in a circular
    manner to serve as a neighborhood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Visualization of the neighborhood radius; the red circle is the
    neighborhood](img/B15923_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Visualization of the neighborhood radius; the red circle is the
    neighborhood'
  prefs: []
  type: TYPE_NORMAL
- en: In this instance, there will be four neighbors of the center point, as can be
    seen in the preceding plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'One key aspect to observe here is that the shape formed by your neighborhood
    search is a circle in two dimensions, and a sphere in three dimensions. This may
    impact the performance of your model simply based on how the data is structured.
    Once again, blobs may seem like an intuitive structure to find – this may not
    always be the case. Fortunately, DBSCAN is well equipped to handle this dilemma
    of clusters that you may be interested in, but that do not fit the explicit blob
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Impact of varying neighborhood radius size](img/B15923_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Impact of varying neighborhood radius size'
  prefs: []
  type: TYPE_NORMAL
- en: On the left, the data point will be classified as random noise. On the right,
    the data point has multiple neighbors and could be its own cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.01: Implementing DBSCAN from Scratch'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During an interview, you are asked to create the DBSCAN algorithm from scratch
    using a generated two-dimensional dataset. To do this, you will need to convert
    the theory behind neighborhood searching into production code, with a recursive
    call that adds neighbors. As explained in the previous section, you will use a
    distance scan in space surrounding a specified point to add these neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Given what you've learned about DBSCAN and distance metrics from prior chapters,
    build an implementation of DBSCAN from scratch in Python. You are free to use
    NumPy and SciPy to evaluate distances here.
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a random cluster dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create functions from scratch that allow you to call DBSCAN on a dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your created DBSCAN implementation to find clusters in the generated dataset.
    Feel free to use hyperparameters as you see fit, tuning them based on their performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the clustering performance of your DBSCAN implementation from scratch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The desired outcome of this exercise is for you to implement how DBSCAN works
    from the ground up before you use the fully packaged implementation in scikit-learn.
    Taking this approach to any machine learning algorithm from scratch is important,
    as it helps you "earn" the ability to use easier implementations, while still
    being able to discuss DBSCAN in depth in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10: Expected outcome](img/B15923_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Expected outcome'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 428.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Attributes – Minimum Points
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other core component to a successful implementation of DBSCAN beyond the
    neighborhood radius is the minimum number of points required to justify membership
    within a cluster. As mentioned earlier, it is more obvious that this lower bound
    benefits your algorithm when it comes to sparser datasets. That's not to say that
    it is a useless parameter when you have very dense data; however, while having
    single data points randomly interspersed through your feature space can be easily
    bucketed as noise, it becomes more of a gray area when we have random patches
    of two to three, for example. Should these data points be their own cluster, or
    should they also be categorized as noise? Minimum points thresholding helps to
    solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the scikit-learn implementation of DBSCAN, this hyperparameter is seen in
    the `min_samples` field passed on DBSCAN instance creation. This field is very
    valuable in tandem with the neighborhood radius size hyperparameter to fully round
    out your density-based clustering approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11: Minimum points threshold deciding whether a group of data points
    is noise or a cluster](img/B15923_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Minimum points threshold deciding whether a group of data points
    is noise or a cluster'
  prefs: []
  type: TYPE_NORMAL
- en: On the right, if the minimum points threshold is 10 points, it will classify
    data in this neighborhood as noise.
  prefs: []
  type: TYPE_NORMAL
- en: In real-world scenarios, you can see minimum points being highly impactful when
    you have truly large amounts of data. Going back to the wine-clustering example,
    if your store was actually a large wine warehouse, you could have thousands of
    individual wines with only one or two bottles that could easily be viewed as their
    own cluster. This may be helpful depending on your use case; however, it is important
    to keep in mind the subjective magnitudes that come with your data. If you have
    millions of data points, then random noise can easily be seen as hundreds or even
    thousands of random one-off sales. However, if your data is on the scale of hundreds
    or thousands, single data points can be seen as random noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.02: Evaluating the Impact of the Minimum Points Threshold'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to *Exercise 3.01*, *Evaluating the Impact of Neighborhood Radius Size*,
    where we explored the value of setting a proper neighborhood radius size, we will
    repeat the exercise, but instead will change the minimum points threshold on a
    variety of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Using our current implementation of DBSCAN, we can easily tune the minimum points
    threshold. Tune this hyperparameter and see how it performs on generated data.
  prefs: []
  type: TYPE_NORMAL
- en: By tuning the minimum points threshold for DBSCAN, you will understand how it
    can affect the quality of your clustering predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, let''s start with randomly generated data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a random cluster dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12: Plot of the data generated'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15923_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.12: Plot of the data generated'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the same plotted data as before, let''s grab one of the better-performing
    neighborhood radius sizes from *Exercise 3.01*, *Evaluating the Impact of Neighborhood
    Radius Size* – `eps = 0.7`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`eps` is a tunable hyperparameter. Earlier in *Step 3* of the previous exercise,
    we used a value of `0.5`. In this step, we are using `eps = 0.7` based on our
    experimentation with this parameter.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After instantiating the DBSCAN clustering algorithm, let''s treat the `min_samples`
    hyperparameters as the variable we wish to tune. We can cycle through a loop to
    find which minimum number of points works best for our use case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at the first plot generated, we can see where we ended if you followed
    *Exercise 3.01*, *Evaluating the Impact of Neighborhood Radius Size* exactly,
    using 10 minimum points to mark the threshold for cluster membership:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13: Plot of the toy problem with a minimum of 10 points](img/B15923_03_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.13: Plot of the toy problem with a minimum of 10 points'
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining two hyperparameter options can be seen to greatly impact the
    performance of your DBSCAN clustering algorithm, and show how a shift in one number
    can greatly influence performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14: Plots of the toy problem](img/B15923_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: Plots of the toy problem'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, simply changing the number of minimum points from 19 to 20 adds
    an additional (incorrect!) cluster to our feature space. Given what you've learned
    about minimum points through this exercise, you can now tweak both epsilon and
    minimum points thresholding in your scikit-learn implementation to achieve the
    optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In our original generation of the data, we created eight clusters. These indicate
    that small changes in minimum points can add entire new clusters that we know
    shouldn't be there.
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fa4L5F](https://packt.live/3fa4L5F).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/31XUeqi](https://packt.live/31XUeqi).
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.02: Comparing DBSCAN with k-means and Hierarchical Clustering'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding chapter, we attempted to group different wines together using
    hierarchical clustering. Let's attempt this approach again with DBSCAN and see
    whether a neighborhood search fares any better. As a reminder, you are managing
    store inventory and have received a large shipment of wine, but the brand labels
    fell off the bottles during transit. Fortunately, your supplier provided you with
    the chemical readings for each bottle along with their respective serial numbers.
    Unfortunately, you aren't able to open each bottle of wine and taste test the
    difference – you must find a way to group the unlabeled bottles back together
    according to their chemical readings! You know from the order list that you ordered
    three different types of wine and are given only two wine attributes to group
    the wine types back together.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections, we were able to see how k-means and hierarchical clustering
    performed on the wine dataset. In our best-case scenario, we were able to achieve
    a silhouette score of 0.59\. Using scikit-learn's implementation of DBSCAN, let's
    see whether we can get even better clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the wine dataset and check what the data looks like.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate clusters using k-means, agglomerative clustering, and DSBSCAN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate a few different options for DSBSCAN hyperparameters and their effect
    on the silhouette score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the final clusters based on the highest silhouette score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize clusters generated using each of the three methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We have sourced this dataset from [https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine).
    [Citation: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science].
    You can also access it at [https://packt.live/3bW8NME](https://packt.live/3bW8NME).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By completing this activity, you will be recreating a full workflow of a clustering
    problem. You have already made yourself familiar with the data in *Chapter 2*,
    *Hierarchical Clustering*, and, by the end of this activity, you will have performed
    model selection to find the best model and hyperparameters for your dataset. You
    will have silhouette scores of the wine dataset for each type of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 431.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN versus k-means and Hierarchical Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you've reached an understanding of how DBSCAN is implemented and how
    many different hyperparameters you can tweak to drive performance, let's survey
    how it compares to the clustering methods we have covered previously – k-means
    clustering and hierarchical clustering.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed in *Activity 3.02*, *Comparing DBSCAN with k-means and
    Hierarchical Clustering,* that DBSCAN can be a bit finicky when it comes to finding
    the optimal clusters via a silhouette score. This is a downside of the neighborhood
    approach – k-means and hierarchical clustering really excel when you have some
    idea regarding the number of clusters in your data. In most cases, this number
    is low enough that you can iteratively try a few different numbers and see how
    it performs. DBSCAN, instead, takes a more bottom-up approach by working with
    your hyperparameters and finding the clusters it views as important. In practice,
    it is helpful to consider DBSCAN when the first two options fail, simply because
    of the amount of tweaking needed to get it to work properly. That said, when your
    DBSCAN implementation is working correctly, it will often immensely outperform
    k-means and hierarchical clustering (in practice, this often happens with highly
    intertwined, yet still discrete, data, such as a feature space containing two
    half-moons).
  prefs: []
  type: TYPE_NORMAL
- en: Compared to k-means and hierarchical clustering, DBSCAN can be seen as being
    potentially more efficient, since it only has to look at each data point once.
    Instead of multiple iterations of finding new centroids and evaluating where their
    nearest neighbors are, once a point has been assigned to a cluster in DBSCAN,
    it does not change cluster membership. The other key feature that DBSCAN and hierarchical
    clustering both share, in comparison with k-means, is not needing to explicitly
    pass a number of clusters expected at the time of creation. This can be extremely
    helpful when you have no external guidance on how to break your dataset down.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed hierarchical clustering and DBSCAN, and in what
    type of situations they are best employed. While hierarchical clustering can,
    in some respects, be seen as an extension of the nearest-neighbor approach seen
    in k-means, DBSCAN approaches the problem of finding neighbors by applying a notion
    of density. This can prove extremely beneficial when it comes to highly complex
    data that is intertwined in a complex fashion. While DBSCAN is very powerful,
    it is not infallible and can even be overkill, depending on what your original
    data looks like.
  prefs: []
  type: TYPE_NORMAL
- en: Combined with k-means and hierarchical clustering, however, DBSCAN completes
    a strong toolbox when it comes to the unsupervised learning task of clustering
    your data. When faced with any problem in this space, it is worthwhile comparing
    the performance of each method and seeing which performs best.
  prefs: []
  type: TYPE_NORMAL
- en: 'With clustering explored, we will now move onto another key piece of rounding
    out your skills in unsupervised learning: dimensionality reduction. Through the
    smart reduction of dimensions, we can make clustering easier to understand and
    communicate to stakeholders. Dimensionality reduction is also key to creating
    all types of machine learning models in the most efficient manner possible. In
    the next chapter, we will dive deeper into topic models and see how the aspects
    of clustering learned in these chapters apply to NLP-type problems.'
  prefs: []
  type: TYPE_NORMAL

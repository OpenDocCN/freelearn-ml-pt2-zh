<html><head></head><body>
		<div>
			<div id="_idContainer167" class="Content">
			</div>
		</div>
		<div id="_idContainer168" class="Content">
			<h1 id="_idParaDest-119"><a id="_idTextAnchor121"/>6. t-Distributed Stochastic Neighbor Embedding</h1>
		</div>
		<div id="_idContainer193" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will discuss <strong class="bold">Stochastic Neighbor Embedding</strong> (<strong class="bold">SNE</strong>) and <strong class="bold">t-Distributed Stochastic Neighbor Embedding</strong> (<strong class="bold">t-SNE</strong>) as a means of visualizing high-dimensional datasets. We will implement t-SNE models in scikit-learn and explain the limitations of t-SNE. Being able to extract high-dimensional information into lower dimensions will prove helpful for visualization and exploratory analysis, as well as being helpful in conjunction with the clustering algorithms we explored in prior chapters. By the end of this chapter, we will be able to find clusters in high-dimensional data, <a id="_idTextAnchor122"/>such as user-level information or images in a low-dimensional space.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor123"/>Introduction</h1>
			<p>So far, we have described a number of different methods for reducing the dimensionality of a dataset as a means of cleaning the data, reducing its size for computational efficiency, or for extracting the most important information available within the dataset. While we have demonstrated many methods for reducing high-dimensional datasets, in many cases, we are unable to reduce the number of dimensions to a size that can be visualized, that is, two or three dimensions, without excessively degrading the quality of the data. Consider the MNIST dataset that we used earlier in this book, which was a collection of digitized handwritten digits of the numbers 0 through 9. Each image is 28 x 28 pixels in size, providing 784 individual dimensions or features. If we were to reduce these 784 dimensions down to 2 or 3 for visualization purposes, we would lose almost all the available information.</p>
			<p>In this chapter, we will discuss SNE and t-SNE as means of visualizing high-dimensional datasets. These techniques are extremely helpful in unsupervised learning and the design of machine learning systems because being able to visualize data is a powerful thing. Being able to visualize data allows relationships to be explored, groups to be identified, and results to be validated. t-SNE techniques have been used to visualize cancerous cell nuclei that have over 30 characteristics of interest, whereas data from documents can have over thousands of dimensions, sometimes even after applying techniques such as PCA.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor124"/>The MNIST Dataset</h1>
			<p>Now, we will explore SNE and t-SNE using the MNIST dataset provided with the accompanying source code as the basis of our practical examples. Before we continue, we will quickly review MNIST and the data that is within it. The complete MNIST dataset is a collection of 60,000 training and 10,000 test examples of handwritten digits of the numbers 0 to 9, represented as black and white (or grayscale) images that are 28 x 28 pixels in size (giving 784 dimensions or features) with equal numbers of each type of digit (or class) in the dataset. Due to its size and the quality of the data, MNIST has become one of the quintessential datasets in machine learning, often being used as the reference dataset for many research papers in machine learning. One of the advantages of using MNIST to explore SNE and t-SNE compared to other datasets is that while the samples contain a high number of dimensions, they can be visualized even after dimensionality reduction because they can be represented as an image. <em class="italic">Figure 6.1</em> shows a sample of the MNIST dataset:</p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B15923_06_01.jpg" alt="Figure 6.1: MNIST data sample&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1: MNIST data sample</p>
			<p>The following figure shows the same sample reduced to 30 components using PCA:</p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/B15923_06_02.jpg" alt="Figure 6.2: MNIST reduced using PCA to 30 components&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2: MNIST reduced using PCA to 30 components</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor125"/>Stochastic Neighbor Embedding (SNE)</h1>
			<p>SNE is one of a number of different methods that fall within the category of <strong class="bold">manifold learning</strong>, which aims to describe high-dimensional spaces within low-dimensional manifolds or bounded areas. At first thought, this seems like an impossible task; how can we reasonably represent data in two dimensions if we have a dataset with at least 30 features? As we work through the derivation of SNE, it is hoped that you will see how this is possible. Don't worry – we will not be covering the mathematical details of this process in great depth as it is outside of the scope of this chapter. Constructing an SNE can be divided into the following steps:</p>
			<ol>
				<li>Convert the distances between datapoints in the high-dimensional space into conditional probabilities. Say we had two points, <em class="italic">x</em><span class="subscript">i</span> and <em class="italic">x</em><span class="subscript">j</span>, in a high-dimensional space and we wanted to determine the probability (<em class="italic">p</em><span class="subscript">i|j</span>) that <em class="italic">x</em><span class="subscript">j</span> would be picked as a neighbor of <em class="italic">x</em><span class="subscript">i</span>. To define this probability, we use a Gaussian curve. By doing this, we see that the probability is high for nearby points, while it is very low for distant points.</li>
				<li>We need to determine the width of the Gaussian curve as this controls the rate of probability selection. A wide curve would suggest that many neighboring points are far away, while a narrow curve suggests that they are tightly compacted.</li>
				<li>Once we project the data into the low-dimensional space, we can also determine the corresponding probability (<em class="italic">q</em><span class="subscript">i|j</span>) between the corresponding low-dimensional data, <em class="italic">y</em><span class="subscript">i</span> and <em class="italic">y</em><span class="subscript">j</span>.</li>
				<li>What SNE aims to do is position the data in the lower dimensions to minimize the differences between <em class="italic">p</em><span class="subscript">i|j</span> and <em class="italic">q</em><span class="subscript">i|j</span> over all the datapoints using a cost function (C). This is known as the <strong class="bold">Kullback-Leibler</strong> (<strong class="bold">KL</strong>) divergence:<div id="_idContainer171" class="IMG---Figure"><img src="image/B15923_06_03.jpg" alt="Figure 6.3: KL divergence&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.3: KL divergence</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For Python code to construct a Gaussian distribution, please refer to the <strong class="source-inline">GaussianDist.ipynb</strong> Jupyter notebook at <a href="https://packt.live/2UMVubU">https://packt.live/2UMVubU</a>.</p>
			<p>When Gaussian distribution is used in SNE, it reduces the dimensions of data by preserving localized patterns. To do this, SNE uses the process of gradient descent to minimize C using the standard parameters of the learning rate and epochs, as we covered in the preceding chapter when we looked at neural networks and autoencoders. SNE implements an additional term in the training process—<strong class="bold">perplexity</strong>. Perplexity is a selection of the effective number of neighbors used in the comparison and is relatively stable for the values of perplexity between 5 and 50. In practice, going through a process of trial and error using perplexity values within this range is recommended.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Perplexity is covered in detail later in this chapter.</p>
			<p>SNE provides an effective way of visualizing high-dimensional data in a low-dimensional space, though it still suffers from an issue known as <strong class="bold">the crowding problem</strong>. The crowding problem can occur if we have some points positioned approximately equidistantly within a region around a point, <em class="italic">i</em>. When these points are visualized in the lower-dimensional space, they crowd around each other, making visualization difficult. This problem is exacerbated if we try to put some more space between these crowded points, because any other points that are further away will be placed very far away within the low-dimensional space. Essentially, we are trying to balance being able to visualize close points while not losing information provided by points that are further away.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor126"/>t-Distributed SNE</h1>
			<p>t-SNE aims to address the crowding problem using a modified version of the KL divergence cost function and by substituting the Gaussian distribution with the Student's t-distribution in the low-dimensional space. The Student's t-distribution is a probability distribution much like Gaussian and is used when we have a small sample size and unknown population standard deviation. It is often used in the Student's t-test. </p>
			<p>The modified KL cost function considers the pairwise distances in the low-dimensional space equally, while the Student's distribution employs a heavy tail in the low-dimensional space to avoid the crowding problem. In the higher-dimensional probability calculation, the Gaussian distribution is still used to ensure that a moderate distance in the higher dimensions is still represented as such in the lower dimensions. This combination of different distributions in the respective spaces allows for the faithful representation of datapoints separated by small and moderate distances.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For some example code regarding how to reproduce the Student's t-distribution in Python, please refer to the Jupyter notebook at <a href="https://packt.live/2UMVubU">https://packt.live/2UMVubU</a>.</p>
			<p>Thankfully, we don't need to worry about implementing t-SNE manually because scikit-learn provides a very effective implementation in its straightforward API. What we need to remember is that both SNE and t-SNE determine the probability of two points being neighbors in both high- and low-dimensionality spaces and aim to minimize the difference in the probability between the two spaces.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor127"/>Exercise 6.01: t-SNE MNIST</h2>
			<p>In this exercise, we will use the MNIST dataset (provided in the accompanying source code) to explore the scikit-learn implementation of t-SNE. As we described earlier, using MNIST allows us to visualize the high-dimensional space in a way that is not possible in other datasets, such as the Boston Housing Price or Iris dataset. Perform the following steps:</p>
			<ol>
				<li value="1">For this exercise, import <strong class="source-inline">pickle</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">PCA</strong>, and <strong class="source-inline">TSNE</strong> from scikit-learn, as well as <strong class="source-inline">matplotlib</strong>:<p class="source-code">import pickle</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from sklearn.manifold import TSNE</p><p class="source-code">np.random.seed(2)</p></li>
				<li>Load and visualize the MNIST dataset that is provided with the accompanying source code:<p class="callout-heading">Note</p><p class="callout">You can find the <strong class="source-inline">mnist.pkl</strong> file at <a href="https://packt.live/3aRuNIH">https://packt.live/3aRuNIH</a>.</p><p>The code is as follows:</p><p class="source-code">with open('mnist.pkl', 'rb') as f:</p><p class="source-code">    mnist = pickle.load(f)</p><p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">for i in range(9):</p><p class="source-code">    plt.subplot(3, 3, i + 1)</p><p class="source-code">    plt.imshow(mnist['images'][i], cmap='gray')</p><p class="source-code">    plt.title(mnist['labels'][i])</p><p class="source-code">    plt.axis('off')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer172" class="IMG---Figure"><img src="image/B15923_06_04.jpg" alt="Figure 6.4: Output after loading the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.4: Output after loading the dataset</p><p>This demonstrates that MNIST has been successfully loaded.</p></li>
				<li>In this exercise, we will use PCA on the dataset to extract the first 30 components.<p class="callout-heading">Note</p><p class="callout">The scikit-learn PCA API requires that the data that's passed to the <strong class="source-inline">fit</strong> method is in the form required (number of samples, number of features, and so on). As such, we need to reshape the MNIST images as they are in the form (number of samples, feature 1, feature 2). Hence, we will make use of the <strong class="source-inline">reshape</strong> method in the following source code.</p><p>The code will look as follows:</p><p class="source-code">model_pca = PCA(n_components=30)</p><p class="source-code">mnist_pca = model_pca.fit(mnist['images'].reshape((-1, 28 ** 2)))</p></li>
				<li>Visualize the effect of reducing the dataset to 30 components. To do this, we must transform the dataset into the lower-dimensional space and then use the <strong class="source-inline">inverse_transform</strong> method to return the data to its original size for plotting. We will, of course, need to reshape the data before and after the transform process:<p class="source-code">mnist_30comp = model_pca.transform\</p><p class="source-code">               (mnist['images'].reshape((-1, 28 ** 2)))</p><p class="source-code">mnist_30comp_vis = model_pca.inverse_transform(mnist_30comp)</p><p class="source-code">mnist_30comp_vis = mnist_30comp_vis.reshape((-1, 28, 28))</p><p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">for i in range(9):</p><p class="source-code">    plt.subplot(3, 3, i + 1)</p><p class="source-code">    plt.imshow(mnist_30comp_vis[i], cmap='gray')</p><p class="source-code">    plt.title(mnist['labels'][i])</p><p class="source-code">    plt.axis('off')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer173" class="IMG---Figure"><img src="image/B15923_06_05.jpg" alt="Figure 6.5: Visualizing the effect of reducing the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.5: Visualizing the effect of reducing the dataset</p><p>Note that while we have lost some clarity in the images, for the most part, the numbers are still clearly visible due to the dimension reduction process. It is interesting to note, however, that the number four (4) seems to have been the most visually affected by this process. Perhaps much of the discarded information from the PCA process contained information specific to the samples of four (4).</p></li>
				<li>Now, we will apply t-SNE to the PCA-transformed data to visualize the 30 components in a two-dimensional space. We can construct a t-SNE model in scikit-learn using the standard model API interface. We will start off by using the default values that specify that we are embedding the 30 dimensions into two for visualization using a perplexity of 30, a learning rate of 200, and 1,000 iterations. We will specify a <strong class="source-inline">random_state</strong> value of 0 and set <strong class="source-inline">verbose</strong> to 1:<p class="source-code">model_tsne = TSNE(random_state=0, verbose=1)</p><p class="source-code">model_tsne</p><p>The output is as follows:</p><div id="_idContainer174" class="IMG---Figure"><img src="image/B15923_06_06.jpg" alt="Figure 6.6: Applying t-SNE to PCA-transformed data&#13;&#10;"/></div><p class="figure-caption">Figure 6.6: Applying t-SNE to PCA-transformed data</p><p>In the preceding screenshot, we can see a number of configuration options that are available for the t-distributed SNE model, with some more important than the others. We will focus on the values of <strong class="source-inline">learning_rate</strong>, <strong class="source-inline">n_components</strong>, <strong class="source-inline">n_iter</strong>, <strong class="source-inline">perplexity</strong>, <strong class="source-inline">random_state</strong>, and <strong class="source-inline">verbose</strong>. For <strong class="source-inline">learning_rate</strong>, as we discussed previously, t-SNE uses stochastic gradient descent to project the high-dimensional data into a low-dimensional space. The learning rate controls the speed at which the process is executed. If the learning rate is too high, the model may fail to converge on a solution, and if it's too slow, it may take a very long time to reach it (if at all). A good rule of thumb is to start with the default; if you find the model producing NaNs (not-a-number values), you may need to reduce the learning rate. Once you are happy with the model, it is also wise to reduce the learning rate and let it run for longer (increase <strong class="source-inline">n_iter</strong>) as you may get a slightly better result. <strong class="source-inline">n_components</strong> is the number of dimensions in the embedding (or visualization space). More often than not, you would like a two-dimensional plot of the data, so you just need the default value of <strong class="source-inline">2</strong>. Now, <strong class="source-inline">n_iter</strong> is the maximum number of iterations of gradient descent. <strong class="source-inline">perplexity</strong> is the number of neighbors to use when visualizing the data. </p><p>Typically, a value between 5 and 50 will be appropriate, knowing that larger datasets typically require more perplexity than smaller ones. <strong class="source-inline">random_state</strong> is an important variable for any model or algorithm that initializes its values randomly at the start of training. The random number generators provided within computer hardware and software tools are not, in fact, truly random; they are actually pseudo-random number generators. They give a good approximation of randomness but are not truly random. Random numbers within computers start with a value known as a seed and are then produced in a complicated manner after that. By providing the same seed at the start of the process, the same "random numbers" are produced each time the process is run. While this sounds counter-intuitive, it is great for reproducing machine learning experiments as you won't see any difference in performance solely due to the initialization of the parameters at the start of training. This can provide more confidence that a change in performance is due to the considered change to the model or training; for example, the architecture of the neural network.</p><p class="callout-heading">Note</p><p class="callout">Producing true random sequences is actually one of the hardest tasks to achieve with a computer. Computer software and hardware is designed so that the instructions that are provided are executed in exactly the same way each time they are run so that you get the same result. Random differences in execution, while being ideal for producing sequences of random numbers, would be a nightmare in terms of automating tasks and debugging problems.</p><p><strong class="source-inline">verbose</strong> is the verbosity level of the model and describes the amount of information that's printed to the screen during the model fitting process. A value of 0 indicates no output, while 1 or greater indicates increasing levels of detail in the output.</p></li>
				<li>Use t-SNE to transform the decomposed dataset of MNIST:<p class="source-code">mnist_tsne = model_tsne.fit_transform(mnist_30comp)</p><p>The output is as follows:</p><div id="_idContainer175" class="IMG---Figure"><img src="image/B15923_06_07.jpg" alt="Figure 6.7: Transforming the decomposed dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.7: Transforming the decomposed dataset</p><p>The output provided during the fitting process provides an insight into the calculations being completed by scikit-learn. We can see that it is indexing and computing neighbors for all the samples and is then determining the conditional probabilities of being neighbors for the data in batches of 10. At the end of the process, it provides a mean standard deviation value of <strong class="source-inline">304.9988</strong> with KL divergence after 250 and 1,000 iterations of gradient descent.</p></li>
				<li>Now, visualize the number of dimensions in the returned dataset:<p class="source-code">mnist_tsne.shape</p><p>The output is as follows:</p><p class="source-code">10000,2</p><p>We have successfully reduced the 784 dimensions down to 2 for visualization, so what does it look like?</p></li>
				<li>Create a scatter plot of the two-dimensional data produced by the model:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.scatter(mnist_tsne[:,0], mnist_tsne[:,1], s=5)</p><p class="source-code">plt.title('Low Dimensional Representation of MNIST');</p><p>The output is as follows:</p><div id="_idContainer176" class="IMG---Figure"><img src="image/B15923_06_08.jpg" alt="Figure 6.8: 2D representation of MNIST (no labels)&#13;&#10;"/></div><p class="figure-caption">Figure 6.8: 2D representation of MNIST (no labels)</p><p>In the preceding plot, we can see that we have represented the MNIST data in two dimensions, but we can also see that it seems to be grouped together. There are a number of different clusters or clumps of data congregated together and separated from other clusters by some white space. There also seem to be about nine different groups of data. All these observations suggest that there is a relationship within and between the individual clusters.</p></li>
				<li>Plot the two-dimensional data that's been grouped by the corresponding image labels and use markers to separate the individual labels. <p class="callout-heading">Note</p><p class="callout">The marker parameter corresponds to the shapes that can be seen for individual points on the plot. They don't show up in detail on the plots in this chapter since there are many samples and hence the resolution is lost with the crops.</p><p>Along with the data, add the image labels to the plot to investigate the structure of the embedded data:</p><p class="source-code">MARKER = ['o', 'v', '1', 'p' ,'*', '+', 'x', 'd', '4', '.']</p><p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.title('Low Dimensional Representation of MNIST');</p><p class="source-code">for i in range(10):</p><p class="source-code">    selections = mnist_tsne[mnist['labels'] == i]</p><p class="source-code">    plt.scatter(selections[:,0], selections[:,1], alpha=0.2, \</p><p class="source-code">                marker=MARKER[i], s=5);</p><p class="source-code">    x, y = selections.mean(axis=0)</p><p class="source-code">    plt.text(x, y, str(i), fontdict={'weight': 'bold', \</p><p class="source-code">                                     'size': 30})</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer177" class="IMG---Figure"><img src="image/B15923_06_09.jpg" alt="Figure 6.9: 2D representation of MNIST with labels&#13;&#10;"/></div><p class="figure-caption">Figure 6.9: 2D representation of MNIST with labels</p><p>The preceding plot is very interesting. Here, we can see that the clusters correspond to each of the different image classes (zero through nine) within the dataset. In an unsupervised fashion, that is, without providing the labels in advance, a combination of PCA and t-SNE has been able to separate and group the individual classes within the MNIST dataset. What is particularly interesting is that there seems to be some confusion within the data regarding the number four images and the number nine images, as well as for the five and three images; the two clusters somewhat overlap. This makes sense if we look at the number nine and number four PCA images we extracted from <em class="italic">Step 4</em> of <em class="italic">Exercise 6.01</em>, <em class="italic">t-SNE MNIST</em>:</p><div id="_idContainer178" class="IMG---Figure"><img src="image/B15923_06_10.jpg" alt="Figure 6.10: PCA images of nine&#13;&#10;"/></div><p class="figure-caption">Figure 6.10: PCA images of nine</p><p>They do, in fact, look quite similar; perhaps this is due to the uncertainty in the shape of the number four. Looking at the image that follows, we can see from the four on the left-hand side that the two vertical lines almost join, while the four on the right-hand side has the two lines parallel:</p><div id="_idContainer179" class="IMG---Figure"><img src="image/B15923_06_11.jpg" alt="Figure 6.11: Shape of number four&#13;&#10;"/></div><p class="figure-caption">Figure 6.11: Shape of number four</p><p>The other interesting feature to note in <em class="italic">Figure 6.9</em> is the edge cases, which are shown in color in the Jupyter notebooks. Around the edges of each cluster, we can see that some samples would be misclassified in the traditional supervised learning sense but represent samples that may have more in common with other clusters than their own. Let's take a look at an example; there are a number of samples of the number three that are quite far from the correct cluster.</p></li>
				<li>Get the index of all the number threes in the dataset:<p class="source-code">threes = np.where(mnist['labels'] == 3)[0]</p><p class="source-code">threes</p><p>The output is as follows:</p><p class="source-code"> array([   7,   10,   12, ..., 9974, 9977, 9991], dtype=int64)</p></li>
				<li>Find the threes that were plotted with an <strong class="source-inline">x</strong> value of less than 0:<p class="source-code">tsne_threes = mnist_tsne[threes]</p><p class="source-code">far_threes = np.where(tsne_threes[:,0]&lt; -30)[0]</p><p class="source-code">far_threes</p><p>The output is as follows:</p><div id="_idContainer180" class="IMG---Figure"><img src="image/B15923_06_12.jpg" alt="Figure 6.12: The threes with an x value less than zero&#13;&#10;"/></div><p class="figure-caption">Figure 6.12: The threes with an x value less than zero</p></li>
				<li>Display the coordinates to find one that is reasonably far from the three cluster:<p class="source-code">tsne_threes[far_threes]</p><p>The output is as follows:</p><div id="_idContainer181" class="IMG---Figure"><img src="image/B15923_06_13.jpg" alt="Figure 6.13: Coordinates away from the three cluster&#13;&#10;"/></div><p class="figure-caption">Figure 6.13: Coordinates away from the three cluster</p></li>
				<li>Choose a sample with a reasonably high negative value as an <strong class="source-inline">x</strong> coordinate. In this example, we will select the second sample, which is sample <strong class="source-inline">11</strong>.<p class="callout-heading">Note</p><p class="callout">This index number has been arbitrarily chosen and is just used for example purposes. It does not have significance in and of itself.</p><p>Display the image for the second sample as follows:</p><p class="source-code">plt.imshow(mnist['images'][11], cmap='gray')</p><p class="source-code">plt.axis('off');</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer182" class="IMG---Figure"><img src="image/B15923_06_14.jpg" alt="Figure 6.14: Image of sample 11&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.14: Image of sample 11</p>
			<p>Looking at this sample image and the corresponding t-SNE coordinates, that is, approximately (-33, 26), it is not surprising that this sample lies near the cluster of eights and fives as there are quite a few features that are common to both of those numbers in this image. In this example, we applied a simplified SNE, demonstrating some of its efficiencies as well as possible sources of confusion and the output of unsupervised learning.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iDsCNf">https://packt.live/3iDsCNf</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3gBdrSK">https://packt.live/3gBdrSK</a></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor128"/>Activity 6.01: Wine t-SNE</h2>
			<p>In this activity, we will reinforce our knowledge of t-SNE using the Wine dataset. By completing this activity, you will be able to build-SNE models for your own custom applications. The Wine dataset (<a href="https://archive.ics.uci.edu/ml/datasets/Wine">https://archive.ics.uci.edu/ml/datasets/Wine</a>) is a collection of attributes regarding the chemical analysis of wine from Italy from three different producers, but the same type of wine for each producer. This information could be used as an example to verify the validity of a bottle of wine made from the grapes from a specific region in Italy. The 13 attributes are Alcohol, Malic acid, Ash, Alkalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, and Proline.</p>
			<p>Each sample contains a class identifier (1 – 3).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/">https://archive.ics.uci.edu/ml/machine-learning-databases/wine/</a> (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science). It can also be downloaded from <a href="https://packt.live/3e1JOcY">https://packt.live/3e1JOcY</a>.</p>
			<p>These steps will help you complete this activity:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as the <strong class="source-inline">t-SNE</strong> and <strong class="source-inline">PCA</strong> models from scikit-learn.</li>
				<li>Load the Wine dataset using the <strong class="source-inline">wine.data</strong> file included in the accompanying source code and display the first five rows of data.<p class="callout-heading">Note</p><p class="callout">You can delete columns within pandas DataFrames by using the <strong class="source-inline">del</strong> keyword. Simply pass <strong class="source-inline">del</strong> the DataFrame and the selected column within the square root.</p></li>
				<li>The first column contains the labels; extract this column and remove it from the dataset.</li>
				<li>Execute PCA to reduce the dataset to the first six components.</li>
				<li>Determine the amount of variance within the data described by these six components.</li>
				<li>Create a t-SNE model using a specified random state and a <strong class="source-inline">verbose</strong> value of 1.</li>
				<li>Fit the PCA data to the t-SNE model.</li>
				<li>Confirm that the shape of the t-SNE fitted data is two-dimensional.</li>
				<li>Create a scatter plot of the two-dimensional data.</li>
				<li>Create a secondary scatter plot of the two-dimensional data with the class labels applied to visualize any clustering that may be present.</li>
			</ol>
			<p>By the end of this activity, you will have constructed a t-SNE visualization of the Wine dataset using its six components and identified some relationships in the location of the data within the plot. The final plot will look similar to the following:</p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/B15923_06_15.jpg" alt="Figure 6.15: The expected plot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15: The expected plot</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 460.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor129"/>Interpreting t-SNE Plots</h1>
			<p>Now that we are able to use t-distributed SNE to visualize high-dimensional data, it is important to understand the limitations of such plots and what aspects are important in interpreting and generating them. In this section, we will highlight some of the important features of t-SNE and demonstrate how care should be taken when using this visualization technique.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor130"/>Perplexity</h2>
			<p>As we described in the introduction to t-SNE, the perplexity values specify the number of nearest neighbors to be used when computing the conditional probability. The selection of this value can make a significant difference to the end result; with a low value of perplexity, local variations in the data dominate because a small number of samples are used in the calculation. Conversely, a large value of perplexity considers more global variations as many more samples are used in the calculation. Typically, it is worth trying a range of different values to investigate the effect of perplexity. Again, values between 5 and 50 tend to work quite well.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor131"/>Exercise 6.02: t-SNE MNIST and Perplexity</h2>
			<p>In this exercise, we will try a range of different values for perplexity and look at the effect in the visualization plot:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pickle</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as <strong class="source-inline">PCA</strong> and <strong class="source-inline">t-SNE</strong> from scikit-learn:<p class="source-code">import pickle</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from sklearn.manifold import TSNE</p><p class="source-code">np.random.seed(2)</p></li>
				<li>Load the MNIST dataset.<p class="callout-heading">Note</p><p class="callout">You can find the <strong class="source-inline">mnist.pkl</strong> file at <a href="https://packt.live/2wpnWHs">https://packt.live/2wpnWHs</a>.</p><p>The code is as follows:</p><p class="source-code">with open('mnist.pkl', 'rb') as f:</p><p class="source-code">    mnist = pickle.load(f)</p></li>
				<li>Using PCA, select only the first 30 components of variance from the image data:<p class="source-code">model_pca = PCA(n_components=30)</p><p class="source-code">mnist_pca = model_pca.fit_transform\</p><p class="source-code">            (mnist['images'].reshape((-1, 28 ** 2)))</p></li>
				<li>In this exercise, we are investigating the effect of perplexity on the t-SNE manifold. Iterate through a model/plot loop with a perplexity of 3, 30, and 300:<p class="source-code">MARKER = ['o', 'v', '1', 'p' ,'*', '+', 'x', 'd', '4', '.']</p><p class="source-code">for perp in [3, 30, 300]:</p><p class="source-code">    model_tsne = TSNE(random_state=0, verbose=1, perplexity=perp)</p><p class="source-code">    mnist_tsne = model_tsne.fit_transform(mnist_pca)</p><p class="source-code">    plt.figure(figsize=(10, 7))</p><p class="source-code">    plt.title(f'Low Dimensional Representation of MNIST \</p><p class="source-code">(perplexity = {perp})')</p><p class="source-code">    for i in range(10):</p><p class="source-code">        selections = mnist_tsne[mnist['labels'] == i]</p><p class="source-code">        plt.scatter(selections[:,0], selections[:,1],\</p><p class="source-code">                    alpha=0.2, marker=MARKER[i], s=5)</p><p class="source-code">        x, y = selections.mean(axis=0)</p><p class="source-code">        plt.text(x, y, str(i), \</p><p class="source-code">                 fontdict={'weight': 'bold', 'size': 30})</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer184" class="IMG---Figure"><img src="image/B15923_06_16.jpg" alt="Figure 6.16: Iterating through a model&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.16: Iterating through a model</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding output has been truncated for presentation purposes. Standard outputs like this would typically be much longer. However, it has been included as it is important to keep an eye on such outputs while the model is training.</p>
			<p>Note the KL divergence in each of the three different perplexity values, along with the increase in the average standard deviation (variance). By looking at the following t-SNE plots with class labels, we can see that with a low perplexity value, the clusters are nicely contained with relatively few overlaps. However, there is almost no space between the clusters. As we increase the perplexity, the space between the clusters improves with reasonably clear distinctions at a perplexity of 30. As the perplexity increases to 300, we can see that the clusters of eight and five, along with nine, four, and seven, are starting to converge.</p>
			<p>Let's start with a low perplexity value:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B15923_06_17.jpg" alt="Figure 6.17: Plot of the low perplexity value&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17: Plot of the low perplexity value</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The plotting function in <em class="italic">Step 4</em> would result in this plot. The subsequent outputs are the plots for varying values of perplexity.</p>
			<p>Increasing the perplexity by a factor of 10 shows much clearer clusters:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B15923_06_18.jpg" alt="Figure 6.18: Plot after increasing perplexity by a factor of 10&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18: Plot after increasing perplexity by a factor of 10</p>
			<p>By increasing the perplexity to 300, we start to merge more of the labels together:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B15923_06_19.jpg" alt="Figure 6.19: Increasing the perplexity value to 300&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.19: Increasing the perplexity value to 300</p>
			<p>In this exercise, we developed our understanding of the effect of perplexity and the sensitivity of this value to the overall result. A small perplexity value can lead to a more homogenous mix of locations with very little space between them. Increasing the perplexity separates the clusters more effectively, but an excessive value leads to overlapping clusters.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gI0zdp">https://packt.live/3gI0zdp</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3gDcjxR">https://packt.live/3gDcjxR</a></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor132"/>Activity 6.02: t-SNE Wine and Perplexity</h2>
			<p>In this activity, we will use the Wine dataset to further reinforce the influence of perplexity on the t-SNE visualization process. In this activity, we will try to determine whether we can identify the source of the wine based on its chemical composition. The t-SNE process provides an effective means of representing and possibly identifying the sources.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/">https://archive.ics.uci.edu/ml/machine-learning-databases/wine/</a> (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science). It can be downloaded from <a href="https://packt.live/3aPOmRJ">https://packt.live/3aPOmRJ</a>.</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as the <strong class="source-inline">t-SNE</strong> and <strong class="source-inline">PCA</strong> models from scikit-learn.</li>
				<li>Load the Wine dataset and inspect the first five rows.</li>
				<li>The first column provides the labels; extract these from the DataFrame and store them in a separate variable. Ensure that the column is removed from the DataFrame.</li>
				<li>Execute PCA on the dataset and extract the first six components.</li>
				<li>Construct a loop that iterates through the perplexity values (1, 5, 20, 30, 80, 160, 320). For each loop, generate a t-SNE model with the corresponding perplexity and print a scatter plot of the labeled wine classes. Note the effect of different perplexity values.</li>
			</ol>
			<p>By the end of this activity, you will have generated a two-dimensional representation of the Wine dataset and inspected the resulting plot for clusters or groupings of data. The plot for perplexity value 320 looks as follows:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B15923_06_20.jpg" alt="Figure 6.20: Expected output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.20: Expected output</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 464.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor133"/>Iterations</h2>
			<p>The final parameter we will experiment with is the number of iterations, which, as per our investigation of autoencoders, is simply the number of training epochs to apply to gradient descent. Thankfully, the number of iterations is a reasonably simple parameter to adjust and often requires only a certain amount of patience as the position of the points in the low-dimensional space stabilize in their final locations.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor134"/>Exercise 6.03: t-SNE MNIST and Iterations</h2>
			<p>In this exercise, we will look at the influence of a range of different iteration parameters that have been applied to the t-SNE model and highlight some indicators that perhaps more training is required. Again, the value of these parameters is highly dependent on the dataset and the volume of data that's available for training. We will use MNIST in this example:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pickle</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as <strong class="source-inline">PCA</strong> and <strong class="source-inline">t-SNE</strong> from scikit-learn:<p class="source-code">import pickle</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from sklearn.manifold import TSNE</p><p class="source-code">np.random.seed(2)</p></li>
				<li>Load the MNIST dataset:<p class="callout-heading">Note</p><p class="callout">You can find the <strong class="source-inline">mnist.pkl</strong> file at <a href="https://packt.live/2xXRJao">https://packt.live/2xXRJao</a>.</p><p>The code is as follows:</p><p class="source-code">with open('mnist.pkl', 'rb') as f:</p><p class="source-code">    mnist = pickle.load(f)</p></li>
				<li>Using PCA, select only the first 30 components of variance from the image data:<p class="source-code">model_pca = PCA(n_components=30)</p><p class="source-code">mnist_pca = model_pca.fit_transform(mnist['images']\</p><p class="source-code">                                    .reshape((-1, 28 ** 2)))</p></li>
				<li>In this exercise, we are investigating the effect of iterations on the t-SNE manifold. Iterate through a model/plot loop with iteration and iterate with the progress values <strong class="source-inline">250</strong>, <strong class="source-inline">500</strong>, and <strong class="source-inline">1000</strong>:<p class="source-code">MARKER = ['o', 'v', '1', 'p' ,'*', '+', 'x', 'd', '4', '.']</p><p class="source-code">for iterations in [250, 500, 1000]:</p><p class="source-code">    model_tsne = TSNE(random_state=0, verbose=1, \</p><p class="source-code">                      n_iter=iterations, \</p><p class="source-code">                      n_iter_without_progress=iterations)</p><p class="source-code">    mnist_tsne = model_tsne.fit_transform(mnist_pca)</p><p>The output is as follows:</p><div id="_idContainer189" class="IMG---Figure"><img src="image/B15923_06_21.jpg" alt="Figure 6.21: Iterating through a model&#13;&#10;"/></div><p class="figure-caption">Figure 6.21: Iterating through a model</p></li>
				<li>Plot the results:<p class="source-code">    plt.figure(figsize=(10, 7))</p><p class="source-code">    plt.title(f'Low Dimensional Representation of MNIST \</p><p class="source-code">(iterations = {iterations})')</p><p class="source-code">    for i in range(10):</p><p class="source-code">        selections = mnist_tsne[mnist['labels'] == i]</p><p class="source-code">        plt.scatter(selections[:,0], selections[:,1], \</p><p class="source-code">                    alpha=0.2, marker=MARKER[i], s=5);</p><p class="source-code">        x, y = selections.mean(axis=0)</p><p class="source-code">        plt.text(x, y, str(i), fontdict={'weight': 'bold', \</p><p class="source-code">                                         'size': 30})</p><p class="source-code">plt.show()</p><p>A reduced number of iterations limits the extent to which the algorithm can find relevant neighbors, leading to ill-defined clusters:</p><div id="_idContainer190" class="IMG---Figure"><img src="image/B15923_06_22.jpg" alt="Figure 6.22: Plot after 250 iterations&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.22: Plot after 250 iterations</p>
			<p>Increasing the number of iterations provides the algorithm with enough time to adequately project the data:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B15923_06_23.jpg" alt="Figure 6.23: Plot after increasing the iterations to 500&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.23: Plot after increasing the iterations to 500</p>
			<p>Once the clusters have settled, increased iterations have an extremely small effect and essentially lead to increased training time:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B15923_06_24.jpg" alt="Figure 6.24: Plot after 1,000 iterations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.24: Plot after 1,000 iterations</p>
			<p>Looking at the previous plots, we can see that the cluster positions with iteration values of 500 and 1,000 are stable and relatively unchanged between the plots. The most interesting plot is that of an iteration value of 250, where it seems as though the clusters are still in a process of motion, making their way to the final positions. As such, there is sufficient evidence to suggest that an iteration value of 500 is sufficient.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Zaw1uZ">https://packt.live/2Zaw1uZ</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3gCOiHf">https://packt.live/3gCOiHf</a></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor135"/>Activity 6.03: t-SNE Wine and Iterations</h2>
			<p>In this activity, we will investigate the effect of the number of iterations on the visualization of the Wine dataset. This is a process that's commonly used during the exploration phase of data processing, cleaning, and understanding the relationships in the data. Depending on the dataset and the type of analysis, we may need to try a number of different iterations, such as the ones we will look at in this activity.</p>
			<p>As we mentioned previously, this process is extremely helpful for projecting high-dimensional data down to a lower, more understandable number of dimensions. In this case, our dataset has 13 features; however, in the real world, you can have datasets with hundreds or even thousands of features. A common instance of this would be person-level data, which can have any number of demographic- or action-related features, which can make regular off-the-shelf analysis impossible. t-SNE is a helpful tool for working high-dimensional data into a more intuitive state. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/">https://archive.ics.uci.edu/ml/machine-learning-databases/wine/</a> (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science). It can be downloaded from <a href="https://packt.live/2xXgHXo">https://packt.live/2xXgHXo</a>.</p>
			<p>These steps will help you complete this activity:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as the <strong class="source-inline">t-SNE</strong> and <strong class="source-inline">PCA</strong> models from scikit-learn.</li>
				<li>Load the Wine dataset and inspect the first five rows.</li>
				<li>The first column provides the labels; extract these from the DataFrame and store them in a separate variable. Ensure that the column is removed from the DataFrame.</li>
				<li>Execute PCA on the dataset and extract the first six components.</li>
				<li>Construct a loop that iterates through the iteration values (<strong class="source-inline">250</strong>, <strong class="source-inline">500</strong>, <strong class="source-inline">1000</strong>). For each loop, generate a t-SNE model with the corresponding number of iterations and an identical number of iterations without progress values.</li>
				<li>Construct a scatter plot of the labeled wine classes. Note the effect of different iteration values.</li>
			</ol>
			<p>By completing this activity, we will have seen the effect of modifying the iteration parameter of the model. This is an important parameter in ensuring that the data has settled into a somewhat final position in the low-dimensional space.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 473.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor136"/>Final Thoughts on Visualizations</h2>
			<p>As we conclude this chapter, there are a couple of important aspects to note regarding visualizations. The first is that the size of the clusters or the relative space between clusters may not actually provide any real indication of proximity. As we discussed earlier in this chapter, a combination of Gaussian and Student's t-distributions is used with SNE to represent high-dimensional data in a low-dimensional space. As such, there is no guarantee of a linear relationship in distance since t-SNE balances the positions of localized and global data structures. The actual distance between the points in local structures may be visually very close within the representation, but still might be some distance away in the high-dimensional space.</p>
			<p>This property also has additional consequences in that, sometimes, random data can appear as if it has some structure, and that it is often required to generate multiple visualizations using different values of perplexity, learning rate, number of iterations, and random seed values.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor137"/>Summary</h1>
			<p>In this chapter, we were introduced to t-distributed SNEs as a means of visualizing high-dimensional information that may have been produced from prior processes, such as PCA or autoencoders. We discussed the means by which t-SNEs produce this representation and generated a number of them using the MNIST and Wine datasets and scikit-learn. In this chapter, we were able to look at some of the power of unsupervised learning because PCA and t-SNE were able to cluster the classes of each image without knowing the ground truth result. In the next chapter, we will build on this practical experience by looking into applications of unsupervised learning, including basket analysis and topic modeling.</p>
		</div>
		<div>
			<div id="_idContainer194" class="Content">
			</div>
		</div>
	</body></html>
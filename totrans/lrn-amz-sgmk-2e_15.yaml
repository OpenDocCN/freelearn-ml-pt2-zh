- en: 'Chapter 11: Deploying Machine Learning Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 11 章：部署机器学习模型
- en: 'In previous chapters, we''ve deployed models in the simplest way possible:
    by configuring an estimator, calling the `fit()` `deploy()` API to create a real-time
    endpoint. This is the simplest scenario for development and testing, but it''s
    not the only one.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们以最简单的方式部署了模型：通过配置估算器，调用 `fit()` 和 `deploy()` API 创建实时端点。这是开发和测试中最简单的场景，但并非唯一的方式。
- en: Models can be imported. For example, you could take an existing model that you
    trained on your local machine, import it into SageMaker, and deploy it as if you
    had trained it on SageMaker.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 模型也可以被导入。例如，你可以将自己在本地机器上训练的现有模型导入到 SageMaker，并像在 SageMaker 上训练一样进行部署。
- en: 'In addition, models can be deployed in different configurations, as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型可以以不同的配置进行部署，如下所示：
- en: A single model on a real-time endpoint, which is what we've done so far, as
    well as several model variants in the same endpoint.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单一模型部署在实时端点上，正如我们目前所做的那样，也可以在同一端点上部署多个模型变种。
- en: A sequence of up to five models, called an **inference pipeline**.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最多包含五个模型的序列，称为 **推理管道**。
- en: An arbitrary number of related models that are loaded on demand on the same
    endpoint, known as a **multi-model endpoint**. We'll examine this configuration
    in [*Chapter 13*](B17705_13_Final_JM_ePub.xhtml#_idTextAnchor290), *Optimizing
    Cost and Performance*.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一端点上按需加载的任意数量的相关模型，称为 **多模型端点**。我们将在 [*第 13 章*](B17705_13_Final_JM_ePub.xhtml#_idTextAnchor290)，*优化成本与性能*
    中详细探讨这种配置。
- en: A single model or an inference pipeline that predicts data in batch mode through
    a feature known as **batch transform**.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一模型或一个推理管道，通过一种称为 **批量转换** 的特性进行批量模式预测。
- en: Of course, models can also be exported. You can grab a training artifact in
    **Simple Storage Service** (**S3**), extract the model, and deploy it anywhere
    you like.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，模型也可以被导出。你可以在 **简单存储服务**（**S3**）中获取训练工件，提取模型并将其部署到任何地方。
- en: 'In this chapter, we''ll cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Examining model artifacts and exporting models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查模型工件并导出模型
- en: Deploying models on real-time endpoints
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实时端点上部署模型
- en: Deploying models on batch transformers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在批处理转换器上部署模型
- en: Deploying models on inference pipelines
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理管道上部署模型
- en: Monitoring prediction quality with Amazon SageMaker Model Monitor
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 模型监控器监控预测质量
- en: Deploying models on container services
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在容器服务上部署模型
- en: Let's get started!
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an **Amazon Web Services** (**AWS**) account to run the examples
    included in this chapter. If you haven't got one already, please browse to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个 **亚马逊网络服务**（**AWS**）账户来运行本章中的示例。如果你还没有账户，请访问 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    创建一个。你还应该熟悉 AWS 免费套餐（[https://aws.amazon.com/free/](https://aws.amazon.com/free/)），它让你可以在某些使用限制内免费使用许多
    AWS 服务。
- en: You will need to install and configure the AWS **Command Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装并为你的账户配置 AWS **命令行界面**（**CLI**）（[https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)）。
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory but
    strongly encouraged as it includes many projects that we will need (Jupyter, `pandas`,
    `numpy`, and more).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个正常工作的 Python 3.x 环境。安装 Anaconda 发行版（[https://www.anaconda.com/](https://www.anaconda.com/)）不是强制的，但强烈建议安装，因为它包含了许多我们需要的项目（Jupyter、`pandas`、`numpy`
    等）。
- en: The code examples included in this book are available on GitHub at https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition.
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例可以在 GitHub 上找到，网址为 [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装一个
    Git 客户端才能访问它们（[https://git-scm.com/](https://git-scm.com/)）。
- en: Examining model artifacts and exporting models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查模型工件并导出模型
- en: A model artifact contains one or several files that are produced by a training
    job and that are required for model deployment. The number and nature of these
    files depend on the algorithm that was trained. As we've seen many times, the
    model artifact is stored as a `model.tar.gz` file, at the S3 output location defined
    in the estimator.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型工件包含一个或多个由训练作业生成的文件，这些文件是模型部署所必需的。这些文件的数量和性质取决于所训练的算法。如我们所见，模型工件通常存储为`model.tar.gz`文件，位于估算器定义的
    S3 输出位置。
- en: Let's look at different examples, where we reuse artifacts from the jobs we
    previously trained.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看不同的示例，在这些示例中，我们重用之前训练作业中的工件。
- en: Examining and exporting built-in models
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查和导出内置模型
- en: Almost all built-in algorithms are implemented with **Apache MXNet**, and their
    artifacts reflect this. For more information on MXNet, please visit [https://mxnet.apache.org/](https://mxnet.apache.org/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有内置的算法都使用**Apache MXNet**实现，其工件也反映了这一点。有关 MXNet 的更多信息，请访问[https://mxnet.apache.org/](https://mxnet.apache.org/)。
- en: 'Let''s see how we can load these models directly. Another option would be to
    use **Multi Model Server** (**MMS**) ([https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server)),
    but we''ll proceed as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何直接加载这些模型。另一种选择是使用**多模型服务器**（**MMS**）（[https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server)），但我们将按如下方式继续：
- en: 'Let''s start from the artifact for the **Linear Learner** model we trained
    in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069), *Training Machine
    Learning Models*, as illustrated in the following code snippet:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从我们在[*第 4 章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)中训练的**线性学习者**模型的工件开始，如以下代码片段所示：
- en: '[PRE0]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We load the symbol file, which contains a **JavaScript Object Notation** (**JSON**)
    definition of the model, as follows:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载符号文件，该文件包含模型的**JavaScript 对象表示法**（**JSON**）定义，如下所示：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We use this JSON definition to instantiate a new Gluon model. We also define
    the name of its input symbol (`data`), as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用这个 JSON 定义来实例化一个新的 Gluon 模型。我们还定义了它的输入符号（`data`），如下所示：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we can easily plot the model, like this:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以轻松地绘制模型，如下所示：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This creates the following output:'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Figure 11.1 – Linear Learner model'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 11.1 – 线性学习者模型'
- en: '](img/B17705_11_1.jpg)'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_11_1.jpg)'
- en: Figure 11.1 – Linear Learner model
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.1 – 线性学习者模型
- en: 'Then, we load the model parameters learned during training, as follows:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们加载在训练过程中学到的模型参数，如下所示：
- en: '[PRE4]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We define a test sample stored in an MXNet `NDArray` (https://mxnet.apache.org/versions/1.6/api/python/docs/api/ndarray/index.html),
    as follows:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个存储在 MXNet `NDArray`（https://mxnet.apache.org/versions/1.6/api/python/docs/api/ndarray/index.html）中的测试样本，如下所示：
- en: '[PRE5]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we forward it through the model and read the output, as follows:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将其通过模型并读取输出，如下所示：
- en: '[PRE6]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The predicted price of this house is **US Dollars** (**USD**) 30,173, as illustrated
    here:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该房子的预测价格为**美元**（**USD**）30,173，如下图所示：
- en: '[PRE7]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This technique should work with all MXNet-based algorithms. Now, let's take
    a look at the built-in algorithms for **Computer Vision** (**CV**).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术应该适用于所有基于 MXNet 的算法。现在，让我们看看**计算机视觉**（**CV**）的内置算法。
- en: Examining and exporting built-in CV models
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查和导出内置的计算机视觉（CV）模型
- en: 'The three built-in algorithms for CV are also based on Apache MXNet. The process
    is exactly the same, as outlined here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个内置的计算机视觉算法也基于 Apache MXNet。过程完全相同，如此处所述：
- en: 'The following is the artifact for the **image classification** model we trained
    on dogs and cats in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091),
    *Training Computer Vision Models*:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是我们在[*第 5 章*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091)中训练的**图像分类**模型的工件，*训练计算机视觉模型*：
- en: '[PRE8]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Load the model and its parameters, as follows:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型及其参数，如下所示：
- en: '[PRE9]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The input shape is a 300x300 color image with three channels (**red, green,
    and blue**, or **RGB**). Accordingly, we create a fake image using random values.
    We forward it through the model and read the results, as follows:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入形状是一个 300x300 的彩色图像，具有三个通道（**红色、绿色和蓝色**，或 **RGB**）。因此，我们使用随机值创建一张假图像。我们将其通过模型并读取结果，如下所示：
- en: '[PRE10]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Funnily enough, this random image is classified as a cat, as defined in the
    following code snippet:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有趣的是，这张随机图片被分类为猫，正如以下代码片段中所定义的：
- en: '[PRE11]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Reusing **Object Detection** is more complicated as the training network needs
    to be modified for prediction. You can find an example at [https://github.com/aws-samples/amazon-sagemaker-aws-greengrass-custom-object-detection-model/](https://github.com/aws-samples/amazon-sagemaker-aws-greengrass-custom-object-detection-model/).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 重用**目标检测**更加复杂，因为训练网络需要针对预测进行修改。你可以在[https://github.com/aws-samples/amazon-sagemaker-aws-greengrass-custom-object-detection-model/](https://github.com/aws-samples/amazon-sagemaker-aws-greengrass-custom-object-detection-model/)找到一个示例。
- en: Now, let's look at **Extreme Gradient Boosting** (**XGBoost**) artifacts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下**极限梯度提升**（**XGBoost**）工件。
- en: Examining and exporting XGBoost models
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查和导出XGBoost模型
- en: An XGBoost artifact contains a single file—the model itself. However, the format
    of the model depends on how you're using XGBoost.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个XGBoost工件包含一个文件——模型本身。然而，模型的格式取决于你使用XGBoost的方式。
- en: 'With the built-in algorithm, the model is a pickled file that stores a `Booster`
    object. Once the artifact has been extracted, we simply unpickle the model and
    load it, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内置算法时，模型是一个存储`Booster`对象的pickle文件。一旦工件被提取，我们只需解压并加载模型，如下所示：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With the built-in framework, the model is just a saved model. Once the artifact
    has been extracted, we load the model directly, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内置框架时，模型只是一个保存的模型。一旦工件被提取，我们直接加载该模型，如下所示：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, let's look at **scikit-learn** artifacts.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下**scikit-learn**工件。
- en: Examining and exporting scikit-learn models
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查和导出scikit-learn模型
- en: 'Scikit-learn models are saved and loaded with `joblib` (https://joblib.readthedocs.io),
    as illustrated in the following code snippet. This library contains a set of tools
    that provide lightweight pipelining, but we''ll only use it to save models:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn模型是通过`joblib`（https://joblib.readthedocs.io）保存和加载的，下面的代码片段展示了这一点。这个库提供了一组轻量级的流水线工具，但我们只使用它来保存模型：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finally, let's look at **TensorFlow** artifacts.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看一下**TensorFlow**工件。
- en: Examining and exporting TensorFlow models
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查和导出TensorFlow模型
- en: 'TensorFlow and **Keras** models are saved in **TensorFlow Serving** format,
    as illustrated in the following code snippet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和**Keras**模型以**TensorFlow Serving**格式保存，下面的代码片段展示了这一点：
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The easiest way to serve such a model is to run the **Docker** image for TensorFlow
    Serving, as illustrated in the following code snippet. You can find more details
    at [https://www.tensorflow.org/tfx/serving/serving_basic](https://www.tensorflow.org/tfx/serving/serving_basic):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提供这种模型最简单的方式是运行TensorFlow Serving的**Docker**镜像，下面的代码片段展示了这一点。你可以在[https://www.tensorflow.org/tfx/serving/serving_basic](https://www.tensorflow.org/tfx/serving/serving_basic)找到更多细节：
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Let's look at a final example where we export a Hugging Face model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个最终的例子，其中我们导出一个Hugging Face模型。
- en: Examining and exporting Hugging Face models
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查和导出Hugging Face模型
- en: 'Hugging Face models can be trained on either TensorFlow or PyTorch. Let''s
    reuse our Hugging Face example from [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130),
    *Extending Machine Learning Services with Built-in Frameworks*, where we trained
    a sentiment analysis model with PyTorch, and proceed as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face模型可以在TensorFlow或PyTorch上进行训练。让我们重用我们在[*第7章*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130)中的Hugging
    Face示例，*使用内置框架扩展机器学习服务*，我们使用PyTorch训练了一个情感分析模型，然后按以下步骤进行：
- en: 'We copy the model artifact from S3 and extract it, like this:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从S3复制模型工件并进行解压，像这样：
- en: '[PRE17]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In a Jupyter notebook, we use the Hugging Face API to load the model configuration.
    We then build the model using a `DistilBertForSequenceClassification` object,
    which corresponds to the model that we trained on SageMaker. Here''s the code
    to accomplish this:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Jupyter笔记本中，我们使用Hugging Face API加载模型配置。然后我们使用`DistilBertForSequenceClassification`对象构建模型，该对象对应我们在SageMaker上训练的模型。以下是实现此操作的代码：
- en: '[PRE18]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we fetch the tokenizer associated with the model, as follows:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们获取与模型关联的分词器，如下所示：
- en: '[PRE19]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We write a short function that will apply `softmax` to the activation values
    returned by the output layer of the model, as follows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们写一个简短的函数，将`softmax`应用于模型输出层返回的激活值，如下所示：
- en: '[PRE20]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we define a sample and predict it with our model, as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们定义一个示例并使用我们的模型进行预测，如下所示：
- en: '[PRE21]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As expected, the sentiment is strongly negative, as we can see here:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如预期的那样，情感非常负面，正如我们在这里看到的：
- en: '[PRE22]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This concludes the section on exporting models from SageMaker. As you can see,
    it's really not difficult at all.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容结束了从SageMaker导出模型的内容。如你所见，这其实一点也不难。
- en: Now, let's learn how to deploy models on real-time endpoints.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何在实时端点上部署模型。
- en: Deploying models on real-time endpoints
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在实时端点上部署模型
- en: SageMaker endpoints serve real-time predictions using models hosted on fully
    managed infrastructure. They can be created and managed with either the SageMaker
    `boto3`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 端点使用托管在完全托管基础设施上的模型进行实时预测。它们可以使用 SageMaker `boto3` 创建和管理。
- en: You can find information on your endpoints in SageMaker Studio, under **SageMaker
    resources**/**Endpoints**.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 SageMaker Studio 中的 **SageMaker 资源**/**端点** 下找到有关您端点的信息。
- en: Now, let's look at the SageMaker SDK in greater detail.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地看一下 SageMaker SDK。
- en: Managing endpoints with the SageMaker SDK
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker SDK 管理端点
- en: 'The SageMaker SDK lets you work with endpoints in several ways, as outlined
    here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker SDK 允许您以多种方式使用端点，如下所述：
- en: Configuring an estimator, training it with `fit()`, deploying an endpoint with
    `deploy()`, and invoking it with `predict()`
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置估算器，使用 `fit()` 进行训练，使用 `deploy()` 部署端点，并使用 `predict()` 调用它
- en: Importing and deploying a model
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入和部署模型
- en: Invoking an existing endpoint
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用现有端点
- en: Updating an existing endpoint
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新现有端点
- en: We've used the first scenario in many examples so far. Let's look at the other
    ones.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在许多示例中使用了第一个场景。让我们看看其他的。
- en: Importing and deploying an XGBoost model
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入和部署 XGBoost 模型
- en: 'This is useful when you want to import a model that wasn''t trained on SageMaker,
    or when you want to redeploy a SageMaker model. In the previous section, we saw
    what model artifacts look like, and how we should use them to package models.
    We''ll now proceed as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当您希望导入不在 SageMaker 上训练的模型或重新部署 SageMaker 模型时，这非常有用。在前一节中，我们看到了模型工件的样子，以及如何使用它们来打包模型。我们将按照以下步骤进行：
- en: 'Starting from an XGBoost model that we trained and saved locally with `save_model()`,
    we first create a model artifact by running the following code:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们使用 `save_model()` 在本地训练并保存的 XGBoost 模型开始，我们首先通过运行以下代码创建一个模型工件：
- en: '[PRE23]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In a Jupyter notebook, we upload the model artifact to our default bucket,
    like this:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Jupyter 笔记本中，我们将模型工件上传到我们的默认存储桶，如下所示：
- en: '[PRE24]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we create an `XGBoostModel` object, passing the location of the artifact
    and an inference script (more on this in a second). We also select a framework
    version, and it should match the one we use to train the model. The code is illustrated
    in the following snippet:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个 `XGBoostModel` 对象，传递工件的位置和推断脚本（稍后将详细介绍）。我们还选择了一个框架版本，它应该与我们用来训练模型的版本匹配。代码在以下片段中说明：
- en: '[PRE25]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The inference script is very simple. It only needs to contain a model-loading
    function, as explained when we discussed deploying framework models in [*Chapter
    7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending Machine Learning
    Services with Built-in Frameworks*. The code is illustrated in the following snippet:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推断脚本非常简单。它只需要包含一个加载模型的函数，就像我们在 [*第7章*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130)
    中讨论部署框架模型时所解释的那样。代码在以下片段中说明：
- en: '[PRE26]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Back in the notebook, we then deploy and predict as usual, as follows:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到笔记本，然后我们像往常一样部署和预测，如下所示：
- en: '[PRE27]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now, let's do the same with a TensorFlow model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用 TensorFlow 模型做同样的事情。
- en: Importing and deploying a TensorFlow model
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入和部署 TensorFlow 模型
- en: 'The process is very similar, as we will see next:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程非常类似，我们将在下面看到：
- en: 'We first use `tar` to package a TensorFlow model that we trained and saved
    in TensorFlow Serving format. Our artifact should look like this (please don''t
    forget to create the top-level directory!):'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先使用 `tar` 打包了一个在 TensorFlow Serving 格式中训练并保存的 TensorFlow 模型。我们的工件应该像这样（请不要忘记创建顶级目录！）：
- en: '[PRE28]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, we upload the artifact to S3, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们按如下方式将工件上传到 S3：
- en: '[PRE29]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we create a SageMaker model from the artifact. By default, we don''t
    have to provide an inference script. We would pass if we needed custom preprocessing
    and postprocessing handlers for feature engineering, exotic serialization, and
    so on. You''ll find more information at https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#deploying-from-an-estimator.
    The code is illustrated in the following snippet:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从工件创建一个 SageMaker 模型。默认情况下，我们不必提供推断脚本。如果需要自定义预处理和后处理处理程序用于特征工程、异构序列化等等，我们会通过。您可以在
    https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#deploying-from-an-estimator
    找到更多信息。代码在以下片段中说明：
- en: '[PRE30]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We then deploy and predict as usual, thanks to the **Deep Learning Container**
    (**DLC**) for TensorFlow.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，由于**TensorFlow**的**深度学习容器**（**DLC**），我们像往常一样部署和预测。
- en: Now, let's do a final example, where we import and deploy a Hugging Face model
    with the DLC for PyTorch and an inference script for model loading and custom
    processing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们做一个最终的示例，在这个示例中，我们导入并部署一个Hugging Face模型，使用PyTorch的DLC和一个用于模型加载及自定义处理的推理脚本。
- en: Importing and deploying a Hugging Face model with PyTorch
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch导入并部署Hugging Face模型
- en: 'Let''s reuse our Hugging Face example, and first focus on the inference script.
    It contains four functions: model loading, preprocessing, prediction, and postprocessing.
    We''ll proceed as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用Hugging Face的示例，并首先关注推理脚本。它包含四个函数：模型加载、预处理、预测和后处理。我们将按以下步骤进行：
- en: 'The model-loading function uses the same code that we used when we exported
    the model. The only difference is that we load the file from `model_dir`, which
    is passed by SageMaker to the PyTorch container. We also load the tokenizer once.
    The code is illustrated in the following snippet:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型加载函数使用我们导出模型时所用的相同代码。唯一的不同是我们从`model_dir`加载文件，这个路径由SageMaker传递给PyTorch容器。我们还只加载一次分词器。代码如下所示：
- en: '[PRE31]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preprocessing and postprocessing functions are simple. They only check
    for the correct content and accept types. You can see these in the following code
    snippet:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理和后处理函数很简单。它们只检查正确的内容和接受的类型。您可以在以下代码片段中看到它们：
- en: '[PRE32]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, the prediction function tokenizes input data, predicts it, and returns
    the name of the most probable class, as follows:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，预测函数对输入数据进行分词，进行预测，并返回最可能的类别名称，如下所示：
- en: '[PRE33]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now our inference script is ready, let''s move to a notebook, import the model,
    and deploy it, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的推理脚本已经准备好，接下来我们进入笔记本，导入模型并进行部署，如下所示：
- en: 'We create a `PyTorchModel` object, passing the location of the model artifact
    in S3 and the location of our inference script, as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`PyTorchModel`对象，传递模型文件在S3中的位置和推理脚本的位置，如下所示：
- en: '[PRE34]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We deploy with `model.deploy()`. Then, we create two samples and send them
    to our endpoint, as follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`model.deploy()`进行部署。然后，我们创建两个样本并将它们发送到我们的端点，如下所示：
- en: '[PRE35]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As expected, the outputs are `positive` and `negative`.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如预期，输出结果为`positive`和`negative`。
- en: This concludes the section on importing and deploying models. Now, let's learn
    how to invoke an endpoint that has already been deployed.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本节关于导入和部署模型的内容到此结束。接下来，让我们学习如何调用已经部署的端点。
- en: Invoking an existing endpoint
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调用现有端点
- en: 'This is useful when you want to work with a live endpoint but don''t have access
    to the predictor. All we need to know is the endpoint''s name. Proceed as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要与一个实时端点交互，但没有访问预测器的权限时，这非常有用。我们只需要知道端点的名称，按以下步骤进行：
- en: 'Build a `TensorFlowPredictor` predictor for the endpoint we deployed in a previous
    example. Again, the object is framework-specific. The code is illustrated in the
    following snippet:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为我们之前部署的端点构建一个`TensorFlowPredictor`预测器。该对象是框架特定的。代码如下所示：
- en: '[PRE36]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, predict it as usual, as follows:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，像往常一样进行预测，如下所示：
- en: '[PRE37]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now, let's learn how to update endpoints.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何更新端点。
- en: Updating an existing endpoint
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新现有端点
- en: The `update_endpoint()` API lets you update the configuration of an endpoint
    in a non-disruptive fashion. The endpoint stays in service, and you can keep predicting
    with it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_endpoint()` API允许你以非破坏性的方式更新端点的配置。端点仍然在服务中，你可以继续使用它进行预测。'
- en: 'Let''s try this on our TensorFlow endpoint, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在TensorFlow端点上尝试这个，如下所示：
- en: 'We set the instance count to `2` and update the endpoint, as follows:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将实例数量设置为`2`并更新端点，如下所示：
- en: '[PRE38]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The endpoint is immediately updated, as shown in the following screenshot.![Figure
    11.2 – Endpoint being updated
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 端点会立即更新，如下图所示。![图 11.2 – 端点正在更新
- en: '](img/B17705_11_2.jpg)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_11_2.jpg)'
- en: Figure 11.2 – Endpoint being updated
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.2 – 端点正在更新
- en: 'Once the update is complete, the endpoint is now backed by two instances, as
    shown in the following screenshot:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦更新完成，端点将由两个实例支持，如下图所示：
- en: '![Figure 11.3 – Endpoint backed by two instances'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3 – 由两个实例支持的端点'
- en: '](img/B17705_11_3.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_11_3.jpg)'
- en: Figure 11.3 – Endpoint backed by two instances
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 由两个实例支持的端点
- en: As you can see, it's very easy to import, deploy, redeploy, and update models
    with the SageMaker SDK. However, some operations require that we work with lower-level
    APIs. They're available in the AWS language SDKs, and we'll use our good friend
    `boto3` to demonstrate them.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用SageMaker SDK导入、部署、重新部署和更新模型非常容易。然而，某些操作要求我们与更底层的API进行交互。它们可以在AWS语言SDK中找到，我们将利用我们亲爱的朋友`boto3`来演示这些操作。
- en: Managing endpoints with the boto3 SDK
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用boto3 SDK管理端点
- en: '`boto3` is the AWS SDK for Python ([https://aws.amazon.com/sdk-for-python/](https://aws.amazon.com/sdk-for-python/)).
    It includes APIs for all AWS services (unless they don''t have APIs!). The SageMaker
    API is available at https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`boto3`是AWS的Python SDK（[https://aws.amazon.com/sdk-for-python/](https://aws.amazon.com/sdk-for-python/)）。它包括所有AWS服务的API（除非它们没有API！）。SageMaker
    API可通过https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html访问。'
- en: '`boto3` APIs are service-level APIs, and they give us full control over all
    service operations. Let''s see how they can help us deploy and manage endpoints
    in ways that the SageMaker SDK doesn''t allow.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`boto3` API是服务级别的API，它们赋予我们对所有服务操作的完全控制。让我们来看看它们如何帮助我们以SageMaker SDK无法实现的方式部署和管理端点。'
- en: Deploying endpoints with the boto3 SDK
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用boto3 SDK部署端点
- en: 'Deploying an endpoint with `boto3` is a four-step operation, outlined as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`boto3`部署端点是一个四步操作，概述如下：
- en: Create one or more models with the `create_model()` API. Alternatively, we could
    use existing models that have been trained or imported with the SageMaker SDK.
    For the sake of brevity, we'll do this here.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`create_model()` API创建一个或多个模型。或者，我们可以使用已经训练或通过SageMaker SDK导入的现有模型。为了简洁起见，我们将在此进行演示。
- en: Define one or more **production variants**, listing the infrastructure requirements
    for each model.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个或多个**生产变体**，列出每个模型的基础设施需求。
- en: Create an `create_endpoint_config()` API, passing the production variants defined
    previously and assigning each one a weight.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`create_endpoint_config()` API，传递之前定义的生产变体，并为每个变体分配一个权重。
- en: Create an endpoint with the `create_endpoint()` API.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`create_endpoint()` API创建端点。
- en: 'Let''s put these APIs to work and deploy an endpoint running two variants of
    the XGBoost model we trained on the Boston Housing dataset, as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用这些API，部署一个运行我们在波士顿住房数据集上训练的XGBoost模型的端点，该端点包含两个变体，如下所示：
- en: 'We define two variants; both are backed by a single instance. However, they
    will receive nine-tenths and one-tenth of incoming requests, respectively—that
    is to say, "variant weight/sum of weights". We could use this setup if we wanted
    to introduce a new model in production and make sure it worked fine before sending
    it traffic. The code is illustrated in the following snippet:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了两个变体；它们都由一个单一实例支持。然而，它们将分别接收九成和一成的请求——也就是说，“变体权重/权重总和”。如果我们想要在生产环境中引入一个新模型并确保它正常工作，再向其发送流量时，可以使用这种设置。代码如以下片段所示：
- en: '[PRE39]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We create an endpoint configuration by passing our two variants and setting
    optional tags, as follows:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过传递这两个变体并设置可选标签来创建端点配置，如下所示：
- en: '[PRE40]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can list all endpoint configurations with `list_endpoint_configs()` and describe
    a particular one with the `describe_endpoint_config()` `boto3` APIs.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用`list_endpoint_configs()`列出所有端点配置，并使用`describe_endpoint_config()` `boto3`
    API描述特定配置。
- en: 'We create an endpoint based on this configuration:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们基于此配置创建一个端点：
- en: '[PRE41]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We can list all the endpoints with `list_endpoints()` and describe a particular
    one with the `describe_endpoint()` `boto3` APIs.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用`list_endpoints()`列出所有端点，并使用`describe_endpoint()` `boto3` API描述特定端点。
- en: 'Creating a `boto3` waiter is a handy way to wait for the endpoint to be in
    service. You can see one being created here:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`boto3`等待器是一种便捷的方式，用于等待端点投入使用。你可以在这里看到创建过程：
- en: '[PRE42]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: After a few minutes, the endpoint is in service. As shown in the following screenshot,
    it now uses two production variants:![Figure 11.4 – Viewing production variants
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几分钟后，端点已投入使用。如以下截图所示，现在它使用了两个生产变体：![图 11.4 – 查看生产变体
- en: '](img/B17705_11_4.jpg)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_11_4.jpg)'
- en: Figure 11.4 – Viewing production variants
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 11.4 – 查看生产变体
- en: 'Then, we invoke the endpoint, as shown in the following code snippet. By default,
    prediction requests are forwarded to variants according to their weights:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们调用端点，如以下代码片段所示。默认情况下，预测请求会根据变体的权重转发：
- en: '[PRE43]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can also select the variant that receives the prediction request. This is
    useful for A/B testing, where we need to stick users to a given model. The following
    code snippet shows you how to do this:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This results in the following output:'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can also update weights—for example, give equal weights to both variants
    so that they receive the same share of incoming traffic—as follows:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can remove one variant entirely and send all traffic to the remaining one.
    Here too, the endpoint stays in service the whole time, and no traffic is lost.
    The code is illustrated in the following snippet:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, we clean up by deleting the endpoint and the two endpoint configurations,
    as follows:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As you can see, the `boto3` API is more verbose, but it also gives us the flexibility
    we need for **machine learning** (**ML**) operations. In the next chapter, we'll
    learn how to automate these.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models on batch transformers
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some use cases don't require a real-time endpoint. For example, you may want
    to predict 10 **gigabytes** (**GB**) of data once a week in one go, get the results,
    and feed them to a downstream application. Batch transformers are a very simple
    way to get this done.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use the scikit-learn script that we trained on the
    Boston Housing dataset in [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130),
    *Extending Machine Learning Services with Built-in Frameworks*. Let''s get started,
    as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure the estimator as usual, by running the following code:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s predict the training set in batch mode. We remove the target value,
    save the dataset to a **comma-separated values** (**CSV**) file, and upload it
    to S3, as follows:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create a transformer object and launch batch processing, as follows:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the training log, we can see that SageMaker creates a temporary endpoint
    and uses it to predict data. For large-scale jobs, we could optimize throughput
    by mini-batching samples for prediction (using the `strategy` parameter), increase
    the level of prediction concurrency (`max_concurrent_transforms`), and increase
    the maximum payload size (`max_payload`).
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the job is complete, predictions are available in S3, as indicated here:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Using the AWS CLI, we can easily retrieve these predictions by running the
    following code:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Just as for training, the infrastructure used by the transformer is shut down
    as soon as the job completes, so there's nothing to clean up.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we will look at inference pipelines and how to use them
    to deploy a sequence of related models.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models on inference pipelines
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-life ML scenarios often involve more than one model; for example, you may
    need to run preprocessing steps on incoming data or reduce its dimensionality
    with the **Principal Component Analysis** (**PCA**) algorithm.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you could deploy each model to a dedicated endpoint. However, orchestration
    code would be required to pass prediction requests to each model in sequence.
    Multiplying endpoints would also introduce additional costs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以将每个模型部署到独立的端点。然而，必须编写编排代码来按顺序将预测请求传递给每个模型。增加多个端点也会带来额外的成本。
- en: Instead, **inference pipelines** let you deploy up to five models on the same
    endpoint or for batch transform and automatically handle the prediction sequence.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，**推理管道** 允许你在同一个端点上部署最多五个模型，或者用于批量转换，并自动处理预测顺序。
- en: 'Let''s say that we wanted to run PCA and then Linear Learner. Building the
    inference pipeline would look like this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想先运行 PCA，然后再运行 Linear Learner。构建推理管道将如下所示：
- en: Train the PCA model on the input dataset.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在输入数据集上训练 PCA 模型。
- en: Process the training and validation sets with PCA and store the results in S3\.
    batch transform is a good way to do this.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 PCA 处理训练集和验证集，并将结果存储在 S3 中。批量转换是实现这一目标的一个好方法。
- en: Train the Linear Learner model using the datasets processed by PCA as input.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 PCA 处理后的数据集作为输入训练 Linear Learner 模型。
- en: 'Use the `create_model()` API to create an inference pipeline, as follows:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `create_model()` API 创建推理管道，如下所示：
- en: '[PRE54]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Create an endpoint configuration and an endpoint in the usual way. We could
    also use the pipeline with a batch transformer.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式创建端点配置和端点。我们还可以使用批量转换器来使用管道。
- en: You can find a complete example that uses scikit-learn and Linear Learner at
    [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline)
    找到一个完整的示例，使用了 scikit-learn 和 Linear Learner。
- en: '**Spark** is a very popular choice for data processing, and SageMaker lets
    you deploy Spark models with the **SparkML Serving** built-in container (https://github.com/aws/sagemaker-sparkml-serving-container),
    which uses the **mleap** library ([https://github.com/combust/mleap](https://github.com/combust/mleap)).
    Of course, these models can be part of an **inference pipeline**. You can find
    several examples at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark** 是一种非常流行的数据处理工具，SageMaker 允许你使用内置的 **SparkML Serving** 容器（https://github.com/aws/sagemaker-sparkml-serving-container）部署
    Spark 模型，该容器使用 **mleap** 库（[https://github.com/combust/mleap](https://github.com/combust/mleap)）。当然，这些模型可以作为
    **推理管道** 的一部分。你可以在 [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality)
    找到几个示例。'
- en: 'This concludes our discussion on model deployment. In the next section, we''ll
    introduce a SageMaker capability that helps us detect data issues that impact
    prediction quality: **SageMaker Model Monitor**.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于模型部署的讨论。接下来，我们将介绍一个 SageMaker 功能，帮助我们检测影响预测质量的数据问题：**SageMaker Model
    Monitor**。
- en: Monitoring prediction quality with Amazon SageMaker Model Monitor
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker Model Monitor 监控预测质量
- en: 'SageMaker Model Monitor has two main features, outlined here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Model Monitor 具有两个主要功能，如下所述：
- en: Capturing data sent to an endpoint, as well as predictions returned by the endpoint.
    This is useful for further analysis, or to replay real-life traffic during the
    development and testing of new models.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获发送到端点的数据以及端点返回的预测结果。这对于进一步分析或在新模型的开发和测试过程中重放真实流量非常有用。
- en: Comparing incoming traffic to a baseline built from the training set, as well
    as sending alerts about data quality issues, such as missing features, mistyped
    features, and differences in statistical properties (also known as "data drift").
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将传入流量与基于训练集构建的基准进行比较，并发送关于数据质量问题的警报，例如缺失特征、拼写错误的特征和统计属性差异（也称为“数据漂移”）。
- en: We'll use the **Linear Learner** example from [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*, where we trained a model on the Boston Housing
    dataset. First, we'll add data capture to the endpoint. Then, we'll build a **baseline**
    and set up a **monitoring schedule** to periodically compare the incoming data
    to that baseline.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自[*第4章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)的**线性学习器**示例，*训练机器学习模型*，在该章节中我们使用波士顿房价数据集训练了一个模型。首先，我们将向端点添加数据捕获功能。然后，我们将建立一个**基准**并设置**监控计划**，以便定期将接收到的数据与基准进行比较。
- en: Capturing data
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 捕获数据
- en: We can set up the data-capture process when we deploy an endpoint. We can also
    enable it on an existing endpoint with the `update_endpoint()` API that we just
    used with production variants.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在部署端点时设置数据捕获过程。我们也可以使用我们刚才在生产变体中使用的`update_endpoint()` API在现有端点上启用它。
- en: 'At the time of writing, there are certain caveats that you should be aware
    of, as outlined here:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，有一些注意事项需要你了解，具体内容如下：
- en: You can only send **one sample at a time** if you want to perform model monitoring.
    Mini-batch predictions will be captured, but they will cause the monitoring job
    to fail.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果要进行模型监控，你一次只能发送**一个样本**。虽然小批量预测会被捕获，但它们会导致监控任务失败。
- en: Likewise, data samples and predictions must be **flat, tabular data**. Structured
    data (such as lists of lists and nested JSON) will be captured, but the model-monitoring
    job will fail to process it. Optionally, you can add a preprocessing script and
    a postprocessing script to flatten it. You can find more information at [https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html).
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，数据样本和预测必须是**平坦的、表格化的数据**。结构化数据（如列表中的列表和嵌套的JSON）会被捕获，但模型监控任务无法处理它。你可以选择添加预处理脚本和后处理脚本将其扁平化。你可以在[https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html)找到更多信息。
- en: The content type and the accept type must be **identical**. You can use either
    CSV or JSON, but you can't mix them.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容类型和接受类型必须是**相同的**。你可以使用CSV或JSON，但不能混合使用。
- en: You cannot delete an endpoint if it has a monitoring schedule attached to it.
    You have to **delete the monitoring schedule first**, then delete the endpoint.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果端点附加了监控计划，你无法删除该端点。你必须**先删除监控计划**，然后才能删除端点。
- en: 'Knowing that, let''s capture some data! Here we go:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 既然知道了这一点，我们来捕获一些数据吧！开始：
- en: Training takes place as usual. You can find the code in the GitHub repository.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练像往常一样进行。你可以在GitHub仓库中找到代码。
- en: 'We create a data-capture configuration for 100% of the prediction requests
    and responses, storing everything in S3, as follows:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为100%的预测请求和响应创建了一个数据捕获配置，将所有数据存储在S3中，如下所示：
- en: '[PRE55]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Once the endpoint is in service, we send data for prediction. Within a minute
    or two, we see captured data in S3 and then copy it locally, as follows:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦端点投入使用，我们就会发送数据进行预测。大约一两分钟内，我们可以在S3中看到捕获的数据，然后将其复制到本地，如下所示：
- en: '[PRE56]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Opening one of the files, we see samples and predictions, as follows:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开其中一个文件，我们可以看到样本和预测，如下所示：
- en: '[PRE57]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: If this were live data, we could use it to test new models later on in order
    to compare their performance to existing models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是实时数据，我们可以使用它来测试新的模型，以便将其性能与现有模型进行比较。
- en: Now, let's learn how to create a baseline from the training set.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何从训练集创建基准。
- en: Creating a baseline
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基准
- en: 'SageMaker Model Monitor includes a built-in container we can use to build the
    baseline, and we can use it directly with the `DefaultModelMonitor` object. You
    can also bring your own container, in which case you would use the `ModelMonitor`
    object instead. Let''s get started, as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker模型监控包含一个内置容器，我们可以用它来构建基准，并可以直接与`DefaultModelMonitor`对象一起使用。你也可以带上自己的容器，在这种情况下你将使用`ModelMonitor`对象。让我们开始，如下所示：
- en: 'A baseline can only be built on CSV datasets and JSON datasets. Our dataset
    is space-separated and needs to be converted into a CSV file, as follows. We can
    then upload it to S3:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基准只能建立在CSV数据集和JSON数据集上。我们的数据集是空格分隔的，需要将其转换为CSV文件，如下所示。然后我们可以将其上传到S3：
- en: '[PRE58]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Note
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'There is a small caveat here: the baselining job is a Spark job running in
    **SageMaker Processing**. Hence, column names need to be Spark-compliant, or your
    job will fail in cryptic ways. In particular, dots are not allowed in column names.
    We don''t have that problem here, but please keep this in mind.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the infrastructure requirements, the location of the training set, and
    its format, as follows:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: As you can guess, this is running as a SageMaker Processing job, and you can
    find its log in `/aws/sagemaker/ProcessingJobs` prefix.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Two JSON artifacts are available at its output location: `statistics.json`
    and `constraints.json`. We can view their content with `pandas` by running the
    following code:'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As shown in the following screenshot, the `constraints` file gives us the inferred
    type of each feature, its completeness in the dataset, and whether it contains
    negative values or not:![Figure 11.5 – Viewing the inferred schema
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_11_5.jpg)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.5 – Viewing the inferred schema
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `statistics` file adds basic statistics, as shown in the following screenshot:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Viewing data statistics'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_11_6.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 – Viewing data statistics
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: It also includes distribution information based on KLL sketches (https://arxiv.org/abs/1603.05346v2),
    a compact way to define quantiles.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Once a baseline has been created, we can set up a monitoring schedule in order
    to compare incoming traffic to the baseline.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a monitoring schedule
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We simply pass the name of the endpoint, the statistics, the constraints, and
    the frequency at which the analysis should run. We will go for hourly, which is
    the shortest frequency allowed. The code is illustrated in the following snippet:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Here, the analysis will be performed by a built-in container. Optionally, we
    could provide our own container with bespoke analysis code. You can find more
    information at [https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-containers.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-containers.html).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's send some nasty data to the endpoint and see if SageMaker Model Monitor
    picks it up.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Sending bad data
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unfortunately, a model may receive incorrect data at times. Maybe it''s been
    corrupted at the source, maybe the application in charge of invoking the endpoint
    is buggy, and so on. Let''s simulate this and see how much impact this has on
    the quality of the prediction, as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from a valid sample, we get a correct prediction, as illustrated here:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The price of this house is USD 30,173:'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, let''s multiply the first feature by 10,000, as shown in the following
    code snippet. Scaling and unit errors are quite frequent in application code:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Ouch! The price is negative, as we can see here. Clearly, this is a bogus prediction:'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Let''s try negating the last feature by running the following code:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The prediction is much higher than what it should be, as we can see in the
    following snippet. This is a sneakier issue, which means it is harder to detect
    and could have serious business consequences:'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: You should try experimenting with bad data and see which features are the most
    brittle. All this traffic will be captured by SageMaker Model Monitor. Once the
    monitoring job has run, you should see entries in its **violation report**.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Examining violation reports
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Previously, we created an hourly monitoring job. Don''t worry if it takes a
    little more than 1 hour to see results; job execution is load-balanced by the
    backend, and short delays are likely:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'We can find more information about our monitoring job in the SageMaker console,
    in the `describe_schedule()` API and list executions with the `list_executions()`
    API, as follows:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Here, we can see three executions:'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The violations report is stored as a JSON file in S3\. We can read it and display
    it with `pandas`, as follows:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This prints out the violations that were detected by the last monitoring job,
    as shown in the following screenshot:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Viewing violations'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_11_7.jpg)'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.7 – Viewing violations
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Of course, we can also fetch the file in S3 and display its contents, as follows:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Here''s a sample entry, warning us that the model received a fractional value
    for the `chas` feature, although it''s defined as an integer in the schema:'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: We could also emit these violations to CloudWatch metrics and trigger alarms
    to notify developers of potential data-quality issues. You can find more information
    at [https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html).
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When you''re done, don''t forget to delete the monitoring schedule and the
    endpoint itself, as follows:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: As you can see, SageMaker Model Monitor helps you capture both incoming data
    and predictions, a useful feature for model testing. In addition, you can also
    perform data-quality analysis using a built-in container or your own.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we're going to move away from endpoints and learn how to
    deploy models to container services.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Deploying models to container services
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we saw how to fetch a model artifact in S3 and how to extract the
    actual model from it. Knowing this, it's pretty easy to deploy it on a container
    service, such as **Amazon Elastic Container Service** (**ECS**), **Amazon Elastic
    Kubernetes Service** (**EKS**), or **Amazon Fargate**.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Maybe it's company policy to deploy everything in containers, maybe you just
    like them, or maybe both! Whatever the reason is, you can definitely do it. There's
    nothing specific to SageMaker here, and the AWS documentation for these services
    will tell you everything you need to know.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample high-level process could look like this:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Train a model on SageMaker.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When training is complete, grab the artifact and extract the model.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Push the model to a Git repository.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a task definition (for ECS and Fargate) or a pod definition (for EKS).
    It could use one of the built-in containers or your own. Then, it could run a
    model server or your own code to clone the model from your Git repository, load
    it, and serve predictions.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using this definition, run a container on your cluster.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's apply this to Amazon Fargate.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Training on SageMaker and deploying on Amazon Fargate
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Fargate** lets you run containers on fully managed infrastructure
    ([https://aws.amazon.com/fargate](https://aws.amazon.com/fargate)). There''s no
    need to create and manage clusters, which makes it ideal for users who don''t
    want to get involved with infrastructure details. However, please note that, at
    the time of writing, Fargate doesn''t support **graphics processing unit** (**GPU**)
    containers.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a model
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We prepare the model using the following steps:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: First, we train a TensorFlow model on Fashion-MNIST. Business as usual.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We find the location of the model artifact in S3 and set it as an environment
    variable, as follows:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We download the artifact from S3 and extract it to a local directory, like
    this:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We open a terminal and commit the model to a public Git repository, as illustrated
    in the following code snippet. I''m using one of mine here ([https://gitlab.com/juliensimon/test-models](https://gitlab.com/juliensimon/test-models));
    you should replace it with yours:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Configuring Fargate
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that the model is available in a repository, we need to configure Fargate.
    We''ll use the command line this time. You could do the same with `boto3` or any
    other language SDK. We''ll proceed as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '`ecs-cli` is a convenient CLI tool used to manage clusters. Let''s install
    it by running the following code:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We use it to "create" a Fargate cluster. In practice, this isn''t creating
    any infrastructure; we''re only defining a cluster name that we''ll use to run
    tasks. Please make sure that your `ecs:CreateCluster`. If not, please add it before
    continuing. The code is illustrated in the following snippet:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We create a log group in **CloudWatch** where our container will write its
    output. We only need to do this once. Here''s the code to accomplish this:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We will need a `8500` for Google remote procedure call (`8501` for the **REpresentational
    State Transfer** (**REST**) API). If you don''t have one already, you can easily
    create one in the **Elastic Compute Cloud** (**EC2**) console. Here, I created
    one in my default **virtual private cloud** (**VPC**). It looks like this:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Viewing the security group'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_11_8.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 – Viewing the security group
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Defining a task
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we need to write a **JSON** file containing a **task definition**: the
    container image to use, its entry point, and its system and network properties.
    Let''s get started, as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the amount of **central processing unit** (**CPU**) and memory
    that the task is allowed to consume. Unlike ECS and EKS, Fargate only allows a
    limited set of values, available at [https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html).
    We will go for 4 **virtual CPUs** (**vCPUs**) and 8 GB of **random-access memory**
    (**RAM**), as illustrated in the following code snippet:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Next, we define a container that will load our model and run predictions. We
    will use the DLC for TensorFlow 2.3.0\. You can find a full list at [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).
    The code is illustrated in the following snippet:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Its entry point creates a directory, clones the repository where we pushed
    the model, and launches TensorFlow Serving, as follows:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Accordingly, we map the two TensorFlow Serving ports, like this:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'We define the log configuration that''s pointing at the CloudWatch log group
    we created earlier, as follows:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We set the networking mode for the container, as illustrated in the following
    code snippet. `awsvpc` is the most flexible option, and it will allow our container
    to be publicly accessible, as explained at [https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html).
    It will create an **elastic network interface** in the subnet of our choice:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Finally, we define an IAM role for the task. If this is the first time you''re
    working with ECS, you should create this role in the IAM console. You can find
    instructions for this at https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html.
    The code is illustrated in the following snippet:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Running a task
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''re now ready to run our task using the security group we created earlier
    and one of the subnets in our default VPC. Let''s get started, as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'We launch the task with the `run-task` API, passing the family name of the
    task definition (not the filename!). Please pay attention to the version number
    as well as it will automatically increase every time you register a new version
    of the task definition, so make sure you''re using the latest one. The code is
    illustrated in the following snippet:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'A few seconds later, we can see our prediction container running (showing the
    task **identifier** (**ID**), state, ports, and task definition), as follows:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Using the public **Internet Protocol** (**IP**) address of the container, we
    build a TensorFlow Serving prediction request with 10 sample images and send it
    to our container, as follows:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'When we''re done, we stop the task using the task `run-task` API and delete
    the cluster, as illustrated in the following code snippet. Of course, you can
    also use the ECS console:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: The processes for ECS and EKS are extremely similar. You can find simple examples
    at [https://gitlab.com/juliensimon/dlcontainers](https://gitlab.com/juliensimon/dlcontainers).
    They should be a good starting point if you wish to build your own workflow.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes fans can also use `kubectl` to train and deploy models. A detailed
    tutorial is available at [https://sagemaker.readthedocs.io/en/stable/workflows/kubernetes/index.html](https://sagemaker.readthedocs.io/en/stable/workflows/kubernetes/index.html).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about model artifacts, what they contain, and how
    to use them to export models outside of SageMaker. You also learned how to import
    and deploy existing models, as well as how to manage endpoints in detail, both
    with the SageMaker SDK and the `boto3` SDK.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discussed alternative deployment scenarios with SageMaker, using either
    batch transform or inference pipelines, as well as outside of SageMaker with container
    services.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned how to use SageMaker Model Monitor to capture endpoint
    data and monitor data quality.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we''ll discuss automating ML workflows with three different
    AWS services: **AWS CloudFormation**, the **AWS Cloud Development Kit** (**AWS
    CDK**), and **Amazon SageMaker Pipelines**.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL

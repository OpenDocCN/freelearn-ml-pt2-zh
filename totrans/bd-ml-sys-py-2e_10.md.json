["```py\n>>> import mahotas as mh\n\n```", "```py\n>>> image = mh.imread('scene00.jpg')\n\n```", "```py\n>>> from matplotlib import pyplot as plt\n>>> plt.imshow(image)\n>>> plt.show()\n\n```", "```py\n>>> image = mh.colors.rgb2grey(image, dtype=np.uint8)\n>>> plt.imshow(image) # Display the image\n\n```", "```py\n>>> plt.gray()\n\n```", "```py\n>>> thresh = mh.thresholding.otsu(image)\n>>> print('Otsu threshold is {}.'.format(thresh))\nOtsu threshold is 138.\n>>> plt.imshow(image > thresh)\n\n```", "```py\n>>> im16 = mh.gaussian_filter(image, 16)\n\n```", "```py\n>>> im = mh.demos.load('lena')\n\n```", "```py\n>>> r,g,b = im.transpose(2,0,1)\n\n```", "```py\n>>> r12 = mh.gaussian_filter(r, 12.)\n>>> g12 = mh.gaussian_filter(g, 12.)\n>>> b12 = mh.gaussian_filter(b, 12.)\n>>> im12 = mh.as_rgb(r12, g12, b12)\n\n```", "```py\n>>> h, w = r.shape # height and width\n>>> Y, X = np.mgrid[:h,:w]\n\n```", "```py\n>>> Y = Y - h/2\\. # center at h/2\n>>> Y = Y / Y.max() # normalize to -1 .. +1\n\n>>> X = X - w/2.\n>>> X = X / X.max()\n\n```", "```py\n>>> C = np.exp(-2.*(X**2+ Y**2))\n\n>>> # Normalize again to 0..1\n>>> C = C - C.min()\n>>> C = C / C.ptp()\n>>> C = C[:,:,None] # This adds a dummy third dimension to C\n\n```", "```py\n>>> ringed = mh.stretch(im*C + (1-C)*im12)\n\n```", "```py\n>>> haralick_features = mh.features.haralick(image)\n>>> haralick_features_mean = np.mean(haralick_features, axis=0)\n>>> haralick_features_all = np.ravel(haralick_features)\n\n```", "```py\n>>> from glob import glob\n>>> images = glob('SimpleImageDataset/*.jpg')\n>>> features = []\n>>> labels = []\n>>> for im in images:\n...   labels.append(im[:-len('00.jpg')])\n...   im = mh.imread(im)\n...   im = mh.colors.rgb2gray(im, dtype=np.uint8)\n...   features.append(mh.features.haralick(im).ravel())\n\n>>> features = np.array(features)\n>>> labels = np.array(labels)\n\n```", "```py\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.linear_model import LogisticRegression\n>>> clf = Pipeline([('preproc', StandardScaler()),\n ('classifier', LogisticRegression())])\n\n```", "```py\n>>> from sklearn import cross_validation\n>>> cv = cross_validation.LeaveOneOut(len(images))\n>>> scores = cross_validation.cross_val_score(\n...     clf, features, labels, cv=cv)\n>>> print('Accuracy: {:.1%}'.format(scores.mean()))\nAccuracy: 81.1%\n\n```", "```py\ndef chist(im):\n\n```", "```py\n im = im // 64\n\n```", "```py\n r,g,b = im.transpose((2,0,1))\n pixels = 1 * r + 4 * b + 16 * g\n hist = np.bincount(pixels.ravel(), minlength=64)\n hist = hist.astype(float)\n\n```", "```py\n hist = np.log1p(hist)\n return hist\n\n```", "```py\n>>> features = []\n>>> for im in images:\n...   image = mh.imread(im)\n...   features.append(chist(im))\n\n```", "```py\n>>> features = []\n>>> for im in images:\n...   imcolor = mh.imread(im)\n...   im = mh.colors.rgb2gray(imcolor, dtype=np.uint8)\n...   features.append(np.concatenate([\n...           mh.features.haralick(im).ravel(),\n...           chist(imcolor),\n...       ]))\n\n```", "```py\n>>> scores = cross_validation.cross_val_score(\n...     clf, features, labels, cv=cv)\n>>> print('Accuracy: {:.1%}'.format(scores.mean()))\nAccuracy: 95.6%\n\n```", "```py\n>>> features = []\n>>> for im in images:\n...   imcolor = mh.imread(im)\n...   # ignore everything in the 200 pixels closest to the borders\n...   imcolor = imcolor[200:-200, 200:-200]\n...   im = mh.colors.rgb2gray(imcolor, dtype=np.uint8)\n...   features.append(np.concatenate([\n...           mh.features.haralick(im).ravel(),\n...           chist(imcolor),\n...       ]))\n\n```", "```py\n>>> sc = StandardScaler()\n>>> features = sc.fit_transform(features)\n>>> from scipy.spatial import distance\n>>> dists = distance.squareform(distance.pdist(features))\n\n```", "```py\n>>> fig, axes = plt.subplots(2, 9)\n>>> for ci,i in enumerate(range(0,90,10)):\n...     left = images[i]\n...     dists_left = dists[i]\n...     right = dists_left.argsort()\n...     # right[0] is same as left[i], so pick next closest\n...     right = right[1]\n...     right = images[right]\n...     left = mh.imread(left)\n...     right = mh.imread(right)\n...     axes[0, ci].imshow(left)\n...     axes[1, ci].imshow(right)\n\n```", "```py\n>>> from sklearn.grid_search import GridSearchCV\n>>> C_range = 10.0 ** np.arange(-4, 3)\n>>> grid = GridSearchCV(LogisticRegression(), param_grid={'C' : C_range})\n>>> clf = Pipeline([('preproc', StandardScaler()),\n...                ('classifier', grid)])\n\n```", "```py\n>>> cv = cross_validation.KFold(len(features), 5,\n...                      shuffle=True, random_state=123)\n>>> scores = cross_validation.cross_val_score(\n...    clf, features, labels, cv=cv)\n>>> print('Accuracy: {:.1%}'.format(scores.mean()))\nAccuracy: 72.1%\n\n```", "```py\n>>> from mahotas.features import surf\n>>> image = mh.demos.load('lena')\n>>> image = mh.colors.rgb2gray(im, dtype=np.uint8)\n>>> descriptors = surf.surf(image, descriptor_only=True)\n\n```", "```py\n>>> from mahotas.features import surf\n>>> descriptors = surf.dense(image, spacing=16)\n\n```", "```py\n>>> alldescriptors = []\n>>> for im in images:\n...   im = mh.imread(im, as_grey=True)\n...   im = im.astype(np.uint8)\n...   alldescriptors.append(surf.dense(image, spacing=16))\n>>> # get all descriptors into a single array\n>>> concatenated = np.concatenate(alldescriptors)\n>>> print('Number of descriptors: {}'.format(\n...        len(concatenated)))\nNumber of descriptors: 2489031\n\n```", "```py\n>>> # use only every 64th vector\n>>> concatenated = concatenated[::64]\n>>> from sklearn.cluster import KMeans\n>>> k = 256\n>>> km = KMeans(k)\n>>> km.fit(concatenated)\n\n```", "```py\n>>> sfeatures = []\n>>> for d in alldescriptors:\n...   c = km.predict(d)\n...   sfeatures.append(\n...       np.array([np.sum(c == ci) for ci in range(k)])\n...   )\n>>> # build single array and convert to float\n>>> sfeatures = np.array(sfeatures, dtype=float)\n\n```", "```py\n>>> scores = cross_validation.cross_val_score(\n...    clf, sfeatures, labels, cv=cv)\n>>> print('Accuracy: {:.1%}'.format(scores.mean()))\nAccuracy: 62.6%\n\n```", "```py\n>>> combined = np.hstack([features, features])\n>>> scores = cross_validation.cross_val_score(\n...    clf, combined, labels, cv=cv)\n>>> print('Accuracy: {:.1%}'.format(scores.mean()))\nAccuracy: 76.1%\n\n```"]
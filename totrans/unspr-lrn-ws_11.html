<html><head></head><body>
		<div id="_idContainer355" class="Content">
			<h1 id="_idParaDest-212"><a id="_idTextAnchor216"/>Appendix</h1>
		</div>
		<div id="_idContainer439" class="Content">
			<h1 id="_idParaDest-213"><a id="_idTextAnchor217"/>1. Introduction to Clustering</h1>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor218"/>Activity 1.01: Implementing k-means Clustering</h2>
			<p>Solution: </p>
			<ol>
				<li>Import the required libraries:<p class="source-code">from sklearn.datasets import make_blobs</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">from sklearn.metrics import accuracy_score, silhouette_score</p><p class="source-code">import matplotlib.pyplot as plt<a id="_idTextAnchor219"/></p><p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from scipy.spatial.distance import cdist</p><p class="source-code">import math</p><p class="source-code">np.random.seed(0)</p><p class="source-code">%matplotlib inline</p></li>
				<li>Load the seeds data file using <strong class="source-inline">pandas</strong>:<p class="source-code">seeds = pd.read_csv('Seed_Data.csv')</p></li>
				<li>Return the first five rows of the dataset, as follows:<p class="source-code">seeds.head()</p><p>The output is as follows:</p><div id="_idContainer356" class="IMG---Figure"><img src="image/B15923_01_25.jpg" alt="Figure 1.25: Displaying the first five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 1.25: Displaying the first five rows of the dataset</p></li>
				<li>Separate the <strong class="source-inline">X</strong> features as follows:<p class="source-code">X = seeds[['A','P','C','LK','WK','A_Coef','LKG']]</p><p class="source-code">y = seeds['target']</p></li>
				<li>Check the features as follows:<p class="source-code">X.head()</p><p>The output is as follows:</p><div id="_idContainer357" class="IMG---Figure"><img src="image/B15923_01_26.jpg" alt="Figure 1.26: Printing the features&#13;&#10;"/></div><p class="figure-caption">Figure 1.26: Printing the features</p></li>
				<li>Define the <strong class="source-inline">k_means</strong> function as follows and initialize the k-centroids randomly. Repeat this process until the difference between the new/old <strong class="source-inline">centroids</strong> equals <strong class="source-inline">0</strong>, using the <strong class="source-inline">while</strong> loop:<p class="source-code-heading">Activity 1.01.ipynb</p><p class="source-code">def k_means(X, K):</p><p class="source-code">    # Keep track of history so you can see K-Means in action</p><p class="source-code">    centroids_history = []</p><p class="source-code">    labels_history = []</p><p class="source-code">    </p><p class="source-code">    # Randomly initialize Kcentroids</p><p class="source-code">    rand_index = np.random.choice(X.shape[0], K)  </p><p class="source-code">    centroids = X[rand_index]</p><p class="source-code">    centroids_history.append(centroids)</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/2JPZ4M8">https://packt.live/2JPZ4M8</a>.</p></li>
				<li>Convert the pandas DataFrame into a NumPy matrix:<p class="source-code">X_mat = X.values</p></li>
				<li>Run our seeds matrix through the <strong class="source-inline">k_means</strong> function we created earlier:<p class="source-code">centroids, labels, centroids_history, labels_history = \</p><p class="source-code">k_means(X_mat, 3)</p></li>
				<li>Print the labels:<p class="source-code">print(labels)</p><p>The output is as follows:</p><div id="_idContainer358" class="IMG---Figure"><img src="image/B15923_01_27.jpg" alt="Figure 1.27: Printing the labels&#13;&#10;"/></div><p class="figure-caption">Figure 1.27: Printing the labels</p></li>
				<li>Plot the coordinates as follows:<p class="source-code">plt.scatter(X['A'], X['LK'])</p><p class="source-code">plt.title('Wheat Seeds - Area vs Length of Kernel')</p><p class="source-code">plt.show()</p><p class="source-code">plt.scatter(X['A'], X['LK'], c=labels, cmap='tab20b')</p><p class="source-code">plt.title('Wheat Seeds - Area vs Length of Kernel')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer359" class="IMG---Figure"><img src="image/B15923_01_28.jpg" alt="Figure 1.28: Plotting the coordinates&#13;&#10;"/></div><p class="figure-caption">Figure 1.28: Plotting the coordinates</p></li>
				<li>Calculate the silhouette score as follows:<p class="source-code">silhouette_score(X[['A','LK']], labels)</p><p>The output is as follows:</p><p class="source-code">0.5875704550892767</p></li>
			</ol>
			<p>By completing this activity, you have gained hands-on experience of tuning a k-means clustering algorithm for a real-world dataset. The seeds dataset is seen as a classic "hello world"-type problem in the data science space and is helpful for testing foundational techniques.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2JPZ4M8">https://packt.live/2JPZ4M8</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Ocncuh">https://packt.live/2Ocncuh</a>.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor220"/>2. Hierarchical Clustering</h1>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor221"/>Activity 2.01: Comparing k-means with Hierarchical Clustering</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the necessary packages from scikit-learn (<strong class="source-inline">KMeans</strong>, <strong class="source-inline">AgglomerativeClustering</strong>, and <strong class="source-inline">silhouette_score</strong>), as follows:<p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">from sklearn.cluster import AgglomerativeClustering</p><p class="source-code">from sklearn.metrics import silhouette_score</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Read the wine dataset into the Pandas DataFrame and print a small sample:<p class="source-code">wine_df = pd.read_csv("wine_data.csv")</p><p class="source-code">print(wine_df.head())</p><p>The output is as follows:</p><div id="_idContainer360" class="IMG---Figure"><img src="image/B15923_02_25.jpg" alt="Figure 2.25: The output of the wine dataset&#13;&#10;"/></div><p class="figure-caption">Figure 2.25: The output of the wine dataset</p></li>
				<li>Visualize the wine dataset to understand the data structure:<p class="source-code">plt.scatter(wine_df.values[:,0], wine_df.values[:,1])</p><p class="source-code">plt.title("Wine Dataset")</p><p class="source-code">plt.xlabel("OD Reading")</p><p class="source-code">plt.ylabel("Proline")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer361" class="IMG---Figure"><img src="image/B15923_02_26.jpg" alt="Figure 2.26: A plot of raw wine data&#13;&#10;"/></div><p class="figure-caption">Figure 2.26: A plot of raw wine data</p></li>
				<li>Use the <strong class="source-inline">sklearn</strong> implementation of k-means on the wine dataset, knowing that there are three wine types:<p class="source-code">km = KMeans(3)</p><p class="source-code">km_clusters = km.fit_predict(wine_df)</p></li>
				<li>Use the <strong class="source-inline">sklearn</strong> implementation of hierarchical clustering on the wine dataset:<p class="source-code">ac = AgglomerativeClustering(3, linkage='average')</p><p class="source-code">ac_clusters = ac.fit_predict(wine_df)</p></li>
				<li>Plot the predicted clusters from k-means as follows:<p class="source-code">plt.scatter(wine_df.values[:,0], \</p><p class="source-code">            wine_df.values[:,1], c=km_clusters)</p><p class="source-code">plt.title("Wine Clusters from K-Means Clustering")</p><p class="source-code">plt.xlabel("OD Reading")</p><p class="source-code">plt.ylabel("Proline")</p><p class="source-code">plt.show()</p><p>The output is as follows</p><div id="_idContainer362" class="IMG---Figure"><img src="image/B15923_02_27.jpg" alt="Figure 2.27: A plot of clusters from k-means clustering&#13;&#10;"/></div><p class="figure-caption">Figure 2.27: A plot of clusters from k-means clustering</p></li>
				<li>Plot the predicted clusters from hierarchical clustering as follows:<p class="source-code">plt.scatter(wine_df.values[:,0], \</p><p class="source-code">            wine_df.values[:,1], c=ac_clusters)</p><p class="source-code">plt.title("Wine Clusters from Agglomerative Clustering")</p><p class="source-code">plt.xlabel("OD Reading")</p><p class="source-code">plt.ylabel("Proline")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer363" class="IMG---Figure"><img src="image/B15923_02_28.jpg" alt="Figure 2.28: A plot of clusters from agglomerative clustering&#13;&#10;"/></div><p class="figure-caption">Figure 2.28: A plot of clusters from agglomerative clustering</p><p class="callout-heading">Note</p><p class="callout">In <em class="italic">Figure 2.23</em> and <em class="italic">Figure 2.24</em>, each color represents a single cluster. The colors of the clusters will change every time the code is executed.</p></li>
				<li>Compare the silhouette score of each clustering method:<p class="source-code">print("Silhouette Scores for Wine Dataset:\n")</p><p class="source-code">print("K-Means Clustering: ", silhouette_score\</p><p class="source-code">     (wine_df, km_clusters))</p><p class="source-code">print("Agg Clustering: ", silhouette_score(wine_df, ac_clusters))</p><p>The output will be as follows:</p><p class="source-code">Silhouette Scores for Wine Dataset:</p><p class="source-code">K-Means Clustering:  0.5809421087616886</p><p class="source-code">Agg Clustering:  0.5988495817462</p></li>
			</ol>
			<p>As you can see from the preceding silhouette metric, agglomerative clustering narrowly beats k-means clustering when it comes to separating the clusters by mean intra-cluster distance. This is not the case for every version of agglomerative clustering, however. Instead, try different linkage types and examine how the silhouette score and clustering changes between each.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2AFA60Z">https://packt.live/2AFA60Z</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fe2lTi">https://packt.live/3fe2lTi</a>.</p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor222"/>3. Neighborhood Approaches and DBSCAN</h1>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor223"/>Activity 3.01: Implementing DBSCAN from Scratch</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Generate a random cluster dataset as follows:<p class="source-code">from sklearn.cluster import DBSCAN</p><p class="source-code">from sklearn.datasets import make_blobs</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">%matplotlib inline</p><p class="source-code">X_blob, y_blob = make_blobs(n_samples=500, \</p><p class="source-code">                            centers=4, n_features=2, \</p><p class="source-code">                            random_state=800)</p></li>
				<li>Visualize the generated data:<p class="source-code">plt.scatter(X_blob[:,0], X_blob[:,1])</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer364" class="IMG---Figure"><img src="image/B15923_03_15.jpg" alt="Figure 3.15: Plot of the data generated&#13;&#10;"/></div><p class="figure-caption">Figure 3.15: Plot of the data generated</p></li>
				<li>Create functions from scratch that allow you to call DBSCAN on a dataset:<p class="source-code-heading">Activity3.01.ipynb</p><p class="source-code">def scratch_DBSCAN(x, eps, min_pts): </p><p class="source-code">    """</p><p class="source-code">    param x (list of vectors): your dataset to be clustered</p><p class="source-code">    param eps (float): neighborhood radius threshold </p><p class="source-code">    param min_pts (int): minimum number of points threshold for </p><p class="source-code">    a neighborhood to be a cluster</p><p class="source-code">    """</p><p class="source-code">    # Build a label holder that is comprised of all 0s</p><p class="source-code">    labels = [0]* x.shape[0]</p><p class="source-code">    # Arbitrary starting "current cluster" ID</p><p class="source-code">    C = 0</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/3c1rONO">https://packt.live/3c1rONO</a>.</p></li>
				<li>Use your created DBSCAN implementation to find clusters in the generated dataset. Feel free to use hyperparameters as you see fit, tuning them based on their performance in <em class="italic">Step 5</em>:<p class="source-code">labels = scratch_DBSCAN(X_blob, 0.6, 5)</p></li>
				<li>Visualize the clustering performance of your DBSCAN implementation:<p class="source-code">plt.scatter(X_blob[:,0], X_blob[:,1], c=labels)</p><p class="source-code">plt.title("DBSCAN from Scratch Performance")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer365" class="IMG---Figure"><img src="image/B15923_03_16.jpg" alt="Figure 3.16: Plot of DBSCAN implementation&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.16: Plot of DBSCAN implementation</p>
			<p>In the preceding output, you can see that there are four clearly defined clusters in our generated data. The non-highlighted points fall out of neighborhood range and thus are considered noise. While it may not be ideal since not every point is accounted for, for most business cases, this noise is acceptable. If it is not acceptable in your scenario, you can tune the supplied hyperparameters to be more forgiving of distance. </p>
			<p>As you may have noticed, it takes quite some time for a custom implementation to run. This is because we explored the non-vectorized version of this algorithm for the sake of clarity. In most cases, you should aim to use the DBSCAN implementation provided by scikit-learn, as it is highly optimized.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3c1rONO">https://packt.live/3c1rONO</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2ZVoFuO">https://packt.live/2ZVoFuO</a>.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor224"/>Activity 3.02: Comparing DBSCAN with k-means and Hierarchical Clustering</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the necessary packages:<p class="source-code">from sklearn.cluster \</p><p class="source-code">import KMeans, AgglomerativeClustering, DBSCAN</p><p class="source-code">from sklearn.metrics import silhouette_score</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p></li>
				<li>Load the wine dataset from <em class="italic">Chapter 2</em>, <em class="italic">Hierarchical Clustering</em>, and familiarize yourself again with what the data looks like:<p class="source-code"># Load Wine data set</p><p class="source-code">wine_df = pd.read_csv("wine_data.csv")</p><p class="source-code"># Show sample of data set</p><p class="source-code">print(wine_df.head())</p><p>The output is as follows:</p><div id="_idContainer366" class="IMG---Figure"><img src="image/B15923_03_17.jpg" alt="Figure 3.17: First five rows of the wine dataset &#13;&#10;"/></div><p class="figure-caption">Figure 3.17: First five rows of the wine dataset </p></li>
				<li>Visualize the data:<p class="source-code">plt.scatter(wine_df.values[:,0], wine_df.values[:,1])</p><p class="source-code">plt.title("Wine Dataset")</p><p class="source-code">plt.xlabel("OD Reading")</p><p class="source-code">plt.ylabel("Proline")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer367" class="IMG---Figure"><img src="image/B15923_03_18.jpg" alt="Figure 3.18: Plot of the data&#13;&#10;"/></div><p class="figure-caption">Figure 3.18: Plot of the data</p></li>
				<li>Generate clusters using k-means, agglomerative clustering, and DBSCAN:<p class="source-code"># Generate clusters from K-Means</p><p class="source-code">km = KMeans(3)</p><p class="source-code">km_clusters = km.fit_predict(wine_df)</p><p class="source-code"># Generate clusters using Agglomerative Hierarchical Clustering</p><p class="source-code">ac = AgglomerativeClustering(3, linkage='average')</p><p class="source-code">ac_clusters = ac.fit_predict(wine_df)</p></li>
				<li>Evaluate a few different options for DSBSCAN hyperparameters and their effect on the silhouette score:<p class="source-code">db_param_options = [[20,5],[25,5],[30,5],[25,7],[35,7],[40,5]]</p><p class="source-code">for ep,min_sample in db_param_options:</p><p class="source-code">    # Generate clusters using DBSCAN</p><p class="source-code">    db = DBSCAN(eps=ep, min_samples = min_sample)</p><p class="source-code">    db_clusters = db.fit_predict(wine_df)</p><p class="source-code">    print("Eps: ", ep, "Min Samples: ", min_sample)</p><p class="source-code">    print("DBSCAN Clustering: ", \</p><p class="source-code">          silhouette_score(wine_df, db_clusters))</p><p>The output is as follows:</p><div id="_idContainer368" class="IMG---Figure"><img src="image/B15923_03_19.jpg" alt="Figure 3.19: Printing the silhouette score for clusters&#13;&#10;"/></div><p class="figure-caption">Figure 3.19: Printing the silhouette score for clusters</p></li>
				<li>Generate the final clusters based on the highest silhouette score (<strong class="source-inline">eps</strong>: <strong class="source-inline">35</strong>, <strong class="source-inline">min_samples</strong>: <strong class="source-inline">3</strong>):<p class="source-code"># Generate clusters using DBSCAN</p><p class="source-code">db = DBSCAN(eps=40, min_samples = 5)</p><p class="source-code">db_clusters = db.fit_predict(wine_df)</p></li>
				<li>Visualize clusters generated using each of the three methods:<p class="source-code">plt.title("Wine Clusters from K-Means")</p><p class="source-code">plt.scatter(wine_df['OD_read'], wine_df['Proline'], \</p><p class="source-code">            c=km_clusters,s=50, cmap='tab20b')</p><p class="source-code">plt.show()</p><p class="source-code">plt.title("Wine Clusters from Agglomerative Clustering")</p><p class="source-code">plt.scatter(wine_df['OD_read'], wine_df['Proline'], \</p><p class="source-code">            c=ac_clusters,s=50, cmap='tab20b')</p><p class="source-code">plt.show()</p><p class="source-code">plt.title("Wine Clusters from DBSCAN")</p><p class="source-code">plt.scatter(wine_df['OD_read'], wine_df['Proline'], \</p><p class="source-code">            c=db_clusters,s=50, cmap='tab20b')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer369" class="IMG---Figure"><img src="image/B15923_03_20.jpg" alt="Figure 3.20: Plot of clusters using different algorithms&#13;&#10;"/></div><p class="figure-caption">Figure 3.20: Plot of clusters using different algorithms</p></li>
				<li>Evaluate the silhouette score of each approach:<p class="source-code"># Calculate Silhouette Scores</p><p class="source-code">print("Silhouette Scores for Wine Dataset:\n")</p><p class="source-code">print("K-Means Clustering: ", \</p><p class="source-code">       silhouette_score(wine_df, km_clusters))</p><p class="source-code">print("Agg Clustering: ", \</p><p class="source-code">      silhouette_score(wine_df, ac_clusters))</p><p class="source-code">print("DBSCAN Clustering: ", \</p><p class="source-code">      silhouette_score(wine_df, db_clusters))</p><p>The output is as follows: </p><p class="source-code">Silhouette Scores for Wine Dataset:</p><p class="source-code">K-Means Clustering:  0.5809421087616886</p><p class="source-code">Agg Clustering:  0.5988495817462</p><p class="source-code">DBSCAN Clustering:  0.5739675293567901</p></li>
			</ol>
			<p>As you can see, DBSCAN isn't automatically the best choice for your clustering needs. One key trait that makes it different from other algorithms is the use of noise as a potential clustering. In some cases, this is great, as it removes outliers; however, there may be situations where it is not tuned well enough and classifies too many points as noise. You can improve the silhouette score further by tuning the hyperparameters while fitting your clustering algorithms – try a few different combinations and see how they affect your score.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2BNSQvC">https://packt.live/2BNSQvC</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3iElboS">https://packt.live/3iElboS</a>.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor225"/>4. Dimensionality Reduction Techniques and PCA</h1>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor226"/>Activity 4.01: Manual PCA versus scikit-learn</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong> plotting libraries and the scikit-learn <strong class="source-inline">PCA</strong> model:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p></li>
				<li>Load the dataset and select only the sepal features as per the previous exercises. Display the first five rows of the data:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')</p><p class="source-code">df = df[['A', 'LK']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer370" class="IMG---Figure"><img src="image/B15923_04_36.jpg" alt="Figure 4.36: The first five rows of the data&#13;&#10;"/></div><p class="figure-caption">Figure 4.36: The first five rows of the data</p></li>
				<li>Compute the <strong class="source-inline">covariance</strong> matrix for the data:<p class="source-code">cov = np.cov(df.values.T)</p><p class="source-code">cov</p><p>The output is as follows:</p><p class="source-code">array([[8.46635078, 1.22470367],</p><p class="source-code">       [1.22470367, 0.19630525]])</p></li>
				<li>Transform the data using the scikit-learn API and only the first principal component. Store the transformed data in the <strong class="source-inline">sklearn_pca</strong> variable:<p class="source-code">model = PCA(n_components=1)</p><p class="source-code">sklearn_pca = model.fit_transform(df.values)</p></li>
				<li>Transform the data using the manual PCA and only the first principal component. Store the transformed data in the <strong class="source-inline">manual_pca</strong> variable:<p class="source-code">eigenvectors, eigenvalues, _ = \</p><p class="source-code">np.linalg.svd(cov, full_matrices=False)</p><p class="source-code">P = eigenvectors[0]</p><p class="source-code">manual_pca = P.dot(df.values.T)</p></li>
				<li>Plot the <strong class="source-inline">sklearn_pca</strong> and <strong class="source-inline">manual_pca</strong> values on the same plot to visualize the difference:<p class="source-code">plt.figure(figsize=(10, 7)) </p><p class="source-code">plt.plot(sklearn_pca, label='Scikit-learn PCA')</p><p class="source-code">plt.plot(manual_pca, label='Manual PCA', linestyle='--')</p><p class="source-code">plt.xlabel('Sample')</p><p class="source-code">plt.ylabel('Transformed Value')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer371" class="IMG---Figure"><img src="image/B15923_04_37.jpg" alt="Figure 4.37: A plot of the data&#13;&#10;"/></div><p class="figure-caption">Figure 4.37: A plot of the data</p></li>
				<li>Notice that the two plots look almost identical, except that one is a mirror image of the other and there is an offset between the two. Display the components of the <strong class="source-inline">sklearn_pca</strong> and <strong class="source-inline">manual_pca</strong> models:<p class="source-code">model.components_</p><p>The output is as follows:</p><p class="source-code">array([[0.98965371, 0.14347657]])</p></li>
				<li>Now, print <strong class="source-inline">P</strong>:<p class="source-code">P</p><p>The output is as follows:</p><p class="source-code">array([-0.98965371, -0.14347657])</p><p>Notice the difference in the signs; the values are identical, but the signs are different, producing the mirror image result. This is just a difference in convention, and nothing meaningful.</p></li>
				<li>Multiply the <strong class="source-inline">manual_pca</strong> models by <strong class="source-inline">-1</strong> and replot:<p class="source-code">manual_pca *= -1</p><p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.plot(sklearn_pca, label='Scikit-learn PCA')</p><p class="source-code">plt.plot(manual_pca, label='Manual PCA', linestyle='--')</p><p class="source-code">plt.xlabel('Sample')</p><p class="source-code">plt.ylabel('Transformed Value')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer372" class="IMG---Figure"><img src="image/B15923_04_38.jpg" alt="Figure 4.38: Replotted data&#13;&#10;"/></div><p class="figure-caption">Figure 4.38: Replotted data</p></li>
				<li>Now, all we need to do is deal with the offset between the two. The scikit-learn API subtracts the mean of the data prior to the transform. Subtract the mean of each column from the dataset before completing the transform with manual PCA:<p class="source-code">mean_vals = np.mean(df.values, axis=0)</p><p class="source-code">offset_vals = df.values - mean_vals</p><p class="source-code">manual_pca = P.dot(offset_vals.T)</p></li>
				<li>Multiply the result by <strong class="source-inline">-1</strong>:<p class="source-code">manual_pca *= -1</p></li>
				<li>Replot the individual <strong class="source-inline">sklearn_pca</strong> and <strong class="source-inline">manual_pca</strong> values:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.plot(sklearn_pca, label='Scikit-learn PCA')</p><p class="source-code">plt.plot(manual_pca, label='Manual PCA', linestyle='--')</p><p class="source-code">plt.xlabel('Sample')</p><p class="source-code">plt.ylabel('Transformed Value')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer373" class="IMG---Figure"><img src="image/B15923_04_39.jpg" alt="Figure 4.39: Replotting the data&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.39: Replotting the data</p>
			<p>The final plot will demonstrate that the dimensionality reduction completed by the two methods is, in fact, the same. The differences lie in the differences in the signs of the <strong class="source-inline">covariance</strong> matrices, as the two methods simply use a different feature as the baseline for comparison. Finally, there is also an offset between the two datasets, which is attributed to the mean samples being subtracted before executing the transform in the scikit-learn PCA.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2O9MEk4">https://packt.live/2O9MEk4</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3gBntTU">https://packt.live/3gBntTU</a>.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor227"/>Activity 4.02: PCA Using the Expanded Seeds Dataset</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong> and <strong class="source-inline">matplotlib</strong>. To enable 3D plotting, you will also need to import <strong class="source-inline">Axes3D</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from mpl_toolkits.mplot3d import Axes3D #Required for 3D plotting</p></li>
				<li>Read in the dataset and select the <strong class="source-inline">A</strong>, <strong class="source-inline">LK</strong>, and <strong class="source-inline">C</strong> columns:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')[['A', 'LK', 'C']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer374" class="IMG---Figure"><img src="image/B15923_04_40.jpg" alt="Figure 4.40: Area, length, and compactness of the kernel&#13;&#10;"/></div><p class="figure-caption">Figure 4.40: Area, length, and compactness of the kernel</p></li>
				<li>Plot the data in three dimensions:<p class="source-code">fig = plt.figure(figsize=(10, 7))</p><p class="source-code"># Where Axes3D is required</p><p class="source-code">ax = fig.add_subplot(111, projection='3d')</p><p class="source-code">ax.scatter(df['A'], df['LK'], df['C'])</p><p class="source-code">ax.set_xlabel('Area of Kernel')</p><p class="source-code">ax.set_ylabel('Length of Kernel')</p><p class="source-code">ax.set_zlabel('Compactness of Kernel')</p><p class="source-code">ax.set_title('Expanded Seeds Dataset')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer375" class="IMG---Figure"><img src="image/B15923_04_41.jpg" alt="Figure 4.41: Expanded Seeds dataset plot&#13;&#10;"/></div><p class="figure-caption">Figure 4.41: Expanded Seeds dataset plot</p></li>
				<li>Create a <strong class="source-inline">PCA</strong> model without specifying the number of components:<p class="source-code">model = PCA()</p></li>
				<li>Fit the model to the dataset:<p class="source-code">model.fit(df.values)</p><p>The output is as follows:</p><p class="source-code">PCA(copy=True, iterated_power='auto', n_components=None, </p><p class="source-code">    random_state=None,</p><p class="source-code">    svd_solver='auto', tol=0.0, whiten=False)</p></li>
				<li>Display the eigenvalues or <strong class="source-inline">explained_variance_ratio_</strong>:<p class="source-code">model.explained_variance_ratio_</p><p>The output is as follows:</p><p class="source-code">array([9.97794495e-01, 2.19418709e-03, 1.13183333e-05])</p><p>We want to reduce the dimensionality of the dataset, but still keep at least 90% of the variance. What is the minimum number of components required to keep 90% of the variance?</p><p>Only the first component is required for at least a 90% variance. The first component provides 99.7% of the variance within the dataset.</p></li>
				<li>Create a new <strong class="source-inline">PCA</strong> model, this time specifying the number of components required to retain at least 90% of the variance:<p class="source-code">model = PCA(n_components=1)</p></li>
				<li>Transform the data using the new model:<p class="source-code">data_transformed = model.fit_transform(df.values)</p></li>
				<li>Restore the transformed data to the original dataspace:<p class="source-code">data_restored = model.inverse_transform(data_transformed)</p></li>
				<li>Plot the restored data in three dimensions in one subplot and the original data in a second subplot to visualize the effect of removing some of the variance:<p class="source-code">fig = plt.figure(figsize=(10, 14))</p><p class="source-code"># Original Data</p><p class="source-code">ax = fig.add_subplot(211, projection='3d')</p><p class="source-code">ax.scatter(df['A'], df['LK'], df['C'], label='Original Data');</p><p class="source-code">ax.set_xlabel('Area of Kernel');</p><p class="source-code">ax.set_ylabel('Length of Kernel');</p><p class="source-code">ax.set_zlabel('Compactness of Kernel');</p><p class="source-code">ax.set_title('Expanded Seeds Dataset');</p><p class="source-code"># Transformed Data</p><p class="source-code">ax = fig.add_subplot(212, projection='3d')</p><p class="source-code">ax.scatter(data_restored[:,0], data_restored[:,1], \</p><p class="source-code">           data_restored[:,2], label='Restored Data');</p><p class="source-code">ax.set_xlabel('Area of Kernel');</p><p class="source-code">ax.set_ylabel('Length of Kernel');</p><p class="source-code">ax.set_zlabel('Compactness of Kernel');</p><p class="source-code">ax.set_title('Restored Seeds Dataset');</p><p>The output is as follows:</p><div id="_idContainer376" class="IMG---Figure"><img src="image/B15923_04_42.jpg" alt="Figure 4.42: Plot of the expanded and the restored Seeds datasets&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.42: Plot of the expanded and the restored Seeds datasets</p>
			<p>Looking at the preceding diagram, we can see that we have removed much of the noise within the data, but retained the most important information regarding the trends within the data. You can see that, in general, the compactness of the wheat kernel increases as the area increases.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">When applying PCA, it is important to keep in mind the size of the data being modeled, along with the available system memory. The singular value decomposition process involves separating the data into the eigenvalues and eigenvectors and can be quite memory-intensive. If the dataset is too large, you may either be unable to complete the process, suffer significant performance loss, or lock up your system.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZVpaFc">https://packt.live/2ZVpaFc</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3gIrR3D">https://packt.live/3gIrR3D</a>.</p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor228"/>5. Autoencoders</h1>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor229"/>Activity 5.01: The MNIST Neural Network</h2>
			<p>Solution:</p>
			<p>In this activity, you will train a neural network to identify images in the MNIST dataset and reinforce your skills in training neural networks:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pickle</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">matplotlib</strong>, and the <strong class="source-inline">Sequential</strong> and <strong class="source-inline">Dense</strong> classes from Keras:<p class="source-code">import pickle</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">import tensorflow.python.util.deprecation as deprecation</p><p class="source-code">deprecation._PRINT_DEPRECATION_WARNINGS = False</p></li>
				<li>Load the <strong class="source-inline">mnist.pkl</strong> file, which contains the first 10,000 images and corresponding labels from the MNIST dataset that are available in the accompanying source code. The MNIST dataset is a series of 28 x 28 grayscale images of handwritten digits 0 through 9. Extract the images and labels:<p class="source-code">with open('mnist.pkl', 'rb') as f:</p><p class="source-code">    data = pickle.load(f)</p><p class="source-code">images = data['images']</p><p class="source-code">labels = data['labels']</p></li>
				<li>Plot the first 10 samples along with the corresponding labels:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">for i in range(10):</p><p class="source-code">    plt.subplot(2, 5, i + 1)</p><p class="source-code">    plt.imshow(images[i], cmap='gray')</p><p class="source-code">    plt.title(labels[i])</p><p class="source-code">    plt.axis('off')</p><p>The output is as follows:</p><div id="_idContainer377" class="IMG---Figure"><img src="image/B15923_05_40.jpg" alt="Figure 5.40: First 10 samples&#13;&#10;"/></div><p class="figure-caption">Figure 5.40: First 10 samples</p></li>
				<li>Encode the labels using one-hot encoding:<p class="source-code">one_hot_labels = np.zeros((images.shape[0], 10))</p><p class="source-code">for idx, label in enumerate(labels):</p><p class="source-code">    one_hot_labels[idx, label] = 1</p><p class="source-code">one_hot_labels</p><p>The output is as follows:</p><p class="source-code">array([[0., 0., 0., ..., 0., 0., 0.],</p><p class="source-code">       [1., 0., 0., ..., 0., 0., 0.],</p><p class="source-code">       [0., 0., 0., ..., 0., 0., 0.],</p><p class="source-code">       ...,</p><p class="source-code">       [0., 0., 0., ..., 0., 0., 0.],</p><p class="source-code">       [0., 0., 0., ..., 0., 0., 1.],</p><p class="source-code">       [0., 0., 0., ..., 1., 0., 0.]])</p></li>
				<li>Prepare the images for input into a neural network. As a hint, there are two separate steps in this process:<p class="source-code">images = images.reshape((-1, 28 ** 2))</p><p class="source-code">images = images / 255.</p></li>
				<li>Construct a neural network model in Keras that accepts the prepared images, has a hidden layer of 600 units with a ReLU activation function, and an output of the same number of units as classes. The output layer uses a <strong class="source-inline">softmax</strong> activation function:<p class="source-code">model = Sequential([Dense(600, input_shape=(784,), \</p><p class="source-code">                    activation='relu'), \</p><p class="source-code">                    Dense(10, activation='softmax'),])</p></li>
				<li>Compile the model using multiclass cross-entropy, stochastic gradient descent, and an accuracy performance metric:<p class="source-code">model.compile(loss='categorical_crossentropy', \</p><p class="source-code">              optimizer='sgd', metrics=['accuracy'])</p></li>
				<li>Train the model. How many epochs are required to achieve at least 95% classification accuracy on the training data? Let's have a look:<p class="source-code">model.fit(images, one_hot_labels, epochs=20)</p><p>The output is as follows:</p><div id="_idContainer378" class="IMG---Figure"><img src="image/B15923_05_41.jpg" alt="Figure 5.41: Training the model&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.41: Training the model</p>
			<p>15 epochs are required to achieve at least 95% classification accuracy on the training set.</p>
			<p>In this example, we have measured the performance of the neural network classifier using the data that the classifier was trained with. In general, this method should not be used as it typically reports a higher level of accuracy than you should expect from the model. In supervised learning problems, there are a number of <strong class="bold">cross-validation</strong> techniques that should be used instead. As this is a course on unsupervised learning, cross-validation lies outside the scope of this book.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VZpLnZ">https://packt.live/2VZpLnZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Z9ueGz">https://packt.live/2Z9ueGz</a>.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor230"/>Activity 5.02: Simple MNIST Autoencoder</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pickle</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as the <strong class="source-inline">Model</strong>, <strong class="source-inline">Input</strong>, and <strong class="source-inline">Dense</strong> classes, from Keras:<p class="source-code">import pickle</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from keras.models import Model</p><p class="source-code">from keras.layers import Input, Dense</p><p class="source-code">import tensorflow.python.util.deprecation as deprecation</p><p class="source-code">deprecation._PRINT_DEPRECATION_WARNINGS = False</p></li>
				<li>Load the images from the supplied sample of the MNIST dataset that is provided with the accompanying source code (<strong class="source-inline">mnist.pkl</strong>):<p class="source-code">with open('mnist.pkl', 'rb') as f:</p><p class="source-code">    images = pickle.load(f)['images']</p></li>
				<li>Prepare the images for input into a neural network. As a hint, there are <strong class="bold">two</strong> separate steps in this process:<p class="source-code">images = images.reshape((-1, 28 ** 2))</p><p class="source-code">images = images / 255.</p></li>
				<li>Construct a simple autoencoder network that reduces the image size to 10 x 10 after the encoding stage:<p class="source-code">input_stage = Input(shape=(784,))</p><p class="source-code">encoding_stage = Dense(100, activation='relu')(input_stage)</p><p class="source-code">decoding_stage = Dense(784, activation='sigmoid')(encoding_stage)</p><p class="source-code">autoencoder = Model(input_stage, decoding_stage)</p></li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and <strong class="source-inline">adadelta</strong> gradient descent:<p class="source-code">autoencoder.compile(loss='binary_crossentropy', \</p><p class="source-code">                    optimizer='adadelta')</p></li>
				<li>Fit the encoder model:<p class="source-code">autoencoder.fit(images, images, epochs=100)</p><p>The output is as follows:</p><div id="_idContainer379" class="IMG---Figure"><img src="image/B15923_05_42.jpg" alt="Figure 5.42: Training the model&#13;&#10;"/></div><p class="figure-caption">Figure 5.42: Training the model</p></li>
				<li>Calculate and store the output of the encoding stage for the first five samples:<p class="source-code">encoder_output = Model(input_stage, encoding_stage)\</p><p class="source-code">                 .predict(images[:5])</p></li>
				<li>Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by 255:<p class="source-code">encoder_output = encoder_output.reshape((-1, 10, 10)) * 255</p></li>
				<li>Calculate and store the output of the decoding stage for the first five samples:<p class="source-code">decoder_output = autoencoder.predict(images[:5])</p></li>
				<li>Reshape the output of the decoder to 28 x 28 and multiply by 255:<p class="source-code">decoder_output = decoder_output.reshape((-1, 28, 28)) * 255</p></li>
				<li>Plot the original image, the encoder output, and the decoder:<p class="source-code">images = images.reshape((-1, 28, 28))</p><p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">for i in range(5):</p><p class="source-code">    plt.subplot(3, 5, i + 1)</p><p class="source-code">    plt.imshow(images[i], cmap='gray')</p><p class="source-code">    plt.axis('off')</p><p class="source-code">    plt.subplot(3, 5, i + 6)</p><p class="source-code">    plt.imshow(encoder_output[i], cmap='gray')</p><p class="source-code">    plt.axis('off')</p><p class="source-code">    plt.subplot(3, 5, i + 11)</p><p class="source-code">    plt.imshow(decoder_output[i], cmap='gray')</p><p class="source-code">    plt.axis('off')</p><p>The output is as follows:</p><div id="_idContainer380" class="IMG---Figure"><img src="image/B15923_05_43.jpg" alt="Figure 5.43: The original image, the encoder output, and the decoder&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.43: The original image, the encoder output, and the decoder</p>
			<p>So far, we have shown how a simple single hidden layer in both the encoding and decoding stage can be used to reduce the data to a lower dimension space. We can also make this model more complicated by adding additional layers to both the encoding and the decoding stages.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3f5ZSdH">https://packt.live/3f5ZSdH</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2W0ZkhP">https://packt.live/2W0ZkhP</a>.</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor231"/>Activity 5.03: MNIST Convolutional Autoencoder</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pickle</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">matplotlib</strong>, and the <strong class="source-inline">Model</strong> class from <strong class="source-inline">keras.models</strong>, and import <strong class="source-inline">Input</strong>, <strong class="source-inline">Conv2D</strong>, <strong class="source-inline">MaxPooling2D</strong>, and <strong class="source-inline">UpSampling2D</strong> from <strong class="source-inline">keras.layers</strong>:<p class="source-code">import pickle</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from keras.models import Model</p><p class="source-code">from keras.layers \</p><p class="source-code">import Input, Conv2D, MaxPooling2D, UpSampling2D</p><p class="source-code">import tensorflow.python.util.deprecation as deprecation</p><p class="source-code">deprecation._PRINT_DEPRECATION_WARNINGS = False</p></li>
				<li>Load the data:<p class="source-code">with open('mnist.pkl', 'rb') as f:</p><p class="source-code">    images = pickle.load(f)['images']</p></li>
				<li>Rescale the images to have values between 0 and 1:<p class="source-code">images = images / 255.</p></li>
				<li>We need to reshape the images to add a single depth channel for use with convolutional stages. Reshape the images to have a shape of 28 x 28 x 1:<p class="source-code">images = images.reshape((-1, 28, 28, 1))</p></li>
				<li>Define an input layer. We will use the same shape input as an image:<p class="source-code">input_layer = Input(shape=(28, 28, 1,))</p></li>
				<li>Add a convolutional stage with 16 layers or filters, a 3 x 3 weight matrix, a ReLU activation function, and using the same padding, which means the output has the same length as the input image:<p class="source-code">hidden_encoding = \</p><p class="source-code">Conv2D(16, # Number of layers or filters in the weight matrix \</p><p class="source-code">       (3, 3), # Shape of the weight matrix \</p><p class="source-code">       activation='relu', \</p><p class="source-code">       padding='same', # How to apply the weights to the images \</p><p class="source-code">       )(input_layer)</p></li>
				<li>Add a max pooling layer to the encoder with a 2 x 2 kernel:<p class="source-code">encoded = MaxPooling2D((2, 2))(hidden_encoding)</p></li>
				<li>Add a decoding convolutional layer:<p class="source-code">hidden_decoding = \</p><p class="source-code">Conv2D(16, # Number of layers or filters in the weight matrix \</p><p class="source-code">       (3, 3), # Shape of the weight matrix \</p><p class="source-code">       activation='relu', \</p><p class="source-code">       padding='same', # How to apply the weights to the images \</p><p class="source-code">       )(encoded)</p></li>
				<li>Add an upsampling layer:<p class="source-code">upsample_decoding = UpSampling2D((2, 2))(hidden_decoding)</p></li>
				<li>Add the final convolutional stage, using one layer as per the initial image depth:<p class="source-code">decoded = \</p><p class="source-code">Conv2D(1, # Number of layers or filters in the weight matrix \</p><p class="source-code">       (3, 3), # Shape of the weight matrix \</p><p class="source-code">       activation='sigmoid', \</p><p class="source-code">       padding='same', # How to apply the weights to the images \</p><p class="source-code">       )(upsample_decoding)</p></li>
				<li>Construct the model by passing the first and last layers of the network to the <strong class="source-inline">Model</strong> class:<p class="source-code">autoencoder = Model(input_layer, decoded)</p></li>
				<li>Display the structure of the model:<p class="source-code">autoencoder.summary()</p><p>The output is as follows:</p><div id="_idContainer381" class="IMG---Figure"><img src="image/B15923_05_44.jpg" alt="Figure 5.44: Structure of the model&#13;&#10;"/></div><p class="figure-caption">Figure 5.44: Structure of the model</p></li>
				<li>Compile the autoencoder using a binary cross-entropy loss function and <strong class="source-inline">adadelta</strong> gradient descent:<p class="source-code">autoencoder.compile(loss='binary_crossentropy', \</p><p class="source-code">                    optimizer='adadelta')</p></li>
				<li>Now, let's fit the model; again, we pass the images as the training data and as the desired output.  Train for 20 epochs as convolutional networks take a lot longer to compute:<p class="source-code">autoencoder.fit(images, images, epochs=20)</p><p>The output is as follows:</p><div id="_idContainer382" class="IMG---Figure"><img src="image/B15923_05_45.jpg" alt="Figure 5.45: Training the model&#13;&#10;"/></div><p class="figure-caption">Figure 5.45: Training the model</p></li>
				<li>Calculate and store the output of the encoding stage for the first five samples:<p class="source-code">encoder_output = Model(input_layer, encoded).predict(images[:5])</p></li>
				<li>Reshape the encoder output for visualization, where each image is X*Y in size:<p class="source-code">encoder_output = encoder_output.reshape((-1, 14 * 14, 16))</p></li>
				<li>Get the output of the decoder for the first five images:<p class="source-code">decoder_output = autoencoder.predict(images[:5])</p></li>
				<li>Reshape the decoder output to 28 x 28 in size:<p class="source-code">decoder_output = decoder_output.reshape((-1, 28, 28))</p></li>
				<li>Reshape the original images back to 28 x 28 in size:<p class="source-code">images = images.reshape((-1, 28, 28))</p></li>
				<li>Plot the original image, the mean encoder output, and the decoder:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">for i in range(5):</p><p class="source-code">    # Plot the original digit images</p><p class="source-code">    plt.subplot(3, 5, i + 1)</p><p class="source-code">    plt.imshow(images[i], cmap='gray')</p><p class="source-code">    plt.axis('off')</p><p class="source-code">    # Plot the encoder output</p><p class="source-code">    plt.subplot(3, 5, i + 6)</p><p class="source-code">    plt.imshow(encoder_output[i], cmap='gray')</p><p class="source-code">    plt.axis('off')</p><p class="source-code">    # Plot the decoder output</p><p class="source-code">    plt.subplot(3, 5, i + 11)</p><p class="source-code">    plt.imshow(decoder_output[i], cmap='gray')</p><p class="source-code">    plt.axis('off')</p><p>The output is as follows:</p><div id="_idContainer383" class="IMG---Figure"><img src="image/B15923_05_46.jpg" alt="Figure 5.46: The original image, the encoder output, and the decoder&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.46: The original image, the encoder output, and the decoder</p>
			<p>At the end of this activity, you will have developed an autoencoder comprising convolutional layers within the neural network. Note the improvements made in the decoder representations. This architecture has a significant performance benefit over fully connected neural network layers and is extremely useful in working with image-based datasets and generating artificial data samples.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2CdpIxY">https://packt.live/2CdpIxY</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3iKz8l2">https://packt.live/3iKz8l2</a>.</p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor232"/>6. t-Distributed Stochastic Neighbor Embedding</h1>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor233"/>Activity 6.01: Wine t-SNE</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as the <strong class="source-inline">t-SNE</strong> and <strong class="source-inline">PCA</strong> models from scikit-learn:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from sklearn.manifold import TSNE</p></li>
				<li>Load the Wine dataset using the <strong class="source-inline">wine.data</strong> file included in the accompanying source code and display the first five rows of data:<p class="source-code">df = pd.read_csv('wine.data', header=None)</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer384" class="IMG---Figure"><img src="image/B15923_06_25.jpg" alt="Figure 6.25: The first five rows of the Wine dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.25: The first five rows of the Wine dataset</p></li>
				<li>The first column contains the labels; extract this column and remove it from the dataset:<p class="source-code">labels = df[0]</p><p class="source-code">del df[0]</p></li>
				<li>Execute PCA to reduce the dataset to the first six components:<p class="source-code">model_pca = PCA(n_components=6)</p><p class="source-code">wine_pca = model_pca.fit_transform(df)</p></li>
				<li>Determine the amount of variance within the data described by these six components:<p class="source-code">np.sum(model_pca.explained_variance_ratio_)</p><p>The output is as follows:</p><p class="source-code">0.99999314824536</p></li>
				<li>Create a t-SNE model using a specified random state and a <strong class="source-inline">verbose</strong> value of 1:<p class="source-code">tsne_model = TSNE(random_state=0, verbose=1)</p><p class="source-code">tsne_model</p><p>The output is as follows:</p><div id="_idContainer385" class="IMG---Figure"><img src="image/B15923_06_26.jpg" alt="Figure 6.26: Creating a t-SNE model&#13;&#10;"/></div><p class="figure-caption">Figure 6.26: Creating a t-SNE model</p></li>
				<li>Fit the PCA data to the t-SNE model:<p class="source-code">wine_tsne = tsne_model.fit_transform\</p><p class="source-code">            (wine_pca.reshape((len(wine_pca), -1)))</p><p>The output is as follows:</p><div id="_idContainer386" class="IMG---Figure"><img src="image/B15923_06_27.jpg" alt="Figure 6.27: Fitting the PCA data to the t-SNE model&#13;&#10;"/></div><p class="figure-caption">Figure 6.27: Fitting the PCA data to the t-SNE model</p></li>
				<li>Confirm that the shape of the t-SNE fitted data is two-dimensional:<p class="source-code">wine_tsne.shape</p><p>The output is as follows:</p><p class="source-code">(178, 2)</p></li>
				<li>Create a scatter plot of the two-dimensional data:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.scatter(wine_tsne[:,0], wine_tsne[:,1])</p><p class="source-code">plt.title('Low Dimensional Representation of Wine')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer387" class="IMG---Figure"><img src="image/B15923_06_28.jpg" alt="Figure 6.28: Scatter plot of the two-dimensional data&#13;&#10;"/></div><p class="figure-caption">Figure 6.28: Scatter plot of the two-dimensional data</p></li>
				<li>Create a secondary scatter plot of the two-dimensional data with the class labels applied to visualize any clustering that may be present:<p class="source-code">MARKER = ['o', 'v', '^',]</p><p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.title('Low Dimensional Representation of Wine')</p><p class="source-code">for i in range(1, 4):</p><p class="source-code">    selections = wine_tsne[labels == i]</p><p class="source-code">    plt.scatter(selections[:,0], selections[:,1], \</p><p class="source-code">                marker=MARKER[i-1], label=f'Wine {i}', s=30)</p><p class="source-code">    plt.legend()</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer388" class="IMG---Figure"><img src="image/B15923_06_15.jpg" alt="Figure 6.29: Secondary plot of the two-dimensional data&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.29: Secondary plot of the two-dimensional data</p>
			<p>Note that while there is an overlap between the wine classes, it can also be seen that there is some clustering within the data. The first wine class is predominantly positioned in the top left-hand corner of the plot, the second wine class in the bottom-right, and the third wine class is between the first two. This representation certainly couldn't be used to classify individual wine samples with great confidence, but it shows an overall trend and the series of clusters contained within the high-dimensional data that we were unable to see earlier.</p>
			<p>In this section, we covered the basics of generating SNE plots. The ability to represent high-dimensional data in a low-dimensional space is critical, especially for developing a thorough understanding of the data at hand. Occasionally, these plots can be tricky to interpret as the exact relationships are sometimes contradictory, sometimes leading to misleading structures.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZSVKrf">https://packt.live/2ZSVKrf</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2CgAWBE">https://packt.live/2CgAWBE</a>.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor234"/>Activity 6.02: t-SNE Wine and Perplexity</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as the <strong class="source-inline">t-SNE</strong> and <strong class="source-inline">PCA</strong> models from scikit-learn:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from sklearn.manifold import TSNE</p></li>
				<li>Load the Wine dataset and inspect the first five rows:<p class="source-code">df = pd.read_csv('wine.data', header=None)</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer389" class="IMG---Figure"><img src="image/B15923_06_30.jpg" alt="Figure 6.30: The first five rows of the Wine dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.30: The first five rows of the Wine dataset</p></li>
				<li>The first column provides the labels; extract them from the DataFrame and store them in a separate variable. Ensure that the column is removed from the DataFrame:<p class="source-code">labels = df[0]</p><p class="source-code">del df[0]</p></li>
				<li>Execute PCA on the dataset and extract the first six components:<p class="source-code">model_pca = PCA(n_components=6)</p><p class="source-code">wine_pca = model_pca.fit_transform(df)</p><p class="source-code">wine_pca = wine_pca.reshape((len(wine_pca), -1))</p></li>
				<li>Construct a loop that iterates through the perplexity values (1, 5, 20, 30, 80, 160, 320). For each loop, generate a t-SNE model with the corresponding perplexity and print a scatter plot of the labeled wine classes. Note the effect of different perplexity values:<p class="source-code">MARKER = ['o', 'v', '^',]</p><p class="source-code">for perp in [1, 5, 20, 30, 80, 160, 320]:</p><p class="source-code">    tsne_model = TSNE(random_state=0, verbose=1, perplexity=perp)</p><p class="source-code">    wine_tsne = tsne_model.fit_transform(wine_pca)</p><p class="source-code">    plt.figure(figsize=(10, 7))</p><p class="source-code">    plt.title(f'Low Dimensional Representation of Wine. \</p><p class="source-code">              Perplexity {perp}');</p><p class="source-code">    for i in range(1, 4):</p><p class="source-code">        selections = wine_tsne[labels == i]</p><p class="source-code">        plt.scatter(selections[:,0], selections[:,1], \</p><p class="source-code">                    marker=MARKER[i-1], label=f'Wine {i}', s=30)</p><p class="source-code">        plt.legend()</p><p class="source-code">plt.show()</p><p>A perplexity value of 1 fails to separate the data into any particular structure:</p><div id="_idContainer390" class="IMG---Figure"><img src="image/B15923_06_31.jpg" alt="Figure 6.31: Plot for perplexity of 1&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.31: Plot for perplexity of 1</p>
			<p>Increasing the perplexity to 5 leads to a very non-linear structure that is difficult to separate, and it's hard to identify any clusters or patterns:</p>
			<div>
				<div id="_idContainer391" class="IMG---Figure">
					<img src="image/B15923_06_32.jpg" alt="Figure 6.32: Plot for perplexity of 5&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.32: Plot for perplexity of 5</p>
			<p>A perplexity of 20 finally starts to show some sort of horse-shoe structure. While visually obvious, this can still be tricky to implement:</p>
			<div>
				<div id="_idContainer392" class="IMG---Figure">
					<img src="image/B15923_06_33.jpg" alt="Figure 6.33: Plot for perplexity of 20&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.33: Plot for perplexity of 20</p>
			<p>A perplexity value of 30 demonstrates quite good results. There is a linear relationship between the projected structure with some separation between the types of wine:</p>
			<div>
				<div id="_idContainer393" class="IMG---Figure">
					<img src="image/B15923_06_34.jpg" alt="Figure 6.34: Plot for perplexity of 30&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.34: Plot for perplexity of 30</p>
			<p>Finally, the last two images in the activity show the extent to which the plots can become increasingly complex and non-linear with increasing perplexity:</p>
			<div>
				<div id="_idContainer394" class="IMG---Figure">
					<img src="image/B15923_06_35.jpg" alt="Figure 6.35: Plot for perplexity of 80&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.35: Plot for perplexity of 80</p>
			<p>Here's the plot for a perplexity value of 160:</p>
			<div>
				<div id="_idContainer395" class="IMG---Figure">
					<img src="image/B15923_06_36.jpg" alt="Figure 6.36: Plot for perplexity of 160&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.36: Plot for perplexity of 160</p>
			<p>Finally, here's the plot for a perplexity value of 320:</p>
			<div>
				<div id="_idContainer396" class="IMG---Figure">
					<img src="image/B15923_06_37.jpg" alt="Figure 6.37: Plot for perplexity of 320&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.37: Plot for perplexity of 320</p>
			<p>By looking at the individual plots for each of the perplexity values, the effect perplexity has on the visualization of data is immediately obvious. Very small or very large perplexity values produce a range of unusual shapes that don't indicate the presence of any persistent pattern. The most plausible value seems to be 30 (<em class="italic">Figure 6.35</em>), which produced the most linear plot.</p>
			<p>In this activity, we demonstrated the need to be careful when selecting the perplexity and that some iteration may be required to determine the correct value.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3faqESn">https://packt.live/3faqESn</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2AF12Oi">https://packt.live/2AF12Oi</a>.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor235"/>Activity 6.03: t-SNE Wine and Iterations</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong>, as well as the <strong class="source-inline">t-SNE</strong> and <strong class="source-inline">PCA</strong> models from scikit-learn:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from sklearn.manifold import TSNE</p></li>
				<li>Load the Wine dataset and inspect the first five rows:<p class="source-code">df = pd.read_csv('wine.data', header=None)</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer397" class="IMG---Figure"><img src="image/B15923_06_38.jpg" alt="Figure 6.38: The first five rows of the Wine dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.38: The first five rows of the Wine dataset</p></li>
				<li>The first column provides the labels; extract these from the DataFrame and store them in a separate variable. Ensure that the column is removed from the DataFrame:<p class="source-code">labels = df[0]</p><p class="source-code">del df[0]</p></li>
				<li>Execute PCA on the dataset and extract the first six components:<p class="source-code">model_pca = PCA(n_components=6)</p><p class="source-code">wine_pca = model_pca.fit_transform(df)</p><p class="source-code">wine_pca = wine_pca.reshape((len(wine_pca), -1))</p></li>
				<li>Construct a loop that iterates through the iteration values (<strong class="source-inline">250</strong>, <strong class="source-inline">500</strong>, <strong class="source-inline">1000</strong>). For each loop, generate a t-SNE model with the corresponding number of iterations and identical number of iterations without progress values:<p class="source-code">MARKER = ['o', 'v', '1', 'p' ,'*', '+', 'x', 'd', '4', '.']</p><p class="source-code">for iterations in [250, 500, 1000]:</p><p class="source-code">    model_tsne = TSNE(random_state=0, verbose=1, \</p><p class="source-code">                      n_iter=iterations, \</p><p class="source-code">                      n_iter_without_progress=iterations)</p><p class="source-code">    wine_tsne = model_tsne.fit_transform(wine_pca)</p></li>
				<li>Construct a scatter plot of the labeled wi<a id="_idTextAnchor236"/>ne classes. Note the effect of different iteration values:<p class="source-code">    plt.figure(figsize=(10, 7))</p><p class="source-code">    plt.title(f'Low Dimensional Representation of Wine \</p><p class="source-code">(iterations = {iterations})')</p><p class="source-code">    for i in range(10):</p><p class="source-code">        selections = wine_tsne[labels == i]</p><p class="source-code">        plt.scatter(selections[:,0], selections[:,1], \</p><p class="source-code">                    alpha=0.7, marker=MARKER[i], s=10);</p><p class="source-code">        x, y = selections.mean(axis=0)</p><p class="source-code">        plt.text(x, y, str(i), \</p><p class="source-code">                 fontdict={'weight': 'bold', 'size': 30})</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer398" class="IMG---Figure"><img src="image/B15923_06_39.jpg" alt="Figure 6.39: Scatter plot of wine classes with 250 iterations&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.39: Scatter plot of wine classes with 250 iterations</p>
			<p>Here's the plot for 500 iterations:</p>
			<div>
				<div id="_idContainer399" class="IMG---Figure">
					<img src="image/B15923_06_40.jpg" alt="Figure 6.40: Scatter plot of wine classes with 500 iterations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.40: Scatter plot of wine classes with 500 iterations</p>
			<p>Here's the plot for 1,000 iterations:</p>
			<div>
				<div id="_idContainer400" class="IMG---Figure">
					<img src="image/B15923_06_41.jpg" alt="Figure 6.41: Scatter plot of wine classes with 1,000 iterations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.41: Scatter plot of wine classes with 1,000 iterations</p>
			<p>Again, we can see an improvement in the structure of the data as the number of iterations increases. Even in a relatively simple dataset such as this, 250 iterations is not sufficient to project any structure of data into the lower-dimensional space.</p>
			<p>As we observed in this activity, there is a balance to find in setting the iteration parameter. In this example, 250 iterations were insufficient, and at least 1,000 iterations were required to stabilize the data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZOJuYv">https://packt.live/2ZOJuYv</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Z8wEoP">https://packt.live/2Z8wEoP</a>.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor237"/>7. Topic Modeling</h1>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor238"/>Activity 7.01: Loading and Cleaning Twitter Data</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the necessary libraries:<p class="source-code">import warnings</p><p class="source-code">warnings.filterwarnings('ignore')</p><p class="source-code">import langdetect </p><p class="source-code">import matplotlib.pyplot </p><p class="source-code">import nltk</p><p class="source-code">nltk.download('wordnet')</p><p class="source-code">nltk.download('stopwords')</p><p class="source-code">import numpy </p><p class="source-code">import pandas </p><p class="source-code">import pyLDAvis </p><p class="source-code">import pyLDAvis.sklearn </p><p class="source-code">import regex </p><p class="source-code">import sklearn </p></li>
				<li>Load the LA Times health Twitter data (<strong class="source-inline">latimeshealth.txt</strong>) from <a href="https://packt.live/2Xje5xF">https://packt.live/2Xje5xF</a>.<p class="callout-heading">Note </p><p class="callout">Pay close attention to the delimiter (it is neither a comma nor a tab) and double-check the header status.</p><p>The code looks as follows:</p><p class="source-code">path = 'latimeshealth.txt' </p><p class="source-code">df = pandas.read_csv(path, sep="|", header=None)</p><p class="source-code">df.columns = ["id", "datetime", "tweettext"]</p></li>
				<li>Run a quick exploratory analysis to ascertain the data size and structure:<p class="source-code">def dataframe_quick_look(df, nrows):</p><p class="source-code">    print("SHAPE:\n{shape}\n".format(shape=df.shape))</p><p class="source-code">    print("COLUMN NAMES:\n{names}\n".format(names=df.columns))</p><p class="source-code">    print("HEAD:\n{head}\n".format(head=df.head(nrows)))</p><p class="source-code">dataframe_quick_look(df, nrows=2)</p><p>The output is as follows:</p><div id="_idContainer401" class="IMG---Figure"><img src="image/B15923_07_49.jpg" alt="Figure 7.49: Shape, column names, and head of data&#13;&#10;"/></div><p class="figure-caption">Figure 7.49: Shape, column names, and head of data</p></li>
				<li>Extract the tweet text and convert it to a list object: <p class="source-code">raw = df['tweettext'].tolist() </p><p class="source-code">print("HEADLINES:\n{lines}\n".format(lines=raw[:5])) </p><p class="source-code">print("LENGTH:\n{length}\n".format(length=len(raw))) </p><p>The output is as follows:</p><div id="_idContainer402" class="IMG---Figure"><img src="image/B15923_07_50.jpg" alt="Figure 7.50: Headlines and their length&#13;&#10;"/></div><p class="figure-caption">Figure 7.50: Headlines and their length</p></li>
				<li>Write a function to perform language detection and tokenization on white spaces, and then replace the screen names and URLs with <strong class="source-inline">SCREENNAME</strong> and <strong class="source-inline">URL</strong>, respectively. The function should also remove punctuation, numbers, and the <strong class="source-inline">SCREENNAME</strong> and <strong class="source-inline">URL</strong> replacements. Convert everything to lowercase, except <strong class="source-inline">SCREENNAME</strong> and <strong class="source-inline">URL</strong>. It should remove all stop words, perform lemmatization, and keep words with five or more letters only.<p class="callout-heading">Note </p><p class="callout">Screen names start with the <strong class="source-inline">@</strong> symbol.</p><p>The code is as follows:</p><p class="source-code-heading">Activity7.01-Activity7.03.ipynb</p><p class="source-code">def do_language_identifying(txt): </p><p class="source-code">    try: </p><p class="source-code">        the_language = langdetect.detect(txt) </p><p class="source-code">    except: </p><p class="source-code">        the_language = 'none' </p><p class="source-code">    return the_language </p><p class="source-code">def do_lemmatizing(wrd): </p><p class="source-code">    out = nltk.corpus.wordnet.morphy(wrd)</p><p class="source-code">    return (wrd if out is None else out)</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/3e3VifV">https://packt.live/3e3VifV</a>.</p></li>
				<li>Apply the function defined in <em class="italic">Step 5</em> to every tweet:<p class="source-code">clean = list(map(do_tweet_cleaning, raw)) </p></li>
				<li>Remove elements of the output list equal to <strong class="source-inline">None</strong>:<p class="source-code">clean = list(filter(None.__ne__, clean)) </p><p class="source-code">print("HEADLINES:\n{lines}\n".format(lines=clean[:5]))</p><p class="source-code">print("LENGTH:\n{length}\n".format(length=len(clean)))</p><p>The output is as follows:</p><div id="_idContainer403" class="IMG---Figure"><img src="image/B15923_07_51.jpg" alt="Figure 7.51: Headlines and length after removing None&#13;&#10;"/></div><p class="figure-caption">Figure 7.51: Headlines and length after removing None</p></li>
				<li>Turn the elements of each tweet back into a string. Concatenate using white space:<p class="source-code">clean_sentences = [" ".join(i) for i in clean]</p><p class="source-code">print(clean_sentences[0:10])</p><p>The first 10 elements of the output list should resemble the following:</p><div id="_idContainer404" class="IMG---Figure"><img src="image/B15923_07_22.jpg" alt="Figure 7.52: Tweets cleaned for modeling&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.52: Tweets cleaned for modeling</p>
			<p>Keep the notebook open for future activities. By completing this activity, you should now be fairly comfortable working with textual data and preparing it for topic modeling. An important callout would be to recognize the subtle differences in data cleaning needs between the exercises and the activity. Modeling is not a one-size-fits-all process, which is obvious if you spend sufficient time exploring the data before starting any modeling work.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3e3VifV">https://packt.live/3e3VifV</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fegXlU">https://packt.live/3fegXlU</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor239"/>Activity 7.02: LDA and Health Tweets</h2>
			<p>Solution: </p>
			<ol>
				<li value="1">Specify the <strong class="source-inline">number_words</strong>, <strong class="source-inline">number_docs</strong>, and <strong class="source-inline">number_features</strong> variables: <p class="source-code">number_words = 10</p><p class="source-code">number_docs = 10</p><p class="source-code">number_features = 1000</p></li>
				<li>Create a bag-of-words model and assign the feature names to another variable for use later on: <p class="source-code">vectorizer1 = sklearn.feature_extraction.text\</p><p class="source-code">              .CountVectorizer(analyzer="word", \</p><p class="source-code">                               max_df=0.95, \</p><p class="source-code">                               min_df=10, \</p><p class="source-code">                               max_features=number_features)</p><p class="source-code">clean_vec1 = vectorizer1.fit_transform(clean_sentences)</p><p class="source-code">print(clean_vec1[0]) </p><p class="source-code">feature_names_vec1 = vectorizer1.get_feature_names()</p><p>The output is as follows: </p><p class="source-code">(0, 320)    1 </p></li>
				<li>Identify the optimal number of topics:<p class="source-code-heading">Activity7.01-Activity7.03.ipynb</p><p class="source-code">def perplexity_by_ntopic(data, ntopics): </p><p class="source-code">    output_dict = {"Number Of Topics": [], \</p><p class="source-code">                   "Perplexity Score": []}</p><p class="source-code">    for t in ntopics: </p><p class="source-code">        lda = sklearn.decomposition\</p><p class="source-code">              .LatentDirichletAllocation(n_components=t, \</p><p class="source-code">                                         learning_method="online", \</p><p class="source-code">                                         random_state=0)</p><p class="source-code">        lda.fit(data)</p><p class="source-code">        output_dict["Number Of Topics"].append(t) </p><p class="source-code">        output_dict["Perplexity Score"]\</p><p class="source-code">        .append(lda.perplexity(data))</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/3e3VifV">https://packt.live/3e3VifV</a>.</p><p>The output is as follows:</p><div id="_idContainer405" class="IMG---Figure"><img src="image/B15923_07_53.jpg" alt="Figure 7.53: Number of topics versus the perplexity score DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 7.53: Number of topics versus the perplexity score DataFrame</p></li>
				<li>Fit the LDA model using the optimal number of topics: <p class="source-code">lda = sklearn.decomposition.LatentDirichletAllocation\</p><p class="source-code">      (n_components=optimal_num_topics, \</p><p class="source-code">       learning_method="online", \</p><p class="source-code">       random_state=0)</p><p class="source-code">lda.fit(clean_vec1) </p><p>The output is as follows: </p><div id="_idContainer406" class="IMG---Figure"><img src="image/B15923_07_54.jpg" alt="Figure 7.54: The LDA model&#13;&#10;"/></div><p class="figure-caption">Figure 7.54: The LDA model</p></li>
				<li>Create and print the word-topic table:<p class="source-code-heading">Activity7.01-Activity7.03.ipynb</p><p class="source-code">def get_topics(mod, vec, names, docs, ndocs, nwords):</p><p class="source-code">    # word to topic matrix </p><p class="source-code">    W = mod.components_ </p><p class="source-code">    W_norm = W / W.sum(axis=1)[:, numpy.newaxis] </p><p class="source-code">    # topic to document matrix </p><p class="source-code">    H = mod.transform(vec) </p><p class="source-code">    W_dict = {} </p><p class="source-code">    H_dict = {} </p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/3e3VifV">https://packt.live/3e3VifV</a>.</p><p>The output is as follows: </p><div id="_idContainer407" class="IMG---Figure"><img src="image/B15923_07_55.jpg" alt="Figure 7.55: Word-topic table for the health tweet data&#13;&#10;"/></div><p class="figure-caption">Figure 7.55: Word-topic table for the health tweet data</p><p class="callout-heading">Note</p><p class="callout">The results can differ slightly from what is shown because of the optimization algorithms that support both LDA and NMF. Many of the functions do not have a seed setting capability.</p></li>
				<li>Print the document-topic table:<p class="source-code">print(H_df)</p><p>The output is as follows: </p><div id="_idContainer408" class="IMG---Figure"><img src="image/B15923_07_56.jpg" alt="Figure 7.56: Document topic table&#13;&#10;"/></div><p class="figure-caption">Figure 7.56: Document topic table</p></li>
				<li>Create a biplot visualization: <p class="source-code">lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, \</p><p class="source-code">                                    vectorizer1, R=10)</p><p class="source-code">pyLDAvis.display(lda_plot)</p><p>The output is as follows:</p><div id="_idContainer409" class="IMG---Figure"><img src="image/B15923_07_39.jpg" alt="Figure 7.57: A histogram and biplot for the LDA model trained on health tweets &#13;&#10;"/></div><p class="figure-caption">Figure 7.57: A histogram and biplot for the LDA model trained on health tweets </p></li>
				<li>Keep the notebook open for future modeling. </li>
			</ol>
			<p>Before discussing the next topic modeling methodology, non-negative matrix factorization, let's work through another bag-of-words modeling approach. You'll recall that the <strong class="source-inline">CountVectorizer</strong> algorithm returned the simple count of the number of times each word appeared in each document. In this new approach, called TF-IDF or Term Frequency – Inverse Document Frequency, weights representing the importance of each word to each document are returned instead of the raw counts. </p>
			<p>Both the <strong class="source-inline">CountVectorizer</strong> and <strong class="source-inline">TfidfVectorizer</strong> approaches are equally valid. When and how each is used depends on the corpus, the topic modeling methodology being used, and the amount of noise in the documents. For this next exercise, we will use <strong class="source-inline">TfidfVectorizer</strong> and use the output to build the NMF models appearing later on in the chapter.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3e3VifV">https://packt.live/3e3VifV</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fegXlU">https://packt.live/3fegXlU</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor240"/>Activity 7.03: Non-negative Matrix Factorization</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Create the appropriate bag-of-words model and output the feature names as another variable:<p class="source-code">vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer\</p><p class="source-code">              (analyzer="word", \</p><p class="source-code">               max_df=0.5,\</p><p class="source-code">               min_df=20,\</p><p class="source-code">               max_features=number_features,\</p><p class="source-code">               smooth_idf=False)</p><p class="source-code">clean_vec2 = vectorizer2.fit_transform(clean_sentences)</p><p class="source-code">print(clean_vec2[0]) </p><p class="source-code">feature_names_vec2 = vectorizer2.get_feature_names() </p></li>
				<li>Define and fit the NMF algorithm using the number of topics (<strong class="source-inline">n_components</strong>) value from <em class="italic">Activity 7.02</em>, <em class="italic">LDA and Health Tweets</em>:<p class="source-code">nmf = sklearn.decomposition.NMF(n_components=optimal_num_topics, \</p><p class="source-code">                                init="nndsvda", \</p><p class="source-code">                                solver="mu", \</p><p class="source-code">                                beta_loss="frobenius", \</p><p class="source-code">                                random_state=0, \</p><p class="source-code">                                alpha=0.1, \</p><p class="source-code">                                l1_ratio=0.5)</p><p class="source-code">nmf.fit(clean_vec2) </p><p>The output is as follows:</p><div id="_idContainer410" class="IMG---Figure"><img src="image/B15923_07_58.jpg" alt="Figure 7.58: Defining the NMF model&#13;&#10;"/></div><p class="figure-caption">Figure 7.58: Defining the NMF model</p></li>
				<li>Get the topic-document and word-topic tables. Take a few minutes to explore the word groupings and try to define the abstract topics:<p class="source-code">W_df, H_df = get_topics(mod=nmf, vec=clean_vec2, \</p><p class="source-code">                        names=feature_names_vec2, \</p><p class="source-code">                        docs=raw, \</p><p class="source-code">                        ndocs=number_docs, \</p><p class="source-code">                        nwords=number_words)</p><p class="source-code">print(W_df)</p><p>The output is as follows:</p><div id="_idContainer411" class="IMG---Figure"><img src="image/B15923_07_59.jpg" alt="Figure 7.59: Word-topic table&#13;&#10;"/></div><p class="figure-caption">Figure 7.59: Word-topic table</p><p>Print the document-topic table as follows:</p><p class="source-code">print(H_df)</p><p>The output is as follows:</p><div id="_idContainer412" class="IMG---Figure"><img src="image/B15923_07_60.jpg" alt="Figure 7.60: The topic-document tables with probabilities&#13;&#10;"/></div><p class="figure-caption">Figure 7.60: The topic-document tables with probabilities</p></li>
				<li>Adjust the model parameters and rerun <em class="italic">Step 3</em> and <em class="italic">Step 4</em>.</li>
			</ol>
			<p>In this activity, we worked through an example topic modeling scenario using the TF-IDF bag-of-words model and non-negative matrix factorization. What is really important in these examples is trying to understand what the algorithms are doing—not just how to fit them—and to comprehend the results. Working with text data is often complex and it is crucial to realize that not every algorithm will return meaningful results every time. Sometimes, the results are just not useful. That is not a reflection on the algorithm or the practitioner; it is simply an example of the challenge of extracting insights from data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3e3VifV">https://packt.live/3e3VifV</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fegXlU">https://packt.live/3fegXlU</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor241"/>8. Market Basket Analysis</h1>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor242"/>Activity 8.01: Loading and Preparing Full Online Retail Data</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import mlxtend.frequent_patterns</p><p class="source-code">import mlxtend.preprocessing</p><p class="source-code">import numpy</p><p class="source-code">import pandas</p></li>
				<li>Load the online retail dataset file:<p class="source-code">online = pandas.read_excel(io="./Online Retail.xlsx", \</p><p class="source-code">                           sheet_name="Online Retail", \</p><p class="source-code">                           header=0)</p></li>
				<li>Clean and prep the data for modeling, including turning the cleaned data into a list of lists:<p class="source-code">online['IsCPresent'] = (online['InvoiceNo'].astype(str)\</p><p class="source-code">                        .apply(lambda x: 1 \</p><p class="source-code">                               if x.find('C') != -1 else 0))</p><p class="source-code">online1 = (online.loc[online["Quantity"] &gt; 0]\</p><p class="source-code">                 .loc[online['IsCPresent'] != 1]\</p><p class="source-code">                 .loc[:, ["InvoiceNo", "Description"]].dropna())</p><p class="source-code">invoice_item_list = []</p><p class="source-code">for num in list(set(online1.InvoiceNo.tolist())):</p><p class="source-code">    tmp_df = online1.loc[online1['InvoiceNo'] == num]</p><p class="source-code">    tmp_items = tmp_df.Description.tolist()</p><p class="source-code">    invoice_item_list.append(tmp_items)</p></li>
				<li>Encode the data and recast it as a DataFrame. The data is fairly large, so to ensure that everything executes correctly, use a machine with at least 8 GB of memory:<p class="source-code">online_encoder = mlxtend.preprocessing.TransactionEncoder()</p><p class="source-code">online_encoder_array = \</p><p class="source-code">online_encoder.fit_transform(invoice_item_list)</p><p class="source-code">online_encoder_df = pandas.DataFrame(\</p><p class="source-code">                    online_encoder_array, \</p><p class="source-code">                    columns=online_encoder.columns_)</p><p class="source-code">online_encoder_df.loc[20125:20135, \</p><p class="source-code">                      online_encoder_df.columns.tolist()\</p><p class="source-code">                      [100:110]]</p><p>The output is as follows:</p><div id="_idContainer413" class="IMG---Figure"><img src="image/B15923_08_16.jpg" alt="Figure 8.34: A subset of the cleaned, encoded, and recast DataFrame built from the complete online retail dataset&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.34: A subset of the cleaned, encoded, and recast DataFrame built from the complete online retail dataset</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Wf2Rcz">https://packt.live/2Wf2Rcz</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor243"/>Activity 8.02: Running the Apriori Algorithm on the Complete Online Retail Dataset</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Run the Apriori algorithm on the full data with reasonable parameter settings:<p class="source-code">mod_colnames_minsupport = mlxtend.frequent_patterns\</p><p class="source-code">                          .apriori(online_encoder_df, \</p><p class="source-code">                                   min_support=0.01, \</p><p class="source-code">                                   use_colnames=True)</p><p class="source-code">mod_colnames_minsupport.loc[0:6]</p><p>The output is as follows:</p><div id="_idContainer414" class="IMG---Figure"><img src="image/B15923_08_35.jpg" alt="Figure 8.35: The Apriori algorithm results using the complete online retail dataset&#13;&#10;"/></div><p class="figure-caption">Figure 8.35: The Apriori algorithm results using the complete online retail dataset</p></li>
				<li>Filter the results down to the item set containing <strong class="source-inline">10 COLOUR SPACEBOY PEN</strong>. Compare the support value with that under <em class="italic">Exercise 8.06</em>, <em class="italic">Executing the Apriori Algorithm</em>:<p class="source-code">mod_colnames_minsupport[mod_colnames_minsupport['itemsets'] \</p><p class="source-code">== frozenset({'10 COLOUR SPACEBOY PEN'})]</p><p>The output is as follows:</p><div id="_idContainer415" class="IMG---Figure"><img src="image/B15923_08_36.jpg" alt="Figure 8.36: Result of item set containing 10 COLOUR SPACEBOY PEN&#13;&#10;"/></div><p class="figure-caption">Figure 8.36: Result of item set containing 10 COLOUR SPACEBOY PEN</p><p>The support value does change. When the dataset is expanded to include all transactions, the support for this item set drops from 0.0178 to 0.015793. That is, in the reduced dataset used for the exercises, this item set appears in 1.78% of the transactions, while in the full dataset, it appears in approximately 1.6% of transactions.</p></li>
				<li>Add another column containing the item set length. Then, filter down to those item sets whose length is two and whose support is in the range [<strong class="source-inline">0.02</strong>, <strong class="source-inline">0.021</strong>]. Are the item sets the same as those found in <em class="italic">Exercise 8.06</em>, <em class="italic">Executing the Apriori Algorithm</em>, <em class="italic">Step 6</em>?<p class="source-code">mod_colnames_minsupport['length'] = (mod_colnames_minsupport\</p><p class="source-code">                                     ['itemsets']\</p><p class="source-code">                                     .apply(lambda x: len(x)))</p><p class="source-code">mod_colnames_minsupport[(mod_colnames_minsupport['length'] == 2) \</p><p class="source-code">                        &amp; (mod_colnames_minsupport['support'] \</p><p class="source-code">                           &gt;= 0.02)\</p><p class="source-code">                        &amp;(mod_colnames_minsupport['support'] \</p><p class="source-code">                           &lt; 0.021)]</p><p>The output is as follows:</p><div id="_idContainer416" class="IMG---Figure"><img src="image/B15923_08_37.jpg" alt="Figure 8.37: The results of filtering based on length and support&#13;&#10;"/></div><p class="figure-caption">Figure 8.37: The results of filtering based on length and support</p><p>The results did change. Before even looking at the particular item sets and their support values, we see that this filtered DataFrame has fewer item sets than the DataFrame in the preceding exercise. When we use the full dataset, there are fewer item sets that match the filtering criteria; that is, only 17 item sets contain 2 items and have a support value greater than or equal to <strong class="source-inline">0.02</strong>, and less than <strong class="source-inline">0.021</strong>. In the previous exercise, 32 item sets met these criteria.</p></li>
				<li>Plot the <strong class="source-inline">support</strong> values:<p class="source-code">mod_colnames_minsupport.hist("support", grid=False, bins=30)</p><p class="source-code">plt.xlabel("Support of item")</p><p class="source-code">plt.ylabel("Number of items")</p><p class="source-code">plt.title("Frequency distribution of Support")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer417" class="IMG---Figure"><img src="image/B15923_08_38.jpg" alt="Figure 8.38: The distribution of support values&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.38: The distribution of support values</p>
			<p>This plot shows the distribution of support values for the full transaction dataset. As you might have assumed, the distribution is right-skewed; that is, most of the item sets have lower support values and there is a long tail of support values on the higher end of the spectrum. Given how many unique item sets exist, it is not surprising that no single item set appears in a high percentage of the transactions. With this information, we could tell management that even the most prominent item set only appears in approximately 10% of transactions and that the vast majority of item sets appear in fewer than 2% of transactions. These results may not support changes in store layout, but could very well inform pricing and discounting strategies. We would gain more information on how to build these strategies by formalizing a number of association rules.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Wf2Rcz">https://packt.live/2Wf2Rcz</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor244"/>Activity 8.03: Finding the Association Rules on the Complete Online Retail Dataset</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Fit the association rule model on the full dataset. Use the confidence metric and a minimum threshold of <strong class="source-inline">0.6</strong>:<p class="source-code">rules = mlxtend.frequent_patterns\</p><p class="source-code">        .association_rules(mod_colnames_minsupport, \</p><p class="source-code">                           metric="confidence", \</p><p class="source-code">                           min_threshold=0.6, \</p><p class="source-code">                           support_only=False)</p><p class="source-code">rules.loc[0:6]</p><p>The output is as follows:</p><div id="_idContainer418" class="IMG---Figure"><img src="image/B15923_08_39.jpg" alt="Figure 8.39: The association rules based on the complete online retail dataset&#13;&#10;"/></div><p class="figure-caption">Figure 8.39: The association rules based on the complete online retail dataset</p></li>
				<li>Count the number of association rules. Is the number different from that found in <em class="italic">Exercise 8.07</em>, <em class="italic">Deriving Association Rules</em>, <em class="italic">Step 1</em>?<p class="source-code">print("Number of Associations: {}".format(rules.shape[0]))</p><p>There are <strong class="source-inline">498</strong> association rules. Yes, the count is different.</p></li>
				<li>Plot confidence against support:<p class="source-code">rules.plot.scatter("support", "confidence", \</p><p class="source-code">                   alpha=0.5, marker="*")</p><p class="source-code">plt.xlabel("Support")</p><p class="source-code">plt.ylabel("Confidence")</p><p class="source-code">plt.title("Association Rules")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer419" class="IMG---Figure"><img src="image/B15923_08_40.jpg" alt="Figure 8.40: The plot of confidence against support&#13;&#10;"/></div><p class="figure-caption">Figure 8.40: The plot of confidence against support</p><p>The plot reveals that there are some association rules featuring relatively high support and confidence values for this dataset.</p></li>
				<li>Look at the distributions of lift, leverage, and conviction:<p class="source-code">rules.hist("lift", grid=False, bins=30)</p><p class="source-code">plt.xlabel("Lift of item")</p><p class="source-code">plt.ylabel("Number of items")</p><p class="source-code">plt.title("Frequency distribution of Lift")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer420" class="IMG---Figure"><img src="image/B15923_08_41.jpg" alt="Figure 8.41: The distribution of lift values&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.41: The distribution of lift values</p>
			<p>Plot the leverage as follows:</p>
			<p class="source-code">rules.hist("leverage", grid=False, bins=30)</p>
			<p class="source-code">plt.xlabel("Leverage of item")</p>
			<p class="source-code">plt.ylabel("Number of items")</p>
			<p class="source-code">plt.title("Frequency distribution of Leverage")</p>
			<p class="source-code">plt.show()</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer421" class="IMG---Figure">
					<img src="image/B15923_08_42.jpg" alt="Figure 8.42: The distribution of leverage values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.42: The distribution of leverage values</p>
			<p>Plot the conviction as follows:</p>
			<p class="source-code">plt.hist(rules[numpy.isfinite(rules['conviction'])]\</p>
			<p class="source-code">         .conviction.values, bins = 3)</p>
			<p class="source-code">plt.xlabel("Conviction of item")</p>
			<p class="source-code">plt.ylabel("Number of items")</p>
			<p class="source-code">plt.title("Frequency distribution of Conviction")</p>
			<p class="source-code">plt.show()</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer422" class="IMG---Figure">
					<img src="image/B15923_08_43.jpg" alt="Figure 8.43: The distribution of conviction values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.43: The distribution of conviction values</p>
			<p>Having derived association rules, we can return to management with additional information, the most important of which would be that there are roughly seven item sets that have reasonably high values for both support and confidence. Look at the scatterplot of confidence against support to see the seven item sets that are separated from all the others. These seven item sets also have high lift values, as can be seen in the lift histogram. It seems that we have identified a number of actionable association rules – rules that we can use to drive business decisions.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Wf2Rcz">https://packt.live/2Wf2Rcz</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor245"/>9. Hotspot Analysis</h1>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor246"/>Activity 9.01: Estimating Density in One Dimension</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Open a new notebook and install all the necessary libraries.<p class="source-code">get_ipython().run_line_magic('matplotlib', 'inline')</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy</p><p class="source-code">import pandas</p><p class="source-code">import seaborn</p><p class="source-code">import sklearn.model_selection</p><p class="source-code">import sklearn.neighbors</p><p class="source-code">seaborn.set()</p></li>
				<li>Sample 1,000 data points from the standard normal distribution. Add 3.5 to each of the last 625 values of the sample (that is, the indices between 375 and 1,000). Set a random state of 100 using <strong class="source-inline">numpy.random.RandomState</strong> to guarantee the same sampled values, and then randomly generate the data points using the <strong class="source-inline">rand.randn(1000)</strong> call:<p class="source-code">rand = numpy.random.RandomState(100)</p><p class="source-code">vals = rand.randn(1000)  # standard normal</p><p class="source-code">vals[375:] += 3.5</p></li>
				<li>Plot the 1,000-point sample data as a histogram and add a scatterplot below it:<p class="source-code">fig, ax = plt.subplots(figsize=(14, 10))</p><p class="source-code">ax.hist(vals, bins=50, density=True, label='Sampled Values')</p><p class="source-code">ax.plot(vals, -0.005 - 0.01 * numpy.random.random(len(vals)), \</p><p class="source-code">        '+k', label='Individual Points')</p><p class="source-code">ax.legend(loc='upper right')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer423" class="IMG---Figure"><img src="image/B15923_09_30.jpg" alt="Figure 9.30: A histogram of the random sample with a scatterplot underneath&#13;&#10;"/></div><p class="figure-caption">Figure 9.30: A histogram of the random sample with a scatterplot underneath</p></li>
				<li>Define a grid of bandwidth values. Then, define and fit a grid search cross-validation algorithm:<p class="source-code">bandwidths = 10 ** numpy.linspace(-1, 1, 100)</p><p class="source-code">grid = sklearn.model_selection.GridSearchCV\</p><p class="source-code">       (estimator=sklearn.neighbors.KernelDensity(kernel="gaussian"),</p><p class="source-code">        param_grid={"bandwidth": bandwidths}, cv=10)</p><p class="source-code">grid.fit(vals[:, None])</p><p>The output is as follows:</p><div id="_idContainer424" class="IMG---Figure"><img src="image/B15923_09_31.jpg" alt="Figure 9.31: Output of cross-validation model&#13;&#10;"/></div><p class="figure-caption">Figure 9.31: Output of cross-validation model</p></li>
				<li>Extract the optimal bandwidth value:<p class="source-code">best_bandwidth = grid.best_params_["bandwidth"]</p><p class="source-code">print("Best Bandwidth Value: {}".format(best_bandwidth))</p><p>The optimal bandwidth value is approximately <strong class="source-inline">0.4</strong>.</p></li>
				<li>Replot the histogram from <em class="italic">Step 3</em> and overlay the estimated density:<p class="source-code">fig, ax = plt.subplots(figsize=(14, 10))</p><p class="source-code">ax.hist(vals, bins=50, density=True, alpha=0.75, \</p><p class="source-code">        label='Sampled Values')</p><p class="source-code">x_vec = numpy.linspace(-4, 8, 10000)[:, numpy.newaxis]</p><p class="source-code">log_density = numpy.exp(grid.best_estimator_.score_samples(x_vec))</p><p class="source-code">ax.plot(x_vec[:, 0], log_density, \</p><p class="source-code">        '-', linewidth=4, label='Kernel = Gaussian')</p><p class="source-code">ax.legend(loc='upper right')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer425" class="IMG---Figure"><img src="image/B15923_09_32.jpg" alt="Figure 9.32: A histogram of the random sample with the optimal estimated density overlaid&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.32: A histogram of the random sample with the optimal estimated density overlaid</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2wmh5yj">https://packt.live/2wmh5yj</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2W0EAGK">https://packt.live/2W0EAGK</a>.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor247"/>Activity 9.02: Analyzing Crime in London</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Load the crime data. Use the path where you saved the downloaded directory, create a list of the year-month tags, use the <strong class="source-inline">read_csv</strong> command to load the individual files iteratively, and then concatenate these files together:<p class="source-code"># define the file base path</p><p class="source-code">base_path = "./metro-jul18-dec18/{yr_mon}/{yr_mon}\</p><p class="source-code">-metropolitan-street.csv"</p><p class="source-code">print(base_path)</p><p>The output is as follows:</p><p class="source-code">./metro-jul18-dec18/{yr_mon}/{yr_mon}-metropolitan-street.csv</p></li>
				<li>Define the list of year month combinations as follows:<p class="source-code">yearmon_list = ["2018-0" + str(i) if i &lt;= 9 else "2018-" + str(i) \</p><p class="source-code">                for i in range(7, 13)]</p><p class="source-code">print(yearmon_list)</p><p>The output is as follows:</p><p class="source-code">['2018-07', '2018-08', '2018-09', \</p><p class="source-code"> '2018-10', '2018-11', '2018-12']</p></li>
				<li>Load the data and print some basic information about it as follows:<p class="source-code">data_yearmon_list = []</p><p class="source-code"># read each year month file individually</p><p class="source-code">#print summary statistics</p><p class="source-code">for idx, i in enumerate(yearmon_list):</p><p class="source-code">    df = pandas.read_csv(base_path.format(yr_mon=i), \</p><p class="source-code">                         header=0)</p><p class="source-code">    data_yearmon_list.append(df)</p><p class="source-code">    if idx == 0:</p><p class="source-code">        print("Month: {}".format(i))</p><p class="source-code">        print("Dimensions: {}".format(df.shape))</p><p class="source-code">        print("Head:\n{}\n".format(df.head(2)))</p><p class="source-code"># concatenate the list of year month data frames together</p><p class="source-code">london = pandas.concat(data_yearmon_list)</p><p>The output is as follows:</p><div id="_idContainer426" class="IMG---Figure"><img src="image/B15923_09_33.jpg" alt="Figure 9.33: An example of one of the individual crime files&#13;&#10;"/></div><p class="figure-caption">Figure 9.33: An example of one of the individual crime files</p><p>This printed information is just for the first of the loaded files, which will be the criminal information from the Metropolitan Police Service for July 2018. This one file has nearly 100,000 entries. You will notice that there is a great deal of interesting information in this dataset, but we will focus on <strong class="source-inline">Longitude</strong>, <strong class="source-inline">Latitude</strong>, <strong class="source-inline">Month</strong>, and <strong class="source-inline">Crime type</strong>.</p></li>
				<li>Print diagnostics of the complete and concatenated dataset:<p class="source-code-heading">Activity9.01-Activity9.02.ipynb</p><p class="source-code">print("Dimensions - Full Data:\n{}\n".format(london.shape))</p><p class="source-code">print("Unique Months - Full Data:\n{}\n".format(london["Month"].unique()))</p><p class="source-code">print("Number of Unique Crime Types - Full Data:\n{}\n"\</p><p class="source-code">      .format(london["Crime type"].nunique()))</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/2wmh5yj">https://packt.live/2wmh5yj</a>.</p><p>The output is as follows:</p><div id="_idContainer427" class="IMG---Figure"><img src="image/B15923_09_34.jpg" alt="Figure 9.34: Descriptors of the full crime dataset&#13;&#10;"/></div><p class="figure-caption">Figure 9.34: Descriptors of the full crime dataset</p></li>
				<li>Subset the data frame down to four variables (<strong class="source-inline">Longitude</strong>, <strong class="source-inline">Latitude</strong>, <strong class="source-inline">Month</strong>, and <strong class="source-inline">Crime type</strong>):<p class="source-code">london_subset = london[["Month", "Longitude", "Latitude", \</p><p class="source-code">                        "Crime type"]]</p><p class="source-code">london_subset.head(5)</p><p>The output is as follows:</p><div id="_idContainer428" class="IMG---Figure"><img src="image/B15923_09_35.jpg" alt="Figure 9.35: Crime data in data frame format&#13;&#10;"/></div><p class="figure-caption">Figure 9.35: Crime data in data frame format</p></li>
				<li>Using the <strong class="source-inline">jointplot</strong> function from <strong class="source-inline">seaborn</strong>, fit and visualize three kernel density estimation models for bicycle theft in July, September, and December 2018:<p class="source-code">crime_bicycle_jul = london_subset\</p><p class="source-code">                    [(london_subset["Crime type"] \</p><p class="source-code">                      == "Bicycle theft") \</p><p class="source-code">                     &amp; (london_subset["Month"] == "2018-07")]</p><p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p><p class="source-code">                  crime_bicycle_jul, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer429" class="IMG---Figure"><img src="image/B15923_09_36.jpg" alt="Figure 9.36: The estimated joint and marginal densities for bicycle thefts in July 2018&#13;&#10;"/></div><p class="figure-caption">Figure 9.36: The estimated joint and marginal densities for bicycle thefts in July 2018</p><p>For the month of September 2018, the code to fit and visualize the kernel density estimation model for bicycle theft is as follows:</p><p class="source-code">crime_bicycle_sept = london_subset\</p><p class="source-code">                     [(london_subset["Crime type"] </p><p class="source-code">                       == "Bicycle theft") </p><p class="source-code">                      &amp; (london_subset["Month"] == "2018-09")]</p><p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p><p class="source-code">                  crime_bicycle_sept, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer430" class="IMG---Figure"><img src="image/B15923_09_37.jpg" alt="Figure 9.37: The estimated joint and marginal densities for bicycle thefts in September 2018&#13;&#10;"/></div><p class="figure-caption">Figure 9.37: The estimated joint and marginal densities for bicycle thefts in September 2018</p><p>For the month of December 2018, the code to fit and visualize the kernel density estimation model for bicycle theft is as follows:</p><p class="source-code">crime_bicycle_dec = london_subset\</p><p class="source-code">                    [(london_subset["Crime type"] \</p><p class="source-code">                      == "Bicycle theft") </p><p class="source-code">                     &amp; (london_subset["Month"] == "2018-12")]</p><p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p><p class="source-code">                  crime_bicycle_dec, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer431" class="IMG---Figure"><img src="image/B15923_09_38.jpg" alt="Figure 9.38: The estimated joint and marginal densities for bicycle thefts in December 2018&#13;&#10;"/></div><p class="figure-caption">Figure 9.38: The estimated joint and marginal densities for bicycle thefts in December 2018</p><p>From month to month, the density of bicycle thefts stays quite constant. There are slight differences between the densities, which is to be expected given that the data that is the foundation of these estimated densities is three one-month samples. Given these results, police or criminologists should be confident in predicting where future bicycle thefts are most likely to occur.</p></li>
				<li>Repeat <em class="italic">Step 4</em>; this time, use shoplifting crimes for the months of August, October, and November 2018:<p class="source-code">crime_shoplift_aug = london_subset\</p><p class="source-code">                     [(london_subset["Crime type"] \</p><p class="source-code">                       == "Shoplifting") </p><p class="source-code">                      &amp; (london_subset["Month"] == "2018-08")]</p><p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p><p class="source-code">                  crime_shoplift_aug, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer432" class="IMG---Figure"><img src="image/B15923_09_39.jpg" alt="Figure 9.39: The estimated joint and marginal densities for shoplifting incidents in August 2018&#13;&#10;"/></div><p class="figure-caption">Figure 9.39: The estimated joint and marginal densities for shoplifting incidents in August 2018</p><p>For the month of October 2018, the code to fit and visualize the kernel density estimation model for shoplifting crimes is as follows:</p><p class="source-code">crime_shoplift_oct = london_subset\</p><p class="source-code">                     [(london_subset["Crime type"] \</p><p class="source-code">                       == "Shoplifting") \</p><p class="source-code">                      &amp; (london_subset["Month"] == "2018-10")]</p><p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p><p class="source-code">                  crime_shoplift_oct, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer433" class="IMG---Figure"><img src="image/B15923_09_40.jpg" alt="Figure 9.40: The estimated joint and marginal densities for shoplifting incidents in October 2018&#13;&#10;"/></div><p class="figure-caption">Figure 9.40: The estimated joint and marginal densities for shoplifting incidents in October 2018</p><p>For the month of November 2018, the code to fit and visualize the kernel density estimation model for shoplifting crimes is as follows:</p><p class="source-code">crime_shoplift_nov = london_subset\</p><p class="source-code">                     [(london_subset["Crime type"] \</p><p class="source-code">                       == "Shoplifting") \</p><p class="source-code">                      &amp; (london_subset["Month"] == "2018-11")]</p><p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p><p class="source-code">                  crime_shoplift_nov, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer434" class="IMG---Figure"><img src="image/B15923_09_41.jpg" alt="Figure 9.41: The estimated joint and marginal densities for shoplifting incidents in November 2018&#13;&#10;"/></div><p class="figure-caption">Figure 9.41: The estimated joint and marginal densities for shoplifting incidents in November 2018</p><p>Like the bicycle theft results, the shoplifting densities are quite stable across the months. The density from August 2018 looks different from the other two months; however, if you look at the longitude and latitude values, you will notice that the density is very similar – just shifted and scaled. The reason for this is that there were probably a number of outliers forcing the creation of a much larger plotting region.</p></li>
				<li>Repeat <em class="italic">Step 5</em>; this time use burglary crimes for the months of July, October, and December 2018:<p class="source-code">crime_burglary_jul = london_subset\</p><p class="source-code">                    [(london_subset["Crime type"] == "Burglary") \</p><p class="source-code">                     &amp; (london_subset["Month"] == "2018-07")]</p><p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p><p class="source-code">                  crime_burglary_jul, kind="kde")</p><p>The output is as follows:</p><div id="_idContainer435" class="IMG---Figure"><img src="image/B15923_09_42.jpg" alt="Figure 9.42: The estimated joint and marginal densities for burglaries in July 2018&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.42: The estimated joint and marginal densities for burglaries in July 2018</p>
			<p>For the month of October 2018, the code to fit and visualize the kernel density estimation model for burglaries is as follows:</p>
			<p class="source-code">crime_burglary_oct = london_subset\</p>
			<p class="source-code">                     [(london_subset["Crime type"] == "Burglary")\</p>
			<p class="source-code">                      &amp; (london_subset["Month"] == "2018-10")]</p>
			<p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p>
			<p class="source-code">                  crime_burglary_oct, kind="kde")</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer436" class="IMG---Figure">
					<img src="image/B15923_09_43.jpg" alt="Figure 9.43: The estimated joint and marginal densities for burglaries in October 2018&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.43: The estimated joint and marginal densities for burglaries in October 2018</p>
			<p>For the month of December 2018, the code to fit and visualize the kernel density estimation model for burglaries is as follows:</p>
			<p class="source-code">crime_burglary_dec = london_subset\</p>
			<p class="source-code">                     [(london_subset["Crime type"] == "Burglary")\</p>
			<p class="source-code">                      &amp; (london_subset["Month"] == "2018-12")]</p>
			<p class="source-code">seaborn.jointplot("Longitude", "Latitude", \</p>
			<p class="source-code">                  crime_burglary_dec, kind="kde")</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer437" class="IMG---Figure">
				</div>
			</div>
			<div>
				<div id="_idContainer438" class="IMG---Figure">
					<img src="image/B15923_09_44.jpg" alt="Figure 9.44: The estimated joint and marginal densities for burglaries in December 2018&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.44: The estimated joint and marginal densities for burglaries in December 2018</p>
			<p>Once again, we can see that the distributions are quite similar across the months. The only difference is that the densities seem to widen or spread from July to December. As always, the noise and inherent lack of information contained in the sample data is causing small shifts in the estimated densities.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2wmh5yj">https://packt.live/2wmh5yj</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2W0EAGK">https://packt.live/2W0EAGK</a>.</p>
		</div>
		<div>
			<div id="_idContainer440" class="Content">
			</div>
		</div>
	</body></html>
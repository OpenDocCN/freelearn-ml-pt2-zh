["```py\n    import pandas as pd\n    dataset = pd.read_csv('housing.csv')\n    ```", "```py\n    print(dataset.shape)\n    ```", "```py\n    (506, 13)\n    ```", "```py\n    dataset[:5]\n    ```", "```py\n    dataset = pd.concat([dataset['medv'],\n                         dataset.drop(['medv'], axis=1)], \n                         axis=1)\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    training_dataset, validation_dataset =  \n        train_test_split(dataset, test_size=0.1)\n    ```", "```py\n    training_dataset.to_csv('training_dataset.csv', \n                            index=False, header=False)\n    validation_dataset.to_csv('validation_dataset.csv', \n                              index=False, header=False)\n    ```", "```py\n    import sagemaker\n    sess = sagemaker.Session()\n    bucket = sess.default_bucket()\n    ```", "```py\n    prefix = 'boston-housing'\n    training_data_path = sess.upload_data(\n        path='training_dataset.csv', \n        key_prefix=prefix + '/input/training')\n    validation_data_path = sess.upload_data(\n        path='validation_dataset.csv', \n        key_prefix=prefix + '/input/validation')\n    print(training_data_path)\n    print(validation_data_path)\n    ```", "```py\n    s3://sagemaker-eu-west-1-123456789012/boston-housing/input/training/training_dataset.csv\n    s3://sagemaker-eu-west-1-123456789012/boston-housing/input/validation/validation_dataset.csv\n    ```", "```py\n    from sagemaker import get_execution_role\n    from sagemaker.image_uris import retrieve\n    region = sess.boto_session.region_name\n    container = retrieve('linear-learner', region)\n    ```", "```py\n    from sagemaker.estimator import Estimator\n    ll_estimator = Estimator(\n        container,\n        role=sagemaker.get_execution_role(),\n        instance_count=1,\n        instance_type='ml.m5.large',\n        output_path='s3://{}/{}/output'.format(bucket, \n                                               prefix))\n    ```", "```py\n    ll_estimator.set_hyperparameters(\n        predictor_type='regressor', \n        mini_batch_size=32)\n    ```", "```py\n    from sagemaker import TrainingInput\n    training_data_channel = TrainingInput(\n        s3_data=training_data_path, \n        content_type='text/csv')\n    validation_data_channel = TrainingInput(\n        s3_data=validation_data_path,  \n        content_type='text/csv')\n    ```", "```py\n    ll_estimator.fit(\n        {'train': training_data_channel, \n         'validation': validation_data_channel})\n    ```", "```py\n    Starting - Starting the training job.\n    ```", "```py\n    Starting - Starting the training job...\n    Starting - Launching requested ML instances......\n    Starting - Preparing the instances for training...\n    Downloading - Downloading input data...\n    Training - Training image download completed.\n    ```", "```py\n    #quality_metric: host=algo-1, validation mse <loss>=13.7226685169\n    #quality_metric: host=algo-1, validation absolute_loss <loss>=2.86944983987\n    ```", "```py\n    ml.m5.large instance is $0.128 per hour. As we trained for 49 seconds, this job cost us (49/3600)*0.128= $0.00174 – less than a fifth of a penny. Any time spent setting up infrastructure ourselves would have certainly cost more!\n    ```", "```py\n    model.tar.gz.\n    ```", "```py\n    from time import strftime, gmtime\n    timestamp = strftime('%d-%H-%M-%S', gmtime())\n    endpoint_name = 'linear-learner-demo-'+timestamp\n    print(endpoint_name)\n    ```", "```py\n    ll_predictor = ll_estimator.deploy(\n        endpoint_name=endpoint_name,\n        initial_instance_count=1,\n        instance_type='ml.t2.medium')\n    ```", "```py\n    ll_predictor.serializer =   \n        sagemaker.serializers.CSVSerializer()\n    ll_predictor.deserializer =\n        sagemaker.deserializers.CSVDeserializer()\n    test_sample = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'\n    response = ll_predictor.predict(test_sample)\n    print(response)\n    ```", "```py\n     [['30.17342185974121']]\n    ```", "```py\n    test_samples = [\n    '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98',\n    '0.02731,0.00,7.070,0,0.4690,6.4210,78.90,4.9671,2,242.0,17.80,9.14']\n    response = ll_predictor.predict(test_samples)\n    print(response)\n    ```", "```py\n     [['30.413358688354492'],['24.884408950805664']]\n    ```", "```py\nll_predictor.delete_endpoint()\n```", "```py\n    from sagemaker import image_uris\n    region = sess.boto_session.region_name     \n    container = image_uris.retrieve('xgboost', region, \n                                    version='latest')\n    ```", "```py\n    xgb_estimator = Estimator(\n       container,\n       role=sagemaker.get_execution_role(),\n       instance_count=1,\n       instance_type='ml.m5.large',\n       output_path='s3://{}/{}/output'.format(bucket, \n                                              prefix))\n    ```", "```py\n    xgb_estimator.set_hyperparameters(\n        objective='reg:linear',\n        num_round=200,\n        early_stopping_rounds=10)\n    ```", "```py\n    from sagemaker import TrainingInput\n    training_data_channel = TrainingInput(\n        s3_data=training_data_path, \n        content_type='text/csv')\n    validation_data_channel = TrainingInput(\n        s3_data=validation_data_path,  \n        content_type='text/csv')\n    ```", "```py\n    xgb_estimator.fit(\n        {'train': training_data_channel, \n         'validation': validation_data_channel})\n    ```", "```py\n    [12]#011train-rmse:1.25702#011validation-rmse:2.43126\n    <output removed>\n    [22]#011train-rmse:0.722193#011validation-rmse:2.43355\n    ```", "```py\n    from time import strftime, gmtime\n    timestamp = strftime('%d-%H-%M-%S', gmtime())\n    endpoint_name = 'xgb-demo'+'-'+timestamp\n    xgb_predictor = xgb_estimator.deploy(\n        endpoint_name=endpoint_name,\n        initial_instance_count=1,\n        instance_type='ml.t2.medium')\n    ```", "```py\n    test_sample = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'\n    xgb_predictor.serializer =\n        sagemaker.serializers.CSVSerializer()\n    xgb_predictor.deserializer =\n        sagemaker.deserializers.CSVDeserializer()\n    response = xgb_predictor.predict(test_sample)\n    print(response)\n    ```", "```py\n    [['23.73023223876953']]\n    ```", "```py\n    xgb_predictor.delete_endpoint()\n    ```", "```py\n    %%sh\n    wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n    unzip ml-100k.zip\n    ```", "```py\n    %cd ml-100k\n    !shuf ua.base -o ua.base.shuffled\n    !head -5 ua.base.shuffled\n    ```", "```py\n    378  43  3  880056609\n    919  558 5  875372988\n    90   285 5  891383687\n    249  245 2  879571999\n    416  64  5  893212929\n    ```", "```py\n    num_users = 943\n    num_movies = 1682\n    num_features = num_users+num_movies\n    num_ratings_train = 90570\n    num_ratings_test = 9430\n    ```", "```py\n    import csv\n    import numpy as np\n    from scipy.sparse import lil_matrix\n    def loadDataset(filename, lines, columns):\n        X = lil_matrix((lines, columns)).astype('float32')\n        Y = []\n        line=0\n        with open(filename,'r') as f:\n            samples=csv.reader(f,delimiter='\\t')\n            for userId,movieId,rating,timestamp in samples:\n                X[line,int(userId)-1] = 1\n                X[line,int(num_users)+int(movieId)-1] = 1\n                Y.append(int(rating))\n                line=line+1       \n        Y=np.array(Y).astype('float32')\n        return X,Y\n    ```", "```py\n    X_train, Y_train = loadDataset('ua.base.shuffled', \n                                   num_ratings_train,  \n                                   num_features)\n    X_test, Y_test = loadDataset('ua.test',\n                                 num_ratings_test, \n                                 num_features)\n    ```", "```py\n    print(X_train.shape)\n    print(Y_train.shape)\n    print(X_test.shape)\n    print(Y_test.shape)\n    ```", "```py\n    (90570, 2625)\n    (90570,)\n    (9430, 2625)\n    (9430,)\n    ```", "```py\n    import io, boto3\n    import sagemaker.amazon.common as smac\n    def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n        buf = io.BytesIO()\n        smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n        buf.seek(0)\n        obj = '{}/{}'.format(prefix, key)  \n\n        boto3.resource('s3').Bucket(bucket).Object(obj).\n        upload_fileobj(buf)\n        return 's3://{}/{}'.format(bucket,obj)\n    ```", "```py\n    import sagemaker\n    sess   = sagemaker.Session()\n    bucket = sess.default_bucket()\n    prefix = 'fm-movielens'\n    train_key      = 'train.protobuf'\n    train_prefix   = '{}/{}'.format(prefix, 'train')\n    test_key       = 'test.protobuf'\n    test_prefix    = '{}/{}'.format(prefix, 'test')\n    output_prefix  = 's3://{}/{}/output'.format(bucket, \n                                                prefix)\n    train_data = writeDatasetToProtobuf(X_train, Y_train, \n                 bucket, train_prefix, train_key)    \n    test_data  = writeDatasetToProtobuf(X_test, Y_test, \n                 bucket, test_prefix, test_key)    \n    ```", "```py\n    $ aws s3 ls s3://sagemaker-eu-west-1-123456789012/fm-movielens/train/train.protobuf\n    5796480 train.protobuf\n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    region = sess.boto_session.region_name    \n    container=retrieve('factorization-machines', region)\n    fm=sagemaker.estimator.Estimator(\n        container,\n        role=sagemaker.get_execution_role(),\n        instance_count=1,\n        instance_type='ml.c5.xlarge',\n        output_path=output_prefix)\n    fm.set_hyperparameters(\n        feature_dim=num_features,\n        predictor_type='regressor',\n        num_factors=64,\n        epochs=10)\n    ```", "```py\n    fm.fit({'train': train_data, 'test': test_data})\n    ```", "```py\n    endpoint_name = 'fm-movielens-100k'\n    fm_predictor = fm.deploy(\n        endpoint_name=endpoint_name,\n        instance_type='ml.t2.medium', \n        initial_instance_count=1)\n    ```", "```py\n    import json\n    from sagemaker.deserializers import JSONDeserializer\n    from sagemaker.serializers import JSONSerializer\n    class FMSerializer(JSONSerializer):\n        def serialize(self, data):\n            js = {'instances': []}\n            for row in data:\n                js['instances'].append({'features':   \n                                row.tolist()})\n            return json.dumps(js)\n    fm_predictor.serializer = FMSerializer()\n    fm_predictor.deserializer = JSONDeserializer()\n    ```", "```py\n    result = fm_predictor.predict(X_test[:3].toarray())\n    print(result)\n    ```", "```py\n    {'predictions': [{'score': 3.3772034645080566}, {'score': 3.4299235343933105}, {'score': 3.6053106784820557}]}\n    ```", "```py\n    fm_predictor.delete_endpoint()\n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    region = sess.boto_session.region_name   \n    container = retrieve('pca', region) \n    pca = sagemaker.estimator.Estimator(\n        container=container,\n        role=sagemaker.get_execution_role(),\n        instance_count=1,                               \n        instance_type='ml.c5.xlarge',\n        output_path=output_prefix)\n    ```", "```py\n    pca.set_hyperparameters(feature_dim=num_features,\n                            num_components=64,\n                            mini_batch_size=1024)\n    ```", "```py\n    pca.fit({'train': train_data, 'test': test_data})\n    pca_predictor = pca.deploy(\n        endpoint_name='pca-movielens-100k',\n        instance_type='ml.t2.medium',\n        initial_instance_count=1)\n    ```", "```py\n    import json\n    from sagemaker.deserializers import JSONDeserializer\n    from sagemaker.serializers import JSONSerializer\n    class PCASerializer(JSONSerializer):\n        def serialize(self, data):\n            js = {'instances': []}\n            for row in data:\n                js['instances'].append({'features': \n                                row.tolist()})\n            return json.dumps(js)\n    pca_predictor.serializer = PCASerializer()\n    pca_predictor.deserializer = JSONDeserializer()\n    result = pca_predictor.predict(X_test[0].toarray())\n    print(result)\n    ```", "```py\n    {'projections': [{'projection': [-0.008711372502148151, 0.0019895541481673717, 0.002355781616643071, 0.012406938709318638, -0.0069608548656105995, -0.009556426666676998, <output removed>]}]} \n    ```", "```py\n    import pandas as pd\n    df = pd.read_csv('item-demand-time.csv', dtype = object, names=['timestamp','value','client'])\n    df.head(3)\n    ```", "```py\n    import matplotlib\n    import matplotlib.pyplot as plt\n    df.value=pd.to_numeric(df.value)\n    df_plot=df.pivot(index='timestamp',columns='item',\n                     values='value')\n    df_plot.plot(figsize=(40,10))\n    ```", "```py\n    df = df[df['item']=='client_12']\n    df = df.drop(['item', 'timestamp'], axis=1)\n    df.value *= 100\n    df.value = df.value.astype('int32')\n    df.head()\n    ```", "```py\n    import sagemaker\n    sess = sagemaker.Session()\n    bucket = sess.default_bucket()\n    prefix = 'electricity'\n    df.to_csv('electricity.csv', index=False, \n              header=False)\n    training_data_path = sess.upload_data(\n                           path='electricity.csv', \n                           key_prefix=prefix + \n                                      '/input/training')\n    ```", "```py\n    training_data_channel = \n        sagemaker.TrainingInput(\n            s3_data=training_data_path,                                                            \n            content_type='text/csv;label_size=0',                                         \n            distribution='ShardedByS3Key')\n    rcf_data = {'train': training_data_channel}\n    ```", "```py\n    from sagemaker.estimator import Estimator\n    from sagemaker.image_uris import retrieve\n    region = sess.boto_session.region_name\n    container = retrieve('randomcutforest', region)\n    rcf_estimator = Estimator(container,\n        role= sagemaker.get_execution_role(),\n        instance_count=1,\n        instance_type='ml.m5.large',\n        output_path='s3://{}/{}/output'.format(bucket, \n                                               prefix))\n    rcf_estimator.set_hyperparameters(feature_dim=1)\n    rcf_estimator.fit(rcf_data)\n    endpoint_name = 'rcf-demo'\n    rcf_predictor = rcf_estimator.deploy(\n        endpoint_name=endpoint_name,\n        initial_instance_count=1,\n        instance_type='ml.t2.medium')\n    ```", "```py\n    rcf_predictor.serializer =\n        sagemaker.serializers.CSVSerializer()\n    rcf_predictor.deserializer =\n        sagemaker.deserializers.JSONDeserializer()\n    values = df['value'].astype('str').tolist()\n    response = rcf_predictor.predict(values)\n    print(response)\n    ```", "```py\n    {'scores': [{'score': 1.0868037776}, {'score': 1.5307718138}, {'score': 1.4208102841} …\n    ```", "```py\n    from statistics import mean,stdev\n    scores = []\n    for s in response['scores']:\n        scores.append(s['score'])\n    score_mean = mean(scores)\n    score_std = stdev(scores)\n    ```", "```py\n    df[2000:2500].plot(figsize=(40,10))\n    plt.figure(figsize=(40,10))\n    plt.plot(scores[2000:2500])\n    plt.autoscale(tight=True)\n    plt.axhline(y=score_mean+3*score_std, color='red')\n    plt.show()\n    ```", "```py\n    rcf_predictor.delete_endpoint()\n    ```"]
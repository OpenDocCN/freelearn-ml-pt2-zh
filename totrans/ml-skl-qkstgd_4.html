<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Categories with Naive Bayes and SVMs</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, you will learn about two popular classification machine learning algorithms: the Naive Bayes algorithm and the linear support vector machine. The Naive Bayes algorithm is a probabilistic model that predicts classes and categories, while the linear support vector machine uses a linear decision boundary to predict classes and categories.</p>
<p>In this chapter, you will learn about the following topics:</p>
<ul>
<li>The theoretical concept behind the Naive Bayes algorithm, explained in mathematical terms</li>
<li>Implementing the Naive Bayes algorithm by <span>using</span> scikit-learn </li>
<li>How the linear support vector machine algorithm works under the hood</li>
<li>Graphically optimizing the hyperparameters of the linear support vector machines</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You will be required to have <span class="fontstyle0">Python 3.6 or greater, </span><span class="fontstyle0">Pandas ≥ 0.23.4,</span><span class="fontstyle2"> </span><span class="fontstyle0">Scikit-learn ≥ 0.20.0, and </span><span class="fontstyle0">Matplotlib ≥ 3.0.0 </span></span><span class="fontstyle0">installed on your system.</span></p>
<p class="mce-root">The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_04.ipynb">https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_04.ipynb</a></p>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2COBMUj">http://bit.ly/2COBMUj</a></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Naive Bayes algorithm </h1>
                </header>
            
            <article>
                
<p>The Naive Bayes algorithm makes use of the Bayes theorem, in order to classify classes and categories. The word <strong>naive</strong> was given to the algorithm because the algorithm assumes that all attributes are independent of one another. This is not actually possible, as every attribute/feature in a dataset is related to another attribute, in one way or another.</p>
<p>Despite being naive, the algorithm does well in actual practice. The formula for the Bayes theorem is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1b16aa8f-172a-40bb-8fcd-29c05d95d1b4.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Bayes theorem formula</div>
<p>We can split the preceding algorithm into the following components: </p>
<ul>
<li><strong>p(h|D)</strong>:<strong> </strong>This is the probability of a hypothesis taking place, provided that we have a dataset. An example of this would be the probability of a fraudulent transaction taking place, provided that we had a dataset that consisted of fraudulent and non-fraudulent transactions. </li>
<li><strong>p(D|h)</strong>:<strong> </strong>This is the probability of having the data, given a hypothesis. An example of this would be the probability of having a dataset that contained fraudulent transactions. </li>
<li><strong>p(h)</strong>:<strong> </strong>This is the probability of a hypothesis taking place, in general. An example of this would be a statement that the average probability of fraudulent transactions taking place in the mobile industry is 2%. </li>
<li><strong>p(D)</strong>:<strong> </strong>This is the probability of having the data before knowing any hypothesis. An example of this would be the probability that a dataset of mobile transactions could be found without knowing what we wanted to do with it (for example, predict fraudulent mobile transactions).</li>
</ul>
<p>In the preceding formula, the <em>p(D)</em><strong> </strong>can be rewritten in terms of <em>p(h)</em> and <em>p(D|h),</em><strong> </strong>as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a6afb98e-348a-465e-9789-3cdf54bf90c7.png" style=""/></div>
<p class="mce-root"/>
<p>Let's take a look at how we can implement this with the method of predicting classes, in the case of the mobile transaction example: </p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>p(D|h)</strong></td>
<td><strong>p(h)</strong></td>
<td><strong>p(D|-h)</strong></td>
<td><strong>(1 - p(h))</strong></td>
</tr>
<tr>
<td>0.8</td>
<td>0.08</td>
<td>0.02</td>
<td>0.92</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Substituting the values in the preceding table into the Bayes theorem formula produces a result of 0.77. This means that the classifier predicts that there is a 77% probability that a transaction will be predicted as fraudulent, using the data that was given previously.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the Naive Bayes algorithm in scikit-learn</h1>
                </header>
            
            <article>
                
<p>Now that you have learned how the Naive Bayes algorithm generates predictions, we will implement the same classifier using scikit-learn, in order to predict whether a particular transaction is fraudulent.</p>
<p>The first step is to import the data, create the feature and target arrays, and split the data into training and test sets.</p>
<p>We can do this by using the following code:</p>
<pre>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The next step is to build the Naive Bayes classifier. We can do this by using the following code:</p>
<pre>from sklearn.naive_bayes import GaussianNB<br/><br/>#Initializing an NB classifier<br/><br/>nb_classifier = GaussianNB()<br/><br/>#Fitting the classifier into the training data<br/><br/>nb_classifier.fit(X_train, y_train)<br/><br/>#Extracting the accuracy score from the NB classifier<br/><br/>nb_classifier.score(X_test, y_test)</pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we import the <kbd>GaussianNB</kbd><em> </em>module from scikit-learn</li>
<li>Next, we initialize a Naive Bayes classifier and store it in the variable <kbd>nb_classifier</kbd></li>
<li>Then, we fit the classifier to the training data and evaluate its accuracy on the test data</li>
</ol>
<p class="mce-root">The Naive Bayes classifier has only one hyperparameter, which is the prior probability of the hypothesis, <em>p(h)</em><strong>. </strong>However, keep the following in mind:</p>
<ul>
<li>The prior probability will not be available to us in most problems</li>
<li>Even if it is, the value is usually fixed as a statistical fact, and therefore, hyperparameter optimization is not performed</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machines</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn about <strong>support vector machines (SVMs),</strong> or, to be more specific, <strong>linear support vector machines</strong>. In order to understand support vector machines, you will need to know what support vectors are. They are illustrated for you in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0a99c06e-4d59-437e-8c66-e6c83ccbe883.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The concept of support vectors</div>
<p class="mce-root">In the preceding diagram, the following applies:</p>
<ul>
<li>The linear support vector machine is a form of linear classifier. A linear decision tree boundary is constructed, and the observations on one side of the boundary (the circles) belong to one class, while the observations on the other side of the boundary (the squares) belong to another class. </li>
<li>The support vectors are the observations that have a triangle on them. </li>
<li>These are the observations that are either very close to the linear decision boundary or have been incorrectly classified. </li>
<li>We can define which observations we want to make support vectors by defining how close to the decision boundary they should be. </li>
<li>This is controlled by the hyperparameter known as the <strong>inverse regularization strength</strong><strong>.</strong></li>
</ul>
<p>In order to understand how the linear support vector machines work, consider the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5bbc50eb-d97f-40c3-9b0a-a27f872768ff.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Concept of max-margins</div>
<p>In the preceding diagram, the following applies:</p>
<ul>
<li>The line between the support vectors and the linear decision boundary is known as the <strong>margin</strong></li>
<li>The goal of the support vector machines is to maximize this margin, so that a new data point will be correctly classified </li>
<li>A low value of inverse regularization strength ensures that this margin is as big as possible</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the linear support vector machine algorithm in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how to implement the linear support vector machines in scikit-learn. The first step is to import the data and split it into training and testing sets. We can do this by using the following code:</p>
<pre>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)<br/><br/></pre>
<p>The next step is to build the linear support vector machine classifier. We can do this by using the following code:</p>
<pre>from sklearn.svm import LinearSVC<br/><br/>#Initializing a SVM model <br/><br/>svm = LinearSVC(random_state = 50)<br/><br/>#Fitting the model to the training data<br/><br/>svm.fit(X_train, y_train)<br/><br/>#Extracting the accuracy score from the training data<br/><br/>svm.score(X_test, y_test)</pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we import the <kbd>LinearSVC</kbd><em> </em>module from scikit-learn</li>
<li>Next, we initialize a linear support vector machine object with a random state of 50, so that the model produces the same result every time</li>
<li>Finally, we fit the model to the training data and evaluate its accuracy on the test data</li>
</ol>
<p>Now that we have built the model, we can find and optimize the most ideal value for the hyperparameters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter optimization for the linear SVMs</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how to optimize the hyperparameters for the linear support vector machines. In particular, there is one hyperparameter of interest: the <strong>inverse regularization strength</strong>. </p>
<p>We will explore how to optimize this hyperparameter, both graphically and algorithmically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Graphical hyperparameter optimization</h1>
                </header>
            
            <article>
                
<p>In order to optimize the inverse regularization strength, we will plot the accuracy scores for the training and testing sets, using the following code:</p>
<pre>import matplotlib.pyplot as plt <br/>from sklearn.svm import LinearSVC<br/><br/>training_scores = []<br/>testing_scores = []<br/><br/>param_list = [0.0001, 0.001, 0.01, 0.1, 10, 100, 1000]<br/><br/># Evaluate the training and test classification errors for each value of the parameter<br/><br/>for param in param_list:<br/>    <br/>    # Create SVM object and fit<br/>    <br/>    svm = LinearSVC(C = param, random_state = 42)<br/>    svm.fit(X_train, y_train)<br/>    <br/>    # Evaluate the accuracy scores and append to lists<br/>    <br/>    training_scores.append(svm.score(X_train, y_train) )<br/>    testing_scores.append(svm.score(X_test, y_test) )<br/>    <br/># Plot results<br/><br/>plt.semilogx(param_list, training_scores, param_list, testing_scores)<br/>plt.legend(("train", "test"))<br/>plt.ylabel('Accuracy scores')<br/>plt.xlabel('C (Inverse regularization strength)')<br/>plt.show()<br/><br/></pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we initialize two empty lists, in order to store the accuracy scores for both the training and testing datasets</li>
<li>The next step is to create a list of values of the hyperparameter, which, in this case, is the inverse regularization strength</li>
<li>We then loop over each value in the hyperparameter list and build a linear support vector machine classifier with each inverse regularization strength value</li>
<li>The accuracy scores for the training and testing datasets are then appended to the empty lists</li>
<li>Using<span> </span><kbd>matplotlib</kbd>, we then create a plot between the inverse regularization strength (along the<span> </span><em>x </em>axis) and the accuracy scores for both the training and test sets (along the<span> </span><em>y </em>axis)</li>
</ol>
<p>This will produce a plot as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f919f6a8-5f0d-4d0d-87f2-6bc652d2ca54.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Graphical hyperparameter optimization </div>
<p>In the preceding diagram, the following applies:</p>
<ul>
<li>We can observe that the accuracy score is highest for the training and testing sets for an inverse regularization strength of 10<sup>-2</sup></li>
<li>It is important to pick a value that has a high value of accuracy for both the training and testing sets, and not just either one of the datasets </li>
<li>This will help you to prevent both overfitting and underfitting</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter optimization using GridSearchCV</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how to optimize the inverse regularization strength using the <kbd>GridSearchCV</kbd> algorithm. We can do this using the following code:</p>
<pre>from sklearn.model_selection import GridSearchCV<br/><br/>#Building the model <br/><br/>svm = LinearSVC(random_state = 50)<br/><br/>#Using GridSearchCV to search for the best parameter<br/><br/>grid = GridSearchCV(svm, {'C':[0.00001, 0.0001, 0.001, 0.01, 0.1, 10]})<br/>grid.fit(X_train, y_train)<br/><br/># Print out the best parameter<br/><br/>print("The best value of the inverse regularization strength is:", grid.best_params_)</pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we import the <kbd>GridSearchCV</kbd><em> </em>module from scikit-learn</li>
<li>The next step is to initialize a linear support vector machine model with a random state of 50, in order to ensure that we obtain the same results every time we build the model</li>
<li>We then initialize a grid of possible hyperparameter values for the inverse regularization strength</li>
<li>Finally, we fit the grid of hyperparameter values to the training set, so that we can build multiple linear SVM models with the different values of the inverse regularization strength</li>
<li>The <kbd>GridSearchCV</kbd> algorithm then evaluates the model that produces the fewest generalization errors and returns the optimal value of the hyperparameter</li>
</ol>
<p>It's a good practice to compare and contrast the results of the graphical method of hyperparameter optimization with that of <kbd>GridSearchCV</kbd>, in order to validate your results. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling the data for performance improvement</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn about how scaling and standardizing the data can lead to an improvement in the overall performance of the linear support vector machines. The concept of scaling remains the same as in the case of the previous chapters, and it will not be discussed here. In order to scale the data, we use the following code:</p>
<pre>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import Pipeline<br/><br/>#Setting up the scaling pipeline <br/><br/>order = [('scaler', StandardScaler()), ('SVM', LinearSVC(C = 0.1, random_state = 50))]<br/><br/>pipeline = Pipeline(order)<br/><br/>#Fitting the classfier to the scaled dataset <br/><br/>svm_scaled = pipeline.fit(X_train, y_train)<br/><br/>#Extracting the score <br/><br/>svm_scaled.score(X_test, y_test)</pre>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we import the <kbd>StandardScaler</kbd> the <kbd>Pipeline</kbd><em> </em>modules from scikit-learn, in order to build a scaling pipeline</li>
<li>We then set up the order of the pipeline, which specifies that we use the <kbd>StandardScaler()</kbd><em> </em>function first, in order to scale the data and build the linear support vector machine on that scaled data</li>
<li>The <kbd>Pipeline()</kbd><em> </em>function is applied to the order of the pipeline which sets up the pipeline</li>
<li>We then fit this pipeline to the training data and extract the scaled accuracy scores from the test data</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter introduced you to two fundamental supervised machine learning algorithms: the Naive Bayes algorithm and linear support vector machines. More specifically, you learned about the following topics:</p>
<ul>
<li>How the Bayes theorem is used to produce a probability, to indicate whether a data point belongs to a particular class or category</li>
<li>Implementing the Naive Bayes classifier in scikit-learn</li>
<li>How the linear support vector machines work under the hood</li>
<li>Implementing the linear support vector machines in scikit-learn</li>
<li>Optimizing the inverse regularization strength, both graphically and by using the <kbd>GridSearchCV</kbd> algorithm</li>
<li>How to scale your data for a potential improvement in performance</li>
</ul>
<p>In the next chapter, you will learn about the other type of supervised machine learning algorithm, which is used to predict numeric values, rather than classes and categories: linear regression!</p>


            </article>

            
        </section>
    </body></html>
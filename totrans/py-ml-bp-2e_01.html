<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Python Machine Learning Ecosystem</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Machine learning is rapidly changing our world. As the centerpiece of artificial intelligence, it is difficult to go a day without reading how it will lead us into either a techno-utopia <span><span>along the lines of</span></span> the Singularity, or into some sort of global Blade Runner-esque nightmare scenario. While pundits may enjoy discussing these hyperbolic futures, the more mundane reality is that machine learning is rapidly becoming a fixture of our daily lives. Through subtle but progressive improvements in how we interact with computers and the world around us, machine learning is progressively making our lives better.</p>
<p>If you shop at online retailers such as Amazon.com, use streaming music or movie services such as Spotify or Netflix, or have even just done a Google search, you have encountered an application that utilizes machine learning. These services collect vast amounts of data—much of it from their users—that is used to build models that improve the user experience.</p>
<p>It's an ideal time to dive into developing machine learning applications, and, as you will discover, Python is an ideal choice with which to develop them. Python has a deep and active developer community, many with roots in the scientific community. This heritage has provided Python with an unparalleled array of libraries for scientific computing. In this book, we will discuss and use a number of the libraries included in this <strong>Python Scientific Stack</strong>.</p>
<p>In the chapters that follow, we'll learn how to build a wide variety of machine learning applications step by step. Before we begin in earnest though, we'll spend the remainder of this chapter discussing the features of these key libraries and how to prepare your environment to best utilize them.</p>
<p>These are the topics that will be covered in this chapter:</p>
<ul>
<li>The data science/machine learning workflow</li>
<li>Libraries for each stage of the workflow</li>
<li>Setting up your environment</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data science/machine learning workflow</h1>
                </header>
            
            <article>
                
<p>Building machine learning applications, while similar in many respects to the standard engineering paradigm, differs in one crucial aspect: the need to work with data as a raw material. The success of your project will, in large part, depend on the quality of the data you acquire, as well as your handling of that data. And because working with data falls into the domain of data science, it is helpful to understand the data science workflow:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/59e5ab55-ba98-4e7c-8779-64d5559fd999.png" style="width:6.58em;height:33.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Data science workflow</div>
<p class="mce-root"/>
<p>The process involves these six steps in the following order:</p>
<ol>
<li>Acquisition</li>
<li>Inspection</li>
<li>Preparation</li>
<li>Modeling</li>
<li>Evaluation</li>
<li>Deployment</li>
</ol>
<p>Frequently, there is a need to circle back to prior steps, such as when inspecting and preparing the data, or when evaluating and modeling, but the process at a high level can be as described in the preceding list.</p>
<p>Let's now discuss each step in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Acquisition</h1>
                </header>
            
            <article>
                
<p>Data for machine learning applications can come from any number of sources; it may be emailed to you as a CSV file, it may come from pulling down server logs, or it may require building a custom web scraper. Data is also likely to exist in any number of formats. In most cases, you will be dealing with text-based data, but, as we'll see, machine learning applications may just as easily be built that utilize images or even video files. Regardless of the format, once you have secured the data, it is crucial that you understand what's in the data, as well as what isn't.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inspection</h1>
                </header>
            
            <article>
                
<p>Once you have acquired your data, the next step is to inspect it. The primary goal at this stage is to sanity check the data, and the best way to accomplish this is to look for things that are either impossible or highly unlikely. As an example, if the data has a unique identifier, check to see that there is indeed only one; if the data is price-based, check that it is always positive; and whatever the data type, check the most extreme cases. Do they make sense? A good practice is to run some simple statistical tests on the data, and visualize it. The outcome of your models is only as good as the data you put in, so it is crucial to get this step right.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparation</h1>
                </header>
            
            <article>
                
<p>When you are confident you have your data in order, next you will need to prepare it by placing it in a format that is amenable to modeling. This stage encompasses a number of processes, such as filtering, aggregating, imputing, and transforming. The type of actions you need to take will be highly dependent on the type of data you're working with, as well as the libraries and algorithms you will be utilizing. For example, if you are working with natural language-based texts, the transformations required will be very different from those required for time-series data. We'll see a number of examples of these types of transformations throughout the book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modeling</h1>
                </header>
            
            <article>
                
<p>Once the data preparation is complete, the next phase is modeling. Here, you will be selecting an appropriate algorithm and using the data to train your model. There are a number of best practices to adhere to during this stage, and we will discuss them in detail, but the basic steps involve splitting your data into training, testing, and validation sets. This splitting up of the data may seem illogical—especially when more data typically yields better models—but as we'll see, doing this allows us to get better feedback on how the model will perform in the real world, and prevents us from the cardinal sin of modeling: overfitting. We will talk more about this in later chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>So, now you've got a shiny new model, but exactly how good is that model? This is the question that the evaluation phase seeks to answer. There are a number of ways to measure the performance of a model, and again it is largely dependent on the type of data you are working with and the type of model used, but on the whole, we are seeking to answer the question of how close the model's predictions are to the actual value. There is an array of confusing sounding terms, such as root mean-square error, or Euclidean distance, or F1 score. But in the end, they are all just a measure of distance between the actual prediction and the estimated prediction.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>Once you are comfortable with the performance of your model, you'll want to deploy it. This can take a number of forms depending on the use case, but common scenarios include utilization as a feature within another larger application, a bespoke web application, or even just a simple <span>cron job</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Python libraries and functions for each stage of the data science workflow</h1>
                </header>
            
            <article>
                
<p>Now that you have an understanding of each step in the data science workflow, we'll take a look at a selection of useful Python libraries and functions within those libraries for each step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Acquisition</h1>
                </header>
            
            <article>
                
<p>Since one of the more common ways to access data is through a RESTful API, one library that you'll want to be aware of is the Python Requests library, <a href="http://www.python-requests.org/en/latest/" target="_blank"><span class="URLPACKT">http://www.python-requests.org/en/latest/</span></a>. Dubbed <em>HTTP for humans</em>, it makes interacting with APIs a clean and simple experience.</p>
<p><span class="BodyTextChar">Let's take a look at a sample interaction, using <kbd>requests</kbd> to pull down data from GitHub's API. Here, we will make a call to the API and request a list of starred repositories for a user:</span></p>
<pre><strong>import requests r = requests.get(r"https://api.github.com/users/acombs/starred") r.json()</strong> </pre>
<p class="mce-root"/>
<p>This will return a JSON of all the repositories the user has starred, along with attributes about each. Here is a snippet of the output for the preceding call:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/b91dee33-9507-4fe6-bc61-6f7ccf15254c.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Output snippet when we <span>return a </span>JSON<span> of all the repositories</span></div>
<p>The <kbd>requests</kbd> library has an amazing number of features—far too many to cover here, but I do suggest you check out the documentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inspection</h1>
                </header>
            
            <article>
                
<p>Because inspecting your data is such a critical step in the development of machine learning applications, we'll now take an in-depth look at several libraries that will serve you well in this task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Jupyter Notebook</h1>
                </header>
            
            <article>
                
<p>There are a number of libraries that will make the data inspection process easier. The first is Jupyter Notebook with IPython (<a href="http://ipython.org/" target="_blank"><span class="URLPACKT">http://ipython.org/</span></a>). This is a fully-fledged, interactive computing environment, and it is ideal for data exploration. Unlike most development environments, Jupyter Notebook is a web-based frontend (to the IPython kernel) that is divided into individual code blocks or cells. Cells can be run individually or all at once, depending on the need. This allows the developer to run a scenario, see the output, then step back through the code, make adjustments, and see the resulting changes—all without leaving the notebook. Here is a sample interaction in the Jupyter Notebook:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/0f2d7643-e871-491e-ada7-57402abffdf7.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span> Sample interaction in the Jupyter Notebook</span></div>
<p>You will notice that we have done a number of things here and have interacted with not only the IPython backend, but the terminal shell as well. Here, I have imported the Python <kbd>os</kbd> library and made a call to find the current working directory (cell #2), which you can see is the output below my input code cell. I then changed directories using the <kbd>os</kbd> library in cell #3, but stopped utilizing the <kbd>os</kbd> library and began using Linux-based commands in cell #4. This is done by adding the <kbd>!</kbd> prepend to the cell. In cell #6, you can see that I was even able to save the shell output to a Python variable (<kbd>file_two</kbd>). This is a great feature that makes file operations a simple task.</p>
<div class="packt_infobox">Note that the results would obviously differ slightly on your machine, since this displays information on the user under which it runs.</div>
<p>Now, let's take a look at some simple data operations using the notebook. This will also be our first introduction to another indispensable library, pandas.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pandas</h1>
                </header>
            
            <article>
                
<p>Pandas is a remarkable tool for data analysis that aims to be the most powerful and flexible open source data analysis/manipulation tool available in any language. And, as you will soon see, if it doesn't already live up to this claim, it can't be too far off. Let's now take a look:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1201 image-border" src="assets/9656705b-7e70-4d66-b324-9e2deeed7cb1.png" style="width:162.50em;height:102.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Importing the iris dataset</div>
<p>You can see from the preceding screenshot that I have imported a classic machine learning dataset, the <kbd>iris</kbd> dataset (also available at <a href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>), using scikit-learn, a library we'll examine in detail later. I then passed the data into a <kbd>pandas</kbd> DataFrame, making sure to assign the column headers. One DataFrame contains flower measurement data, and the other DataFrame contains a number that represents the <kbd>iris</kbd> species. This is coded <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> for <kbd>setosa</kbd>, <kbd>versicolor</kbd>, and <kbd>virginica</kbd> respectively. I then concatenated the two DataFrames.</p>
<p>For working with datasets that will fit on a single machine, pandas is the ultimate tool; you can think of it a bit like Excel on steroids. And, like the popular spreadsheet program, the basic units of operation are columns and rows of data that form tables. In the terminology of pandas, columns of data are series and the table is a DataFrame.</p>
<p>Using the same <kbd>iris</kbd> DataFrame we loaded previously, let's now take a look at a few common operations, including the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/10aa8f4a-04fb-4db6-a38b-bc8c4d1fd422.png" style="width:43.42em;height:24.92em;"/></div>
<p>The first action was just to use the <kbd>.head()</kbd> command to get the first five rows. The second command was to select a single column from the DataFrame by referencing it by its column name. Another way we perform this <strong>data slicing</strong> is to use the <kbd>.iloc[row,column]</kbd> or <kbd>.loc[row,column]</kbd> notation. The former slices data using a numeric index for the columns and rows (positional indexing), while the latter uses a numeric index for the rows, but allows for using named columns (label-based indexing).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's select the first two columns and the first four rows using the <kbd>.iloc</kbd> notation. We'll then look at the <kbd>.loc</kbd> notation:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/3a7b24de-b62e-4118-afb7-aa8c649ed7cb.png" style="width:43.33em;height:26.33em;"/></div>
<p>Using the <kbd>.iloc</kbd> notation and the Python list slicing syntax, we were able to select a slice of this DataFrame.</p>
<p>Now, let's try something more advanced. We'll use a list iterator to select just the width feature columns:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/0c9b8e0e-6855-45ff-8c6a-521b129482d3.png" style="width:43.33em;height:28.58em;"/></div>
<p>What we have done here is create a list that is a subset of all columns. <kbd>df.columns</kbd> returns a list of all columns, and our iteration uses a conditional statement to select only those with <kbd>width</kbd> in the title. Obviously, in this situation, we could have just as easily typed out the columns we wanted into a list, but this gives you a sense of the power available when dealing with much larger datasets.</p>
<p>We've seen how to select slices based on their position within the DataFrame, but let's now look at another method to select data. This time, we will select a subset of the data based upon satisfying conditions that we specify:</p>
<ol>
<li>Let's now see the unique list of <kbd>species</kbd> available, and select just one of those:</li>
</ol>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a2244c3e-0207-47d4-8f7c-4ce31f2fa31f.png" style="width:43.33em;height:26.92em;"/></div>
<ol start="2">
<li>In the far-right column, you will notice that our DataFrame only contains data for the <kbd>Iris-virginica</kbd> species (represented by the <kbd>2</kbd>) now. In fact, the size of the DataFrame is now 50 rows, down from the original 150 rows:</li>
</ol>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/0039c8de-1b67-47a5-8893-df42c4cf4a49.png" style="width:27.25em;height:22.25em;"/></div>
<ol start="3">
<li>You can also see that the index on the left retains the original row numbers. If we wanted to save just this data, we could save it as a new DataFrame, and reset the index as shown in the following diagram:</li>
</ol>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/1081ae3b-92d2-4009-813b-461fbf0a02e7.png" style="width:43.33em;height:25.25em;"/></div>
<p class="mce-root"/>
<ol start="4">
<li>We have selected data by placing a condition on one column; let's now add more conditions. We'll go back to our original DataFrame and add two conditions:</li>
</ol>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/ace6fa80-3595-4244-9956-238b2050fd54.png" style="width:43.42em;height:20.00em;"/></div>
<p style="padding-left: 60px">The DataFrame now only includes data from the <kbd>virginica</kbd> species with a petal width greater than <kbd>2.2</kbd>.</p>
<p>Let's now move on to using pandas to get some quick descriptive statistics from our <kbd>iris</kbd> dataset:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/4cc98651-0e08-4d8a-9e02-b38cf7ddf095.png" style="width:45.25em;height:20.42em;"/></div>
<p class="mce-root"/>
<p>With a call to the <kbd>.describe()</kbd> function, I have received a breakdown of the descriptive statistics for each of the relevant columns. (Notice that species was automatically removed as it is not relevant for this.) I could also pass in my own percentiles if I wanted more granular information:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/434fcf61-d65d-46b1-86fc-fa823575b981.png" style="width:48.58em;height:33.42em;"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Next, let's check whether there is any correlation between these features. That can be done by calling <kbd>.corr()</kbd> on our DataFrame:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/dec8063d-60fd-4bd5-8fe8-3cddd2a9917e.png"/></div>
<p>The default returns the <strong>Pearson correlation coefficient</strong> for each row-column pair. This can be switched to <strong>Kendall's Tau</strong> or <strong>Spearman's rank correlation coefficient</strong> by passing in a method argument (for example, <kbd>.corr(method="spearman")</kbd> or <kbd>.corr(method="kendall")</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualization</h1>
                </header>
            
            <article>
                
<p>So far, we have seen how to select portions of a DataFrame and how to get summary statistics from our data, but let's now move on to learning how to visually inspect the data. But first, why even bother with visual inspection? Let's see an example to understand why.</p>
<p>Here is the summary statistics for four distinct series of <em>x</em> and <em>y</em> values:</p>
<table border="1" style="border-collapse: collapse;width: 63.5488%">
<tbody>
<tr>
<td style="width: 37%">
<p><strong>Series of <em>x</em> and <em>y</em></strong></p>
</td>
<td style="width: 24.6364%">
<p><strong>Values</strong></p>
</td>
</tr>
<tr>
<td style="width: 37%">
<p>Mean of <em>x</em></p>
</td>
<td style="width: 24.6364%">
<p>9</p>
</td>
</tr>
<tr>
<td style="width: 37%">
<p>Mean of <em>y</em></p>
</td>
<td style="width: 24.6364%">
<p>7.5</p>
</td>
</tr>
<tr>
<td style="width: 37%">
<p>Sample variance of <em>x</em></p>
</td>
<td style="width: 24.6364%">
<p>11</p>
</td>
</tr>
<tr>
<td style="width: 37%">
<p>Sample variance of <em>y</em></p>
</td>
<td style="width: 24.6364%">
<p>4.1</p>
</td>
</tr>
<tr>
<td style="width: 37%">
<p>Correlation between <em>x</em> and <em>y</em></p>
</td>
<td style="width: 24.6364%">
<p>0.816</p>
</td>
</tr>
<tr>
<td style="width: 37%">
<p>Regression line</p>
</td>
<td style="width: 24.6364%">
<p><em>y</em> = 3.00 + 0.500<em>x</em></p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Based on the series having identical summary statistics, you might assume that these series would appear visually similar. You would, of course, be wrong. Very wrong. The four series are part of <strong>Anscombe's quartet</strong>, and they were deliberately created to illustrate the importance of visual data inspection. Each series is plotted as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/8a039b4d-bd6a-41a3-be70-72f761c00ab1.png"/></div>
<p>Clearly, we would not treat these datasets as identical after having visualized them. So, now that we understand the importance of visualization, let's take a look at a pair of useful Python libraries for this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The matplotlib library</h1>
                </header>
            
            <article>
                
<p>The first library we'll take a look at is <kbd>matplotlib</kbd>. The <kbd>matplotlib</kbd> library is the center of the Python plotting library universe. Originally created to emulate the plotting functionality of MATLAB, it grew into a fully-featured library in its own right with an enormous range of functionality. If you have not come from a MATLAB background, it can be hard to understand how all the pieces work together to create the graphs you see. I'll do my best to break down the pieces into logical components so you can get up to speed quickly. But before diving into <kbd>matplotlib</kbd> in full, let's set up our Jupyter Notebook to allow us to see our graphs inline. To do this, add the following lines to your <kbd>import</kbd> statements:</p>
<pre><strong>import matplotlib.pyplot as plt 
plt.style.use('ggplot') 
%matplotlib inline</strong> </pre>
<p>The first line imports <kbd>matplotlib</kbd>, the second line sets the styling to approximate R's <kbd>ggplot</kbd> library (requires matplotlib 1.41 or greater), and the last line sets the plots so that they are visible within the notebook.</p>
<p>Now, let's generate our first graph using our <kbd>iris</kbd> dataset:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(6,4)) 
ax.hist(df['petal width (cm)'], color='black'); 
ax.set_ylabel('Count', fontsize=12) 
ax.set_xlabel('Width', fontsize=12) 
plt.title('Iris Petal Width', fontsize=14, y=1.01)</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2d5d7bf0-1f1e-468d-b4d8-1f71fafd2967.png" style="width:28.33em;height:19.75em;"/></p>
<p>There is a lot going on even in this simple example, but we'll break it down line by line. The first line creates a single subplot with a width of <kbd>6</kbd> inches and a height of <kbd>4</kbd> inches. We then plot a histogram of the petal width from our <kbd>iris</kbd> DataFrame by calling <kbd>.hist()</kbd> and passing in our data. We also set the bar color to <kbd>black</kbd> here. The next two lines place labels on our <em>y</em> and <em>x</em> axes, respectively, and the final line sets the title for our graph. We tweak the title's <em>y</em> position relative to the top of the graph with the <em>y</em> parameter, and increase the font size slightly over the default. This gives us a nice histogram of our petal width data. Let's now expand on that, and generate histograms for each column of our <kbd>iris</kbd> dataset:</p>
<pre><strong>fig, ax = plt.subplots(2,2, figsize=(6,4)) 
 
ax[0][0].hist(df['petal width (cm)'], color='black'); 
ax[0][0].set_ylabel('Count', fontsize=12) 
ax[0][0].set_xlabel('Width', fontsize=12) 
ax[0][0].set_title('Iris Petal Width', fontsize=14, y=1.01) 
 
ax[0][1].hist(df['petal length (cm)'], color='black'); 
ax[0][1].set_ylabel('Count', fontsize=12) 
ax[0][1].set_xlabel('Length', fontsize=12) 
ax[0][1].set_title('Iris Petal Length', fontsize=14, y=1.01) 
 
ax[1][0].hist(df['sepal width (cm)'], color='black'); 
ax[1][0].set_ylabel('Count', fontsize=12) 
ax[1][0].set_xlabel('Width', fontsize=12) 
ax[1][0].set_title('Iris Sepal Width', fontsize=14, y=1.01) 
 
ax[1][1].hist(df['sepal length (cm)'], color='black'); 
ax[1][1].set_ylabel('Count', fontsize=12) 
ax[1][1].set_xlabel('Length', fontsize=12) 
ax[1][1].set_title('Iris Sepal Length', fontsize=14, y=1.01) 
 
plt.tight_layout()  
 </strong></pre>
<p>The output for the preceding code is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1084 image-border" src="assets/d6900b26-a80f-4f45-8f0f-e7a40f2bf1ee.png" style="width:40.92em;height:26.83em;"/></p>
<p>Obviously, this is not the most efficient way to code this, but it is useful for demonstrating how <kbd>matplotlib</kbd> works. Notice that instead of the single subplot object, <kbd>ax</kbd>, as we had in the first example, we now have four subplots, which are accessed through what is now the <kbd>ax</kbd> array. A new addition to the code is the call to <kbd>plt.tight_layout()</kbd>; this function will nicely auto-space your subplots to avoid crowding.</p>
<p>Let's now take a look at a few other types of plots available in <kbd>matplotlib</kbd>. One useful plot is a <strong>scatterplot</strong>. Here, we will plot the petal width against the petal length:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(6,6)) 
ax.scatter(df['petal width (cm)'],df['petal length (cm)'],                      color='green') 
ax.set_xlabel('Petal Width') 
ax.set_ylabel('Petal Length') 
ax.set_title('Petal Scatterplot') <br/></strong></pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bed222bf-d0e9-4ce9-92f2-3240ee9a3459.png" style="width:36.58em;height:34.33em;"/></p>
<p>As before, we could add in multiple subplots to examine each facet.</p>
<p>Another plot we could examine is a simple line plot. Here, we will look at a plot of the petal length:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(6,6)) 
ax.plot(df['petal length (cm)'], color='blue') 
ax.set_xlabel('Specimen Number') 
ax.set_ylabel('Petal Length') 
ax.set_title('Petal Length Plot')</strong> </pre>
<p>The preceding code generates the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/76a9ae21-4b20-4963-b1f7-da912d47c549.png" style="width:37.42em;height:35.00em;"/></div>
<p>We can already begin to see, based on this simple line plot, that there are distinctive clusters of lengths for each species—remember our sample dataset had 50 ordered examples of each type. This tells us that petal length is likely to be a useful feature to discriminate between the species if we were to build a classifier.</p>
<p>Let's look at one final type of chart from the <kbd>matplotlib</kbd> library, the bar chart. This is perhaps one of the more common charts you'll see. Here, we'll plot a bar chart for the mean of each feature for the three species of irises, and to make it more interesting, we'll make it a stacked bar chart with a number of additional <kbd>matplotlib</kbd> features:</p>
<pre><strong>import numpy as np</strong><br/><strong>fig, ax = plt.subplots(figsize=(6,6))</strong><br/><strong>bar_width = .8</strong><br/><strong>labels = [x for x in df.columns if 'length' in x or 'width' in x]</strong><br/><strong>set_y = [df[df['species']==0][x].mean() for x in labels]</strong><br/><strong>ver_y = [df[df['species']==1][x].mean() for x in labels]</strong><br/><strong>vir_y = [df[df['species']==2][x].mean() for x in labels]</strong><br/><strong>x = np.arange(len(labels))</strong><br/><strong>ax.bar(x, set_y, bar_width, color='black')</strong><br/><strong>ax.bar(x, ver_y, bar_width, bottom=set_y, color='darkgrey')</strong><br/><strong>ax.bar(x, vir_y, bar_width, bottom=[i+j for i,j in zip(set_y, ver_y)], color='white')</strong><br/><strong>ax.set_xticks(x + (bar_width/2))</strong><br/><strong>ax.set_xticklabels(labels, rotation=-70, fontsize=12);</strong><br/><strong>ax.set_title('Mean Feature Measurement By Species', y=1.01)</strong><br/><strong>ax.legend(['Setosa','Versicolor','Virginica'])  </strong> </pre>
<p>The output for the preceding snippet is given here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/1d9e505e-c134-40d2-adce-a3ca7804caf8.png" style="width:32.25em;height:36.17em;"/></div>
<p>To generate the bar chart, we need to pass the <kbd>x</kbd> and <kbd>y</kbd> values into the <kbd>.bar()</kbd> function. In this case, the <kbd>x</kbd> values will just be an array of the length of the features we are interested in—four here, or one for each column in our DataFrame. The <kbd>np.arange()</kbd> function is an easy way to generate this, but we could nearly as easily input this array manually. Since we don't want the <em>x</em> axis to display this as 1 through 4, we call the <kbd>.set_xticklabels()</kbd> function and pass in the column names we wish to display. To line up the <kbd>x</kbd> labels properly, we also need to adjust the spacing of the labels. This is why we set the <kbd>xticks</kbd> to <kbd>x</kbd> plus half the size of the <kbd>bar_width</kbd>, which we also set earlier at <kbd>0.8</kbd>. The <kbd>y</kbd> values come from taking the mean of each feature for each species. We then plot each by calling <kbd>.bar()</kbd>. It is important to note that we pass in a <kbd>bottom</kbd> parameter for each series, which sets the minimum <em>y</em> point and the maximum <em>y</em> point of the series below it. This creates the stacked bars. And finally, we add a legend, which describes each series. The names are inserted into the legend list in order of the placement of the bars from top to bottom.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The seaborn library</h1>
                </header>
            
            <article>
                
<p>The next visualization library we'll look at is called <kbd>seaborn</kbd>, (<a href="http://seaborn.pydata.org/index.html" target="_blank"><span class="URLPACKT">http://seaborn.pydata.org/index.html</span></a>). It is a library that was created specifically for statistical visualizations. In fact, it is perfect for use with <kbd>pandas</kbd> DataFrames, where the columns are features and the rows are observations. This style of DataFrame is called <strong>tidy</strong> data, and is the most common form for machine learning applications.</p>
<p>Let's now take a look at the power of <kbd>seaborn</kbd>:</p>
<pre><strong>import seaborn as sns 
sns.pairplot(df, hue='species')</strong> </pre>
<p>With just those two lines of code, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/755a6034-36e0-495b-bcef-83c32e7d3267.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Seaborn plot</div>
<p>Having just detailed the intricate nuances of <kbd>matplotlib</kbd>, you will immediately appreciate the simplicity with which we generated this plot. All of our features have been plotted against each other and properly labeled with just two lines of code. You might wonder if I just wasted dozens of pages teaching you <kbd>matplotlib</kbd> when <kbd>seaborn</kbd> makes these types of visualizations so simple. Well, that isn't the case, as <kbd>seaborn</kbd> is built on top of <kbd>matplotlib</kbd>. In fact, you can use all of what you learned about <kbd>matplotlib</kbd> to modify and work with <kbd>seaborn</kbd>. Let's take a look at another visualization:</p>
<pre><strong>fig, ax = plt.subplots(2, 2, figsize=(7, 7)) <br/>sns.set(style='white', palette='muted') <br/>sns.violinplot(x=df['species'], y=df['sepal length (cm)'], ax=ax[0,0]) sns.violinplot(x=df['species'], y=df['sepal width (cm)'], ax=ax[0,1]) sns.violinplot(x=df['species'], y=df['petal length (cm)'], ax=ax[1,0]) sns.violinplot(x=df['species'], y=df['petal width (cm)'], ax=ax[1,1]) fig.suptitle('Violin Plots', fontsize=16, y=1.03) <br/>for i in ax.flat:<br/> plt.setp(i.get_xticklabels(), rotation=-90) <br/>fig.tight_layout()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/b7f548cf-894e-42d8-ab6a-8b4b4cebe4ea.png" style="width:39.33em;height:39.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Violin Plots</div>
<p>Here, we have generated a violin plot for each of the four features. A violin plot displays the distribution of the features. For example, you can easily see that the <span class="packt_screen">petal length</span> of <kbd>setosa</kbd> (0) is highly clustered between 1 cm and 2 cm, while <kbd>virginica</kbd> (2) is much more dispersed, from nearly 4 cm to over 7 cm. You will also notice that we have used much of the same code we used when constructing the <kbd>matplotlib</kbd> graphs. The main difference is the addition of the <kbd>sns.plot()</kbd> calls, in place of the <kbd>ax.plot()</kbd> calls previously. We have also added a title above all of the subplots, rather than over each individually, with the <kbd>fig.suptitle()</kbd> function. One other notable addition is the iteration over each of the subplots to change the rotation of the <kbd>xticklabels</kbd>. We call <kbd>ax.flat()</kbd> and then iterate over each subplot axis to set a particular property using <kbd>.setp()</kbd>. This prevents us from having to individually type out <kbd>ax[0][0]...ax[1][1]</kbd> and set the properties, as we did previously in the earlier <kbd>matplotlib</kbd> subplot code.</p>
<p>There are hundreds of styles of graphs you can generate using <kbd>matplotlib</kbd> and <kbd>seaborn</kbd>, and I highly recommend digging into the documentation for these two libraries—it will be time well spent—but the graphs I have detailed in the preceding section should go a long way toward helping you to understand the dataset you have, which in turn will help you when building your machine learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparation</h1>
                </header>
            
            <article>
                
<p>We've learned a great deal about inspecting the data we have, but now let's move on to learning how to process and manipulate our data. Here, we will learn about the <kbd>.map()</kbd>, <kbd>.apply()</kbd>, <kbd>.applymap()</kbd>, and <kbd>.groupby()</kbd> functions of pandas. These are invaluable for working with data, and are especially useful in the context of machine learning for feature engineering, a concept we will discuss in detail in later chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">map</h1>
                </header>
            
            <article>
                
<p>We'll now begin with the <kbd>map</kbd> function. The <kbd>map</kbd> function works on series, so in our case we will use it to transform a column of our DataFrame, which you will recall is just a pandas series. Suppose we decide that the species numbers are not suitable for our needs. We'll use the <kbd>map</kbd> function with a Python dictionary as the argument to accomplish this. We'll pass in a replacement for each of the unique <kbd>iris</kbd> types:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a142343c-7ea1-4a61-9819-b6a992fbeda2.png" style="width:43.33em;height:27.33em;"/></div>
<p>Let's look at what we have done here. We have run the <kbd>map</kbd> function over each of the values of the existing <kbd>species</kbd> column. As each value was found in the Python dictionary, it was added to the return series. We assigned this return series to the same <kbd>species</kbd> name, so it replaced our original <kbd>species</kbd> column. Had we chosen a different name, say <kbd>short code</kbd>, that column would have been appended to the DataFrame, and we would then have the original <kbd>species</kbd> column plus the new <kbd>short code</kbd> column.</p>
<p>We could have instead passed the <kbd>map</kbd> function a series or a function to perform this transformation on a column, but this is a functionality that is also available through the <kbd>apply</kbd> function, which we'll take a look at next. The dictionary functionality is unique to the <kbd>map</kbd> function, and the most common reason to choose <kbd>map</kbd> over <kbd>apply</kbd> for a single column transformation. But, let's now take a look at the <kbd>apply</kbd> function.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">apply</h1>
                </header>
            
            <article>
                
<p>The <kbd>apply</kbd> function allows us to work with both DataFrames and series. We'll start with an example that would work equally well with <kbd>map</kbd>, before moving on to examples that would only work with <kbd>apply</kbd>.</p>
<p>Using our <kbd>iris</kbd> DataFrame, let's make a new column based on petal width. We previously saw that the mean for the petal width was <kbd>1.3</kbd>. Let's now create a new column in our DataFrame, <kbd>wide petal</kbd>, that contains binary values based on the value in the <kbd>petal width</kbd> column. If the <kbd>petal width</kbd> is equal to or wider than the median, we will code it with a <kbd>1</kbd>, and if it is less than the median, we will code it <kbd>0</kbd>. We'll do this using the <kbd>apply</kbd> function on the <kbd>petal width</kbd> column:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/130a51fb-e890-4b1d-b68a-855ba42b2ca6.png"/></div>
<p>A few things happened here, so let's walk through them step by step. The first is that we were able to append a new column to the DataFrame simply by using the column selection syntax for a column name, which we want to create, in this case <kbd>wide petal</kbd>. We set that new column equal to the output of the <kbd>apply</kbd> function. Here, we ran <kbd>apply</kbd> on the <kbd>petal width</kbd> column that returned the corresponding values in the <kbd>wide petal</kbd> column. The <kbd>apply</kbd> function works by running through each value of the <kbd>petal width</kbd> column. If the value is greater than or equal to <kbd>1.3</kbd>, the function returns <kbd>1</kbd>, otherwise it returns <kbd>0</kbd>. This type of transformation is a fairly common feature engineering transformation in machine learning, so it is good to be familiar with how to perform it.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's now take a look at using <kbd>apply</kbd> on a DataFrame rather than a single series. We'll now create a feature based on the <kbd>petal area</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/50618111-1c9a-4d1a-bd5c-d5b9212ddbf7.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Creating a new feature</div>
<p>Notice that we called <kbd>apply</kbd> not on a series here, but on the entire DataFrame, and because <kbd>apply</kbd> was called on the entire DataFrame, we passed in <kbd>axis=1</kbd> in order to tell pandas that we want to apply the function row-wise. If we passed in <kbd>axis=0</kbd>, then the function would operate column-wise. Here, each column is processed sequentially, and we choose to multiply the values from the <kbd>petal length (cm)</kbd> and <kbd>petal width (cm)</kbd> columns. The resultant series then becomes the <kbd>petal area</kbd> column in our DataFrame. This type of power and flexibility is what makes pandas an indispensable tool for data manipulation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">applymap</h1>
                </header>
            
            <article>
                
<p>We've looked at manipulating columns and explained how to work with rows, but suppose you'd like to perform a function across all data cells in your DataFrame. This is where <kbd>applymap</kbd> is the correct tool. Let's take a look at an example:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/dc75d666-8800-4472-a5ad-4aab04203a7a.png" style="width:44.92em;height:17.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Using applymap function</div>
<p>Here, we called <kbd>applymap</kbd> on our DataFrame in order to get the log of every value (<kbd>np.log()</kbd> utilizes the NumPy library to return this value), if that value is of the float type. This type checking prevents returning an error or a float for the <kbd>species</kbd> or <kbd>wide petal</kbd> columns, which are string and integer values respectively. Common uses of <kbd>applymap</kbd> include transforming or formatting each cell based on meeting a number of conditional criteria.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">groupby</h1>
                </header>
            
            <article>
                
<p>Let's now look at an operation that is highly useful, but often difficult for new pandas users to get their heads around: the <kbd>.groupby()</kbd> function. We'll walk through a number of examples step by step in order to illustrate the most important functionality.</p>
<p>The <kbd>groupby</kbd> operation does exactly what it says: it groups data based on some class or classes you choose. Let's take a look at a simple example using our <kbd>iris</kbd> dataset. We'll go back and reimport our original <kbd>iris</kbd> dataset, and run our first <kbd>groupby</kbd> operation:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/95d40a45-53e1-4933-b659-e49668bab553.png" style="width:47.58em;height:12.00em;"/></div>
<p>Here, data for each species is partitioned and the mean for each feature is provided. Let's take it a step further now and get full descriptive statistics for each <kbd>species</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/1d842d79-d829-4838-afc0-3e120126af1e.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Statistics for each species</div>
<p>And now, we can see the full breakdown bucketed by <kbd>species</kbd>. Let's now look at some other <kbd>groupby</kbd> operations we can perform. We saw previously that petal length and width had some relatively clear boundaries between species. Now, let's examine how we might use <kbd>groupby</kbd> to see that:</p>
<p class="CDPAlignCenter CDPAlign"><span class="URLPACKT"><img src="assets/dae8f90a-1b47-4c14-9c37-bc7c2ceaf936.png" style="width:43.25em;height:28.58em;"/></span></p>
<p class="mce-root"/>
<p>In this case, we have grouped each unique species by the <kbd>petal width</kbd> they were associated with. This is a manageable number of measurements to group by, but if it were to become much larger, we would likely need to partition the measurements into brackets. As we saw previously, that can be accomplished by means of the <kbd>apply</kbd> function.</p>
<p>Let's now take a look at a custom aggregation function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ee9e15bb-4bbc-4886-b084-5d777252a6ed.png"/></p>
<p>In this code, we grouped petal width by species using the <kbd>.max()</kbd> and <kbd>.min()</kbd> <span>functions,</span> and a <kbd>lambda</kbd> function that returns a maximum petal width less than the minimum petal width.</p>
<div class="packt_infobox">We've only just touched on the functionality of the <kbd>groupby</kbd> function; there is a lot more to learn, so I encourage you to read the documentation available at <a href="http://pandas.pydata.org/pandas-docs/stable/" target="_blank"><span class="URLPACKT">http://pandas.pydata.org/pandas-docs/stable/</span></a>.</div>
<p>Hopefully, you now have a solid base-level understanding of how to manipulate and prepare data in preparation for our next step, which is modeling. We will now move on to discuss the primary libraries in the Python machine learning ecosystem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modeling and evaluation</h1>
                </header>
            
            <article>
                
<p>In this section ,we will go through different libraries such as <kbd>statsmodels</kbd> and <kbd>Scikit-learn</kbd> and also understand what is deployment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statsmodels</h1>
                </header>
            
            <article>
                
<p>The first library we'll cover is the <kbd>statsmodels</kbd> library (<a href="http://statsmodels.sourceforge.net/" target="_blank">http://statsmodels.sourceforge.net/</a>). Statsmodels is a Python package that is well documented and developed for exploring data, estimating models, and running statistical tests. Let's use it here to build a simple linear regression model of the relationship between sepal length and sepal width for the <kbd>setosa</kbd> species.</p>
<p>First, let's visually inspect the relationship with a scatterplot:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(7,7)) 
ax.scatter(df['sepal width (cm)'][:50], df['sepal length (cm)'][:50]) 
ax.set_ylabel('Sepal Length') 
ax.set_xlabel('Sepal Width') 
ax.set_title('Setosa Sepal Width vs. Sepal Length', fontsize=14, y=1.02)</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8dd510a9-5de5-4967-a9fb-6f804204c0a3.png" style="width:37.75em;height:37.83em;"/></p>
<p class="mce-root"/>
<p>So, we can see that there appears to be a positive linear relationship; that is, as the sepal width increases, the sepal length does as well. We'll next run a linear regression on the data using <kbd>statsmodels</kbd> to estimate the strength of that relationship:</p>
<pre><strong>import statsmodels.api as sm 
 
y = df['sepal length'][:50] 
x = df['sepal width'][:50] 
X = sm.add_constant(x) 
 
results = sm.OLS(y, X).fit() 
print results.summary()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7d3255fb-4a45-4918-800c-7d5c4ab75ba6.png"/></p>
<p>In the preceding diagram, we have the results of our simple regression model. Since this is a linear regression, the model takes the format of <span class="NormalPACKTChar"><em>Y = Β<sub>0</sub>+ Β<sub>1</sub>X</em>,</span> where <em>B<sub>0</sub></em> is the intercept and <em>B<sub>1</sub></em> is the regression coefficient. Here, the formula would be <em>Sepal Length = 2.6447 + 0.6909 * Sepal Width</em>. We can also see that the <em>R<sup>2</sup></em> for the model is a respectable <kbd>0.558</kbd>, and the <em>p</em>-value, (<kbd>Prob</kbd>), is highly significant—at least for this species.</p>
<p>Let's now use the <kbd>results</kbd> object to plot our regression line:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(7,7)) 
ax.plot(x, results.fittedvalues, label='regression line') 
ax.scatter(x, y, label='data point', color='r') 
ax.set_ylabel('Sepal Length') 
ax.set_xlabel('Sepal Width') 
ax.set_title('Setosa Sepal Width vs. Sepal Length', fontsize=14, y=1.02) 
ax.legend(loc=2)</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/03f2c04e-4350-4284-981e-e9e0627cb691.png" style="width:37.42em;height:35.92em;"/></p>
<p>By plotting <kbd>results.fittedvalues</kbd>, we can get the resulting regression line from our regression.</p>
<p class="mce-root"/>
<p>There are a number of other statistical functions and tests in the <kbd>statsmodels</kbd> package, and I invite you to explore them. It is an exceptionally useful package for standard statistical modeling in Python. Let's now move on to the king of Python machine learning packages: scikit-learn.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scikit-learn</h1>
                </header>
            
            <article>
                
<p>Scikit-learn is an amazing Python library with unrivaled documentation, designed to provide a consistent API to dozens of algorithms. It is built upon, and is itself, a core component of the Python scientific stack, which includes NumPy, SciPy, pandas, and matplotlib. Here are some of the areas scikit-learn covers: classification, regression, clustering, dimensionality reduction, model selection, and preprocessing.</p>
<p class="mce-root">We'll look at a few examples. First, we will build a classifier using our <kbd>iris</kbd> data, and then we'll look at how we can evaluate our model using the tools of scikit-learn:</p>
<ol>
<li>The first step to building a machine learning model in scikit-learn is understanding how the data must be structured.</li>
<li>The independent variables should be a numeric <em>n ×</em><em> m</em> matrix, <em>X</em>, and the dependent variable, <em>y</em>, an <em>n ×</em><em> 1</em> vector.</li>
<li>The <em>y</em> vector may be either a numeric continuous or categorical, or a string categorical.</li>
<li>These are then passed into the <kbd>.fit()</kbd> method on the chosen classifier.</li>
<li>This is the great benefit of using scikit-learn: each classifier utilizes the same methods to the extent possible. This makes swapping them in and out a breeze.</li>
</ol>
<p><span>Let's see this in action in our first example:</span></p>
<pre><strong>from sklearn.ensemble import RandomForestClassifier 
from sklearn.cross_validation import train_test_split 
 
clf = RandomForestClassifier(max_depth=5, n_estimators=10) 
 
X = df.ix[:,:4] 
y = df.ix[:,4] 
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3) 
 
clf.fit(X_train,y_train) 
 
y_pred = clf.predict(X_test) 
 
rf = pd.DataFrame(zip(y_pred, y_test), columns=['predicted', 'actual']) 
rf['correct'] = rf.apply(lambda r: 1 if r['predicted'] == r['actual'] else 0, axis=1) 
     
rf</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9e72ccce-769a-406c-8334-e70e7a7a5176.png" style="width:14.33em;height:33.08em;"/></p>
<p>Now, let's execute the following line of code:</p>
<pre><strong>rf['correct'].sum()/rf['correct'].count()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/acb8d103-530a-4462-b7b3-befed4ea8e0c.png" style="width:15.25em;height:2.33em;"/></p>
<p>In the preceding few lines of code, we built, trained, and tested a classifier that has a 95% accuracy level on our <kbd>iris</kbd> dataset. Let's unpack each of the steps. Up at the top, we made a couple of imports; the first two are from scikit-learn, which thankfully is shortened to <kbd>sklearn</kbd> in import statements. The first import is a random forest classifier, and the second is a module for splitting your data into training and testing cohorts. This data partitioning is critical in building machine learning applications for a number of reasons. We'll get into this in later chapters, but suffice to say at this point it is a must. This <kbd>train_test_split</kbd> module also shuffles your data, which again is important as the order can contain information that would bias your actual predictions.</p>
<p>The first curious-looking line after the imports instantiates our classifier, in this case a random forest classifier. We select a forest that uses 10 decision tress, and each tree is allowed a maximum split depth of five. This is put in place to avoid overfitting, something we will discuss in depth in later chapters.</p>
<p>The next two lines create our <em>X</em> matrix and <em>y</em> vector. If you remember our original <kbd>iris</kbd> DataFrame, it contained four features: petal width and length, and sepal width and length. These features are selected and become our independent feature matrix, <em>X</em>. The last column, the <kbd>iris</kbd> class names, then becomes our dependent <em>y</em> vector.</p>
<p>These are then passed into the <kbd>train_test_split</kbd> method, which shuffles and partitions our data into four subsets, <kbd>X_train</kbd>, <kbd>X_test</kbd>, <kbd>y_train</kbd>, and <kbd>y_test</kbd>. The <kbd>test_size</kbd> parameter is set to <kbd>.3</kbd>, which means 30% of our dataset will be allocated to the <kbd>X_test</kbd> and <kbd>y_test</kbd> partitions, while the rest will be allocated to the training partitions, <kbd>X_train</kbd> and <kbd>y_train</kbd>.</p>
<p>Next, our model is fitted using the training data. Having trained the model, we then call the predict method on our classifier using our test data. Remember, the test data is data the classifier has not seen. The return of this prediction is a list of prediction labels. We then create a DataFrame of the actual labels versus the predicted labels. We finally total the correct predictions and divide by the total number of instances, which we can see gave us a very accurate prediction. Let's now see which features gave us the most discriminative or predictive power:</p>
<pre><strong>f_importances = clf.feature_importances_ <br/>f_names = df.columns[:4] <br/>f_std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0) <br/><br/>zz = zip(f_importances, f_names, f_std) <br/>zzs = sorted(zz, key=lambda x: x[0], reverse=True) <br/><br/>imps = [x[0] for x in zzs] <br/>labels = [x[1] for x in zzs] <br/>errs = [x[2] for x in zzs] <br/><br/>plt.bar(range(len(f_importances)), imps, color="r", yerr=errs, align="center") <br/>plt.xticks(range(len(f_importances)), labels);</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1085 image-border" src="assets/ab3a7c70-9cf5-4832-bfe3-30d93822cd23.png" style="width:38.33em;height:25.75em;"/></p>
<p>As we expected, based upon our earlier visual analysis, the petal length and width have more discriminative power when differentiating between the <kbd>iris</kbd> classes. Where exactly did these numbers come from though? The random forest has a method called <kbd>.feature_importances_</kbd> that returns the relative performance of the feature for splitting at the leaves. If a feature is able to consistently and cleanly split a group into distinct classes, it will have a high feature importance. This number will always total one. As you will notice here, we have included the standard deviation, which helps to illustrate how consistent each feature is. This is generated by taking the feature importance, for each of the features, for each ten trees, and calculating the standard deviation.</p>
<p>Let's now take a look at one more example using scikit-learn. We will now switch out our classifier and use a <strong>support vector machine</strong> (<strong>SVM</strong>):</p>
<pre><strong>from sklearn.multiclass import OneVsRestClassifier 
from sklearn.svm import SVC 
from sklearn.cross_validation import train_test_split 
 
clf = OneVsRestClassifier(SVC(kernel='linear')) 
 
X = df.ix[:,:4] 
y = np.array(df.ix[:,4]).astype(str) 
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3) 
 
clf.fit(X_train,y_train) 
 
y_pred = clf.predict(X_test) 
 
rf = pd.DataFrame(zip(y_pred, y_test), columns=['predicted', 'actual']) 
rf['correct'] = rf.apply(lambda r: 1 if r['predicted'] == r['actual'] else 0, axis=1) 
     
rf</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-236 image-border" src="assets/aac65c4a-d541-4397-be05-a2c38437f19c.png" style="width:14.42em;height:26.50em;"/></p>
<p class="mce-root"/>
<p>Now, let's execute the following line of code:</p>
<pre><strong>rf['correct'].sum()/rf['correct'].count() </strong></pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-237 image-border" src="assets/56d1a215-3be1-4641-9683-59b9520e6bc9.png" style="width:12.75em;height:2.08em;"/></p>
<p>Here, we have swapped in an SVM without changing virtually any of our code. The only changes were the ones related to the importing of the SVM instead of the random forest, and the line that instantiates the classifier. (I did have to make one small change to the format of the <kbd>y</kbd> labels, as the SVM wasn't able to interpret them as NumPy strings like the random forest classifier was. Sometimes, these data type conversions have to be made specific or it will result in an error, but it's a minor annoyance.)</p>
<p>This is only a small sample of the functionality of scikit-learn, but it should give you a hint of the power of this magnificent tool for machine learning applications. There are a number of additional machine learning libraries we won't have a chance to discuss here but will explore in later chapters, but I strongly suggest that if this is your first time utilizing a machine learning library, and you want a strong general-purpose tool, scikit-learn is your go-to choice.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>There are a number of options you can choose from when you decide to put your machine learning model into production. It depends substantially on the nature of the application. Deployment could include anything from a cron job run on your local machine to a full-scale implementation deployed on an Amazon EC2 instance.</p>
<p>We won't go into detail regarding specific implementations here, but we will have a chance to delve into different deployment examples throughout the book.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up your machine learning environment</h1>
                </header>
            
            <article>
                
<p>We've covered a number of libraries, and it could be somewhat of a chore to install if you were to do each individually—which you certainly can, since most can be installed with pip, Python's package manager, but I would strongly urge you to go with a prepacked solution such as the Anaconda Python distribution (<a href="http://anaconda.org" target="_blank"><span class="URLPACKT">http://anaconda.org</span></a>). This allows you to download and install a single executable with all the packages and dependencies handled for you. And since the distribution is targeted to Python scientific stack users, it is essentially a one-and-done solution.</p>
<p>Anaconda also includes a package manager that makes updating your packages a simple task. Simply type <kbd>conda update &lt;package_name&gt;</kbd>, and you will be updated to the most recent stable release.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about the data science/machine learning workflow. We learned how to take our data step by step through each stage of the pipeline, going from acquisition all the way through to deployment. We also learned key features of each of the most important libraries in the Python scientific stack. We will now take this knowledge and these lessons and begin to apply them to create unique and useful machine learning applications. Let's get started!</p>


            </article>

            
        </section>
    </body></html>
- en: '*Chapter 8*: XGBoost Alternative Base Learners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will analyze and apply different `gbtree`, additional options
    for base learners include `gblinear` and `dart`. Furthermore, XGBoost has its
    own implementations of random forests as base learners and as tree ensemble algorithms
    that you will experiment with in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: By learning how to apply alternative base learners, you will greatly extend
    your range with XGBoost. You will have the capacity to build many more models
    and you will learn new approaches to developing linear, tree-based, and random
    forest machine learning algorithms. The goal of the chapter is to give you proficiency
    in building XGBoost models with alternative base learners so that you can leverage
    advanced XGBoost options to find the best possible model for a range of situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring alternative base learners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying `gblinear`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing `dart`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding XGBoost random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code and datasets for this chapter may be found at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring alternative base learners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The base learner is the machine learning model that XGBoost uses to build the
    first model in its ensemble. The word *base* is used because it's the model that
    comes first, and the word *learner* is used because the model iterates upon itself
    after learning from the errors.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees have emerged as the preferred base learners for XGBoost on account
    of the excellent scores that boosted trees consistently produce. The popularity
    of decision trees extends beyond XGBoost to other ensemble algorithms such as
    random forests and `ExtraTreesClassifier` and `ExtraTreesRegressor` ([https://scikit-learn.org/stable/modules/ensemble.html](https://scikit-learn.org/stable/modules/ensemble.html)).
  prefs: []
  type: TYPE_NORMAL
- en: In XGBoost, the default base learner, known as `gbtree`, is one of several base
    learners. There is also `gblinear`, a gradient boosted linear model, and `dart`,
    a variation of decision trees that includes a dropout technique based on neural
    networks. Furthermore, there are XGBoost random forests. In the next section,
    we will explore the differences between these base learners before applying them
    in subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: gblinear
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees are optimal for **non-linear data** as they can easily access
    points by splitting the data as many times as needed. Decision trees are often
    preferable as base learners because real data is usually non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: There may be cases, however, where a `gblinear` as an option for a **linear
    base learner**.
  prefs: []
  type: TYPE_NORMAL
- en: The general idea behind boosted linear models is the same as boosted tree models.
    A base model is built, and each subsequent model is trained upon the residuals.
    At the end, the individual models are summed for the final result. The primary
    distinction with linear base learners is that each model in the ensemble is linear.
  prefs: []
  type: TYPE_NORMAL
- en: Like `gblinear` also adds regularization terms to linear regression. Tianqi
    Chin, the founder and developer of XGBoost commented on GitHub that multiple rounds
    of boosting `gblinear` may be used *to get back a single lasso regression* ([https://github.com/dmlc/xgboost/issues/332](https://github.com/dmlc/xgboost/issues/332)).
  prefs: []
  type: TYPE_NORMAL
- en: '`gblinear` may also be used for classification problems via **logistic regression**.
    This works because logistic regression is also built by finding optimal coefficients
    (weighted inputs), as in **linear regression**, and summed via the **sigmoid equation**
    (see [*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022), *Machine Learning
    Landscape*).'
  prefs: []
  type: TYPE_NORMAL
- en: We will explore the details and applications of `gblinear` in the *Applying
    gblinear* section in this chapter. For now, let's learn about `dart`.
  prefs: []
  type: TYPE_NORMAL
- en: DART
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Dropouts meet Multiple Additive Regression Trees**, simply known as **DART**,
    was introduced in 2015 by K. V. Rashmi from UC Berkeley and Ran Gilad-Bachrach
    from Microsoft in the following paper: [http://proceedings.mlr.press/v38/korlakaivinayak15.pdf](http://proceedings.mlr.press/v38/korlakaivinayak15.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Rashmi and Gilad-Bachrach highlight **Multiple Additive Regression Trees** (**MART**)
    as a successful model that suffers from too much dependency on earlier trees.
    Instead of focusing on **shrinkage**, a standard penalization term, they use the
    **dropout** technique from **neural networks**. Simply put, the dropout technique
    eliminates nodes (mathematical points) from each layer of learning in a neural
    network, thereby reducing overfitting. In other words, the dropout technique slows
    down the learning process by eliminating information from each round.
  prefs: []
  type: TYPE_NORMAL
- en: In DART, in each new round of boosting, instead of summing the residuals from
    all previous trees to build a new model, DART selects a random sample of previous
    trees and normalizes the leaves by a scaling factor ![](img/Formula_08_001.png)
    where ![](img/Formula_08_002.png) is the number of trees dropped.
  prefs: []
  type: TYPE_NORMAL
- en: DART is a variation of decision trees. The XGBoost implementation of DART is
    similar to `gbtree` with additional hyperparameters to accommodate dropouts.
  prefs: []
  type: TYPE_NORMAL
- en: For the mathematical details of DART, reference the original paper highlighted
    in the first paragraph of this section.
  prefs: []
  type: TYPE_NORMAL
- en: You will practice building machine learning models with `DART` base learners
    in the *Comparing dart* section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final option that we'll explore in this section is XGBoost random forests.
    Random forests may be implemented as base learners by setting `num_parallel_trees`
    equal to an integer greater than `1`, and as class options within XGBoost defined
    as `XGBRFRegressor` and `XGBRFClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that gradient boosting was designed to improve upon the errors
    of relatively weak base learners, not strong base learners like random forests.
    Nevertheless, there may be fringe cases where random forest base learners can
    be advantageous so it's a nice option to have.
  prefs: []
  type: TYPE_NORMAL
- en: As an additional bonus, XGBoost provides `XGBRFRegressor` and `XGBRFClassifier`
    as random forest machine learning algorithms that are not base learners, but algorithms
    in their own right. These algorithms work in a similar manner as scikit-learn's
    random forests (see [*Chapter 3*](B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070),
    *Bagging with Random Forests*). The primary difference is that XGBoost includes
    default hyperparameters to counteract overfitting and their own methods for building
    individual trees. XGBoost random forests have been in the experimental stage,
    but they are starting to outperform scikit-learn's random forests as of late 2020
    as you willwill see in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the final section of this chapter, we will experiment with XGBoost's random
    forests, both as base learners and as models in their own right.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an overview of XGBoost base learners, let's apply them one
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Applying gblinear
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's challenging to find real-world datasets that work best with linear models.
    It's often the case that real data is messy and more complex models like tree
    ensembles produce better scores. In other cases, linear models may generalize
    better.
  prefs: []
  type: TYPE_NORMAL
- en: The success of machine learning algorithms depends on how they perform with
    real-world data. In the next section, we will apply `gblinear` to the Diabetes
    dataset first and then to a linear dataset by construction.
  prefs: []
  type: TYPE_NORMAL
- en: Applying gblinear to the Diabetes dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Diabetes dataset is a regression dataset of 442 diabetes patients provided
    by scikit-learn. The prediction columns include age, sex, **BMI** (**body mass
    index**), **BP** (**blood pressure**), and five serum measurements. The target
    column is the progression of the disease after 1 year. You can read about the
    dataset in the original paper here: [http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf](http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn's datasets are already split into predictor and target columns
    for you. They are preprocessed for machine learning with `X`, the predictor columns,
    and `y`, the target column, loaded separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full list of imports that you will need to work with this dataset
    and the rest of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s begin! To use the Diabetes dataset, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You first need to define `X` and `y` using `load_diabetes` with the `return_X_y`
    parameter set equal to `True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The plan is to use `cross_val_score` and `GridSearchCV`, so let's create folds
    in advance to obtain consistent scores. In [*Chapter 6*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136),
    *XGBoost Hyperparameters*, we used `StratifiedKFold`, which stratifies the target
    column, ensuring that each test set includes the same number of classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This approach works for classification, but not for regression, where the target
    column takes on continuous values and classes are not involved. `KFold` achieves
    a similar goal without stratification by creating consistent splits in the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, shuffle the data and use `5` splits with `KFold` using the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a function with `cross_val_score` that takes a machine learning model
    as input and returns the mean score of `5` folds as the output, making sure to
    set `cv=kfold`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To use `gblinear` as the base model, just set `booster=''gblinear''` for `XGBRegressor`
    inside the regression function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s check this score against other linear models including `LinearRegression`,
    `Lasso`, which uses `Ridge`, which uses `LinearRegression` is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'b) `Lasso` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'c) `Ridge` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, `XGBRegressor` with `gblinear` as the base learner performs
    the best, along with `LinearRegression`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now place `booster=''gbtree''` inside `XGBRegressor`, which is the default
    base learner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the `gbtree` base learner does not perform nearly as well as
    the `gblinear` base learner in this case indicating that a linear model is ideal.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see if we can modify hyperparameters to make some gains with `gblinear`
    as the base learner.
  prefs: []
  type: TYPE_NORMAL
- en: gblinear hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's important to understand the differences between `gblinear` and `gbtree`
    when adjusting hyperparameters. Many of the XGBoost hyperparameters presented
    in [*Chapter 6*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136)*, XGBoost Hyperparameters*,
    are tree hyperparameters and do not apply to `gblinear`. For instance, `max_depth`
    and `min_child_weight` are hyperparameters specifically designed for trees.
  prefs: []
  type: TYPE_NORMAL
- en: The following list is a summary of XGBoost `gblinear` hyperparameters that are
    designed for linear models.
  prefs: []
  type: TYPE_NORMAL
- en: reg_lambda
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scikit-learn uses `reg_lambda` instead of `lambda`, which is a reserved keyword
    for lambda functions in Python. This is the standard L2 regularization used by
    `Ridge`. Values close to `0` tend to work best:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [0, inf)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Increasing prevents overfitting*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alias: lambda*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reg_alpha
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scikit-learn accepts both `reg_alpha` and `alpha`. This is the standard L1
    regularization used by `Lasso`. Values close to `0` tend to work best:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [0, inf)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Increasing prevents overfitting*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alias: alpha*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: updater
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the algorithm that XGBoost uses to build the linear model during each
    round of boosting. `shotgun` uses `hogwild` parallelism with coordinate descent
    to produce a non-deterministic solution. By contrast, `coord_descent` is ordinary
    coordinate descent with a deterministic solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: shotgun*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: shotgun, coord_descent*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Coordinate descent* is a machine learning term defined as minimizing the error
    by finding the gradient one coordinate at a time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: feature_selector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`feature_selector` determines how the weights are selected with the following
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: a) `cyclic` – cycles through features iteratively
  prefs: []
  type: TYPE_NORMAL
- en: b) `shuffle` – cyclic with random feature-shuffling in each round
  prefs: []
  type: TYPE_NORMAL
- en: c) `random` – the coordinate selector during coordinate descent is random
  prefs: []
  type: TYPE_NORMAL
- en: d) `greedy` – time-consuming; selects the coordinate with the greatest gradient
    magnitude
  prefs: []
  type: TYPE_NORMAL
- en: e) `thrifty` – approximately greedy, reorders features according to weight changes
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: cyclic*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range must be used in conjunction with updater as follows:*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a) `shotgun`: `cyclic`, `shuffle`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `coord_descent`: `random`, `greedy`, `thrifty`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`greedy` will be computationally expensive for large datasets, but the number
    of features that `greedy` considers may be reduced by changing the parameter `top_k`
    (see the following).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: top_k
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`top_k` is the number of features that `greedy` and `thrifty` select from during
    coordinate descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 0 (all features)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [0, max number of features]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For more information on XGBoost `gblinear` hyperparameters consult the official
    XGBoost documentation page at [https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: gblinear grid search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you are familiar with the range of hyperparameters that `gblinear`
    may use, let''s use `GridSearchCV` in a customized `grid_search` function to find
    the best ones:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a version of our `grid_search` function from [*Chapter 6*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136),
    *XGBoost Hyperparameters*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s start by modifying `alpha` with a standard range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The score is about the same, with a very slight improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let''s modify `reg_lambda` with the same range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This score here is very similar but slightly worse.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now let's use `feature_selector` in tandem with `updater`. By default, `updater=shotgun`
    and `feature_selector=cyclic`. When `updater=shotgun`, the only other option for
    `feature_selector` is `shuffle`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s see if `shuffle` can perform better than `cyclic`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, `shuffle` does not perform better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now let''s change `updater` to `coord_descent`. As a result, `feature_selector`
    may take on `random`, `greedy`, or `thrifty`. Try all `feature_selector` alternatives
    in `grid_search` by entering the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The final hyperparameter to check is `top_k`, which defines the number of features
    that `greedy` and `thrifty` check during coordinate descent. A range from `2`
    to `9` is acceptable since there are 10 features in total.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Enter a range for `top_k` inside `grid_search` for `greedy` and `thrifty` to
    find the best option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the best score yet.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, note that additional hyperparameters that are not limited
    to trees, such as `n_estimators` and `learning_rate`, may be used as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how `gblinear` works on a dataset that is linear by construction.
  prefs: []
  type: TYPE_NORMAL
- en: Linear datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to ensure that a dataset is linear is by construction. We can choose
    a range of `X` values, say `1` to `99`, and multiply them by a scaling factor
    with some randomness involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to construct a linear dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the range of `X` values from `1` to `100`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Declare a random seed using NumPy to ensure the consistency of the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an empty list defined as `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through `X`, multiplying each entry by a random number from `-0.2` to
    `0.2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform `y` to a `numpy` array for machine learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape `X` and `y` so that they contain as many rows as members in the array
    and one column since columns are expected as machine learning inputs with scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We now have a linear dataset that includes randomness in terms of `X` and `y`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s run the `regression_model` function again with `gblinear` as the base
    learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run the `regression_model` function with `gbtree` as the base learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `gblinear` performs much better in our constructed linear dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For good measure, let''s try `LinearRegression` on the same dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `gblinear` performs slightly better, perhaps negligibly, scoring
    `0.00002` points lower than `LinearRegression`.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing gblinear
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`gblinear` is a compelling option, but it should only be used when you have
    reason to believe that a linear model may perform better than a tree-based model.
    `gblinear` did outperform `LinearRegression` in the real and constructed datasets
    by a very slight margin. Within XGBoost, `gblinear` is a strong option for a base
    learner when datasets are large and linear. `gblinear` is an option for classification
    datasets as well, an option that you will apply in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing dart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The base learner `dart` is similar to `gbtree` in the sense that both are gradient
    boosted trees. The primary difference is that `dart` removes trees (called dropout)
    during each round of boosting.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will apply and compare the base learner `dart` to other
    base learners in regression and classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: DART with XGBRegressor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s see how `dart` performs on the Diabetes dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, redefine `X` and `y` using `load_diabetes` as before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To use `dart` as the XGBoost base learner, set the `XGBRegressor` parameter
    `booster=''dart''` inside the `regression_model` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `dart` base learner gives the same result as the `gbtree` base learner down
    to two decimal places. The similarity of results is on account of the small dataset
    and the success of the `gbtree` default hyperparameters to prevent overfitting
    without requiring the dropout technique.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how `dart` performs compared to `gbtree` on a larger dataset with
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: dart with XGBClassifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have used the Census dataset in multiple chapters throughout this book.
    A clean version of the dataset that we modified in [*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022),
    *Machine Learning Landscape*, has been pre-loaded for you along with the code
    for [*Chapter 8*](B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189), *XGBoost Alternative
    Base Learners*, at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08).
    Let''s now begin to test how `dart` performs on a larger dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the Census dataset into a DataFrame and split the predictor and target
    columns into `X` and `y` using the last index (`-1`) as the target column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a new classification function that uses `cross_val_score` with the machine
    learning model as input and the mean score as output similar to the regression
    function defined earlier in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now call the function twice using `XGBClassifier` with `booster=''gbtree''`
    and `booster=''dart''` to compare results. Note that the run time will be longer
    since the dataset is larger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) Let''s first call `XGBClassifier` with `booster=''gbtree''`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'b) Now, let''s call `XGBClassifier` with `booster=''dart''`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is surprising. `dart` gives the exact same result as `gbtree` for all 16
    decimal places! It's unclear whether trees have been dropped or the dropping of
    trees has had zero effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can adjust hyperparameters to ensure that trees are dropped, but first,
    let''s see how `dart` compares to `gblinear`. Recall that `gblinear` also works
    for classification by using the sigmoid function to scale weights as with logistic
    regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the `classification_model` function with `XGBClassifier` and set `booster=''gblinear''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This linear base learner does not perform as well as the tree base learners.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s see how `gblinear` compares with logistic regression. Since the dataset
    is large, it''s best to adjust logistic regression''s `max_iter` hyperparameter
    from `100` to `1000` to allow more time for convergence and to silence warnings.
    Note that increasing `max_iter` increases the accuracy in this case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`gblinear` maintains a clear edge over logistic regression in this case. It''s
    worth underscoring that XGBoost''s `gblinear` option in classification provides
    a viable alternative to logistic regression.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you have seen how `dart` compares with `gbtree` and `gblinear` as a
    base learner, let's modify `dart`'s hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: DART hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`dart` includes all `gbtree` hyperparameters along with its own set of additional
    hyperparameters designed to adjust the percentage, frequency, and probability
    of dropout trees. See the XGBoost documentation at [https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart](https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart)
    for detailed information.'
  prefs: []
  type: TYPE_NORMAL
- en: The following sections are a summary of XGBoost hyperparameters that are unique
    to `dart`.
  prefs: []
  type: TYPE_NORMAL
- en: sample_type
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The options for `sample_type` include `uniform`, which drops trees uniformly,
    and `weighted`, which drops trees in proportion to their weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: "uniform"*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: ["uniform", "weighted"]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Determines how dropped trees are selected*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: normalize_type
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The options for `normalize_type` include `tree`, where new trees have the same
    weight as dropped trees, and `forest`, where new trees have the same weight as
    the sum of dropped trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: "tree"*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: ["tree", "forest"]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Calculates weights of trees in terms of dropped trees*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rate_drop
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`rate_drop` allows the user to set exactly how many trees are dropped percentage-wise:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 0.0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [0.0, 1.0]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Percentage of trees that are dropped*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one_drop
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When set to `1`, `one_drop` ensures that at least one tree is always dropped
    during the boosting round:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [0, 1]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Used to ensure drops*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: skip_drop
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`skip_drop` gives the probability of skipping the dropout entirely. In the
    official documentation, XGBoost says that `skip_drop` has a higher priority than
    `rate_drop` or `one_drop`. By default, each tree is dropped with the same probability
    so there is a probability that no trees are dropped for a given boosting round.
    `skip_drop` allows this probability to be updated to control the number of dropout
    rounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 0.0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [0.0, 1.0]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Probability of skipping the dropout*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's modify `dart` hyperparameters to differentiate scores.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying dart hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To ensure that at least one tree in each boosting round is dropped, we can
    set `one_drop=1`. Do this with the Census dataset via the `classification_model`
    function now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This is an improvement by a tenth of a percentage point, indicating that dropping
    at least one tree per boosting round can be advantageous.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are dropping trees to change scores, let''s return to the smaller
    and faster Diabetes dataset to modify the remaining hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `regression_model` function, change `sample_type` from `uniform`
    to `weighted`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is 0.002 points better than the `gbtree` model scored earlier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Change `normalize_type` to `forest` to include the sum of trees when updating
    weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is no change in the score, which may happen with a shallow dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Change `one_drop` to `1` guaranteeing that at least one tree is dropped each
    boosting round:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a clear improvement, gaining four full points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When it comes to `rate_drop`, the percentage of trees that will be dropped,
    a range of percentages may be used with the `grid_search` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This is the best result yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement a similar range with `skip_drop`, which gives the probability
    that a given tree is *not* dropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: This is a good score, but `skip_drop` has resulted in no substantial gains.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you see how `dart` works in action, let's analyze the results.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing dart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`dart` provides a compelling option within the XGBoost framework. Since `dart`
    accepts all `gbtree` hyperparameters, it''s easy to change the base learner from
    `gbtree` to `dart` when modifying hyperparameters. In effect, the advantage is
    that you can experiment with new hyperparameters including `one_drop`, `rate_drop`,
    `normalize`, and others to see if you can make additional gains. `dart` is definitely
    worth trying as a base learner in your research and model-building with XGBoost.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a good understanding of `dart`, it's time to move on to random
    forests.
  prefs: []
  type: TYPE_NORMAL
- en: Finding XGBoost random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two strategies to implement random forests within XGBoost. The first
    is to use random forests as the base learner, the second is to use XGBoost's original
    random forests, `XGBRFRegressor` and `XGBRFClassifier`. We start with our original
    theme, random forests as alternative base learners.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests as base learners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is not an option to set the booster hyperparameter to a random forest.
    Instead, the hyperparameter `num_parallel_tree` may be increased from its default
    value of `1` to transform `gbtree` (or `dart`) into a boosted random forest. The
    idea here is that each boosting round will no longer consist of one tree, but
    a number of parallel trees, which in turn make up a forest.
  prefs: []
  type: TYPE_NORMAL
- en: The following is a quick summary of the XGBoost hyperparameter `num_parallel_tree`.
  prefs: []
  type: TYPE_NORMAL
- en: num_parallel_tree
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`num_parallel_tree` gives the number of trees, potentially more than 1, that
    are built during each boosting round:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 1*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [1, inf)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gives number of trees boosted in parallel*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Value greater than 1 turns booster into random forest*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By including multiple trees per round, the base learner is no longer a tree,
    but a forest. Since XGBoost includes the same hyperparameters as random forests,
    the base learner is appropriately classified as a random forest when `num_parallel_tree`
    exceeds 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how XGBoost random forest base learners work in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Call `regression_model` with `XGBRegressor` and set `booster=''gbtree''`. Additionally,
    set `num_parallel_tree=25` meaning that each boosted round consists of a forest
    of `25` trees:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The score is respectable, and in this case, it's nearly the same as boosting
    a single `gbtree`. The reason is that gradient boosting is designed to learn from
    the mistakes of the previous trees. By starting with a robust random forest, there
    is little to be learned and the gains are minimal at best.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Understanding the fundamental point that gradient boosting's strength as an
    algorithm comes from the learning process is essential. It makes sense, therefore,
    to try a much smaller value for `num_parallel_tree`, such as `5`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set `num_parallel_tree=5` inside the same regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Technically, this score is 0.002 points better than the score produced by a
    forest of 25 trees. Although the improvement is not much, generally speaking,
    when building XGBoost random forests, low values of `num_parallel_tree` are better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you have seen how random forests may be implemented as base learners
    within XGBoost, it's time to build random forests as original XGBoost models.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests as XGBoost models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to `XGBRegressor` and `XGBClassifier`, `XGBoost` also comes with
    `XGBRFRegressor` and `XGBRFClassifier` to build random forests.
  prefs: []
  type: TYPE_NORMAL
- en: According to the official XGBoost documentation at [https://xgboost.readthedocs.io/en/latest/tutorials/rf.html](https://xgboost.readthedocs.io/en/latest/tutorials/rf.html),
    the random forest scikit-learn wrapper is still in the experimentation stage and
    the defaults may be changed at any time. At the time of writing, in 2020, the
    following `XGBRFRegressor` and `XGBRFClassifier` defaults are included.
  prefs: []
  type: TYPE_NORMAL
- en: n_estimators
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Use `n_estimators` and not `num_parallel_tree` when using `XGBRFRegressor`
    or `XGBRFClassifier` to build a random forest. Keep in mind that when using `XGBRFRegressor`
    and `XGBRFClassifier`, you are not gradient boosting but bagging trees in one
    round only as is the case with a traditional random forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 100*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [1, inf)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automatically converted to num_parallel_tree for random forests*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learning_rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`learning_rate` is generally designed for models that learn, including boosters,
    not `XGBRFRegressor` or `XGBRFClassifier` since they consist of one round of trees.
    Nevertheless, changing `learning_rate` from 1 will change the scores, so modifying
    this hyperparameter is generally not advised:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Default: 1*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [0, 1]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: subsample, colsample_by_node
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scikit-learn''s random forest keeps these defaults at `1`, making the default
    `XGBRFRegressor` and `XGBRFClassifier` less prone to overfitting. This is the
    primary difference between the XGBoost and scikit-learn random forest default
    implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Defaults: 0.8*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range: [0, 1]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Decreasing helps prevent overfitting*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s see how XGBoost''s random forests work in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, place `XGBRFRegressor` inside of the `regression_model` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This score is a little better than the `gbtree` model presented earlier, and
    a little worse than the best linear models presented in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As a comparison, let''s see how `RandomForestRegressor` performs by placing
    it inside the same function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This score is slightly worse than `XGBRFRegressor`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now let''s compare the XGBoost random forest with scikit-learn''s standard
    random forest using the larger Census dataset for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Place `XGBRFClassifier` inside of the `classification_model` function to see
    how well it predicts user income:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a good score, a little off the mark from `gbtree`, which previously
    gave 87%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now place `RandomForestClassifier` inside the same function to compare results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is slightly worse than XGBoost's implementation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since XGBoost's random forests are still in the developmental stage, we'll stop
    here and analyze the results.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing XGBoost random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can try a random forest as your XGBoost base learner anytime by increasing
    `num_parallel_tree` to a value greater than `1`. Although, as you have seen in
    this section, boosting is designed to learn from weak models, not strong models,
    so values for `num_parallel_tree` should remain close to `1`. Trying random forests
    as base learners should be used sparingly. If boosting single trees fails to produce
    optimal scores, random forest base learners are an option.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, the XGBoost random forest's `XGBRFRegressor` and `XGBRFClassifier`
    may be implemented as alternatives to scikit-learn's random forests. XGBoost's
    new `XGBRFRegressor` and `XGBRFClassifier` outperformed scikit-learn's `RandomForestRegressor`
    and `RandomForestClassifier`, although the comparison was very close. Given the
    overall success of XGBoost in the machine learning community, it's definitely
    worth using `XGBRFRegressor` and `XGBRFClassifier` as viable options going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you greatly extended your range of XGBoost by applying all
    XGBoost base learners, including `gbtree`, `dart`, `gblinear`, and random forests,
    to regression and classification datasets. You previewed, applied, and tuned hyperparameters
    unique to base learners to improve scores. Furthermore, you experimented with
    `gblinear` using a linearly constructed dataset and with `XGBRFRegressor` and
    `XGBRFClassifier` to build XGBoost random forests without any boosting whatsoever.
    Now that you have worked with all base learners, your comprehension of the range
    of XGBoost is at an advanced level.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will analyze tips and tricks from Kaggle masters to
    advance your XGBoost skills even further!
  prefs: []
  type: TYPE_NORMAL

- en: Classification and Regression with Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tree based algorithms are very popular for two reasons: they are interpretable,
    and they make sound predictions that have won many machine learning competitions
    on online platforms, such as Kaggle. Furthermore, they have many use cases outside
    of machine learning for solving problems, both simple and complex.'
  prefs: []
  type: TYPE_NORMAL
- en: Building a tree is an approach to decision-making used in almost all industries.
    Trees can be used to solve both classification- and regression-based problems,
    and have several use cases that make them the go-to solution!
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is broadly divided into the following two sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each section will cover the fundamental theory of different types of tree based
    algorithms, along with their implementation in scikit-learn. By the end of this
    chapter, you will have learned how to aggregate several algorithms into an **ensemble**
    and have them vote on what the best prediction is.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, and Matplotlib ≥ 3.0.0 installed on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb)[.](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2SrPP7R](http://bit.ly/2SrPP7R)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification trees are used to predict a category or class. This is similar
    to the classification algorithms that you have learned about previously in this
    book, such as the k-nearest neighbors algorithm or logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly speaking, there are three tree based algorithms that are used to solve
    classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The random forest classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AdaBoost classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you will learn how each of these tree based algorithms works,
    in order to classify a row of data as a particular class or category.
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The decision tree is the simplest tree based algorithm, and serves as the foundation
    for the other two algorithms. Let''s consider the following simple decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93f38146-379b-4b23-8502-5318cb631c9f.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple decision tree
  prefs: []
  type: TYPE_NORMAL
- en: 'A decision tree, in simple terms, is a set of rules that help us classify observations
    into distinct groups. In the previous diagram, the rule could be written as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding decision tree perfectly divides the observations into two distinct
    groups. This is a characteristic of the ideal decision tree. The first box on
    the top is called the **root** of the tree, and is the most important feature
    of the tree when it comes to deciding how to group the observations.
  prefs: []
  type: TYPE_NORMAL
- en: The boxes under the root node are known as the **children**. In the preceding
    tree, the **children** are also the **leaf** nodes. The **leaf** is the last set
    of boxes, usually in the bottommost part of the tree. As you might have guessed,
    the decision tree represents a regular tree, but inverted, or upside down.
  prefs: []
  type: TYPE_NORMAL
- en: Picking the best feature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How does the decision tree decide which feature is the best? The best feature
    is one that offers the best possible split, and divides the tree into two or more
    distinct groups, depending on the number of classes or categories that we have
    in the data. Let''s have a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/019a959a-1aad-4ef8-841b-bfc4290a4c20.png)'
  prefs: []
  type: TYPE_IMG
- en: A decision tree showing a good split
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: The tree splits the data from the root node into two distinct groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the left-hand group, we see that there are two triangles and one circle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand group, we see that there are two circles and one triangle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the tree got the majority of each class into one group, we can say that
    the tree has done a good job when it comes to splitting the data into distinct
    groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s take a look at another example—this time, one in which the split is
    bad. Consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff259ed3-b1df-482f-aab2-76d4b8185849.png)'
  prefs: []
  type: TYPE_IMG
- en: A decision tree with a bad split
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: The tree splits the data in the root node into four distinct groups. This is
    bad in itself, as it is clear that there are only two categories (circle and triangle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Furthermore, each group has one triangle and one circle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no majority class or category in any one of the four groups. Each group
    has 50% of one category; therefore, the tree cannot come to a conclusive decision,
    unless it relies on more features, which then increases the complexity of the
    tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Gini coefficient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The metric that the decision tree uses to decide if the root node is called
    the *Gini coefficient*. The higher the value of this coefficient, the better the
    job that this particular feature does at splitting the data into distinct groups.
    In order to learn how to compute the Gini coefficient for a feature, let''s consider
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9879aa0-1163-476e-bbcf-1e026de8d171.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing the Gini coefficient
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: The feature splits the data into two groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the left-hand group, we have two triangles and one circle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, the Gini for the left-hand group is (2 triangles/3 total data points)^2+
    (1 circle/3 total data points)^2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To calculate this, do the following: ![](img/c7c76b50-1651-43c4-92b6-066af865a3eb.png)0.55.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A value of 0.55 for the Gini coefficient indicates that the root of this tree
    splits the data in such a way that each group has a majority category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A perfect root feature would have a Gini coefficient of 1\. This means that
    each group has only one class/category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A bad root feature would have a Gini coefficient of 0.5, which indicates that
    there is no distinct class/category in a group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In reality, the decision tree is built in a recursive manner, with the tree
    picking a random attribute for the root and then computing the Gini coefficient
    for that attribute. It does this until it finds the attribute that best splits
    the data in a node into groups that have distinct classes and categories.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the decision tree classifier in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to implement the decision tree classifier
    in scikit-learn. We will work with the same fraud detection dataset. The first
    step is to load the dataset into the Jupyter Notebook. We can do this by using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to split the data into training and test sets. We can do this
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now build the initial decision tree classifier on the training data,
    and test its accuracy on the test data, by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we import `DecisionTreeClassifier` from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then initialize a `DecisionTreeClassifier` object with two arguments. The
    first, `criterion`, is the metric with which the tree picks the most important
    features in a recursive manner, which, in this case, is the Gini coefficient.
    The second is `random_state`, which is set to 50 so that the model produces the
    same result every time we run it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we fit the model on the training data and evaluate its accuracy on
    the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameter tuning for the decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The decision tree has a plethora of hyperparameters that require fine-tuning
    in order to derive the best possible model that reduces the generalization error
    as much as possible. In this section, we will focus on two specific hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Max depth**: This is the maximum number of children nodes that can grow out
    from the decision tree until the tree is cut off. For example, if this is set
    to 3, then the tree will use three children nodes and cut the tree off before
    it can grow any more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Min samples leaf:** This is the minimum number of samples, or data points,
    that are required to be present in the leaf node. The leaf node is the last node
    of the tree. If this parameter is, for example, set to a value of 0.04, it tells
    the tree that it must grow until the last node contains 4% of the total samples
    in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to optimize the ideal hyperparameter and to extract the best possible
    decision tree, we use the `GridSearchCV` module from scikit-learn. We can set
    this up using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import the `GridSearchCV` module from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we create a dictionary of possible values for the hyperparameters and
    store it as `grid_params`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we create a `GridSearchCV` object with the decision tree classifier
    as the estimator; that is, the dictionary of hyperparameter values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set the `scoring` argument as `accuracy`, since we want to extract the accuracy
    of the best model found by `GridSearchCV`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then fit this grid object to the training data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then extract the best set of parameters using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code indicates that a maximum depth of 1 and a
    minimum number of samples at the leaf node of 0.02 are the best parameters for
    this data. We can use these optimal parameters and construct a new decision tree
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing the decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the best aspects of building and implementing a decision tree in order
    to solve problems is that it can be interpreted quite easily, using a decision
    tree diagram that explains how the algorithm that you built works. In order to
    visualize a simple decision tree for the fraud detection dataset, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We start by importing the required packages. The new packages here are the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StringIO`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Image`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`export_graphviz`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pydotplus`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The installations of the packages were covered in [Chapter 1](d81461f2-02a5-4154-a9b1-7a1f91882534.xhtml),
    *Introducing Machine Learning with scikit-learn*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we read in the dataset and initialize a decision tree classifier, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we fit the tree on the features and target, and then extract the feature
    names separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then visualize the decision tree using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `tree.export_graphviz()` function in order to construct the decision
    tree object, and store it in a variable called `data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This function uses a couple of arguments: `dt` is the decision tree that you
    built; `out_file` is set to `None`, as we do not want to send the tree visualization
    to any file outside our Jupyter Notebook; the `feature_names` are those we defined
    earlier; and `proportion` is, set to `True` (this will be explained in more detail
    later).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then construct a graph of the data contained within the tree so that we can
    visualize this decision tree graph by using the `pydotplus. graph_from_dot_data()`
    function on the `data` variable, which contains data about the decision tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we visualize the decision tree using the `Image()` function, by passing
    the graph of the decision tree to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This results in a decision tree like that illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39330691-35d8-4b9e-a633-54f1aa59bf15.png)'
  prefs: []
  type: TYPE_IMG
- en: The resultant decision tree
  prefs: []
  type: TYPE_NORMAL
- en: 'The tree might seem pretty complex to interpret at first, but it''s not! In
    order to interpret this tree, let''s consider the root node and the first two
    children only. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cd82ac6-bd08-4705-9ffb-c72ef6898597.png)'
  prefs: []
  type: TYPE_IMG
- en: A snippet of the decision tree
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In the root node, the tree has identified the 'step' feature as the feature
    with the highest Gini value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The root node makes the split in such a way that 0.71, or 71%, of the data falls
    into the non-fraudulent transactions, while 0.29, or 29%, of the transactions
    fall into the fraudulent transactions category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the step is greater than or equal to 7.5 (the right-hand side), then all
    of the transactions are classified as fraudulent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the step is less than or equal to 7.5 (the left-hand side), then 0.996, or
    99.6%, of the transactions are classified as non-fraudulent, while 0.004, or 0.4%,
    of the transactions are classified as fraudulent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the amount is greater than or equal to 4,618,196.0, then all of the transactions
    are classified as fraudulent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the amount is less than or equal to 4,618,196.0, then 0.996, or 99.6%, of
    the transactions are classified as non-fraudulent, while 0.004, or 0.4%, of the
    transactions are classified as fraudulent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note how the decision tree is simply a set of If-then rules, constructed in
    a nested manner.
  prefs: []
  type: TYPE_NORMAL
- en: The random forests classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you understand the core principles of the decision tree at a very foundational
    level, we will next explore what random forests are. Random forests are a form
    of *ensemble* learning. An ensemble learning method is one that makes use of multiple
    machine learning models to make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85f521c4-4bb7-43af-afbb-d53dece02a4f.png)'
  prefs: []
  type: TYPE_IMG
- en: The concept of ensemble learning
  prefs: []
  type: TYPE_NORMAL
- en: 'The random forest algorithm operates as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume that you initially have a dataset with 100 features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From this, we will build a decision tree with 10 features initially. The features
    are selected randomly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, using a random selection of the remaining 90 features, we construct the
    next decision tree, again with 10 features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process continues until there are no more features left to build a decision
    tree with.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point in time, we have 10 decision trees, each with 10 features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each decision tree is known as the **base estimator** of the random forest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, we have a forest of trees, each built using a random set of 10 features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step for the algorithm is to make the prediction. In order to better
    understand how the random forest algorithm makes predictions, consider the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33763702-7826-4c87-a6b2-f1d7a923426d.png)'
  prefs: []
  type: TYPE_IMG
- en: The process of making predictions in random forests
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that there are 10 decision trees in the random forest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each decision tree makes a single prediction for the data that comes in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If six trees predict class A, and four trees predict class B, then the final
    prediction of the random forest algorithm is class A, as it had the majority vote.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process of voting on a prediction, based on the outputs of multiple models,
    is known as ensemble learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you have learned how the algorithm works internally, we can implement
    it using scikit-learn!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the random forest classifier in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will implement the random forest classifier in scikit-learn.
    The first step is to read in the data, and split it into training and test sets.
    This can be done by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build the random forest classifier. We can do that using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import `RandomForestClassifier` from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we initialize a random forest classifier model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then fit this model to our training data, and evaluate its accuracy on the
    test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameter tuning for random forest algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to optimize the hyperparameters of the random
    forest algorithm. Since random forests are fundamentally based on multiple decision
    trees, the hyperparameters are very similar. In order to optimize the hyperparameters,
    we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import the `GridSearchCV` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We initialize a dictionary of hyperparameter values. The `max_depth` and `min_samples_leaf`
    values are similar to those of the decision tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, `n_estimators` is a new parameter, covering the total number of trees
    that you want your random forest algorithm to consider while making the final
    prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then build and fit the `gridsearch` object to the training data and extract
    the optimal parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The best model is then extracted using these optimal hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The AdaBoost classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the section, you will learn how the AdaBoost classifier works internally,
    and how the concept of boosting might be used to give you better results. Boosting
    is a form of ensemble machine learning, in which a machine learning model learns
    from the mistakes of the models that were previously built, thereby increasing
    its final prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost stands for Adaptive Boosting, and is a boosting algorithm in which
    a lot of importance is given to the rows of data that the initial predictive model
    got wrong. This ensures that the next predictive model will not make the same
    mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process by which the AdaBoost algorithm works is illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0a5e241-8f39-4f5c-a0df-b6a79aab1801.png)'
  prefs: []
  type: TYPE_IMG
- en: An outline of the AdaBoost algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram of the AdaBoost algorithm, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: The first decision tree is built and outputs a set of predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The predictions that the first decision tree got wrong are given a weight of
    `w`. This means that, if the weight is set to 2, then two instances of that particular
    sample are introduced into the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This enables decision tree 2 to learn at a faster rate, since we have more samples
    of the data in which an error was made beforehand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is repeated until all the trees are built.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the predictions of all the trees are gathered, and a weighted vote
    is initiated in order to determine the final prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing the AdaBoost classifier in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how we can implement the AdaBoost classifier
    in scikit-learn in order to predict if a transaction is fraudulent or not. As
    usual, the first step is to import the data and split it into training and testing
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build the AdaBoost classifier. We can do this using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import the `AdaBoostClassifier` package from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we initialize a decision tree that forms the base of our AdaBoost classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then build the AdaBoost classifier, with the base estimator as the decision
    tree, and we specify that we want 100 decision trees in total.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we fit the classifier to the training data, and extract the accuracy
    scores from the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameter tuning for the AdaBoost classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to tune the hyperparameters of the AdaBoost
    classifier. The AdaBoost classifier has only one parameter of interest—the number
    of base estimators, or decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can optimize the hyperparameters of the AdaBoost classifier using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import the `GridSearchCV` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We initialize a dictionary of hyperparameter values. In this case, `n_estimators`
    is the number of decision trees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then build and fit the `gridsearch` object to the training data and extract
    the best parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The best model is then extracted using these optimal hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regression trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have learned how trees are used in order to classify a prediction as belonging
    to a particular class or category. However, trees can also be used to solve problems
    related to predicting numeric outcomes. In this section, you will learn about
    the three types of tree based algorithms that you can implement in scikit-learn
    in order to predict numeric outcomes, instead of classes:'
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree regressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The random forest regressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient boosted tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision tree regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have data that is non-linear in nature, a linear regression model might
    not be the best model to choose. In such situations, it makes sense to choose
    a model that can fully capture the non-linearity of such data. A decision tree
    regressor can be used to predict numeric outcomes, just like that of the linear
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the decision tree regressor, we use the mean squared error, instead
    of the Gini metric, in order to determine how the tree is built. You will learn
    about the mean squared error in detail in [Chapter 8](99286f39-a802-4285-a217-547b2ff62d71.xhtml),
    *Performance Evaluation Methods*. In a nutshell, the mean squared error is used
    to tell us about the prediction error rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the tree shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42ed3bf2-2be0-462f-8fdf-66e18617c5db.png)'
  prefs: []
  type: TYPE_IMG
- en: An example decision tree for regression
  prefs: []
  type: TYPE_NORMAL
- en: 'When considering the preceding diagram of the decision tree, note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We are trying to predict the amount of a mobile transaction using the tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the tree tries to decide on a split, it chooses the node in such a way
    that the target value is closest to the mean values of the target in that node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will notice that, as you go down the tree to the left, along the `True`
    cases, the mean squared error of the nodes decreases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the nodes are built in a recursive fashion, such that it reduces
    the overall mean squared error, thereby obtaining the `True` value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding tree, if the old balance of origination is less than 600,281,
    then the amount (here, coded as `value`) is 80,442, and if it's greater than 600,281,
    then the amount is 1,988,971.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the decision tree regressor in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to implement the decision tree regressor
    in scikit-learn. The first step is to import the data, and create the features
    and target variables. We can do this using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note how, in the case of regression, the target variable is the amount, and
    not the `isFraud` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we split the data into training and test sets, and build the decision
    tree regressor, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import the required packages and split the data into training and test
    sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we build the decision tree regressor using the `DecisionTreeRegressor()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We specify two hyperparameter arguments: `max_depth`, which tells the algorithm
    how many branches the tree must have, and `min_sample_leaf`, which tells the tree
    about the minimum number of samples that each node must have. The latter is set
    to 20%, or 0.2 of the total data, in this case.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`random_state` is set to 50 to ensure that the same tree is built every time
    we run the code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then fit the tree to the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualizing the decision tree regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as we visualized the decision tree classifier, we can also visualize the
    decision tree regressor. Instead of showing you the classes or categories to which
    the node of a tree belongs, you will now be shown the value of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize the decision tree regressor by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The code follows the exact same methodology as that of the decision tree classifier,
    and will not be discussed in detail here. This produces a decision tree regressor
    like that in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e84a5850-7db2-49e0-b8fa-b16ba817ec8d.png)'
  prefs: []
  type: TYPE_IMG
- en: A visualization of the decision tree regressor
  prefs: []
  type: TYPE_NORMAL
- en: The random forest regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The random forest regressor takes the decision tree regressor as the base estimator,
    and makes predictions in a method similar to that of the random forest classifier,
    as illustrated by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0365b85-8717-4817-9b84-c3324b1c7c59.png)'
  prefs: []
  type: TYPE_IMG
- en: Making the final prediction in the random forest regressor
  prefs: []
  type: TYPE_NORMAL
- en: The only difference between the random forest classifier and the random forest
    regressor is the fact that, in the case of the latter, the base estimator is a
    decision tree regressor.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the random forest regressor in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how you can implement the random forest regressor
    in scikit-learn. The first step is to import the data and split it into training
    and testing sets. This can be done using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build the random forest regressor. We can do this using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import the `RandomForestRegressor` module from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then initialize a random forest regressor object, called `rf_reg`, with a
    maximum depth of 10 for each decision tree, and the minimum number of data and
    samples in each tree as 20% of the total data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then fit the tree to the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradient boosted tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how the gradient boosted tree is used for regression,
    and how you can implement this using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the AdaBoost classifier that you learned about earlier in this chapter,
    weights are added to the examples that the classifier predicted in correctly.
    In the gradient boosted tree, however, instead of weights, the residual errors
    are used as labels in each tree in order to make future predictions. This concept
    is illustrated for you in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d3486ae-2764-4bc3-8d70-ac0625150960.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is what occurs in the preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: The first decision tree is trained with the data that you have, and the target
    variable **Y**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then compute the residual error for this tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The residual error is given by the difference between the predicted value and
    the actual value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second tree is now trained, using the residuals as the target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process of building multiple trees is iterative, and continues for the
    number of base estimators that we have.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final prediction is made by adding the target value predicted by the first
    tree to the product of the shrinkage and the residuals for all the other trees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The shrinkage is a factor with which we control the rate at which we want this
    gradient boosting process to take place.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A small value of shrinkage (learning rate) implies that the algorithm will learn
    more quickly, and therefore, must be compensated with a larger number of base
    estimators (that is, decision trees) in order to prevent overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A larger value of shrinkage (learning rate) implies that the algorithm will
    learn more slowly, and thus requires fewer trees in order to reduce the computational
    time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing the gradient boosted tree in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how we can implement the gradient boosted regressor
    in scikit-learn. The first step, as usual, is to import the dataset, define the
    features and target arrays, and split the data into training and test sets. This
    can be done using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build the gradient boosted regressor. This can be done
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import `GradientBoostingRegressor` from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We the build a gradient boosted regressor object with three main arguments:
    the maximum depth of each tree, the total number of trees, and the learning rate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then fit the regressor on the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensemble classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of ensemble learning was explored in this chapter, when we learned
    about random forests, AdaBoost, and gradient boosted trees. However, this concept
    can be extended to classifiers outside of trees.
  prefs: []
  type: TYPE_NORMAL
- en: If we had built a logistic regression, random forest, and k-nearest neighbors
    classifiers, and we wanted to group them all together and extract the final prediction
    through majority voting, then we could do this by using the ensemble classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concept can be better understood with the aid of the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15463931-a9d9-4b97-aec4-75fa2bfc4f6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Ensemble learning with a voting classifier to predict fraud transactions
  prefs: []
  type: TYPE_NORMAL
- en: 'When examining the preceding diagram, note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The random forest classifier predicted that a particular transaction was fraudulent,
    while the other two classifiers predicted that the transaction was not fraudulent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The voting classifier sees that two out of three (that is, a majority) of the
    predictions are **Not Fraud**, and hence, outputs the final prediction as **Not
    Fraud**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the voting classifier in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to implement the voting classifier in scikit-learn.
    The first step is to import the data, create the feature and target arrays, and
    create the training and testing splits. This can be done using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will build two classifiers that include the voting classifier: the
    decision tree classifier and the random forest classifier. This can be done using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will build the voting classifier by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We first import the `VotingClassifier` module from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we create a list of all the models that we want to use in our voting classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the list of classifiers, each model is stored in a tuple, along with the
    model's name in a string and the model itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then initialize a voting classifier with the list of models built in step
    2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the model is fitted to the training data and the accuracy is extracted
    from the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While this chapter was rather long, you have entered the world of tree based
    algorithms, and left with a wide arsenal of tools that you can implement in order
    to solve both small- and large-scale problems. To summarize, you have learned
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to use decision trees for classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use random forests for classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use AdaBoost for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use gradient boosted trees for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the voting classifier can be used to build a single model out of different
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the upcoming chapter, you will learn how you can work with data that does
    not have a target variable or labels, and how to perform unsupervised machine
    learning in order to solve such problems!
  prefs: []
  type: TYPE_NORMAL

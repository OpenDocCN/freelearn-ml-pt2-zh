<html><head></head><body>
    <section>
      <header class="header-title chapter-title">
                    Making Decisions with Trees
                </header>
      <article>
        <p>In this chapter, we are going to start by looking at our first supervised learning algorithm—decision trees. The decision tree algorithm is versatile and easy to understand. It is widely used and also serves as a building block for the numerous advanced algorithms that we will encounter later on in this book. In this chapter, we will learn how to train a decision tree and use it for either classification or regression problems. We will also understand the details of its learning process in order to know how to set its different hyperparameters. Furthermore, we will use a real-world dataset to apply what we are going to learn here in practice. We will start by getting and preparing the data and apply our algorithm to it. Along the way, we will also try to understand key machine learning concepts, such as cross-validation and model evaluation metrics. By the end of this chapter, you will have a very good understanding of the following topics:</p>
        <ul>
          <li>Understanding decision trees</li>
          <li>How do decision trees learn?</li>
          <li>Getting a more reliable score</li>
          <li>Tuning the hyperparameters for higher accuracy</li>
          <li>Visualizing the tree's decision boundaries</li>
          <li>Building decision tree regressors</li>
        </ul>
        <h1 id="uuid-bd7e3489-957d-4a3e-bf66-7898b34ef0f4">Understanding decision trees</h1>
        <p>I chose to start this book with decision trees because I've noticed that the majority of new machine learning practitioners have previous experience in one of two fields—software development, or statistics and mathematics. Decision trees can conceptually resemble some of the concepts software developers are used to, such as nested <kbd>if-else</kbd> conditions and binary search trees. As for the statisticians, bear with me—soon, you will feel at home when we reach the chapter about linear models.</p>
        <h2 id="uuid-6a4a7195-9ddf-467b-954f-fb4f72368f7c" class="p1">What are decision trees?</h2>
        <p class="p1">I think the best way to explain what decision trees are is by showing the rules they generate after they are trained. Luckily, we can access those rules and print them. Here is an example of how decision tree rules look:</p>
        <pre>Shall I take an umbrella with me?<br/>|--- Chance of Rainy &lt;= 0.6<br/>|    |--- UV Index &lt;= 7.0<br/>|    |    |--- class: False<br/>|    |--- UV Index &gt;  7.0<br/>|    |    |--- class: True<br/>|--- Chance of Rainy &gt;  0.6<br/>|    |--- class: True</pre>
        <p>As you can see, it's basically a set of conditions. If the chance of rain falling is above <kbd>0.6</kbd> (60%), then I need to take an umbrella with me. If it is below <kbd>0.6</kbd>, then it all depends on the UV index. If the UV index is above <kbd>7</kbd>, then an umbrella is needed; otherwise, I will be fine without one. Now, you might be thinking <q>well, a few nested <kbd>if-else</kbd> conditions will do the trick.</q> True, but the main difference here is that I didn't write any of these conditions myself. The algorithm just learned the preceding conditions automatically after it went through the following data:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/cd239667-c242-4ff7-8ac2-2ced3473d72d.png" style="width:20.75em;"/>
        </p>
        <p class="p1">Of course, for this simple case, anyone can manually go through the data and come up with the same conditions. Nevertheless, when dealing with a bigger dataset, the number of conditions we need to program will quickly grow with the number of columns and the values in each column. At such a scale, it is not possible to manually perform the same job, and an algorithm that can learn the conditions from the data is needed.</p>
        <p class="p1">Conversely, it is also possible to map a constructed tree back to the nested <kbd>if-else</kbd> conditions. This means that you can use Python to build a tree from data, then export the underlying conditions to be implemented in a different language or even to put them in <strong>Microsoft Excel</strong> if you want.</p>
        <h2 id="uuid-bad8fcc0-9899-4eb4-bc39-66449c6c8785" class="p1">Iris classification</h2>
        <p class="p1">scikit-learn comes loaded with a number of datasets that we can use to test new algorithms. One of these datasets is the Iris set. Iris is a genus of 260–300 species of flowering plants with showy flowers. However, in our dataset, just three species are covered—<strong>Setosa</strong>, <strong>Versicolor</strong>, and <strong>Virginica</strong>. Each example in our dataset has the length and the widths of the sepal and petal of each plant (the features), along with whether it is a Setosa, a Versicolor, or a Virginica (the target). Our task is to be able to identify the species of a plant given its sepal and petal dimensions. Clearly, this is a classification problem. It is a supervised learning problem since the targets are provided with the data. Furthermore, it is a classification problem since we take a limited number of predefined values (three species). </p>
        <h3 id="uuid-531ed794-c4c5-47c5-8c16-506917336869" class="p1">Loading the Iris dataset</h3>
        <p>Let's now startby loading the dataset:</p>
        <ol>
          <li>We import the dataset's module from scikit-learn, and then load the Iris data into a variable, which we are going to call <kbd>iris</kbd> as well:</li>
        </ol>
        <pre style="padding-left: 60px" class="p1">from sklearn import datasets<br/>import pandas as pd<br/>iris = datasets.load_iris()</pre>
        <ol start="2">
          <li>Using<kbd>dir</kbd>, we can see what methods and attributes the dataset provides:</li>
        </ol>
        <pre style="padding-left: 60px" class="p1">dir(iris)</pre>
        <p style="padding-left: 60px" class="p1">We get a list of the <kbd>DESCR</kbd>, <kbd>data</kbd>, <kbd>feature_names</kbd>, <kbd>filename</kbd>, <kbd>target</kbd>, and <kbd>target_names</kbd>methods.</p>
        <p style="padding-left: 60px" class="p1">It's nice of the data creators to provide descriptions with each one, which we can access using <kbd>DESCR</kbd>.<em/>This is rarely the case with real-life data, however. Usually, in real life, we need to talk to the people who produced the data in the first place to understand what each value means, or at least use some descriptive statistics to understand the data before using it.</p>
        <ol start="3">
          <li class="p1">For now, let's print the Iris data's description:</li>
        </ol>
        <pre style="padding-left: 60px" class="p1">print(iris.DESCR)</pre>
        <p style="padding-left: 60px">Have a look at the description now and try to think of some of the main takeaways from it. I will list my own takeaways afterward: </p>
        <pre style="padding-left: 60px">.. _iris_dataset:<br/> Iris plants dataset<br/> --------------------<br/><strong>Data Set Characteristics:<br/></strong> :Number of Instances: 150 (50 in each of three classes)<br/>  :Number of Attributes: 4 numeric, predictive attributes and the class<br/>   :Attribute Information:        <br/>- sepal length in cm<br/>- sepal width in cm<br/>- petal length in cm<br/>- petal width in cm<br/>- class:<br/>    - Iris-Setosa<br/>    - Iris-Versicolor<br/>    - Iris-Virginica<br/>:Summary Statistics:<br/>    ============== ==== ==== ======= ===== ====================<br/>                Min  Max   Mean    SD     Class   Correlation<br/>    ============== ==== ==== ======= ===== ====================<br/>sepal length:   4.3  7.9   5.84   0.83    0.7826<br/>sepal width:    2.0  4.4   3.05   0.43   -0.4194<br/>petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)<br/>petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)<br/>petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)<br/>    ============== ==== ==== ======= ===== ====================<br/>:Missing Attribute Values: None<br/>:Class Distribution: 33.3% for each of 3 classes.<br/><br/>:Creator: R.A. Fisher</pre>
        <p style="padding-left: 60px">This description holds some useful information for us, and I found the following points the most interesting:</p>
        <ul>
          <li style="padding-left: 30px">The data is composed of 150 rows (or 150 samples). This is a reasonably small dataset. Later on, we will see how to deal with this fact when evaluating our model.</li>
          <li style="padding-left: 30px">The class labels or targets take three values—<kbd>Iris-Setosa</kbd>, <kbd>Iris-Versicolor</kbd>, and <kbd>Iris-Virginica</kbd>. Some classification algorithms can only deal with two class labels; we call them binary classifiers. Luckily, the decision tree algorithm can deal with more than two classes, so we have no problems this time.</li>
        </ul>
        <ul>
          <li style="padding-left: 30px">The data is balanced; there are 50 samples for each class. This is something we need to keep in mind when training and evaluating our model later on.</li>
          <li style="padding-left: 30px">We have four features—<kbd>sepal length</kbd>, <kbd>sepal width</kbd>, <kbd>petal length</kbd>, and <kbd>petal width</kbd>—and all four features are numeric. In <a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=26&amp;action=edit">Chapter 3</a>, <em>Preparing Your Data</em>, we will learn how to deal with non-numeric data.</li>
          <li style="padding-left: 30px">There are no missing attribute values. In other words, none of our samples contains null values. Later on in this book, we will learn how to deal with missing values if we encounter them.</li>
          <li style="padding-left: 30px">The petal dimensions correlate with the class values more than the sepal dimensions. I wish we had never seen this piece of information. Understanding your data is useful, but the problem here is that this correlation is calculated for the entire dataset. Ideally, we will only calculate it for our training data. Anyway, let's ignore this information for now and just use it for a sanity check later on.</li>
        </ul>
        <ol start="4">
          <li>It's time to put all the dataset information into one DataFrame.</li>
        </ol>
        <p style="padding-left: 60px">The <kbd>feature_names</kbd>methodreturns the names of our features, while the <kbd>data</kbd> method returns their values in the form of a NumPy array. Similarly, the <kbd>target</kbd>variable has the values of the target in the form of zeros, ones, and twos, and <kbd>target_names</kbd> maps <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> to<kbd>Iris-Setosa</kbd>, <kbd>Iris-Versicolor</kbd>, and <kbd>Iris-Virginica</kbd>, respectively.</p>
        <div class="packt_tip">NumPy arrays are efficient to deal with, but they do not allow columns to have names. I find column names to be useful for debugging purposes. I find <kbd>pandas</kbd> DataFrames to be more suitable here since we can use column names and combine the features and target into one DataFrame.</div>
        <p style="padding-left: 60px">Here, we can see the first eight rows we get using<kbd>iris.data[:8]</kbd>:</p>
        <pre style="padding-left: 60px">array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2], [5.4, 3.9, 1.7, 0.4], [4.6, 3.4, 1.4, 0.3], [5. , 3.4, 1.5, 0.2]])</pre>
        <p style="padding-left: 60px" class="p1">The following code uses the <kbd>data</kbd>, <kbd>feature_names</kbd>, and <kbd>target</kbd> methods to combine all the dataset information into one DataFrame and assign its column names accordingly:</p>
        <pre style="padding-left: 60px" class="mce-root">df = pd.DataFrame(<br/>    iris.data,<br/>    columns=iris.feature_names<br/>)<br/><br/>df['target'] = pd.Series(<br/> iris.target<br/>)</pre>
        <div class="packt_infobox">scikit-learn versions 0.23 and up support loading datasets as <kbd>pandas</kbd> DataFrames right away. You can do this by setting <kbd>as_frame=True</kbd> in <kbd>datasets.load_iris</kbd> and its similar data-loading methods. Nevertheless, this has not been tested in this book since version 0.22 is the most stable release at the time of writing.</div>
        <ol start="5">
          <li>The <kbd>target</kbd> column now has the class IDs. However, for more clarity, we can also create a new column called <kbd>target_names</kbd>, where we can map our numerical target values to the class names:</li>
        </ol>
        <pre style="padding-left: 60px">df['target_names'] = df['target'].apply(lambda y: iris.target_names[y])</pre>
        <ol start="6">
          <li>Finally, let's print a sample of six rows to see how our new DataFrame looks. Running the following code in a Jupyter notebook or a Jupyter lab will just print the contents of the DataFrame; otherwise, you need to surround your code with a <kbd>print</kbd> statement. I will assume that a Jupyter notebook environment is used in all later code snippets:</li>
        </ol>
        <pre style="padding-left: 60px"># print(df.sample(n=6))<br/>df.sample(n=6)</pre>
        <p>This gave me the following random sample:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/4904624a-669a-4eb1-b625-1e3e15cf59b7.png" style="width:39.58em;"/>
        </p>
        <p>The sample methods picked six random rows to display. This means that you will get a different set of rows each time you run the same code. Sometimes, we need to get the same random results every time we run the same code. Then, we use a pseudo-random number generator with a preset seed. A pseudo-random number generator initialized with the same seed will produce the same results every time it runs.</p>
        <p>So, set the<kbd>random_state</kbd>parameter in the<kbd>sample()</kbd>method to <kbd>42</kbd>, as follows:</p>
        <pre>df.sample(n=6, random_state=42) </pre>
        <p>You will get the exact same rows shown earlier.</p>
        <h3 id="uuid-7074c659-7af4-40a5-9820-41dcea48944c">Splitting the data</h3>
        <p>Let's split the DataFrame we have just created into two—70% of the records (that is, 105 records) should go into the training set, while 30% (45 records) should go into testing. The choice of 70/30 is arbitrary for now. We will use the <kbd>train_test_split()</kbd> function provided by scikit-learn and specify<kbd>test_size</kbd> to be <kbd>0.3</kbd>:</p>
        <pre>from sklearn.model_selection import train_test_split<br/>df_train, df_test = train_test_split(df, test_size=0.3)</pre>
        <p>We can use<kbd>df_train.shape[0]</kbd>and <kbd>df_test.shape[0]</kbd>to check how many rows thereare in the newly created DataFrames. We can also list the columns of the new DataFrames using <kbd>df_train.columns</kbd> and <kbd>df_test.columns</kbd>. They both have the same six columns:</p>
        <ul>
          <li>
            <kbd>sepal length (cm)</kbd>
          </li>
          <li>
            <kbd>sepal width (cm)</kbd>
          </li>
          <li>
            <kbd>petal length (cm)</kbd>
          </li>
          <li>
            <kbd>petal width (cm)</kbd>
          </li>
          <li>
            <kbd>target</kbd>
          </li>
          <li>
            <kbd>target_names</kbd>
          </li>
        </ul>
        <p>The first four columns are our features, while the fifth column is our target (or label). The sixthcolumn will not be needed for now. Visually, you could say that we have split our data vertically into training and test sets. Usually, it makes sense to further split each of our DataFrames horizontally into two parts—one part for the features, which we usually call <em>x</em>, and another part for the targets, which is usually called <em>y</em>. We will continue to use this <em>x</em> and <em>y</em> naming convention throughout the rest of this book.</p>
        <div class="packt_infobox">Some prefer to use a capital <em>X</em> to illustrate that it is a two-dimensional array (or DataFrames) and use a small letter for <em>y</em> when it is a single-dimensional array (or series). I find it more practical to stick to a single case. </div>
        <p>As you know, the<kbd>feature_names</kbd> method in<kbd>iris</kbd> contains a list of the corresponding column names to our features. We will use this information, along with the <kbd>target</kbd> label, to create our <em>x</em> and <em>y</em> sets, as follows:</p>
        <pre>x_train = df_train[iris.feature_names]<br/>x_test = df_test[iris.feature_names]<br/><br/>y_train = df_train['target']<br/>y_test = df_test['target']</pre>
        <h3 id="uuid-0ec1aa7d-e3cb-4c25-871e-c3ed6d9778f1">Training the model and using it for prediction</h3>
        <p>To get a feel for how everything works, we will train our algorithm using its default configuration for now. Later on in this chapter, I will explain the details of the decision tree algorithms and how to configure them. </p>
        <p>We need to import <kbd>DecisionTreeClassifier</kbd> first, and then create an instance of it, as follows:</p>
        <pre>from sklearn.tree import DecisionTreeClassifier<br/><br/># It is common to call the classifier instance clf<br/>clf = DecisionTreeClassifier()</pre>
        <p>One commonly used synonym for training is fitting. This is how an algorithm uses the training data (<em>x</em> and <em>y</em>) to learn its parameters. All scikit-learn models implement a<kbd>fit()</kbd>method that takes<kbd>x_train</kbd>and<kbd>y_train</kbd>, and <kbd>DecisionTreeClassifier</kbd> is no different:</p>
        <pre>clf.fit(x_train, y_train)</pre>
        <p>By calling the <kbd>fit()</kbd> method, the <kbd>clf</kbd> instance is trained and ready to be used for predictions. We then call the<kbd>predict()</kbd>method on<kbd>x_test</kbd>:</p>
        <pre class="mce-root"># If y_test is our truth, then let's call our predictions y_test_pred<br/>y_test_pred = clf.predict(x_test)</pre>
        <p>When predicting, we usually don't know the actual targets (<em>y</em>) for our features (<em>x</em>). That's why we only provide the <kbd>predict()</kbd> method here with<kbd>x_test</kbd>. In this particular case, we happened to know<kbd>y_test</kbd>; nevertheless, we will pretend that we don't know it for now, and only use it later on for evaluation. As our actual targets are called <kbd>y_test</kbd>, we will call the predicted ones <kbd>y_test_pred</kbd> and compare the two later on. </p>
        <h3 id="uuid-7608df60-685b-4593-b610-3bfb0ac61d05">Evaluating our predictions</h3>
        <p>As we have <kbd>y_test_predict</kbd>, all we need now is to compare it to<kbd>y_test</kbd> to check how good our predictions are. If you remember from the previous chapter, there are multiple metrics for evaluating a classifier, such as<kbd>precision</kbd>,<kbd>recall</kbd>, and<kbd>accuracy</kbd>. The Iris dataset is a balanced dataset; it has the same number of instances for each class. Therefore, it is apt to use the accuracy metric here. </p>
        <p>Calculating the accuracy, as follows, gives us a score of<kbd>0.91</kbd>:</p>
        <pre>from sklearn.metrics import accuracy_score<br/>accuracy_score(y_test, y_test_pred)</pre>
        <div class="packt_infobox">Did you get a different score than mine? Don't worry. In the <em>Getting a more reliable score</em> section, I will explain why the accuracy score calculated here may vary.</div>
        <p>Congratulations! You've just trained your first supervised learning algorithm. From now on, all the algorithms we are going to use in this book have a similar interface: </p>
        <ul>
          <li>The <kbd>fit()</kbd> method takes the <em>x</em> and <em>y</em> parts of your training data.</li>
          <li>The <kbd>predict()</kbd> method takes <em>x</em> only and returns a predicted <em>y</em>.</li>
        </ul>
        <h3 id="uuid-29eb8aa3-dff8-4974-919b-3c6f4c026bab">Which features were more important?</h3>
        <p>We may now ask ourselves,<em>Which features did the model find more useful in deciding the iris species?</em> Luckily,<kbd>DecisionTreeClassifier</kbd> has a method called <kbd>feature_importances_</kbd>, which is computed after the classifier is fitted and scores how important each feature is to the model's decision. In the following code snippet, we will create a DataFrames where we will put the features' names and their importance together and then sort the features by their importance:</p>
        <pre>pd.DataFrame(<br/>  {<br/>    'feature_names': iris.feature_names,<br/>    'feature_importances': clf.feature_importances_<br/>  }<br/>).sort_values(<br/>  'feature_importances', ascending=False<br/>).set_index('feature_names')</pre>
        <p>This is the output we get:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/7886cc3b-3c70-4db3-ac76-e483c6f2140e.png" style="width:20.67em;"/>
        </p>
        <p>As you will recall, when we printed the dataset's description, the petal length and width values started to correlate highly with the target. They also have high feature importance scores here, which confirms what is stated in the description.</p>
        <h3 id="uuid-4bfb3ba2-1f74-44aa-9c3c-882d6894b193">Displaying the internal tree decisions </h3>
        <p>We can also print the internal structure of the learned tree using the following code snippet: </p>
        <pre>from sklearn.tree import export_text<br/>print(<br/>  export_text(clf, feature_names=iris.feature_names, spacing=3, decimals=1)<br/>) </pre>
        <p>This will print the following text:</p>
        <pre>|--- petal width (cm) &lt;= 0.8<br/>| |--- class: 0<br/>|--- petal width (cm) &gt; 0.8<br/>| |--- petal width (cm) &lt;= 1.8<br/>| | |--- petal length (cm) &lt;= 5.3<br/>| | | |--- sepal length (cm) &lt;= 5.0<br/>| | | | |--- class: 2<br/>| | | |--- sepal length (cm) &gt; 5.0<br/>| | | | |--- class: 1<br/>| | |--- petal length (cm) &gt; 5.3<br/>| | | |--- class: 2<br/>| |--- petal width (cm) &gt; 1.8<br/>| | |--- class: 2</pre>
        <p class="mce-root">If you print the complete dataset description, you will notice that toward the end, it says the following:</p>
        <p class="mce-root">One class is linearly separable from the other two; the latter are NOT linearly separable from each other.</p>
        <p>This means that one class is easier to separate from the other two, while the other two are harder to separate from each other. Now, look at the internal tree's structure. You may notice that in the first step, it decided that anything with a petal width below or equal to <kbd>0.8</kbd> belongs to class <kbd>0</kbd> (<kbd>Setosa</kbd>). Then, for petal widths above <kbd>0.8</kbd>, the tree kept on branching, trying to differentiate between classes <kbd>1</kbd> and <kbd>2</kbd> (<kbd>Versicolor</kbd> and <kbd>Virginica</kbd>). Generally, the harder it is to separate classes, the deeper the branching goes. </p>
        <h1 id="uuid-ba3d626d-2cd7-4719-be8e-501f4cde2cce">How do decision trees learn? </h1>
        <p>It's time to find out how decision trees actually learn in order to configure them. In the internal structure we just printed, the tree decided to use a petal width of <kbd>0.8</kbd> as its initial splitting decision. This was done because decision trees try to build the smallest possible tree using the following technique.</p>
        <p>It went through all the features trying to find a feature (<kbd>petal width</kbd>, here) and a value within that feature (<kbd>0.8</kbd>, here) so that if we split all our training data into two parts (one for <kbd>petal width ≤ 0.8</kbd>, and one for <kbd>petal width &gt; 0.8</kbd>), we get the purest split possible. In other words, it tries to find a condition where we can separate our classes as much as possible. Then, for each side, it iteratively tries to split the data further using the same technique. </p>
        <h2 id="uuid-8da45a0d-7b57-4833-95cd-130ec6a9bcaf">Splitting criteria</h2>
        <p>If we onlyhad two classes, an ideal split would put members of one class on one side and members of the others on the other side. In our case, we succeeded in putting members of class <kbd>0</kbd> on one side and members of classes <kbd>1</kbd> and <kbd>2</kbd> on the other. Obviously, we are not always guaranteed to get such a pure split. As we can see in the other branches further down the tree, we always had a mix of samples from classes <kbd>1</kbd> and <kbd>2</kbd> on each side.</p>
        <p>Having said that, we need a way to measure purity. We need a criterion based on if one split is purer than the other. There are two criteria that scikit-learn uses for classifiers' purity—<kbd>gini</kbd> and <kbd>entropy</kbd>—with the <kbd>gini</kbd> criterion as its default option. When it comes to decision tree regression, there are other criteria that we will come across later on.</p>
        <h2 id="uuid-3eb24af0-ccf9-429b-b2de-00a0ef8127b8">Preventing overfitting</h2>
        <div class="packt_quote">"If you look for perfection, you'll never be content."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– Leo Tolstoy</div>
        <p>After the first split, the tree went on to try to separate between the remaining classes; the <kbd>Versicolor</kbd> and the <kbd>Virginica</kbd> irises. However, are we really sure that our training data is detailed enough to explain all the nuances that differentiate between the two classes? Isn't it possible that all those branches are driving the algorithm to learn things that happen to exist in the training data, but will not generalize well enough when faced with future data? Allowing a tree to grow so much results in what is called overfitting. The tree tries to perfectly fit the training data, forgetting that the data it may encounter in the future may be different. To prevent overfitting, the following settings may be used to limit the growth of a tree:</p>
        <ul>
          <li><kbd>max_depth</kbd>:This is the maximum depth a tree can get to. A lower number means that the tree will stop branching earlier. Setting it to <kbd>None</kbd> means that the tree will continue to grow until all the leaves are pure or until all the leaves contain fewer than the <kbd>min_samples_split</kbd> samples.</li>
          <li><kbd>min_samples_split</kbd>: The minimum number of samples needed in a level to allow further splitting there. A higher number means that the tree will stop branching earlier.</li>
          <li><kbd>min_samples_leaf</kbd>:<strong/>The minimum number of samples needed in a level to allow it to become a leaf node. A leaf node is a node where there are no further splits and where decisions are made. A higher number may have the effect of smoothing the model, especially in regression.</li>
        </ul>
        <div class="packt_tip">One quick way to check for overfitting is to compare the classifier's accuracy on the test set to its accuracy on the training set. Having a much higher score for your training set compared to the test set is a sign of overfitting. A smaller and more pruned tree is recommended in this case.</div>
        <p class="mce-root">If <kbd>max_depth</kbd> is not set at training time to limit the tree's growth, then alternatively, you can prune the tree after it has been built. Curious readers can check the <kbd>cost_complexity_pruning_path()</kbd> method of the decision tree and find out how to use it to prune an already-grown tree.</p>
        <h2 id="uuid-920ece85-e7f0-47e6-aa62-0f654f40075f">Predictions</h2>
        <p>At the end of the training process, nodes that aren't split any further are called leaf nodes. Within a leaf node, we may have five samples—four of them from class <kbd>1</kbd>, one from class <kbd>2</kbd>, and none from class <kbd>0</kbd>. Then, at prediction time, if a sample ends up in the same leaf node, we can easily decide that the new sample belongs to class <kbd>1</kbd> since this leaf node had a 4:1 ratio of its training samples from class <kbd>1</kbd> compared to the other two classes.</p>
        <p>When we make predictions on the test set, we can evaluate the classifier's accuracy versus the actual labels we have in the test set. Nevertheless, the manner in which we split our data may affect the reliability of the scores we get. In the next section, we will see how to get more reliable scores. </p>
        <h1 id="uuid-03dd158b-7eda-41f1-862d-b0392a31ee5b">Getting a more reliable score</h1>
        <p>The Iris dataset is a small set of just 150 samples. When we randomly split it into training and test sets, we ended up with 45 instances in the test set. With such a small number, we may have variations in the distribution of our targets. For example, when I randomly split the data, I got 13 samples from class <kbd>0</kbd> and 16 samples from each one of the two other classesin my test set. Knowing that predicting class <kbd>0</kbd> is easier than the other two classes in this particular dataset, we can tell that if I was luckier and had more samples of class <kbd>0</kbd> in the test set, I'd have had a higher score. Furthermore, decision trees are very sensitive to data changes, and you may get a very different tree with every slight change in your training data.</p>
        <h2 id="uuid-f368502f-f2b5-41a0-9e11-11b5a7d1027e">What to do now to get a more reliable score</h2>
        <p>A statistician would say <q>let's run the whole process of data splitting, training, and predicting, more than once, and get the distribution of the different accuracy scores we get each time</q>. The following code does exactly that for 100 iterations:</p>
        <pre>import pandas as pd<br/><br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.metrics import accuracy_score<br/><br/># A list to store the score from each iteration<br/>accuracy_scores = []</pre>
        <p>After importing the required modules and defining an <kbd>accuracy_scores</kbd> list to store the scores we are going get with each iteration, it is time to write a <kbd>for</kbd> loop to freshly split the data and recalculate the classifier's accuracy with each iteration:</p>
        <pre class="mce-root">for _ in range(100):<br/><br/>    # At each iteration we freshly split our data<br/>    df_train, df_test = train_test_split(df, test_size=0.3) <br/>    x_train = df_train[iris.feature_names]<br/>    x_test = df_test[iris.feature_names]<br/><br/>    y_train = df_train['target']<br/>    y_test = df_test['target']<br/><br/>    # We then create a new classifier<br/>    clf = DecisionTreeClassifier()<br/><br/>    # And use it for training and prediction<br/>    clf.fit(x_train, y_train)<br/>    y_pred = clf.predict(x_test)<br/><br/>    # Finally, we append the score to our list<br/>    accuracy_scores.append(round(accuracy_score(y_test, y_pred), 3))<br/><br/># Better convert accuracy_scores from a list into a series<br/># Pandas series provides statistical methods to use later<br/>accuracy_scores = pd.Series(accuracy_scores)</pre>
        <p>The following snippet lets us plot the accuracy's distribution using a box plot:</p>
        <pre>accuracy_scores.plot(<br/>    title='Distribution of classifier accuracy',<br/>    kind='box',<br/>)<br/><br/>print(<br/>    'Average Score: {:.3} [5th percentile: {:.3} &amp; 95th percentile: {:.3}]'.format(<br/>        accuracy_scores.mean(),<br/>        accuracy_scores.quantile(.05),<br/>        accuracy_scores.quantile(.95),<br/>    )<br/>)</pre>
        <p>This will give us the following graphical analysis of the accuracy. Your results might vary slightlydue to the random split of the training and test sets and the random initial settings of the decision trees. Almost all of the scikit-learn modules support a pseudo-random number generator that can be initialized via a <kbd>random_state</kbd> hyperparameter. This can be used to enforce code reproducibility. Nevertheless, I deliberately ignored it this time to show how the model's results may vary from one run to the other, and to show the importance of estimating the distributions of your models' errors via iterations:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/b2fcecc8-a3cc-43a5-85ab-bee4e8a38b41.png" style="width:50.50em;"/>
        </p>
        <p>Box plots are good at showing distributions. Rather than having a single number, we now have an estimation of the best- and the worst-case scenarios of our classifier's performance. </p>
        <div class="packt_tip">If, at any point, you do not have access to NumPy, you can still calculate a sample's mean and standard deviation using the <kbd>mean()</kbd> and <kbd>stdev()</kbd> methods provided by Python's built-in <kbd>statistics</kbd> module. It also provides functionalities for calculating the geometric and harmonic mean, as well as the median and quantiles. </div>
        <h2 id="uuid-fc04a0dc-1ad0-4552-acac-d5910a197e0b">ShuffleSplit</h2>
        <p>Generating different train and test splits is called cross-validation. This helps us have a more reliable estimation of our model's accuracy. What we did in the previous section is one of many cross-validation strategies called r<span class="mw-headline">epeated random sub-sampling validation,</span> or Monte Carlo cross-validation.</p>
        <div class="packt_infobox">In probability theory, the law of large numbers states that if we repeat the same experiment a large number of times, the average of the results obtained should be close to the expected outcome. The Monte Carlo methods make use of random sampling in order to repeat an experiment over and over to reach better estimates for the results, thanks to the law of large numbers. The Monte Carlo methods were made possible due to the existence of computers, and here we use the same method to repeat the training/test split over and over to reach a better estimation of the model's accuracy.</div>
        <p>scikit-learn's <kbd>ShuffleSplit</kbd> module provides us with the functionality to perform Monte Carlo cross-validation. Rather than us splitting the data ourselves, <kbd>ShuffleSplit</kbd> gives us lists of indices to use for splitting our data. In the following code, we are going to use the DataFrame's <kbd>loc()</kbd> method and the indices we get from <kbd>ShuffleSplit</kbd>to randomly split the dataset into 100 training and test pairs:</p>
        <pre>import pandas as pd<br/><br/>from sklearn.model_selection import ShuffleSplit<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.metrics import accuracy_score<br/><br/>accuracy_scores = []<br/><br/># Create a shuffle split instance<br/>rs = ShuffleSplit(n_splits=100, test_size=0.3)<br/><br/># We now get 100 pairs of indices <br/>for train_index, test_index in rs.split(df):<br/><br/> x_train = df.loc[train_index, iris.feature_names]<br/> x_test = df.loc[test_index, iris.feature_names]<br/><br/> y_train = df.loc[train_index, 'target']<br/> y_test = df.loc[test_index, 'target']<br/><br/> clf = DecisionTreeClassifier()<br/><br/> clf.fit(x_train, y_train)<br/> y_pred = clf.predict(x_test)<br/><br/> accuracy_scores.append(round(accuracy_score(y_test, y_pred), 3))<br/><br/>accuracy_scores = pd.Series(accuracy_scores)</pre>
        <p>Alternatively, we can simplify the preceding code even further by using scikit-learn's<kbd>cross_validate</kbd><em><strong/></em>functionality. This time, we are not event splitting the data into training and test sets ourselves. We will give <kbd>cross_validate</kbd> the<kbd>x</kbd>and<kbd>y</kbd> values for the entire set, and then give it our <kbd>ShuffleSplit</kbd> instance for it to use internally to split the data. We also give it the classifier and specify what kind of scoring metric to use. When done, it will give us back a list with the calculated test set scores:</p>
        <pre>import pandas as pd<br/><br/>from sklearn.model_selection import ShuffleSplit<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import cross_validate<br/><br/>clf = DecisionTreeClassifier()<br/>rs = ShuffleSplit(n_splits=100, test_size=0.3)<br/><br/>x = df[iris.feature_names]<br/>y = df['target']<br/><br/>cv_results = cross_validate(<br/>    clf, x, y, cv=rs, scoring='accuracy'<br/>)<br/><br/>accuracy_scores = pd.Series(cv_results['test_score'])</pre>
        <p>We can plot the resulting series of accuracy scores now to get the same box plot as earlier. Cross-validation is recommended when dealing with a small dataset since a group of accuracy scores will give us a better understanding of the classifier's performance compared to a single score calculated after a single trial. </p>
        <h1 id="uuid-9a0dd929-e83e-4d6f-9434-408338276e51">Tuning the hyperparameters for higher accuracy</h1>
        <p>Now that we have learned how to evaluate the model's accuracy more reliably using the <kbd>ShuffleSplit</kbd> cross-validation method, it is time to test our earlier hypothesis: would a smaller tree be more accurate?</p>
        <p>Here is what we are going to do in the following sub sections:</p>
        <ol>
          <li>Split the data into training and test sets.</li>
          <li>Keep the test side to one side now.</li>
          <li>Limit the tree's growth using different values of <kbd>max_depth</kbd>.</li>
          <li>For each <kbd>max_depth</kbd> setting, we will use the <kbd>ShuffleSplit</kbd> cross-validation method on the training set to get an estimation of the classifier's accuracy.</li>
          <li>Once we decide which value to use for <kbd>max_depth</kbd>, we will train the algorithm one last time on the entire training set and predict on the test set.</li>
        </ol>
        <h2 id="uuid-4eaaa2de-932a-4bb6-a91a-5aef5bf09003">Splitting the data</h2>
        <p>Here is the usual code for splitting the data into training and test sets:</p>
        <pre>from sklearn.model_selection import train_test_split<br/><br/>df_train, df_test = train_test_split(df, test_size=0.25)<br/><br/>x_train = df_train[iris.feature_names]<br/>x_test = df_test[iris.feature_names]<br/><br/>y_train = df_train['target']<br/>y_test = df_test['target']</pre>
        <h2 id="uuid-5b168941-a4a7-477c-a8e3-dc4e573076fa">Trying different hyperparameter values</h2>
        <p>If we allowed our earlier treeto grow indefinitely, we would get a tree depth of <kbd>4</kbd>. You can check the depth of a tree by calling<kbd>clf.get_depth()</kbd>once it is trained. So, it doesn't make sense to try any <kbd>max_depth</kbd> values above <kbd>4</kbd>. Here, we are going to loop over the maximum depths from <kbd>1</kbd> to <kbd>4</kbd> and use <kbd>ShuffleSplit</kbd> to get the classifier's accuracy:</p>
        <pre class="mce-root">import pandas as pd<br/>from sklearn.model_selection import ShuffleSplit<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import cross_validate<br/><br/>for max_depth in [1, 2, 3, 4]:<br/><br/>    # We initialize a new classifier each iteration with different max_depth<br/>    clf = DecisionTreeClassifier(max_depth=max_depth)<br/>    # We also initialize our shuffle splitter<br/>    rs = ShuffleSplit(n_splits=20, test_size=0.25)<br/><br/>    cv_results = cross_validate(<br/>        clf, x_train, y_train, cv=rs, scoring='accuracy'<br/>    )<br/>    accuracy_scores = pd.Series(cv_results['test_score'])<br/><br/>print(<br/>        '@ max_depth = {}: accuracy_scores: {}~{}'.format(<br/>            max_depth, <br/>            accuracy_scores.quantile(.1).round(3), <br/>            accuracy_scores.quantile(.9).round(3)<br/>        )<br/>    )</pre>
        <p class="mce-root">We called the <kbd>cross_validate()</kbd> method as we did earlier, giving it the classifier's instance, as well as the <kbd>ShuffleSplit</kbd> instance. We also defined our evaluation score as <kbd>accuracy</kbd>. Finally, we print the scores we get with each iteration. We will look more at the printed values in the next section.</p>
        <h2 id="uuid-993b7e38-ae5e-455b-92f6-59c32d8095d1" class="mce-root">Comparing the accuracy scores</h2>
        <p>Since we have a list of scores for each iteration, we can calculate their mean, or, as we will do here, we will print their 10<sup>th</sup> and 90<sup>th</sup> percentiles to get an idea of the accuracy ranges versus each <kbd>max_depth</kbd>setting.</p>
        <p>Running the preceding code gave me the following results:</p>
        <pre>@ max_depth = 1: accuracy_scores: 0.532~0.646
@ max_depth = 2: accuracy_scores: 0.925~1.0
@ max_depth = 3: accuracy_scores: 0.929~1.0
@ max_depth = 4: accuracy_scores: 0.929~1.0</pre>
        <p>One thing I am sure about now is that a single-level tree (usually called a stub) is not as accurate as deeper trees. In other words, having a single decision based on whether the petal width is less than <kbd>0.8</kbd> is not enough. Allowing the tree to grow further improves the accuracy, but I can't see many differences between trees of depths <kbd>2</kbd>, <kbd>3</kbd>, and <kbd>4</kbd>. I'd conclude that contrary to my earlier speculations, we shouldn't worry too much about overfitting here. </p>
        <div class="packt_infobox">Here, we tried different values for a single parameter, <kbd>max_depth</kbd>. That's why a simple <kbd>for</kbd> loop over its different values was feasible. In later chapters, we will see what to do when we need to tune multiple hyperparameters at once to reach a combination that gives the best accuracy. </div>
        <p>Finally, you can train your model once more using the entire training set and a <kbd>max_depth</kbd> value of, say, <kbd>3</kbd>. Then, use the trained model to predict the classes for the test set in order to evaluate your final model. I won't bore you with the code for it this time as you can easily do it yourself. </p>
        <p>In addition to printing the classifier's decision and descriptive statistics about its accuracy, it is useful to also see its decision boundaries visually. Mapping those boundaries versus the data samples helps us understand why the classifier made certain mistakes. In the next section, we are going to check the decision boundaries we got for the Iris dataset.</p>
        <h1 id="uuid-1b72da27-b1a2-4e3e-a963-51cdb761da49">Visualizing the tree's decision boundaries</h1>
        <p>To be able to pick the right algorithm for the problem, it is important to have a conceptual understanding of how an algorithm makes its decision. As we already know by now, decision trees pick one feature at a time and try to split the data accordingly. Nevertheless, it is important to be able to visualize those decisions as well. Let me first plot our classes versus our features, then I will explain further:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/5c2e9fe5-3635-4869-968b-cca3102ef337.png" style="width:42.58em;"/>
        </p>
        <p>When the tree made a decision to split the data around a petal width of <kbd>0.8</kbd>, you can think of it as drawing a horizontal line in the right-hand side graph at the value of <kbd>0.8</kbd>. Then, with every later split, the tree splits the space further using combinations of horizontal and vertical lines. By knowing this, you should not expect the algorithm to use curves or 45-degree lines to separate the classes. </p>
        <p>One trick to plot the decision boundaries that a tree has after it has been trained is to use contour plots. For simplicity, let's assume we only have two features—petal length and petal width. We then generate almost all the possible values for those two features and predict the class labels for our new hypothetical data. Then, we create a contour plot using those predictions to see the boundaries between the classes. The following function, created by Richard Johanssonof the University of Gothenburg, does exactly that:</p>
        <pre>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/><br/>def plot_decision_boundary(clf, x, y):<br/><br/> feature_names = x.columns<br/> x, y = x.values, y.values<br/><br/> x_min, x_max = x[:,0].min(), x[:,0].max()<br/> y_min, y_max = x[:,1].min(), x[:,1].max()<br/><br/> step = 0.02<br/><br/> xx, yy = np.meshgrid(<br/> np.arange(x_min, x_max, step),<br/> np.arange(y_min, y_max, step)<br/> )<br/> Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br/> Z = Z.reshape(xx.shape)<br/><br/> plt.figure(figsize=(12,8))<br/> plt.contourf(xx, yy, Z, cmap='Paired_r', alpha=0.25)<br/> plt.contour(xx, yy, Z, colors='k', linewidths=0.7)<br/> plt.scatter(x[:,0], x[:,1], c=y, edgecolors='k')<br/> plt.title("Tree's Decision Boundaries")<br/> plt.xlabel(feature_names[0])<br/> plt.ylabel(feature_names[1])</pre>
        <p>This time, we will train our classifier using two features only, and then call the preceding function using the newly trained model:</p>
        <pre>x = df[['petal width (cm)', 'petal length (cm)']]<br/>y = df['target']<br/><br/>clf = DecisionTreeClassifier(max_depth=3)<br/>clf.fit(x, y)<br/><br/>plot_decision_boundary(clf, x, y)</pre>
        <p>Richard Johansson's functions overlay the contour graph over our samples to give us the following graph:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/cb06f70a-1971-4b6c-af62-8b99a2cf0943.png" style="width:49.17em;"/>
        </p>
        <p>By seeing the decision boundaries as well as the data samples, you can make better decisions on whether one algorithm is good for the problem at hand. </p>
        <h2 id="uuid-5a368cdc-21db-44da-b6be-7d43c5e25b2d">Feature engineering</h2>
        <div class="packt_quote">"Every man takes the limits of his own field of vision for the limits of the world."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– <span class="authorOrTitle">Arthur Schopenhauer</span></div>
        <p>On seeing the class distribution versus the petal lengths and widths, you may wonder: <em>what if the decision trees could also draw boundaries that are at 40 degrees? Wouldn't 40-degree boundaries be more apt than those horizontal and vertical jigsaws?</em>Unfortunately, decision trees cannot do that, but let's put the algorithm aside for a moment and think about the data instead. How about creating a new axis where the class boundaries change their orientation? </p>
        <p>Let's create two new columns—<kbd>petal length x width (cm)</kbd> and <kbd>sepal length x width (cm)</kbd>—and see how the class distribution will look:</p>
        <pre>df['petal length x width (cm)'] = df['petal length (cm)'] * df['petal width (cm)']<br/>df['sepal length x width (cm)'] = df['sepal length (cm)'] * df['sepal width (cm)']</pre>
        <p>The following code will plot the classes versus the newly derived features:</p>
        <pre>fig, ax = plt.subplots(1, 1, figsize=(12, 6));<br/><br/>h_label = 'petal length x width (cm)'<br/>v_label = 'sepal length x width (cm)'<br/><br/>for c in df['target'].value_counts().index.tolist():<br/>    df[df['target'] == c].plot(<br/>        title='Class distribution vs the newly derived features',<br/>        kind='scatter',<br/>x=h_label,<br/>y=v_label,<br/>color=['r', 'g', 'b'][c], # Each class different color<br/>marker=f'${c}$', # Use class id as marker<br/>s=64,<br/>        alpha=0.5,<br/>        ax=ax,<br/>    )<br/><br/>fig.show()</pre>
        <p class="mce-root"/>
        <p>Running this code will produce the following graph:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/64d6b59e-2a67-4eb3-9178-a5e76bc876de.png" style="width:48.58em;"/>
        </p>
        <p>This new projection looks better; it makes the data more vertically separable. Nevertheless, the proof of the pudding is still in the eating. So, let's train two classifiers—one on the original features and one on the newly derived features—and see</p>
        <p>how their accuracies compare. The following code goes through 500 iterations, each time splitting the data randomly, and then training both models, each with its own set of features, and storing the accuracy we get with each iteration:</p>
        <pre>features_orig = iris.feature_names<br/>features_new = ['petal length x width (cm)', 'sepal length x width (cm)']<br/><br/>accuracy_scores_orig = []<br/>accuracy_scores_new = []<br/><br/>for _ in range(500):<br/><br/>    df_train, df_test = train_test_split(df, test_size=0.3)<br/><br/>x_train_orig = df_train[features_orig]<br/>x_test_orig = df_test[features_orig]<br/><br/>x_train_new = df_train[features_new]<br/>x_test_new = df_test[features_new]<br/><br/>     y_train = df_train['target']<br/>y_test = df_test['target']<br/><br/>clf_orig = DecisionTreeClassifier(max_depth=2)<br/>clf_new = DecisionTreeClassifier(max_depth=2)<br/><br/>     clf_orig.fit(x_train_orig, y_train)<br/>clf_new.fit(x_train_new, y_train)<br/><br/>y_pred_orig = clf_orig.predict(x_test_orig)<br/>y_pred_new = clf_new.predict(x_test_new)<br/><br/>accuracy_scores_orig.append(round(accuracy_score(y_test, y_pred_orig), <br/>                                       3))<br/>accuracy_scores_new.append(round(accuracy_score(y_test, y_pred_new), <br/>                                      3))<br/><br/>accuracy_scores_orig = pd.Series(accuracy_scores_orig)<br/>accuracy_scores_new = pd.Series(accuracy_scores_new)</pre>
        <p>Then, we can use box plots to compare the accuracies of the two classifiers:</p>
        <pre>fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True);<br/><br/>accuracy_scores_orig.plot(<br/>    title='Distribution of classifier accuracy [Original Features]',<br/>    kind='box',<br/>grid=True,<br/>ax=axs[0]<br/>)<br/><br/>accuracy_scores_new.plot(<br/>title='Distribution of classifier accuracy [New Features]',<br/>kind='box',<br/>grid=True,<br/>ax=axs[1]<br/>)<br/><br/>fig.show()</pre>
        <p>Here, we put the top plots side by side to be able to compare them to each other:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/9df33baf-3354-44c7-bc30-e30c7c988f9f.png" style="width:47.83em;"/>
        </p>
        <p>Clearly, the derived features helped a bit. Its accuracy is higher on average (<kbd>0.96</kbd> versus <kbd>0.93</kbd>), and its lower bound is also higher. </p>
        <h1 id="uuid-372603a3-34af-4d29-876f-688fa6b14f5a">Building decision tree regressors</h1>
        <p>Decision tree regressors work in a similar fashion to their classifier counterparts. The algorithm splits the data recursively using one feature at a time. At the end of the process, we end up with leaf nodes—that is, nodes where there are no further splits. In the case of a classifier, if, at training time, a leaf node has three instances of class <kbd>A</kbd> and one instance of class <kbd>B</kbd>, then at prediction time, if an instance lands in the same leaf node, the classifier decides that it belongs to the majority class (class <kbd>A</kbd>). In the case of a regressor, if, at training time, a leaf node has three instances of values <kbd>12</kbd>, <kbd>10</kbd>, and <kbd>8</kbd>,then, at prediction time, if an instance lands in the same leaf node, the regressor predicts its value to be <kbd>10</kbd> (the average of the three values at training time).</p>
        <div class="packt_infobox">Actually, picking the average is not always the best case. It rather depends on the splitting criterion used. In the next section, we are going to see this in more detail with the help of an example.</div>
        <h2 id="uuid-213b45fc-6bce-4f9e-a4c8-f0a1a26c4717">Predicting people's heights</h2>
        <p>Say we have two populations. Population <kbd>1</kbd> has an average height of 155 cm for females, with a standard deviation of <kbd>4</kbd>, and an average height of 175 cm for males, with a standard deviation of <kbd>5</kbd>. Population 2 has an average height of 165 cm for females, with a standard deviation of <kbd>15</kbd>, and an average height of 185 cm for males, with a standard deviation of <kbd>12</kbd>. We decide to take 200 males and 200 females from each population. To be able to simulate this, we can use a function provided by NumPy that draws random samples from a normal (Gaussian) distribution.</p>
        <p>Here is the code for generating random samples:</p>
        <pre># It's customary to call numpy np<br/>import numpy as np<br/><br/># We need 200 samples from each<br/>n = 200<br/><br/># From each population we get 200 male and 200 female samples<br/>height_pop1_f = np.random.normal(loc=155, scale=4, size=n)<br/>height_pop1_m = np.random.normal(loc=175, scale=5, size=n)<br/>height_pop2_f = np.random.normal(loc=165, scale=15, size=n)<br/>height_pop2_m = np.random.normal(loc=185, scale=12, size=n)</pre>
        <p>At the moment, we don't actually care about which population each sample comes from. So, we will use<kbd>concatenate</kbd>to group all the males and all the females together:</p>
        <pre class="mce-root"># We group all females together and all males together<br/>height_f = np.concatenate([height_pop1_f, height_pop2_f])<br/>height_m = np.concatenate([height_pop1_m, height_pop2_m])</pre>
        <p>We then put this data into a DataFrame (<kbd>df_height</kbd>) to be able to deal with it easily. There, we also give a label of <kbd>1</kbd> to females and <kbd>2</kbd> to males: </p>
        <pre>df_height = pd.DataFrame(<br/>    {<br/>        'Gender': [1 for i in range(height_f.size)] + <br/>                   [2 for i in range(height_m.size)],<br/>        'Height': np.concatenate((height_f, height_m))<br/>    }<br/>)</pre>
        <p>Let's plot our fictional data using histograms to see the height distributions among each gender:</p>
        <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 5))<br/><br/>df_height[df_height['Gender'] == 1]['Height'].plot(<br/>    label='Female', kind='hist', <br/>    bins=10, alpha=0.7, ax=ax<br/>)<br/>df_height[df_height['Gender'] == 2]['Height'].plot(<br/>    label='Male', kind='hist', <br/>    bins=10, alpha=0.7, ax=ax<br/>)<br/><br/>ax.legend()<br/><br/>fig.show()</pre>
        <p>The preceding code gives us the following graph:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/f1e0d999-9406-45ad-ac4b-8ec91eff3917.png" style="width:43.08em;"/>
        </p>
        <p>As you can see, the resulting distributions are notsymmetrical. Although normal distributions are symmetrical, these artificial distributions are made of two sub-distributions combined. We can use this line of code to see that their mean and median values are not equal:</p>
        <pre>df_height.groupby('Gender')[['Height']].agg([np.mean, np.median]).round(1)</pre>
        <p>Here, we have the mean and median heights for each group:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/c33b936d-270e-47fd-816a-13413360c83c.png" style="width:11.33em;"/>
          <br/>
        </p>
        <p>Now, we want to predict people's heights using one feature—their gender. Therefore, we are going to split our data into training and test sets and create our <em>x</em> and <em>y</em> sets, as follows:</p>
        <pre class="mce-root">df_train, df_test = train_test_split(df_height, test_size=0.3)<br/>x_train, x_test = df_train[['Gender']], df_test[['Gender']]<br/>y_train, y_test = df_train['Height'], df_test['Height']</pre>
        <p>Remember that in the case of classifications, the trees use either <kbd>gini</kbd> or <kbd>entropy</kbd> to decide the best split at each step during the training process. The goal for these criteria was to find a split where each of the two resulting sub-groups is as pure as possible. In the case of regression, we have a different goal. We want the members of each group to have target values that are as close as possible to the predictions they make. scikit-learn implements two criteria to achieve this goal:</p>
        <ul>
          <li><strong>Mean squared error</strong> (<strong>MSE or L2</strong>):Say after the split, we get three samples in one group with targets of <kbd>5</kbd>, <kbd>5</kbd>, and <kbd>8</kbd>. We calculate the mean value of these three numbers (<kbd>6</kbd>). Then, we calculate the squared differences between each sample and the calculated mean—<kbd>1</kbd>, <kbd>1</kbd>, and <kbd>4</kbd>. We then take the mean of these squared differences, which is <kbd>2</kbd>.</li>
          <li><strong>Mean absolute error</strong> (<strong>MAE or L1</strong>): Say after the split, we get three samples in one group with targets of <kbd>5</kbd>, <kbd>5</kbd>, and <kbd>8</kbd>. We calculate the median value of these three numbers (<kbd>5</kbd>). Then, we calculate the absolute differences between each sample and the calculated median—<kbd>0</kbd>, <kbd>0</kbd>, and <kbd>3</kbd>. We then take the mean of these absolute differences, which is <kbd>1</kbd>.</li>
        </ul>
        <p>For each possible split at training time, the tree calculates either L1 or L2 for each of the expected sub-groups after the split. A split with the minimum L1 or L2 is then chosen at this step. L1 may be preferred sometimes due to its robustness to outliers. The other important difference to keep in mind is that L1 uses median while L2 uses mean in its calculations.</p>
        <div class="packt_infobox">If, at training time, we see 10 samples with almost identical features but different targets, they may all end up together in one leaf node. Now, if we use L1 as the splitting criterion when building our regressor, then if we get a sample at prediction time with identical features to the 10 training samples, we should expect the prediction to be close to the median value of the targets of the 10 training samples. Likewise, if L2 is used for building the regressor, we should then expect the prediction of the new sample to be close to the mean value of the targets of the 10 training samples. </div>
        <p>Let's now compare the effect of the splitting criteria on our height dataset:</p>
        <pre>from sklearn.tree import export_text<br/>from sklearn.tree import DecisionTreeRegressor<br/><br/>for criterion in ['mse', 'mae']:<br/>    rgrsr = DecisionTreeRegressor(criterion=criterion)<br/>    rgrsr.fit(x_train, y_train)<br/><br/>    print(f'criterion={criterion}:\n')<br/>    print(export_text(rgrsr, feature_names=['Gender'], spacing=3, decimals=1))</pre>
        <p>We get the following two trees depending on the chosen criterion:</p>
        <pre>criterion=mse:<br/><br/>|--- Gender &lt;= 1.5<br/>|    |--- value: [160.2]<br/>|--- Gender &gt; 1.5<br/>|    |--- value: [180.8]<br/><br/>criterion=mae:<br/><br/>|--- Gender &lt;= 1.5<br/>|    |--- value: [157.5]<br/>|--- Gender &gt; 1.5<br/>|    |--- value: [178.6]</pre>
        <p>As expected, when MSE was used, the predictions were close to the mean of each gender, while for MAE, the predictions were close to the median. </p>
        <p>Of course, we only had one binary feature in our dataset—gender. That's why we had a very shallow tree with a single split (a <strong>stub</strong>). Actually, in this case, we do not even need to train a decision tree; we could have easily calculated the mean heights for males and females and used them as our expected values right away. The decisions made by such a shallow tree are called biased decisions. If we would have allowed each individual to express themselves using more information, rather than just their gender, then we would have been able to make more accurate predictions for each individual. </p>
        <p>Finally, just as in the classification trees, we have the same knobs, such as <kbd>max_depth</kbd>, <kbd>min_samples_split</kbd>, and <kbd>min_samples_leaf</kbd><strong>, </strong>to control the growth of a regression tree.</p>
        <h2 id="uuid-be6b23cd-0312-4841-a013-6acd74d184ec">Regressor's evaluation  </h2>
        <p>The very same MSE and MAE scores can also be used to evaluate a regressor's accuracy. We use them to compare the regressor's predictions to the actual targets in the test set. Here is the code predicting and evaluating the predictions made:</p>
        <pre>from sklearn.metrics import mean_squared_error, mean_absolute_error<br/><br/>y_test_pred = rgrsr.predict(x_test)<br/>print('MSE:', mean_squared_error(y_test, y_test_pred))<br/>print('MAE:', mean_absolute_error(y_test, y_test_pred))</pre>
        <p>Using MSE as a splitting criterion gives us an MSE of <kbd>117.2</kbd> and an MAE of <kbd>8.2</kbd>, while using MAE as a splitting criterion gives us an MSE of <kbd>123.3</kbd>and an MAE of <kbd>7.8</kbd>. Clearly, using MAE as the splitting criterion gives a lower MAE at test time, and vice versa. In other words, if your aim is to reduce the error of your predictions based on a certain metric, it is advised to use the same metric when growing your tree at the time of training. </p>
        <h2 id="uuid-e6cdbc34-5c9a-4b29-8017-5c2c72b7b2b2">Setting sample weights</h2>
        <p>Both the decision tree classifiers and the regressors allow us to give more or less emphasis to the individual training samples via setting their weights while fitting. This is a common feature in many estimators, and decision trees are no exception here. To see the effect of sample weights, we are going to give 10 times more weight to users above 150 cm versus the remaining users:</p>
        <pre>rgrsr = DecisionTreeRegressor(criterion='mse')<br/>sample_weight = y_train.apply(lambda h: 10 if h &gt; 150 else 1)<br/>rgrsr.fit(x_train, y_train, sample_weight=sample_weight)</pre>
        <p>Conversely, we can also give more weights to users who are 150 cm and below by changing the <kbd>sample_weight</kbd> calculations, as follows:</p>
        <pre>sample_weight = y_train.apply(lambda h: 10 if h &lt;= 150 else 1)</pre>
        <p>By using the <kbd>export_text()</kbd> function, as we did in the previous section, we can display the resulting trees. We can see how <kbd>sample_weight</kbd><em><strong/></em>affected their final structures:</p>
        <pre>Emphasis on "below 150":<br/><br/>|--- Gender &lt;= 1.5<br/>|    |--- value: [150.7]<br/>|--- Gender &gt; 1.5<br/>|    |--- value: [179.2]<br/><br/>Emphasis on "above 150":<br/><br/>|--- Gender &lt;= 1.5<br/>|    |--- value: [162.4]<br/>|--- Gender &gt; 1.5<br/>|    |--- value: [180.2]<br/><br/></pre>
        <p>By default, all samples are given the same weight. Weighting individual samples differently is useful when dealing with imbalanced data or imbalanced business decisions; maybe you can tolerate delaying a shipment for a new customer more than you can do for your loyal ones. In <a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=30&amp;action=edit">Chapter 8</a>, <em>Ensembles – When One Model Is Not Enough</em>, we will also see how sample weights are an integral part of how the AdaBoost algorithm learns. </p>
        <h1 id="uuid-216b7f7c-58cd-496c-8c5f-ab5320a894a9">Summary</h1>
        <p>Decision trees are intuitive algorithms that are capable of performing classification and regression tasks. They allow users to print out their decision rules, which is a plus when communicating the decisions you made to business personnel and non-technical third parties. Additionally, decision trees are easy to configure since they have a limited number of hyperparameters. The two main decisions you need to make when training a decision tree are your splitting criterion and how to control the growth of your tree to have a good balance between <em>overfitting</em> and <em>underfitting</em>. Your understanding of the limitations of the tree's decision boundaries is paramount in deciding whether the algorithm is good enough for the problem at hand. </p>
        <p>In this chapter, we looked at how decision trees learn and used them to classify a well-known dataset. We also learned about the different evaluation metrics and how the size of our data affects our confidence in a model's accuracy. We then learned how to deal with the evaluation's uncertainties using different data-splitting strategies. We saw how to tune the algorithm's hyperparameters for a good balance between overfitting and underfitting. Finally, we built on the knowledge we gained to build decision tree regressors and learned how the choice of a splitting criterion affects our resulting predictions.</p>
        <p>I hope this chapter has served as a good introduction to scikit-learn and its consistent interface. With this knowledge at hand, we can move on to our next algorithm and see how it compares to this one. In the next chapter, we will learn about linear models. This set of algorithms has its roots back in the 18<sup>th</sup> century, and it is still one of the most commonly used algorithms today.</p>
      </article>
    </section>
  </body></html>
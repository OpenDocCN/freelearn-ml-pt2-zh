- en: '*Chapter 9*: XGBoost Kaggle Masters'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：XGBoost Kaggle大师'
- en: In this chapter, you will learn valuable tips and tricks from `VotingClassifier`
    and `VotingRegressor` to build non-correlated machine learning ensembles, and
    the advantages of **stacking** a final model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习从`VotingClassifier`和`VotingRegressor`中获得的宝贵技巧和窍门，以构建非相关的机器学习集成模型，并了解**堆叠**最终模型的优势。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Exploring Kaggle competitions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Kaggle竞赛
- en: Engineering new columns of data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建新的数据列
- en: Building non-correlated ensembles
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建非相关集成模型
- en: Stacking final models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠最终模型
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09)找到。
- en: Exploring Kaggle competitions
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Kaggle竞赛
- en: '"I used only XGBoost (tried others but none of them performed well enough to
    end up in my ensemble)."'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '"我只用了XGBoost（尝试过其他的，但没有一个能达到足够的表现以最终加入我的集成模型中）。"'
- en: – *Qingchen Wang, Kaggle Winner*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: – *Qingchen Wang，Kaggle获胜者*
- en: ([https://www.cnblogs.com/yymn/p/4847130.html](https://www.cnblogs.com/yymn/p/4847130.html))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://www.cnblogs.com/yymn/p/4847130.html](https://www.cnblogs.com/yymn/p/4847130.html))
- en: In this section, we will investigate Kaggle competitions by looking at a brief
    history of Kaggle competitions, how they are structured, and the importance of
    a hold-out/test set as distinguished from a validation/test set.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过回顾Kaggle竞赛的简短历史、它们的结构以及区分验证/测试集与保留/测试集的重要性，来探讨Kaggle竞赛。
- en: XGBoost in Kaggle competitions
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost在Kaggle竞赛中的表现
- en: XGBoost built its reputation as the leading machine learning algorithm on account
    of its unparalleled success in winning Kaggle competitions. XGBoost often appeared
    in winning ensembles along with deep learning models such as **neural networks**,
    in addition to winning outright. A sample list of XGBoost Kaggle competition winners
    appears on the *Distributed (Deep) Machine Learning Community* web page at [https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).
    For a list of more XGBoost Kaggle competition winners, it's possible to sort through
    *Winning solutions of Kaggle competitions* ([https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions))
    to research the winning models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost因其在赢得Kaggle竞赛中的无与伦比的成功而建立了作为领先机器学习算法的声誉。XGBoost常常与深度学习模型如**神经网络**一起出现在获胜的集成模型中，除了单独获胜之外。一个XGBoost
    Kaggle竞赛获胜者的样例列表出现在*分布式（深度）机器学习社区*的网页上，[https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)。要查看更多XGBoost
    Kaggle竞赛获胜者，可以通过*Winning solutions of Kaggle competitions* ([https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions))来研究获胜模型。
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: While XGBoost is regularly featured among the winners, other machine learning
    models make appearances as well.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然XGBoost经常出现在获胜者中，但其他机器学习模型也有出现。
- en: As mentioned in [*Chapter 5*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117),
    *XGBoost Unveiled*, Kaggle competitions are machine learning competitions where
    machine learning practitioners compete against one another to obtain the best
    possible score and win cash prizes. When XGBoost exploded onto the scene in 2014
    during the *Higgs Boson Machine Learning Challenge*, it immediately jumped the
    leaderboard and became one of the most preferred machine learning algorithms in
    Kaggle competitions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第5章*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117)《XGBoost揭秘》中提到的，Kaggle竞赛是机器学习竞赛，机器学习从业者相互竞争，争取获得最佳分数并赢得现金奖励。当XGBoost在2014年参加*希格斯玻色子机器学习挑战*时，它迅速跃升至排行榜顶端，并成为Kaggle竞赛中最受欢迎的机器学习算法之一。
- en: Between 2014 and 2018, XGBoost consistently outperformed the competition on
    tabular data—data organized in rows and columns as contrasted with unstructured
    data such as images or text, where neural networks had an edge. With the emergence
    of **LightGBM** in 2017, a lightning-fast Microsoft version of gradient boosting,
    XGBoost finally had some real competition with tabular data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'The following introductory paper, *LightGBM: A Highly Efficient Gradient Boosting
    Decision Tree*, written by eight authors, is recommended for an introduction to
    LightGBM: [https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a great machine algorithm such as XGBoost or LightGBM in Kaggle
    competitions isn't enough. Similarly, fine-tuning a model's hyperparameters often
    isn't enough. While individual model predictions are important, it's equally important
    to engineer new data and to combine optimal models to attain higher scores.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The structure of Kaggle competitions
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's worth understanding the structure of Kaggle competitions to gain insights
    into why techniques such as non-correlated ensemble building and stacking are
    widespread. Furthermore, exploring the structure of Kaggle competitions will give
    you confidence in entering Kaggle competitions down the road if you choose to
    pursue that route.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Kaggle recommends *Housing Prices: Advanced Regression Techniques*, [https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques),
    for machine learning students looking to transition beyond the basics to advanced
    competitions. This is one of many knowledge-based competitions that do not offer
    cash prizes.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Kaggle competitions exist on the Kaggle website. Here is the website from *Avito
    Context Ad Clicks* from 2015 won by XGBoost user Owen Zhang: [https://www.kaggle.com/c/avito-context-ad-clicks/overview](https://www.kaggle.com/c/avito-context-ad-clicks/overview).
    Several XGBoost Kaggle competition winners, Owen Zhang included, are from 2015,
    indicating XGBoost''s circulation before Tianqi Chin''s landmark paper, *XGBoost:
    A Scalable Tree Boosting System* published in 2016: [https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the top of the *Avito Context Ad Clicks* website:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Avito Context Ad Clicks Kaggle competition website](img/B15551_09_01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Avito Context Ad Clicks Kaggle competition website
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'This overview page explains the competition as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Additional links next to **Overview** (highlighted in blue) include **Data**,
    where you access data for the competition.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notebooks**, where Kagglers post solutions and starter notebooks.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discussion**, where Kagglers post and answer questions.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leaderboard**, where the top scores are displayed.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules**, which explains how the competition works.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规则**，解释了竞赛的运作方式。'
- en: Additionally, note the **Late Submission** link on the far-right side, which
    indicates that submissions are still acceptable even though the competition is
    over, a general Kaggle policy.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外，请注意右侧的**延迟提交**链接，表示即使竞赛已经结束，提交仍然是被接受的，这是 Kaggle 的一项常规政策。
- en: To download the data, you need to enter the competition by signing up for a
    free account. The data is typically split into two datasets, `training.csv`, the
    training set used to build a model, and `test.csv`, the test set used to score
    the model. After submitting a model, you earn a score on the public leaderboard.
    At the competition's end, a final model is submitted against a private test set
    to determine the winning solution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 若要下载数据，你需要通过注册一个免费的账户来参加竞赛。数据通常被分为两个数据集，`training.csv` 是用于构建模型的训练集，`test.csv`
    是用于评估模型的测试集。提交模型后，你会在公共排行榜上获得一个分数。竞赛结束时，最终模型会提交给一个私有测试集，以确定获胜的解决方案。
- en: Hold-out sets
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留集
- en: It's important to make the distinction between building machine learning models
    for Kaggle competitions and building them on your own. Up to this point, we have
    split datasets into training and test sets to ensure that our models generalize
    well. In Kaggle competitions, however, models must be tested in a competitive
    environment. For that reason, data from the test set remains hidden.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建机器学习模型时，区分在 Kaggle 竞赛中构建模型与独立构建模型非常重要。到目前为止，我们已经将数据集分为训练集和测试集，以确保我们的模型能够很好地泛化。然而，在
    Kaggle 竞赛中，模型必须在竞争环境中进行测试。因此，测试集的数据会保持隐藏。
- en: 'Here are the differences between Kaggle''s training set and test set:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 Kaggle 的训练集和测试集之间的区别：
- en: '`training.csv`: This is where you train and score models on your own. This
    training set should be split into its own training and test sets using `train_test_split`
    or `cross_val_score` to build models that generalize well to new data. The test
    sets used during training are often referred to as **validation sets** since they
    validate the models.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training.csv`：这是你自己训练和评分模型的地方。这个训练集应该使用 `train_test_split` 或 `cross_val_score`
    将其划分为自己的训练集和测试集，从而构建能够很好泛化到新数据的模型。在训练过程中使用的测试集通常被称为**验证集**，因为它们用来验证模型。'
- en: '`test.csv`: This is a separate hold-out set. You don''t use the test set until
    you have a final model ready to test on data it has never seen before. The purpose
    of the hidden test set is to maintain the integrity of the competition. The test
    data is hidden from participants and the results are only revealed after participants
    submit a model.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test.csv`：这是一个独立的保留集。在模型准备好并可以在它从未见过的数据上进行测试之前，你不会使用测试集。隐藏测试集的目的是保持竞赛的公正性。测试数据对参与者是隐藏的，结果只会在参与者提交模型之后才会公开。'
- en: It's always good practice to keep a test set aside when building a model for
    research or industry. When a model is tested using data it has already seen, the
    model risks overfitting the test set, a possibility that often arises in Kaggle
    competitions when competitors obsess over improving their position in the public
    leaderboard by few thousandths of a percent.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建研究或行业模型时，将测试集留存一旁始终是一个良好的做法。当一个模型在已经见过的数据上进行测试时，模型有过拟合测试集的风险，这种情况通常出现在 Kaggle
    竞赛中，参赛者通过千分之一的微小差异来过度优化自己的成绩，从而在公共排行榜上提升名次。
- en: Kaggle competitions intersect with the real world regarding this hold-out set.
    The purpose of building machine learning models is to make accurate predictions
    using unknown data. For example, if a model gives 100% accuracy on the training
    set, but only gives 50% accuracy on unknown data, the model is basically worthless.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 竞赛与现实世界在保留集的使用上有所交集。构建机器学习模型的目的是使用未知数据进行准确的预测。例如，如果一个模型在训练集上达到了 100%
    的准确率，但在未知数据上只有 50% 的准确率，那么这个模型基本上是没有价值的。
- en: This distinction, between validating models on test sets and testing models
    on hold-out sets, is very important.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上验证模型与在保留集上测试模型之间的区别非常重要。
- en: 'Here is a general approach for validating and testing machine learning models
    on your own:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是验证和测试机器学习模型的一般方法：
- en: '**Split data into a training set and a hold-out set**: Keep the hold-out set
    away and resist the temptation to look at it.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将数据划分为训练集和保留集**：将保留集隔离开，并抵制查看它的诱惑。'
- en: '**Split the training set into a training and test set or use cross-validation**:
    Fit new models on the training set and validate the model, going back and forth
    to improve scores.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**After obtaining a final model, test it on the hold-out set**: This is the
    real test of the model. If the score is below expectations, return to *step 2*
    and repeat. Do not—and this is important—use the hold-out set as the new validation
    set, going back and forth adjusting hyperparameters. When this happens, the model
    is adjusting itself to match the hold-out set, which defeats the purpose of a
    hold-out set in the first place.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Kaggle competitions, adjusting the machine learning model too closely to
    the test set will not work. Kaggle often splits test sets into an additional public
    and private component. The public test set gives participants a chance to score
    their models and work on improvements, adjusting and resubmitting along the way.
    The private test set is not revealed until the last day of the competition. Although
    rankings are displayed for the public test set, competition winners are announced
    based on the results of the unseen test set.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Winning a Kaggle competition requires getting the best possible score on the
    private test set. In Kaggle competitions, every percentage point matters. The
    need for this kind of precision, sometimes scoffed at by the industry, has led
    to innovative machine learning practices to improve scores. Understanding these
    techniques, as presented in this chapter, can lead to stronger models and a deeper
    understanding of machine learning overall.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Engineering new columns
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Almost always I can find open source code for what I want to do, and my time
    is much better spent doing research and feature engineering."'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: – *Owen Zhang, Kaggle Winner*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: ([https://medium.com/kaggle-blog/profiling-top-kagglers-owen-zhang-currently-1-in-the-world-805b941dbb13](https://medium.com/kaggle-blog/profiling-top-kagglers-owen-zhang-currently-1-in-the-world-805b941dbb13))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Many Kagglers and data scientists have confessed to spending considerable time
    on research and feature engineering. In this section, we will use `pandas` to
    engineer new columns of data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: What is feature engineering?
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning models are as good as the data that they train on. When data
    is insufficient, building a robust machine learning model is impossible.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: A more revealing question is whether the data can be improved. When new data
    is extracted from other columns, these new columns of data are said to be *engineered*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is the process of developing new columns of data from the
    original columns. The question is not whether you should implement feature engineering,
    but how much feature engineering you should implement.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Let's practice feature engineering on a dataset predicting the cab fare of **Uber**
    and **Lyft** rides.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Uber and Lyft data
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to hosting competitions, Kaggle hosts a large number of datasets
    that include public datasets such as the following one, which predicts Uber and
    Lyft cab prices: [https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices](https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, first import all the libraries and modules needed for this
    section and silence the warnings:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, load the `''cab_rides.csv''` CSV file and view the first five rows. Limit
    `nrows` to `10000` to expedite computations. There are 600,000+ rows in total:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the expected output:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Cab rides dataset](img/B15551_09_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Cab rides dataset
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: This display reveals a wide range of columns, including categorical features
    and a timestamp.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Null values
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As always, check for null values before making any computations:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that `df.info()` also provides information about column types:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see from the output, null values exist in the `price` column since
    there are less than `10,000` non-null floats.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It''s worth checking the null values to see whether more information can be
    gained about the data:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are the first five rows of the output:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Null values in the cab rides dataset](img/B15551_09_03.jpg)'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9.3 – Null values in the cab rides dataset
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, there is nothing particularly glaring about these rows. It could
    be that the price of the ride was never recorded.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since `price` is the target column, these rows can be deleted with `dropna`
    using the `inplace=True` parameter to ensure that drops occur within the DataFrame:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can verify that no null values are present by using `df.na()` or `df.info()`
    one more time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering time columns
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Timestamp** columns often represent **Unix time**, which is the number of
    milliseconds since January 1st, 1970\. Specific time data can be extracted from
    the timestamp column that may help predict cab fares, such as the month, hour
    of the day, whether it is rush hour, and so on:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'First, convert the timestamp column into a time object using `pd.to_datetime`,
    and then view the first five rows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the expected output:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.4 – The cab rides dataset after time_stamp conversion](img/B15551_09_04.jpg)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9.4 – The cab rides dataset after time_stamp conversion
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Something is wrong with this data. It doesn't take much domain expertise to
    know that Lyft and Uber were not around in 1970\. The extra decimal places are
    a clue that the conversion is incorrect.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After trying several multipliers to make an appropriate conversion, I discovered
    that `10**6` gives the appropriate results:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the expected output:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.5 – The cab rides dataset after ''date'' conversion](img/B15551_09_05.jpg)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9.5 – The cab rides dataset after 'date' conversion
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With a datetime column, you can extract new columns, such as `month`, `hour`,
    and `day of week`, after importing `datetime`, as follows:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, you can use these columns to feature engineer more columns, such as whether
    it's the weekend or rush hour.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following function determines whether a day of the week is a weekend by
    checking whether `''dayofweek''` is equivalent to `5` or `6`, which represent
    Saturday or Sunday, according to the official documentation: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekday.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekday.html):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, apply the function to the DataFrame as a new column, `df[''weekend'']`,
    as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The same strategy can be implemented to create a rush hour column by seeing
    whether the hour is between 6–10 AM (hours `6–10`) and 3–7 PM (hours `15–19`):'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, apply the function to a new `''rush_hour''` column:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The last five rows show variation in the new columns, as `df.tail()` reveals:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is an excerpt from the output revealing the new columns:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.6 – The last five rows of the cab rides dataset after feature engineering](img/B15551_09_06.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – The last five rows of the cab rides dataset after feature engineering
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The process of extracting and engineering new time columns can continue.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: When engineering a lot of new columns, it's worth checking to see whether new
    features are strongly correlated. The correlation of data will be explored later
    in this chapter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the practice of feature engineering time columns, let's
    feature engineer categorical columns.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering categorical columns
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously, we used `pd.get_dummies` to convert categorical columns into numerical
    columns. Scikit-learn's `OneHotEncoder` feature is another option designed to
    transform categorical data into 0s and 1s using sparse matrices, a technique that
    you will apply in [*Chapter 10*](B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *XGBoost Model Deployment*. While converting categorical data into numerical data
    using either of these options is standard, alternatives exist.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Although 0s and 1s make sense as numerical values for categorical columns, since
    0 indicates absence and 1 indicates presence, it's possible that other values
    may deliver better results.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: One strategy would be to convert categorical columns into their frequencies,
    which equates to the percentage of times each category appears within the given
    column. So, instead of a column of categories, each category is converted into
    its percentage within the column.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Let's view the steps to convert categorical values into numerical values next.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Engineering frequency columns
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To engineer a categorical column, such as `''cab_type''`, first view the number
    of values for each category:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `.value_counts()` method to see the frequency of types:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result is as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use `groupby` to place the counts in a new column. `df.groupby(column_name)`
    is `groupby`, while `[column_name].transform` specifies the column to be transformed
    followed by the aggregate in parentheses:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Divide the new column by the total number of rows to obtain the frequency:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Verify that changes have been made as expected:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is an excerpt from the output showing the new columns:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.7 – The cab rides dataset after engineering the frequency of cabs](img/B15551_09_07.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – The cab rides dataset after engineering the frequency of cabs
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The cab frequency now displays the expected output.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle tip – mean encoding
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will conclude this section with a competition-tested approach to feature
    engineering called **mean encoding** or **target encoding**.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Mean encoding transforms categorical columns into numerical columns based on
    the mean target variable. For instance, if the color orange led to seven target
    values of 1 and three target values of 0, the mean encoded column would be 7/10
    = 0.7\. Since there is data leakage while using the target values, additional
    regularization techniques are required.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**Data leakage** occurs when information between training and test sets, or
    predictor and target columns, are shared. The risk here is that the target column
    is being directly used to influence the predictor columns, which is generally
    a bad idea in machine learning. Nevertheless, mean encoding has been shown to
    produce outstanding results. It can work when datasets are deep, and the distribution
    of mean values are approximately the same for incoming data. Regularization is
    an extra precaution taken to reduce the possibility of overfitting.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, scikit-learn provides `TargetEncoder` to handle mean conversions
    for you:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import `TargetEndoder` from `category_encoders`. If this does not work,
    install `category_encoders` using the following code:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, initialize `encoder`, as follows:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, introduce a new column and apply mean encoding using the `fit_transform`
    method on the encoder. Include the column that is being changed and the target
    column as parameters:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, verify that the changes are as expected:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is an excerpt of the output with the new column in view:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.8 – The cab rides dataset after mean encoding](img/B15551_09_08.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – The cab rides dataset after mean encoding
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The far-right column, `cab_type_mean`, is as expected.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on mean encoding, refer to this Kaggle study: [https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is not to say that mean encoding is better than one-hot encoding,
    but rather that mean encoding is a proven technique that has done well in Kaggle
    competitions and may be worth implementing to try and improve scores.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: More feature engineering
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is no reason to stop here. More feature engineering may include statistical
    measures on other columns using `groupby` and additional encoders. Other categorical
    columns, such as the destination and arrival columns, may be converted to latitude
    and longitude and then to new measures of distance, such as the taxicab distance
    or the **Vincenty** distance, which takes spherical geometry into account.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由止步于此。更多的特征工程可能包括对其他列进行统计度量，使用`groupby`和附加编码器。其他类别型列，比如目的地和到达列，可以转换为纬度和经度，然后转换为新的距离度量方式，例如出租车距离或**Vincenty**距离，它考虑了球面几何。
- en: In Kaggle competitions, participants may engineer thousands of new columns hoping
    to gain a few extra decimal places of accuracy. If you have a high number of engineered
    columns, you can select the most significant ones using `.feature_importances_`,
    as outlined in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*. You can also eliminate highly correlated columns (explained
    in the next section, *Building non-correlated ensembles*).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，参与者可能会进行数千列新的特征工程，希望能获得几位小数的准确度。如果你有大量的工程化列，可以使用`.feature_importances_`选择最重要的列，正如在[*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入剖析》中所述。你还可以去除高度相关的列（将在下一节“构建无相关性的集成模型”中解释）。
- en: For this particular cab rides dataset, there is an additional CSV file that
    includes the weather. But what if there wasn't a weather file? You could always
    research the weather data from the provided dates and include the weather data
    on your own.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的出租车乘车数据集，还附带了一个包含天气数据的CSV文件。但如果没有天气文件该怎么办呢？你可以自行查找提供日期的天气数据，并将其添加到数据集中。
- en: Feature engineering is an essential skill for any data scientist to build robust
    models. The strategies covered here are only a fraction of the options that exist.
    Feature engineering involves research, experimentation, domain expertise, standardizing
    columns, feedback on the machine learning performance of new columns, and narrowing
    down the final columns at the end.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是任何数据科学家构建鲁棒模型的必要技能。这里讲解的策略只是现存选项的一部分。特征工程涉及研究、实验、领域专业知识、标准化列、对新列的机器学习性能反馈，并最终缩小最终列的范围。
- en: Now that you understand the various strategies for feature engineering, let's
    move on to building non-correlated ensembles.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了各种特征工程策略，让我们继续讨论构建无相关性的集成模型。
- en: Building non-correlated ensembles
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建无相关性的集成模型
- en: '"In our final model, we had XGBoost as an ensemble model, which included 20
    XGBoost models, 5 random forests, 6 randomized decision tree models, 3 regularized
    greedy forests, 3 logistic regression models, 5 ANN models, 3 elastic net models
    and 1 SVM model."'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: “在我们的最终模型中，我们使用了XGBoost作为集成模型，其中包含了20个XGBoost模型，5个随机森林，6个随机化决策树模型，3个正则化贪婪森林，3个逻辑回归模型，5个ANN模型，3个弹性网模型和1个SVM模型。”
- en: – *Song, Kaggle Winner*
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: – *Song, Kaggle获胜者*
- en: ([https://hunch243.rssing.com/chan-68612493/all_p1.html](https://hunch243.rssing.com/chan-68612493/all_p1.html))
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://hunch243.rssing.com/chan-68612493/all_p1.html](https://hunch243.rssing.com/chan-68612493/all_p1.html))
- en: The winning models of Kaggle competitions are rarely individual models; they
    are almost always ensembles. By ensembles, I do not mean boosting or bagging models,
    such as random forests or XGBoost, but pure ensembles that include any distinct
    models, including XGBoost, random forests, and others.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle竞赛的获胜模型很少是单一模型；它们几乎总是集成模型。这里所说的集成模型，并不是指提升（boosting）或袋装（bagging）模型，如随机森林（random
    forests）或XGBoost，而是纯粹的集成模型，包含任何不同的模型，包括XGBoost、随机森林等。
- en: In this section, we will combine machine learning models into non-correlated
    ensembles to gain accuracy and reduce overfitting.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将结合机器学习模型，构建无相关性的集成模型，以提高准确性并减少过拟合。
- en: Range of models
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型范围
- en: The Wisconsin Breast Cancer dataset, used to predict whether a patient has breast
    cancer, has 569 rows and 30 columns, and can be viewed at [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星州乳腺癌数据集用于预测患者是否患有乳腺癌，包含569行和30列数据，可以在[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer)查看。
- en: 'Here are the steps to prepare and score the dataset using several classifiers:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用几种分类器准备和评分数据集的步骤：
- en: 'Import the `load_breast_cancer` dataset from scikit-learn so that we can quickly
    start building models:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Assign the predictor columns to `X` and the target column to `y` by setting
    the `return_X_y=True` parameter:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Prepare 5-fold cross-validation using `StratifiedKFold` for consistency:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, build a simple classification function that takes a model as input and
    returns the mean cross-validation score as output:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Get the scores of several default classifiers, including XGBoost, along with
    its alternative base learners, a random forest, and logistic regression:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) Score with XGBoost:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The score is as follows:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'b) Score with `gblinear`:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The score is as follows:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'c) Score with `dart`:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The score is as follows:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that for the dart booster, we set `one_drop=True` to ensure that trees
    are actually dropped.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) Score with `RandomForestClassifier`:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The score is as follows:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'e) Score with `LogisticRegression`:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The score is as follows:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Most models perform respectably, with the XGBoost classifier obtaining the highest
    score. The `gblinear` base learner did not perform particularly well, however,
    so we will not use it going forward.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, each of these models should be tuned. Since we have already covered
    hyperparameter tuning in multiple chapters, that option is not pursued here. Nevertheless,
    knowledge of hyperparameters can give confidence in trying a quick model with
    some adjusted values. For instance, as done in the following code, lowering `max_depth`
    to `2`, increasing `n_estimators` to `500`, and making sure that `learning_rate`
    is set to `0.1` may be attempted on XGBoost:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The score is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This is a very good score. Although it's not the highest, it may be of value
    in our ensemble.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a variety of models, let's learn about the correlations between
    them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Correlation
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of this section is not to select all models for the ensemble, but
    rather to select the non-correlated models.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: First, let's understand what **correlation** represents.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Correlation is a statistical measure between `-1` and `1` that indicates the
    strength of the linear relationship between two sets of points. A correlation
    of `1` is a perfectly straight line, while a correlation of `0` shows no linear
    relationship whatsoever.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Some visuals on correlation should make things clear. The following visuals
    are taken from Wikipedia''s *Correlation and Dependence* page at [https://en.wikipedia.org/wiki/Correlation_and_dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Scatter plots with listed correlations look as follows:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Listed Correlations](img/B15551_09_09.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Listed Correlations
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: License information
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: By DenisBoigelot, the original uploader was Imagecreator – own work, CC0, [https://commons.wikimedia.org/w/index.php?curid=15165296](https://commons.wikimedia.org/w/index.php?curid=15165296).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Anscombe''s quartet – four scatter plots with a correlation of **0.816** –
    looks as follows:'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Correlation of 0.816](img/B15551_09_10.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Correlation of 0.816
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: License information
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'By Anscombe.svg: Schutz (label using subscripts): Avenue – Anscombe.svg, CC
    BY-SA 3.0, [https://commons.wikimedia.org/w/index.php?curid=9838454](https://commons.wikimedia.org/w/index.php?curid=9838454)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The first example shows that the higher the correlation, the closer the dots
    generally are to a straight line. The second example shows that data points of
    the same correlation can differ widely. In other words, correlation provides valuable
    information, but it doesn't tell the whole story.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand what correlation means, let's apply correlation to building
    machine learning ensembles.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Correlation in machine learning ensembles
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we choose which models to include in our ensemble.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: A high correlation between machine learning models is undesirable in an ensemble.But
    why?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Consider the case of two classifiers with 1,000 predictions each. If these classifiers
    all make the same predictions, no new information is gained from the second classifier,
    making it superfluous.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Using a *majority rules* implementation, a prediction is only wrong if the majority
    of classifiers get it wrong. It's desirable, therefore, to have a diversity of
    models that score well but give different predictions. If most models give the
    same predictions, the correlation is high, and there is little value in adding
    the new model to the ensemble. Finding differences in predictions where a strong
    model may be wrong gives the ensemble the chance to produce better results. Predictions
    will be different when the models are non-correlated.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: To compute correlations between machine learning models, we first need data
    points to compare. The different data points that machine learning models produce
    are their predictions. After obtaining predictions, we concatenate them into a
    DataFrame, and then apply the `.corr` method to obtain all correlations at once.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to find correlations between machine learning models:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function that returns predictions for each machine learning model:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Prepare the data for one-fold predictions using `train_test_split`:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Obtain the predictions of all classifier candidates using the previously defined
    function:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) `XGBClassifier` uses the following:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The accuracy score is as follows:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'b) `XGBClassifier` with `dart` uses the following:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The accuracy score is as follows:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'c) `RandomForestClassifier` uses the following:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The accuracy score is as follows:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'd) `LogisticRegression` uses the following:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The accuracy score is as follows:'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The accuracy score is as follows:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Concatenate the predictions into a new DataFrame using `np.c` (the `c` is short
    for concatenation):'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Run correlations on the DataFrame using the `.corr()` method:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You should see the following output:'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Correlations between various machine learning models](img/B15551_09_11.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Correlations between various machine learning models
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all correlations on the diagonal are `1.0` because the correlation
    between the model and itself must be perfectly linear. All other values are reasonably
    high.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: There is no clear cut-off to obtain a non-correlated threshold. It ultimately
    depends on the values of correlation and the number of models to choose from.
    For this example, we could pick the next two least correlated models with our
    best model, `xgb`, which are the random forest and logistic regression.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have chosen our models, we will combine them into a single ensemble
    using the `VotingClassifier` ensemble, introduced next.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The VotingClassifier ensemble
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scikit-learn's `VotingClassifier` ensemble is designed to combine multiple classification
    models and select the output for each prediction using majority rules. Note that
    scikit-learn also comes with `VotingRegressor`, which combines multiple regression
    models by taking the average of each one.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to create an ensemble in scikit-learn:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize an empty list:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Initialize the first model:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Append the model to the list as a tuple in the form `(model_name, model)`:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Repeat *steps 2* and *3* as many times as desired:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Initialize `VotingClassifier` (or `VotingRegressor`) using the list of models
    as input:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Score the classifier using `cross_val_score`:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The score is as follows:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: As you can see, the score has improved.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the purpose and technique of building non-correlated
    machine learning ensembles, let's move on to a similar but potentially advantageous
    technique called stacking.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Stacking models
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"For stacking and boosting I use xgboost, again primarily due to familiarity
    and its proven results."'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: – *David Austin, Kaggle Winner*
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: ([https://www.pyimagesearch.com/2018/03/26/interview-david-austin-1st-place-25000-kaggles-popular-competition/](https://www.pyimagesearch.com/2018/03/26/interview-david-austin-1st-place-25000-kaggles-popular-competition/))
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: In this final section, we will examine one of the most powerful tricks frequently
    used by Kaggle winners, called stacking.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: What is stacking?
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Stacking combines machine learning models at two different levels: the base
    level, whose models make predictions on all the data, and the meta level, which
    takes the predictions of the base models as input and uses them to generate final
    predictions.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the final model in stacking does not take the original data
    as input, but rather takes the predictions of the base machine learning models
    as input.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Stacked models have found huge success in Kaggle competitions. Most Kaggle competitions
    have merger deadlines, where individuals and teams can join together. These mergers
    can lead to greater success as teams rather than individuals because competitors
    can build larger ensembles and stack their models together.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Note that stacking is distinct from a standard ensemble on account of the meta-model
    that combines predictions at the end. Since the meta-model takes predictive values
    as the input, it's generally advised to use a simple meta-model, such as linear
    regression for regression and logistic regression for classification.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an idea of what stacking is, let's apply stacking with scikit-learn.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Stacking in scikit-learn
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fortunately, scikit-learn comes with a stacking regressor and classifier that
    makes the process fairly straightforward. The general idea is very similar to
    the ensemble model in the last section. A variety of base models are chosen, and
    then linear regression or logistic regression is chosen for the meta-model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to use stacking with scikit-learn:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an empty list of base models:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Append all base models to the base model list as tuples using the syntax `(name,
    model)`:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: More models may be chosen when stacking since there are no majority rules limitations
    and linear weights adjust more easily to new data. An optimal approach is to use
    non-correlation as loose a guideline and to experiment with different combinations.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose a meta model, preferably linear regression for regression and logistic
    regression for classification:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Initialize `StackingClassifier` (or `StackingRegressor`) using `base_models`
    for `estimators` and `meta_model` for `final_estimator`:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Validate the stacked model using `cross_val_score` or any other scoring method:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The score is as follows:'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: This is the strongest result yet.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, stacking is an incredibly powerful method and outperformed the
    non-correlated ensemble from the previous section.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned some of the well-tested tips and tricks from the
    winners of Kaggle competitions. In addition to exploring Kaggle competitions and
    understanding the importance of a hold-out set, you gained essential practice
    in feature engineering time columns, feature engineering categorical columns,
    mean encoding, building non-correlated ensembles, and stacking. These advanced
    techniques are widespread among elite Kagglers, and they can give you an edge
    when developing machine learning models for research, competition, and industry.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we will shift gears from the competitive world
    to the tech world, where we will build an XGBoost model from beginning to end
    using transformers and pipelines to complete a model ready for industry deployment.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL

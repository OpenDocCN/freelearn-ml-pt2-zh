["```py\n#Function to compute Euclidean Distance. \ndef euclidean(v1, v2):\n\n    #Convert 1-D Python lists to numpy vectors\n    v1 = np.array(v1)\n    v2 = np.array(v2)\n\n    #Compute vector which is the element wise square of the difference\n    diff = np.power(np.array(v1)- np.array(v2), 2)\n\n    #Perform summation of the elements of the above vector\n    sigma_val = np.sum(diff)\n\n    #Compute square root and return final Euclidean score\n    euclid_score = np.sqrt(sigma_val)\n\n    return euclid_score\n```", "```py\n#Define 3 users with ratings for 5 movies\nu1 = [5,1,2,4,5]\nu2 = [1,5,4,2,1]\nu3 = [5,2,2,4,4]\n```", "```py\neuclidean(u1, u2)\n\nOUTPUT:\n7.4833147735478827\n```", "```py\neuclidean(u1, u3)\n\nOUTPUT:\n1.4142135623730951\n```", "```py\nalice = [1,1,3,2,4]\nbob = [2,2,4,3,5]\n\neuclidean(alice, bob)\n\nOUTPUT:\n2.2360679774997898\n```", "```py\neve = [5,5,3,4,2]\n\neuclidean(eve, alice)\n\nOUTPUT:\n6.324555320336759\n```", "```py\nfrom scipy.stats import pearsonr\n\npearsonr(alice, bob)\n\nOUTPUT:\n(1.0, 0.0)\npearsonr(alice, eve)\n\nOUTPUT:\n(-1.0, 0.0)\n```", "```py\n#Import the function that enables us to plot clusters\nfrom sklearn.datasets.samples_generator import make_blobs\n\n#Get points such that they form 3 visually separable clusters\nX, y = make_blobs(n_samples=300, centers=3,\n                       cluster_std=0.50, random_state=0)\n\n#Plot the points on a scatterplot\nplt.scatter(X[:, 0], X[:, 1], s=50)\n```", "```py\n#Import the K-Means Class\nfrom sklearn.cluster import KMeans\n\n#Initializr the K-Means object. Set number of clusters to 3, \n#centroid initilalization as 'random' and maximum iterations to 10\nkmeans = KMeans(n_clusters=3, init='random', max_iter=10)\n\n#Compute the K-Means clustering \nkmeans.fit(X)\n\n#Predict the classes for every point\ny_pred = kmeans.predict(X)\n\n#Plot the data points again but with different colors for different classes\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50)\n\n#Get the list of the final centroids\ncentroids = kmeans.cluster_centers_\n\n#Plot the centroids onto the same scatterplot.\nplt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X')\n```", "```py\n#List that will hold the sum of square values for different cluster sizes\nss = []\n\n#We will compute SS for cluster sizes between 1 and 8.\nfor i in range(1,9):\n\n    #Initialize the KMeans object and call the fit method to compute clusters \n    kmeans = KMeans(n_clusters=i, random_state=0, max_iter=10, init='random').fit(X)\n\n    #Append the value of SS for a particular iteration into the ss list\n    ss.append(kmeans.inertia_)\n\n#Plot the Elbow Plot of SS v/s K\nsns.pointplot(x=[j for j in range(1,9)], y=ss)\n```", "```py\n#Import the half moon function from scikit-learn\nfrom sklearn.datasets import make_moons\n\n#Get access to points using the make_moons function\nX_m, y_m = make_moons(200, noise=.05, random_state=0)\n\n#Plot the two half moon clusters\nplt.scatter(X_m[:, 0], X_m[:, 1], s=50)\n```", "```py\n#Initialize K-Means Object with K=2 (for two half moons) and fit it to our data\nkmm = KMeans(n_clusters=2, init='random', max_iter=10)\nkmm.fit(X_m)\n\n#Predict the classes for the data points\ny_m_pred = kmm.predict(X_m)\n\n#Plot the colored clusters as identified by K-Means\nplt.scatter(X_m[:, 0], X_m[:, 1], c=y_m_pred, s=50)\n```", "```py\n#Import Spectral Clustering from scikit-learn\nfrom sklearn.cluster import SpectralClustering\n\n#Define the Spectral Clustering Model\nmodel = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')\n\n#Fit and predict the labels\ny_m_sc = model.fit_predict(X_m)\n\n#Plot the colored clusters as identified by Spectral Clustering\nplt.scatter(X_m[:, 0], X_m[:, 1], c=y_m_sc, s=50)\n```", "```py\n# Load the Iris dataset into Pandas DataFrame\niris = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n                 names=['sepal_length','sepal_width','petal_length','petal_width','class'])\n\n#Display the head of the dataframe\niris.head()\n```", "```py\n#Import Standard Scaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler\n\n#Separate the features and the class\nX = iris.drop('class', axis=1)\ny = iris['class']\n\n# Scale the features of X\nX = pd.DataFrame(StandardScaler().fit_transform(X), \n                 columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n\nX.head()\n```", "```py\n#Import PCA\nfrom sklearn.decomposition import PCA\n\n#Intialize a PCA object to transform into the 2D Space.\npca = PCA(n_components=2)\n\n#Apply PCA\npca_iris = pca.fit_transform(X)\npca_iris = pd.DataFrame(data = pca_iris, columns = ['PC1', 'PC2'])\n\npca_iris.head()\n```", "```py\npca.explained_variance_ratio\n\nOUTPUT:\narray([ 0.72770452, 0.23030523])\n```", "```py\n#Concatenate the class variable\npca_iris = pd.concat([pca_iris, y], axis = 1)\n\n#Display the scatterplot\nsns.lmplot(x='PC1', y='PC2', data=pca_iris, hue='class', fit_reg=False)\n```", "```py\n#Import LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#Define the LDA Object to have two components\nlda = LinearDiscriminantAnalysis(n_components = 2)\n\n#Apply LDA\nlda_iris = lda.fit_transform(X, y)\nlda_iris = pd.DataFrame(data = lda_iris, columns = ['C1', 'C2'])\n\n#Concatenate the class variable\nlda_iris = pd.concat([lda_iris, y], axis = 1)\n\n#Display the scatterplot\nsns.lmplot(x='C1', y='C2', data=lda_iris, hue='class', fit_reg=False)\n\n```", "```py\n#Divide the dataset into the feature dataframe and the target class series.\nX, y = iris.drop('class', axis=1), iris['class']\n\n#Split the data into training and test datasets. \n#We will train on 75% of the data and assess our performance on 25% of the data\n\n#Import the splitting function\nfrom sklearn.model_selection import train_test_split\n\n#Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n#Import the Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Apply Gradient Boosting to the training data\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\n\n#Compute the accuracy on the test set\ngbc.score(X_test, y_test)\n\nOUTPUT:\n0.97368421052631582\n```", "```py\n#Display a bar plot of feature importances\nsns.barplot(x= ['sepal_length','sepal_width','petal_length','petal_width'],       y=gbc.feature_importances_)\n\n```"]
<html><head></head><body>
    <section>
      <header class="header-title chapter-title">
                    Classifying Text Using Naive Bayes
                </header>
      <article>
        <div class="packt_quote">"Language is a process of free creation; its laws and principles are fixed, but the manner in which the principles of generation are used is free and infinitely varied. Even the interpretation and use of words involves a process of free creation."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– Noam Chomsky</div>
        <p>Not all information exists in tables. From Wikipedia to social media, there are billions of written words that we would like our computers to process and extract bits of information from. The sub-field of machine learning that deals with textual data goes by names such as <strong>Text Mining</strong> and <strong>Natural Language Processing</strong> (<strong>NLP</strong>). These different names reflect the fact that the field inherits from multiple disciplines. On the one hand, we have computer science and statistics, and on the other hand, we have linguistics. I'd argue that the influence of linguistics was stronger when the field was at its infancy, but in later stages, practitioners came to favor mathematical and statistical tools, as they require less human intervention and can get away without humans manually codifying linguistic rules into the algorithms:</p>
        <div class="packt_quote">"Every time I fire a linguist, the performance of our speech recognition system goes up." </div>
        <div class="packt_quote CDPAlignRight CDPAlign">– Fred Jelinek</div>
        <p>Having said that, it is essential to have a basic understanding of how things have progressed over time and not jump to the bleeding-edge solutions right away. This enables us to pick our tools wisely while being aware of the tradeoffs we are making. Thus, we will start this chapter by processing textual data and presenting it to our algorithms in formats they understand. This preprocessing stage has an important effect on the performance of the downstream algorithms. Therefore, I will make sure to shed light on the pros and cons of each method explained here. Once the data is ready, we will use a <strong>Naive Bayes</strong> classifier to detect the sentiment of different Twitter users based on the messages they send to multiple airway services.</p>
        <p>In this chapter, the following topics will be covered:</p>
        <ul>
          <li>Splitting sentences into tokens</li>
          <li>Token normalization</li>
          <li>Using bag of words to represent tokens</li>
          <li>Using n-grams to represent tokens</li>
          <li>Using Word2Vec to represent tokens</li>
          <li>Text classification with a Naive Bayes classifier </li>
        </ul>
        <h1 id="uuid-f14598f6-dde7-409a-a878-1e4f442e1bcb">Splitting sentences into tokens</h1>
        <div class="packt_quote">"A word after a word after a word is power."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– Margaret Atwood</div>
        <p>So far, the data we have dealt with has either been table data with columns as features or image data with pixels as features. In the case of text, things are less obvious. Shall we use sentences, words, or characters as our features? Sentences are very specific. For example, it is very unlikely to have the exact same sentence appearing in two or more Wikipedia articles. Therefore, if we use sentences as features, we will end up with tons of features that do not generalize well.</p>
        <p>Characters, on the other hand, are limited. For example, there are only 26 letters in the English language. This small variety is likely to limit the ability of the separate characters to carry enough information for the downstream algorithms to extract. As a result, words are typically used as features for most tasks.</p>
        <p>Later in this chapter, we will see that fairly specific tokens are still possible, but let's stick to words as features for now. Finally, we do not want to limit ourselves to dictionary words; Twitter hashtags, numbers, and URLs can also be extracted from text and treated as features. That's why we prefer to use the term <em>token</em> instead <em>word</em>, since it is more generic. The process where a stream of text is split into tokens is called tokenization, and we are going to learn about that in the next section. </p>
        <h2 id="uuid-586c9e94-493a-48eb-af88-0581fbbc28d1">Tokenizing with string split</h2>
        <p>Different tokenization methods lead to different results. To demonstrate these differences, let's take the following three lines of text and see how can we tokenize them.</p>
        <p>Here I write the lines of text as strings and put them into a list:</p>
        <pre>lines = [<br/>    'How to tokenize?\nLike a boss.',<br/>    'Google is accessible via http://www.google.com',<br/>    '1000 new followers! #TwitterFamous',<br/>]</pre>
        <p>One obvious way to do this is to use Python's built-in <kbd>split()</kbd> method as follows:</p>
        <pre>for line in lines:<br/>    print(line.split())</pre>
        <p>When no parameters are given, <kbd>split()</kbd> uses white spaces to split strings based on. Thus, we get the following output:</p>
        <pre>['How', 'to', 'tokenize?', 'Like', 'a', 'boss.']
['Google', 'is', 'accessible', 'via', 'http://www.google.com']
['1000', 'new', 'followers!', '#TwitterFamous']</pre>
        <p>You may notice that the punctuation was kept as part of the tokens. The question mark was left at the end of <kbd>tokenize</kbd>, and the period remained attached to <kbd>boss</kbd>. The hashtag is made of two words, but since there are no spaces between them, it was kept as a single token along with its leading hash sign. </p>
        <h2 id="uuid-8be9abdd-3e5b-43fb-9d03-1b0b18084946">Tokenizing using regular expressions</h2>
        <p>We may also use regular expressions to treat sequences of letters and numbers as tokens, and split our sentences accordingly. The pattern used here, <kbd>"\w+"</kbd>, refers to any sequence of one or more alphanumeric characters or underscores. Compiling our patterns gives us a regular expression object that we can use for matching. Finally, we loop over each line and use the regular expression object to split it into tokens:</p>
        <pre>import re<br/>_token_pattern = r"\w+"<br/>token_pattern = re.compile(_token_pattern)<br/><br/>for line in lines:<br/>    print(token_pattern.findall(line))</pre>
        <p>This gives us the following output:</p>
        <pre>['How', 'to', 'tokenize', 'Like', 'a', 'boss']
['Google', 'is', 'accessible', 'via', 'http', 'www', 'google', 'com']
['1000', 'new', 'followers', 'TwitterFamous']</pre>
        <p>Now, the punctuation has been removed, but the URL has been split into four tokens. </p>
        <div class="packt_infobox">Scikit-learn uses regular expressions for tokenization by default. However, the following pattern, <kbd>r"(?u)\b\w\w+\b"</kbd>, is used instead of <kbd>r"\w+"</kbd>. This pattern ignores all punctuation and words shorter than two letters. So, the "a" token would be omitted. You can still overwrite the default pattern by providing your custom one.</div>
        <h2 id="uuid-5d85cdc7-45d8-42f0-a8e5-98d832a223ac">Using placeholders before tokenizing</h2>
        <p>To deal with the previous problem, we may decide to replace the numbers, URLs, and hashtags with placeholders before tokenizing our sentences. This is useful if we don't really care to differentiate between their content. A URL may be just a URL to me, regardless of where it leads to. The following function converts its input into lower case, then replaces any URL it finds with a <kbd>_url_</kbd> placeholder. Similarly, it converts the hashtags and numbers into their corresponding placeholders. Finally, the input is split based on white spaces, and the resulting tokens are returned:</p>
        <pre>_token_pattern = r"\w+"<br/>token_pattern = re.compile(_token_pattern)<br/><br/>def tokenizer(line):<br/>    line = line.lower()<br/>    line = re.sub(r'http[s]?://[\w\/\-\.\?]+','_url_', line)<br/>    line = re.sub(r'#\w+', '_hashtag_', line)<br/>    line = re.sub(r'\d+','_num_', line)<br/>    return token_pattern.findall(line)<br/><br/>for line in lines:<br/>    print(tokenizer(line))</pre>
        <p>This gives us the following output:</p>
        <pre>['how', 'to', 'tokenize', 'like', 'a', 'boss']
['google', 'is', 'accessible', 'via', '_url_']
['_num_', 'new', 'followers', '_hashtag_']</pre>
        <p>As you can see, the new placeholder tells us that a URL existed in the second sentence, but it doesn't really care where the URL links to. If we have another sentence with a different URL, it will just get the same placeholder as well. The same goes for the numbers and hashtags.</p>
        <p>Depending on your use case, this may not be ideal if your hashtags carry information that you would not like to lose. Again, this is a tradeoff you have to make based on your use case. Usually, you can intuitively tell which technique is more suitable for the problem at hand, but sometimes evaluating a model after multiple tokenization techniques can be the only way to tell which one is more suitable. Finally, in practice, you may use libraries such as <strong>NLTK</strong> and <strong>spaCy</strong> to tokenize your text. They already have the necessary regular expressions under the hood. We will be using spaCy later on in this chapter. </p>
        <div class="packt_tip">Note how I converted the sentence into lower case before processing it. This is called normalization. Without normalization, a capitalized word and a lowercase version of it will be seen as two different tokens. This is not ideal, since <em>Boy</em> and <em>boy</em> are conceptually the same, hence normalization is usually required. Scikit-learn converts input text to lower case by default.</div>
        <h1 id="uuid-67e00712-4f6d-4d83-9f50-c0c32d8cc0c4">Vectorizing text into matrices</h1>
        <p>In text mining, a dataset is usually called a <strong>corpus</strong>. Each data sample in it is usually called a <strong>document</strong>. Documents are made of <strong>tokens</strong>, and a set of distinct tokens is called a <strong>vocabulary</strong>. Putting this information into a matrix is called <strong>vectorization</strong>. In the following sections, we are going to see the different kinds of vectorizations that we can get.</p>
        <h2 id="uuid-341f632c-4c5d-4620-9ca9-d26fcc702156">Vector space model</h2>
        <p>We still miss our beloved feature matrices, where we expect each token to have its own column and each document to be represented by a separate row. This kind of representation for textual data is known as the <strong>vecto</strong><strong>r</strong><strong>space mo</strong><strong>del</strong>. From a linear-algebraic point of view, the documents in this representation are seen as vectors (rows), and the different terms are the dimensions of this space (columns), hence the name vector space model. In the next section, we will learn how to vectorize our documents.</p>
        <h3 id="uuid-d8ff5255-f389-4e3a-a0a8-6199d6c4b7bf">Bag of words</h3>
        <p>We need to convert the documents into tokens and put them into the vector space model. <kbd>CountVectorizer</kbd> can be used here to tokenize the documents and put them into the desired matrix. Here, we are going to use it with the help of the tokenizer we created in the previous section. As usual, we import and initialize <kbd>CountVectorizer</kbd>, and then we use its <kbd>fit_transform</kbd> method to convert our documents. We also specified that we want to use the tokenizer we built in the previous section: </p>
        <pre>from sklearn.feature_extraction.text import CountVectorizer<br/>vec = CountVectorizer(lowercase=True, tokenizer=tokenizer)<br/>x = vec.fit_transform(lines)</pre>
        <p>Most of the cells in the returned matrix are zeros. To save space, it is saved as a sparse matrix; however, we can turn it into a dense matrix using its <kbd>todense()</kbd> method. The vectorizer holds the set of encountered vocabulary, which can be retrieved using <kbd>get_feature_names()</kbd>. Using this information, we can convert <kbd>x</kbd> into a DataFrame as follows:</p>
        <pre>pd.DataFrame(<br/>    x.todense(), <br/>    columns=vec.get_feature_names()<br/>)</pre>
        <p>This gives us the following matrix:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/d6c4ca9d-16c6-4fef-9180-13492778d67f.png" style="width:56.83em;"/>
        </p>
        <p>Each cell contains the number of times each token appears in each document. However, the vocabulary does not follow any order; therefore, it is not possible to tell the order of the tokens in each document from this matrix.</p>
        <h3 id="uuid-f4bf16ab-f350-4f00-b954-f3acbfa9f9c1">Different sentences, same representation</h3>
        <p>Take these two sentences with opposite meanings:</p>
        <pre>flight_delayed_lines = [<br/>    'Flight was delayed, I am not happy',<br/>    'Flight was not delayed, I am happy'<br/>]</pre>
        <p>If we use the count vectorizer to represent them, we will end up with the following matrix:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/2f1e5f0c-b2aa-4795-b849-0fe56cb9ca8a.png" style="width:25.08em;"/>
        </p>
        <p>As you can see, the order of the tokens in the sentences is lost. That is why this method is known as <strong>bag of words</strong> – the result is like a bag that words are just put into without any order. Obviously, this makes it impossible to tell which of the two people is happy and which is not. To fix this problem, we may need to use <strong>n-grams</strong>, as we will do in the following section. </p>
        <h3 id="uuid-aac99f6f-caae-4ec4-9e79-5006f65fc1c7">N-grams</h3>
        <p>Rather than treating each term as a token, we can treat the combinations of each two consecutive terms as a single token. All we have to do is to set <kbd>ngram_range</kbd> in <kbd>CountVectorizer</kbd> to <kbd>(2,2)</kbd>, as follows:</p>
        <pre>from sklearn.feature_extraction.text import CountVectorizer<br/>vec = CountVectorizer(ngram_range=(2,2))<br/>x = vec.fit_transform(flight_delayed_lines)</pre>
        <p>Using similar code to that used in the previous section, we can put the resulting <kbd>x</kbd> into a DataFrame and get the following matrix:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/2c2c898e-bdec-4f61-ab94-ef3bbe0bdc6b.png" style="width:51.08em;"/>
        </p>
        <p>Now we can tell who is happy and who is not. When using word pairs, this is known as <strong>bigrams</strong>. We can also do 3-grams (with three consecutive words), 4-grams, or any other number of grams. Setting <kbd>ngram_range</kbd> to (1,1) takes us back to the original representation where each separate word is a token, which is <strong>unigrams</strong>. We can also mix unigrams with bigrams by setting <kbd>ngram_range</kbd><em><strong/></em>to (1,2). In brief, this range tells the tokenizer the minimum and maximum values for<em>n</em> to use in our n-grams.</p>
        <div class="packt_tip">If you set <em>n</em> to a high value – say, 8 – this means that sequences of eight words are treated as tokens. Now, how likely do you think it is that a sequence of eight words will appear more than once in your dataset? Most likely, you will see it once in your training set and never again into the test set. That's why <em>n</em> is usually set to something between 2 and 3, with some unigrams also being used to capture rare words. </div>
        <h3 id="uuid-01c4dcd0-b196-49db-8f53-fd946f0d0d7b">Using characters instead of words</h3>
        <p>Up until now, words have been the atoms of our textual universe. However, some situations may require us to tokenize our documents based on characters instead. In situations where word boundaries are not clear, such as in hashtags and URLs, the use of characters as tokens may help. Natural languages tend to have different frequencies for their characters. The letter <strong>e</strong> is the most commonly used character in the English language, and character combinations such as <strong>th</strong>, <strong>er</strong>, and <strong>on</strong> are also very common. Other languages, such as French and Dutch, have different character frequencies. If our aim is to classify documents based on their languages, the use of characters instead of words can come in handy.</p>
        <p>The very same <kbd>CountVectorizer</kbd> can help us tokenize our documents into characters. We can also combine this with the <kbd>n-grams</kbd> setting to get subsequences within words, as follows:</p>
        <pre>from sklearn.feature_extraction.text import CountVectorizer<br/>vec = CountVectorizer(analyzer='char', ngram_range=(4,4))<br/>x = vec.fit_transform(flight_delayed_lines)</pre>
        <p>We can put the resulting <kbd>x</kbd> into a DataFrame, as we did earlier, to get the following matrix:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/ea3c937d-0287-467f-8625-0844a79f8a9a.png" style="width:54.08em;"/>
        </p>
        <p>All our tokens are made of four characters now. Whitespaces are also treated as characters, as you can see. With characters, it is more common to go for higher values of <em>n</em>.</p>
        <h3 id="uuid-6e84759f-b2c7-4072-a2e8-2ee65ec29c9b">Capturing important words with TF-IDF</h3>
        <p>Another discipline that we borrow lots of ideas from here is the <strong>information retrieval</strong> field. It's the field responsible for the algorithms that run search engines such as Google, Bing, and DuckDuckGo.</p>
        <p>Now, take the following quotation:</p>
        <div class="b-qt qt_848830 packt_quote">"From a linguistic point of view, you can't really take much objection to the notion that a show is a show is a show."</div>
        <div class="b-qt qt_848830 packt_quote CDPAlignRight CDPAlign">– Walter Becker</div>
        <p>The word <strong>linguistic</strong> and the word <strong>that</strong> both appeared exactly once in the previous quotation. Nevertheless, we would only worry about the word <strong>linguistic</strong>, not the word <strong>that</strong>, if we were searching for this quotation on the internet. We know that it is more significant, although it appeared only once, just as many times as <strong>that</strong>. The word <strong>show</strong> appeared three times. From a count vectorizer's point of view, it should carry three times more information than the word <strong>linguistic</strong>. I assume you also disagree with the vectorizer about that. Those issues are fundamentally the raison d'être of <strong>Term Frequency</strong>-<strong>Inverse Document Frequency</strong><em/>(<strong>TF-IDF</strong>). The IDF part not only involves weighting the value of the words based on how frequently they appear in a certain document, but also discounting weights from them if they happen to be very common in other documents. The word <strong>that</strong> is so common across other documents that it shouldn't be given as much value as <strong>linguistic</strong>. Furthermore, IDF uses a logarithmic scale to better represent the information a word carries based on its frequency in a document.</p>
        <p>Let's use the following three documents to demonstrate how TF-IDF works:</p>
        <pre>lines_fruits = [<br/>    'I like apples',<br/>    'I like oranges',<br/>    'I like pears',<br/>]</pre>
        <p><kbd>TfidfVectorizer</kbd> has an almost identical interface to that of<kbd>CountVectorizer</kbd>:</p>
        <pre>from sklearn.feature_extraction.text import TfidfVectorizer<br/>vec = TfidfVectorizer(token_pattern=r'\w+')<br/>x = vec.fit_transform(lines_fruits)</pre>
        <p>Here is a comparison for the outputs of the two vectorizers side by side:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/224edc67-6a5f-4823-bfe1-b259b922069c.png" style="width:49.67em;"/>
        </p>
        <p>As you can see, unlike in <kbd>CountVectorizer</kbd>, not all words were treated equally by <kbd>TfidfVectorizer</kbd>. More emphasis was given to the fruit names compared to the other, less informative words that happened to appear in all three sentences. </p>
        <p>Both <kbd>CountVectorizer</kbd> and<em><strong/></em><kbd>TfidfVectorizer</kbd><em><strong/></em>have a parameter called <kbd>stop_words</kbd>. It can be used to specify tokens to be ignored. You can provide your own list of less informative words, such as <strong>a</strong>, <strong>an</strong>, and <strong>the</strong>. You can also provide the <kbd>english</kbd><em/>keyword to specify the common stop words in the English language. Having said that, it is important to note that some words can be informative for one task but not for another. Furthermore, IDF usually does what you need it to do automatically and gives low weights to non-informative words. That is why I usually prefer not to manually remove stop words, instead trying things such as <kbd>TfidfVectorizer</kbd>, feature selection, and regularization<em><strong/></em>first. </p>
        <p>Besides its original use case,<kbd>TfidfVectorizer</kbd> is commonly used as a preprocessing step for text classification. Nevertheless, it usually gives good results when longer documents are to be classified. For short documents, it may produce noisy transformation, and it is advised to give<kbd>CountVectorizer</kbd> a try in such cases.</p>
        <div class="packt_infobox">In a basic search engine, when someone types a query, it gets converted into the same vector space where all the documents to be searched exist, using TF-IDF. Once the search query and the documents exist as vectors in the same space, a simple distance measure such as cosine distance can be used to find the closest documents to the query. Modern search engines vary from this basic idea, but it is a good base to build your understanding of information retrieval on.</div>
        <h2 id="uuid-7d7c3abe-31af-49a1-9243-397d88a6fbf3">Representing meanings with word embedding</h2>
        <p>As documents are collections of tokens, their vector representations are basically the sum of the vectors of the tokens they contain. As we have seen earlier, the<strong> I like apples</strong> document was represented by <kbd>CountVectorizer</kbd> using the vector [1,1,1,0,0]:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/7838de63-46e3-4cae-84cc-810e05362a52.png" style="width:26.83em;"/>
        </p>
        <p>From this representation, we can also deduce that the terms <strong>I</strong>, <strong>like</strong>, <strong>apples</strong>, and <strong>oranges </strong>are represented by the following four five-dimensional vectors, [0,1,0,0,0], [0,0,1,0,0], [1,0,0,0,0], and [0,0,0,1,0]. We have a five-dimensional space, given our vocabulary of five terms. Each term has a magnitude of 1 in one dimension and 0 in the other four dimensions. From a linear algebraic point of view, all five terms are orthogonal (perpendicular) to each other. Nevertheless, <strong>apples</strong>, <strong>pears</strong>, and <strong>oranges </strong>are all fruits, and conceptually they have some similarity that was not captured by this model. Therefore, we would ideally like to represent them with vectors that are closer to each other, unlike these orthogonal vectors. The same issue here applied to <kbd>TfidfVectorizer</kbd>, by the way<em><strong>.</strong></em> This was the driver for researchers to come up with better representations, and word embedding is the coolest kid on the natural language processing block nowadays, as it tries to capture meaning better than traditional vectorizers. In the next section, we will get to know one popular embedding technique, Word2Vec.</p>
        <h3 id="uuid-3124ebff-c512-4587-8b36-3eee234a1a0f">Word2Vec</h3>
        <p>Without getting into the details too much, Word2Vec uses neural networks to predict words from their context, that is, from their surrounding words. By doing so, it learns better representations for the different words, and these representations incorporate the meanings of the words they represent. Unlike the previously mentioned vectorizers, the dimensionality of the word representation is not directly linked to the size of our vocabulary. We get to choose the length of our embedding vectors. Once each word is represented by a vector, the document's representation is usually the summation of all the vectors of its words. Averaging is also an option instead of summation.</p>
        <p>Since the size of our vectors is independent of the size of the vocabulary of the documents we are dealing with, researchers can reuse a pre-trained Word2Vec model that wasn't made specifically for their particular problem. This ability to re-use pre-trained models is known as transfer learning. Some researchers can train an embedding on a huge amount of documents using expensive machines and release the resulting vectors for the entire world to use. Then, the next time we deal with a specific natural language processing task, all we need to do is to get these vectors and use them to represent our new documents. spaCy (<a href="https://spacy.io/">https://spacy.io/</a>) is an open source software library that comes with word vectors for different languages.</p>
        <p>In the following few lines of code, we will install spaCy, download its language model data, and use it to convert words into vectors:</p>
        <ol>
          <li>To use spaCy, we can install the library and download its pre-trained models for the English language by running the following commands in our terminal:</li>
        </ol>
        <pre style="padding-left: 60px">
          <strong>pip install spacy</strong>
          <br/>
          <strong>python -m spacy download en_core_web_lg</strong>
        </pre>
        <ol start="2">
          <li>Then, we can assign the downloaded vectors to our five words as follows:</li>
        </ol>
        <pre style="padding-left: 60px">import spacy<br/>nlp = spacy.load('en_core_web_lg')<br/><br/>terms = ['I', 'like', 'apples', 'oranges', 'pears']<br/>vectors = [<br/>    nlp(term).vector.tolist() for term in terms<br/>]</pre>
        <ol start="3">
          <li>Here is the representation for <strong>apples</strong>:</li>
        </ol>
        <pre style="padding-left: 60px"># pd.Series(vectors[terms.index('apples')]).rename('apples')<br/><br/>0     -0.633400
1      0.189810
2     -0.535440
3     -0.526580
         ...   
296   -0.238810
297   -1.178400
298    0.255040
299    0.611710
Name: apples, Length: 300, dtype: float64</pre>
        <p>I promised you that the representations for <strong>apples</strong>, <strong>oranges</strong>, and <strong>pears </strong>would not be orthogonal as in the case with <kbd>CountVectorizer</kbd>. However, with 300 dimensions, it is hard for me to visually prove that. Luckily, we have already learned how to calculate the cosine of the angle between two vectors. Orthogonal vectors should have 90<sup>o</sup> angles between them, whose cosines are equal to 0. The cosine for the zero angle between two vectors going in the exact same direction is 1.</p>
        <p>Here, we calculate the cosine between all the five vectors we got from spaCy. I used some pandas and seaborn styling to make the numbers clearer:</p>
        <pre>import seaborn as sns<br/>from sklearn.metrics.pairwise import cosine_similarity<br/><br/>cm = sns.light_palette("Gray", as_cmap=True)<br/><br/>pd.DataFrame(<br/>    cosine_similarity(vectors),<br/>    index=terms, columns=terms,<br/>).round(3).style.background_gradient(cmap=cm)</pre>
        <p>Then, I showed the results in the following DataFrame:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/54477c03-db2a-4444-8191-78cc4fc479e5.png" style="width:24.08em;"/>
        </p>
        <p>Clearly, the new representation understands that fruit names are more similar to each other than they are to words such as <strong>I </strong>and <strong>like</strong>. It also considered <strong>apples </strong>and <strong>pears</strong> to be very similar to each other, as opposed to <strong>oranges</strong>.   </p>
        <div class="packt_infobox">You may have noticed that Word2Vec suffers from the same problem as unigrams; words are encoded without much attention being paid to their context. The representation for the word "book" in "I will read a book" is the same as its representation in "I will book a flight." That's why newer techniques, such as <strong>Embeddings from Language Models</strong> (<strong>ELMo</strong>), <strong>Bidirectional Encoder Representations from Transformers</strong> (<strong>BERT</strong>) and OpenAI's recent <strong>GPT-3</strong> are gaining more popularity nowadays as they respect the words' context. I expect them to be included in more libraries soon for anyone to easily use them. <br/><br/>
The embedding concept is recycled and reused by machine learning practitioners everywhere nowadays. Apart from its use in natural language processing, it is used for feature reduction and in recommendation systems. For instance, every time a customer adds an item to their online shopping cart, if we treat the cart as a sentence and the items as words, we end up with item embeddings (<strong>Item2Vec</strong>). These new representations for the items can easily be plugged into a downstream classifier or a recommender system.</div>
        <p>Before moving to text classification, we need to stop and spend some time first to learn about the classifier we are going to use – the <strong>Naive Bayes classifier</strong>. </p>
        <h1 id="uuid-f41bafde-34c9-4b26-ae38-a35c329c02a9">Understanding Naive Bayes</h1>
        <p>The Naive Bayes classifier is commonly used in classifying textual data. In the following sections, we are going to see its different flavors and learn how to configure their parameters. But first, to understand the Naive Bayes classifier, we need to first go through Thomas Bayes' theorem, which he published in the 18<sup>th</sup> century.</p>
        <h2 id="uuid-331db4cc-3b46-47b3-9209-f1cdba6fa69e">The Bayes rule </h2>
        <p>When talking about classifiers, we can describe the probability of a certain sample belonging to a certain class using conditional probability, <em>P(y|x)</em>. This is the probability of a sample belonging to class <em>y</em> given its features, <em>x</em>. The pipe sign (|) is what we use to refer to conditional probability, that is, <em>y</em> given <em>x</em>. The Bayes rule is capable of expressing this conditional probability in terms of <em>P(x|y)</em>, <em>P(x)</em>, and <em>P(y)</em>, using the following formula:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/5af08cd0-2cfe-4726-95b2-787a4098441d.png" style="width:11.75em;"/>
        </p>
        <p>Usually, we ignore the denominator part of the equation and convert it into a proportion as follows:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/82a8bcff-9301-4949-8174-c274048cad51.png" style="width:11.08em;"/>
        </p>
        <p>The probability of a class, <em>P(y)</em>, is known as the prior probability. It's basically the number of samples that belong to a certain class out of all training samples. The conditional probability, <em>P(x|y)</em>, is known as the likelihood. It's what we calculate from the training samples. Once the two probabilities are known at training time, we can use them to predict the chance of a new sample belonging to a certain class at prediction time, <em>P(y|x)</em>, also known as the posterior probability. Calculating the likelihood part of the equation is not as simple as we expect. So, in the next section, we are going to discuss the assumption we can make to ease this calculation.</p>
        <h2 id="uuid-d67c3435-2eea-40c5-ac44-376031a1e06b">Calculating the likelihood naively </h2>
        <p>A data sample is made of multiple features, which means that in reality, the <em>x</em> part of <em>P(x|y)</em> is made of <em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, <em>x<sub>3</sub></em>, .... <em>x<sub>k</sub></em>, where <em>k</em> is the number of features. Thus, the conditional probability can be expressed as <em>P(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, .... x<sub>k</sub>|y)</em>. In practice, this means that we need to calculate this conditional probability for all possible combinations of <em>x</em>. The main drawback of this is the lack of generalization of our models. </p>
        <p>Let's use the following toy example to make things clearer:</p>
        <table style="border-collapse: collapse;width: 100%" border="1">
          <tbody>
            <tr>
              <td style="width: 32%">
                <strong>Text</strong>
              </td>
              <td style="width: 20.3704%">
                <strong>Does the text suggest that the writer likes fruit?</strong>
              </td>
            </tr>
            <tr>
              <td style="width: 32%">I like apples</td>
              <td style="width: 20.3704%" class="CDPAlignCenter CDPAlign">Yes</td>
            </tr>
            <tr>
              <td style="width: 32%">I like oranges</td>
              <td style="width: 20.3704%" class="CDPAlignCenter CDPAlign">Yes</td>
            </tr>
            <tr>
              <td style="width: 32%">I hate pears</td>
              <td style="width: 20.3704%" class="CDPAlignCenter CDPAlign">No</td>
            </tr>
          </tbody>
        </table>
        <p/>
        <p>If the previous table is our training data, the likelihood probability, <em>P(x|y)</em>, for the first sample is the probability of seeing the three words <strong>I</strong>, <strong>like</strong>, and <strong>apples</strong> together, given the target, <strong>Yes</strong>. Similarly, for the second sample, it is the probability of seeing the three words <strong>I</strong>, <strong>like</strong>, and <strong>oranges</strong> together, given the target, <strong>Yes</strong>. The same goes for the third sample, where the target is <strong>No</strong> instead of <strong>Yes</strong>. Now, say we are given a new sample, <strong>I hate apples</strong>. The problem is that we have never seen these three words together before. You might say, "But we've seen each individual word of the sentence before, just separately!" That's correct, but our formula only cares about combinations of words. It cannot learn anything from each separate feature on its own.</p>
        <p>You may recall from <a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=27&amp;action=edit">Chapter 4</a>, <em>Preparing Your Data</em>, that <em>P(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, .... x<sub>k</sub>|y)</em> can only be expressed as <em>P(x<sub>1</sub>|y)* P(x<sub>2</sub>|y)x<sub>3</sub>* .. * P(x<sub>k</sub>|y)</em> if <em>x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, .... x<sub>k</sub></em> are independent. Their independence is not something we can be sure of, yet we still make this naive assumption in order to make the model more generalizable. As a result of this assumption and dealing with separate words, we can now learn something about the phrase <strong>I hate apples</strong>, despite not seeing it before. This naive yet useful assumption of independence is what gave the classifier's name its "naive" prefix.</p>
        <h2 id="uuid-43027924-711e-4dcf-97d5-4c6b90789e7e">Naive Bayes implementations</h2>
        <p>In scikit-learn, there are various Naive Bayes implementations. </p>
        <ul>
          <li>The <strong>multinomial Naive Bayes</strong> classifier is the most commonly used implementation for text classification. Its implementation is most similar to what we saw in the previous section.</li>
          <li>The <strong>Bernoulli Naive Bayes</strong><em><strong/></em>classifier assumes the features to be binary. Rather than counting how many times a term appears in each document, in the Bernoulli version, we only care whether a term exists or not. The way the likelihood is calculated explicitly penalizes the non-occurrence of the terms in the documents, and it might perform better on some datasets, especially those with shorter documents. </li>
          <li><strong>Gaussian Naive Bayes</strong> is used with continuous features. It assumes the features to be normally distributed and calculates the likelihood probabilities using maximum likelihood estimation. This implementation is useful for other cases aside from text analysis. </li>
        </ul>
        <p>Furthermore, you can also read about two other implementations, <strong>complement Naive Bayes</strong> and <strong>categorical Naive Bayes</strong>, in the scikit-learn user guide (<a href="https://scikit-learn.org/stable/modules/naive_bayes.html">https://scikit-learn.org/stable/modules/naive_bayes.html</a>).</p>
        <h3 id="uuid-458e4d64-7b80-455e-997a-be482a63eba6">Additive smoothing</h3>
        <p>When a term not seen during training appears during prediction, we set its probability to 0. This sounds logical, yet it is a problematic decision to make given our naive assumption. Since <em>P(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, .... x<sub>k</sub>|y)</em> is equal to <em>P(x<sub>1</sub>|y)* P(x<sub>2</sub>|y)*P(x<sub>3</sub>|y) * .. * P(x<sub>k</sub>|y),</em> setting the conditional probability for any term to zero will set the entire <em>P(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, .... x<sub>k</sub>|y)</em> to zero as a result. To avoid this problem, we pretend that a new document that contains the whole vocabulary was added to each class. Conceptually, this new hypothetical document takes a portion of the probability mass assigned to the terms we have seen and reassigns it to the unseen terms. The <kbd>alpha</kbd> parameter controls how much of the probability mass we want to reassign to the unseen terms. Setting <kbd>alpha</kbd> to 1 is called <strong>Laplace smoothing</strong>, while setting it to values between 0 and 1 is called <strong>Lidstone</strong><strong>smoothing</strong>. </p>
        <p>I find myself using Laplace smoothing a lot when calculating ratios. In addition to preventing us from dividing by zero, it also helps to deal with uncertainties. Let me explain further using the following two examples:</p>
        <ul>
          <li><strong>Example 1</strong>: 10,000 people saw a link, and 9,000 of them clicked on it. We can obviously estimate the click-through rate to be 90%.</li>
          <li><strong>Example 2</strong>: If our data has only one person, and that person saw the link and clicked on it, would we be confident enough to say that the click-through rate was 100%?<br/></li>
        </ul>
        <p>In the previous examples, if we pretended that there were two additional users, where only one of them clicked on the link, the click-through rate in the first example would become 9,001 out of 10,002, which is still almost 90%. In the second example, though, we would be dividing 2 by 3, which would leave 60%, instead of the 100% calculated earlier. Laplace smoothing and Lidstone smoothing can be linked to the Bayesian way of thinking. Those two users, where 50% of them clicked on the link, are our prior belief. Initially, we do not know much, so we assume a 50% click-through rate. Now, in the first example, we have enough data to overrule this prior belief, while in the second case, the fewer data points were only able to move the prior so much. </p>
        <p>That's enough theory for now – let's use everything we have learned so far to tell whether some reviewers are happy about their movie-watching experience or not. </p>
        <h1 id="uuid-72d73a89-9e8d-4e43-8939-56cbd0da9231">Classifying text using a Naive Bayes classifier</h1>
        <p>In this section, we are going to get a list of sentences and classify them based on the user's sentiment. We want to tell whether the sentence carries a positive or a negative sentiment. <em>Dimitrios Kotzias et al</em> created this dataset for their research paper, <em>From Group to Individual Labels using Deep Features</em>. They collected a list of random sentences from three different websites, where each sentence is labeled with either 1 (positive sentiment) or 0 (negative sentiment). </p>
        <p>In total, there are 2,745 sentences in the data set. In the following sections, we are going to download the dataset, preprocess it, and classify the sentences in it. </p>
        <h2 id="uuid-9238359f-6527-495a-b983-8b7f1e318e1b">Downloading the data</h2>
        <p>You can just open the browser, download the CSV files into a local folder, and use pandas to load the files into DataFrames. However, I prefer to use Python to download the files, rather than the browser. I don't do this out of geekiness, but to ensure the reproducibility of my entire process by putting it into code. Anyone can just run my Python code and get the same results, without having to read a lousy documentation file, find a link to the compressed file, and follow the instructions to get the data.</p>
        <p>Here are the steps to download the data we need:</p>
        <ol>
          <li>First, let's create a folder to store the downloaded data into it. The following code checks whether the required folder exists or not. If it is not there, it creates it into the current working directory:</li>
        </ol>
        <pre style="padding-left: 60px">import os<br/><br/>data_dir = f'{os.getcwd()}/data'<br/><br/>if not os.path.exists(data_dir):<br/>    os.mkdir(data_dir)</pre>
        <ol start="2">
          <li>Then we need to install the <kbd>requests</kbd> library using <kbd>pip</kbd>, as we will use it to download the data:</li>
        </ol>
        <pre style="padding-left: 60px">
          <strong>pip install requests</strong>
        </pre>
        <ol start="3">
          <li>Then, we download the compressed data as follows:</li>
        </ol>
        <pre style="padding-left: 60px">import requests<br/><br/>url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'<br/><br/>response = requests.get(url)</pre>
        <ol start="4">
          <li>Now, we can uncompress the data and store it into the data folder we have just created. We will be using the <kbd>zipfile</kbd> module to uncompress our data. The <kbd>ZipFile</kbd> method expects to read a file object. Thus, we use <kbd>BytesIO</kbd> to convert the content of the response into a file-like object. Then we extract the content of the zip file into our folder as follows:</li>
        </ol>
        <pre style="padding-left: 60px">import zipfile<br/><br/>from io import BytesIO<br/><br/>with zipfile.ZipFile(file=BytesIO(response.content), mode='r') as compressed_file:<br/>    compressed_file.extractall(data_dir)</pre>
        <ol start="5">
          <li>Now that our data is written into 3 separate files in our data folder, we can load each one of the 3 files into a separate data frame. Then, we can combine the 3 data frames into a single data frame as follows:</li>
        </ol>
        <pre style="padding-left: 60px">df_list = []<br/><br/>for csv_file in ['imdb_labelled.txt', 'yelp_labelled.txt', 'amazon_cells_labelled.txt']:<br/><br/>    csv_file_with_path = f'{data_dir}/sentiment labelled sentences/{csv_file}'<br/>    temp_df = pd.read_csv(<br/>        csv_file_with_path, <br/>        sep="\t", header=0, <br/>        names=['text', 'sentiment']<br/>    ) <br/>    df_list.append(temp_df)<br/><br/>df = pd.concat(df_list)</pre>
        <ol start="6">
          <li>We can display the distribution of the sentiment labels using the following code:</li>
        </ol>
        <pre style="padding-left: 60px">explode = [0.05, 0.05]<br/>colors = ['#777777', '#111111']<br/>df['sentiment'].value_counts().plot(<br/>    kind='pie', colors=colors, explode=explode<br/>)</pre>
        <p style="padding-left: 60px">As we can see, the two classes are more or less equal. It is a good practice to check the distribution of your classes before running any classification task:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/2a79736b-17ac-4eb2-bed3-ed7cbe53ea12.png" style="width:18.00em;"/>
        </p>
        <ol start="7">
          <li>We can also display a few sample sentences using the following code, after tweaking pandas' settings to display more characters per cell:</li>
        </ol>
        <pre style="padding-left: 60px">pd.options.display.max_colwidth = 90<br/>df[['text', 'sentiment']].sample(5, random_state=42)</pre>
        <p>I set the <kbd>random_state</kbd> to an arbitrary value to make sure we both get the same samples as below: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/62567e34-644b-408c-bb6c-8ef36434604f.png" style="width:60.50em;"/>
        </p>
        <h2 id="uuid-992c3290-65b6-478f-8147-a9d8c92c5cb8">Preparing the data</h2>
        <p>Now we need to prepare the data for our classifier to use it:</p>
        <ol>
          <li>As we usually do, we start by splitting the DataFrame into training and testing sets. I kept 40% of the data set for testing, and also set <kbd>random_state</kbd> to an arbitrary value to make sure we both get the same random split:</li>
        </ol>
        <pre style="padding-left: 60px">from sklearn.model_selection import train_test_split<br/>df_train, df_test = train_test_split(df, test_size=0.4, random_state=42)</pre>
        <ol start="2">
          <li>Then we get our labels from the sentiment column as follows:</li>
        </ol>
        <pre style="padding-left: 60px">y_train = df_train['sentiment']<br/>y_test = df_test['sentiment']</pre>
        <ol start="3">
          <li>As for the textual features, let's convert them using <kbd>CountVectorizer</kbd>. We will include unigrams as well as bigrams and trigrams. We can also ignore rare words by setting <kbd>min_df</kbd> to <kbd>3</kbd> to exclude words appearing in fewer than three documents. This is a useful practice for removing spelling mistakes and noisy tokens. Finally, we can strip accents from letters and convert them to <kbd>ASCII</kbd>:</li>
        </ol>
        <pre style="padding-left: 60px">from sklearn.feature_extraction.text import CountVectorizer<br/><br/>vec = CountVectorizer(ngram_range=(1,3), min_df=3, strip_accents='ascii')<br/>x_train = vec.fit_transform(df_train['text'])<br/>x_test = vec.transform(df_test['text'])</pre>
        <ol start="4">
          <li>In the end, we can use the Naive Bayes classifier to classify our data. We set <kbd>fit_prior=True</kbd> for the model to use the distribution of the class labels in the training data as its prior:</li>
        </ol>
        <pre style="padding-left: 60px">from sklearn.naive_bayes import MultinomialNB<br/>clf = MultinomialNB(fit_prior=True)<br/>clf.fit(x_train, y_train)<br/>y_test_pred = clf.predict(x_test)</pre>
        <p>This time, our old good accuracy score may not be informative enough. We want to know how accurate we are per class. Furthermore, depending on our use case, we may need to tell whether the model was able to identify all the negative tweets, even if it did that at the expense of misclassifying some positive tweets. To be able to get this information, we need to use the <kbd>precision</kbd> and <kbd>recall</kbd> scores. </p>
        <h2 id="uuid-c382f03a-e1cb-4044-820c-8a907408d448">Precision, recall, and F1 score</h2>
        <p>Out of the samples that were assigned to the positive class, the percentage of them that were actually positive is the<strong>precision</strong> of this class. For the positive tweets, the percentage of them that the classifier correctly predicted to be positive is the <strong>recall</strong> for this class. As you can see, the precision and recall are calculated per class. Here is how we formally express the <strong>precision score</strong> in terms of true positives and false positives:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/eeb004df-1cbb-438e-9d77-7389cd083b56.png" style="width:18.83em;"/>
        </p>
        <p>The <strong>recall score</strong> is expressed in terms of true positives and false negatives<em>:</em></p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/c8d8af97-42d7-4c94-a66d-33da59706fa2.png" style="width:18.08em;"/>
        </p>
        <p>To summarize the two previous scores into one number, the <em>F<sub>1</sub> score</em> can be used. It combines the precision and recall scores using the following formula:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/bb6db0cd-d232-4b46-b24b-27bfe55625de.png" style="width:16.75em;"/>
        </p>
        <p>Here we calculate the three aforementioned metrics for our classifier:</p>
        <pre>p, r, f, s = precision_recall_fscore_support(y_test, y_test_pred)</pre>
        <p>To make it clear, I put the resulting metrics into the following table. Keep in mind that the support is just the number of samples in each class:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/a748541e-1612-4be1-863d-635d2eee912e.png" style="width:17.50em;"/>
        </p>
        <p>We have equivalent scores given that the sizes of the two classes are almost equal. In cases where the classes are imbalanced, it is more common to see one class achieving a higher precision or a higher recall compared to the other.</p>
        <div class="packt_infobox">Since these metrics are calculated per class label, we can also get their macro averages. For this example here, the macro average precision score will be the average of <strong>0.81</strong>, and <strong>0.77</strong>, which is <strong>0.79</strong>. A micro average, on the other hand, calculates these scores globally based on the overall number of true positive, false positive, and false negative samples.</div>
        <h2 id="uuid-8b687ce1-a5ec-4aae-baa4-25676eada16d">Pipelines</h2>
        <p>In the previous chapters, we used a grid search to find the optimal hyperparameters for our estimators. Now, we have multiple things to optimize at once. One the one hand, we want to optimize the Naive Bayes hyperparameters, but on the other hand, we also want to optimize the parameters of the vectorizer used at the preprocessing step. Since a grid search expects one object only, scikit-learn provides a <kbd>pipeline</kbd> wrapper where we can combine multiple transformers and estimators into one. </p>
        <p>As the name suggests, the pipeline is made of a set of sequential steps. Here we start with <kbd>CountVectorizer</kbd> and have <kbd>MultinomialNB</kbd> as the second and final step:</p>
        <pre>from sklearn.pipeline import Pipeline<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.naive_bayes import MultinomialNB<br/><br/>pipe = Pipeline(steps=[<br/>    ('CountVectorizer', CountVectorizer()), <br/>    ('MultinomialNB', MultinomialNB())]<br/>)</pre>
        <p>All objects but the one in the last step are expected to be <kbd>transformers</kbd>; that is, they should have the <kbd>fit</kbd>, <kbd>transform</kbd>, and <kbd>fit_transform</kbd> methods. The object in the last step is expected to be <kbd>estimator</kbd>, meaning it should have the <kbd>fit</kbd> and <kbd>predict</kbd> methods. You can also build your custom transformers and estimators and use them in the pipeline as long as they have the expected methods. </p>
        <p>Now that we have our pipeline ready, we can plug it into <kbd>GridSearchCV</kbd> to find the optimal hyperparameters. </p>
        <h3 id="uuid-4123fda1-59e9-4078-9e4a-5d5f109077e9">Optimizing for different scores</h3>
        <div class="packt_quote">"What gets measured gets managed."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– Peter Drucker</div>
        <p>When we used <kbd>GridSearchCV</kbd> before, we did not specify which metric we want to optimize our hyperparameters for. The classifier's accuracy was used by default. Alternatively, you can also choose to optimize your hyperparameters for the precision score or the recall score. We will set our grid search here to optimize for the macro precision score.</p>
        <p>We start by setting the different hyperparameters that we want to search within. Since we are using a pipeline here, we prefix each hyperparameter with the name of the step it is designated for, in order for the pipeline to assign the parameter to the correct step:</p>
        <pre>param_grid = {<br/>    'CountVectorizer__ngram_range': [(1,1), (1,2), (1,3)],<br/>    'MultinomialNB__alpha': [0.1, 1],<br/>    'MultinomialNB__fit_prior': [True, False],<br/>}</pre>
        <p>By default, the priors, <kbd>P(y)</kbd>, in the Bayes rule are set based on the number of samples in each class. However, we can set them to be constant for all classes by setting <kbd>fit_prior=False</kbd>. </p>
        <p>Here, we run <kbd>GridSearchCV</kbd> while letting it know that we care about precision the most:</p>
        <pre>from sklearn.model_selection import GridSearchCV<br/>search = GridSearchCV(pipe, param_grid, scoring='precision_macro', n_jobs=-1)<br/>search.fit(df_train['text'], y_train)<br/>print(search.best_params_)</pre>
        <p>This gives us the following hyperparameters:</p>
        <ul>
          <li><kbd>ngram_range</kbd>: (1, 3)</li>
          <li><kbd>alpha</kbd>: 1</li>
          <li><kbd>fit_prior</kbd>: False</li>
        </ul>
        <p>We get a macro precision of 80.5% and macro recall of 80.5%. </p>
        <p>Due to the balanced class distributions, it was expected for the prior not to add much value. We also get similar precision and recall scores. Thus, it doesn't make sense now to re-run the grid search again for an optimized recall. We will most likely get identical results anyway. Nevertheless, things will likely be different when you deal with highly imbalanced classes, and you want to maximize the recall of one class at the expense of the others.</p>
        <p>In the next section, we are going to use word embeddings to represent our tokens. Let's see if this form of transfer learning will help our classifier perform better. </p>
        <h1 id="uuid-c0fd07d6-4f68-488e-bcc8-73b11c2c603f">Creating a custom transformer</h1>
        <p>Before ending this chapter, we can also create a custom transformer based on the <kbd>Word2Vec</kbd> embedding and use it in our classification pipeline instead of <kbd>CountVectorizer</kbd>. In order to be able to use our custom transformer in the pipeline, we need to make sure it has <kbd>fit</kbd>, <kbd>transform</kbd>, and <kbd>fit_transform</kbd> methods.</p>
        <p>Here is our new transformer, whichwe will call <kbd>WordEmbeddingVectorizer</kbd>:</p>
        <pre>import spacy<br/><br/>class WordEmbeddingVectorizer:<br/><br/>    def __init__(self, language_model='en_core_web_md'):<br/>        self.nlp = spacy.load(language_model)<br/><br/>    def fit(self):<br/>        pass<br/><br/>    def transform(self, x, y=None):<br/>        return pd.Series(x).apply(<br/>            lambda doc: self.nlp(doc).vector.tolist()<br/>        ).values.tolist()<br/><br/>    def fit_transform(self, x, y=None):<br/>        return self.transform(x)</pre>
        <p class="mce-root">The <kbd>fit</kbd> method here is impotent—it does not do anything since we are using a pre-trained model from spaCy. We can use the newly created transformer as follows:</p>
        <pre class="mce-root">vec = WordEmbeddingVectorizer()<br/>x_train_w2v = vec.transform(df_train['text'])</pre>
        <p>Instead of the Naive Bayes classifier, we can also use this transformer with other classifiers, such as <kbd>LogisticRegression</kbd> or <kbd>Multi-layer Perceptron</kbd>. </p>
        <div class="packt_tip">The <kbd>apply</kbd> function in pandas can be slow, especially when dealing with high volumes of data. I like to use a library called <kbd>tqdm</kbd>, which allows me to replace the <kbd>apply()</kbd> method with <kbd>progress_apply()</kbd>, which then displays a progress bar while running. All you have to do after importing the library is run <kbd>tqdm.pandas()</kbd>; this adds the <kbd>progress_apply()</kbd> method to the pandas Series and DataFrame objects. Fun fact: the word <kbd>tqdm</kbd>means <em>progress</em> in Arabic. </div>
        <h1 id="uuid-40eda192-3e3b-4e4c-9450-6cb7d0a19dc2">Summary</h1>
        <p>Personally, I find the field of natural language processing very exciting. The vast majority of our knowledge as humans is contained in books, documents, and web pages. Knowing how to automatically extract this information and organize it with the help of machine learning is essential to our scientific progress and endeavors in automation. This is why multiple scientific fields, such as information retrieval, statistics, and linguistics, borrow ideas from each other and try to solve the same problem from different angles. In this chapter, we also borrowed ideas from all these fields and learned how to represent textual data in formats suitable to machine learning algorithms. We also learned about the utilities that scikit-learn provides to aid in building and optimizing end-to-end solutions. We also encountered concepts such as transfer learning, and we were able to seamlessly incorporate spaCy's language models into scikit-learn. </p>
        <p>From the next chapter, we are going to deal with slightly advanced topics. In the next chapter, we will learn about artificial neural networks (multi-layer perceptron). This is a very hot topic nowadays, and understanding its main concepts helps anyone who wants to get deeper into deep learning. Since neural networks are commonly used in image processing, we will seize the opportunity to build on what we learned in <a href="https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=28&amp;action=edit">Chapter 5</a>, Image Processing with Nearest Neighbors and expand our image processing knowledge even further. </p>
      </article>
    </section>
  </body></html>
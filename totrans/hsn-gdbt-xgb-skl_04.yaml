- en: '*Chapter 3*: Bagging with Random Forests'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will gain proficiency in building **random forests**, a
    leading competitor to XGBoost. Like XGBoost, random forests are ensembles of decision
    trees. The difference is that random forests combine trees via **bagging**, while
    XGBoost combines trees via **boosting**. Random forests are a viable alternative
    to XGBoost with advantages and limitations that are highlighted in this chapter.
    Learning about random forests is important because they provide valuable insights
    into the structure of tree-based ensembles (XGBoost), and they allow a deeper
    understanding of boosting in comparison and contrast with their own method of
    bagging.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will build and evaluate **random forest classifiers** and
    **random forest regressors**, gain mastery of random forest hyperparameters, learn
    about bagging in the machine learning landscape, and explore a case study that
    highlights some random forest limitations that spurred the development of gradient
    boosting (XGBoost).
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning random forest hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushing random forest boundaries – case study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is available at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03)
  prefs: []
  type: TYPE_NORMAL
- en: Bagging ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn why ensemble methods are usually superior to
    individual machine learning models. Furthermore, you will learn about the technique
    of bagging. Both are essential features of random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, an ensemble method is a machine learning model that aggregates
    the predictions of individual models. Since ensemble methods combine the results
    of multiple models, they are less prone to error, and therefore tend to perform
    better.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine your goal is to determine whether a house will sell within the first
    month of being on the market. You run several machine learning algorithms and
    find that **logistic regression** gives 80% accuracy, **decision trees** 75% accuracy,
    and **k-nearest neighbors** 77% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: One option is to use logistic regression, the most accurate model, as your final
    model. A more compelling option is to combine the predictions of each individual
    model.
  prefs: []
  type: TYPE_NORMAL
- en: For classifiers, the standard option is to take the majority vote. If at least
    two of three models predict that a house will sell within the first month, the
    prediction is *YES*. Otherwise, it's *NO*.
  prefs: []
  type: TYPE_NORMAL
- en: Overall accuracy is usually higher with ensemble methods. For a prediction to
    be wrong, it's not enough for one model to get it wrong; the majority of classifiers
    must get it wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods are generally classified into two types. The first type combines
    different machine learning models, such as scikit-learn's `VotingClassifier`,
    as chosen by the user. The second type of ensemble method combines many versions
    of the same model, as is the case with XGBoost and random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests are among the most popular and widespread of all ensemble methods.
    The individual models of random forests are decision trees, the focus of the previous
    chapter, [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision
    Trees in Depth*. A random forest may consist of hundreds or thousands of decision
    trees whose predictions are combined for the final result.
  prefs: []
  type: TYPE_NORMAL
- en: Although random forests use majority rules for classifiers, and the average
    of all models for regressors, they also use a special method called bagging, short
    for bootstrap aggregation, to select individual trees.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bootstrapping** means sampling with replacement.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a bag of 20 shaded marbles. You are going to select 10 marbles,
    one at a time. Each time you select a marble, you put it back in the bag. This
    means that it's possible, though extremely unlikely, that you could pick the same
    marble 10 times.
  prefs: []
  type: TYPE_NORMAL
- en: It's more likely that you will pick some marbles more than once, and some not
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a visual of the marbles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia
    Commons, https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg)](img/B15551_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia
    Commons, [https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg](https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg))'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding diagram, bootstrap samples are achieved by
    sampling with replacement. If the marbles were not replaced, it would be impossible
    to obtain a sample with more black (*blue* in the original diagram) marbles than
    the original bag, as in the far-right box.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to random forests, bootstrapping works under the hood. The bootstrapping
    occurs when each decision tree is made. If the decision trees all consisted of
    the same samples, the trees would give similar predictions making the aggregate
    result similar to the individual tree. Instead, with random forests, the trees
    are built using bootstrapping, usually with the same number of samples as in the
    original dataset. Mathematical estimations are that two-thirds of the samples
    for each tree are unique, and one-third include duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: After the bootstrapping phase of the model-build, each decision tree makes its
    own individual predictions. The result is a forest of trees whose predictions
    are aggregated into one final prediction using majority rules for classifiers
    and the average for regressors.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, a random forest aggregates the predictions of bootstrapped decision
    trees. This general ensemble method is known in machine learning as bagging.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get a better sense of how random forests work, let's build one using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s use a random forest classifier to predict whether a user makes more
    or less than USD 50,000 using the census dataset we cleaned and scored in [*Chapter
    1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022), *Machine Learning Landscape*,
    and revisited in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*. We are going to use `cross_val_score` to ensure that
    our test results generalize well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps build and score a random forest classifier using the census
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas`, `numpy`, `RandomForestClassifier`, and `cross_val_score` before
    silencing warnings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset `census_cleaned.csv` and split it into `X` (a predictor column)
    and `y` (a target column):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With our imports and data ready to go, it's time to build a model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we initialize the random forest classifier. In practice, ensemble algorithms
    work just like any other machine learning algorithm. A model is initialized, fit
    to the training data, and scored against the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We initialize a random forest by setting the following hyperparameters in advance:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) `random_state=2` to ensure that your results are consistent with ours.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `n_jobs=-1` to speed up computations by taking advantage of parallel processing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `n_estimators=10`, a previous scikit-learn default sufficient to speed up
    computations and avoid ambiguity; new defaults have set `n_estimators=100`. `n_esmitators`
    will be explored in further detail in the next section:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ll use `cross_val_score`. `Cross_val_score` requires a model, predictor
    columns, and a target column as inputs. Recall that `cross_val_score` splits,
    fits, and scores the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The default random forest classifier provides a better score for the census
    dataset than the decision tree in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth* (81%), but not quite as good as XGBoost in [*Chapter
    1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022), *Machine Learning Landscape*
    (86%). Why does it perform better than individual decision trees?
  prefs: []
  type: TYPE_NORMAL
- en: The improved performance is likely on account of the bagging method described
    in the previous section. With 10 trees in this forest (since `n_estimators=10`),
    each prediction is based on 10 decision trees instead of 1\. The trees are bootstrapped,
    which increases diversity, and aggregated, which reduces variance.
  prefs: []
  type: TYPE_NORMAL
- en: By default, random forest classifiers select from the square root of the total
    number of features when looking for a split. So, if there are 100 features (columns),
    each decision tree will only consider 10 features when choosing a split. Thus
    two trees with duplicate samples may give very different predictions due to the
    different splits. This is another way that random forests reduce variance.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to classification, random forests also work with regression.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest regressors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a random forest regressor, the samples are bootstrapped, as with the random
    forest Classifier, but the max number of features is the total number of features
    instead of the square root. This change is due to experimental results (see [https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the final prediction is made by taking the average of the predictions
    of all the trees, instead of a majority rules vote.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see a random forest regressor in action, complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Upload the bike rental dataset from [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*, and pull up the first five rows for a refresher:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code should result in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Bike rentals dataset – cleaned](img/B15551_03_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.2 – Bike rentals dataset – cleaned
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into `X` and `y`, the predictive and target columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the regressor, then initialize it using the same default hyperparameters,
    `n_estimators=10`, `random_state=2`, and `n_jobs=-1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to use `cross_val_score`. Place the regressor, `rf`, along with
    predictor and target columns inside `cross_val_score`. Note that the negative
    mean squared error (`''neg_mean_squared_error''`) should be defined as the scoring
    parameter. Select 10 folds (`cv=10`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find and display the **root mean squared error** (**RMSE**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The random forest performs respectably, though not as well as other models that
    we have seen. We will further examine the bike rentals dataset in the case study
    later in this chapter to see why.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's examine random forest hyperparameters in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The range of random forest hyperparameters is large, unless one already has
    a working knowledge of decision tree hyperparameters, as covered in [*Chapter
    2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision Trees in Depth*.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go over additional random forest hyperparameters before
    grouping the hyperparameters that you have already seen. Many of these hyperparameters
    will be used by XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: oob_score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first hyperparameter, and perhaps the most intriguing, is `oob_score`.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests select decision trees via bagging, meaning that samples are selected
    with replacement. After all of the samples have been chosen, some samples should
    remain that have not been chosen.
  prefs: []
  type: TYPE_NORMAL
- en: It's possible to hold back these samples as the test set. After the model is
    fit on one tree, the model can immediately be scored against this test set. When
    the hyperparameter is set to `oob_score=True`, this is exactly what happens.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, `oob_score` provides a shortcut to get a test score. `oob_score`
    may be printed out immediately after the model has been fit.
  prefs: []
  type: TYPE_NORMAL
- en: Let's use `oob_score` on the census dataset to see how it works in practice.
    Since we are using `oob_score` to test the model, it's not necessary to split
    the data into a training set and test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The random forest may be initialized as usual with `oob_score=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, `rf` may be fit on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `oob_score=True`, the score is available after the model has been fit.
    It may be accessed using the model attribute `.oob_score_` as follows (note the
    underscore after `score`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As described previously, `oob_score` is created by scoring samples on individual
    trees excluded during the training phase. When the number of trees in the forest
    is small, as is the case with 10 estimators, there may not be enough test samples
    to maximize accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: More trees mean more samples, and often greater accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: n_estimators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random forests are powerful when there are many trees in the forest. How many
    is enough? Recently, scikit-learn defaults changed from 10 to 100\. While 100
    trees may be enough to cut down on variance and obtain good scores, for larger
    datasets, 500 or more trees may be required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with `n_estimators=50` to see how `oob_score` changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: A definite improvement. What about 100 trees?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The gain is smaller. As `n_estimators` continues to rise, scores will eventually
    level off.
  prefs: []
  type: TYPE_NORMAL
- en: warm_start
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `warm_start` hyperparameter is great for determining the number of trees
    in the forest (`n_estimators`). When `warm_start=True`, adding more trees does
    not require starting over from scratch. If you change `n_estimators` from 100
    to 200, it may take twice as long to build the forest with 200 trees. When `warm_start=True`,
    the random forest with 200 trees does not start from scratch, but rather starts
    where the previous model stopped.
  prefs: []
  type: TYPE_NORMAL
- en: '`warm_start` may be used to plot various scores with a range of `n_estimators`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, the following code takes increments of 50 trees, starting with
    50 and ending at 500, to display a range of scores. This code may take time to
    run as it is building 10 random forests by adding 50 new trees each round! The
    code is broken down in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import matplotlib and seaborn, then set the seaborn dark grid with `sns.set()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize an empty list of scores and initialize a random forest classifier
    with 50 estimators, making sure that `warm_start=True` and `oob_score=True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit `rf` to the dataset, then append `oob_score` to the `oob_scores` list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare a list of estimators that contains the number of trees starting with
    50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write a for loop that adds 50 trees each round. For each round, add 50 to `est`,
    append `est` to the `estimators` list, change `n_estimators` with `rf.set_params(n_estimators=est)`,
    fit the random forest on the data, then append the new `oob_score_`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For a nice display, show a larger graph, then plot the estimators and `oob_scores`.
    Add the appropriate labels, then save and show the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Random forest Warm Start – oob_score per number of trees](img/B15551_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Random forest Warm Start – oob_score per number of trees
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the number of trees tends to peak at around 300\. It's more
    costly and time-consuming to use more trees than 300, and the gains are minimal
    at best.
  prefs: []
  type: TYPE_NORMAL
- en: bootstrap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although random forests are traditionally bootstrapped, the `bootstrap` hyperparameter
    may be set to `False`. If `bootstrap=False`, `oob_score` cannot be included since
    `oob_score` is only possible when samples have been left out.
  prefs: []
  type: TYPE_NORMAL
- en: We will not pursue this option, although it makes sense if underfitting occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Verbose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `verbose` hyperparameter may be changed to a higher number to display more
    information when building a model. You may try it on your own for experimentation.
    When building large models, `verbose=1` may provide helpful information along
    the way.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The remaining hyperparameters all come from decision trees. It turns out that
    decision tree hyperparameters are not as significant within random forests since
    random forests cut down on variance by design.
  prefs: []
  type: TYPE_NORMAL
- en: Here are decision tree hyperparameters grouped according to category for you
    to review.
  prefs: []
  type: TYPE_NORMAL
- en: Depth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The hyperparameters that fall under this category are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`: Always good to tune. Determines the number of times splits occur.
    Known as the length of the tree. A great way to reduce variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The hyperparameters that fall under this category are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features`: Limits the number of features to choose from when making splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: Increases the number of samples required for new splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_impurity_decrease`: Limits splits to decrease impurity greater than the
    set threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaves
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The hyperparameters that fall under this category are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf`: Increases the minimum number of samples required for a
    node to be a leaf.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_weight_fraction_leaf`: The fraction of the total weights required to be
    a leaf.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more information on the preceding hyperparameters, check out the official
    random forest regressor documentation: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Pushing random forest boundaries – case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you work for a bike rental company and your goal is to predict the number
    of bike rentals per day depending upon the weather, the time of day, the time
    of year, and the growth of the company.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, you implemented a random forest regressor with cross-validation
    to obtain an RMSE of 945 bikes. Your goal is to modify the random forest to obtain
    the lowest error score possible.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, you downloaded the dataset `df_bikes` and split it
    into `X_bikes` and `y_bikes`. Now that you are doing some serious testing, you
    decide to split `X_bikes` and `y_bikes` into training sets and test sets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: n_estimators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by choosing a reasonable value for `n_estimators`. Recall that `n_estimators`
    can be increased to improve accuracy at the cost of computational resources and
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a graph of RMSE using the `warm_start` method for a variety
    of `n_estimators` using the same general code provided previously under the *warm_start*
    heading:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees](img/B15551_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees
  prefs: []
  type: TYPE_NORMAL
- en: This graph is very interesting. The random forest provides the best score with
    50 estimators. After 100 estimators, the error gradually starts to go up, a concept
    that will be revisited later.
  prefs: []
  type: TYPE_NORMAL
- en: For now, it's sensible to use `n_estimators=50` as the starting point.
  prefs: []
  type: TYPE_NORMAL
- en: cross_val_score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With errors ranging from 620 to 690 bike rentals according to the preceding
    graph, it's time to see how the dataset performs with cross-validation using `cross_val_score`.
    Recall that in cross-validation the purpose is to divide the samples into *k*
    different folds, and to use all samples as test sets over the different folds.
    Since all samples are used to test the model, `oob_score` will not work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code contains the same steps that you used earlier in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Score the model, using `cross_val_score` with the model, predictor columns,
    target column, scoring, and the number of folds as parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the RMSE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the cross-validation scores and the mean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This score is better than earlier in the chapter. Notice that the error in the
    last fold is much higher according to the last entry in the RMSE array. This could
    be due to errors within the data or outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It''s time to create a grid of hyperparameters to fine-tune our model using
    `RandomizedSearchCV`. Here is a function that uses `RandomizedSearchCV` to display
    the RMSEs along with the mean score and best hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a starter''s grid of hyperparameters placed inside the new `randomized_search_reg`
    function to obtain the first results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a major improvement. Let''s see if we can do better by narrowing the
    range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The score has improved yet again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s increase the number of runs, and give more options for `max_depth`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The score keeps getting better. At this point, it may be worth narrowing the
    ranges further, based upon the previous results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The test score has gone back up. Increasing `n_estimators` at this point could
    be a good idea. The more trees in the forest, the more potential there may be
    to realize small gains.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also increase the number of runs to `20` to look for better hyperparameter
    combinations. Keep in mind that results are based on a randomized search, not
    a full grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This matches the best score achieved thus far. We could keep tinkering. It's
    possible with enough experimentation that the test score may drop to under 600
    bikes. But we also seem to be peaking around the low 600 mark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s place our best model in `cross_val_score` to see how the result
    compares with the original:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The RMSE goes back up to `817`. The score is much better than `903`, but it's
    considerably worse than `619`. What's going on here?
  prefs: []
  type: TYPE_NORMAL
- en: 'There may be an issue with the last split in `cross_val_score` since its score
    is twice as bad as the others. Let''s see if shuffling the data does the trick.
    Scikit-learn has a shuffle module that may be imported from `sklearn.utils` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can shuffle the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now split the data into a new `X` and `y` and run `RandomForestRegressor` with
    `cross_val_score` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In the shuffled data, there is no issue with the last split, and the score is
    much higher, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest drawbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the end of the day, the random forest is limited by its individual trees.
    If all trees make the same mistake, the random forest makes this mistake. There
    are scenarios, as is revealed in this case study before the data was shuffled,
    where random forests are unable to significantly improve upon errors due to challenges
    within the data that individual trees are unable to address.
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble method capable of improving upon initial shortcomings, an ensemble
    method that will learn from the mistakes of trees in future rounds, could be advantageous.
    Boosting was designed to learn from the mistakes of trees in early rounds. Boosting,
    in particular gradient boosting – the focus of the next chapter – addresses this
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In closure, the following graph displays the results of the tuned random forest
    regressor and the default XGBoost regressor when increasing the number of trees
    in the bike rentals dataset if the data is not shuffled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15551_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Comparing the XGBoost default model with a tuned random forest
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, XGBoost does a much better job of learning as the number of
    trees increases. And the XGBoost model has not even been tuned!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the importance of ensemble methods. In particular,
    you learned about bagging, the combination of bootstrapping, sampling with replacement,
    and aggregation, combining many models into one. You built random forest classifiers
    and regressors. You adjusted `n_estimators` with the `warm_start` hyperparameter
    and used `oob_score_` to find errors. Then you modified random forest hyperparameters
    to fine-tune models. Finally, you examined a case study where shuffling the data
    gave excellent results but adding more trees to the random forest did not result
    in any gains with the unshuffled data, as contrasted with XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn the fundamentals of boosting, an ensemble
    method that learns from its mistakes to improve upon accuracy as more trees are
    added. You will implement gradient boosting to make predictions, thereby setting
    the stage for Extreme gradient boosting, better known as XGBoost.
  prefs: []
  type: TYPE_NORMAL

- en: '*Chapter 3*: Bagging with Random Forests'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will gain proficiency in building **random forests**, a
    leading competitor to XGBoost. Like XGBoost, random forests are ensembles of decision
    trees. The difference is that random forests combine trees via **bagging**, while
    XGBoost combines trees via **boosting**. Random forests are a viable alternative
    to XGBoost with advantages and limitations that are highlighted in this chapter.
    Learning about random forests is important because they provide valuable insights
    into the structure of tree-based ensembles (XGBoost), and they allow a deeper
    understanding of boosting in comparison and contrast with their own method of
    bagging.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will build and evaluate **random forest classifiers** and
    **random forest regressors**, gain mastery of random forest hyperparameters, learn
    about bagging in the machine learning landscape, and explore a case study that
    highlights some random forest limitations that spurred the development of gradient
    boosting (XGBoost).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Bagging ensembles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring random forests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning random forest hyperparameters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushing random forest boundaries – case study
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is available at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Bagging ensembles
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn why ensemble methods are usually superior to
    individual machine learning models. Furthermore, you will learn about the technique
    of bagging. Both are essential features of random forests.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, an ensemble method is a machine learning model that aggregates
    the predictions of individual models. Since ensemble methods combine the results
    of multiple models, they are less prone to error, and therefore tend to perform
    better.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Imagine your goal is to determine whether a house will sell within the first
    month of being on the market. You run several machine learning algorithms and
    find that **logistic regression** gives 80% accuracy, **decision trees** 75% accuracy,
    and **k-nearest neighbors** 77% accuracy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: One option is to use logistic regression, the most accurate model, as your final
    model. A more compelling option is to combine the predictions of each individual
    model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: For classifiers, the standard option is to take the majority vote. If at least
    two of three models predict that a house will sell within the first month, the
    prediction is *YES*. Otherwise, it's *NO*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Overall accuracy is usually higher with ensemble methods. For a prediction to
    be wrong, it's not enough for one model to get it wrong; the majority of classifiers
    must get it wrong.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods are generally classified into two types. The first type combines
    different machine learning models, such as scikit-learn's `VotingClassifier`,
    as chosen by the user. The second type of ensemble method combines many versions
    of the same model, as is the case with XGBoost and random forests.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Random forests are among the most popular and widespread of all ensemble methods.
    The individual models of random forests are decision trees, the focus of the previous
    chapter, [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision
    Trees in Depth*. A random forest may consist of hundreds or thousands of decision
    trees whose predictions are combined for the final result.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Although random forests use majority rules for classifiers, and the average
    of all models for regressors, they also use a special method called bagging, short
    for bootstrap aggregation, to select individual trees.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap aggregation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bootstrapping** means sampling with replacement.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a bag of 20 shaded marbles. You are going to select 10 marbles,
    one at a time. Each time you select a marble, you put it back in the bag. This
    means that it's possible, though extremely unlikely, that you could pick the same
    marble 10 times.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: It's more likely that you will pick some marbles more than once, and some not
    at all.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a visual of the marbles:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia
    Commons, https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg)](img/B15551_03_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia
    Commons, [https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg](https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg))'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding diagram, bootstrap samples are achieved by
    sampling with replacement. If the marbles were not replaced, it would be impossible
    to obtain a sample with more black (*blue* in the original diagram) marbles than
    the original bag, as in the far-right box.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to random forests, bootstrapping works under the hood. The bootstrapping
    occurs when each decision tree is made. If the decision trees all consisted of
    the same samples, the trees would give similar predictions making the aggregate
    result similar to the individual tree. Instead, with random forests, the trees
    are built using bootstrapping, usually with the same number of samples as in the
    original dataset. Mathematical estimations are that two-thirds of the samples
    for each tree are unique, and one-third include duplicates.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: After the bootstrapping phase of the model-build, each decision tree makes its
    own individual predictions. The result is a forest of trees whose predictions
    are aggregated into one final prediction using majority rules for classifiers
    and the average for regressors.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: In summary, a random forest aggregates the predictions of bootstrapped decision
    trees. This general ensemble method is known in machine learning as bagging.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，随机森林聚合了自助法生成的决策树的预测。这种通用的集成方法在机器学习中被称为自助法（bagging）。
- en: Exploring random forests
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索随机森林
- en: To get a better sense of how random forests work, let's build one using scikit-learn.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解随机森林的工作原理，让我们使用 scikit-learn 构建一个随机森林。
- en: Random forest classifiers
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: 'Let''s use a random forest classifier to predict whether a user makes more
    or less than USD 50,000 using the census dataset we cleaned and scored in [*Chapter
    1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022), *Machine Learning Landscape*,
    and revisited in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*. We are going to use `cross_val_score` to ensure that
    our test results generalize well:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个随机森林分类器，使用我们在[**第1章**](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)，*机器学习概览*中清理和评分的普查数据集，预测用户收入是否超过
    50,000 美元，并在[**第2章**](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)，*决策树深入剖析*中重新检查。我们将使用`cross_val_score`确保我们的测试结果具有良好的泛化能力：
- en: 'The following steps build and score a random forest classifier using the census
    dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤使用普查数据集构建并评分一个随机森林分类器：
- en: 'Import `pandas`, `numpy`, `RandomForestClassifier`, and `cross_val_score` before
    silencing warnings:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`、`RandomForestClassifier`和`cross_val_score`，然后关闭警告：
- en: '[PRE0]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the dataset `census_cleaned.csv` and split it into `X` (a predictor column)
    and `y` (a target column):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集`census_cleaned.csv`并将其拆分为`X`（预测变量）和`y`（目标变量）：
- en: '[PRE1]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With our imports and data ready to go, it's time to build a model.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在准备好导入和数据后，现在是时候构建模型了。
- en: Next, we initialize the random forest classifier. In practice, ensemble algorithms
    work just like any other machine learning algorithm. A model is initialized, fit
    to the training data, and scored against the test data.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化随机森林分类器。在实践中，集成算法与其他机器学习算法一样工作。一个模型被初始化、拟合训练数据，并在测试数据上评分。
- en: 'We initialize a random forest by setting the following hyperparameters in advance:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过提前设置以下超参数来初始化随机森林：
- en: a) `random_state=2` to ensure that your results are consistent with ours.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `random_state=2`确保你的结果与我们的结果一致。
- en: b) `n_jobs=-1` to speed up computations by taking advantage of parallel processing.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `n_jobs=-1`通过利用并行处理加速计算。
- en: 'c) `n_estimators=10`, a previous scikit-learn default sufficient to speed up
    computations and avoid ambiguity; new defaults have set `n_estimators=100`. `n_esmitators`
    will be explored in further detail in the next section:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `n_estimators=10`，这是 scikit-learn 的默认值，足以加速计算并避免歧义；新的默认值已设置为`n_estimators=100`。`n_estimators`将在下一节中详细探讨：
- en: '[PRE2]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we''ll use `cross_val_score`. `Cross_val_score` requires a model, predictor
    columns, and a target column as inputs. Recall that `cross_val_score` splits,
    fits, and scores the data:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用`cross_val_score`。`cross_val_score`需要一个模型、预测列和目标列作为输入。回顾一下，`cross_val_score`会对数据进行拆分、拟合和评分：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Display the results:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示结果：
- en: '[PRE4]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The default random forest classifier provides a better score for the census
    dataset than the decision tree in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth* (81%), but not quite as good as XGBoost in [*Chapter
    1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022), *Machine Learning Landscape*
    (86%). Why does it perform better than individual decision trees?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的随机森林分类器在[**第2章**](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)，*决策树深入剖析*（81%）的数据集上，比决策树表现更好，但还不如[**第1章**](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)，*机器学习概览*（86%）中的
    XGBoost。为什么它比单一的决策树表现更好？
- en: The improved performance is likely on account of the bagging method described
    in the previous section. With 10 trees in this forest (since `n_estimators=10`),
    each prediction is based on 10 decision trees instead of 1\. The trees are bootstrapped,
    which increases diversity, and aggregated, which reduces variance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 性能提升可能与上一节中描述的自助法（bagging）有关。在这个森林中有 10 棵树（因为`n_estimators=10`），每个预测是基于 10 棵决策树，而不是
    1 棵。树是通过自助法生成的，这增加了多样性，并通过聚合减少了方差。
- en: By default, random forest classifiers select from the square root of the total
    number of features when looking for a split. So, if there are 100 features (columns),
    each decision tree will only consider 10 features when choosing a split. Thus
    two trees with duplicate samples may give very different predictions due to the
    different splits. This is another way that random forests reduce variance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，随机森林分类器在寻找分裂时会从特征总数的平方根中选择特征。因此，如果有100个特征（列），每棵决策树在选择分裂时只会考虑10个特征。因此，两个样本重复的树可能由于分裂的不同而给出完全不同的预测。这是随机森林减少方差的另一种方式。
- en: In addition to classification, random forests also work with regression.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分类，随机森林还可以用于回归。
- en: Random forest regressors
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林回归器
- en: In a random forest regressor, the samples are bootstrapped, as with the random
    forest Classifier, but the max number of features is the total number of features
    instead of the square root. This change is due to experimental results (see [https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林回归器中，样本是通过自助法（bootstrap）抽取的，和随机森林分类器一样，但最大特征数是特征总数，而不是平方根。这个变化是基于实验结果（参见
    [https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf)）。
- en: Furthermore, the final prediction is made by taking the average of the predictions
    of all the trees, instead of a majority rules vote.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最终的预测是通过对所有决策树的预测结果求平均得出的，而不是通过多数规则投票。
- en: 'To see a random forest regressor in action, complete the following steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看随机森林回归器的实际应用，请完成以下步骤：
- en: 'Upload the bike rental dataset from [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*, and pull up the first five rows for a refresher:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深度剖析》上传自行车租赁数据集，并提取前五行以供回顾：
- en: '[PRE5]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code should result in the following output:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码应生成以下输出：
- en: '![Figure 3.2 – Bike rentals dataset – cleaned](img/B15551_03_02.jpg)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.2 – 自行车租赁数据集 – 已清理](img/B15551_03_02.jpg)'
- en: Figure 3.2 – Bike rentals dataset – cleaned
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.2 – 自行车租赁数据集 – 已清理
- en: 'Split the data into `X` and `y`, the predictive and target columns:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据划分为`X`和`y`，即预测列和目标列：
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Import the regressor, then initialize it using the same default hyperparameters,
    `n_estimators=10`, `random_state=2`, and `n_jobs=-1`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入回归器，然后使用相同的默认超参数进行初始化，`n_estimators=10`，`random_state=2`，`n_jobs=-1`：
- en: '[PRE7]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we need to use `cross_val_score`. Place the regressor, `rf`, along with
    predictor and target columns inside `cross_val_score`. Note that the negative
    mean squared error (`''neg_mean_squared_error''`) should be defined as the scoring
    parameter. Select 10 folds (`cv=10`):'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要使用`cross_val_score`。将回归器`rf`与预测器和目标列一起放入`cross_val_score`。请注意，负均方误差（`'neg_mean_squared_error'`）应定义为评分参数。选择10折交叉验证（`cv=10`）：
- en: '[PRE8]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Find and display the **root mean squared error** (**RMSE**):'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找并显示**均方根误差**（**RMSE**）：
- en: '[PRE9]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The random forest performs respectably, though not as well as other models that
    we have seen. We will further examine the bike rentals dataset in the case study
    later in this chapter to see why.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的表现令人满意，尽管不如我们之前看到的其他模型。我们将在本章后面的案例研究中进一步分析自行车租赁数据集，以了解原因。
- en: Next, let's examine random forest hyperparameters in detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细查看随机森林的超参数。
- en: Random forest hyperparameters
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林超参数
- en: The range of random forest hyperparameters is large, unless one already has
    a working knowledge of decision tree hyperparameters, as covered in [*Chapter
    2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision Trees in Depth*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林超参数的范围很大，除非已经具备决策树超参数的工作知识，如在[*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深度剖析》中所讨论的那样。
- en: In this section, we will go over additional random forest hyperparameters before
    grouping the hyperparameters that you have already seen. Many of these hyperparameters
    will be used by XGBoost.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在介绍您已见过的超参数分组之前，讨论一些额外的随机森林超参数。许多超参数将被 XGBoost 使用。
- en: oob_score
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: oob_score
- en: Our first hyperparameter, and perhaps the most intriguing, is `oob_score`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个超参数，可能也是最引人注目的，是`oob_score`。
- en: Random forests select decision trees via bagging, meaning that samples are selected
    with replacement. After all of the samples have been chosen, some samples should
    remain that have not been chosen.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通过袋装（bagging）选择决策树，这意味着样本是带替换地选取的。所有样本选择完后，应该会有一些未被选择的样本。
- en: It's possible to hold back these samples as the test set. After the model is
    fit on one tree, the model can immediately be scored against this test set. When
    the hyperparameter is set to `oob_score=True`, this is exactly what happens.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将这些样本作为测试集保留。在模型拟合完一棵树后，模型可以立即用这个测试集进行评分。当超参数设置为`oob_score=True`时，正是发生了这种情况。
- en: In other words, `oob_score` provides a shortcut to get a test score. `oob_score`
    may be printed out immediately after the model has been fit.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，`oob_score`提供了一种获取测试分数的快捷方式。在模型拟合后，可以立即打印出`oob_score`。
- en: Let's use `oob_score` on the census dataset to see how it works in practice.
    Since we are using `oob_score` to test the model, it's not necessary to split
    the data into a training set and test set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在普查数据集上使用`oob_score`，看看它在实践中的表现。由于我们使用`oob_score`来测试模型，因此不需要将数据拆分为训练集和测试集。
- en: 'The random forest may be initialized as usual with `oob_score=True`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以像往常一样初始化，设置`oob_score=True`：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, `rf` may be fit on the data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，可以在数据上拟合`rf`：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since `oob_score=True`, the score is available after the model has been fit.
    It may be accessed using the model attribute `.oob_score_` as follows (note the
    underscore after `score`):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`oob_score=True`，在模型拟合后可以获得分数。可以通过模型的属性`.oob_score_`来访问分数，如下所示（注意`score`后有下划线）：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The score is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As described previously, `oob_score` is created by scoring samples on individual
    trees excluded during the training phase. When the number of trees in the forest
    is small, as is the case with 10 estimators, there may not be enough test samples
    to maximize accuracy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`oob_score`是通过对训练阶段被排除的个别树上的样本进行评分生成的。当森林中的树木数量较少时（例如使用10个估计器），可能没有足够的测试样本来最大化准确度。
- en: More trees mean more samples, and often greater accuracy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的树意味着更多的样本，通常也意味着更高的准确性。
- en: n_estimators
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n_estimators
- en: Random forests are powerful when there are many trees in the forest. How many
    is enough? Recently, scikit-learn defaults changed from 10 to 100\. While 100
    trees may be enough to cut down on variance and obtain good scores, for larger
    datasets, 500 or more trees may be required.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当森林中有很多树时，随机森林的效果非常强大。那么多少棵树才足够呢？最近，scikit-learn的默认设置已从10改为100。虽然100棵树可能足够减少方差并获得良好的分数，但对于较大的数据集，可能需要500棵或更多的树。
- en: 'Let''s start with `n_estimators=50` to see how `oob_score` changes:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`n_estimators=50`开始，看看`oob_score`是如何变化的：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The score is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A definite improvement. What about 100 trees?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有了明显的提升。那么100棵树呢？
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The score is as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The gain is smaller. As `n_estimators` continues to rise, scores will eventually
    level off.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 增益较小。随着`n_estimators`的不断增加，分数最终会趋于平稳。
- en: warm_start
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: warm_start
- en: The `warm_start` hyperparameter is great for determining the number of trees
    in the forest (`n_estimators`). When `warm_start=True`, adding more trees does
    not require starting over from scratch. If you change `n_estimators` from 100
    to 200, it may take twice as long to build the forest with 200 trees. When `warm_start=True`,
    the random forest with 200 trees does not start from scratch, but rather starts
    where the previous model stopped.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`warm_start`超参数非常适合确定森林中树的数量（`n_estimators`）。当`warm_start=True`时，添加更多树木不需要从头开始。如果将`n_estimators`从100改为200，构建200棵树的森林可能需要两倍的时间。使用`warm_start=True`时，200棵树的随机森林不会从头开始，而是从先前模型停止的位置继续。'
- en: '`warm_start` may be used to plot various scores with a range of `n_estimators`.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`warm_start`可以用来绘制不同`n_estimators`值下的各种分数。'
- en: 'As an example, the following code takes increments of 50 trees, starting with
    50 and ending at 500, to display a range of scores. This code may take time to
    run as it is building 10 random forests by adding 50 new trees each round! The
    code is broken down in the following steps:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，以下代码每次增加50棵树，从50开始，到500结束，显示一系列分数。由于每轮都在通过添加50棵新树来构建10个随机森林，这段代码可能需要一些时间才能完成运行！代码按以下步骤分解：
- en: 'Import matplotlib and seaborn, then set the seaborn dark grid with `sns.set()`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入matplotlib和seaborn，然后通过`sns.set()`设置seaborn的暗色网格：
- en: '[PRE19]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize an empty list of scores and initialize a random forest classifier
    with 50 estimators, making sure that `warm_start=True` and `oob_score=True`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空的分数列表，并用50个估计器初始化随机森林分类器，确保`warm_start=True`和`oob_score=True`：
- en: '[PRE20]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Fit `rf` to the dataset, then append `oob_score` to the `oob_scores` list:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`rf`拟合到数据集上，然后将`oob_score`添加到`oob_scores`列表中：
- en: '[PRE21]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Prepare a list of estimators that contains the number of trees starting with
    50:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个估计器列表，其中包含从50开始的树的数量：
- en: '[PRE22]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Write a for loop that adds 50 trees each round. For each round, add 50 to `est`,
    append `est` to the `estimators` list, change `n_estimators` with `rf.set_params(n_estimators=est)`,
    fit the random forest on the data, then append the new `oob_score_`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For a nice display, show a larger graph, then plot the estimators and `oob_scores`.
    Add the appropriate labels, then save and show the graph:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This generates the following graph:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Random forest Warm Start – oob_score per number of trees](img/B15551_03_03.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Random forest Warm Start – oob_score per number of trees
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the number of trees tends to peak at around 300\. It's more
    costly and time-consuming to use more trees than 300, and the gains are minimal
    at best.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: bootstrap
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although random forests are traditionally bootstrapped, the `bootstrap` hyperparameter
    may be set to `False`. If `bootstrap=False`, `oob_score` cannot be included since
    `oob_score` is only possible when samples have been left out.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: We will not pursue this option, although it makes sense if underfitting occurs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Verbose
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `verbose` hyperparameter may be changed to a higher number to display more
    information when building a model. You may try it on your own for experimentation.
    When building large models, `verbose=1` may provide helpful information along
    the way.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree hyperparameters
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The remaining hyperparameters all come from decision trees. It turns out that
    decision tree hyperparameters are not as significant within random forests since
    random forests cut down on variance by design.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Here are decision tree hyperparameters grouped according to category for you
    to review.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Depth
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The hyperparameters that fall under this category are:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`: Always good to tune. Determines the number of times splits occur.
    Known as the length of the tree. A great way to reduce variance.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splits
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The hyperparameters that fall under this category are:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features`: Limits the number of features to choose from when making splits.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: Increases the number of samples required for new splits.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_impurity_decrease`: Limits splits to decrease impurity greater than the
    set threshold.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaves
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The hyperparameters that fall under this category are:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf`: Increases the minimum number of samples required for a
    node to be a leaf.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_weight_fraction_leaf`: The fraction of the total weights required to be
    a leaf.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more information on the preceding hyperparameters, check out the official
    random forest regressor documentation: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Pushing random forest boundaries – case study
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you work for a bike rental company and your goal is to predict the number
    of bike rentals per day depending upon the weather, the time of day, the time
    of year, and the growth of the company.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, you implemented a random forest regressor with cross-validation
    to obtain an RMSE of 945 bikes. Your goal is to modify the random forest to obtain
    the lowest error score possible.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，您实现了一个带有交叉验证的随机森林回归器，得到了945辆自行车的RMSE。您的目标是修改随机森林以获得尽可能低的误差得分。
- en: Preparing the dataset
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'Earlier in this chapter, you downloaded the dataset `df_bikes` and split it
    into `X_bikes` and `y_bikes`. Now that you are doing some serious testing, you
    decide to split `X_bikes` and `y_bikes` into training sets and test sets as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，您下载了数据集`df_bikes`并将其分割为`X_bikes`和`y_bikes`。现在，您进行一些严肃的测试，决定将`X_bikes`和`y_bikes`拆分为训练集和测试集，如下所示：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: n_estimators
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n_estimators
- en: Start by choosing a reasonable value for `n_estimators`. Recall that `n_estimators`
    can be increased to improve accuracy at the cost of computational resources and
    time.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先选择一个合理的`n_estimators`值。回想一下，`n_estimators`可以增加以提高准确性，但会以计算资源和时间为代价。
- en: 'The following is a graph of RMSE using the `warm_start` method for a variety
    of `n_estimators` using the same general code provided previously under the *warm_start*
    heading:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`warm_start`方法对多种`n_estimators`进行RMSE图形展示，所使用的代码与之前在*warm_start*部分提供的相同：
- en: '![Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees](img/B15551_03_04.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – 随机森林自行车租赁 – 每棵树的RMSE](img/B15551_03_04.jpg)'
- en: Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 随机森林自行车租赁 – 每棵树的RMSE
- en: This graph is very interesting. The random forest provides the best score with
    50 estimators. After 100 estimators, the error gradually starts to go up, a concept
    that will be revisited later.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表非常有趣。随机森林在50个估计器下提供了最佳得分。在100个估计器后，误差开始逐渐增加，这是一个稍后会重新讨论的概念。
- en: For now, it's sensible to use `n_estimators=50` as the starting point.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`n_estimators=50`作为起点是合理的选择。
- en: cross_val_score
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: cross_val_score
- en: With errors ranging from 620 to 690 bike rentals according to the preceding
    graph, it's time to see how the dataset performs with cross-validation using `cross_val_score`.
    Recall that in cross-validation the purpose is to divide the samples into *k*
    different folds, and to use all samples as test sets over the different folds.
    Since all samples are used to test the model, `oob_score` will not work.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的图表，误差范围从620到690辆自行车租赁，现在是时候看看数据集在使用`cross_val_score`进行交叉验证时的表现了。回想一下，在交叉验证中，目的是将样本划分为*k*个不同的折，并在不同的折中使用所有样本作为测试集。由于所有样本都用于测试模型，`oob_score`将无法使用。
- en: 'The following code contains the same steps that you used earlier in the chapter:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码包含了您在本章早期使用的相同步骤：
- en: Initialize the model.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型。
- en: Score the model, using `cross_val_score` with the model, predictor columns,
    target column, scoring, and the number of folds as parameters.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型进行评分，使用`cross_val_score`与模型、预测列、目标列、评分标准和折数作为参数。
- en: Compute the RMSE.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算RMSE。
- en: Display the cross-validation scores and the mean.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示交叉验证得分和平均值。
- en: 'Here is the code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This score is better than earlier in the chapter. Notice that the error in the
    last fold is much higher according to the last entry in the RMSE array. This could
    be due to errors within the data or outliers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个得分比本章之前的得分更好。注意，最后一个折中的误差显著更高，根据RMSE数组中的最后一项。这可能是由于数据中的错误或异常值所致。
- en: Fine-tuning hyperparameters
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调超参数
- en: 'It''s time to create a grid of hyperparameters to fine-tune our model using
    `RandomizedSearchCV`. Here is a function that uses `RandomizedSearchCV` to display
    the RMSEs along with the mean score and best hyperparameters:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候创建一个超参数网格，使用`RandomizedSearchCV`来微调我们的模型了。以下是一个使用`RandomizedSearchCV`的函数，用于显示RMSE和平均得分以及最佳超参数：
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is a starter''s grid of hyperparameters placed inside the new `randomized_search_reg`
    function to obtain the first results:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个初学者的超参数网格，放入新的`randomized_search_reg`函数中以获得初步结果：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This is a major improvement. Let''s see if we can do better by narrowing the
    range:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的改进。让我们看看通过缩小范围是否能够得到更好的结果：
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The score has improved yet again.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 得分再次提高了。
- en: 'Now let''s increase the number of runs, and give more options for `max_depth`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们增加运行次数，并为`max_depth`提供更多选项：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The score keeps getting better. At this point, it may be worth narrowing the
    ranges further, based upon the previous results:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The test score has gone back up. Increasing `n_estimators` at this point could
    be a good idea. The more trees in the forest, the more potential there may be
    to realize small gains.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also increase the number of runs to `20` to look for better hyperparameter
    combinations. Keep in mind that results are based on a randomized search, not
    a full grid search:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This matches the best score achieved thus far. We could keep tinkering. It's
    possible with enough experimentation that the test score may drop to under 600
    bikes. But we also seem to be peaking around the low 600 mark.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s place our best model in `cross_val_score` to see how the result
    compares with the original:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The RMSE goes back up to `817`. The score is much better than `903`, but it's
    considerably worse than `619`. What's going on here?
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'There may be an issue with the last split in `cross_val_score` since its score
    is twice as bad as the others. Let''s see if shuffling the data does the trick.
    Scikit-learn has a shuffle module that may be imported from `sklearn.utils` as
    follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we can shuffle the data as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now split the data into a new `X` and `y` and run `RandomForestRegressor` with
    `cross_val_score` again:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In the shuffled data, there is no issue with the last split, and the score is
    much higher, as expected.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Random forest drawbacks
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the end of the day, the random forest is limited by its individual trees.
    If all trees make the same mistake, the random forest makes this mistake. There
    are scenarios, as is revealed in this case study before the data was shuffled,
    where random forests are unable to significantly improve upon errors due to challenges
    within the data that individual trees are unable to address.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble method capable of improving upon initial shortcomings, an ensemble
    method that will learn from the mistakes of trees in future rounds, could be advantageous.
    Boosting was designed to learn from the mistakes of trees in early rounds. Boosting,
    in particular gradient boosting – the focus of the next chapter – addresses this
    topic.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'In closure, the following graph displays the results of the tuned random forest
    regressor and the default XGBoost regressor when increasing the number of trees
    in the bike rentals dataset if the data is not shuffled:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15551_03_05.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Comparing the XGBoost default model with a tuned random forest
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, XGBoost does a much better job of learning as the number of
    trees increases. And the XGBoost model has not even been tuned!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the importance of ensemble methods. In particular,
    you learned about bagging, the combination of bootstrapping, sampling with replacement,
    and aggregation, combining many models into one. You built random forest classifiers
    and regressors. You adjusted `n_estimators` with the `warm_start` hyperparameter
    and used `oob_score_` to find errors. Then you modified random forest hyperparameters
    to fine-tune models. Finally, you examined a case study where shuffling the data
    gave excellent results but adding more trees to the random forest did not result
    in any gains with the unshuffled data, as contrasted with XGBoost.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn the fundamentals of boosting, an ensemble
    method that learns from its mistakes to improve upon accuracy as more trees are
    added. You will implement gradient boosting to make predictions, thereby setting
    the stage for Extreme gradient boosting, better known as XGBoost.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL

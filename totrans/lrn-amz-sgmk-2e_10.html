<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer132">
			<h1 id="_idParaDest-148"><a id="_idTextAnchor147"/>Chapter 8: Using Your Algorithms and Code</h1>
			<p>In the previous chapter, you learned how to train and deploy models with built-in frameworks such as <strong class="bold">scikit-learn</strong> and <strong class="bold">TensorFlow</strong>. Thanks to <strong class="bold">script mode</strong>, these frameworks make it easy to use your own code, without having to manage any training or inference containers.</p>
			<p>In some cases, your business or technical environment could make it difficult or even impossible to use these containers. Maybe you need to be in full control of how containers are built. Maybe you'd like to implement your own prediction logic. Maybe you're working with a framework or language that's not natively supported by SageMaker.</p>
			<p>In this chapter, you'll learn how to tailor training and inference containers to your own needs. You'll also learn how to train and deploy your own custom code, using either the SageMaker SDK directly or command-line open source tools. </p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Understanding how SageMaker invokes your code</li>
				<li>Customizing built-in framework containers</li>
				<li>Building custom training containers with the SageMaker Training Toolkit</li>
				<li>Building fully custom containers for training and inference with Python and R</li>
				<li>Training and deploying with your custom Python code on MLflow</li>
				<li>Building fully custom containers for SageMaker Processing</li>
			</ul>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor148"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you haven't got one already, please point your browser at <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create it. You should also familiarize yourself with the AWS Free Tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the AWS <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>) for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).  </p>
			<p>You will need a working Python 3.x environment. Installing the Anaconda distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory but strongly encouraged as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>You will need a working Docker installation. You'll find installation instructions and documentation at <a href="https://docs.docker.com">https://docs.docker.com</a>. </p>
			<p>The code examples included in this book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor149"/>Understanding how SageMaker invokes your code</h1>
			<p>When we worked <a id="_idIndexMarker806"/>with built-in algorithms and frameworks, we didn't pay much attention to how SageMaker actually invoked the training and deployment code. After all, that's what "built-in" means: grab what you need off the shelf and get to work.</p>
			<p>Of course, things are different if we want to use our own custom code and containers. We need to understand how they interface with SageMaker so that we implement them exactly right.</p>
			<p>In this section, we'll discuss this interface in detail. Let's start with the file layout.</p>
			<h3>Understanding the file layout inside a SageMaker container</h3>
			<p>To make our life simpler, SageMaker estimators automatically copy hyperparameters and input data inside training containers. Likewise, they automatically <a id="_idIndexMarker807"/>copy the trained model (and any checkpoints) from<a id="_idIndexMarker808"/> the container to S3. At deployment time, they do the reverse operation, copying the model from S3 into the container.</p>
			<p>As you can imagine, this requires a file layout convention:</p>
			<ul>
				<li>Hyperparameters are stored as a JSON dictionary in <strong class="source-inline">/opt/ml/input/config/hyperparameters.json</strong>. </li>
				<li>Input channels are stored in <strong class="source-inline">/opt/ml/input/data/CHANNEL_NAME</strong>. We saw in the previous <a id="_idIndexMarker809"/>chapter that the channel names match the ones passed to the <strong class="source-inline">fit()</strong> API.</li>
				<li>The model should <a id="_idIndexMarker810"/>be saved in and loaded from <strong class="source-inline">/opt/ml/model</strong>.</li>
			</ul>
			<p>Hence, we'll need to use these paths in our custom code. Now, let's see how the training and deployment code is invoked.</p>
			<h3>Understanding the options for custom training</h3>
			<p>In <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services Using Built-In Frameworks</em>, we studied script mode and how SageMaker uses it to invoke our training script. This feature <a id="_idIndexMarker811"/>is enabled by additional Python code present in <a id="_idIndexMarker812"/>the framework containers, namely, the SageMaker Training Toolkit (<a href="https://github.com/aws/sagemaker-training-toolkit">https://github.com/aws/sagemaker-training-toolkit</a>). </p>
			<p>In a nutshell, this training toolkit copies the entry point script, its <a id="_idIndexMarker813"/>hyperparameters, and its dependencies inside the container. It also copies data from the input channels inside the container. Then, it invokes the entry point script. Curious minds can read the code at <strong class="source-inline">src/sagemaker_training/entry_point.py</strong>.</p>
			<p>When it comes to customizing your training code, you have the following options:</p>
			<ul>
				<li>Customize an existing framework container, adding only your extra dependencies and code. Script mode and the framework estimator will be available.</li>
				<li>Build a custom container based solely on the SageMaker Training Toolkit. Script mode and the generic <strong class="source-inline">Estimator</strong> module will be available, but you'll have to install everything else.</li>
				<li>Build a fully custom container. If you want to start from a blank page or don't want any extra code inside your container, this is the way to go. You'll train with the generic <strong class="source-inline">Estimator</strong> module, and script mode won't be available. Your training code will be invoked directly (more on this later).</li>
			</ul>
			<h3>Understanding the options for custom deployment</h3>
			<p>Framework <a id="_idIndexMarker814"/>containers include additional Python code for deployment. Here are the repositories for the most popular frameworks:</p>
			<ul>
				<li><strong class="bold">TensorFlow</strong>: <a href="https://github.com/aws/sagemaker-tensorflow-serving-container">https://github.com/aws/sagemaker-tensorflow-serving-container</a>. Models <a id="_idIndexMarker815"/>are served with <strong class="bold">TensorFlow Serving</strong> (<a href="https://www.tensorflow.org/tfx/guide/serving">https://www.tensorflow.org/tfx/guide/serving</a>).</li>
				<li><strong class="bold">PyTorch</strong>: <a href="https://github.com/aws/sagemaker-pytorch-inference-toolkit">https://github.com/aws/sagemaker-pytorch-inference-toolkit</a>.<span class="hidden"> </span>Models <a id="_idIndexMarker816"/>are <a id="_idIndexMarker817"/>served with <strong class="bold">TorchServe</strong> (<a href="https://pytorch.org/serve">https://pytorch.org/serve</a>).</li>
				<li><strong class="bold">Apache MXNet</strong>: <a href="https://github.com/aws/sagemaker-mxnet-inference-toolkit">https://github.com/aws/sagemaker-mxnet-inference-toolkit</a>. Models <a id="_idIndexMarker818"/>are served <a id="_idIndexMarker819"/>with the <strong class="bold">Multi-Model Server</strong> (<a href="https://github.com/awslabs/multi-model-server">https://github.com/awslabs/multi-model-server</a>), integrated <a id="_idIndexMarker820"/>into the <strong class="bold">SageMaker Inference Toolkit</strong> (<a href="https://github.com/aws/sagemaker-inference-toolkit">https://github.com/aws/sagemaker-inference-toolkit</a>).</li>
				<li><strong class="bold">Scikit-learn</strong>: <a href="https://github.com/aws/sagemaker-scikit-learn-container">https://github.com/aws/sagemaker-scikit-learn-container</a>. Models are <a id="_idIndexMarker821"/>served with the Multi-Model Server.</li>
				<li><strong class="bold">XGBoost</strong>: <a href="https://github.com/aws/sagemaker-xgboost-container">https://github.com/aws/sagemaker-xgboost-container</a>. Models are served <a id="_idIndexMarker822"/>with the Multi-Model Server.</li>
			</ul>
			<p>Just like for training, you have three options:</p>
			<ul>
				<li>Customize an existing framework container. Models will be served using the existing inference logic.</li>
				<li>Build a custom container based solely on the SageMaker Inference Toolkit. Models will be served by the Multi-Model Server.</li>
				<li>Build a fully custom container, doing away with any inference logic and implementing your own instead.</li>
			</ul>
			<p>Whether you use a single container for training and deployment or two different containers is up to you. A lot of different factors come into <a id="_idIndexMarker823"/>play: who builds the containers, who runs them, and so on. Only you can decide what the best option for your particular setup is.</p>
			<p>Now, let's run some examples!</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>Customizing an existing framework container</h1>
			<p>Of course, we could <a id="_idIndexMarker824"/>simply write a Dockerfile referencing<a id="_idIndexMarker825"/> one of the Deep Learning Containers images (<a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md">https://github.com/aws/deep-learning-containers/blob/master/available_images.md</a>) and add our own commands. See the following example:</p>
			<p class="source-code">FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.4.1-cpu-py37-ubuntu18.04</p>
			<p class="source-code">. . .</p>
			<p>Instead, let's customize and rebuild the <strong class="bold">PyTorch</strong> training and inference containers on our local machine. The process is similar to other frameworks.</p>
			<p class="callout-heading">Build environment</p>
			<p class="callout">Docker needs <a id="_idIndexMarker826"/>to be installed and running. To avoid <a id="_idIndexMarker827"/>throttling when pulling base images, I recommend that you create a <strong class="bold">Docker Hub</strong> account (<a href="https://hub.docker.com">https://hub.docker.com</a>) and log in with <strong class="source-inline">docker login</strong> or <strong class="bold">Docker Desktop</strong>.</p>
			<p class="callout">To avoid bizarre dependency issues (I'm looking at you, macOS), I also recommend that you build images on an <strong class="bold">Amazon EC2</strong> instance powered by <strong class="bold">Amazon Linux 2</strong>. You don't need <a id="_idIndexMarker828"/>a large one (<strong class="source-inline">m5.large</strong> should suffice), but please make sure to provision <a id="_idIndexMarker829"/>more storage than the default 8 GB. I recommend 64 GB. You also need to make sure that the <strong class="bold">IAM</strong> role for the instance allows you to push and pull EC2 images. If you're unsure how to create and connect to an EC2 instance, this tutorial will get you started: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html</a>.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor151"/>Setting up your build environment on EC2</h2>
			<p>We will get started using the following steps:</p>
			<ol>
				<li>Once your <a id="_idIndexMarker830"/>EC2 instance is up, we connect to it with <strong class="source-inline">ssh</strong>. We first install Docker and add the <strong class="source-inline">ec2-user</strong> to the <strong class="source-inline">docker</strong> group. This will <a id="_idIndexMarker831"/>allow us to run Docker commands as a non-root user:<p class="source-code"><strong class="bold">$ sudo yum -y install docker</strong></p><p class="source-code"><strong class="bold">$ sudo usermod -a -G docker ec2-user</strong></p></li>
				<li>In order to apply this permission change, we log out and log in again. </li>
				<li>We make sure that <strong class="source-inline">docker</strong> is running and we log in to Docker Hub:<p class="source-code"><strong class="bold">$ service docker start</strong></p><p class="source-code"><strong class="bold">$ docker login</strong></p></li>
				<li>We install <strong class="source-inline">git</strong>, Python 3, and <strong class="source-inline">pip</strong>:<p class="source-code"><strong class="bold">$ sudo yum -y install git python3-devel python3-pip</strong></p></li>
			</ol>
			<p>Our EC2 instance is now ready, and we can move on to building containers.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor152"/>Building training and inference containers</h2>
			<p>This can be done<a id="_idIndexMarker832"/> using the following steps:</p>
			<ol>
				<li value="1">We clone the <strong class="source-inline">deep-learning-containers</strong> repository, which centralizes all training and <a id="_idIndexMarker833"/>inference code for TensorFlow, PyTorch, Apache MXNet, and Hugging Face, and adds convenient scripts to <a id="_idIndexMarker834"/>build their containers:<p class="source-code"><strong class="bold">$ git clone </strong>https://github.com/aws/deep-learning-containers.git</p><p class="source-code"><strong class="bold">$ cd deep-learning-containers</strong></p></li>
				<li>We set <a id="_idIndexMarker835"/>environment variables for our account ID, the region we're running in, and the name of a new repository we're going to create in Amazon ECR:<p class="source-code"><strong class="bold">$ export ACCOUNT_ID=123456789012</strong></p><p class="source-code"><strong class="bold">$ export REGION=eu-west-1</strong></p><p class="source-code"><strong class="bold">$ export REPOSITORY_NAME=my-pt-dlc</strong></p></li>
				<li>We create <a id="_idIndexMarker836"/>the repository in Amazon ECR, and we log in. Please refer to the documentation for details (<a href="https://docs.aws.amazon.com/ecr/index.html">https://docs.aws.amazon.com/ecr/index.html</a>):<p class="source-code"><strong class="bold">$ aws ecr create-repository </strong></p><p class="source-code"><strong class="bold">--repository-name $REPOSITORY_NAME --region $REGION</strong></p><p class="source-code"><strong class="bold">$ aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com</strong></p></li>
				<li>We create a virtual environment, and we install the Python requirements:<p class="source-code"><strong class="bold">$ python3 -m venv dlc</strong></p><p class="source-code"><strong class="bold">$ source dlc/bin/activate</strong></p><p class="source-code"><strong class="bold">$ pip install -r src/requirements.txt</strong></p></li>
				<li>Here, we'd like to build the training and inference containers for PyTorch 1.8, on both the CPU and GPU. We can find the corresponding Docker files in <strong class="source-inline">pytorch/training/docker/1.8/py3/</strong> and customize them to our needs. For example, we could pin Deep Graph Library to version 0.6.1: <p class="source-code">&amp;&amp; conda install -c dglteam -y dgl==0.6.1 \</p></li>
				<li>Once we've <a id="_idIndexMarker837"/>edited the Docker files, we take a look at the build<a id="_idIndexMarker838"/> configuration file for the latest PyTorch version (<strong class="source-inline">pytorch/buildspec.yml</strong>). We decide to customize image tags to make sure each image is clearly identifiable:<p class="source-code">BuildCPUPTTrainPy3DockerImage:</p><p class="source-code">    tag: !join [ *VERSION, "-", *DEVICE_TYPE, "-", *TAG_PYTHON_VERSION, "-", *OS_VERSION, <strong class="bold">"-training"</strong> ]</p><p class="source-code">BuildGPUPTTrainPy3DockerImage:</p><p class="source-code">    tag: !join [ *VERSION, "-", *DEVICE_TYPE, "-", *TAG_PYTHON_VERSION, "-", *CUDA_VERSION, "-", *OS_VERSION, <strong class="bold">"-training"</strong> ]</p><p class="source-code">BuildCPUPTInferencePy3DockerImage:</p><p class="source-code">    tag: !join [ *VERSION, "-", *DEVICE_TYPE, "-", *TAG_PYTHON_VERSION, "-", *OS_VERSION, <strong class="bold">"-inference"</strong> ]</p><p class="source-code">BuildGPUPTInferencePy3DockerImage:</p><p class="source-code">    tag: !join [ *VERSION, "-", *DEVICE_TYPE, "-", *TAG_PYTHON_VERSION, "-", *CUDA_VERSION, "-", *OS_VERSION, <strong class="bold">"-inference"</strong>]</p></li>
				<li>Finally, we run the setup script and launch the build process:<p class="source-code"><strong class="bold">$ bash src/setup.sh pytorch</strong></p><p class="source-code"><strong class="bold">$ python src/main.py --buildspec pytorch/buildspec.yml --framework pytorch --device_types cpu,gpu --image_types training,inference</strong></p></li>
				<li>After a little while, all four images are built (plus an example image), and we can see them in our local Docker:<p class="source-code"><strong class="bold">$ docker images</strong></p><p class="source-code"><strong class="bold">123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-gpu-py36-cu111-ubuntu18.04-example-2021-05-28-10-14-15     </strong></p><p class="source-code"><strong class="bold">123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-gpu-py36-cu111-ubuntu18.04-training-2021-05-28-10-14-15    </strong></p><p class="source-code"><strong class="bold">123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-gpu-py36-cu111-ubuntu18.04-inference-2021-05-28-10-14-15</strong></p><p class="source-code"><strong class="bold">123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-cpu-py36-ubuntu18.04-inference-2021-05-28-10-14-15         </strong></p><p class="source-code"><strong class="bold">123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-cpu-py36-ubuntu18.04-training-2021-05-28-10-14-15          </strong></p></li>
				<li>We can <a id="_idIndexMarker839"/>also see them in our ECR repository, as <a id="_idIndexMarker840"/>shown in the following screenshot:<div id="_idContainer129" class="IMG---Figure"><img src="Images/B17705_08_1.jpg" alt="Figure 8.1 – Viewing images in ECR&#13;&#10;" width="966" height="584"/></div><p class="figure-caption">Figure 8.1 – Viewing images in ECR</p></li>
				<li>The <a id="_idIndexMarker841"/>images are now available with the SageMaker SDK. Let's<a id="_idIndexMarker842"/> train with our new CPU image. All we have to do is pass its name in the <strong class="source-inline">image_uri</strong> parameter of the <strong class="source-inline">PyTorch</strong> estimator. Please note that we can remove <strong class="source-inline">py_version</strong> and <strong class="source-inline">framework_version</strong>:<p class="source-code">Estimator = PyTorch(</p><p class="source-code">    image_uri='123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc:1.8.1-cpu-py36-ubuntu18.04-training-2021-05-28-10-14-15',</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    entry_point='karate_club_sagemaker.py',</p><p class="source-code">    hyperparameters={'node_count': 34, 'epochs': 30},</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large')</p></li>
			</ol>
			<p>As you can see, it's pretty easy to customize Deep Learning Containers. Now, let's go one level deeper and work only with the training toolkit.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor153"/>Using the SageMaker Training Toolkit with scikit-learn</h1>
			<p>In this example, we're going to build a custom Python container with the SageMaker Training Toolkit. We'll use it to train a scikit-learn <a id="_idIndexMarker843"/>model on the Boston <a id="_idIndexMarker844"/>Housing dataset, using script mode and the <strong class="source-inline">SKLearn</strong> estimator.</p>
			<p>We need three building blocks:</p>
			<ul>
				<li>The training script. Since script mode will be available, we can use exactly the same code as in the scikit-learn example from <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services Using Built-In Frameworks</em>.</li>
				<li>We need a Dockerfile and Docker commands to build our custom container.</li>
				<li>We also need an <strong class="source-inline">SKLearn</strong> estimator configured to use our custom container.</li>
			</ul>
			<p>Let's take care of the container:</p>
			<ol>
				<li value="1">A Dockerfile can <a id="_idIndexMarker845"/>get quite <a id="_idIndexMarker846"/>complicated. No need for that here! We start from the official Python 3.7 image available on Docker Hub (<a href="https://hub.docker.com/_/python">https://hub.docker.com/_/python</a>). We install scikit-learn, <strong class="source-inline">numpy</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">joblib</strong>, and the SageMaker Training Toolkit:<p class="source-code">FROM python:3.7</p><p class="source-code">RUN pip3 install --no-cache scikit-learn numpy pandas joblib sagemaker-training</p></li>
				<li>We build the image with the <strong class="source-inline">docker build</strong> command, tagging it as <strong class="source-inline">sklearn-customer:sklearn</strong>: <p class="source-code"><strong class="bold">$ docker build -t sklearn-custom:sklearn -f Dockerfile .</strong></p><p>Once the image is built, we find its identifier:</p><p class="source-code"><strong class="bold">$ docker images</strong></p><p class="source-code"><strong class="bold">REPOSITORY          TAG         IMAGE ID   </strong></p><p class="source-code"><strong class="bold">sklearn-custom      sklearn     bf412a511471         </strong></p></li>
				<li>Using the AWS CLI, we <a id="_idIndexMarker847"/>create a repository in Amazon ECR to host this image, and we log in to the repository:<p class="source-code"><strong class="bold">$ aws ecr create-repository --repository-name sklearn-custom --region eu-west-1</strong></p><p class="source-code"><strong class="bold">$ aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:latest</strong></p></li>
				<li>Using the image identifier, we tag the image with the repository identifier:<p class="source-code"><strong class="bold">$ docker tag bf412a511471 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:sklearn</strong></p></li>
				<li>We push <a id="_idIndexMarker848"/>the image to the repository:<p class="source-code"><strong class="bold">$ docker push 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:sklearn</strong></p><p>The image <a id="_idIndexMarker849"/>is now ready for training with a SageMaker estimator.</p></li>
				<li>We define an <strong class="source-inline">SKLearn</strong> estimator, setting the <strong class="source-inline">image_uri</strong> parameter to the name of the <a id="_idIndexMarker850"/>container we just created:<p class="source-code">sk = SKLearn(</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    entry_point='sklearn-boston-housing.py',</p><p class="source-code">    image_name='123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:sklearn',</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large',</p><p class="source-code">    output_path=output,</p><p class="source-code">    hyperparameters={</p><p class="source-code">         'normalize': True,</p><p class="source-code">         'test-size': 0.1</p><p class="source-code">    }</p><p class="source-code">)</p></li>
				<li>We set the location of the training <a id="_idIndexMarker851"/>channel and launch the <a id="_idIndexMarker852"/>training as usual. In the training log, we see that our code is indeed invoked with script mode:<p class="source-code"><strong class="bold">/usr/local/bin/python -m sklearn-boston-housing </strong></p><p class="source-code"><strong class="bold">--normalize True --test-size 0.1</strong></p></li>
			</ol>
			<p>As you can see, it's easy to customize training containers. Thanks to the SageMaker Training Toolkit, you can work just as with a built-in framework container. We used scikit-learn here, and you can do the same with all other frameworks.</p>
			<p>However, we cannot use this container for deployment, as it doesn't contain any model-serving code. We should add bespoke code to launch a web app, which is exactly what we're going to do in the next example.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor154"/>Building a fully custom container for scikit-learn</h1>
			<p>In this example, we're <a id="_idIndexMarker853"/>going to build a fully custom <a id="_idIndexMarker854"/>container without any AWS code. We'll use it to train a scikit-learn model on the Boston Housing dataset, using a generic <strong class="source-inline">Estimator</strong> module. With the same container, we'll deploy the model thanks to a Flask web application.</p>
			<p>We'll proceed in a logical way, first taking care of the training, and then updating the code to handle deployment.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor155"/>Training with a fully custom container</h2>
			<p>Since we can't rely on script mode anymore, the training code <a id="_idIndexMarker855"/>needs to be modified. This is what it looks like, and you'll easily figure out what's happening here:</p>
			<p class="source-code">#!/usr/bin/env python</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import joblib, os, json</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    config_dir = '/opt/ml/input/config'</p>
			<p class="source-code">    training_dir = '/opt/ml/input/data/training'</p>
			<p class="source-code">    model_dir = '/opt/ml/model'</p>
			<p class="source-code">    with open(os.path.join(config_dir, </p>
			<p class="source-code">    'hyperparameters.json')) as f:</p>
			<p class="source-code">        hp = json.load(f)</p>
			<p class="source-code">        normalize = hp['normalize']</p>
			<p class="source-code">        test_size = float(hp['test-size'])</p>
			<p class="source-code">        random_state = int(hp['random-state'])</p>
			<p class="source-code">    filename = os.path.join(training_dir, 'housing.csv')</p>
			<p class="source-code">    data = pd.read_csv(filename)</p>
			<p class="source-code">    # Train model</p>
			<p class="source-code">    . . . </p>
			<p class="source-code">    joblib.dump(regr, </p>
			<p class="source-code">                os.path.join(model_dir, 'model.joblib'))</p>
			<p>Using the standard file layout for SageMaker containers, we read hyperparameters from their JSON file. Then, we load the dataset, train the model, and save it at <a id="_idIndexMarker856"/>the correct location.</p>
			<p>There's another very important difference, and we have to dive a bit into Docker to explain it. SageMaker will run the training container as <strong class="source-inline">docker run &lt;IMAGE_ID&gt; train</strong>, passing the <strong class="source-inline">train</strong> argument to the entry point of the container. </p>
			<p>If your container has a predefined entry point, the <strong class="source-inline">train</strong> argument will be passed to it, say, <strong class="source-inline">/usr/bin/python train</strong>. If your container doesn't have a predefined entry point, <strong class="source-inline">train</strong> is the actual command that will be run. </p>
			<p>To avoid annoying issues, I recommend that your training code ticks the following boxes:</p>
			<ul>
				<li>Name it <strong class="source-inline">train</strong>—no extension, just <strong class="source-inline">train</strong>.</li>
				<li>Make it executable.</li>
				<li>Make sure it's in the <strong class="source-inline">PATH</strong> value.</li>
				<li>The first line of the script should define the path to the interpreter, for example, <strong class="source-inline">#!/usr/bin/env python</strong>.</li>
			</ul>
			<p>This should <a id="_idIndexMarker857"/>guarantee that your training code is invoked correctly whether your container has a predefined entry point or not.  </p>
			<p>We'll take care of this in the Dockerfile, starting from an official Python image. Note that we're not installing the SageMaker Training Toolkit any longer:</p>
			<p class="source-code">FROM python:3.7</p>
			<p class="source-code">RUN pip3 install --no-cache scikit-learn numpy pandas joblib</p>
			<p class="source-code">COPY sklearn-boston-housing-generic.py /usr/bin/train</p>
			<p class="source-code">RUN chmod 755 /usr/bin/train</p>
			<p>The name of the script is correct. It's executable, and <strong class="source-inline">/usr/bin</strong> is in <strong class="source-inline">PATH</strong>. </p>
			<p>We should be all set—let's create our custom container and launch a training job with it:</p>
			<ol>
				<li value="1">We build and push the image, using a different tag:<p class="source-code"><strong class="bold">$ docker build -t sklearn-custom:estimator -f Dockerfile-generic .</strong></p><p class="source-code"><strong class="bold">$ docker tag &lt;IMAGE_ID&gt; 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:estimator</strong></p><p class="source-code"><strong class="bold">$ docker push 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:estimator</strong></p></li>
				<li>We update our notebook code to use the generic <strong class="source-inline">Estimator</strong> module:<p class="source-code">from sagemaker.estimator import Estimator</p><p class="source-code">sk = Estimator(</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    image_name='123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:estimator',</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large',</p><p class="source-code">    output_path=output,</p><p class="source-code">    hyperparameters={</p><p class="source-code">         'normalize': True,</p><p class="source-code">         'test-size': 0.1,</p><p class="source-code">         'random-state': 123</p><p class="source-code">    }</p><p class="source-code">)</p></li>
				<li>We <a id="_idIndexMarker858"/>train as usual.</li>
			</ol>
			<p>Now let's add code to deploy this model.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor156"/>Deploying a fully custom container</h2>
			<p>Flask is a highly popular web framework for Python (<a href="https://palletsprojects.com/p/flask">https://palletsprojects.com/p/flask</a>). It's simple <a id="_idIndexMarker859"/>and well <a id="_idIndexMarker860"/>documented. We're going to use it to build a simple prediction API hosted in our container.</p>
			<p>Just like for our training code, SageMaker requires that the deployment script is copied inside the container. The image will be run as <strong class="source-inline">docker run &lt;IMAGE_ID&gt; serve</strong>. </p>
			<p>HTTP requests will be sent to port <strong class="source-inline">8080</strong>. The container must provide a <strong class="source-inline">/ping</strong> URL for health checks and an<strong class="source-inline">/invocations</strong> URL for prediction <a id="_idIndexMarker861"/>requests. We'll use CSV as the input format.</p>
			<p>Hence, your deployment code needs to tick the following boxes:</p>
			<ul>
				<li>Name it <strong class="source-inline">serve</strong>—no extension, just <strong class="source-inline">serve</strong>.</li>
				<li>Make it executable.</li>
				<li>Make sure it's in <strong class="source-inline">PATH</strong>.</li>
				<li>Make sure port <strong class="source-inline">8080</strong> is exposed by the container.</li>
				<li>Provide code to handle the <strong class="source-inline">/ping</strong> and <strong class="source-inline">/invocations</strong> URLs.</li>
			</ul>
			<p>Here's the updated Dockerfile. We install Flask, copy the deployment code, and open port <strong class="source-inline">8080</strong>:</p>
			<p class="source-code">FROM python:3.7</p>
			<p class="source-code">RUN pip3 install --no-cache scikit-learn numpy pandas joblib</p>
			<p class="source-code">RUN pip3 install --no-cache flask</p>
			<p class="source-code">COPY sklearn-boston-housing-generic.py /usr/bin/train</p>
			<p class="source-code">COPY sklearn-boston-housing-serve.py /usr/bin/serve</p>
			<p class="source-code">RUN chmod 755 /usr/bin/train /usr/bin/serve</p>
			<p class="source-code">EXPOSE 8080</p>
			<p>This is how we could implement a simple prediction service with Flask:</p>
			<ol>
				<li value="1">We import <a id="_idIndexMarker862"/>the required modules. We load the model from <strong class="source-inline">/opt/ml/model</strong> and initialize the Flask application:<p class="source-code">#!/usr/bin/env python</p><p class="source-code">import joblib, os</p><p class="source-code">import pandas as pd</p><p class="source-code">from io import StringIO</p><p class="source-code">import flask</p><p class="source-code">from flask import Flask, Response</p><p class="source-code">model_dir = '/opt/ml/model'</p><p class="source-code">model = joblib.load(os.path.join(model_dir, </p><p class="source-code">                    'model.joblib'))</p><p class="source-code">app = Flask(__name__)</p></li>
				<li>We implement the <strong class="source-inline">/ping</strong> URL for health checks, by simply returning HTTP code 200 (OK):<p class="source-code">@app.route("/ping", methods=["GET"])</p><p class="source-code">def ping():</p><p class="source-code">    return Response(response="\n", status=200)</p></li>
				<li>We implement the <strong class="source-inline">/invocations</strong> URL. If the content type is not <strong class="source-inline">text/csv</strong>, we return HTTP code 415 (Unsupported Media Type). If it is, we decode the request <a id="_idIndexMarker863"/>body and store it in a file-like memory buffer. Then, we read the CSV samples, predict them, and send the results:<p class="source-code">@app.route("/invocations", methods=["POST"])</p><p class="source-code">def predict():</p><p class="source-code">    if flask.request.content_type == 'text/csv':</p><p class="source-code">        data = flask.request.data.decode('utf-8')</p><p class="source-code">        s = StringIO(data)</p><p class="source-code">        data = pd.read_csv(s, header=None)</p><p class="source-code">        response = model.predict(data)</p><p class="source-code">        response = str(response)</p><p class="source-code">    else:</p><p class="source-code">        return flask.Response(</p><p class="source-code">            response='CSV data only', </p><p class="source-code">            status=415, mimetype='text/plain')</p><p class="source-code">    return Response(response=response, status=200)</p></li>
				<li>At startup, the script launches the Flask app on port <strong class="source-inline">8080</strong>:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    app.run(host="0.0.0.0", port=8080)</p><p>That's not too difficult, even if you're not yet familiar with Flask.</p></li>
				<li>We rebuild and push the image, and then we train again with the same estimator. No change is required here.</li>
				<li>We deploy the model:<p class="source-code">sk_predictor = sk.deploy(instance_type='ml.t2.medium',</p><p class="source-code">                         initial_instance_count=1)</p><p class="callout-heading">Reminder</p><p class="callout">If you see some weird behavior here (the endpoint not deploying, cryptic error messages, and so on), Docker is probably hosed. <strong class="source-inline">sudo service docker restart</strong> should fix most problems. Cleaning <strong class="source-inline">tmp*</strong> cruft in <strong class="source-inline">/tmp</strong> may also help.</p></li>
				<li>We prepare <a id="_idIndexMarker864"/>a couple of test samples, set the content type to <strong class="source-inline">text/csv</strong>, and invoke the prediction API:<p class="source-code">test_samples = ['0.00632, 18.00, 2.310, 0, 0.5380, 6.5750, 65.20, 4.0900, 1,296.0, 15.30, 396.90, 4.98',             </p><p class="source-code">'0.02731, 0.00, 7.070, 0, 0.4690, 6.4210, 78.90, 4.9671, 2,242.0, 17.80, 396.90, 9.14']</p><p class="source-code">sk_predictor.serializer =</p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">response = sk_predictor.predict(test_samples)</p><p class="source-code">print(response)</p><p>You should see something similar to this. The API has been successfully invoked:</p><p class="source-code"><strong class="bold">b'[[29.801388899699845], [24.990809475886078]]'</strong></p></li>
				<li>When we're done, we delete the endpoint:<p class="source-code">sk_predictor.delete_endpoint()</p></li>
			</ol>
			<p>In the next example, we're going to train and deploy a model using the R environment. This will give us an opportunity to step out of the Python world for a bit. As you will see, things are not really different.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor157"/>Building a fully custom container for R</h1>
			<p>R is a popular language for data exploration and <a id="_idIndexMarker865"/>analysis. In this example, we're going to build a custom <a id="_idIndexMarker866"/>container to train and deploy a linear regression model on the Boston <a id="_idIndexMarker867"/>Housing dataset. </p>
			<p>The overall process is similar to building a custom container for Python. Instead <a id="_idIndexMarker868"/>of using Flask to build our prediction API, we'll use <strong class="source-inline">plumber</strong> (<a href="https://www.rplumber.io">https://www.rplumber.io</a>). </p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/>Coding with R and plumber</h2>
			<p>Don't worry if you're <a id="_idIndexMarker869"/>not familiar with R. This is a <a id="_idIndexMarker870"/>really simple example, and I'm sure you'll be able to follow along:</p>
			<ol>
				<li value="1">We write a function to train our model. It loads the hyperparameters and the dataset from the conventional paths. It normalizes the dataset if we requested it:<p class="source-code"># train_function.R</p><p class="source-code">library("rjson")</p><p class="source-code">train &lt;- function() {</p><p class="source-code">    hp &lt;- fromJSON(file = </p><p class="source-code">          '/opt/ml/input/config/hyperparameters.json')</p><p class="source-code">    normalize &lt;- hp$normalize</p><p class="source-code">    data &lt;- read.csv(file = </p><p class="source-code">            '/opt/ml/input/data/training/housing.csv', </p><p class="source-code">            header=T)</p><p class="source-code">    if (normalize) {</p><p class="source-code">        data &lt;- as.data.frame(scale(data))</p><p class="source-code">    }</p><p>It trains a linear <a id="_idIndexMarker871"/>regression model, taking all features into <a id="_idIndexMarker872"/>account to predict the median house price (the <strong class="source-inline">medv</strong> column). Finally, it saves the model in the right place:</p><p class="source-code">    model = lm(medv~., data)</p><p class="source-code">    saveRDS(model, '/opt/ml/model/model.rds')</p><p class="source-code">}</p></li>
				<li>We write a function to serve predictions. Using <strong class="source-inline">plumber</strong> annotations, we define a <strong class="source-inline">/ping</strong> URL for health checks and an<strong class="source-inline">/invocations</strong> URL for predictions:<p class="source-code"># serve_function.R</p><p class="source-code">#' @get /ping</p><p class="source-code">function() {</p><p class="source-code">  return('')</p><p class="source-code">}</p><p class="source-code">#' @post /invocations</p><p class="source-code">function(req) {</p><p class="source-code">    model &lt;- readRDS('/opt/ml/model/model.rds')</p><p class="source-code">    conn &lt;- textConnection(gsub('\\\\n', '\n', </p><p class="source-code">                           req$postBody))</p><p class="source-code">    data &lt;- read.csv(conn)</p><p class="source-code">    close(conn)</p><p class="source-code">    medv &lt;- predict(model, data)</p><p class="source-code">    return(medv)</p><p class="source-code">}</p></li>
				<li>Putting these two pieces together, we write a main function that will serve as the entry <a id="_idIndexMarker873"/>point for our script. SageMaker will pass either a <strong class="source-inline">train</strong> or <strong class="source-inline">serve</strong> command-line argument, and we'll call the corresponding <a id="_idIndexMarker874"/>function in our code:<p class="source-code">library('plumber')</p><p class="source-code">source('train_function.R')</p><p class="source-code">serve &lt;- function() {</p><p class="source-code">    app &lt;- plumb('serve_function.R')</p><p class="source-code">    app$run(host='0.0.0.0', port=8080)}</p><p class="source-code">args &lt;- commandArgs()</p><p class="source-code">if (any(grepl('train', args))) {</p><p class="source-code">    train()</p><p class="source-code">}</p><p class="source-code">if (any(grepl('serve', args))) {</p><p class="source-code">    serve()</p><p class="source-code">}</p></li>
			</ol>
			<p>This is all of the R code that we need. Now, let's take care of the container.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>Building a custom container </h2>
			<p>We need to build a custom container storing the R runtime, as well <a id="_idIndexMarker875"/>as our script. The Dockerfile is as follows:</p>
			<ol>
				<li value="1">We start from an official R image in <strong class="bold">Docker Hub</strong> and add the <a id="_idIndexMarker876"/>dependencies we need (these are the ones I needed on my machine; your mileage may vary):<p class="source-code">FROM r-base:latest</p><p class="source-code">WORKDIR /opt/ml/</p><p class="source-code">RUN apt-get update</p><p class="source-code">RUN apt-get install -y libcurl4-openssl-dev libsodium-dev</p><p class="source-code">RUN R -e "install.packages(c('rjson', 'plumber')) "</p></li>
				<li>Then, we copy our code inside the container and define the main function as its explicit entry point:<p class="source-code">COPY main.R train_function.R serve_function.R /opt/ml/</p><p class="source-code">ENTRYPOINT ["/usr/bin/Rscript", "/opt/ml/main.R", "--no-save"]</p></li>
				<li>We create <a id="_idIndexMarker877"/>a new repository in ECR. Then, we build the image (this could take a while and involve compilation steps) and push it:<p class="source-code"><strong class="bold">$ aws ecr create-repository --repository-name r-custom --region eu-west-1</strong></p><p class="source-code"><strong class="bold">$ aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.eu-west-1.amazonaws.com/r-custom:latest</strong></p><p class="source-code"><strong class="bold">$ docker build -t r-custom:latest -f Dockerfile .</strong></p><p class="source-code"><strong class="bold">$ docker tag &lt;IMAGE_ID&gt; 123456789012.dkr.ecr.eu-west-1.amazonaws.com/r-custom:latest</strong></p><p class="source-code"><strong class="bold">$ docker push 123456789012.dkr.ecr.eu-west-1.amazonaws.com/r-custom:latest</strong></p></li>
			</ol>
			<p>We're all set, so let's train and deploy.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/>Training and deploying a custom container on SageMaker</h2>
			<p>Jumping to a Jupyter notebook, we use the <a id="_idIndexMarker878"/>SageMaker SDK <a id="_idIndexMarker879"/>to train <a id="_idIndexMarker880"/>and deploy our container:</p>
			<ol>
				<li value="1">We configure an <strong class="source-inline">Estimator</strong> module <a id="_idIndexMarker881"/>with our custom container:<p class="source-code">r_estimator = Estimator(</p><p class="source-code">    role = sagemaker.get_execution_role(),</p><p class="source-code">    image_uri='123456789012.dkr.ecr.eu-west-1.amazonaws.com/r-custom:latest',</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large',</p><p class="source-code">    output_path=output,</p><p class="source-code">    hyperparameters={'normalize': False}</p><p class="source-code">)</p><p class="source-code">r_estimator.fit({'training':training})</p></li>
				<li>Once the training job is complete, we deploy the model as usual:<p class="source-code">r_predictor = r_estimator.deploy(</p><p class="source-code">    initial_instance_count=1, </p><p class="source-code">    instance_type='ml.t2.medium')</p></li>
				<li>Finally, we read the full dataset (why not?) and send it to the endpoint:<p class="source-code">import pandas as pd</p><p class="source-code">data = pd.read_csv('housing.csv')</p><p class="source-code">data.drop(['medv'], axis=1, inplace=True)</p><p class="source-code">data = data.to_csv(index=False)</p><p class="source-code">r_predictor.serializer = </p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">response = r_predictor.predict(data)</p><p class="source-code">print(response)</p><p>The <a id="_idIndexMarker882"/>output should look like this:</p><p class="source-code"><strong class="bold">b'[30.0337,25.0568,30.6082,28.6772,27.9288. . .</strong></p></li>
				<li>When we're done, we delete the endpoint:<p class="source-code">r_predictor.delete_endpoint()</p></li>
			</ol>
			<p>Whether you're using Python, R, or something else, it's reasonably easy to build and deploy your own custom container. Still, you need to build your own <a id="_idIndexMarker883"/>little web application, which is something <a id="_idIndexMarker884"/>you may neither know how to do <a id="_idIndexMarker885"/>nor enjoy doing. Wouldn't it be nice if we had a tool that took care of all of that pesky container and web stuff?</p>
			<p>As a matter of fact, there is one: <strong class="bold">MLflow</strong>.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor161"/>Training and deploying with your own code on MLflow</h1>
			<p>MLflow is an <a id="_idIndexMarker886"/>open source <a id="_idIndexMarker887"/>platform <a id="_idIndexMarker888"/>for machine learning (<a href="https://mlflow.org">https://mlflow.org</a>). It was initiated by Databricks (<a href="https://databricks.com">https://databricks.com</a>), who also brought us <strong class="bold">Spark</strong>. MLflow has lots <a id="_idIndexMarker889"/>of features, including the ability to <a id="_idIndexMarker890"/>deploy Python-trained models on SageMaker.</p>
			<p>This section is not intended to be an MLflow tutorial. You <a id="_idIndexMarker891"/>can find documentation and examples at <a href="https://www.mlflow.org/docs/latest/index.html">https://www.mlflow.org/docs/latest/index.html</a>. </p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/>Installing MLflow</h2>
			<p>On our local machine, let's set up a virtual environment for MLflow and install the <a id="_idIndexMarker892"/>required libraries. The following example was tested with MLflow 1.17:</p>
			<ol>
				<li value="1">We first initialize a new virtual environment named <strong class="source-inline">mlflow-example</strong>. Then, we activate it:<p class="source-code"><strong class="bold">$ virtualenv mlflow-example</strong></p><p class="source-code"><strong class="bold">$ source mlflow-example/bin/activate</strong></p></li>
				<li>We install MLflow and the libraries required by our training script:<p class="source-code"><strong class="bold">$ pip install mlflow gunicorn pandas sklearn xgboost boto3</strong></p></li>
				<li>Finally, we download the Direct Marketing dataset we already used with XGBoost in <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services Using Built-In Frameworks</em>:<p class="source-code"><strong class="bold">$ wget -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</strong></p><p class="source-code"><strong class="bold">$ unzip -o bank-additional.zip</strong></p></li>
			</ol>
			<p>The setup is complete. Let's train the model.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/>Training a model with MLflow</h2>
			<p>The training script sets the MLflow experiment for this run so that <a id="_idIndexMarker893"/>we may log metadata (hyperparameters, metrics, and so on). Then, it loads the dataset, trains an XGBoost classifier, and logs the model:</p>
			<p class="source-code"># train-xgboost.py</p>
			<p class="source-code">import mlflow.xgboost</p>
			<p class="source-code">import xgboost as xgb</p>
			<p class="source-code">from load_dataset import load_dataset</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    mlflow.set_experiment('dm-xgboost')</p>
			<p class="source-code">    with mlflow.start_run(run_name='dm-xgboost-basic') </p>
			<p class="source-code">    as run:</p>
			<p class="source-code">        x_train, x_test, y_train, y_test = load_dataset(</p>
			<p class="source-code">            'bank-additional/bank-additional-full.csv')</p>
			<p class="source-code">        cls = xgb.XGBClassifier(</p>
			<p class="source-code">                  objective='binary:logistic', </p>
			<p class="source-code">                  eval_metric='auc')</p>
			<p class="source-code">        cls.fit(x_train, y_train)</p>
			<p class="source-code">        auc = cls.score(x_test, y_test)</p>
			<p class="source-code">        mlflow.log_metric('auc', auc)</p>
			<p class="source-code">        mlflow.xgboost.log_model(cls, 'dm-xgboost-model')</p>
			<p class="source-code">        mlflow.end_run()</p>
			<p>The <strong class="source-inline">load_dataset()</strong> function does what its name implies and<a id="_idIndexMarker894"/> logs several parameters:</p>
			<p class="source-code"># load_dataset.py</p>
			<p class="source-code">import mlflow</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">def load_dataset(path, test_size=0.2, random_state=123):</p>
			<p class="source-code">    data = pd.read_csv(path)</p>
			<p class="source-code">    data = pd.get_dummies(data)</p>
			<p class="source-code">    data = data.drop(['y_no'], axis=1)</p>
			<p class="source-code">    x = data.drop(['y_yes'], axis=1)</p>
			<p class="source-code">    y = data['y_yes']</p>
			<p class="source-code">    mlflow.log_param("dataset_path", path)</p>
			<p class="source-code">    mlflow.log_param("dataset_shape", data.shape)</p>
			<p class="source-code">    mlflow.log_param("test_size", test_size)</p>
			<p class="source-code">    mlflow.log_param("random_state", random_state)</p>
			<p class="source-code">    mlflow.log_param("one_hot_encoding", True)</p>
			<p class="source-code">    return train_test_split(x, y, test_size=test_size, </p>
			<p class="source-code">                            random_state=random_state)</p>
			<p>Let's train the model <a id="_idIndexMarker895"/>and visualize its results in the MLflow web application:</p>
			<ol>
				<li value="1">Inside the virtual environment we just created on our local machine, we run the training script just like any Python program:<p class="source-code"><strong class="bold">$ python train-xgboost.py</strong></p><p class="source-code"><strong class="bold">INFO: 'dm-xgboost' does not exist. Creating a new experiment</strong></p><p class="source-code"><strong class="bold">AUC  0.91442097596504</strong></p></li>
				<li>We launch the MLflow web application:<p class="source-code"><strong class="bold">$ mlflow ui &amp;</strong></p></li>
				<li>Pointing our browser at <a href="http://localhost:5000">http://localhost:5000</a>, we see information on our run, as shown in the following screenshot:</li>
			</ol>
			<p> </p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="Images/B17705_08_2.jpg" alt="Figure 8.2 – Viewing our job in MLflow&#13;&#10;" width="1349" height="153"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Viewing our job in MLflow</p>
			<p>The training was successful. Before we can deploy the model on SageMaker, we must build a SageMaker container. As it turns out, it's the simplest thing.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/>Building a SageMaker container with MLflow</h2>
			<p>All it takes is a single command on our local machine:</p>
			<p class="source-code"><strong class="bold">$ mlflow sagemaker build-and-push-container</strong></p>
			<p>MLflow will automatically build a Docker container compatible with SageMaker, with all required dependencies. Then, it creates a <a id="_idIndexMarker896"/>repository in Amazon ECR named <strong class="source-inline">mlflow-pyfunc</strong> and pushes the image to it. Obviously, this requires your AWS credentials <a id="_idIndexMarker897"/>to be properly set up. MLflow will use the default region configured by the AWS CLI.</p>
			<p>Once this command completes, you should see the image in ECR, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="Images/B17705_08_3.jpg" alt="Figure 8.3 – Viewing our container in ECR&#13;&#10;" width="1170" height="512"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Viewing our container in ECR</p>
			<p>Our container is now ready for deployment.</p>
			<h3>Deploying a model locally with MLflow</h3>
			<p>We will deploy our model using the following steps:</p>
			<ol>
				<li value="1">We can deploy <a id="_idIndexMarker898"/>our model locally with a single command, passing its run identifier (visible in the MLflow URL for the run) and the HTTP <a id="_idIndexMarker899"/>port to use. This fires up a local web application based on <strong class="source-inline">gunicorn</strong>:<p class="source-code"><strong class="bold">$ mlflow sagemaker run-local -p 8888 -m runs:/d08ab8383ee84f72a92164d3ca548693/dm-xgboost-model</strong></p><p>You should see something similar to this:</p><p class="source-code"><strong class="bold">[2021-05-26 20:21:23 +0000] [370] [INFO] Starting gunicorn 20.1.0</strong></p><p class="source-code"><strong class="bold">[2021-05-26 20:21:23 +0000] [370] [INFO] Listening at: http://127.0.0.1:8000 (370)</strong></p><p class="source-code"><strong class="bold">[2021-05-26 20:21:23 +0000] [370] [INFO] Using worker: gevent</strong></p><p class="source-code"><strong class="bold">[2021-05-26 20:21:23 +0000] [381] [INFO] Booting worker with pid: 381 </strong></p></li>
				<li>Our prediction code is quite straightforward. We load CSV samples from the dataset, convert them into JSON format, and send them to the endpoint using the <strong class="source-inline">requests</strong> library, a popular Python library for HTTP (<a href="https://requests.readthedocs.io">https://requests.readthedocs.io</a>):<p class="source-code"># predict-xgboost-local.py </p><p class="source-code">import json</p><p class="source-code">import requests</p><p class="source-code">from load_dataset import load_dataset</p><p class="source-code">port = 8888</p><p class="source-code">if __name__ == '__main__':</p><p class="source-code">    x_train, x_test, y_train, y_test = load_dataset(</p><p class="source-code">        'bank-additional/bank-additional-full.csv')</p><p class="source-code">    input_data = x_test[:10].to_json(orient='split')</p><p class="source-code">    endpoint = 'http://localhost:{}/invocations'</p><p class="source-code">               .format(port)</p><p class="source-code">    headers = {'Content-type': 'application/json; </p><p class="source-code">                format=pandas-split'}</p><p class="source-code">    prediction = requests.post(</p><p class="source-code">        endpoint, </p><p class="source-code">        json=json.loads(input_data),</p><p class="source-code">        headers=headers)</p><p class="source-code">    print(prediction.text)</p></li>
				<li>Running this <a id="_idIndexMarker900"/>code in another shell invokes the local <a id="_idIndexMarker901"/>model and prints out predictions:<p class="source-code"><strong class="bold">$ source mlflow-example/bin/activate</strong></p><p class="source-code"><strong class="bold">$ python predict-xgboost-local.py</strong></p><p class="source-code"><strong class="bold">[0.00046298891538754106, 0.10499032586812973, . . . </strong></p></li>
				<li>When we're done, we terminate the local server with <em class="italic">Ctrl</em> + <em class="italic">C</em>.</li>
			</ol>
			<p>Now that we're confident that our model works locally, we can deploy it on SageMaker.</p>
			<h3>Deploying a model on SageMaker with MLflow</h3>
			<p>This is a one-liner again:</p>
			<ol>
				<li value="1">We need to pass an application name, the model path, and the name <a id="_idIndexMarker902"/>of the SageMaker role. You can <a id="_idIndexMarker903"/>use the same role you've used in previous chapters:<p class="source-code"><strong class="bold">$ mlflow sagemaker deploy \</strong></p><p class="source-code"><strong class="bold">--region-name eu-west-1 \</strong></p><p class="source-code"><strong class="bold">-t ml.t2.medium \</strong></p><p class="source-code"><strong class="bold">-a mlflow-xgb-demo \</strong></p><p class="source-code"><strong class="bold">-m runs:/d08ab8383ee84f72a92164d3ca548693/dm-xgboost-model \</strong></p><p class="source-code"><strong class="bold">-e arn:aws:iam::123456789012:role/Sagemaker-fullaccess</strong></p></li>
				<li>After a <a id="_idIndexMarker904"/>few minutes, the endpoint is in service. We invoke it with <a id="_idIndexMarker905"/>the following code. It loads the test dataset and sends the first 10 samples in JSON format to the endpoint named after our application:<p class="source-code"># predict-xgboost.py </p><p class="source-code">import boto3</p><p class="source-code">from load_dataset import load_dataset</p><p class="source-code">app_name = 'mlflow-xgb-demo'</p><p class="source-code">region = 'eu-west-1'</p><p class="source-code">if __name__ == '__main__':</p><p class="source-code">    sm = boto3.client('sagemaker', region_name=region)</p><p class="source-code">    smrt = boto3.client('runtime.sagemaker', </p><p class="source-code">                        region_name=region)</p><p class="source-code">    endpoint = sm.describe_endpoint(</p><p class="source-code">              EndpointName=app_name)</p><p class="source-code">    print("Status: ", endpoint['EndpointStatus'])</p><p class="source-code">    x_train, x_test, y_train, y_test = load_dataset(</p><p class="source-code">        'bank-additional/bank-additional-full.csv')</p><p class="source-code">    input_data = x_test[:10].to_json(orient="split")</p><p class="source-code">    prediction = smrt.invoke_endpoint(</p><p class="source-code">        EndpointName=app_name,</p><p class="source-code">        Body=input_data,</p><p class="source-code">        ContentType='application/json;</p><p class="source-code">                     format=pandas-split')</p><p class="source-code">    prediction = prediction['Body']</p><p class="source-code">                 .read().decode("ascii")</p><p class="source-code">    print(prediction)</p><p>Wait a minute! We are not using the SageMaker SDK. What's going on here?</p><p>In this example, we're dealing with an existing endpoint, not an endpoint that we created <a id="_idIndexMarker906"/>by fitting an estimator and deploying a predictor. </p><p>We could still rebuild a predictor using the SageMaker SDK, as we'll see in <a href="B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Deploying Machine Learning Models</em>. Instead, we use our good old friend <strong class="source-inline">boto3</strong>, the AWS SDK for Python. We first invoke the <strong class="source-inline">describe_endpoint()</strong> API to check that the endpoint is in service. Then, we use the <strong class="source-inline">invoke_endpoint()</strong> API to…invoke the endpoint! For now, we don't need to know more. </p><p>We run the prediction code on our local machine, and it produces the following output:</p><p class="source-code"><strong class="bold">$ python3 predict-xgboost.py</strong></p><p class="source-code"><strong class="bold">Status:  InService</strong></p><p class="source-code"><strong class="bold">[0.00046298891538754106, 0.10499032586812973, 0.016391035169363022, . . .</strong></p></li>
				<li>When we're done, we delete the endpoint with the MLflow CLI. This cleans up all <a id="_idIndexMarker907"/>resources created for deployment:<p class="source-code"><strong class="bold">$ mlflow sagemaker delete -a mlflow-xgb-demo –region-name eu-west-1</strong></p></li>
			</ol>
			<p>The development <a id="_idIndexMarker908"/>experience with MLflow is <a id="_idIndexMarker909"/>pretty simple. It also has plenty of other features you may want to explore. </p>
			<p>So far, we've run <a id="_idIndexMarker910"/>examples for training and prediction. There's another area of SageMaker that lets us use custom containers, <strong class="bold">SageMaker Processing</strong>, which we studied in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>. To close this chapter, let's build a custom Python container for SageMaker Processing.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor165"/>Building a fully custom container for SageMaker Processing</h1>
			<p>We'll reuse the news headlines example from <a href="B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 6</em></a>, <em class="italic">Training Natural Processing Models</em>:</p>
			<ol>
				<li value="1">We start <a id="_idIndexMarker911"/>with a Dockerfile <a id="_idIndexMarker912"/>based on a minimal Python image. We install dependencies, add our processing script, and define it as our entry point:<p class="source-code">FROM python:3.7-slim</p><p class="source-code">RUN pip3 install --no-cache gensim nltk sagemaker</p><p class="source-code">RUN python3 -m nltk.downloader stopwords wordnet</p><p class="source-code">ADD preprocessing-lda-ntm.py /</p><p class="source-code">ENTRYPOINT ["python3", "/preprocessing-lda-ntm.py"]</p></li>
				<li>We build the image and tag it as <strong class="source-inline">sm-processing-custom:latest</strong>:<p class="source-code"><strong class="bold">$ docker build -t sm-processing-custom:latest -f Dockerfile .</strong></p><p>The resulting image is 497 MB. For comparison, it's 1.2 GB if we start from <strong class="source-inline">python:3.7</strong> instead of <strong class="source-inline">python:3.7-slim</strong>. This makes it faster to push and download.</p></li>
				<li>Using the AWS CLI, we create a repository in Amazon ECR to host this image, and we log in to the repository:<p class="source-code"><strong class="bold">$ aws ecr create-repository --repository-name sm-processing-custom --region eu-west-1</strong></p><p class="source-code"><strong class="bold">$ aws ecr get-login-password | docker login --username AWS --password-stdin 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sm-processing-custom:latest</strong></p></li>
				<li>Using the <a id="_idIndexMarker913"/>image identifier, we tag the image with the repository identifier:<p class="source-code"><strong class="bold">$ docker tag &lt;IMAGE_ID&gt; 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sm-processing-custom:latest</strong></p></li>
				<li>We push <a id="_idIndexMarker914"/>the image to the repository:<p class="source-code"><strong class="bold">$ docker push 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sm-processing-custom:latest</strong></p></li>
				<li>Moving to a Jupyter notebook, we configure a generic <strong class="source-inline">Processor</strong> object with our new container, which is the equivalent of the generic <strong class="source-inline">Estimator</strong> module we used for training. Accordingly, no <strong class="source-inline">framework_version</strong> parameter is required:<p class="source-code">from sagemaker.processing import Processor</p><p class="source-code">sklearn_processor = Processor( </p><p class="source-code">    image_uri='123456789012.dkr.ecr.eu-west-1.amazonaws.com/sm-processing-custom:latest',</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    instance_type='ml.c5.2xlarge',</p><p class="source-code">    instance_count=1)</p></li>
				<li>Using the same <strong class="source-inline">ProcessingInput</strong> and <strong class="source-inline">ProcessingOutput</strong> objects, we run the<a id="_idIndexMarker915"/> processing job. As<a id="_idIndexMarker916"/> our processing code is now stored inside the container, we don't need to pass a <strong class="source-inline">code</strong> parameter as we did with <strong class="source-inline">SKLearnProcessor</strong>:<p class="source-code">from sagemaker.processing import ProcessingInput, ProcessingOutput</p><p class="source-code">sklearn_processor.run(</p><p class="source-code">    inputs=[</p><p class="source-code">        ProcessingInput(</p><p class="source-code">            source=input_data,</p><p class="source-code">            destination='/opt/ml/processing/input')</p><p class="source-code">    ],</p><p class="source-code">    outputs=[</p><p class="source-code">        ProcessingOutput(</p><p class="source-code">            output_name='train_data',</p><p class="source-code">            source='/opt/ml/processing/train/')</p><p class="source-code">    ],</p><p class="source-code">    arguments=[</p><p class="source-code">        '--filename', 'abcnews-date-text.csv.gz'</p><p class="source-code">    ]</p><p class="source-code">)</p></li>
				<li>Once the training job is complete, we can fetch its outputs in S3.</li>
			</ol>
			<p>This concludes our exploration of custom containers in SageMaker. As you can see, you can pretty much run anything as long as it fits inside a Docker container.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor166"/>Summary</h1>
			<p>Built-in frameworks are extremely useful, but sometimes you need something a little—or very—different. Whether starting from built-in containers or from scratch, SageMaker lets you build your training and deployment containers exactly the way you want them. Freedom for all!</p>
			<p>In this chapter, you learned how to customize Python and R containers for data processing, training, and deployment. You saw how you could use them with the SageMaker SDK and its usual workflow. You also learned about MLflow, a nice open source tool that lets you train and deploy models using a CLI.</p>
			<p>This concludes our extensive coverage of modeling options in SageMaker: built-in algorithms, built-in frameworks, and custom code. In the next chapter, you'll learn about SageMaker features that help you to scale your training jobs.</p>
		</div>
	</div></body></html>
["```py\n>>> import numpy as np\n>>> from sklearn.impute import SimpleImputer \n```", "```py\n>>> data_origin = [[30, 100],\n...                [20, 50],\n...                [35, np.nan],\n...                [25, 80],\n...                [30, 70],\n...                [40, 60]] \n```", "```py\n>>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n>>> imp_mean.fit(data_origin) \n```", "```py\n>>> data_mean_imp = imp_mean.transform(data_origin)\n>>> print(data_mean_imp)\n[[ 30\\. 100.]\n [ 20\\. 50.]\n [ 35\\. 72.]\n [ 25\\. 80.]\n [ 30\\. 70.]\n [ 40\\. 60.]] \n```", "```py\n>>> imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n>>> imp_median.fit(data_origin)\n>>> data_median_imp = imp_median.transform(data_origin)\n>>> print(data_median_imp)\n[[ 30\\. 100.]\n [ 20\\. 50.]\n [ 35\\. 70.]\n [ 25\\. 80.]\n [ 30\\. 70.]\n [ 40\\. 60.]] \n```", "```py\n>>> new = [[20, np.nan],\n...        [30, np.nan],\n...        [np.nan, 70],\n...        [np.nan, np.nan]]\n>>> new_mean_imp = imp_mean.transform(new)\n>>> print(new_mean_imp)\n[[ 20\\. 72.]\n [ 30\\. 72.]\n [ 30\\. 70.]\n [ 30\\. 72.]] \n```", "```py\n    >>> from sklearn import datasets\n    >>> dataset = datasets.load_diabetes()\n    >>> X_full, y = dataset.data, dataset.target \n    ```", "```py\n    >>> m, n = X_full.shape\n    >>> m_missing = int(m * 0.25)\n    >>> print(m, m_missing)\n    442 110 \n    ```", "```py\n    >>> np.random.seed(42)\n    >>> missing_samples = np.array([True] * m_missing + [False] * (m - m_missing))\n    >>> np.random.shuffle(missing_samples) \n    ```", "```py\n    >>> missing_features = np.random.randint(low=0, high=n, size=m_missing) \n    ```", "```py\n    >>> X_missing = X_full.copy()\n    >>> X_missing[np.where(missing_samples)[0], missing_features] = np.nan \n    ```", "```py\n    >>> X_rm_missing = X_missing[~missing_samples, :]\n    >>> y_rm_missing = y[~missing_samples] \n    ```", "```py\n    >>> from sklearn.ensemble import RandomForestRegressor\n    >>> from sklearn.model_selection import cross_val_score\n    >>> regressor = RandomForestRegressor(random_state=42,\n                                      max_depth=10, n_estimators=100)\n    >>> score_rm_missing = cross_val_score(regressor,X_rm_missing,\n                                              y_rm_missing).mean()\n    >>> print(f'Score with the data set with missing samples removed: {score_rm_missing:.2f}')\n    Score with the data set with missing samples removed: 0.38 \n    ```", "```py\n    >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    >>> X_mean_imp = imp_mean.fit_transform(X_missing) \n    ```", "```py\n    >>> regressor = RandomForestRegressor(random_state=42,\n                                          max_depth=10,\n                                          n_estimators=100)\n    >>> score_mean_imp = cross_val_score(regressor, X_mean_imp, y).mean()\n    >>> print(f'Score with the data set with missing values replaced by mean: {score_mean_imp:.2f}')\n    Score with the data set with missing values replaced by mean: 0.41 \n    ```", "```py\n    >>> regressor = RandomForestRegressor(random_state=42,\n                                          max_depth=10,\n                                          n_estimators=500)\n    >>> score_full = cross_val_score(regressor, X_full, y).mean()\n    >>> print(f'Score with the full data set: {score_full:.2f}')\n    Score with the full data set: 0.42 \n    ```", "```py\n    >>> from sklearn.datasets import load_digits\n    >>> dataset = load_digits()\n    >>> X, y = dataset.data, dataset.target\n    >>> print(X.shape)\n    (1797, 64) \n    ```", "```py\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.model_selection import cross_val_score\n    >>> classifier = SVC(gamma=0.005, random_state=42)\n    >>> score = cross_val_score(classifier, X, y).mean()\n    >>> print(f'Score with the original data set: {score:.2f}')\n    Score with the original data set: 0.90 \n    ```", "```py\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> random_forest = RandomForestClassifier(n_estimators=100,\n                                               criterion='gini',\n                                               n_jobs=-1,\n                                               random_state=42)\n    >>> random_forest.fit(X, y)\n    >>> feature_sorted = np.argsort(random_forest.feature_importances_) \n    ```", "```py\n    >>> K = [10, 15, 25, 35, 45]\n    >>> for k in K:\n    ...     top_K_features = feature_sorted[-k:]\n    ...     X_k_selected = X[:, top_K_features]\n    ...     # Estimate accuracy on the data set with k\n              selected features\n    ...     classifier = SVC(gamma=0.005)\n    ...     score_k_features = cross_val_score(classifier,\n                                  X_k_selected, y).mean()\n    ...     print(f'Score with the dataset of top {k} features:\n                  {score_k_features:.2f}')\n    ...\n    Score with the dataset of top 10 features: 0.86\n    Score with the dataset of top 15 features: 0.92\n    Score with the dataset of top 25 features: 0.95\n    Score with the dataset of top 35 features: 0.93\n    Score with the dataset of top 45 features: 0.90 \n    ```", "```py\n>>> from sklearn.decomposition import PCA\n>>> # Keep different number of top components\n>>> N = [10, 15, 25, 35, 45]\n>>> for n in N:\n...     pca = PCA(n_components=n)\n...     X_n_kept = pca.fit_transform(X)\n...     # Estimate accuracy on the data set with top n components\n...     classifier = SVC(gamma=0.005)\n...     score_n_components =\n                   cross_val_score(classifier, X_n_kept, y).mean()\n...     print(f'Score with the dataset of top {n} components: \n                                    {score_n_components:.2f}')\nScore with the dataset of top 10 components: 0.94\nScore with the dataset of top 15 components: 0.95\nScore with the dataset of top 25 components: 0.93\nScore with the dataset of top 35 components: 0.91\nScore with the dataset of top 45 components: 0.90 \n```", "```py\n>>> from sklearn.preprocessing import Binarizer\n>>> X = [[4], [1], [3], [0]]\n>>> binarizer = Binarizer(threshold=2.9)\n>>> X_new = binarizer.fit_transform(X)\n>>> print(X_new)\n[[1]\n [0]\n [1]\n [0]] \n```", "```py\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> X = [[2, 4],\n...      [1, 3],\n...      [3, 2],\n...      [0, 3]]\n>>> poly = PolynomialFeatures(degree=2)\n>>> X_new = poly.fit_transform(X)\n>>> print(X_new)\n[[ 1\\. 2\\. 4\\. 4\\. 8\\. 16.]\n [ 1\\. 1\\. 3\\. 1\\. 3\\. 9.]\n [ 1\\. 3\\. 2\\. 9\\. 6\\. 4.]\n [ 1\\. 0\\. 3\\. 0\\. 0\\. 9.]] \n```", "```py\n>>> from gensim.models import Word2Vec \n```", "```py\n>>> sentences = [\n    [\"i\", \"love\", \"machine\", \"learning\", \"by\", \"example\"],\n    [\"machine\", \"learning\", \"and\", \"deep\", \"learning\", \"are\", \"fascinating\"],\n    [\"word\", \"embedding\", \"is\", \"essential\", \"for\", \"many\", \"nlp\", \"tasks\"],\n    [\"word2vec\", \"produces\", \"word\", \"embeddings\"]\n] \n```", "```py\n>>> model = Word2Vec(sentences=sentences, vector_size=100, window=5,\n                     min_count=1, sg=0) \n```", "```py\n>>> vector = model.wv[\"machine\"]\n>>> print(\"Vector for 'machine':\", vector)\nVector for 'machine': [ 9.2815855e-05  3.0779743e-03 -6.8117767e-03 -1.3753572e-03 7.6693585e-03  7.3465472e-03 -3.6724545e-03  2.6435424e-03 -8.3174659e-03  6.2051434e-03 -4.6373457e-03 -3.1652437e-03 9.3113342e-03  8.7273103e-04  7.4911476e-03 -6.0739564e-03 5.1591368e-03  9.9220201e-03 -8.4587047e-03 -5.1362212e-03 -7.0644980e-03 -4.8613679e-03 -3.7768795e-03 -8.5355258e-03 7.9550967e-03 -4.8430962e-03  8.4243221e-03  5.2609886e-03 -6.5501807e-03  3.9575580e-03  5.4708594e-03 -7.4282014e-03 -7.4055856e-03 -2.4756377e-03 -8.6252270e-03 -1.5801827e-03 -4.0236043e-04  3.3001360e-03  1.4415972e-03 -8.8241365e-04 -5.5940133e-03  1.7302597e-03 -8.9826871e-04  6.7939684e-03 3.9741215e-03  4.5290575e-03  1.4341431e-03 -2.6994087e-03 -4.3666936e-03 -1.0321270e-03  1.4369689e-03 -2.6467817e-03 -7.0735654e-03 -7.8056543e-03 -9.1217076e-03 -5.9348154e-03 -1.8470082e-03 -4.3242811e-03 -6.4605214e-03 -3.7180765e-03 4.2892280e-03 -3.7388816e-03  8.3797537e-03  1.5337169e-03 -7.2427099e-03  9.4338059e-03  7.6304432e-03  5.4950463e-03 -6.8496312e-03  5.8225882e-03  4.0093577e-03  5.1861661e-03 4.2569390e-03  1.9407619e-03 -3.1710821e-03  8.3530620e-03 9.6114948e-03  3.7916750e-03 -2.8375010e-03  6.6632601e-06 1.2186278e-03 -8.4594022e-03 -8.2233679e-03 -2.3177716e-04 1.2370384e-03 -5.7435711e-03 -4.7256653e-03 -7.3463405e-03 8.3279097e-03  1.2112247e-04 -4.5090448e-03  5.7024667e-03 9.1806483e-03 -4.0998533e-03  7.9661217e-03  5.3769764e-03 5.8786790e-03  5.1239668e-04  8.2131373e-03 -7.0198057e-03] \n```", "```py\n>>> import torch\n>>> import torch.nn as nn\n>>> input_data = torch.LongTensor([[1, 2, 3, 4], [5, 1, 6, 3]])\n>>> # Define the embedding layer\n>>> vocab_size = 10  # Total number of unique words\n>>> embedding_dim = 3  # Dimensionality of the embeddings\n>>> embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n...\n>>> embedded_data = embedding_layer(input_data) \n>>> print(\"Embedded Data:\\n\", embedded_data)\nEmbedded Data:\ntensor([[[-1.2462,  0.4035,  0.4463],\n        [-0.5218,  0.8302, -0.6920],\n        [-0.4720, -1.2894,  1.0763],\n        [-2.2879, -0.4834,  0.3416]],\n       [[ 1.5886, -0.3489, -0.4579],\n        [-1.2462,  0.4035,  0.4463],\n        [-1.2322, -0.5981, -0.1349],\n        [-0.4720, -1.2894,  1.0763]]], grad_fn=<EmbeddingBackward0>) \n```", "```py\n>>> dataset = datasets.load_diabetes()\n>>> X, y = dataset.data, dataset.target\n>>> num_new = 30 # the last 30 samples as new data set\n>>> X_train = X[:-num_new, :]\n>>> y_train = y[:-num_new]\n>>> X_new = X[-num_new:, :]\n>>> y_new = y[-num_new:] \n```", "```py\n>>> from sklearn.preprocessing import StandardScaler\n>>> scaler = StandardScaler()\n>>> scaler.fit(X_train) \n```", "```py\n>>> import pickle\n>>> pickle.dump(scaler, open(\"scaler.p\", \"wb\" )) \n```", "```py\n>>> X_scaled_train = scaler.transform(X_train)\n>>> from sklearn.svm import SVR\n>>> regressor = SVR(C=20)\n>>> regressor.fit(X_scaled_train, y_train) \n```", "```py\n>>> pickle.dump(regressor, open(\"regressor.p\", \"wb\")) \n```", "```py\n>>> my_scaler = pickle.load(open(\"scaler.p\", \"rb\" ))\n>>> my_regressor = pickle.load(open(\"regressor.p\", \"rb\")) \n```", "```py\n>>> X_scaled_new = my_scaler.transform(X_new)\n>>> predictions = my_regressor.predict(X_scaled_new) \n```", "```py\n    >>> import tensorflow as tf\n    >>> from tensorflow import keras\n    >>> from sklearn import datasets\n    >>> cancer_data = datasets.load_breast_cancer()\n    >>> X = cancer_data.data\n    >>> X = scaler.fit_transform(X)\n    >>> y = cancer_data.target \n    ```", "```py\n    >>> learning_rate = 0.005\n    >>> n_iter = 10\n    >>> tf.random.set_seed(42)\n    >>> model = keras.Sequential([\n    ...     keras.layers.Dense(units=1, activation='sigmoid')\n    ... ])\n    >>> model.compile(loss='binary_crossentropy',\n    ...         optimizer=tf.keras.optimizers.Adam(learning_rate)) \n    ```", "```py\n    >>> model.fit(X, y, epochs=n_iter)\n    Epoch 1/10 \n    18/18 [==============================] - 0s 943us/step - loss: 0.2288 \n    Epoch 2/10 \n    18/18 [==============================] - 0s 914us/step - loss: 0.1591 \n    Epoch 3/10 \n    18/18 [==============================] - 0s 825us/step - loss: 0.1303 \n    Epoch 4/10 \n    18/18 [==============================] - 0s 865us/step - loss: 0.1147 \n    Epoch 5/10 \n    18/18 [==============================] - 0s 795us/step - loss: 0.1042 \n    Epoch 6/10 \n    18/18 [==============================] - 0s 796us/step - loss: 0.0971 \n    Epoch 7/10 \n    18/18 [==============================] - 0s 862us/step - loss: 0.0917 \n    Epoch 8/10 \n    18/18 [==============================] - 0s 913us/step - loss: 0.0871 \n    Epoch 9/10 \n    18/18 [==============================] - 0s 795us/step - loss: 0.0835 \n    Epoch 10/10 \n    18/18 [==============================] - 0s 767us/step - loss: 0.0806 \n    ```", "```py\n    >>> model.summary()\n    Model: \"sequential\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #\n    =================================================================\n    dense (Dense)                multiple                  31\n    =================================================================\n    Total params: 31\n    Trainable params: 31\n    Non-trainable params: 0\n    _________________________________________________________ \n    ```", "```py\n    >>> path = './model_tf'\n    >>> model.save(path) \n    ```", "```py\n    >>> new_model = tf.keras.models.load_model(path)\n    >>> new_model.summary()\n    Model: \"sequential\"\n    _________________________________________________________\n    Layer (type)                 Output Shape              Param #\n    =========================================================\n    dense (Dense)                multiple                  31\n    =========================================================\n    Total params: 31\n    Trainable params: 31\n    Non-trainable params: 0\n    _________________________________________________________ \n    ```", "```py\n    >>> X_torch = torch.FloatTensor(X)\n    >>> y_torch = torch.FloatTensor(y.reshape(y.shape[0], 1)) \n    ```", "```py\n    >>> torch.manual_seed(42)\n    >>> model = nn.Sequential(nn.Linear(X.shape[1], 1),\n                              nn.Sigmoid())\n    >>> loss_function = nn.BCELoss()\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n    ```", "```py\n    >>> def train_step(model, X_train, y_train, loss_function, optimizer):\n            pred_train = model(X_train)\n            loss = loss_function(pred_train, y_train)\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n        return loss.item()\n    >>> for epoch in range(n_iter):\n            loss = train_step(model, X_torch, y_torch, loss_function, optimizer)\n            print(f\"Epoch {epoch} - loss: {loss}\")\n    Epoch 0 - loss: 0.8387020826339722\n    Epoch 1 - loss: 0.7999904751777649\n    Epoch 2 - loss: 0.76298588514328\n    Epoch 3 - loss: 0.7277476787567139\n    Epoch 4 - loss: 0.6943162679672241\n    Epoch 5 - loss: 0.6627081036567688\n    Epoch 6 - loss: 0.6329135298728943\n    Epoch 7 - loss: 0.6048969030380249\n    Epoch 8 - loss: 0.5786024332046509\n    Epoch 9 - loss: 0.5539639592170715 \n    ```", "```py\n    >>> print(model)\n    Sequential(\n      (0): Linear(in_features=30, out_features=1, bias=True)\n      (1): Sigmoid()\n    ) \n    ```", "```py\n    >>> path = './model.pth '\n    >>> torch.save(model, path) \n    ```", "```py\n    >>> new_model = torch.load(path)\n    >>> print(new_model)\n    Sequential(\n      (0): Linear(in_features=30, out_features=1, bias=True)\n      (1): Sigmoid()\n    ) \n    ```", "```py\n>>> from sklearn.metrics import r2_score\n>>> print(f'Health check on the model, R^2: {r2_score(y_new,\n          predictions):.3f}')\nHealth check on the model, R^2: 0.613 \n```"]
["```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')\n    df.head()\n    ```", "```py\n    df = df[['A', 'LK']]\n    df.head()\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.scatter(df['A'], df['LK'])\n    plt.xlabel('Area of Kernel')\n    plt.ylabel('Length of Kernel')\n    plt.title('Kernel Area versus Length')\n    plt.show()\n    ```", "```py\n    df.mean()\n    ```", "```py\n    A     14.847524\n    LK     5.628533\n    dtype: float64\n    ```", "```py\n    np.mean(df.values, axis=0)\n    ```", "```py\n    array([14.84752381,  5.62853333])\n    ```", "```py\n    df.std()\n    ```", "```py\n    A     2.909699\n    LK    0.443063\n    dtype: float64\n    ```", "```py\n    np.std(df.values, axis=0)\n    ```", "```py\n    array([2.90276331, 0.44200731])\n    ```", "```py\n    df.var()\n    ```", "```py\n    A     8.466351\n    LK    0.196305\n    dtype: float64\n    ```", "```py\n    np.var(df.values, axis=0)\n    ```", "```py\n    array([8.42603482, 0.19537046])\n    ```", "```py\n    df.cov()\n    ```", "```py\n    np.cov(df.values.T)\n    ```", "```py\n    array([[8.46635078, 1.22470367],\n           [1.22470367, 0.19630525]])\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')\n    df.head()\n    ```", "```py\n    df = df[['A', 'LK']]\n    df.head()\n    ```", "```py\n    eigenvalues, eigenvectors = np.linalg.eig(np.cov(df.T))\n    ```", "```py\n    eigenvalues\n    ```", "```py\n    array([8.64390408, 0.01875194])\n    ```", "```py\n    eigenvalues = np.cumsum(eigenvalues)\n    eigenvalues\n    ```", "```py\n    array([8.64390408, 8.66265602])\n    ```", "```py\n    eigenvalues /= eigenvalues.max()\n    eigenvalues\n    ```", "```py\n    array([0.99783531, 1.])\n    ```", "```py\n    eigenvectors\n    ```", "```py\n    array([[ 0.98965371, -0.14347657],\n           [ 0.14347657,  0.98965371]])\n    ```", "```py\n    eigenvectors.shape\n    ```", "```py\n    (2, 2)\n    ```", "```py\n    P = eigenvectors[0]\n    P\n    ```", "```py\n    array([0.98965371, -0.14347657])\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')\n    df.head()\n    ```", "```py\n    df = df[['A', 'LK']]\n    df.head()\n    ```", "```py\n    data = np.cov(df.values.T)\n    \"\"\"\n    The transpose is required to ensure the covariance matrix is \n    based on features, not samples data\n    \"\"\"\n    data\n    ```", "```py\n    array([[8.46635078, 1.22470367],\n           [1.22470367, 0.19630525]])\n    ```", "```py\n    eigenvectors, eigenvalues, _ = np.linalg\\\n                                   .svd(data, full_matrices=False)\n    ```", "```py\n    eigenvalues\n    ```", "```py\n    array([8.64390408, 0.01875194])\n    ```", "```py\n    eigenvectors\n    ```", "```py\n    array([[-0.98965371, -0.14347657],\n           [-0.14347657,  0.98965371]])\n    ```", "```py\n    eigenvalues = np.cumsum(eigenvalues)\n    eigenvalues /= eigenvalues.max()\n    eigenvalues\n    ```", "```py\n    array([0\\. 99783531, 1\\.        ])\n    ```", "```py\n    P = eigenvectors[0]\n    P\n    ```", "```py\n    array([-0.98965371, -0.14347657])\n    ```", "```py\n    x_t_p = P.dot(df.values.T)\n    x_t_p\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.plot(x_t_p)\n    plt.title('Principal Component of Selected Seeds Dataset')\n    plt.xlabel('Sample')\n    plt.ylabel('Component Value')\n    plt.show() \n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')\n    df.head()\n    ```", "```py\n    df = df[['A', 'LK']]\n    df.head()\n    ```", "```py\n    model = PCA()\n    model.fit(df.values)\n    ```", "```py\n    PCA(copy=True, iterated_power='auto', n_components=None, \n        random_state=None, \n        svd_solver='auto', tol=0.0, whiten=False)\n    ```", "```py\n    model.explained_variance_ratio_\n    ```", "```py\n    array([0.99783531, 0.00216469])\n    ```", "```py\n    model.components_\n    ```", "```py\n    array([[0.98965371, 0.14347657]])\n    ```", "```py\n    model = PCA(n_components=1)\n    ```", "```py\n    model.fit(df.values)\n    ```", "```py\n    PCA(copy=True, iterated_power='auto', n_components=1, \n        random_state=None,\n        svd_solver='auto', tol=0.0, whiten=False)\n    ```", "```py\n    model.components_\n    ```", "```py\n    array([[0.98965371, 0.14347657]])\n    ```", "```py\n    data_t = model.fit_transform(df.values)\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.plot(data_t)\n    plt.xlabel('Sample') \n    plt.ylabel('Transformed Data')\n    plt.title('The dataset transformed by the principal component')\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')[['A', 'LK']]\n    df.head()\n    ```", "```py\n    means = np.mean(df.values, axis=0)\n    means\n    ```", "```py\n    array([14.84752381,  5.62853333])\n    ```", "```py\n    data = df.values - means\n    data\n    ```", "```py\n    eigenvectors, eigenvalues, _ = np.linalg.svd(np.cov(data.T), \\\n                                   full_matrices=False)\n    P = eigenvectors[0]\n    P\n    ```", "```py\n    array([-0.98965371, -0.14347657])\n    ```", "```py\n    data_transformed = P.dot(data.T)\n    ```", "```py\n    P = P.reshape((-1, 1))\n    ```", "```py\n    P_transformed = np.linalg.pinv(P)\n    P_transformed\n    ```", "```py\n    array([[-0.98965371, -0.14347657]])\n    ```", "```py\n    data_transformed = data_transformed.reshape((-1, 1))\n    ```", "```py\n    data_restored = data_transformed.dot(P_transformed)\n    data_restored\n    ```", "```py\n    data_restored += means\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.plot(data_restored[:,0], data_restored[:,1], \\\n             linestyle=':', label='PCA restoration')\n    plt.scatter(df['A'], df['LK'], marker='*', label='Original')\n    plt.legend()\n    plt.xlabel('Area of Kernel')\n    plt.ylabel('Length of Kernel')\n    plt.title('Inverse transform after removing variance')\n    plt.show()\n    ```", "```py\n    P = eigenvectors\n    data_transformed = P.dot(data.T)\n    ```", "```py\n    data_transformed = data_transformed.T\n    ```", "```py\n    data_restored = data_transformed.dot(P)\n    data_restored\n    ```", "```py\n    data_restored += means\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.scatter(data_restored[:,0], data_restored[:,1], \\\n                marker='d', label='PCA restoration', c='k')\n    plt.scatter(df['A'], df['LK'], marker='o', \\\n                label='Original', c='#1f77b4')\n    plt.legend()\n    plt.xlabel('Area of Kernel')\n    plt.ylabel('Length of Kernel')\n    plt.title('Inverse transform after removing variance')\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')[['A', 'LK']]\n    df.head()\n    ```", "```py\n    model = PCA(n_components=1)\n    data_p = model.fit_transform(df.values)\n    ```", "```py\n    data = model.inverse_transform(data_p)\n    plt.figure(figsize=(10, 7))\n    plt.plot(data[:,0], data[:,1], linestyle=':', \\\n             label='PCA restoration')\n    plt.scatter(df['A'], df['LK'], marker='*', label='Original')\n    plt.legend()\n    plt.xlabel('Area of Kernel')\n    plt.ylabel('Length of Kernel')\n    plt.title('Inverse transform after removing variance')\n    plt.show()\n    ```", "```py\n    model = PCA()\n    data_p = model.fit_transform(df.values)\n    data = model.inverse_transform(data_p)\n    plt.figure(figsize=(10, 7))\n    plt.scatter(data[:,0], data[:,1], marker='d', \\\n                label='PCA restoration', c='k')\n    plt.scatter(df['A'], df['LK'], marker='o', \\\n                label='Original', c='#1f77b4')\n    plt.legend()\n    plt.xlabel('Area of Kernel')\n    plt.ylabel('Length of Kernel')\n    plt.title('Inverse transform after removing variance')\n    plt.show()\n    ```", "```py\n    from mpl_toolkits.mplot3d import Axes3D\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')[['A', 'LK', 'C']]\n    df.head()\n    ```", "```py\n    fig = plt.figure(figsize=(10, 7))\n    # Where Axes3D is required\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(df['A'], df['LK'], df['C'])\n    ax.set_xlabel('Area of Kernel')\n    ax.set_ylabel('Length of Kernel')\n    ax.set_zlabel('Compactness of Kernel')\n    ax.set_title('Expanded Seeds Dataset')\n    plt.show()\n    ```", "```py\n    fig = plt.figure(figsize=(10, 14))\n    # Original Data\n    ax = fig.add_subplot(211, projection='3d')\n    # Transformed Data\n    ax = fig.add_subplot(212, projection='3d')\n    ```"]
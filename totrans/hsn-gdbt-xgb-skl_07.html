<html><head></head><body>
		<div id="_idContainer127">
			<h1 id="_idParaDest-110"><em class="italic"><a id="_idTextAnchor117"/>Chapter 5</em>: XGBoost Unveiled</h1>
			<p>In this chapter, you will finally see <strong class="bold">Extreme Gradient Boosting</strong>, or <strong class="bold">XGBoost</strong>, as it is. XGBoost is presented in the context of the machine learning narrative that we have built up, from decision trees to gradient boosting. The first half of the chapter focuses on the theory behind the distinct advancements that XGBoost brings to tree ensemble algorithms. The second half focuses on building XGBoost models within the <strong class="bold">Higgs Boson Kaggle Competition</strong>, which unveiled XGBoost to the world.</p>
			<p>Specifically, you will identify speed enhancements that make XGBoost faster, discover how XGBoost handles missing values, and learn the mathematical derivation behind XGBoost's <strong class="bold">regularized parameter selection</strong>. You will establish model templates for building XGBoost classifiers and regressors. Finally, you will look at the <strong class="bold">Large Hadron Collider</strong>, where the Higgs boson was discovered, where you will weigh data and make predictions using the original XGBoost Python API.</p>
			<p>This chapter covers the following main topics:</p>
			<ul>
				<li><p>Designing XGBoost</p></li>
				<li><p>Analyzing XGBoost parameters</p></li>
				<li><p>Building XGBoost models</p></li>
				<li><p>Finding the Higgs boson – case study</p></li>
			</ul>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor118"/>Designing XGBoost</h1>
			<p>XGBoost is a significant <a id="_idIndexMarker283"/>upgrade from gradient boosting. In this section, you will identify the key features of XGBoost that distinguish it from gradient boosting and other tree ensemble algorithms.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor119"/>Historical narrative</h2>
			<p>With the acceleration of big data, the quest to find awesome machine learning algorithms to <a id="_idIndexMarker284"/>produce accurate, optimal predictions began. Decision trees produced machine learning models that were too accurate and failed to generalize well to new data. Ensemble methods proved more effective by combining many decision trees via <strong class="bold">bagging</strong> and <strong class="bold">boosting</strong>. A leading algorithm that emerged from the tree ensemble trajectory was gradient boosting.</p>
			<p>The consistency, power, and outstanding results of gradient boosting convinced Tianqi Chen from the University of Washington to enhance its capabilities. He called the new algorithm XGBoost, short for <strong class="bold">Extreme Gradient Boosting</strong>. Chen's new form of gradient boosting included built-in regularization and impressive gains in speed.</p>
			<p>After finding initial success in Kaggle competitions, in 2016, Tianqi Chen and Carlos Guestrin authored <em class="italic">XGBoost: A Scalable Tree Boosting System</em> to present their algorithm to the larger machine learning community. You can check out the original paper at <a href="https://arxiv.org/pdf/1603.02754.pdf">https://arxiv.org/pdf/1603.02754.pdf</a>. The key points are summarized in the following section.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor120"/>Design features</h2>
			<p>As indicated in <a href="B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a>, <em class="italic">From Gradient Boosting to XGBoost</em>, the need for faster algorithms is <a id="_idIndexMarker285"/>evident when dealing with big data. The <em class="italic">Extreme</em> in <em class="italic">Extreme Gradient Boosting</em> means pushing computational limits to the extreme. Pushing computational limits requires knowledge not just of model-building but also of disk-reading, compression, cache, and cores.</p>
			<p>Although the focus of this book remains on building XGBoost models, we will take a glance under the hood of the XGBoost algorithm to distinguish key advancements, such as handling missing values, speed gains, and accuracy gains that make XGBoost faster, more accurate, and more desirable. Let's look at these key advancements next.</p>
			<h3>Handling missing values</h3>
			<p>You <a id="_idIndexMarker286"/>spent significant time in <a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Landscape</em>, practicing different ways to correct <strong class="bold">null values</strong>. This is an essential skill for all machine learning practitioners.</p>
			<p>XGBoost, however, is capable of handling missing values for you. There is a <strong class="source-inline">missing</strong> hyperparameter that can be set to any value. When given a missing data point, XGBoost scores different split options and chooses the one with the best results.</p>
			<h3>Gaining speed</h3>
			<p>XGBoost was specifically designed for speed. Speed gains allow machine learning models to <a id="_idIndexMarker287"/>build more quickly which is especially important when dealing with millions, billions, or trillions of rows of data. This is not uncommon in the world of big data, where each day, industry and science accumulate more data than ever before. The following new design features give XGBoost a big edge in speed over comparable ensemble algorithms:</p>
			<ul>
				<li><p><strong class="bold">Approximate split-finding algorithm</strong></p></li>
				<li><p><strong class="bold">Sparsity aware split-finding</strong></p></li>
				<li><p><strong class="bold">Parallel computing</strong></p></li>
				<li><p><strong class="bold">Cache-aware access</strong></p></li>
				<li><p><strong class="bold">Block compression and sharding</strong></p></li>
			</ul>
			<p>Let's learn about these features in a bit more detail.</p>
			<h4>Approximate split-finding algorithm</h4>
			<p>Decision <a id="_idIndexMarker288"/>trees need optimal splits to produce optimal results. A <em class="italic">greedy algorithm</em> selects the best split at each step and does not backtrack to look at previous branches. Note that decision tree splitting is usually performed in a greedy manner.</p>
			<p>XGBoost presents an exact greedy algorithm in addition to a new approximate split-finding algorithm. The <a id="_idIndexMarker289"/>split-finding algorithm uses <strong class="bold">quantiles</strong>, percentages that split data, to propose candidate splits. In a global proposal, the same quantiles are used throughout the entire training, and in a local proposal, new quantiles are provided for each round of splitting.</p>
			<p>A previously known <a id="_idIndexMarker290"/>algorithm, <strong class="bold">quantile sketch</strong>, works well with equally weighted datasets. XGBoost presents a novel weighted quantile sketch based on merging and pruning with a theoretical guarantee. Although the mathematical details of this algorithm are beyond the scope of this book, you are encouraged to check out the appendix of the original XGBoost paper at <a href="https://arxiv.org/pdf/1603.02754.pdf">https://arxiv.org/pdf/1603.02754.pdf</a>.</p>
			<h4>Sparsity-aware split finding</h4>
			<p><strong class="bold">Sparse</strong> data occurs <a id="_idIndexMarker291"/>when the majority of entries are 0 or null. This may occur when datasets consist primarily of null values or when they have been <strong class="bold">one-hot encoded</strong>. In <a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Landscape</em>, you used <strong class="source-inline">pd.get_dummies</strong> to transform <strong class="bold">categorical columns</strong> into <strong class="bold">numerical columns</strong>. This resulted in a larger dataset with many values of 0. This method of converting categorical columns into numerical columns, where 1 indicates presence and 0 indicates absence, is generally referred to as one-hot e<a id="_idTextAnchor121"/>ncoding. You will gain practice with one-hot-encoding in <a href="B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230"><em class="italic">Chapter 10</em></a>, <em class="italic">XGBoost Model Deployment</em>.</p>
			<p>Sparse matrices are <a id="_idIndexMarker292"/>designed to only store data points with non-zero and non-null values. This saves valuable space. A sparsity-aware split indicates that when looking for splits, XGBoost is faster because its matrices are sparse.</p>
			<p>According to the original paper, <em class="italic">XGBoost: A Scalable Tree Boosting System</em>, the sparsity-aware split-finding algorithm performed 50 times faster than the standard approach on the <strong class="bold">All-State-10K</strong> dataset.</p>
			<h4>Parallel computing</h4>
			<p>Boosting <a id="_idIndexMarker293"/>is not ideal for <strong class="bold">parallel computing</strong> since each tree depends on the results of the previous tree. There are opportunities, however, where parallelization may take place.</p>
			<p>Parallel computing occurs <a id="_idIndexMarker294"/>when multiple computational units are working together on the same problem at the same time. XGBoost sorts and compresses the data into blocks. These blocks may be distributed to multiple machines, or to external memory (out of core).</p>
			<p>Sorting the data is faster with blocks. The split-finding algorithm takes advantage of blocks and the search for quantiles is faster due to blocks. In each of these cases, XGBoost provides parallel computing to expedite the model-building process.</p>
			<h4>Cache-aware access</h4>
			<p>The <a id="_idIndexMarker295"/>data on your computer is separated into <strong class="bold">cache</strong> and <strong class="bold">main memory</strong>. The cache, what you use most often, is reserved for high-speed memory. The data that you use less often is held back for lower-speed memory. Different cache levels have different orders of magnitude of latency, as outlined here: <a href="https://gist.github.com/jboner/2841832">https://gist.github.com/jboner/2841832</a>.</p>
			<p>When it comes to gradient statistics, XGBoost uses <strong class="bold">cache-aware prefetching</strong>. XGBoost allocates an internal buffer, fetches the gradient statistics, and performs accumulation with mini batches. According to <em class="italic">XGBoost: A Scalable Tree Boosting System</em>, prefetching lengthens read/write dependency and reduces runtimes by approximately 50% for datasets with a large number of rows.</p>
			<h4>Block compression and sharding</h4>
			<p>XGBoost <a id="_idIndexMarker296"/>delivers additional <a id="_idIndexMarker297"/>speed gains through <strong class="bold">block compression</strong> and <strong class="bold">block sharding</strong>.</p>
			<p>Block compression <a id="_idIndexMarker298"/>helps with computationally expensive disk reading by compressing columns. Block <a id="_idIndexMarker299"/>sharding decreases read times by sharding the data into multiple disks that alternate when reading the data.</p>
			<h3>Accuracy gains </h3>
			<p>XGBoost <a id="_idIndexMarker300"/>adds built-in regularization to achieve accuracy gains beyond <a id="_idIndexMarker301"/>gradient boosting. <strong class="bold">Regularization</strong> is the process of adding information to reduce variance and prevent overfitting.</p>
			<p>Although data may be regularized through hyperparameter fine-tuning, regularized algorithms may also be attempted. For example, <strong class="source-inline">Ridge</strong> and <strong class="source-inline">Lasso</strong> are regularized machine learning alternatives to <strong class="source-inline">LinearRegression</strong>.</p>
			<p>XGBoost includes regularization as part of the learning objective, as contrasted with gradient boosting and random forests. The regularized parameters penalize complexity and smooth out the final weights to prevent overfitting. XGBoost is a regularized version of gradient boosting.</p>
			<p>In the next section, you will meet the math behind the learning objective of XGBoost, which <a id="_idIndexMarker302"/>combines regularization with the loss function. While you don't need to know the math to use XGBoost effectively, mathematical knowledge may provide a deeper understanding. You can skip the next section if desired.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor122"/>Analyzing XGBoost parameters</h1>
			<p>In this section, we <a id="_idIndexMarker303"/>will analyze the parameters that XGBoost uses to create state-of-the-art machine learning models with a mathematical derivation.</p>
			<p>We will maintain the distinction between parameters and hyperparameters as presented in <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>, <em class="italic">Decision Trees in Depth</em>. Hyperparameters are chosen before the model is trained, whereas parameters are chosen while the model is being trained. In other words, the parameters are what the model learns from the data.</p>
			<p>The derivation that follows is taken from the XGBoost official documentation, <em class="italic">Introduction to Boosted Trees</em>, at <a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">https://xgboost.readthedocs.io/en/latest/tutorials/model.html</a>.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor123"/>Learning objective</h2>
			<p>The learning <a id="_idIndexMarker304"/>objective of a machine learning model determines how well the model fits the data. In the case of XGBoost, the learning objective consists of two parts: the <strong class="bold">loss function</strong> and the <strong class="bold">regularization term</strong>.</p>
			<p>Mathematically, XGBoost's learning objective may be defined as follows:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/Formula_05_001.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_05_002.png" alt=""/> is the <a id="_idIndexMarker305"/>loss function, which is the <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>) for regression, or the log loss for classification, and <img src="image/Formula_05_003.png" alt=""/> is the regularization function, a penalty term to prevent over-fitting. Including a regularization term as part of the objective function distinguishes XGBoost from most tree ensembles.</p>
			<p>Let's look at the objective function in more detail, by considering the MSE for regression.</p>
			<h3>Loss function</h3>
			<p>The loss <a id="_idIndexMarker306"/>function, defined as the MSE for regression, can be written in summation notation, as follows:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/Formula_05_004.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_05_005.png" alt=""/> is the target value for the <img src="image/Formula_05_006.png" alt=""/><span class="superscript">th</span> row and <img src="image/Formula_05_007.png" alt=""/> is the value predicted by the machine learning model for the <img src="image/Formula_05_008.png" alt=""/><span class="superscript">th</span> row. The summation symbol, <img src="image/Formula_05_009.png" alt=""/>, indicates that all rows are summed starting with <img src="image/Formula_05_010.png" alt=""/> and ending with <img src="image/Formula_05_011.png" alt=""/>, the number of rows.</p>
			<p>The prediction, <img src="image/Formula_05_012.png" alt=""/>, for a given tree requires a function that starts at the tree root and ends at a leaf. Mathematically, this can be expressed as follows:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/Formula_05_013.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <em class="italic">x</em><span class="subscript">i</span> is a vector whose entries are the columns of the <img src="image/Formula_05_014.png" alt=""/><span class="superscript">th</span> row and <img src="image/Formula_05_015.png" alt=""/> means that the function <img src="image/Formula_05_016.png" alt=""/> is a <a id="_idIndexMarker307"/>member of <img src="image/Formula_05_017.png" alt=""/>, the set of all possible CART functions. <strong class="bold">CART</strong> is an acronym for <strong class="bold">Classification And Regression Trees</strong>. CART provides a real value for all leaves, even for classification algorithms.</p>
			<p>In gradient boosting, the function that determines the prediction for the <img src="image/Formula_05_006.png" alt=""/><span class="superscript">th</span> row includes the sum of all previous functions, as outlined in <a href="B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a>, <em class="italic">From Gradient Boosting to XGBoost</em>. Therefore, it's possible to write the following:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/Formula_05_019.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <em class="italic">T</em> is the number <a id="_idIndexMarker308"/>of boosted trees. In other words, to obtain the prediction for the <img src="image/Formula_05_020.png" alt=""/><span class="superscript">th</span> tree, sum the predictions of the previous trees in addition to the prediction for the new tree. The notation <img src="image/Formula_05_021.png" alt=""/> insists that the functions belong to <img src="image/Formula_05_022.png" alt=""/>, the set of all possible CART functions.</p>
			<p>The learning objective for the <img src="image/Formula_05_023.png" alt=""/><span class="superscript">th</span> boosted tree can now be rewritten as follows:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/Formula_05_024.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_05_025.png" alt=""/> is the general loss function of the <img src="image/Formula_05_026.png" alt=""/><span class="superscript">th</span> boosted tree and <img src="image/Formula_05_027.png" alt=""/> is the regularization <a id="_idIndexMarker309"/>term.</p>
			<p>Since boosted trees sum the predictions of previous trees, in addition to the prediction of the new tree, it must be the case that <img src="image/Formula_05_028.png" alt=""/>. This is the idea behind additive training.</p>
			<p>By substituting this into the preceding learning objective, we obtain the following:</p>
			<p><img src="image/Formula_05_029.png" alt=""/></p>
			<p>This can be rewritten as follows for the least square regression case:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/Formula_05_030.jpg" alt=""/>
				</div>
			</div>
			<p>Multiplying the polynomial out, we obtain the following:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/Formula_05_031.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_05_032.png" alt=""/> is a constant term that does not depend on <img src="image/Formula_05_033.png" alt=""/>. In terms of polynomials, this is a quadratic <a id="_idIndexMarker310"/>equation with the variable <img src="image/Formula_05_034.png" alt=""/>. Recall that the goal is to find an optimal value of <img src="image/Formula_05_035.png" alt=""/>, the optimal function mapping the roots (samples) to the leaves (predictions).</p>
			<p>Any sufficiently smooth function, such as second-degree polynomial (quadratic), can be <a id="_idIndexMarker311"/>approximated by a <strong class="bold">Taylor polynomial</strong>. XGBoost uses Newton's method with a second-degree Taylor polynomial to obtain the following:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/Formula_05_036.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_05_037.png" alt=""/> and <img src="image/Formula_05_038.png" alt=""/> can be written as the following partial derivatives:</p>
			<p><img src="image/Formula_05_039.png" alt=""/></p>
			<p><img src="image/Formula_05_040.png" alt=""/></p>
			<p>For a general discussion of how XGBoost uses the <strong class="bold">Taylor expansion</strong>, check out <a href="https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion">https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion</a>.</p>
			<p>XGBoost implements <a id="_idIndexMarker312"/>this learning objective function by taking a solver that uses only <img src="image/Formula_05_041.png" alt=""/> and <img src="image/Formula_05_038.png" alt=""/> as input. Since the loss function is general, the same inputs can be used for regression and classification.</p>
			<p>This leaves the regularization function, <img src="image/Formula_05_043.png" alt=""/>.</p>
			<h3>Regularization function</h3>
			<p>Let <img src="image/Formula_05_044.png" alt=""/> be the <a id="_idIndexMarker313"/>vector space of leaves. Then, <img src="image/Formula_05_045.png" alt=""/>, the function mapping the tree root to the leaves, can be recast in terms of <img src="image/Formula_05_044.png" alt=""/>, as follows:</p>
			<p><img src="image/Formula_05_047.png" alt=""/></p>
			<p>Here, <em class="italic">q</em> is the function assigning data points to leaves and <em class="italic">T</em> is the number of leaves.</p>
			<p>After practice and experimentation, XGBoost settled on the following as the regularization function where <img src="image/Formula_05_048.png" alt=""/> and <img src="image/Formula_05_049.png" alt=""/> are penalty constants to reduce overfitting:</p>
			<p><img src="image/Formula_05_050.png" alt=""/></p>
			<h3>Objective function</h3>
			<p>Combining the loss <a id="_idIndexMarker314"/>function with the regularization function, the learning objective function becomes the following:</p>
			<p><img src="image/Formula_05_051.png" alt=""/></p>
			<p>We can define the set of indices of data points assigned to the <img src="image/Formula_05_052.png" alt=""/><span class="superscript">th</span> leaf as follows:</p>
			<p><img src="image/Formula_05_053.png" alt=""/></p>
			<p>The objective function can then be written as follows:</p>
			<p><img src="image/Formula_05_054.png" alt=""/></p>
			<p>Finally, setting the <img src="image/Formula_05_055.png" alt=""/> and <img src="image/Formula_05_056.png" alt=""/>, after rearranging the indices and combining like terms, we <a id="_idIndexMarker315"/>obtain the final form of the objective function, which is the following:</p>
			<p><img src="image/Formula_05_057.png" alt=""/></p>
			<p>Minimizing the objective function by taking the derivative with respect to <img src="image/Formula_05_058.png" alt=""/> and setting the left side equal to zero, we obtain the following:</p>
			<p><img src="image/Formula_05_059.png" alt=""/></p>
			<p>This can be substituted back into the objection function to give the following:</p>
			<p><img src="image/Formula_05_060.png" alt=""/></p>
			<p>This is the result XGBoost uses to determine how well the model fits the data.</p>
			<p>Congratulations on making it through a long and challenging derivation!</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor124"/>Building XGBoost models</h1>
			<p>In the first two sections, you <a id="_idIndexMarker316"/>learned how XGBoost works under the hood with parameter derivations, regularization, speed enhancements, and new features such as the <strong class="source-inline">missing</strong> parameter to compensate for null values.</p>
			<p>In this book, we primarily build XGBoost models with scikit-learn. The scikit-learn XGBoost wrapper was released in 2019. Before full immersion with scikit-learn, building XGBoost models required a steeper learning curve. Converting NumPy arrays to <strong class="source-inline">dmatrices</strong>, for instance, was mandatory to take advantage of the XGBoost framework.</p>
			<p>In scikit-learn, however, these conversions happen behind the scenes. Building XGBoost models in scikit-learn is very similar to building other machine learning models in scikit-learn, as you have experienced throughout this book. All standard scikit-learn methods, such as <strong class="source-inline">.fit</strong>, and <strong class="source-inline">.predict</strong>, are available, in addition to essential tools such as <strong class="source-inline">train_test_split</strong>, <strong class="source-inline">cross_val_score</strong>, <strong class="source-inline">GridSearchCV</strong>, and <strong class="source-inline">RandomizedSearchCV</strong>.</p>
			<p>In this section, you will develop templates for building XGBoost models. Going forward, these templates can be referenced as starting points for building XGBoost classifiers and regressors.</p>
			<p>We will build templates for two classic datasets: the <strong class="bold">Iris dataset</strong> for classification and the <strong class="bold">Diabetes dataset</strong> for regression. Both datasets are small, built into scikit-learn, and have been tested frequently throughout the machine learning community. As part of the model-building process, you will explicitly define default hyperparameters that give XGBoost great scores. These hyperparameters are explicitly defined so that you can learn what they are in <a id="_idIndexMarker317"/>preparation for adjusting them going forward.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor125"/>The Iris dataset</h2>
			<p>The Iris dataset, a staple <a id="_idIndexMarker318"/>of the machine learning community, was introduced by statistician Robert Fischer in 1936. Its easy accessibility, small size, clean data, and symmetry of values have made it a popular choice for testing classification algorithms.</p>
			<p>We will introduce the Iris dataset by downloading it directly from scikit-learn using the<strong class="source-inline"> datasets</strong> library with the <strong class="source-inline">load_iris()</strong> method, as follows:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from sklearn import datasets</p>
			<p class="source-code">iris = datasets.load_iris()</p>
			<p>Scikit-learn datasets are stored as <strong class="bold">NumPy arrays</strong>, the array storage method of choice for machine learning algorithms. <strong class="source-inline">pandas</strong> DataFrames are used more for data analysis and data visualization. Viewing NumPy arrays as DataFrames requires the <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong> method. This scikit-learn dataset is split into predictor and target columns in advance. Bringing them together requires concatenating the NumPy arrays with the code <strong class="source-inline">np.c_</strong> before conversion. Column names are also added, as follows:</p>
			<p class="source-code">df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])</p>
			<p>You can view the first five rows of the DataFrame using <strong class="source-inline">df.head()</strong>:</p>
			<p class="source-code">df.head()</p>
			<p>The resulting DataFrame will look like this:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B15551_05_01.jpg" alt="Figure 5.1 – The Iris dataset"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – The Iris dataset</p>
			<p>The predictor <a id="_idIndexMarker319"/>columns are self-explanatory, measuring sepal and petal length and width. The target column, according to the scikit-learn documentation, <a href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html">https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html</a>, consists of three different iris flowers, <strong class="bold">setosa</strong>, <strong class="bold">versicolor</strong>, and <strong class="bold">virginica</strong>. There are 150 rows.</p>
			<p>To prepare the data for machine learning, import <strong class="source-inline">train_test_split</strong>, then split the data accordingly. You can use the original NumPy arrays, <strong class="source-inline">iris['data']</strong> and <strong class="source-inline">iris['target']</strong>, as inputs for <strong class="source-inline">train_test_split</strong>:</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=2)</p>
			<p>Now that we have split the data, let's build the classification template.</p>
			<h3>XGBoost classification template</h3>
			<p>The following <a id="_idIndexMarker320"/>template is for building an XGBoost classifier, assuming the dataset has already been split into <strong class="source-inline">X_train</strong>, <strong class="source-inline">X_test</strong>, <strong class="source-inline">y_train</strong>, and <strong class="source-inline">y_test</strong> sets:</p>
			<ol>
				<li value="1"><p>Import <strong class="source-inline">XGBClassifier</strong> from the <strong class="source-inline">xgboost</strong> library:</p><p class="source-code">from xgboost import XGBClassifier</p></li>
				<li><p>Import a classification scoring method as needed.</p><p>While <strong class="source-inline">accuracy_score</strong> is standard, other scoring methods, such as <strong class="source-inline">auc</strong> (<strong class="bold">Area Under Curve</strong>), will be <a id="_idIndexMarker321"/>discussed later:</p><p class="source-code">from sklearn.metrics import accuracy_score</p></li>
				<li><p>Initialize the XGBoost classifier with hyperparameters.</p><p>Fine-tuning hyperparameters is the focus of <a href="B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136"><em class="italic">Chapter 6</em></a>, <em class="italic">XGBoost Hyperparameters</em>. In this chapter, the most important default hyperparameters are explicitly stated ahead: </p><p class="source-code">xgb = XGBClassifier(booster='gbtree', objective='multi:softprob', max_depth=6, learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)</p><p>The brief <a id="_idIndexMarker322"/>descriptions of the preceding hyperparameters are as follows:</p><p>a) <strong class="source-inline">booster='gbtree'</strong>: The <a id="_idIndexMarker323"/>booster is the <strong class="bold">base learner</strong>. It's the machine learning model that is constructed during every round of boosting. You may have guessed that <strong class="source-inline">'gbtree'</strong> stands for gradient boosted tree, the XGBoost default base learner. It's uncommon but possible to work with other base learners, a strategy we employ in <a href="B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189"><em class="italic">Chapter 8</em></a>, <em class="italic">XGBoost Alternative Base Learners</em>.</p><p>b) <strong class="source-inline">objective='multi:softprob'</strong>: Standard options for the objective can be viewed in the XGBoost official documentation, <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">https://xgboost.readthedocs.io/en/latest/parameter.html</a>, under <em class="italic">Learning Task Parameters</em>. The <strong class="source-inline">multi:softprob</strong> objective is a standard alternative to <strong class="source-inline">binary:logistic</strong> when the dataset includes multiple classes. It computes the probabilities of classification and chooses the highest one. If not explicitly stated, XGBoost will often find the right objective for you.</p><p>c) <strong class="source-inline">max_depth=6</strong>: The <strong class="source-inline">max_depth</strong> of a tree determines the number of branches each tree has. It's one of the most important hyperparameters in making balanced predictions. XGBoost uses a default of <strong class="source-inline">6</strong>, unlike random forests, which don't provide a value unless explicitly programmed.</p><p>d) <strong class="source-inline">learning_rate=0.1</strong>: Within XGBoost, this hyperparameter is often referred to as <strong class="source-inline">eta</strong>. This <a id="_idIndexMarker324"/>hyperparameter limits the variance by reducing the weight of each tree to the given percentage. The <strong class="source-inline">learning_rate</strong> hyperparameter <a id="_idIndexMarker325"/>was explored in detail in <a href="B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a>, <em class="italic">From Gradient Boosting to XGBoost</em>.</p><p>e) <strong class="source-inline">n_estimators=100</strong>: Popular among ensemble methods, <strong class="source-inline">n_estimators</strong> is the number of boosted trees in the model. Increasing this number while decreasing <strong class="source-inline">learning_rate</strong> can lead to more robust results.</p></li>
				<li><p>Fit the classifier to the data.</p><p>This is where the magic happens. The entire XGBoost system, the details explored in the previous two sections, the selection of optimal parameters, including regularization constraints, and speed enhancements, such as the approximate split-finding algorithm, and blocking and sharding all occur during this one powerful line of scikit-learn code:</p><p class="source-code">xgb.fit(X_train, y_train)</p></li>
				<li><p>Predict the <em class="italic">y</em> values as <strong class="source-inline">y_pred</strong>:</p><p class="source-code">y_pred = xgb.predict(X_test)</p></li>
				<li><p>Score the model by comparing <strong class="source-inline">y_pred</strong> against <strong class="source-inline">y_test</strong>:</p><p class="source-code">score = accuracy_score(y_pred, y_test)</p></li>
				<li><p>Display your results:</p><p class="source-code">print('Score: ' + str(score))</p><p class="source-code">Score: 0.9736842105263158</p></li>
			</ol>
			<p>Unfortunately, there is no official list of Iris dataset scores. There are too many to compile in one place. An initial score of <strong class="source-inline">97.4</strong> percent on the Iris dataset using default hyperparameters is very good (see <a href="https://www.kaggle.com/c/serpro-iris/leaderboard">https://www.kaggle.com/c/serpro-iris/leaderboard</a>).</p>
			<p>The XGBoost classifier <a id="_idIndexMarker326"/>template provided in the preceding paragraphs is not meant to be definitive, but rather a starting point going forward.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor126"/>The Diabetes dataset</h2>
			<p>Now that you are <a id="_idIndexMarker327"/>becoming familiar with scikit-learn and XGBoost, you are developing the ability to build and score XGBoost models fairly quickly. In this section, an XGBoost regressor template is provided using <strong class="source-inline">cross_val_score</strong> with scikit-learn's Diabetes dataset.</p>
			<p>Before building the template, import the predictor columns as <strong class="source-inline">X</strong> and the target columns as <strong class="source-inline">y</strong>, as follows:</p>
			<p class="source-code">X,y = datasets.load_diabetes(return_X_y=True)</p>
			<p>Now that we have imported the predictor and target columns, let's start building the template.</p>
			<h3>The XGBoost regressor template (cross-validation)</h3>
			<p>Here are the <a id="_idIndexMarker328"/>essential steps to build an XGBoost regression model in scikit-learn using cross-validation, assuming that the predictor columns, <strong class="source-inline">X</strong>, and the target column, <strong class="source-inline">y</strong>, have been defined:</p>
			<ol>
				<li value="1"><p>Import <strong class="source-inline">XGBRegressor</strong> and <strong class="source-inline">cross_val_score</strong>:</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">from xgboost import XGBRegressor</p></li>
				<li><p>Initialize <strong class="source-inline">XGBRegressor</strong>.</p><p>Here, we initialize <strong class="source-inline">XGBRegressor</strong> with <strong class="source-inline">objective='reg:squarederror'</strong>, the MSE. The most important hyperparameter defaults are explicitly given:</p><p class="source-code">xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror', max_depth=6, learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)</p></li>
				<li><p>Fit and score the regressor with <strong class="source-inline">cross_val_score</strong>.</p><p>With <strong class="source-inline">cross_val_score</strong>, fitting and scoring are done in one step using the model, the predictor columns, the target column, and the scoring as inputs:</p><p class="source-code">scores = cross_val_score(xgb, X, y, scoring='neg_mean_squared_error', cv=5)</p></li>
				<li><p>Display the results.</p><p>Scores for <a id="_idIndexMarker329"/>regression are commonly displayed as the <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>) to keep the units the same:</p><p class="source-code">rmse = np.sqrt(-scores)</p><p class="source-code">print('RMSE:', np.round(rmse, 3))</p><p class="source-code">print('RMSE mean: %0.3f' % (rmse.mean()))</p><p>The result is as follows:</p><p class="source-code">RMSE: [63.033 59.689 64.538 63.699 64.661]</p><p class="source-code">RMSE mean: 63.124</p></li>
			</ol>
			<p>Without a <a id="_idIndexMarker330"/>baseline of comparison, we have no idea what that score means. Converting the target column, <strong class="source-inline">y</strong>, into a <strong class="source-inline">pandas</strong> DataFrame with the <strong class="source-inline">.describe()</strong> method will give the quartiles and the general statistics of the predictor column, as follows:</p>
			<p class="source-code">pd.DataFrame(y).describe()</p>
			<p>Here is the expected output:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B15551_05_02.jpg" alt="Figure 5.2 – Describing the statistics of y, the Diabetes target column"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Describing the statistics of y, the Diabetes target column</p>
			<p>A score of <strong class="source-inline">63.124</strong> is less than 1 standard deviation, a respectable result.</p>
			<p>You now have XGBoost <a id="_idIndexMarker331"/>classifier and regressor templates that can be used for building models going forward.</p>
			<p>Now that you are accustomed to building XGBoost models in scikit-learn, it's time for a deep dive into high energy physics.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor127"/>Finding the Higgs boson – case study</h1>
			<p>In this section, we <a id="_idIndexMarker332"/>will review the Higgs Boson Kaggle Competition, which brought XGBoost into the machine learning spotlight. In order to set the stage, the historical background is given before moving on to model development. The models that we build include a default model provided by XGBoost at the time of the competition and a reference to the winning solution provided by Gabor Melis. Kaggle accounts are not required for this text, so <a id="_idIndexMarker333"/>we will not take the time to show you how to make submissions. We have provided guidelines if you are interested.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor128"/>Physics background</h2>
			<p>In popular culture, the <a id="_idIndexMarker334"/>Higgs boson is known as the <em class="italic">God particle</em>. Theorized <a id="_idIndexMarker335"/>by Peter Higgs in 1964, the Higgs boson was introduced to explain why particles have mass.</p>
			<p>The search to find the Higgs boson culminated in its discovery in 2012 in the <strong class="bold">Large Hadron Collider</strong> at CERN (Geneva, Switzerland). Nobel Prizes were awarded and the Standard Model of physics, the model that accounts for every force known to physics except for gravity, stood taller than ever before.</p>
			<p>The Higgs boson was discovered by smashing protons into each other at extremely high speeds and observing the results. Observations came from the <strong class="bold">ATLAS</strong> detector, which records data resulting from <em class="italic">hundreds of millions of proton-proton collisions per second</em>, according to the competition's technical documentation, <em class="italic">Learning to discover: the Higgs boson machine learning challenge</em>, <a href="https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf">https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf</a>.</p>
			<p>After discovering the Higgs boson, the next step was to precisely measure the characteristics of its decay. The ATLAS experiment found the Higgs boson decaying into two <strong class="bold">tau</strong> particles from data wrapped in background noise. To better understand the data, ATLAS called upon the machine learning community.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor129"/>Kaggle competitions</h2>
			<p>The Kaggle competition <a id="_idIndexMarker336"/>is a machine learning competition designed to solve a particular problem. Machine learning competitions became famous in 2006 when Netflix offered 1 million dollars to anyone who could improve upon their movie recommendations by 10%. In 2009, the 1 million dollar prize was awarded to <em class="italic">BellKor</em>'s <em class="italic">Pragmatic Chaos</em> team (<a href="https://www.wired.com/2009/09/bellkors-pragmatic-chaos-wins-1-million-netflix-prize/">https://www.wired.com/2009/09/bellkors-pragmatic-chaos-wins-1-million-netflix-prize/</a>).</p>
			<p>Many businesses, computer scientists, mathematicians, and students became aware of the increasing value that machine learning held in society. Machine learning competitions became hot, with mutual benefits going to company hosts and machine learning practitioners. Starting in 2010, many early adopters went to Kaggle to try their hand at machine learning competitions.</p>
			<p>In 2014, Kaggle announced the <em class="italic">Higgs Boson Machine Learning Challenge</em> with ATLAS (<a href="https://www.kaggle.com/c/higgs-boson">https://www.kaggle.com/c/higgs-boson</a>). With a $13,000 prize pool, 1,875 teams entered the competition.</p>
			<p>In Kaggle competitions, training data is provided, along with a required scoring method. Teams build machine learning models on the training data before submitting their results. The target column of the test data is not provided. Multiple submissions are permitted, however, and <a id="_idIndexMarker337"/>scores are returned so that competitors can improve upon their models before the final date.</p>
			<p>Kaggle competitions are fertile ground for testing machine learning algorithms. Unlike in industry, Kaggle competitions draw thousands of competitors, making the machine learning models that win prizes very well tested.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor130"/>XGBoost and the Higgs challenge</h2>
			<p>XGBoost was released <a id="_idIndexMarker338"/>to the general public on March 27, 2014, 6 months before the Higgs challenge. In the competition, XGBoost soared, helping competitors climb the Kaggle leaderboard while saving valuable time.</p>
			<p>Let's access the data to see what the competitors were working with.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor131"/>Data</h2>
			<p>Instead of <a id="_idIndexMarker339"/>using the data provided by Kaggle, we use the original data provided by the CERN open data portal where it originated: <a href="http://opendata.cern.ch/record/328">http://opendata.cern.ch/record/328</a>. The difference between the CERN data and the Kaggle data is that the CERN dataset is significantly larger. We will select the first 250,000 rows and make some modifications to match the Kaggle data.</p>
			<p>You can <a id="_idIndexMarker340"/>download the CERN Higgs boson dataset directly from <a href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter05">https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter05</a>.</p>
			<p>Read the <strong class="source-inline">atlas-higgs-challenge-2014-v2.csv.gz</strong> file into a <strong class="source-inline">pandas</strong> DataFrame. Please note that we are selecting the first 250,000 rows only, and the <strong class="source-inline">compression=gzip</strong> parameter is used since the dataset is zipped as a <strong class="source-inline">csv.gz</strong> file. After accessing the data, view the first five rows, as follows:</p>
			<p class="source-code">df = pd.read_csv('atlas-higgs-challenge-2014-v2.csv.gz', nrows=250000, compression='gzip')</p>
			<p class="source-code">df.head()</p>
			<p>The far-right columns of the output should be as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B15551_05_03.jpg" alt="Figure 5.3 – CERN Higgs boson data – Kaggle columns included"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – CERN Higgs boson data – Kaggle columns included</p>
			<p>Notice the <strong class="source-inline">Kaggleset</strong> and <strong class="source-inline">KaggleWeight</strong> columns. Since the Kaggle dataset was smaller, Kaggle used a different number for their weight column which is denoted in the preceding diagram as <strong class="source-inline">KaggleWeight</strong>. The <strong class="source-inline">t</strong> value under <strong class="source-inline">Kaggleset</strong> indicates that it's part of the training set for the <a id="_idIndexMarker341"/>Kaggle dataset. In other words, these two columns, <strong class="source-inline">Kaggleset</strong> and <strong class="source-inline">KaggleWeight</strong>, are columns in the CERN dataset designed to include information that will be used for the Kaggle dataset. In this chapter, we will restrict our subset of the CERN data to the Kaggle training set.</p>
			<p>To match the Kaggle training data, let's delete the <strong class="source-inline">Kaggleset</strong> and <strong class="source-inline">Weight</strong> columns, convert <strong class="source-inline">KaggleWeight</strong> into <strong class="source-inline">'Weight'</strong>, and move the <strong class="source-inline">'Label'</strong> column to the last column, as follows:</p>
			<p class="source-code">del df[‹Weight›]</p>
			<p class="source-code">del df[‹KaggleSet›]</p>
			<p class="source-code">df = df.rename(columns={«KaggleWeight»: «Weight»})</p>
			<p>One way to move the <strong class="source-inline">Label</strong> column is to store it as a variable, delete the column, and add a new column by assigning it to the new variable. Whenever assigning a new column to a DataFrame, the new column appears at the end:</p>
			<p class="source-code">label_col = df['Label']</p>
			<p class="source-code">del df['Label']</p>
			<p class="source-code">df['Label'] = label_col</p>
			<p>Now that all changes have been made, the CERN data matches the Kaggle data. Go ahead and view the first five rows:</p>
			<p class="source-code">df.head()</p>
			<p>Here is the <a id="_idIndexMarker342"/>left side of the expected output:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B15551_05_04.jpg" alt="Figure 5.4 – CERN Higgs boson data – physics columns"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – CERN Higgs boson data – physics columns</p>
			<p>Many columns are not shown, and an unusual value of <strong class="source-inline">-999.00</strong> occurs in multiple places.</p>
			<p>The columns beyond <strong class="source-inline">EventId</strong> include variables prefixed with <strong class="source-inline">PRI</strong>, which stands for <em class="italic">primitives</em>, which are values directly measured by the detector during collisions. By contrast, columns labeled <strong class="source-inline">DER</strong> are numerical derivations from these measurements.</p>
			<p>All column names and types are revealed by <strong class="source-inline">df.info()</strong>:</p>
			<p class="source-code">df.info()</p>
			<p>Here is a sample of the output, with the middle columns truncated to save space:</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">RangeIndex: 250000 entries, 0 to 249999</p>
			<p class="source-code">Data columns (total 33 columns):</p>
			<p class="source-code"> #   Column                       Non-Null Count   Dtype  </p>
			<p class="source-code">---  ------                       --------------   -----  </p>
			<p class="source-code"> 0   EventId                      250000 non-null  int64  </p>
			<p class="source-code"> 1   DER_mass_MMC                 250000 non-null  float64</p>
			<p class="source-code"> 2   DER_mass_transverse_met_lep  250000 non-null  float64</p>
			<p class="source-code"> 3   DER_mass_vis                 250000 non-null  float64</p>
			<p class="source-code"> 4   DER_pt_h                     250000 non-null  float64</p>
			<p class="source-code">…</p>
			<p class="source-code"> 28  PRI_jet_subleading_eta       250000 non-null  float64</p>
			<p class="source-code"> 29  PRI_jet_subleading_phi       250000 non-null  float64</p>
			<p class="source-code"> 30  PRI_jet_all_pt               250000 non-null  float64</p>
			<p class="source-code"> 31  Weight                       250000 non-null  float64</p>
			<p class="source-code"> 32  Label                        250000 non-null  object  </p>
			<p class="source-code">dtypes: float64(30), int64(3)</p>
			<p class="source-code">memory usage: 62.9 MB</p>
			<p>All columns have non-null values, and only the final column, <strong class="source-inline">Label</strong>, is non-numerical. The columns can be <a id="_idIndexMarker343"/>grouped as follows:</p>
			<ul>
				<li><p>Column <strong class="source-inline">0</strong> : <strong class="source-inline">EventId</strong> – irrelevant for the machine learning model.</p></li>
				<li><p>Columns <strong class="source-inline">1-30</strong>: Physics columns derived from LHC collisions. Details for these columns can be found in the link to the technical documentation at <a href="http://higgsml.lal.in2p3.fr/documentation">http://higgsml.lal.in2p3.fr/documentation</a>. These are the machine learning predictor columns.</p></li>
				<li><p>Column <strong class="source-inline">31</strong> : <strong class="source-inline">Weight</strong> – this column is used to scale the data. The issue here is that Higgs boson events are very rare, so a machine learning model with 99.9 percent accuracy may not be able to find them. Weights compensate for this imbalance, but weights are not available for the test data. Strategies for dealing with weights will be discussed later in this chapter, and in <a href="B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161"><em class="italic">Chapter 7</em></a>, <em class="italic">Discovering Exoplanets with XGBoost</em>.</p></li>
				<li><p>Column <strong class="source-inline">32</strong>: <strong class="source-inline">Label</strong> – this is the target column, labeled <strong class="source-inline">s</strong> for signal and <strong class="source-inline">b</strong> for background. The training data has been simulated from real data, so there are many more signals than otherwise would be found. The signal is the occurrence of the Higgs boson decay.</p></li>
			</ul>
			<p>The only issue with the data is that the target column, <strong class="source-inline">Label</strong>, is not numerical. Convert the <strong class="source-inline">Label</strong> column into a numerical column by replacing the <strong class="source-inline">s</strong> values with <strong class="source-inline">1</strong> and the <strong class="source-inline">b</strong> values with <strong class="source-inline">0</strong>, as follows:</p>
			<p class="source-code">df['Label'].replace(('s', 'b'), (1, 0), inplace=True)</p>
			<p>Now that all <a id="_idIndexMarker344"/>columns are numerical with non-null values, you can split the data into predictor and target columns. Recall that the predictor columns are indexed 1–30 and the target column is the last column, indexed <strong class="source-inline">32</strong> (or -1). Note that the <strong class="source-inline">Weight</strong> column should not be included because it's not available for the test data:</p>
			<p class="source-code">X = df.iloc[:,1:31]</p>
			<p class="source-code">y = df.iloc[:,-1]</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor132"/>Scoring</h2>
			<p>The Higgs Challenge is not <a id="_idIndexMarker345"/>your average Kaggle competition. In addition to the difficulty of understanding high energy physics for feature engineering (a route we will not pursue), the scoring method is not standard. The Higgs <a id="_idIndexMarker346"/>Challenge requires optimizing the <strong class="bold">Approximate Median Significance</strong> (<strong class="bold">AMS</strong>).</p>
			<p>The AMS is defined as follows:</p>
			<p><img src="image/Formula_05_061.png" alt=""/></p>
			<p>Here, <img src="image/Formula_05_062.png" alt=""/> is the true positive rate, <img src="image/Formula_05_063.png" alt=""/> is the false positive rate, and <img src="image/Formula_05_064.png" alt=""/> is a constant regularization term given as <strong class="source-inline">10</strong>.</p>
			<p>Fortunately, XGBoost provided an AMS scoring method for the competition, so it does not need to be formally <a id="_idIndexMarker347"/>defined. A high AMS results from many true positives and few false negatives. Justification for the AMS instead of other scoring methods is given in the technical documentation at <a href="http://higgsml.lal.in2p3.fr/documentation">http://higgsml.lal.in2p3.fr/documentation</a>.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">It's possible to build your own scoring methods, but it's not usually needed. In the rare event that you need to build your own scoring method, you can check out <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a> for more information.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor133"/>Weights</h2>
			<p>Before building a <a id="_idIndexMarker348"/>machine learning model for the Higgs boson, it's important to understand and utilize weights.</p>
			<p>In machine learning, weights can be used to improve the accuracy of imbalanced datasets. Consider the <strong class="source-inline">s</strong> (signal) and <strong class="source-inline">b</strong> (background) columns in the Higgs challenge. In reality, <strong class="source-inline">s</strong> &lt;&lt; <strong class="source-inline">b</strong>, so signals are very rare among the background noise. Let's say, for example, that signals are 1,000 times rarer than background noise. You can create a weight column where <strong class="source-inline">b</strong> = 1 and <strong class="source-inline">s</strong> = 1/1000 to compensate for this imbalance.</p>
			<p>According to the technical documentation of the competition, the weight column is a <strong class="bold">scale factor</strong> that, when summed, gives the expected number of signal and background events during the time of data collection in 2012. This means that weights are required for the predictions to represent reality. Otherwise, the model will predict way too many <strong class="source-inline">s</strong> (signal) events. </p>
			<p>The weights should first be scaled to match the test data since the test data provides the expected number of signal and background events generated by the test set. The test data has 550,000 rows, more than twice the 250,000 rows (<strong class="source-inline">len(y)</strong>) provided by the training data. Scaling weights to match the test data can be achieved by multiplying the weight column by the percentage of increase, as follows:</p>
			<p class="source-code">df['test_Weight'] = df['Weight'] * 550000 / len(y)</p>
			<p>Next, XGBoost provides a hyperparameter, <strong class="source-inline">scale_pos_weight</strong>, which takes the scaling factor into account. The scaling factor is the sum of the weights of the background noises divided by the sum of the weight of the signal. The scaling factor can be computed using <strong class="source-inline">pandas</strong> conditional notation, as follows:</p>
			<p class="source-code">s = np.sum(df[df['Label']==1]['test_Weight'])</p>
			<p class="source-code">b = np.sum(df[df['Label']==0]['test_Weight'])</p>
			<p>In the preceding code, <strong class="source-inline">df[df['Label']==1]</strong> narrows the DataFrame down to rows where the <strong class="source-inline">Label</strong> column <a id="_idIndexMarker349"/>equals <strong class="source-inline">1</strong>, then <strong class="source-inline">np.sum</strong> adds the values of these rows using the <strong class="source-inline">test_Weight</strong> column.</p>
			<p>Finally, to see the actual rate, divide <strong class="source-inline">b</strong> by <strong class="source-inline">s</strong>:</p>
			<p class="source-code">b/s</p>
			<p class="source-code">593.9401931492318</p>
			<p>In summary, the weights represent the expected number of signal and background events generated by the data. We scale the weights to match the size of the test data, then divide the sum of the background weights by the sum of the signal weights to establish the <strong class="source-inline">scale_pos_weight=b/s</strong> hyperparameter.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">For a more detailed discussion on weights, check out the excellent introduction from KDnuggets at <a href="https://www.kdnuggets.com/2019/11/machine-learning-what-why-how-weighting.html">https://www.kdnuggets.com/2019/11/machine-learning-what-why-how-weighting.html</a>.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor134"/>The model</h2>
			<p>It's time to build an XGBoost <a id="_idIndexMarker350"/>model to predict the signal – that is, the simulated occurrences of the Higgs boson decay.</p>
			<p>At the time of the competition, XGBoost was new, and the scikit-learn wrapper was not yet available. Even today (2020), the majority of information online about implementing XGBoost in Python is pre-scikit-learn. Since you are likely to encounter the pre-scikit-learn XGBoost Python API online, and this is what all competitors used in the Higgs Challenge, we present code using the original Python API in this chapter only.</p>
			<p>Here are the steps to build an XGBoost model for the Higgs Challenge:</p>
			<ol>
				<li value="1"><p>Import <strong class="source-inline">xgboost</strong> as <strong class="source-inline">xgb</strong>:</p><p class="source-code">import xgboost as xgb</p></li>
				<li><p>Initialize the XGBoost <a id="_idIndexMarker351"/>model as a <strong class="bold">DMatrix</strong> with the missing values and weights filled in.</p><p>All XGBoost models were initialized as a DMatrix before scikit-learn. The scikit-learn wrapper automatically converts the data into a DMatrix for you. The sparse matrices that XGBoost optimizes for speed are DMatrices.</p><p>According to the documentation, all values set to <strong class="source-inline">-999.0</strong> are unknown values. Instead of converting these values into the median, mean, mode, or other null replacement, in XGBoost, unknown values can be set to the <strong class="source-inline">missing</strong> hyperparameter. During the model build phase, XGBoost automatically <a id="_idIndexMarker352"/>chooses the value leading to the best split.</p></li>
				<li><p>The <strong class="source-inline">weight</strong> hyperparameter can equal the new column, <strong class="source-inline">df['test_Weight']</strong>, as defined in the <strong class="source-inline">weight</strong> section:</p><p class="source-code">xgb_clf = xgb.DMatrix(X, y, missing=-999.0, weight=df['test_Weight'])</p></li>
				<li><p>Set additional hyperparameters.</p><p>The hyperparameters that follow are defaults provided by XGBoost for the competition:</p><p>a) Initialize a blank dictionary called <strong class="source-inline">param</strong>:</p><p class="source-code">param = {}</p><p>b) Define the objective as <strong class="source-inline">'binary:logitraw'</strong>.</p><p>This means a binary model is created from logistic regression probabilities. This objective defines the model as a classifier and allows a ranking of the target column, which is required of submissions for this particular Kaggle competition:</p><p class="source-code">param['objective'] = 'binary:logitraw'</p><p>c) Scale the positive examples using the background weights divided by the signal weights. This will help the model perform better on the test set:</p><p class="source-code">param['scale_pos_weight'] = b/s</p><p>d) The learning rate, <strong class="source-inline">eta</strong>, is given as <strong class="source-inline">0.1</strong>:</p><p class="source-code">param['eta'] = 0.1</p><p>e) <strong class="source-inline">max_depth</strong> is given as <strong class="source-inline">6</strong>:</p><p class="source-code">param['max_depth'] = 6</p><p>f) Set the scoring method as <strong class="source-inline">'auc'</strong> for display purposes:</p><p class="source-code">param['eval_metric'] = 'auc'</p><p>Although the AMS score will be printed, the evaluation metric is given as <strong class="source-inline">auc</strong>, which stands for <strong class="bold">Area Under Curve</strong>. <strong class="source-inline">auc</strong> is the true positive versus false positive curve that is <a id="_idIndexMarker353"/>perfect when it equals <strong class="source-inline">1</strong>. Similar to accuracy, <strong class="source-inline">auc</strong> is a standard scoring metric for classification, although it's often superior to accuracy since accuracy is limited for imbalanced datasets, as discussed in <a href="B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161"><em class="italic">Chapter 7</em></a>, <em class="italic">Discovering Exoplanets with XGBoost</em>.</p></li>
				<li><p>Create a list of parameters that includes the preceding items, along with the evaluation metric (<strong class="source-inline">auc</strong>) and <strong class="source-inline">ams@0.15</strong>, XGBoost's implementation of the AMS score using a 15% threshold:</p><p class="source-code">plst = list(param.items())+[('eval_metric', 'ams@0.15')]</p></li>
				<li><p>Create a watchlist that includes the initialized classifier and <strong class="source-inline">'train'</strong> so that you can view scores as the trees continue to boost:</p><p class="source-code">watchlist = [ (xg_clf, 'train') ]</p></li>
				<li><p>Set the number of boosting rounds to <strong class="source-inline">120</strong>:</p><p class="source-code">num_round = 120</p></li>
				<li><p>Train and save the model. Train the model by placing the parameter list, the classifier, the number of rounds, and the watchlist as inputs. Save the model using the <strong class="source-inline">save_model</strong> method so that you do not have to go through a time-consuming training process a second time. Then, run the code and watch how the scores improve as the trees are boosted:</p><p class="source-code">print ('loading data end, start to boost trees')</p><p class="source-code">bst = xgb.train( plst, xgmat, num_round, watchlist )</p><p class="source-code">bst.save_model('higgs.model')</p><p class="source-code">print ('finish training')</p><p>The end of your <a id="_idIndexMarker354"/>results should have the following output:</p><p class="source-code">[110]	train-auc:0.94505	train-ams@0.15:5.84830</p><p class="source-code">[111]	train-auc:0.94507	train-ams@0.15:5.85186</p><p class="source-code">[112]	train-auc:0.94519	train-ams@0.15:5.84451</p><p class="source-code">[113]	train-auc:0.94523	train-ams@0.15:5.84007</p><p class="source-code">[114]	train-auc:0.94532	train-ams@0.15:5.85800</p><p class="source-code">[115]	train-auc:0.94536	train-ams@0.15:5.86228</p><p class="source-code">[116]	train-auc:0.94550	train-ams@0.15:5.91160</p><p class="source-code">[117]	train-auc:0.94554	train-ams@0.15:5.91842</p><p class="source-code">[118]	train-auc:0.94565	train-ams@0.15:5.93729</p><p class="source-code">[119]	train-auc:0.94580	train-ams@0.15:5.93562</p><p class="source-code">finish training</p></li>
			</ol>
			<p>Congratulations on building an XGBoost classifier that can predict Higgs boson decay!</p>
			<p>The model performs with <strong class="source-inline">94.58</strong> percent <strong class="source-inline">auc</strong>, and an AMS of <strong class="source-inline">5.9</strong>. As far as the AMS is concerned, the top values of the competition were in the upper threes. This model achieves an AMS of around <strong class="source-inline">3.6</strong> when submitted with the test data.</p>
			<p>The model that you just built was provided as a baseline by Tanqi Chen for XGBoost users during the competition. The winner of the competition, Gabor Melis, used this baseline to build his model. As can be seen from viewing the winning solution at <a href="https://github.com/melisgl/higgsml">https://github.com/melisgl/higgsml</a> and clicking on <strong class="bold">xgboost-scripts</strong>, changes made to the baseline model are not significant. Melis, like most Kaggle competitors, also performed feature engineering to add more relevant columns to the data, a practice we will address in <a href="B15551_09_Final_NM_ePUB.xhtml#_idTextAnchor211"><em class="italic">Chapter 9</em></a>, <em class="italic">XGBoost Kaggle Masters</em>. </p>
			<p>It is possible to build and train your own model after the deadline and submit it through Kaggle. For Kaggle competitions, submissions must be ranked, properly indexed, and delivered with the Kaggle API topics that require further explanation. If you want to submit models for the actual competition, the XGBoost <a id="_idIndexMarker355"/>ranking code, which you may find helpful, is available at <a href="https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-pred.py">https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-pred.py</a>.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor135"/>Summary</h1>
			<p>In this chapter, you learned how XGBoost was designed to improve the accuracy and speed of gradient boosting with missing values, sparse matrices, parallel computing, sharding, and blocking. You learned the mathematical derivation behind the XGBoost objective function that determines the parameters for gradient descent and regularization. You built <strong class="source-inline">XGBClassifier</strong> and <strong class="source-inline">XGBRegressor</strong> templates from classic scikit-learn datasets, obtaining very good scores. Finally, you built the baseline model provided by XGBoost for the Higgs Challenge that led to the winning solution and lifted XGBoost into the spotlight.</p>
			<p>Now that you have a solid understanding of the overall narrative, design, parameter selection, and model-building templates of XGBoost, in the next chapter, you will fine-tune XGBoost's hyperparameters to achieve optimal scores.</p>
		</div>
	</body></html>
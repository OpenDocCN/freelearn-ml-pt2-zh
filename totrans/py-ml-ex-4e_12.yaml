- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making Predictions with Sequences Using Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we focused on **Convolutional Neural Networks** (**CNNs**)
    and used them to deal with image-related tasks. In this chapter, we will explore
    **Recurrent Neural Networks** (**RNNs**), which are suitable for sequential data
    and time-dependent data, such as daily temperature, DNA sequences, and customers’
    shopping transactions over time. You will learn how the recurrent architecture
    works and see variants of the model. We will then work on their applications,
    including sentiment analysis, time series prediction, and text generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracking sequential learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the RNN architecture by example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an RNN model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overcoming long-term dependencies with **Long Short-Term Memory** (**LSTM**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing movie review sentiment with RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting stock price forecasting with LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing your own War and Peace with LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing sequential learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The machine learning problems we have solved so far in this book have been time
    independent. For example, ad click-through doesn’t depend on the user’s historical
    ad clicks under our previous approach; in face classification, the model only
    takes in the current face image, not previous ones. However, there are many cases
    in life that depend on time. For example, in financial fraud detection, we can’t
    just look at the present transaction; we should also consider previous transactions
    so that we can model based on their discrepancy. Another example is **Part-of-Speech**
    (**PoS**) tagging, where we assign a PoS (verb, noun, adverb, and so on) to a
    word. Instead of solely focusing on the given word, we must look at some previous
    words, and sometimes the next words too.
  prefs: []
  type: TYPE_NORMAL
- en: In time-dependent cases like those just mentioned, the current output is dependent
    on not only the current input but also the previous inputs; note that the length
    of the previous inputs is not fixed. Using machine learning to solve such problems
    is called **sequence learning** or **sequence modeling**. Obviously, the time-dependent
    event is called a **sequence**. Besides events that occur in disjointed time intervals
    (such as financial transactions and phone calls), text, speech, and video are
    also sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why we can’t just regularly model the sequential data by
    feeding in the entire sequence. This can be quite limiting as we have to fix the
    input size. One problem is that we will lose information if an important event
    lies outside of the fixed window. But can we just use a very large time window?
    Note that the feature space grows along with the window size. The feature space
    will become excessive if we want to cover enough events in a certain time window.
    Hence, overfitting can be another problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope you now see why we need to model sequential data in a different way.
    In the next section, we will talk about an example of a modeling technique used
    for modern sequence learning: RNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning the RNN architecture by example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can imagine, RNNs stand out because of their recurrent mechanism. We
    will start with a detailed explanation of this in the next section. We will talk
    about different types of RNNs after that, along with some typical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that in feedforward networks (such as vanilla neural networks and CNNs),
    data moves one way, from the input layer to the output layer. In RNNs, the recurrent
    architecture allows data to circle back to the input layer. This means that data
    is not limited to a feedforward direction. Specifically, in a hidden layer of
    an RNN, the output from the previous time point will become part of the input
    for the current time point. The following diagram illustrates how data flows in
    an RNN in general:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a network  Description automatically generated with medium confidence](img/B21047_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: The general form of an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: Such a recurrent architecture makes RNNs work well with sequential data, including
    time series (such as daily temperatures, daily product sales, and clinical EEG
    recordings) and general consecutive data with order (such as words in a sentence
    and DNA sequences). Take a financial fraud detector as an example; the output
    features from the previous transaction go into the training for the current transaction.
    In the end, the prediction for one transaction depends on all of its previous
    transactions. Let me explain the recurrent mechanism in a mathematical and visual
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have some inputs, *x*[t]. Here, *t* represents a time step or a sequential
    order. In a feedforward neural network, we simply assume that inputs at different
    *t* are independent of each other. We denote the output of a hidden layer at a
    time step, *t*, as *h*[t] = *f*(*x*[t]), where *f* is the abstract of the hidden
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, diagram, font, design  Description automatically
    generated](img/B21047_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: General form of a feedforward neural network'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the contrary, the feedback loop in an RNN feeds the information of the previous
    state to the current state. The output of a hidden layer of an RNN at a time step,
    *t*, can be expressed as *h*[t] = *f*(*h*[t][−1], *x*[t]). This is depicted in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, diagram, rectangle, square  Description
    automatically generated](img/B21047_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Unfolded recurrent layer over time steps'
  prefs: []
  type: TYPE_NORMAL
- en: The same task, *f*, is performed on each element of the sequence, and the output,
    *h*[t], is dependent on the output that’s generated from previous computations,
    *h*[t][−1]. The chain-like architecture captures the “memory” that has been calculated
    so far. This is what makes RNNs so successful in dealing with sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, thanks to the recurrent architecture, RNNs also have great flexibility
    in dealing with different combinations of input sequences and/or output sequences.
    In the next section, we will talk about different categories of RNNs based on
    input and output, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Many to one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One to many
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many to many (synced)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many to many (unsynced)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start by looking at many-to-one RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-one RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most intuitive type of RNN is probably **many to one**. A many-to-one RNN
    can have input sequences with as many time steps as you want, but it only produces
    one output after going through the entire sequence. The following diagram depicts
    the general structure of a many-to-one RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, diagram, rectangle  Description automatically
    generated](img/B21047_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: General form of a many-to-one RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *f* represents one or more recurrent hidden layers, where an individual
    layer takes in its own output from the previous time step. Here is an example
    of three hidden layers stacking up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, diagram, rectangle, square  Description
    automatically generated](img/B21047_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Example of three recurrent layers stacking up'
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-one RNNs are widely used for classifying sequential data. Sentiment
    analysis is a good example of this and is where the RNN reads the entire customer
    review, for instance, and assigns a sentiment score (positive, neutral, or negative
    sentiment). Similarly, we can also use RNNs of this kind in the topic classification
    of news articles. Identifying the genre of a song is another application as the
    model can read the entire audio stream. We can also use many-to-one RNNs to determine
    whether a patient is having a seizure based on an EEG trace.
  prefs: []
  type: TYPE_NORMAL
- en: One-to-many RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**One-to-many** RNNs are the exact opposite of many-to-one RNNs. They take
    in only one input (not a sequence) and generate a sequence of outputs. A typical
    one-to-many RNN is presented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a flowchart  Description automatically generated with low confidence](img/B21047_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: General form of a one-to-many RNN'
  prefs: []
  type: TYPE_NORMAL
- en: Again, *f* represents one or more recurrent hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Note that “one” here refers to a single time step or a non-sequential input,
    rather than the number of input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'One-to-many RNNs are commonly used as sequence generators. For example, we
    can generate a piece of music given a starting note and/or a genre. Similarly,
    we can write a movie script like a professional screenwriter using one-to-many
    RNNs with a starting word we specify. Image captioning is another interesting
    application: the RNN takes in an image and outputs the description (a sentence
    of words) of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-many (synced) RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The third type of RNN, many to many (synced), allows each element in the input
    sequence to have an output. Let’s look at how data flows in the following many-to-many
    (synced) RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a flowchart  Description automatically generated with low confidence](img/B21047_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: General form of a many-to-many (synced) RNN'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, each output is calculated based on its corresponding input and
    all the previous outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'One common use case for this type of RNN is time series forecasting, where
    we want to perform rolling prediction at every time step based on the current
    and previously observed data. Here are some examples of time series forecasting
    where we can leverage many-to-many (synced) RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: Product sales each day for a store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The daily closing price of a stock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power consumption of a factory each hour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are also widely used in solving NLP problems, including PoS tagging, named-entity
    recognition, and real-time speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-many (unsynced) RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, we only want to generate the output sequence *after* we’ve processed
    the entire input sequence. This is the **unsynced** version of a many-to-many
    RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following diagram for the general structure of a many-to-many
    (unsynced) RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, diagram, white, font  Description automatically
    generated](img/B21047_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: General form of a many-to-many (unsynced) RNN'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the length of the output sequence (T*y* in the preceding diagram)
    can be different from that of the input sequence (T*x* in the preceding diagram).
    This provides us with some flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'This type of RNN is the go-to model for machine translation. In French-English
    translation, for example, the model first reads a complete sentence in French
    and then produces a translated sentence in English. Multi-step ahead forecasting
    is another popular example: sometimes, we are asked to predict sales for multiple
    days in the future when given data from the past month.'
  prefs: []
  type: TYPE_NORMAL
- en: You have now learned about four types of RNN based on the model’s input and
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Wait, what about one-to-one RNNs? There is no such thing. One-to-one is just
    a regular feedforward model.
  prefs: []
  type: TYPE_NORMAL
- en: We will be applying some of these types of RNN to solve projects, including
    sentiment analysis and word generation, later in this chapter. Now, let’s figure
    out how an RNN model is trained.
  prefs: []
  type: TYPE_NORMAL
- en: Training an RNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To explain how we optimize the weights (parameters) of an RNN, we first annotate
    the weights and the data on the network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*U* denotes the weights connecting the input layer and the hidden layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V* denotes the weights between the hidden layer and the output layer. Note
    here that we use only one recurrent layer for simplicity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W* denotes the weights of the recurrent layer; that is, the feedback layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[t] denotes the inputs at time step *t*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s*[t] denotes the hidden state at time step *t*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h*[t] denotes the outputs at time step *t*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we unfold the simple RNN model over three time steps: *t* − 1, *t*, and
    *t* + 1, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, diagram  Description automatically generated](img/B21047_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Unfolding a recurrent layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'We describe the mathematical relationships between the layers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We let *a* denote the activation function for the hidden layer. In RNNs, we
    usually choose tanh or ReLU as the activation function for the hidden layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the current input, *x*[t], and the previous hidden state, *s*[t][−1],
    we compute the current hidden state, *s*[t], by *s*[t] = *a*(*Ux*[t] + *Ws*[t][−1]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feel free to read *Chapter 6*, *Predicting Stock Prices with Artificial Neural
    Networks*, again to brush up on your knowledge of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In a similar manner, we compute *s*[t][−1] based on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s*[t][-2]:*s*[t][-1]=a(*Ux*[t][-1]+*Ws*[t][-2])'
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat this until *s*[1], which depends on:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s*[0]:*s*[1]=a(*Ux*[1]+*Ws*[0])'
  prefs: []
  type: TYPE_NORMAL
- en: We usually set *s*[0] to all zeros.
  prefs: []
  type: TYPE_NORMAL
- en: We let *g* denote the activation function for the output layer. It can be a
    sigmoid function if we want to perform binary classification, a softmax function
    for multi-class classification, and a simple linear function (that is, no activation)
    for regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we compute the output at time step *t*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h*[t]:*h*[t]=*g*(*Vs*[t])'
  prefs: []
  type: TYPE_NORMAL
- en: With the dependency in hidden states over time steps (that is, *s*[t] depends
    on *s*[t][−1], *s*[t][−1] depends on *s*[t][−2], and so on), the recurrent layer
    brings memory to the network, which captures and retains information from all
    the previous time steps.
  prefs: []
  type: TYPE_NORMAL
- en: As we did for traditional neural networks, we apply the backpropagation algorithm
    to optimize all the weights, *U*, *V*, and *W*, in RNNs. However, as you may have
    noticed, the output at a time step is indirectly dependent on all the previous
    time steps (*h*^t depends on *s*[t], while *s*[t] depends on all the previous
    ones). Hence, we need to compute the loss over all previous *t*-1 time steps,
    besides the current time step. Consequently, the gradients of the weights are
    calculated this way. For example, if we want to compute the gradients at time
    step *t* = 4, we need to backpropagate the previous four time steps (*t* = 3,
    *t* = 2, *t* = 1, *t* = 0) and sum up the gradients over these five time steps.
    This version of the backpropagation algorithm is called **Back Propagation Through
    Time** (**BPTT**).
  prefs: []
  type: TYPE_NORMAL
- en: The recurrent architecture enables RNNs to capture information from the very
    beginning of the input sequence. This advances the predictive capability of sequence
    learning. You may be wondering whether vanilla RNNs can handle long sequences.
    They can in theory, but not in practice due to the **vanishing gradient** problem.
    A vanishing gradient means the gradient will become vanishingly small over long
    time steps, which prevents the weight from updating. I will explain this in detail
    in the next section, as well as introduce a variant architecture, LSTM, that helps
    solve this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming long-term dependencies with LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with the vanishing gradient issue in vanilla RNNs. Where does it
    come from? Recall that during backpropagation, the gradient decays along with
    each time step in the RNN (that is, *s*[t]=*a*(*Ux*[t]+*Ws*[t-1]); early elements
    in a long input sequence will have little contribution to the computation of the
    current gradient. This means that vanilla RNNs can only capture the temporal dependencies
    within a short time window. However, dependencies between time steps that are
    far away are sometimes critical signals to the prediction. RNN variants, including
    LSTM and **gated recurrent units** (**GRUs**), are specifically designed to solve
    problems that require learning long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: We will be focusing on LSTM in this book as it is a lot more popular than GRU.
    LSTM was introduced a decade earlier and is more mature than GRU. If you are interested
    in learning more about GRU and its applications, feel free to check out *Hands-On
    Deep Learning Architectures with Python* by Yuxi Hayden Liu (Packt Publishing).
  prefs: []
  type: TYPE_NORMAL
- en: In LSTM, we use a grating mechanism to handle long-term dependencies. Its magic
    comes from a memory unit and three information gates built on top of the recurrent
    cell. The word “gate” is taken from the logic gate in a circuit ([https://en.wikipedia.org/wiki/Logic_gate](https://en.wikipedia.org/wiki/Logic_gate)).
    It is basically a sigmoid function whose output value ranges from `0` to `1`.
    `0` represents the “off” logic, while `1` represents the “on” logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LSTM version of the recurrent cell is depicted in the following diagram,
    right after the vanilla version for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram, text, technical drawing, plan  Description
    automatically generated](img/B21047_12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Recurrent cell in vanilla RNNs versus LSTM RNNs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the LSTM recurrent cell in detail from left to right:'
  prefs: []
  type: TYPE_NORMAL
- en: '*c*[t] is the **memory unit**. It memorizes information from the very beginning
    of the input sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f* stands for the **forget gate**. It determines how much information from
    the previous memory state, *c*[t][−1], to forget, or, in other words, how much
    information to pass forward. Let *W*^f denote the weights between the forget gate
    and the previous hidden state, *s*[t][−1], and *U*^f denote the weights between
    the forget gate and the current input, *x*[t].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*i* represents the **input gate**. It controls how much information from the
    current input to put through. *W*^i and *U*^i are the weights connecting the input
    gate to the previous hidden state, *s*[t][−1], and the current input, *x*[t],
    respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*tanh* is simply the activation function for the hidden state. It acts as the
    *a* in the vanilla RNN. Its output is computed based on the current input, *x*[t],
    along with the associated weights, *U*^c, the previous hidden state, *s*[t][−1],
    and the corresponding weights, *W*^c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`o` serves as the **output gate**. It defines how much information is extracted
    from the internal memory for the output of the entire recurrent cell. As always,
    *W*^o and *U*^o are the associated weights for the previous hidden state and current
    input, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We describe the relationship between these components as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the forget gate, *f*, at time step *t* is computed as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f*=*sigmoid*(*W*^f*s*[t-1]+*U*^f*x*[t])'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the input gate, *i*, at time step *t* is computed as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*i*=*sigmoid*(*W*^i*s*[t-1]+*U*^i*x*[t])'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the tanh activation, *c’*, at time step *t* is computed as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*c’*=*tanh*(*W*^c*s*[t-1]+*U*^c*x*[t])'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the output gate, *o*, at time step *t* is computed as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*o*=*sigmoid*(*W*^o*s*[t-1]+*U*^o*x*[t])'
  prefs: []
  type: TYPE_NORMAL
- en: The memory unit, *c*[t], at time step *t* is updated using *c*[t]=*f*.**c*[t-1]+*i*.**c’*
    (here, the operator .* denotes element-wise multiplication). Again, the output
    of a sigmoid function has a value from 0 to 1\. Hence, the forget gate, *f*, and
    input gate, *i*, control how much of the previous memory, *c*[t][−1], and the
    current memory input, *c’*, to carry forward, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we update the hidden state, *s*[t], at time step *t* by *s*[t]=*o*.**c*[t]
    . Here, the output gate, *o*, governs how much of the updated memory unit, *c*[t],
    will be used as the output of the entire cell.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM is often considered the default choice for RNN models in practice due
    to its ability to effectively capture long-term dependencies in sequential data
    while mitigating the vanishing gradient problem. However, GRUs are also commonly
    used depending on the specific task and dataset characteristics. The choice between
    LSTM and GRU depends on the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model complexity**: LSTMs typically have more parameters than GRUs due to
    their additional gating mechanisms. If you have limited computational resources
    or are working with smaller datasets, GRUs may be more suitable due to their simpler
    architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training speed**: GRUs are generally faster to train than LSTMs. If training
    time is a concern, GRUs might be a better choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: LSTMs tend to have better performance on tasks that require
    modeling long-term dependencies in sequential data. If your task involves capturing
    complex temporal patterns and you’re concerned about overfitting, LSTMs might
    be preferable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As always, we apply the **BPTT** algorithm to train all the weights in LSTM
    RNNs, including four sets each of weights, *U* and *W*, associated with three
    gates and the tanh activation function. By learning these weights, the LSTM network
    explicitly models long-term dependencies in an efficient way. Hence, LSTM is the
    go-to or default RNN model in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how to use LSTM RNNs to solve real-world problems. We will
    start by categorizing movie review sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing movie review sentiment with RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, here comes our first RNN project: movie review sentiment. We’ll use the
    IMDb ([https://www.imdb.com/](https://www.imdb.com/)) movie review dataset ([https://ai.stanford.edu/~amaas/data/sentiment/](https://ai.stanford.edu/~amaas/data/sentiment/))
    as an example. It contains 25,000 highly popular movie reviews for training and
    another 25,000 for testing. Each review is labeled as 1 (positive) or 0 (negative).
    We’ll build our RNN-based movie sentiment classifier in the following three sections:
    *Analyzing and preprocessing the movie review data, Developing a simple LSTM network,*
    and *Boosting the performance with multiple LSTM layers*.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and preprocessing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll start with data analysis and preprocessing, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch’s `torchtext` has a built-in IMDb dataset, so first, we load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We just load 25,000 training samples and 25,000 test samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you encounter any errors while running the code, consider installing the
    `torchtext` and `portalocker` packages. You could use the following commands for
    installation via `conda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, via `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s explore the vocabulary within the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we define a function to extract tokens (words, in our case) from a given
    document (movie review, in our case). It first removes HTML-like tags, then extracts
    and standardizes emoticons, removes non-alphanumeric characters, and tokenizes
    the text into a list of words for further processing. We store the tokens and
    their occurrences in the `Counter` object `token_counts`.
  prefs: []
  type: TYPE_NORMAL
- en: As evident, the training set comprises approximately 76,000 unique words, and
    it exhibits a perfect balance with an equal count of positive (labeled as “`2`")
    and negative (labeled as “`1`") samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will feed the word tokens into an embedding layer, `nn.Embedding`. The embedding
    layer requires integer input because it’s specifically designed to handle discrete
    categorical data, such as word indices, and transform them into continuous representations
    that a neural network can work with and learn from. Therefore, we need to first
    encode each token into a unique integer as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use the `vocab` module in PyTorch to create a vocabulary (token mapping)
    based on the frequency of words in the corpus. But this vocabulary is not complete
    yet. Let’s see why in the next two steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'When examining the document lengths within the training set, you’ll notice
    that they range from 10 to 2,498 words. It’s common practice to apply padding
    to sequences to ensure uniform length during batch processing. So, we insert the
    special token, `"<pad>"`, representing padding into the vocabulary mapping at
    index `0` as a **placeholder**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to handle unseen words during inference. Similar to the previous
    step, we insert the special token `"<unk>"` (short for “unknown”) into the vocabulary
    mapping at index `1`. The token represents out-of-vocabulary words or tokens that
    are not found in the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We also set the default vocabulary mapping to `1`. This means `"<unk>"` (index
    1) is used as the default index for unseen or out-of-vocabulary words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the following examples showing the mappings of given words,
    including an unseen one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By now, we have the complete vocabulary mapping.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using special tokens like `<pad>` and `<unk>` in RNNs is a common practice
    for handling variable-length sequences and out-of-vocabulary words. Here are some
    best practices for their usage:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `<pad>` tokens to pad sequences to a fixed length. This ensures that all
    input sequences have the same length, which is necessary for efficient batch processing
    in neural networks. Pad sequences at the end rather than the beginning to preserve
    the order of the input data. When tokenizing text data, assign a unique integer
    index to the `<pad>` token and ensure that it corresponds to a vector of zeros
    in the embedding matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `<unk>` tokens to represent out-of-vocabulary words that are not present
    in the vocabulary of the model. During inference, replace any words that are not
    present in the vocabulary with the `<unk>` token to ensure that the model can
    process the input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exclude `<pad>` tokens from contributing to the loss during training to avoid
    skewing the learning process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the distribution of `<unk>` tokens in the dataset to assess the prevalence
    of out-of-vocabulary words and adjust the vocabulary size accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we define the function defining how batches of samples should be collated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Besides generating inputs and label outputs as we used to do, we also generate
    the length of individual samples in a given batch. Note that we convert the positive
    label from the raw 2 to 1 here, for label standardization and loss function compatibility
    for binary classification. The length information is used for handling variable-length
    sequences efficiently. Take a small batch of four samples and examine the processed
    batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can see the processed text sequences have been standardized to a length
    of 247 tokens, with the first, second, and fourth samples padded with 0s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we batch the training and testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The generated data loaders are ready to use for sentiment prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to building an LSTM network.
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple LSTM network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the training and testing data loaders are ready, we can build our
    first RNN model with an embedding layer that encodes the input word tokens, and
    an LSTM layer followed by a fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the network hyperparameters, including the input dimension
    and the embedding dimension of the embedding layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also define the number of hidden nodes in the LSTM layer and the fully connected
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build our RNN model class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `nn.Embedding` layer is used to convert input word indices into dense word
    embeddings. The `padding_idx` parameter is set to 0, indicating that padding tokens
    should be ignored during embedding.
  prefs: []
  type: TYPE_NORMAL
- en: The recurrent layer, `nn.LSTM`, takes the embedded input sequence and processes
    it sequentially. `batch_first=True` means that the input has a batch size as the
    first dimension.
  prefs: []
  type: TYPE_NORMAL
- en: The fully connected hidden layer, `fc1`, follows the LSTM layer, and the ReLU
    activation is applied to the output of the fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: The final layer has a single output because this model is used for binary classification
    (sentiment analysis).
  prefs: []
  type: TYPE_NORMAL
- en: In the forward pass method, `pack_padded_sequence` is used to pack and pad sequences
    for efficient processing in the LSTM layer. The packed sequence is passed through
    the LSTM layer, and the final hidden state (`hidden[-1, :, :]`) is extracted.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create an instance of the LSTM model with the specific hyperparameters
    we defined earlier. We also ensure that the model is placed on the specified computing
    device (GPU if available) for training and inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As for the loss function, we use `nn.BCELoss()` since it is a binary classification
    problem. We also set the corresponding optimizer and try with a learning rate
    of `0.003` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we define a training function responsible for training the model for one
    iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It also displays the training loss and accuracy at the end of an iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then train the model for 10 iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The training accuracy is close to 100% after 10 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we evaluate the performance on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We obtained a test accuracy of 86%.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking multiple LSTM layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also stack two (or more) recurrent layers. The following diagram shows
    how two recurrent layers can be stacked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, diagram, line  Description automatically
    generated](img/B21047_12_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Unfolding two stacked recurrent layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyTorch, stacking multiple RNN layers is simple. Using LSTM as an example
    once more, it suffices to specify the number of LSTM layers in the `num_layers`
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we stack two LSTM layers. Feel free to experiment with a multi-layer
    RNN model and see whether you can beat the previous single-layered model.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve just finished the review sentiment classification project using
    RNNs. In the next project, we will revisit stock price prediction and solve it
    using RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting stock price forecasting with LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall in *Chapter 6**, Predicting Stock Prices with Artificial Neural Networks*,
    we derived features from past prices and performance within a specific time step
    and then trained a standard neural network. In this instance, we will utilize
    RNNs as the sequential model and harness features from five consecutive time steps
    rather than just one. Let’s examine the process in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, we load the stock data, create the features and labels, and then
    split it into training and test sets, mirroring our approach in *Chapter 6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we reuse the feature and label generation function, `generate_features`,
    defined in *Chapter 6*. Similarly, we scale the feature space using `StandardScaler`
    and covert data into `FloatTensor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a function to create sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, every generated sequence comprises two components: the input sequence,
    which encompasses features from five successive days, and the label, representing
    the price of the last day in that five-day period. We generate sequences for the
    training and test sets respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, we establish a data loader for the training sequences in preparation
    for model construction and training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We set 128 as the batch size in this project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we define an RNN model with a two-layered LSTM followed by a fully connected
    layer and an output layer for regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The LSTM layer captures sequential dependencies in the input data, and the fully
    connected layers perform the final regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we initiate a model after specifying the input dimension and hidden layer
    dimensions, and use MSE as the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Small to medium values (like 16) are often used as starting points for hidden
    dimensions in RNNs, for computational efficiency. The chosen optimizer (Adam)
    and learning rate (`0.01`) are hyperparameters that can be tuned for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we train the model for 1000 iterations as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The MSE during training is displayed every 100 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we apply the trained model on the test set and evaluate the performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are able to obtain an *R*² of `0.9` on the test set. You may observe that
    this doesn’t outperform our previous standard neural network. The reason is that
    we have a relatively small training dataset of only eight thousand samples. RNNs
    typically require a larger dataset to excel.
  prefs: []
  type: TYPE_NORMAL
- en: The two RNN models we’ve explored up to this point followed the many-to-one
    structure. In our upcoming project, we’ll create an RNN using the many-to-many
    structure, and the objective is to generate a “novel.”
  prefs: []
  type: TYPE_NORMAL
- en: Writing your own War and Peace with RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we’ll work on an interesting language modeling problem – text
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: An RNN-based text generator can write anything, depending on what text we feed
    it. The training text can be from a novel such as *A Game of Thrones*, a poem
    from Shakespeare, or the movie scripts for *The Matrix*. The artificial text that’s
    generated should read similarly (but not identically) to the original one if the
    model is well trained. In this section, we are going to write our own *War and
    Peace* with RNNs, a novel written by the Russian author Leo Tolstoy. Feel free
    to train your own RNNs on any of your favorite books.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with data acquisition and analysis before constructing the training
    set. After that, we will build and train an RNN model for text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring and analyzing the training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I recommend downloading text data for training from books that are not currently
    protected by copyright. Project Gutenberg ([www.gutenberg.org](https://www.gutenberg.org))
    is a great place for this. It provides over 60,000 free e-books whose copyright
    has expired.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original work, *War and Peace*, can be downloaded from [http://www.gutenberg.org/ebooks/2600](http://www.gutenberg.org/ebooks/2600),
    but note that there will be some cleanup, such as removing the extra beginning
    section “*The Project Gutenberg EBook*,” the table of contents, and the extra
    appendix “*End of the Project Gutenberg EBook of War and Peace*” of the plain
    text UTF-8 file ([http://www.gutenberg.org/files/2600/2600-0.txt](http://www.gutenberg.org/files/2600/2600-0.txt)),
    required. So, instead of doing this, we will download the cleaned text file directly
    from [https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt](https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we read the file and convert the text into lowercase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we take a quick look at the training text data by printing out the first
    200 characters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we count the number of unique words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we count the total number of characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'From these 3 million characters, we obtain the unique characters, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The raw training text is made up of 57 unique characters and close to 40,000
    unique words. Generating words, which requires computing 40,000 probabilities
    at one step, is far more difficult than generating characters, which requires
    computing only 57 probabilities at one step. Hence, we treat a character as a
    token, and the vocabulary here is composed of 57 characters.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we feed the characters to the RNN model and generate output characters?
    Let’s see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the training set for the RNN text generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that in a synced “many-to-many” RNN, the network takes in a sequence
    and simultaneously produces a sequence; the model captures the relationships among
    the elements in a sequence and reproduces a new sequence based on the learned
    patterns. As for our text generator, we can feed in fixed-length sequences of
    characters and let it generate sequences of the same length, where each output
    sequence is one character shifted from its input sequence. The following example
    will help you understand this better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say that we have a raw text sample, “`learning`,” and we want the sequence
    length to be 5\. Here, we can have an input sequence, “`learn`,” and an output
    sequence, “`earni`.” We can put them into the network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a flowchart  Description automatically generated with low confidence](img/B21047_12_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: Feeding a training set (“learn,” “earni”) to the RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve just constructed a training sample `("learn`,” “`earni`"). Similarly,
    to construct training samples from the entire original text, first, we need to
    split the original text into fixed-length sequences, *X*; then, we need to ignore
    the first character of the original text and split shift it into sequences of
    the same length, *Y*. A sequence from *X* is the input of a training sample, while
    the corresponding sequence from *Y* is the output of the sample. Let’s say we
    have a raw text sample, “machine learning by example,” and we set the sequence
    length to 5\. We will construct the following training samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, number  Description automatically generated](img/B21047_12_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.13: Training samples constructed from “machine learning by example”'
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![](img/Icon.png) denotes space. Note that the remaining subsequence,
    “le,” is not long enough, so we simply ditch it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to one-hot encode the input and output characters since neural
    network models only take in numerical data. We simply map the 57 unique characters
    to indices from 0 to 56, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: For instance, the character `c` becomes a vector of length 57 with `1` in index
    28 and `0`s in all other indices; the character `h` becomes a vector of length
    57 with `1` in index 33 and `0`s in all other indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the character lookup dictionary is ready, we can construct the entire
    training set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set the sequence length to `40` and obtain training samples of a length
    of `41` where the first 40 elements represent the input, and the last 40 elements
    represent the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we initialize the training dataset object and data loader, which will
    be used for model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We just create a data loader in batches of 64 sequences, shuffle the data at
    the beginning of each epoch, and drop any remaining data points that don’t fit
    into a complete batch.
  prefs: []
  type: TYPE_NORMAL
- en: We finally got the training set ready and it is time to build and fit the RNN
    model. Let’s do this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building and training an RNN text generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first build the RNN model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This class defines a sequence-to-sequence model that takes tokenized input,
    converts token indices into dense vector representation with an embedding layer,
    processes the dense vectors through an LSTM layer, and generates logits for the
    next token in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this class, the `init_hidden` method initializes the hidden state and cell
    state of the LSTM. It takes `batch_size` as a parameter, which is used to determine
    the batch size for the initial states. Two tensors are created: `hidden` and `cell`,
    both initialized with zeros. The `forward` method receives two additional inputs,
    `hidden` and `cell`, which correspond to the many-to-many architecture of our
    RNN model.'
  prefs: []
  type: TYPE_NORMAL
- en: One more thing to note, we use logits as outputs of the model here instead of
    probabilities, as we will sample from the predicted logits to generate new sequences
    of characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s train the RNN model we just defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we specify the embedding dimension and the size of the LSTM hidden layer,
    and initiate the RNN model object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A relatively high embedding dimension (like 256) allows for capturing richer
    semantic information about words. This can be beneficial for tasks like text generation.
    However, excessively high dimensions can increase computational cost and might
    lead to overfitting. 256 provides a good balance between these factors.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation often requires the model to learn long-term dependencies between
    words in a sequence. A hidden layer size of 512 offers a good capacity to capture
    these complex relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next task is to define a loss function and an optimizer. In the case of
    multiclass classification, where there is a single logit output for each target
    character, we utilize `CrossEntropyLoss` as the appropriate loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we train the model for 10,000 epochs. In each epoch, we train our many-to-many
    RNN on one training batch selected from the data loader, and we display the training
    loss for every 500 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For each element in a given sequence, we feed the recurrent layer with the previous
    hidden state along with the current input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model training is complete, and now it’s time to assess its performance. We
    can generate text by providing a few starting words, for instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We generate a 500-character text beginning with our given input “the emperor.”
    Specifically, we first initialize the hidden and cell state for the RNN model.
    This is required to start generating text. Then, in the `for` loop, we iterate
    over the characters in the starting text except the last one. For each character
    in the input, we pass it through the model, updating the hidden and cell states.
    To generate the next character index, we predict the logits for all possible characters
    and sample it based on the logits utilizing a `Categorical` distribution. With
    that, we’ve successfully used a many-to-many type of RNN to generate text.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to tweak the model so that the RNN-based text generator can write
    a more realistic and interesting version of *War and Peace*.
  prefs: []
  type: TYPE_NORMAL
- en: An RNN with a many-to-many structure is a type of sequence-to-sequence (seq2seq)
    model that takes in a sequence and outputs another sequence. A typical example
    is machine translation, where a sequence of words from one language is transformed
    into a sequence in another language. The state-of-the-art seq2seq model is the
    **Transformer** model, and it was developed by Google Brain. We will discuss it
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we worked on three NLP projects: sentiment analysis, stock
    price prediction, and text generation using RNNs. We started with a detailed explanation
    of the recurrent mechanism and different RNN structures for different forms of
    input and output sequences. You also learned how LSTM improves vanilla RNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on the Transformer, a recent state-of-the-art
    sequential learning model, and generative models.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use a bi-directional recurrent layer (it is easy enough to learn about it by
    yourself) and apply it to the sentiment analysis project. Can you beat what we
    achieved? Hint: set the `bidirectional` argument to `True` in the LSTM layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to fine-tune the hyperparameters in the text generator, and see whether
    you can generate a more realistic and interesting version of *War and Peace*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you train an RNN model on any of your favorite books in order to write your
    own version?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code187846872178698968.png)'
  prefs: []
  type: TYPE_IMG

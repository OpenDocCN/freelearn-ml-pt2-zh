<html><head></head><body>
<section id="chapter-10-inference-engines" class="level2 chapterHead" data-number="1.14">&#13;
<h1 class="chapterHead" data-number="1.14">Chapter 10<br/>&#13;
<span id="x1-18900010"/>Inference Engines</h1>&#13;
<blockquote>&#13;
<p>The first principle is that you must not fool yourself—and you are the easiest person to fool. – Richard Feynman</p>&#13;
</blockquote>&#13;
<p>So far, we have focused on model building, interpretation of results, and criticism of models. We have relied on the magic of the <code>pm.sample </code>function to compute posterior distributions for us. Now we will focus on learning some of the details of the inference engines behind this function.</p>&#13;
<p>The whole purpose of probabilistic programming tools, such as PyMC, is that the user should not care about how sampling is carried out, but understanding how we get samples from the posterior is important for a full understanding of the inference process, and could also help us to get an idea of when and how these methods fail and what to do about it. If you are not interested in understanding how these methods work, you can skip most of this chapter, but I strongly recommend you at least read the <em>Diagnosing sample</em>s section, as this section provides a few guidelines that will help you to check whether your posterior samples are reliable. There are many methods for computing the posterior distribution. In this chapter, we will discuss some general ideas and will focus on the most important methods implemented in PyMC. We will learn about:</p>&#13;
<ul>&#13;
<li><p>Inference engines</p></li>&#13;
<li><p>Metropolis-Hastings</p></li>&#13;
<li><p>Hamiltonian Monte Carlo</p></li>&#13;
<li><p>Sequential Monte Carlo</p></li>&#13;
<li><p>Diagnosing samples</p></li>&#13;
</ul>&#13;
<p><span id="x1-189001r448"/></p>&#13;
<section id="inference-engines" class="level3 sectionHead" data-number="1.14.1">&#13;
<h2 class="sectionHead" data-number="1.14.1">10.1 <span id="x1-1900001"/>Inference engines</h2>&#13;
<p><span id="dx1-190001"/></p>&#13;
<p>While conceptually simple, Bayesian methods can be mathematically and numerically challenging. The main reason is that the marginal likelihood, the denominator in Bayes’ theorem, usually takes the form of an intractable or computationally expensive integral to solve. For this reason, the posterior is usually estimated numerically using algorithms from the <strong>Markov Chain</strong> <strong>Monte Carlo</strong> (<strong>MCMC</strong>) family. These methods are sometimes called inference engines, because, at <span id="dx1-190002"/>least in principle, they are capable of approximating the posterior distribution for any probabilistic model. Even though inference does not always work that well in practice, the existence of such methods has motivated the development of probabilistic programming languages such as PyMC.</p>&#13;
<p>The goal of probabilistic programming languages is to separate the model-building process from the inference process to facilitate the iterative steps of model-building, evaluation, and model modification/expansion. By treating the inference process (but not the model-building process) as a black box, users of probabilistic programming languages such as PyMC are free to focus on their specific problems, leaving PyMC to handle the computational details for them. This is exactly what we have been doing up to this point. So, you may be biased toward thinking that this is the obvious or natural approach. But it is important to notice that before probabilistic programming languages, people working with probabilistic models were also used to writing their own sampling methods, generally tailored to their models, or they were used to simplifying their models to make them suitable for certain mathematical approximations. In fact, this is still true in some academic circles. This tailored approach can be more elegant and can even provide a more efficient way of computing a posterior (for a very specific model), but it is also error-prone and time-consuming, even for experts. Furthermore, the tailored approach is not suitable for most practitioners interested in solving problems with probabilistic models. Software such as PyMC invites people from a very broad background to work with probabilistic models, lowering the mathematical and computational entry barrier. I personally think this is fantastic and also an invitation to learn more about good practices in statistical modeling so we try to avoid fooling ourselves.</p>&#13;
<p>The previous chapters have been mostly about learning the basics of Bayesian modeling; now we are going to learn, at a conceptual level, how automatic inference is achieved, when and why it fails, and what to do when it fails.</p>&#13;
<p>Before discussing MCMC methods, however, let me explain two other methods that can be useful sometimes, and also provide an intuition of why we usually use MCMC as general methods. <span id="x1-190003r451"/></p>&#13;
</section>&#13;
<section id="the-grid-method" class="level3 sectionHead" data-number="1.14.2">&#13;
<h2 class="sectionHead" data-number="1.14.2">10.2 <span id="x1-1910002"/>The grid method</h2>&#13;
<p><span id="dx1-191001"/></p>&#13;
<p>The grid method is a simple brute-force approach. Even if you are not able to compute the whole posterior, you may be able to compute the prior and the likelihood point-wise; this is a pretty common scenario, if not the most common one.</p>&#13;
<p>Let’s assume we want to compute the posterior for a model with a single parameter. The grid approximation is as follows:</p>&#13;
<ol>&#13;
<li><div id="x1-191003x1">&#13;
<p>Define a reasonable interval for the parameter (the prior should give you a hint).</p>&#13;
</div></li>&#13;
<li><div id="x1-191005x2">&#13;
<p>Place a grid of points (generally equidistant) on that interval.</p>&#13;
</div></li>&#13;
<li><div id="x1-191007x3">&#13;
<p>For each point in the grid, multiply the likelihood and the prior.</p>&#13;
</div></li>&#13;
</ol>&#13;
<p>Optionally, we may normalize the computed values, that is, we divide each value in the <code>posterior </code>array by the total area under the curve, ensuring that the total area equals 1.</p>&#13;
<p>The following code block implements the grid method for the coin-flipping model:</p>&#13;
<p><span id="x1-191008r1"/> <span id="x1-191009"/><strong>Code 10.1</strong></p>&#13;
<pre id="listing-126" class="source-code"><code>def posterior_grid(grid_points=50, heads=6, tails=9): </code>&#13;
<code>    """ </code>&#13;
<code>    A grid implementation for the coin-flipping problem </code>&#13;
<code>    """ </code>&#13;
<code>    grid = np.linspace(0, 1, grid_points) </code>&#13;
<code>    prior = np.repeat(1/grid_points, grid_points)  # uniform prior </code>&#13;
<code>    likelihood = pz.Binomial(n=heads+tails, p=grid).pdf(heads) </code>&#13;
<code>    posterior = likelihood * prior </code>&#13;
<code>    posterior /= posterior.sum() * (1/grid_points) </code>&#13;
<code>    return grid, posterior</code></pre>&#13;
<p><em>Figure <a href="#x1-191020r1">10.1</a></em> shows the posterior we get for flipping a coin 13 times and observing 3 heads under a Uniform prior. The curve is very rugged, as we used a grid of only 10 points. If you increase the number of points, the curve will look smoother, the computation will be more accurate, and the cost will be higher.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file248.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-191020r1"/><strong>Figure 10.1</strong>: Posterior computed using the grid method</p>&#13;
<p>The biggest caveat of the grid approach is that this method scales poorly with the number of parameters, also referred to as dimensions. We can see this with a simple example. Suppose we want to sample a unit interval (see <em>Figure <a href="#x1-191021r2">10.2</a></em>) like in the coin-flipping problem, and we use four equidistant points; this would mean a resolution of 0.25 units. Now, suppose we have a 2D problem (the square in <em>Figure <a href="#x1-191021r2">10.2</a></em>) and we want to use a grid with the same resolution; we will need 16 points. And lastly, for a 3D problem, we will need 64 (see the cube in <em>Figure <a href="#x1-191021r2">10.2</a></em>). In this example, we need 16 times as many resources to sample from a cube of side 1 than for a line of length 1 with a resolution of 0.25. If we decide instead to have a resolution of 0.1 units, we will have to sample 10 points for the line and 1,000 for the cube.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file249.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-191021r2"/><strong>Figure 10.2</strong>: A grid with the same resolution in 1, 2, and 3 dimensions</p>&#13;
<p>Besides how fast the number of points increases, there is another phenomenon that is not a property of the grid method, or any other method for that matter. It is a property of high-dimensional spaces. As you increase the number of parameters, the region of the parameter space where most of the posterior is concentrated gets smaller and smaller compared to the sampled volume. This is a pervasive phenomenon and is usually known as the curse of dimensionality, or as mathematicians prefer to call it, the concentration of measure.</p>&#13;
<p>The curse of dimensionality is the term used to refer to various related phenomena that are absent in low-dimensional spaces but present in high-dimensional spaces. Here are some examples of these phenomena:</p>&#13;
<ul>&#13;
<li><p>As the number of dimensions increases, the Euclidean distance between any pair of samples tends to resemble the distance between other pairs. That is, in high-dimensional spaces, most points are basically at the same distance from one another.</p></li>&#13;
<li><p>For a hypercube, most of the volume is at its corners, not in the middle. For a hypersphere, most of the volume is at its surface and not in the middle.</p></li>&#13;
<li><p>In high dimensions, most of the mass of a multivariate Gaussian distribution is not close to the mean (or mode), but in a shell around it that moves away from the mean to the tails as the dimensionality increases. This shell is referred to as the typical set.</p></li>&#13;
</ul>&#13;
<p>For code examples illustrating these concepts, please check out the repository for this book at <a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>.</p>&#13;
<p>For our current discussion, all these facts mean that if we do not choose wisely where to evaluate the posterior, we will spend most of our time computing values with an almost null contribution to the posterior, and thus we will be wasting valuable resources. The grid method is not a very smart method to choose to evaluate the posterior distribution, thus making it not very useful as a general method for high-dimensional problems. <span id="x1-191022r452"/></p>&#13;
</section>&#13;
<section id="quadratic-method" class="level3 sectionHead" data-number="1.14.3">&#13;
<h2 class="sectionHead" data-number="1.14.3">10.3 <span id="x1-1920003"/>Quadratic method</h2>&#13;
<p><span id="dx1-192001"/> <span id="dx1-192002"/></p>&#13;
<p>The quadratic approximation, also known as the Laplace method or the normal approximation, consists of approximating the posterior with a Gaussian distribution. To do this, we first find the model of the posterior distribution; numerically, we can do this with an optimization method. Then we compute the Hessian matrix, from which we can then estimate the standard deviation. If you are wondering, the Hessian matrix is a square matrix of second-order partial derivatives. For what we care we can use it to obtain the standard deviation of in general a covariance matrix.</p>&#13;
<p>Bambi can solve Bayesian models using the quadratic method for us. In the following code block, we first define a model for the coin-flipping problem, the same one we already defined for the grid method, and then we fit it using the quadratic method, called <code>laplace </code>in Bambi:</p>&#13;
<p><span id="x1-192003r2"/> <span id="x1-192004"/><strong>Code 10.2</strong></p>&#13;
<pre id="listing-127" class="source-code"><code>data = pd.DataFrame(data, columns=["w"]) </code>&#13;
<code>priors = {"Intercept": bmb.Prior("Uniform", lower=0, upper=1)} </code>&#13;
<code>model = bmb.Model("w ∼ 1", data=data, family="bernoulli", priors=priors, </code>&#13;
<code>                  link="identity") </code>&#13;
<code>results = model.fit(draws=4000, inference_method="laplace")</code></pre>&#13;
<p><em>Figure <a href="#x1-192010r3">10.3</a></em> shows the computed posterior and the exact posterior. Notice that Bambi also returns samples when using this method. It first approximates the posterior as a Gaussian (or multivariate Gaussian) and then takes samples from it.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file250.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-192010r3"/><strong>Figure 10.3</strong>: A quadratic approximation to the posterior</p>&#13;
<p>The quadratic/Laplace method is included in Bambi mostly for pedagogical purposes. One nice feature, though, is that Bambi takes into account the boundaries. For example, for the coin-flipping problem, we know the solution must be in the interval [0, 1]. Bambi ensures this is true, even when we use a Gaussian under the hood. Bambi achieves this by fitting a Gaussian in an unbounded parameter space, and then transforming to the proper bounded space.</p>&#13;
<p>The quadratic/Laplace method, while very limited in itself, can be used as the building block of more advanced methods. For instance, the <strong>Integrated Nested</strong> <strong>Laplace Approximation</strong> (<strong>INLA</strong>) can be used to fit a wide variety of models very efficiently. <span id="x1-192011r455"/></p>&#13;
</section>&#13;
<section id="markovian-methods" class="level3 sectionHead" data-number="1.14.4">&#13;
<h2 class="sectionHead" data-number="1.14.4">10.4 <span id="x1-1930004"/>Markovian methods</h2>&#13;
<p><span id="dx1-193001"/></p>&#13;
<p>There is a family of related methods, collectively known as the <strong>Markov chain</strong> <strong>Monte Carlo</strong> or <strong>MCMC</strong> methods. These are stochastic methods that allow us to get samples from the true posterior distribution as long as we can compute the likelihood and the prior point-wise. You may remember that this is the same condition we needed for the grid method, but contrary to them, MCMC methods can efficiently sample from higher-probability regions in very high dimensions.</p>&#13;
<p>MCMC methods visit each region of the parameter space following their relative probabilities. If the probability of region A is twice that of region B, we will obtain twice as many samples from A as we will from B. Hence, even if we are not capable of computing the whole posterior analytically, we could use MCMC methods to take samples from it. In theory, MCMC will give us samples from the correct distribution – the catch is that this theoretical guarantee only holds asymptotically, that is, for an infinite number of samples! In practice, we always have a finite number of samples, thus we need to check that the samples are trustworthy. We are going to learn about that, but let’s not get ahead of ourselves; first, let’s get some intuition for how MCMC methods work. This will help us understand the diagnostic later. To understand what MCMC methods are, we are going to split the method into the ”two MC parts”: the Monte Carlo part and the Markov chain part. <span id="x1-193002r437"/></p>&#13;
<section id="monte-carlo" class="level4 subsectionHead" data-number="1.14.4.1">&#13;
<h3 class="subsectionHead" data-number="1.14.4.1">10.4.1 <span id="x1-1940001"/>Monte Carlo</h3>&#13;
<p><span id="dx1-194001"/></p>&#13;
<p>The use of random numbers explains the Monte Carlo part of the name. Monte Carlo methods are a very broad family of algorithms that use random sampling to compute or simulate a given process. Monte Carlo is a very famous casino located in the Principality of Monaco. One of the developers of the Monte Carlo method, Stanislaw Ulam, had an uncle who used to gamble there. The key idea Stan had was that while many problems are difficult to solve or even formulate in an exact way, they can be effectively studied by taking samples from them. In fact, as the story goes, the motivation was to answer questions about the probability of getting a particular hand in a game of Solitaire.</p>&#13;
<p>One way to solve this problem is to follow the analytical combinatorial problem. Another way, Stanislaw argued, is to play several games of Solitaire and count how many of the hands that we play match the particular hand we are interested in! Maybe this sounds obvious to you, or at least pretty reasonable; you may even have used resampling methods to solve statistical problems. But, remember this mental experiment was performed about 70 years ago, a time when the first practical computers were beginning to be developed!</p>&#13;
<p>The first application of the Monte Carlo method was to solve a problem of nuclear physics, a hard-to-tackle problem using the tools at the time. Nowadays, even personal computers are powerful enough to solve many interesting problems using the Monte Carlo approach; hence, these methods are applied to a wide variety of problems in science, engineering, industry, and the arts. A classic pedagogical example of using a Monte Carlo method to compute a quantity of interest is the numerical estimation of the number <em>π</em>. In practice, there are better methods for this particular computation, but its pedagogical value remains.</p>&#13;
<p>We can estimate the value of <em>π</em> with the following procedure:</p>&#13;
<ol>&#13;
<li><div id="x1-194003x1">&#13;
<p>Throw <em>N</em> points at random into a square of side 2<em>R</em>.</p>&#13;
</div></li>&#13;
<li><div id="x1-194005x2">&#13;
<p>Draw a circle of radius <em>R</em> inscribed in the square and count the number of points <em>M</em> inside the circle.</p>&#13;
</div></li>&#13;
<li><div id="x1-194007x3">&#13;
<p>Compute <img src="../media/hat_Pi.png" style="width:0.75em;"/> as the ratio 4<img src="../media/file251.jpg" class="frac" data-align="middle" alt="MN-"/>.</p>&#13;
</div></li>&#13;
</ol>&#13;
<p>Here are a few notes:</p>&#13;
<ul>&#13;
<li><p>The area of the circle is proportional to the number of points inside it (<em>M</em>) and the area of the square is proportional to the total points (<em>N</em>).</p></li>&#13;
<li><p>We know a point is inside a circle if the following relation holds: <img src="../media/file252.jpg" class="sqrt" alt="∘ (x2-+-y2) ≤-R-"/>.</p></li>&#13;
<li><p>The area of the square is (2<em>R</em>)<sup>2</sup> and the area of the circle is <em>πR</em><sup>2</sup>. Thus, we know that the ratio of the area of the square to the area of the circle is <em>π</em>.</p></li>&#13;
</ul>&#13;
<p>Using a few lines of Python, we can run this simple Monte Carlo simulation and compute <em>π</em>, and also the relative error of our estimate compared to the true value of <em>π</em>. The result of a run is shown in <em>Figure <a href="#x1-194008r4">10.4</a></em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file253.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-194008r4"/><strong>Figure 10.4</strong>: A Monte Carlo approximation of <em>π</em></p>&#13;
<p><span id="x1-194009r459"/></p>&#13;
</section>&#13;
<section id="markov-chain" class="level4 subsectionHead" data-number="1.14.4.2">&#13;
<h3 class="subsectionHead" data-number="1.14.4.2">10.4.2 <span id="x1-1950002"/>Markov chain</h3>&#13;
<p><span id="dx1-195001"/></p>&#13;
<p>A Markov chain is a mathematical object that consists of a sequence of states and a set of transition probabilities that describe how to move among the states. You can create a Markov chain yourself; flip a coin and if you get heads take a step to the right, otherwise step to the left. That is a simple 1-dimensional Markov chain. A chain is Markovian if the probability of moving to any other state depends only on the current state.</p>&#13;
<p>As a practitioner, you just need to know that Markov chains provide a framework to study the properties of MCMC samplers (among other useful applications). They are not that hard to understand, at least not the most basic properties. But going into the details is not that useful for you as a modeler and thus we will not discuss them any further. You can check <a href="Bibliography.xhtml#Xblitzstein_2019">Blitzstein</a> [<a href="Bibliography.xhtml#Xblitzstein_2019">2019</a>] for a nice intro if you want.</p>&#13;
<p>The most popular MCMC method is probably the Metropolis-Hasting algorithm, and we will discuss it in the following section. <span id="x1-195002r460"/></p>&#13;
</section>&#13;
<section id="metropolis-hastings" class="level4 subsectionHead" data-number="1.14.4.3">&#13;
<h3 class="subsectionHead" data-number="1.14.4.3">10.4.3 <span id="x1-1960003"/>Metropolis-Hastings</h3>&#13;
<p><span id="dx1-196001"/></p>&#13;
<p>For some distributions, such as the Gaussian, we have very efficient algorithms to get samples from, but for other distributions, this is not the case. Metropolis-Hastings enables us to obtain samples from any probability distribution given that we can compute at least a value proportional to it, thus ignoring the normalization factor. This is very useful since many times the harder part is precisely to compute the normalization factor. This is the case with Bayesian statistics, where the computation of the marginal likelihood can be a deal-breaker.</p>&#13;
<p>To conceptually understand this method, we are going to use the following analogy. Suppose we are interested in finding the volume of water in a lake and which part of the lake has the deepest point. The water is really muddy so we can’t estimate the depth just by looking through the water to the bottom, and the lake is really big, so a grid approximation does not seem like a very good idea. To develop a sampling strategy, we seek help from two of our best friends: Markovia and Monty. After a fruitful discussion, they came up with the following algorithm, which requires a boat—nothing fancy, we can even use a wooden raft and a very long stick. This is cheaper than sonar and we have already spent all our money on the boat, anyway! Check out these steps:</p>&#13;
<ol>&#13;
<li><div id="x1-196003x1">&#13;
<p>Choose a random place in the lake, and move the boat there.</p>&#13;
</div></li>&#13;
<li><div id="x1-196005x2">&#13;
<p>Use the stick to measure the depth of the lake.</p>&#13;
</div></li>&#13;
<li><div id="x1-196007x3">&#13;
<p>Move the boat to another point and take a new measurement.</p>&#13;
</div></li>&#13;
<li><div id="x1-196009x4">&#13;
<p>Compare the two measures in the following way:</p>&#13;
<ol>&#13;
<li><div id="x1-196011x1">&#13;
<p>If the new spot is deeper than the first one, write down in your notebook the depth of the new spot and repeat from step 3.</p>&#13;
</div></li>&#13;
<li><div id="x1-196013x2">&#13;
<p>If the spot is shallower than the first one, we have two options: to accept or reject. Accepting means we write down the depth of the new spot and repeat from step 3. Rejecting means we go back to the first spot and write down (yes, again!) the value for the depth of the first spot.</p>&#13;
</div></li>&#13;
</ol>&#13;
</div></li>&#13;
</ol>&#13;
<p>The rule for deciding whether to accept or reject is known as the Metropolis-Hastings criteria, and it basically says that we must accept the new spot with a probability that is proportional to the ratio of the depth of the new and old spots.</p>&#13;
<p>If we follow this iterative procedure, we will get not only the total volume of the lake and the deepest point, but also an approximation of the entire curvature of the bottom of the lake. As you may have guessed, in this analogy, the curvature of the bottom of the lake is the posterior distribution and the deepest point is the mode. According to our friend Markovia, the larger the number of iterations, the better the approximation. Indeed, the theory guarantees that under certain general circumstances, we are going to get the exact answer if we get an infinite number of samples. Luckily for us, in practice, and for many, many problems, we can get a very accurate approximation using a finite and relatively small number of samples.</p>&#13;
<p>The preceding explanation is enough to get a conceptual-level understanding of Metropolis-Hastings. The next few pages contain a more detailed and formal explanation in case you want to dig deeper.</p>&#13;
<p>The Metropolis-Hastings algorithm has the following steps:</p>&#13;
<ol>&#13;
<li><div id="x1-196015x1">&#13;
<p>Choose an initial value for a parameter <em>x</em><sub><em>i</em></sub>. This can be done randomly or by making an educated guess.</p>&#13;
</div></li>&#13;
<li><div id="x1-196017x2">&#13;
<p>Choose a new parameter value <em>x</em><sub><em>i</em>+1</sub>, by sampling from <em>Q</em>(<em>x</em><sub><em>i</em>+1</sub><span class="cmsy-10x-x-109">|</span><em>x</em><sub><em>i</em></sub>). We can think of this step as perturbing the state <em>x</em><sub><em>i</em></sub> somehow.</p>&#13;
</div></li>&#13;
<li><div id="x1-196019x3">&#13;
<p>Compute the probability of accepting a new parameter value by using the Metropolis-Hastings criteria:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file254.jpg" class="math-display" alt=" ( ) pa(xi+1 | xi) = min 1, p(xi+1) q(xi-| xi+1) p(xi) q(xi+1 | xi) "/>&#13;
</div>&#13;
</div></li>&#13;
<li><div id="x1-196021x4">&#13;
<p>If the probability computed in step 3 is larger than the value taken from a Uniform distribution on the [0, 1] interval, we accept the new state; otherwise, we stay in the old state.</p>&#13;
</div></li>&#13;
<li><div id="x1-196023x5">&#13;
<p>We iterate from step 2 until we have enough samples.</p>&#13;
</div></li>&#13;
</ol>&#13;
<p>Here are a couple of notes to take into account:</p>&#13;
<ul>&#13;
<li><p><em>Q</em> is called the proposal distribution. It can be anything we want, but it makes sense that we choose a distribution that we find simple to sample from, such as a Gaussian or Uniform distribution.</p></li>&#13;
<li><p>Note that <em>Q</em> is not the prior or likelihood or any part of the model. It is a component of the MCMC method, not of the model.</p></li>&#13;
<li><p>If <em>Q</em> is symmetric, the terms <em>q</em>(<em>x</em><sub><em>i</em></sub><span class="cmsy-10x-x-109">|</span><em>x</em><sub><em>i</em>+1</sub>) and <em>q</em>(<em>x</em><sub><em>i</em>+1</sub><span class="cmsy-10x-x-109">|</span><em>x</em><sub><em>i</em></sub>) will cancel out. Hence we will just need to evaluate the ratio <img src="../media/file255.jpg" class="frac" data-align="middle" alt="p(xi+1) p(xi)"/>.</p></li>&#13;
<li><p>Step 3 and step 4 imply that we will always accept moving to a more probable state. Less probable parameter values are accepted probabilistically, given the ratio between the probability of the new parameter value <em>x</em><sub><em>i</em>+1</sub> and the old parameter value <em>x</em><sub><em>i</em></sub>. This criteria for accepting proposed steps gives us a more efficient sampling approach compared to the grid method while ensuring a correct sampling.</p></li>&#13;
<li><p>The target distribution (the posterior distribution in Bayesian statistics) is approximated by a list of sampled parameter values. If we accept, we add <em>x</em><sub><em>i</em>+1</sub> to the list of the new sampled values. If we reject, we add <em>x</em><sub><em>i</em></sub> to the list, even if the value is repeated.</p></li>&#13;
</ul>&#13;
<p>At the end of the process, we will have a list of values. If everything was done the right way, these samples would be an approximation of the posterior. The most frequent values in our trace will be the most probable values according to the posterior. An advantage of this procedure is that analyzing the posterior is as simple as manipulating an array of values, as you have already experimented with in all the previous chapters.</p>&#13;
<p>The following code illustrates a very basic implementation of the Metropolis algorithm. It is not meant to solve a real problem, only to show it is possible to sample from a probability distribution if we know how to compute its density point-wise. Notice that the following implementation has nothing Bayesian in it; there is no prior and we do not even have data! Remember that MCMC methods are very general algorithms that can be applied to a broad array of problems.</p>&#13;
<p>The first argument of the metropolis function is a PreliZ distribution; we are assuming we do not know how to directly get samples from this distribution:</p>&#13;
<p><span id="x1-196024r3"/> <span id="x1-196025"/><strong>Code 10.3</strong></p>&#13;
<pre id="listing-128" class="source-code"><code>def metropolis(dist, draws=1000): </code>&#13;
<code>    """A very simple Metropolis implementation""" </code>&#13;
<code>    trace = np.zeros(draws) </code>&#13;
<code>    old_x = 0.5 </code>&#13;
<code>    old_prob = dist.pdf(old_x) </code>&#13;
<code> </code>&#13;
<code>    delta = np.random.normal(0, 0.5, draws) </code>&#13;
<code>    for i in range(draws): </code>&#13;
<code>        new_x = old_x + delta[i] </code>&#13;
<code>        new_prob = dist.pdf(new_x) </code>&#13;
<code>        acceptance = new_prob / old_prob </code>&#13;
<code>        if acceptance &gt;= np.random.random(): </code>&#13;
<code>            trace[i] = new_x </code>&#13;
<code>            old_x = new_x </code>&#13;
<code>            old_prob = new_prob </code>&#13;
<code>        else: </code>&#13;
<code>            trace[i] = old_x </code>&#13;
<code>    return trace</code></pre>&#13;
<p>The result of our simple metropolis algorithm is shown in <em>Figure <a href="#x1-196044r5">10.5</a></em>. The black line shows the true distribution while the bars show the samples we computed.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file256.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-196044r5"/><strong>Figure 10.5</strong>: Samples from a simple metropolis algorithm</p>&#13;
<p>The efficiency of the algorithm depends heavily on the proposal distribution; if the proposed state is very far away from the current state, the chance of rejection is very high, and if the proposed state is very close, we explore the parameter space very slowly. In both scenarios, we will need many more samples than for a less extreme situation. Usually, the proposal is a multivariate Gaussian distribution whose covariance matrix is determined during the tuning phase. PyMC tunes the covariance adaptively by following the rule of thumb that the ideal acceptance is around 50% for a unidimensional Gaussian and around 23% for an n-dimensional Gaussian target distribution.</p>&#13;
<p>MCMC methods often take some time before they start getting samples from the target distribution. So, in practice, people perform a burn-in step, which consists of eliminating the first portion of the samples. Doing a burn-in is a practical trick and not part of the Markovian theory; in fact, it will not be necessary for an infinite sample. Thus, removing the first portion of the samples is just an <em>ad hoc</em> trick to get better results, given that we can only compute a finite sample. Having theoretical guarantees or guidance is better than not having them, but for any practical problem, it is important to understand the difference between theory and practice. Remember, we should not get confused by mixing mathematical objects with the approximation of those objects. Spheres, Gaussians, Markov chains, and all the mathematical objects live only in the Platonic world of ideas, not in our imperfect, real world.</p>&#13;
<p>At this point, I hope you have a good conceptual grasp of the Metropolis-Hastings method. You may need to go back and read this section a couple of times; that’s totally fine. The main ideas are simple but also subtle. <span id="x1-196045r461"/></p>&#13;
</section>&#13;
<section id="hamiltonian-monte-carlo" class="level4 subsectionHead" data-number="1.14.4.4">&#13;
<h3 class="subsectionHead" data-number="1.14.4.4">10.4.4 <span id="x1-1970004"/>Hamiltonian Monte Carlo</h3>&#13;
<p><span id="dx1-197001"/> <span id="dx1-197002"/></p>&#13;
<p>MCMC methods, including Metropolis-Hastings, come with the theoretical guarantee that if we take enough samples, we will get an accurate approximation of the correct distribution. However, in practice, it could take more time than we have to get enough samples. For that reason, alternatives to the Metropolis-Hastings algorithm have been proposed.</p>&#13;
<p>Many of those alternative methods, such as the Metropolis-Hastings algorithm itself, were developed originally to solve problems in statistical mechanics, a branch of physics that studies properties of atomic and molecular systems, and thus can be interpreted in a very natural way using analogies of physical systems. One such modification is known as <strong>Hamiltonian Monte Carlo</strong>, or <strong>Hybrid</strong> <strong>Monte Carlo</strong> (<strong>HMC</strong>). In simple terms, a Hamiltonian is a description of the total energy of a physical system. The term <em>hybrid</em> is also used because it was originally conceived as a hybridization of Metropolis-Hastings and molecular mechanics, a widely used simulation technique for molecular systems.</p>&#13;
<p>Conceptually, we can think of the HMC method as a Metropolis-Hastings but with a proposal distribution that is not random. To get a general conceptual understanding of HMC without going into the mathematical details, let’s use the lake and boat analogy again. Instead of moving the boat randomly, we do so by following the curvature of the bottom of the lake. To decide where to move the boat, we let a ball roll onto the bottom of the lake starting from our current position. Our ball is a very special one: not only is it perfectly spherical, it also has no friction and thus is not slowed down by the water or mud. We throw the ball and let it roll for a short moment until we suddenly stop it. Then we accept or reject this proposed step using the Metropolis criteria, just as we did in the vanilla Metropolis-Hastings method. Then the whole procedure is repeated many times. Nicely, this modified procedure results in a higher chance of accepting new positions, even if they are far away relative to the previous position.</p>&#13;
<p>Moving according to the curvature of the parameter space turns out to be a smarter way of moving because it avoids one of the main drawbacks of Metropolis-Hastings: an efficient exploration of the sample space requires rejecting most of the proposed steps. Instead, using HMC, it is possible to get a high acceptance rate even for faraway points in the parameter space, thus resulting in a very efficient sampling method.</p>&#13;
<p>Let’s get out of our Gedankenexperiment and back to the real world. We have to pay a price for this very clever Hamiltonian-based proposal. We need to compute the gradients of our function. A gradient is the generalization of the concept of the derivative to more than one dimension; computing the derivative of a function at one point tells us in which direction the function increases and in which direction it decreases. We can use gradient information to simulate the ball moving in a curved space; in fact, we use the same laws of motion and mathematical machinery used in classical physics to simulate classical mechanical systems, such as balls rolling, the orbits in planetary systems, and the jiggling of molecules.</p>&#13;
<p>Computing gradients make us face a trade-off; each HMC step is more expensive to compute than a Metropolis-Hastings step, but the probability of accepting that step is much higher with HMC than with Metropolis. To balance this trade-off in favor of HMC, we need to tune a few parameters of the HMC model (in a similar fashion to how we need to tune the width of the proposal distribution for an efficient Metropolis-Hastings sampler). When this tuning is done by hand, it takes some trial and error and also requires an experienced user, making this procedure a less universal inference engine than we may want. Luckily for us, modern probabilistic programming languages come equipped with efficient adaptive Hamiltonian Monte Carlo methods, such as the NUTS sampler in PyMC. This method has proven remarkably useful and efficient for solving Bayesian models without requiring human intervention (or at least minimizing it).</p>&#13;
<p>One caveat of Hamiltonian Monte Carlo methods is that they only work for continuous distribution; the reason is that we cannot compute gradients for discrete distribution. PyMC solves this problem by assigning NUTS to continuous parameters and other samplers to other parameters, such as PGBART for BART random variables or Metropolis to discrete ones.</p>&#13;
<div id="tcolobox-21" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>JAX-Based Sampling</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<span id="dx1-197003"/>&#13;
<p>JAX is a library designed to provide high-performance numerical computing and automatic differentiation for complex mathematical operations. PyMC use a Python version of NUTS. But you can also use JAX-based implementations of this sampler. Depending on your model, these samplers can be much faster than the default NUTS sampler from PyMC. To used them we need to specify the argument <code>nuts_sampler </code>for <code>pm.sample()</code>. The currently supported options are <code>"nutpie"</code>, <code>"blackjax"</code>, and <code>"numpyro"</code>. None of these three samples comes installed with PyMC by default, so you will need to install them. For CPUs, nutpie is probably the faster option available: <a href="https://github.com/pymc-devs/nutpie" class="url">https://github.com/pymc-devs/nutpie</a>. In this book, we used nutpie to sample from GPs – see the Jupyter notebooks for <em>Chapter <a href="CH08.xhtml#x1-1560008">8</a></em>.</p>&#13;
</div>&#13;
</div>&#13;
<p>I strongly recommend you complement this section with this very cool application by Chi Feng: <a href="https://chi-feng.github.io/mcmc-demo" class="url">https://chi-feng.github.io/mcmc-demo</a>. <span id="x1-197004r458"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="sequential-monte-carlo-1" class="level3 sectionHead" data-number="1.14.5">&#13;
<h2 class="sectionHead" data-number="1.14.5">10.5 <span id="x1-1980005"/>Sequential Monte Carlo</h2>&#13;
<p><span id="dx1-198001"/></p>&#13;
<p>One of the caveats of Metropolis-Hastings and NUTS (and other Hamiltonian Monte Carlo variants) is that if the posterior has multiple peaks and these peaks are separated by regions of very low probability, these <span id="dx1-198002"/>methods can get stuck in a single mode and miss the others!</p>&#13;
<p>Many of the methods developed to overcome this multiple minima problem are based on the idea of tempering. This idea, once again, is borrowed from statistical mechanics. The number of states a physical system can populate depends on the temperature of the system; at 0 Kelvin (the lowest possible temperature), every system is stuck in a single state. On the other extreme, for an infinite temperature, all possible states are equally likely. Generally, we are interested in systems at some intermediate temperature. For Bayesian models, there is a very intuitive way to adapt this tempering idea by writing Bayes’ theorem with a twist.</p>&#13;
<div class="math-display">&#13;
<img src="../media/file257.jpg" class="math-display" alt=" β p(θ | y)β = p(y | θ) p(θ) "/>&#13;
</div>&#13;
<p>The parameter <em>β</em> is known as the inverse temperature or tempering parameter. Notice that for <em>β</em> = 0 we get <em>p</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>θ</em>)<sup><em>β</em></sup> = 1 and thus the tempered posterior <em>p</em>(<em>θ</em><span class="cmsy-10x-x-109">|</span><em>y</em>)<sub><em>β</em></sub> is just the prior <em>p</em>(<em>θ</em>), and when <em>β</em> = 1 the <em>tempered</em> posterior is the actual full posterior. As sampling from the prior is generally easier than sampling from the posterior (by increasing the value of <em>β</em>), we start sampling from an easier distribution and slowly morph it into the more complex distribution we really care about.</p>&#13;
<p>There are many methods that <span id="dx1-198003"/>exploit this idea; one of them is known as <strong>Sequential Monte Carlo</strong> (<strong>SMC</strong>). The SMC method, as implemented in PyMC, can be summarized as follows (also see <em>Figure <a href="#x1-198018r6">10.6</a></em>):</p>&#13;
<ol>&#13;
<li><div id="x1-198005x1">&#13;
<p>Initialize <em>β</em> at 0.</p>&#13;
</div></li>&#13;
<li><div id="x1-198007x2">&#13;
<p>Generate <em>N</em> samples <em>S</em><sub><em>β</em></sub> from the tempered posterior.</p>&#13;
</div></li>&#13;
<li><div id="x1-198009x3">&#13;
<p>Increase <em>β</em> a <em>little bit</em>.</p>&#13;
</div></li>&#13;
<li><div id="x1-198011x4">&#13;
<p>Compute a set of <em>N</em> weights <em>W</em>. The weights are computed according to the new tempered posterior.</p>&#13;
</div></li>&#13;
<li><div id="x1-198013x5">&#13;
<p>Obtain <em>S</em><sub><em>w</em></sub> by resampling <em>S</em><sub><em>b</em></sub> according to <em>W</em>.</p>&#13;
</div></li>&#13;
<li><div id="x1-198015x6">&#13;
<p>Run <em>N</em> Metropolis chains, starting each one from a different sample in <em>S</em><sub><em>w</em></sub>.</p>&#13;
</div></li>&#13;
<li><div id="x1-198017x7">&#13;
<p>Repeat from step 3 until <em>β</em> <span class="cmsy-10x-x-109">≥ </span>1.</p>&#13;
</div></li>&#13;
</ol>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file258.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-198018r6"/><strong>Figure 10.6</strong>: Schematic representation of SMC</p>&#13;
<p>The resampling step works by removing samples with a low probability and replacing them with samples with a higher probability. The Metropolis step perturbs these samples, helping to explore the parameter space.</p>&#13;
<p>The efficiency of the tempered method depends heavily on the intermediate values of <em>β</em>, which is usually referred to as the cooling schedule. The smaller the difference between two successive values of <em>β</em>, the closer the two successive tempered posteriors will be, and thus the easier the transition from one stage to the next. But if the steps are too small, we will need many intermediate stages, and beyond some point, this will translate into wasting a lot of computational <span id="dx1-198019"/>resources without really improving the accuracy of the results.</p>&#13;
<p>Fortunately, SMC can automatically compute the intermediate values of <em>β</em>. The exact cooling schedule will be adapted to the difficulty of the problem; distributions that are more difficult to sample will require more stages than simpler ones.</p>&#13;
<p>At the top of <em>Figure <a href="#x1-198018r6">10.6</a></em>, we have nine samples or particles (gray dots) that we obtained from the prior, represented as the very wide distribution on top of everything (stage 0). For the rest of the stages, we re-weight the samples from the previous stage according to their tempered posterior density. And then we resample proportional to those weights. As a result, some particles are lost and replaced by other samples, so the total number is fixed. We then mutate the sample, that is, we apply one or more MCMC steps to the particles. We then increase <em>β</em> and repeat. When we reach <em>β</em> = 1, the particles (or samples) will be distributed as the posterior.</p>&#13;
<p>Besides the <span id="dx1-198020"/>intermediate values of <em>β</em>, two more parameters are dynamically computed based on the acceptance rate of the previous stage: the number of steps of each Markov chain and the width of the proposal distribution. <span id="x1-198021r465"/></p>&#13;
</section>&#13;
<section id="diagnosing-the-samples" class="level3 sectionHead" data-number="1.14.6">&#13;
<h2 class="sectionHead" data-number="1.14.6">10.6 <span id="x1-1990006"/>Diagnosing the samples</h2>&#13;
<p><span id="dx1-199001"/></p>&#13;
<p>In this book, we have used numerical methods to compute the posterior for virtually all models. That will most likely be the case for you, too, when using Bayesian methods for your own problems. Since we are approximating the posterior with a finite number of samples, it is important to check whether we have a valid sample; otherwise, any analysis from it will be totally flawed. There are several tests we can perform, some of which are visual and others quantitative. These tests are designed to spot problems with our samples, but they are unable to prove we have the correct distribution; they can only provide evidence that the sample seems reasonable. If we find problems with the sample, there are many solutions to try. We will discuss them along with the diagnostics.</p>&#13;
<p>To make the explanations concrete, we are going to use minimalist models, with two parameters: a global parameter <em>a</em> and a group parameter <em>b</em>. And that’s it, we do not even have likelihood/data in these models!</p>&#13;
<p><span id="x1-199002r4"/> <span id="x1-199003"/><strong>Code 10.4</strong></p>&#13;
<pre id="listing-129" class="source-code"><code>with pm.Model() as model_c: </code>&#13;
<code>    a = pm.HalfNormal('a', 10) </code>&#13;
<code>    b = pm.Normal('b', 0, a, shape=10) </code>&#13;
<code>    idata_cm = pm.sample(tune=2000) </code>&#13;
<code> </code>&#13;
<code>with pm.Model() as model_nc: </code>&#13;
<code>    a = pm.HalfNormal('a', 10) </code>&#13;
<code> </code>&#13;
<code>    b_offset = pm.Normal('b_offset', mu=0, sigma=1, shape=10) </code>&#13;
<code>    b = pm.Deterministic('b', 0 + b_offset * a) </code>&#13;
<code>    idata_ncm = pm.sample(tune=2000)</code></pre>&#13;
<p>The difference between <code>model_c </code>and <code>model_nc </code>models is that for the former, we fit the group-level parameter directly, and for the latter, we model the group-level parameter as a shifted and scaled Gaussian.</p>&#13;
<p>These two models may look too artificial to you, or just weird. However, it is important to notice that these two models have essentially the same structure as the centered and non-centered parametrization we already discussed in <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>.</p>&#13;
<p>From the discussion in that chapter, we should expect better samples from <code>model_nc </code>than from <code>model_c</code>. Let’s check if our expectations hold. <span id="x1-199015r466"/></p>&#13;
</section>&#13;
<section id="convergence" class="level3 sectionHead" data-number="1.14.7">&#13;
<h2 class="sectionHead" data-number="1.14.7">10.7 <span id="x1-2000007"/>Convergence</h2>&#13;
<p><span id="dx1-200001"/></p>&#13;
<p>Theoretically, MCMC methods are guaranteed to converge once we take infinite samples. In practice, we need to check that we have reasonable finite samples. Usually, we say the sampler has converged once we have collected evidence showing that samples are <em>stable</em> in some sense. A simple test to do is to run the same MCMC simulation multiple times and check whether we get the same result every time. This is the reason why PyMC runs more by default than on chain. For modern computers, this is virtually free as we have multiple cores. Also, they do not create any waste, as we can combine samples from different chains to compute summaries, plots, etc.</p>&#13;
<p>There are many ways to check that different chains are practically equivalent, both visually and with formal tests. We are not going to get too technical here; we are just going to show a few examples and hope they are enough for you to develop an intuition for interpreting diagnostics. <span id="x1-200002r464"/></p>&#13;
<section id="trace-plot" class="level4 subsectionHead" data-number="1.14.7.1">&#13;
<h3 class="subsectionHead" data-number="1.14.7.1">10.7.1 <span id="x1-2010001"/>Trace plot</h3>&#13;
<p><span id="dx1-201001"/> <span id="dx1-201002"/></p>&#13;
<p>One way to check for convergence is to visually check whether chains look similar. For instance, we can use ArviZ’s <code>plot_trace </code>function. To better understand what we should look for when inspecting these plots, let’s compare the results for the two previously defined models.</p>&#13;
<p>The variable <code>b </code>is 10-dimensional. For clarity and brevity we are only going to show one of its dimensions. Feel free to visualize all of them on your own computer. <em>Figure <a href="#x1-201003r7">10.7</a></em> shows many issues. In the left column, we have four KDEs, one per chain. We can see that they look different. This is an indication that each chain is sampling slightly different regions of the posterior. In the right column, we have the trace itself. We also have four lines, one per chain, which can be messy, but still we see that one chain is stuck in the neighborhood of 0 from the first step until almost step 400. We see something similar at step <span class="cmsy-10x-x-109">≈</span>800.</p>&#13;
<p>The issues become even more clear when we compare <em>Figure <a href="#x1-201003r7">10.7</a></em> with <em>Figure <a href="#x1-201004r8">10.8</a></em>. For the latter, we see that the KDEs for the four chains look much more similar to each other, and the trace looks much more fuzzy, more like <em>noise</em>, and very difficult to see a pattern. We want a curve freely meandering around. When this happens, we say we have <strong>good</strong> <strong>mixing</strong>. We express it like this because it will be difficult to distinguish one chain from the other; they are mixed. This is good because it means that even when we run four (or more) separated chains starting from different points, they all describe the same distribution. This is not proof of convergence but at least we don’t see evidence of non-convergence or poor mixing.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file259.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-201003r7"/><strong>Figure 10.7</strong>: Trace plot for <code>model_c</code></p>&#13;
<p><em>Figure <a href="#x1-201003r7">10.7</a></em> also has a few black vertical bars at the top that are absent from <em>Figure <a href="#x1-201004r8">10.8</a></em>. These are divergences; there is a section dedicated to them later in this chapter.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file260.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-201004r8"/><strong>Figure 10.8</strong>: Trace plot for <code>model_nc</code></p>&#13;
<p><span id="x1-201005r470"/></p>&#13;
</section>&#13;
<section id="rank-plot" class="level4 subsectionHead" data-number="1.14.7.2">&#13;
<h3 class="subsectionHead" data-number="1.14.7.2">10.7.2 <span id="x1-2020002"/>Rank plot</h3>&#13;
<p><span id="dx1-202001"/> <span id="dx1-202002"/></p>&#13;
<p>Trace plots can be difficult to read, especially when we have multiple chains as it is easy to miss some details. An alternative is rank plots [<a href="Bibliography.xhtml#Xvehtari_2021">Vehtari et al.</a>, <a href="Bibliography.xhtml#Xvehtari_2021">2021</a>]. To build a rank plot for a given parameter we first take all the samples from all the chains, order them, and assign an integer: this is sample 0, this is 1, this is 2, etc. We then group all the ranks according to the original chains. Finally, we plot as many histograms as chains. If all chains are sampled from the same distribution, we can expect that all chains have the same number of low ranks, high ranks, medium ranks, etc. In other words, a histogram of the rank should be uniform.</p>&#13;
<p>To get a rank plot we can call ArviZ’s <code>plot_trace </code>with the <code>kind="rank_bars"</code> argument. Figures <a href="#x1-202003r9">10.9</a> and <a href="#x1-202004r10">10.10</a> are examples of such plots.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file261.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-202003r9"/><strong>Figure 10.9</strong>: Rank plot for <code>model_c</code></p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file262.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-202004r10"/><strong>Figure 10.10</strong>: Rank plot for <code>model_nc</code></p>&#13;
<p>On the left, we have the same KDEs we have already shown. On the right, we have the rank plots. Again the result for <code>model_nc </code>looks much better; the deviations from uniformity are very small. On the other hand, we can see a few issues from <em>Figure <a href="#x1-202003r9">10.9</a></em>; for instance, the histograms for rank 500 or lower look very bad for parameter <code>a </code>and also very bad for parameter <code>b </code>around the rank 2000. There are issues in other regions as well.</p>&#13;
<p><span id="x1-202005r471"/></p>&#13;
</section>&#13;
<section id="r-r-hat" class="level4 subsectionHead" data-number="1.14.7.3">&#13;
<h3 class="subsectionHead" data-number="1.14.7.3">10.7.3 <span id="x1-2030003"/><img src="../media/hat_R.png" style="width:0.65em;"/> (R hat)</h3>&#13;
<p><span id="dx1-203001"/> <span id="dx1-203002"/></p>&#13;
<p>A quantitative way of comparing independent chains is by using the <img src="../media/hat_R.png" style="width:0.65em;"/> statistic. The idea of this test is to compute the variance between chains with the variance within chains. Ideally, we should expect a value of 1. As an empirical rule, we will be OK with a value below 1.01; higher values signal a lack of convergence. We can compute it using the <code>az.r_hat </code>function (see <em>Table <a href="#x1-203003r1">10.1</a></em>). The <img src="../media/hat_R.png" style="width:0.65em;"/> diagnostic is also computed by default with the <code>az.summary</code> function and optionally with <code>az.plot_forest </code>(using the <code>r_hat=True</code> argument).</p>&#13;
<table id="TBL-13" class="tabular">&#13;
<tbody>&#13;
<tr id="TBL-13-1-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-13-1-1" class="td11" style="text-align: left; white-space: nowrap;"/>&#13;
<td id="TBL-13-1-2" class="td11" style="text-align: center; white-space: nowrap;"><em>a</em></td>&#13;
<td id="TBL-13-1-3" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">0</span></sub></td>&#13;
<td id="TBL-13-1-4" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">1</span></sub></td>&#13;
<td id="TBL-13-1-5" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">2</span></sub></td>&#13;
<td id="TBL-13-1-6" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">3</span></sub></td>&#13;
<td id="TBL-13-1-7" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">4</span></sub></td>&#13;
<td id="TBL-13-1-8" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">5</span></sub></td>&#13;
<td id="TBL-13-1-9" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">6</span></sub></td>&#13;
<td id="TBL-13-1-10" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">7</span></sub></td>&#13;
<td id="TBL-13-1-11" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">8</span></sub></td>&#13;
<td id="TBL-13-1-12" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">9</span></sub></td>&#13;
</tr>&#13;
<tr id="TBL-13-2-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-13-2-1" class="td11" style="text-align: left; white-space: nowrap;">model_c</td>&#13;
<td id="TBL-13-2-2" class="td11" style="text-align: center; white-space: nowrap;">1.2</td>&#13;
<td id="TBL-13-2-3" class="td11" style="text-align: center; white-space: nowrap;">1.17</td>&#13;
<td id="TBL-13-2-4" class="td11" style="text-align: center; white-space: nowrap;">1.05</td>&#13;
<td id="TBL-13-2-5" class="td11" style="text-align: center; white-space: nowrap;">1.17</td>&#13;
<td id="TBL-13-2-6" class="td11" style="text-align: center; white-space: nowrap;">1.17</td>&#13;
<td id="TBL-13-2-7" class="td11" style="text-align: center; white-space: nowrap;">1.15</td>&#13;
<td id="TBL-13-2-8" class="td11" style="text-align: center; white-space: nowrap;">1.11</td>&#13;
<td id="TBL-13-2-9" class="td11" style="text-align: center; white-space: nowrap;">1.09</td>&#13;
<td id="TBL-13-2-10" class="td11" style="text-align: center; white-space: nowrap;">1.17</td>&#13;
<td id="TBL-13-2-11" class="td11" style="text-align: center; white-space: nowrap;">1.18</td>&#13;
<td id="TBL-13-2-12" class="td11" style="text-align: center; white-space: nowrap;">1.17</td>&#13;
</tr>&#13;
<tr id="TBL-13-3-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-13-3-1" class="td11" style="text-align: left; white-space: nowrap;">model_nc</td>&#13;
<td id="TBL-13-3-2" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-3" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-4" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-5" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-6" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-7" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-8" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-9" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-10" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-11" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
<td id="TBL-13-3-12" class="td11" style="text-align: center; white-space: nowrap;">1.0</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="IMG---Caption"><span id="x1-203003r1"/> <span id="x1-203004"/><strong>Table 10.1</strong>: <img src="../media/hat_R.png" style="width:0.65em;"/><span> values for models <code>model_c </code>and <code>model_ncm</code> </span></p>&#13;
<p>Values around 1.1 could be OK, at the initial phase of modeling, when you are just checking whether a likelihood makes sense, or just trying to find out which model you really want to build. Also, the threshold 1.01 could be too tight for a model with a lot of parameters. The reason is that even when you really have convergence, you could still get a few <img src="../media/hat_R.png" style="width:0.65em;"/> values larger than this threshold by chance. For instance, the PyMC-BART package includes the <code>plot_convergence</code> function. This function is intended to check the convergence of BART random variables. When using a BART model, you will get one <img src="../media/hat_R.png" style="width:0.65em;"/> per observation, and that could be a lot. Thus, <code>plot_convergence </code>shows the cumulative distribution of <img src="../media/hat_R.png" style="width:0.65em;"/> values and a threshold that includes a correction for multiple comparisons that is automatically computed by taking into account the number of observations.</p>&#13;
<p><em>Figure <a href="#x1-203005r11">10.11</a></em> shows an example of such a plot. On the right, we have a cumulative distribution of <img src="../media/hat_R.png" style="width:0.65em;"/>s and a gray dashed line showing the adjusted threshold. Ideally, the entire cumulative curve should be to the left of the dashed line. On the left subplot of <em>Figure <a href="#x1-203005r11">10.11</a></em>, we have the <strong>Effective Sample Size</strong> (<strong>ESS</strong>). We explain the ESS in the next section.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file263.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-203005r11"/><strong>Figure 10.11</strong>: Diagnostic plot computed with <code>pmb.plot_convergence(.)</code></p>&#13;
<p><span id="x1-203006r469"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="effective-sample-size-ess" class="level3 sectionHead" data-number="1.14.8">&#13;
<h2 class="sectionHead" data-number="1.14.8">10.8 <span id="x1-2040008"/>Effective Sample Size (ESS)</h2>&#13;
<p><span id="dx1-204001"/></p>&#13;
<p>MCMC samples can be correlated. The reason is that we use the current position to generate a new position and we accept or reject the next position taking into account the old position. This dependency is usually lower for well-tuned modern methods, such as Hamiltonian Monte Carlo, but it can be high. We can compute and plot the autocorrelation with <code>az.plot_autocorrelation</code>. But usually, a more useful metric is to compute the <strong>Effective Sample Size</strong> (<strong>ESS</strong>). We can think of this number as the number of useful draws we have in our sample. Due to autocorrelation, this number is usually going to be lower than the actual number of samples. We can compute it using the <code>az.ess </code>function (see <em>Table <a href="#x1-204002r2">10.2</a></em>). The ESS diagnostic is also computed by default with the <code>az.summary </code>function and optionally with <code>az.plot_forest </code>(using the <code>ess=True</code> argument).</p>&#13;
<table id="TBL-14" class="tabular">&#13;
<tbody>&#13;
<tr id="TBL-14-1-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-14-1-1" class="td11" style="text-align: left; white-space: nowrap;"/>&#13;
<td id="TBL-14-1-2" class="td11" style="text-align: center; white-space: nowrap;"><em>a</em></td>&#13;
<td id="TBL-14-1-3" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">0</span></sub></td>&#13;
<td id="TBL-14-1-4" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">1</span></sub></td>&#13;
<td id="TBL-14-1-5" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">2</span></sub></td>&#13;
<td id="TBL-14-1-6" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">3</span></sub></td>&#13;
<td id="TBL-14-1-7" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">4</span></sub></td>&#13;
<td id="TBL-14-1-8" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">5</span></sub></td>&#13;
<td id="TBL-14-1-9" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">6</span></sub></td>&#13;
<td id="TBL-14-1-10" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">7</span></sub></td>&#13;
<td id="TBL-14-1-11" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">8</span></sub></td>&#13;
<td id="TBL-14-1-12" class="td11" style="text-align: center; white-space: nowrap;"><em>b</em><sub><span class="cmr-7">9</span></sub></td>&#13;
</tr>&#13;
<tr id="TBL-14-2-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-14-2-1" class="td11" style="text-align: left; white-space: nowrap;">model_cm</td>&#13;
<td id="TBL-14-2-2" class="td11" style="text-align: center; white-space: nowrap;">14</td>&#13;
<td id="TBL-14-2-3" class="td11" style="text-align: center; white-space: nowrap;">339</td>&#13;
<td id="TBL-14-2-4" class="td11" style="text-align: center; white-space: nowrap;">3893</td>&#13;
<td id="TBL-14-2-5" class="td11" style="text-align: center; white-space: nowrap;">5187</td>&#13;
<td id="TBL-14-2-6" class="td11" style="text-align: center; white-space: nowrap;">4025</td>&#13;
<td id="TBL-14-2-7" class="td11" style="text-align: center; white-space: nowrap;">5588</td>&#13;
<td id="TBL-14-2-8" class="td11" style="text-align: center; white-space: nowrap;">4448</td>&#13;
<td id="TBL-14-2-9" class="td11" style="text-align: center; white-space: nowrap;">4576</td>&#13;
<td id="TBL-14-2-10" class="td11" style="text-align: center; white-space: nowrap;">4025</td>&#13;
<td id="TBL-14-2-11" class="td11" style="text-align: center; white-space: nowrap;">4249</td>&#13;
<td id="TBL-14-2-12" class="td11" style="text-align: center; white-space: nowrap;">4973</td>&#13;
</tr>&#13;
<tr id="TBL-14-3-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-14-3-1" class="td11" style="text-align: left; white-space: nowrap;">model_ncm</td>&#13;
<td id="TBL-14-3-2" class="td11" style="text-align: center; white-space: nowrap;">2918</td>&#13;
<td id="TBL-14-3-3" class="td11" style="text-align: center; white-space: nowrap;">4100</td>&#13;
<td id="TBL-14-3-4" class="td11" style="text-align: center; white-space: nowrap;">4089</td>&#13;
<td id="TBL-14-3-5" class="td11" style="text-align: center; white-space: nowrap;">3942</td>&#13;
<td id="TBL-14-3-6" class="td11" style="text-align: center; white-space: nowrap;">3806</td>&#13;
<td id="TBL-14-3-7" class="td11" style="text-align: center; white-space: nowrap;">4171</td>&#13;
<td id="TBL-14-3-8" class="td11" style="text-align: center; white-space: nowrap;">3632</td>&#13;
<td id="TBL-14-3-9" class="td11" style="text-align: center; white-space: nowrap;">4653</td>&#13;
<td id="TBL-14-3-10" class="td11" style="text-align: center; white-space: nowrap;">3975</td>&#13;
<td id="TBL-14-3-11" class="td11" style="text-align: center; white-space: nowrap;">4092</td>&#13;
<td id="TBL-14-3-12" class="td11" style="text-align: center; white-space: nowrap;">3647</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="IMG---Caption"><span id="x1-204002r2"/> <span id="x1-204003"/><strong>Table 10.2</strong>: ESS values for models <code>model_c </code>and <code>model_ncm</code></p>&#13;
<p>The rule of thumb is that we need, at least, an effective sample size of 400 (100 ESS per chain). If we get values lower than this, not only could our estimates be excessively noisy, but even diagnostics such as <img src="../media/hat_R.png" style="width:0.65em;"/> might become unreliable.</p>&#13;
<p>The quality of the MCMC samples can be different from different regions of the posterior. For instance, at least for some problems, it could be easier to sample the bulk of the distribution than its tails. Thus, we may want to compute ESS for different regions of the posterior. The default value returned by <code>az.ess() </code>is the bulk-ESS, which estimates how well the center of the distribution was resolved. This is the ESS you need to check if you are interested in values such as the mean or median of a parameter. If you want to report posterior intervals or you are interested in rare events, you should check the value of the tail-ESS, which is computed as the minimum ESS at the percentiles 5 and 95. If you are interested in specific quantiles, you can ask ArviZ for those specific values using <code>az.ess(., method=’quantile’)</code>. We can even plot the ESS for many quantiles at the same time with the <code>az.plot_ess(., kind="quantiles" </code>function, as in <em>Figure <a href="#x1-204004r12">10.12</a></em> for parameter <code>a</code>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file264.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-204004r12"/><strong>Figure 10.12</strong>: ESS for quantiles of parameter <code>a</code></p>&#13;
<p>Finally, when we are running a model and find out that we have a very low ESS, the first reaction may be to increase the number of samples. Sometimes this is enough. But sometimes even a 10-fold increase is not enough. Instead of trial and error, we could use <code>az.plot_ess(., kind="evolution"</code>. This will give us a plot of samples versus ESS, as in <em>Figure <a href="#x1-204005r13">10.13</a></em>. We can use the information to estimate how many samples we need to reach a given value of ESS. For example, in <em>Figure <a href="#x1-204005r13">10.13</a></em> we can see that there is not much hope of getting a good ESS value for parameter <code>a </code>in <code>model_c </code>just by increasing the number of samples. Compare this with <code>model_nc</code>, where the ESS for the bulk is very close to the actual number of samples.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file265.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-204005r13"/><strong>Figure 10.13</strong>: Evolution of the ESS for parameter <code>a</code></p>&#13;
<p><span id="x1-204006r474"/></p>&#13;
</section>&#13;
<section id="monte-carlo-standard-error" class="level3 sectionHead" data-number="1.14.9">&#13;
<h2 class="sectionHead" data-number="1.14.9">10.9 <span id="x1-2050009"/>Monte Carlo standard error</h2>&#13;
<p><span id="dx1-205001"/></p>&#13;
<p>Even if we have a very low <img src="../media/hat_R.png" style="width:0.65em;"/> and a very high value of ESS. The samples from MCMC are still finite, and thus we are introducing an error in the estimation of the posterior parameters. Fortunately, we can estimate the error, and it is called the <strong>Monte Carlo Standard Error</strong> (<strong>MCSE</strong>). The estimation of the MCSE takes into account that the samples are not truly independent of each other. The precision we want in our results is limited by this value. If the MCSE for a parameter is 0.2, it does not make sense to report a parameter as 2.54. Instead, if we repeat the simulation (with a different random seed), we should expect that for 68% of the results, we obtain values in the range 2<em>.</em>54 <span class="cmsy-10x-x-109">± </span>0<em>.</em>2. Similarly, for 95% of them, we should get values in the range 2<em>.</em>54 <span class="cmsy-10x-x-109">± </span>0<em>.</em>4. Here, I am assuming the MCSE distributes normally and then using the fact that <span class="cmsy-10x-x-109">≈ </span>68% of the value of a Gaussian is within one standard deviation and <span class="cmsy-10x-x-109">≈ </span>95% is within two standard deviations.</p>&#13;
<p>The <img src="../media/hat_R.png" style="width:0.65em;"/>, ESS, and MCSE are related. In practice, we should use the ESS as a scale-free diagnostic to ensure we have enough useful samples. It is scale-free because it does not matter if one parameter goes from 0 to 1 and another from 0 to 100. We can compare their ESSs. With ESS, the larger the better, with a minimum of at least 400. If we have the minimum, we check we have a low enough <img src="../media/hat_R.png" style="width:0.65em;"/>. We can also visually check a rank plot or a trace plot (we should also check for divergences, as we will explain later). If everything looks fine, then we check that we have a low enough MCSE for the parameters and precision we want to report. Hopefully, for most problems, we will have an MCSE that is way below the precision we want.</p>&#13;
<div id="tcolobox-22" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Too Many Digits can Hurt</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>When reporting results in text, tables, or plots, it is important to be aware that excessive digits can make the numbers difficult to read and comprehend. It is easier to read a number like 0.9 than 0.909297, and it is also easier to retain in working memory. Also note that when a number is reported with more digits than warranted, a technical audience may assume that you are implying a higher level of significance than actually exists. So you will mislead this audience into trying to find meaning in differences that are actually meaningless. Finally, including too many digits can make your figures, tables, and graphs look cluttered and visually overwhelming. So always remember to be aware of the context of the data interests of your audience.</p>&#13;
</div>&#13;
</div>&#13;
<p><span id="x1-205002r476"/></p>&#13;
</section>&#13;
<section id="divergences" class="level3 sectionHead" data-number="1.14.10">&#13;
<h2 class="sectionHead" data-number="1.14.10">10.10 <span id="x1-20600010"/>Divergences</h2>&#13;
<p><span id="dx1-206001"/></p>&#13;
<p>We will now explore divergences, a diagnostic that is exclusive to NUTS, as it is based on the inner workings of the method and not a property of the generated samples. Divergences are a powerful and sensitive method that indicate the sampler has most likely found a region of high curvature in the posterior that cannot be explored properly. A nice feature of divergences is that they usually appear close to the problematic parameter space region, and thus we can use them to identify where the problem may be.</p>&#13;
<p>Let’s discuss divergences with a visual aid:</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file266.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-206002r14"/><strong>Figure 10.14</strong>: Pair plot for selected parameters from models <code>model_c </code>and <code>model_nc</code></p>&#13;
<p>As you can see, <em>Figure <a href="#x1-206002r14">10.14</a></em> shows the following three subplots:</p>&#13;
<ul>&#13;
<li><p>The left subplot: We have a scatter plot for two parameters of model <code>model_c</code>; namely, one dimension of the parameter <code>b </code>(we just picked one at random – feel free to pick a different one), and the logarithm of the parameter <code>a</code>. We take the logarithm because <code>a </code>is restricted to be positive (it is a scale parameter). Before sampling, PyMC transforms all bounded parameters into unbounded ones. For parameters such as <code>a</code>, the transformation is a logarithm. We do the same here because we want to understand what the sampler is <em>seeing</em>. OK, so we have a scatter plot where the gray dots are the samples. Look at the shape of the parameter. This shape is known as Neal’s funnel and it is <span id="dx1-206003"/>typical in hierarchical models. The black dots are divergences; they are scattered around, but we can see that many of them are around the tip of the funnel. This geometry is problematic for most MCMC methods because it is difficult to tune the sampler in such a way that we can get both good samples from the tip and the top of a funnel. One is a more ”spherical” region, where the sampler can move both up-down and left-right, and the other is ”narrower,” where the sampler has to move more in the up-down direction and very little in the left-right direction.</p></li>&#13;
<li><p>The middle subplot: We basically have the same as before but for model <code>model_nc</code>, now the funnel shape is even more accentuated. But we don’t have divergences. And we already know from previous sections that samples from this model are actually better. What is going on? The key to understanding this is in the model definition. You will notice that for this model, <code>b </code>is not actually sampled: <code>b </code>is a deterministic variable, a combination of <code>b_offset </code>and <code>a</code>, and those two are plotted on the last subplot.</p></li>&#13;
<li><p>The right subplot: We have <code>b_offset </code>versus <code>a</code>, and we can see that the geometry is more ”spherical”. It is this and not the middle subplot that the sampler is ”seeing.” Because this geometry is easier to sample, we do not get divergences and we get much better diagnostics overall.</p></li>&#13;
</ul>&#13;
<p>Changing the parametrization of a model is a way to remove divergences, but unless you are already aware of an alternative parametrization of your model, it can be very time-consuming to find one. An alternative that is often easy to try is to change the value of <code>target_accept</code>, an argument of <code>pm.sample</code>. Sometimes you may need both a different parametrization and a different value for <code>target_accept</code>. But what is <code>target_accept</code>? It is a parameter that controls the tuning of the NUTS sampler in PyMC. It controls the acceptance rate of the proposed samples, which defaults to 0.8. This means accepting 80% of the proposed samples. The NUTS sampler adaptively adjusts the step size of the Hamiltonian dynamics simulation to achieve the target acceptance rate. 80% is a good default, but for some models, you may want to try larger values like 0.90, 0.95, 0.99, or even 0.999 if you refuse to lose all hope. <span id="x1-206004r477"/></p>&#13;
</section>&#13;
<section id="keep-calm-and-keep-trying" class="level3 sectionHead" data-number="1.14.11">&#13;
<h2 class="sectionHead" data-number="1.14.11">10.11 <span id="x1-20700011"/>Keep calm and keep trying</h2>&#13;
<p><span id="dx1-207001"/></p>&#13;
<p>What should we do when diagnostics show problems? We should try to fix them. Sometimes, PyMC will provide suggestions on what to change. Pay attention to those suggestions, and you will save a lot of debugging time. Here, I have listed a few common actions you could take:</p>&#13;
<ul>&#13;
<li><p>Check for typos or other silly mistakes. It is super common even for experts to make ”silly” mistakes. If you misspell the name of a variable, it is highly likely that the model will not even run. But sometimes the mistake is more subtle, and you still get a syntactically valid model that runs, but with the wrong semantics.</p></li>&#13;
<li><p>Increase the number of samples. This might help for very mild problems, like when you’re close to the target ESS (or MCSE), or when <span class="accentvec"><span class="arrow">^</span><em>R</em></span> is slightly higher than 1.01 but not too much.</p></li>&#13;
<li><p>Remove some samples from the beginning of the trace. When checking a trace plot, you may observe that a few samples from the first few steps have overall higher or lower values compared to the rest of the trace, which otherwise looks OK. If that’s the case, simply removing those first few samples may be enough. This is known as burn-in, and it was a very common practice in the old days. Modern samplers have reduced the need for it. Also, PyMC already discards the samples from the tuning phase, so this tip is not as useful as it used to be.</p></li>&#13;
<li><p>Modify sampler parameters, such as increasing the length of the tuning phase, or increasing the <code>target_accept </code>parameter for the NUTS sampler.</p></li>&#13;
<li><p>Transform the data. For example, for linear regression models, centering the covariates (subtracting their means) usually speeds up the sampler and also reduces sampling issues.</p></li>&#13;
<li><p>Spend some time thinking about your priors. You should not tweak the priors to speed up the sampler or get rid of bad diagnostics. You should use your priors to encode prior knowledge. But it is often the case that when you do that, you also make the sampler’s life much easier. Use tools such as PreliZ and prior predictive checks to help you encode better priors.</p></li>&#13;
<li><p>Re-parametrize the model, that is, express the model in a different but equivalent way. This is not always easy to do, but for some common models such as hierarchical models, you already know of alternative parametrizations.</p></li>&#13;
</ul>&#13;
<p><span id="x1-207002r478"/></p>&#13;
</section>&#13;
<section id="summary-9" class="level3 sectionHead" data-number="1.14.12">&#13;
<h2 class="sectionHead" data-number="1.14.12">10.12 <span id="x1-20800012"/>Summary</h2>&#13;
<p>In this chapter, we have taken a conceptual walk through some of the most common methods used to compute the posterior distribution. We have put special emphasis on MCMC methods, which are designed to work on any given model (or at least a broad range of models), and thus are sometimes called universal inference engines. These methods are the core of any probabilistic programming language as they allow for automatic inference, letting users concentrate on iterative model design and interpretations of the results.</p>&#13;
<p>We also discussed numerical and visual tests for diagnosing samples. Without good approximations of the posterior distribution, all the advantages and flexibility of the Bayesian framework vanish. Thus, evaluating the quality of the samples is a crucial step before doing any other type of analysis. <span id="x1-208001r479"/></p>&#13;
</section>&#13;
<section id="exercises-9" class="level3 sectionHead" data-number="1.14.13">&#13;
<h2 class="sectionHead" data-number="1.14.13">10.13 <span id="x1-20900013"/>Exercises</h2>&#13;
<ol>&#13;
<li><div id="x1-209002x1">&#13;
<p>Use the grid method with other priors; for example, try with <code>prior = (grid &lt;= 0.5).astype(int) </code>or <code>prior = abs(grid - 0.5)</code>, or try defining your own crazy priors. Experiment with other data, such as increasing the total amount of data or making it more or less even in terms of the number of heads you observe.</p>&#13;
</div></li>&#13;
<li><div id="x1-209004x2">&#13;
<p>In the code we use to estimate <em>π</em>, keep <code>N </code>fixed and re-run the code a couple of times. Notice that the results are different because we are using random numbers, but also check that the errors are more or less in the same order. Try changing the number of <code>N </code>points and re-run the code. Can you guesstimate how the number of N points and the error are related? For a better estimation, you may want to modify the code to compute the error as a function of <code>N</code>. You can also run the code a few times with the same <code>N </code>and compute the mean error and standard deviation of the error. You can plot these results using the <code>plt.errorbar() </code>function from Matplotlib. Try using a set of <code>N</code>s, such as 100, 1,000, and 10,000; that is, a difference of one order of magnitude or so.</p>&#13;
</div></li>&#13;
<li><div id="x1-209006x3">&#13;
<p>Modify the <code>dist </code>argument you pass to the metropolis function; try using the values of the prior from <em>Chapter <a href="CH01.xhtml#x1-160001">1</a></em>. Compare this code to the grid method; which part should be modified to be able to use it to solve a Bayesian inference problem?</p>&#13;
</div></li>&#13;
<li><div id="x1-209008x4">&#13;
<p>Compare your answer from the previous exercise to this code by Thomas Wiecki: <a href="http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/" class="url">http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/</a></p>&#13;
</div></li>&#13;
<li><div id="x1-209010x5">&#13;
<p>Revisit at least a few of the models from previous chapters and run all the diagnostic tools we saw in this chapter.</p>&#13;
</div></li>&#13;
<li><div id="x1-209012x6">&#13;
<p>Revisit the code from all previous chapters, find those with divergences, and try to reduce the number of them.</p>&#13;
</div></li>&#13;
</ol>&#13;
</section>&#13;
<section id="join-our-community-discord-space-10" class="level3 likesectionHead" data-number="1.14.14">&#13;
<h2 class="likesectionHead" data-number="1.14.14"><span id="x1-21000013"/>Join our community Discord space</h2>&#13;
<p>Join our Discord community to meet like-minded people and learn alongside more than 5000 members at: <a href="https://packt.link/bayesian">https://packt.link/bayesian</a></p>&#13;
<p><img src="../media/file1.png" alt="PIC"/></p>&#13;
<p><span id="x1-210001r450"/></p>&#13;
</section>&#13;
</section>&#13;
</body></html>
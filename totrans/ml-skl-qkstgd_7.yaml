- en: Clustering Data with Unsupervised Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the data that you will encounter out in the wild will not come with
    labels. It is impossible to apply supervised machine learning techniques when
    your data does not come with labels. Unsupervised machine learning addresses this
    issue by grouping data into clusters; we can then assign labels based on those
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been clustered into a specific number of groups, we can proceed
    to give those groups labels. Unsupervised machine learning is the first step that
    you, as the data scientist, will have to implement, before you can apply supervised
    machine learning techniques (such as classification) to make meaningful predictions.
  prefs: []
  type: TYPE_NORMAL
- en: A common application of the unsupervised machine learning algorithm is customer
    data, which can be found across a wide range of industries. As a data scientist,
    your job is to find groups of customers that you can segment and deliver targeted
    products and advertisements to.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm and how it works internally, in order to cluster unlabeled
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the k-means algorithm in scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using feature engineering to optimize unsupervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going from unsupervised to supervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, NumPy ≥ 1.15.1, Matplotlib ≥ 3.0.0, Pydotplus ≥ 2.0.2, Image ≥ 3.1.2,
    Seaborn ≥ 0.9.0, and SciPy ≥ 1.1.0 installed on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb)[.](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2qeEJpI](http://bit.ly/2qeEJpI)'
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn about how the k-means algorithm works under
    the hood, in order to cluster data into groups that make logical sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a set of points, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d2492a0-3cb0-4ab3-8685-d523c395d4dd.png)'
  prefs: []
  type: TYPE_IMG
- en: A random set of points
  prefs: []
  type: TYPE_NORMAL
- en: Assignment of centroids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step that the algorithm takes is to assign a set of random centroids.
    Assuming that we want to find two distinct clusters or groups, the algorithm can
    assign two centroids, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4f79e48-a120-4fdb-a6c9-ca18b6d3d209.png)'
  prefs: []
  type: TYPE_IMG
- en: Centroids, represented by stars
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the stars represent the centroids of the algorithm.
    Note that in this case, the clusters' centers perfectly fit the two distinct groups.
    This is the most ideal case. In reality, the means (or centroids) are assigned
    randomly, and, with every iteration, the cluster centroids move closer to the
    center of the two groups.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is known as the k-means algorithm, as we try to find the mean
    of a group of points as the centroid. Since the mean can only be computed for
    a set of numeric points, such clustering algorithms only work with numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In reality, the process of grouping these points into two distinct clusters
    is not this straightforward. A visual representation of the process can be illustrated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97902875-2328-417d-89d6-f0a4c32d7342.png)'
  prefs: []
  type: TYPE_IMG
- en: The process of assigning centroids in the k-means algorithm
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the process of assigning the random centroids begins
    in the upper-left corner. As we go down and toward the upper-right corner, note
    how the centroids move closer to the center of the two distinct groups. In reality,
    the algorithm does not have an optimal endpoint at which it stops the iteration.
  prefs: []
  type: TYPE_NORMAL
- en: When does the algorithm stop iterating?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Typically, the algorithm looks for two metrics, in order to stop the iteration
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: The distance between the distinct groups (or clusters) that are formed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance between each point and the centroid of a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optimal case of cluster formation is when the distance between the distinct
    groups or clusters are as large as possible, while the distances between each
    point and the centroid of a cluster are as small as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the k-means algorithm in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you understand how the k-means algorithm works internally, we can proceed
    to implement it in scikit-learn. We are going to work with the same fraud detection
    dataset that we used in all of the previous chapters. The key difference is that
    we are going to drop the target feature, which contains the labels, and identify
    the two clusters that are used to detect fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the base k-means model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to load the dataset into our workspace and drop the target feature
    with the labels, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can implement the k-means algorithm with two cluster means. The choice
    of using two cluster means is arbitrary in nature, since we know that there should
    be two distinct clusters as a result of two labels: fraud and not fraud transactions.
    We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, first, we import the `KMeans` package from scikit-learn
    and initialize a model with two clusters. We then fit this model to the data by
    using the `.fit()`function. This results in a set of labels as the output. We
    can extract the labels by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output produced by the preceding code is an array of labels for each mobile
    transaction, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d6d2097-7765-4adb-8135-6402afc75127.png)'
  prefs: []
  type: TYPE_IMG
- en: Array of labels
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a set of labels, we know which cluster each transaction falls
    into. Mobile transactions that have a label of `0` fall into one group, while
    transactions that have a label of `1` fall into the second group.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal number of clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While explaining how the k-means algorithm works, we mentioned how the algorithm
    terminates once it finds the optimal number of clusters. When picking clusters
    arbitrarily using scikit-learn, this is not always the case. We need to find the
    optimal number of clusters, in this case.
  prefs: []
  type: TYPE_NORMAL
- en: One way that we can do this is by a measure known as **inertia.** Inertia measures
    how close the data points in a cluster are to its centroid. Obviously, a lower
    inertia signifies that the groups or clusters are tightly packed, which is good.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compute the inertia for the model, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code produced an inertia value of *4.99 × 10 ^ 17*, which is extremely
    large with respect to the other values of inertia produced by different numbers
    of clusters (explained as follows), and is not a good value of inertia. This suggests
    that the individual data points are spread out, and are not tightly packed together.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, we do not really know what the optimal numbers of clusters are,
    so we need to plot the inertia scores for different numbers of clusters. We can
    do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c094e7ab-092b-4c7f-8b34-2e0508722eaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Inertia as a function of the number of clusters
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, first, we create a list of clusters that have values
    from 1 to 10\. Each value denotes the number of clusters that will be used in
    the machine learning model. Next, we create an empty list that will store all
    of the inertia values that each model will produce.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we loop over the list of clusters and build and evaluate a k-means model
    for each cluster value in the list. Each model now produces an inertia, which
    is stored in the list that we initialized at the start of the code block. A simple
    line plot is then constructed by using the list of clusters along the xaxis and
    the corresponding inertia values along the yaxis, using `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: The plot tells us that the inertia values are the lowest when the number of
    clusters is equal to 10\. However, having a large number of clusters is also something
    that we must aim at avoiding, as having too many groups does not help us to generalize
    well, and the characteristics about each group become very specific.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the ideal way to choose the best number of clusters for a problem,
    given that we do not have prior information about the number of groups that we
    want beforehand, is to identify the **elbow point** of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The elbow point is the point at which the rate of decrease in inertia values
    slows down. The elbow point is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa54e78e-83a7-4135-80b2-98fa7b8ebe95.png)'
  prefs: []
  type: TYPE_IMG
- en: Elbow point of the graph
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding plot, it is clear that the elbow point corresponds to four
    clusters. This could mean that there are four distinct types of fraudulent transactions,
    apart from the standard categorizations of fraud and not fraud. However, since
    we know beforehand that the dataset has a binary target feature with two categories,
    we will dig too deeply into why four is the ideal number of groups/clusters for
    this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering for optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Engineering the features in your dataset is a concept that is fundamentally
    used to improve the performance of your model. Fine-tuning the features to the
    algorithm''s design is beneficial, because it can lead to an improvement in accuracy,
    while reducing the generalization errors at the same time. The different kinds
    of feature engineering techniques for optimizing your dataset that you will learn
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scaling is the process of standardizing your data so that the values under
    every feature fall within a certain range, such as -1 to +1\. In order to scale
    the data, we subtract each value of a particular feature with the mean of that
    feature, and divide it by the variance of that feature. In order to scale the
    features in our fraud detection dataset, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we use the `StandardScalar()`function to scale our dataframe,
    and then we build a k-means model with two clusters on the scaled data. After
    evaluating the inertia of the model, the value output is 295,000, which is substantially
    better than the value of *4.99 × 10^(17)*, produced by the model without scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then create a new plot of the number of clusters versus the inertia
    values, using the same code that we did earlier, with the only difference being
    replacing the original dataframe with the scaled dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7945b2c1-962a-4a74-a2f6-0d301b06041c.png)'
  prefs: []
  type: TYPE_IMG
- en: Optimal number of clusters, post-scaling
  prefs: []
  type: TYPE_NORMAL
- en: We notice that the preceding plot does not have a very clear elbow point, where
    the rate of decrease in the inertia values is lower. However, if we look closely,
    we can find this point at 8 clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **principal component analysis** (**PCA**) is a subset of dimensionality
    reduction. **Dimensionality reduction** is the process of reducing the number
    of features that provide no predictive value to a predictive model. We also optimize
    and improve the computational efficiency of processing the algorithms. This is
    because a dataset with a smaller number of features will make it easier for the
    algorithm to detect patterns more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in PCA is called **decorrelation**. Features that are highly
    correlated with each other provide no value to the predictive model. Therefore,
    in the decorrelation step, the PCA takes two highly correlated features and spreads
    their data points such that it''s aligned across the axis, and is not correlated
    anymore. This process can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ca670b5-80b4-42eb-9604-968e02b5a01e.png)'
  prefs: []
  type: TYPE_IMG
- en: The process of decorrelation
  prefs: []
  type: TYPE_NORMAL
- en: Once the features are decorrelated, the principal components (or features) are
    extracted from the data. These features are the ones that have high variance,
    and, in turn, provide the most value to a predictive model. The features with
    low variance are discarded, and thus, the number of dimensions in the dataset
    is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to perform dimensionality reduction using PCA, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, first, we import the `PCA`method from scikit-learn. Next,
    we initialize a PCAmodel with five components. Here, we are specifying that we
    want the PCA to reduce the dataset to only the five most important features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then fit the PCA model to the dataframe and transform it, in order to obtain
    the decorrelated features. Checking the shape of the final array of features,
    we can see that it only has five features. Finally, we create a new k-means model
    with only the principal component features, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluating the inertia of the new model improved its performance. We obtained
    a lower value of inertia than in the case of the scaled model. Now, let''s evaluate
    the inertia scores for different numbers of principal components or features.
    In order to this, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize a list to store the different principal component values
    that we want to build our models with. These values are from 1 to 10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we initialize an empty list, in order to store the inertia values from
    each and every model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using each principal component value, we build a new k-means model and append
    the inertia value for that model into the empty list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, a plot is constructed between the inertia values and the different
    values of components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This plot is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eef35bb7-907b-437c-8a0a-5a282fe3ea82.png)'
  prefs: []
  type: TYPE_IMG
- en: Inertia values versus the numbers of principal components
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding plot, it is clear that the inertia value is lowest for one
    component.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Visualizing how your clusters are formed is no easy task when the number of
    variables/dimensions in your dataset is very large. There are two main methods
    that you can use in order to visualize how the clusters are distributed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**t-SNE**: This creates a map of the dataset in two-dimensional space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical clustering**: This uses a tree-based visualization, known as
    a **dendrogram**, in order to create hierarchies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you will learn how to implement these visualization techniques,
    in order to create compelling cluster visuals.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **t-SNE** is an abbreviation that stands for **t-distributed stochastic
    neighbor embedding**. The fundamental concept behind the t-SNE is to map a higher
    dimension to a two-dimensional space. In simple terms, if your dataset has more
    than two features, the t-SNE does a great job at showing you how your entire dataset
    can be visualized on your computer screen!
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to implement the k-means algorithm and create a set of prediction
    labels that we can merge into the unlabeled dataset. We can do this by using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Don't worry about how the preceding segment of code works for the moment, as
    this will be explained in detail in a later section within this chapter, when
    we deal with converting an unsupervised machine learning problem into a supervised
    learning one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create a t-SNE object and fit that into our array of data points
    that consists of only the features. We will then transform the features at the
    same time so that we can view all the features on a two-dimensional space. This
    is done in the following code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize the t-SNE object by using the `TSNE()`function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the t-SNE object, we fit and transform the data in our features, using
    the `fit_transform()`method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we create the t-SNE visualization by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: We extract the first and second features from the set of transformed features
    for the x axis and *y* axis, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then plot a scatter plot and color it by the target labels, which were generated
    earlier, using the k-means algorithm. This generates the following plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3e1e748d-08c5-4dce-9d61-07117f8f82af.png)'
  prefs: []
  type: TYPE_IMG
- en: t-SNE visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding plot, the yellow color represents the transactions that have
    been assigned the fraud label, while the purple color represents the transactions
    that have been assigned the non-fraudulent label. (Please refer to the color version
    of the image.)
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed initially, the hierarchical clustering technique uses the dendrogram
    to visualize clusters or groups. In order to explain how the dendrogram works,
    we will consider a dataset with four features.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Individual features as individual clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first step, each feature in the dataset is considered to be its own
    cluster. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d56db01-84a4-4320-b24a-d491ae54b4f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Each feature as a single cluster in the dendrogram
  prefs: []
  type: TYPE_NORMAL
- en: Each feature in the preceding diagram is one single cluster, at this point in
    time. The algorithm now searches to find the two features that are closest to
    each other, and merges them into a single cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – The merge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, the algorithm merges the data points in the two closest features
    together, into one single cluster. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0cffef7-2d65-4b61-b6ef-e0dd0683c8a7.png)'
  prefs: []
  type: TYPE_IMG
- en: The process in which features merge into a single cluster
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, it is clear that the algorithm has now chosen **Feature
    2** and **Feature 3**, and has decided that the data under these two features
    are the closest to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm now continues the process of merging features together iteratively,
    until no more clusters can be formed. The final dendrogram that is formed is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, **Feature 2** and **Feature 3** were grouped into
    a single cluster. The algorithm then decided that **Feature 1** and the cluster
    of **Feature 2** and **3** were closest to each other. Therefore, these three
    features were clustered into one group. Finally, **Feature 4** was grouped together
    with **Feature 3**.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have learned how hierarchical clustering works, we can implement
    this concept. In order to create a hierarchical cluster, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in a dendrogram, as illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab00c657-6b5d-4608-b2d6-a64f8123ed7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Dendrogram
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create an array with four columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then use the `linkage` function to create the clusters. Within the function,
    we specify the `method` argument as complete, in order to indicate that we want
    the entire dendrogram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we use the `dendrogram`function to create the dendrogram with the clusters.
    We set the label names to the list of feature names that was created earlier in
    the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Going from unsupervised to supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The eventual goal of unsupervised learning is to take a dataset with no labels
    and assign labels to each row of the dataset, so that we can run a supervised
    learning algorithm through it. This allows us to create predictions that make
    use of the labels.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will learn how to convert the labels generated by the unsupervised
    machine learning algorithm into a decision tree that makes use of those labels.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a labeled dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to convert the labels generated by an unsupervised machine
    learning algorithm, such as the k-means algorithm, and append it to the dataset.
    We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we read in the fraud detection dataset and drop the
    target and index columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, in the preceding code we initialize and fit a k-means model with two
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we create the target labels by using the `predict()`method, and convert
    it into a `pandas` series. We then merge this series into the dataframe, in order
    to create our labeled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Building the decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the labeled dataset, we can create a decision tree, in order
    to convert the unsupervised machine learning problem into a supervised machine
    learning one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do this, we start with all of the necessary package imports, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we rename the target column to a name that is appropriate (when we merged
    the target labels created by the k-means algorithm, it produced `0` as the default
    name). We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build the decision tree classification algorithm, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, first, we create the features and target variables and
    initialize a decision tree classifier. We then fit the classifier onto the features
    and target.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we want to visualize the decision tree. We can do this by using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the decision tree shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d761508f-2bf5-4d69-b782-b2a616cc3b09.png)'
  prefs: []
  type: TYPE_IMG
- en: A part of the decision tree that was created
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about how the k-means algorithm works, in order
    to cluster unlabeled data points into clusters or groups. You then learned how
    to implement the same using scikit-learn, and we expanded upon the feature engineering
    aspect of the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Having learned how to visualize clusters using hierarchical clustering and t-SNE,
    you then learned how to map a multi-dimensional dataset into a two-dimensional
    space. Finally, you learned how to convert an unsupervised machine learning problem
    into a supervised learning one, using decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: In the next (and final) chapter, you will learn how to formally evaluate the
    performance of all of the machine learning algorithms that you have built so far!
  prefs: []
  type: TYPE_NORMAL

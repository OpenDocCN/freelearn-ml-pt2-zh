<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer423">
    <h1 class="chapterNumber">12</h1>
    <h1 class="chapterTitle" id="_idParaDest-287">Making Predictions with Sequences Using Recurrent Neural Networks</h1>
    <p class="normal">In the previous chapter, we focused on <strong class="keyWord">Convolutional Neural Networks</strong> (<strong class="keyWord">CNNs</strong>) and used them to deal with image-related tasks. In this chapter, we will explore <strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>), which are suitable for sequential data and time-dependent data, such as daily temperature, DNA sequences, and customers’ shopping transactions over time. You will learn how the recurrent architecture works and see variants of the model. We will then work on their applications, including sentiment analysis, time series prediction, and text generation.</p>
    <p class="normal">We will cover the following topics in this chapter:</p>
    <ul>
      <li class="bulletList">Tracking sequential learning</li>
      <li class="bulletList">Learning the RNN architecture by example</li>
      <li class="bulletList">Training an RNN model</li>
      <li class="bulletList">Overcoming long-term dependencies with <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>)</li>
      <li class="bulletList">Analyzing movie review sentiment with RNNs</li>
      <li class="bulletList">Revisiting stock price forecasting with LSTM</li>
      <li class="bulletList">Writing your own War and Peace with LSTM</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-288">Introducing sequential learning</h1>
    <p class="normal">The machine learning <a id="_idIndexMarker1191"/>problems we have solved so far in this book have been time independent. For example, ad click-through doesn’t depend on the user’s historical ad clicks under our previous approach; in face classification, the model only takes in the current face image, not previous ones. However, there are many cases in life that depend on time. For example, in financial fraud detection, we can’t just look at the present transaction; we should also consider previous transactions so that we can model based on their discrepancy. Another<a id="_idIndexMarker1192"/> example is <strong class="keyWord">Part-of-Speech</strong> (<strong class="keyWord">PoS</strong>) tagging, where we assign a PoS (verb, noun, adverb, and so on) to a word. Instead of solely focusing on the given word, we must look at some previous words, and sometimes the next words too.</p>
    <p class="normal">In time-dependent cases like those just mentioned, the current output is dependent on not only the current input but also the previous inputs; note that the length of the previous inputs is not fixed. Using machine learning to solve such problems is called <strong class="keyWord">sequence learning</strong> or <strong class="keyWord">sequence modeling</strong>. Obviously, the time-dependent event is called a <strong class="keyWord">sequence</strong>. Besides <a id="_idIndexMarker1193"/>events<a id="_idIndexMarker1194"/> that occur in disjointed time intervals (such as financial transactions and phone calls), text, speech, and video are also sequential data.</p>
    <p class="normal">You may be wondering why we can’t just regularly model the sequential data by feeding in the entire sequence. This can be quite limiting as we have to fix the input size. One problem is that we will lose information if an important event lies outside of the fixed window. But can we just use a very large time window? Note that the feature space grows along with the window size. The feature space will become excessive if we want to cover enough events in a certain time window. Hence, overfitting can be another problem.</p>
    <p class="normal">I hope you now see why we need to model sequential data in a different way. In the next section, we will talk about an example of a modeling technique used for modern sequence learning: RNNs.</p>
    <h1 class="heading-1" id="_idParaDest-289">Learning the RNN architecture by example</h1>
    <p class="normal">As you can imagine, RNNs <a id="_idIndexMarker1195"/>stand out because of their recurrent mechanism. We will start with a detailed explanation of this in the next section. We will talk about different types of RNNs after that, along with some typical applications.</p>
    <h2 class="heading-2" id="_idParaDest-290">Recurrent mechanism</h2>
    <p class="normal">Recall that in<a id="_idIndexMarker1196"/> feedforward networks (such as vanilla neural networks and CNNs), data moves one way, from the input layer to the output layer. In RNNs, the recurrent architecture allows data to circle back to the input layer. This means that data is not limited to a feedforward direction. Specifically, in a hidden layer of an RNN, the output from the previous time point will become part of the input for the current time point. The following diagram illustrates how data flows in an RNN in general:</p>
    <figure class="mediaobject"><img alt="A diagram of a network  Description automatically generated with medium confidence" src="../Images/B21047_12_01.png"/></figure>
    <p class="packt_figref">Figure 12.1: The general form of an RNN</p>
    <p class="normal">Such a recurrent architecture makes RNNs work well with sequential data, including time series (such as daily temperatures, daily product sales, and clinical EEG recordings) and general consecutive data with order (such as words in a sentence and DNA sequences). Take a financial fraud detector as an example; the output features from the previous transaction go into the training for the current transaction. In the end, the prediction for one transaction depends on all of its previous transactions. Let me explain the recurrent mechanism in a mathematical and visual way.</p>
    <p class="normal">Suppose we have some inputs, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>. Here, <em class="italic">t</em> represents a time step or a sequential order. In a feedforward neural network, we simply assume that inputs at different <em class="italic">t</em> are independent of each other. We denote the output of a hidden layer at a time step, <em class="italic">t</em>, as <em class="italic">h</em><sub class="subscript-italic" style="font-style: italic;">t</sub> = <em class="italic">f</em>(<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>), where <em class="italic">f</em> is the abstract of the hidden layer.</p>
    <p class="normal">This is depicted in the <a id="_idIndexMarker1197"/>following diagram:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, diagram, font, design  Description automatically generated" src="../Images/B21047_12_02.png"/></figure>
    <p class="packt_figref">Figure 12.2: General form of a feedforward neural network</p>
    <p class="normal">On the contrary, the feedback loop in an RNN feeds the information of the previous state to the current state. The output of a hidden layer of an RNN at a time step, <em class="italic">t</em>, can be expressed as <em class="italic">h</em><sub class="subscript-italic" style="font-style: italic;">t</sub> = <em class="italic">f</em>(<em class="italic">h</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>). This is depicted in the following diagram:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, diagram, rectangle, square  Description automatically generated" src="../Images/B21047_12_03.png"/></figure>
    <p class="packt_figref">Figure 12.3: Unfolded recurrent layer over time steps</p>
    <p class="normal">The same task, <em class="italic">f</em>, is performed on each element of the sequence, and the output, <em class="italic">h</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, is dependent on the output that’s generated from previous computations, <em class="italic">h</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>. The chain-like architecture captures the “memory” that has been calculated so far. This is what makes RNNs so successful in dealing with sequential data.</p>
    <p class="normal">Moreover, thanks to the <a id="_idIndexMarker1198"/>recurrent architecture, RNNs also have great flexibility in dealing with different combinations of input sequences and/or output sequences. In the next section, we will talk about different categories of RNNs based on input and output, including the following:</p>
    <ul>
      <li class="bulletList">Many to one</li>
      <li class="bulletList">One to many</li>
      <li class="bulletList">Many to many (synced)</li>
      <li class="bulletList">Many to many (unsynced)</li>
    </ul>
    <p class="normal">We will start by looking at many-to-one RNNs.</p>
    <h2 class="heading-2" id="_idParaDest-291">Many-to-one RNNs</h2>
    <p class="normal">The most intuitive type <a id="_idIndexMarker1199"/>of RNN is probably <strong class="keyWord">many to one</strong>. A many-to-one RNN can have <a id="_idIndexMarker1200"/>input sequences with as many time steps as you want, but it only produces one output after going through the entire sequence. The following diagram depicts the general structure of a many-to-one RNN:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, diagram, rectangle  Description automatically generated" src="../Images/B21047_12_04.png"/></figure>
    <p class="packt_figref">Figure 12.4: General form of a many-to-one RNN</p>
    <p class="normal">Here, <em class="italic">f</em> represents<a id="_idIndexMarker1201"/> one or more recurrent hidden layers, where an individual layer takes in its own output from the previous time step. Here is <a id="_idIndexMarker1202"/>an example of three hidden layers stacking up:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, diagram, rectangle, square  Description automatically generated" src="../Images/B21047_12_05.png"/></figure>
    <p class="packt_figref">Figure 12.5: Example of three recurrent layers stacking up</p>
    <p class="normal">Many-to-one RNNs are widely used for classifying sequential data. Sentiment analysis is a good example of this and is where the RNN reads the entire customer review, for instance, and assigns a sentiment score (positive, neutral, or negative sentiment). Similarly, we can also use RNNs of this kind in the topic classification of news articles. Identifying the genre of a song is another application as the model can read the entire audio stream. We can also use many-to-one RNNs to determine whether a patient is having a seizure based on an EEG trace.</p>
    <h2 class="heading-2" id="_idParaDest-292">One-to-many RNNs</h2>
    <p class="normal"><strong class="keyWord">One-to-many</strong> RNNs are the <a id="_idIndexMarker1203"/>exact opposite of many-to-one RNNs. They take in only one<a id="_idIndexMarker1204"/> input (not a sequence) and generate a sequence of outputs. A typical one-to-many RNN is presented in the following diagram:</p>
    <figure class="mediaobject"><img alt="A diagram of a flowchart  Description automatically generated with low confidence" src="../Images/B21047_12_06.png"/></figure>
    <p class="packt_figref">Figure 12.6: General form of a one-to-many RNN</p>
    <p class="normal">Again, <em class="italic">f</em> represents one or more recurrent hidden layers.</p>
    <p class="normal">Note that “one” here refers to a single time step or a non-sequential input, rather than the number of input features.</p>
    <p class="normal">One-to-many <a id="_idIndexMarker1205"/>RNNs are commonly used as sequence generators. For example, we <a id="_idIndexMarker1206"/>can generate a piece of music given a starting note and/or a genre. Similarly, we can write a movie script like a professional screenwriter using one-to-many RNNs with a starting word we specify. Image captioning is another interesting application: the RNN takes in an image and outputs the description (a sentence of words) of the image.</p>
    <h2 class="heading-2" id="_idParaDest-293">Many-to-many (synced) RNNs</h2>
    <p class="normal">The third type of<a id="_idIndexMarker1207"/> RNN, many to many (synced), allows each element in the <a id="_idIndexMarker1208"/>input sequence to have an output. Let’s look at how data flows in the following many-to-many (synced) RNN:</p>
    <figure class="mediaobject"><img alt="A diagram of a flowchart  Description automatically generated with low confidence" src="../Images/B21047_12_07.png"/></figure>
    <p class="packt_figref">Figure 12.7: General form of a many-to-many (synced) RNN</p>
    <p class="normal">As you can see, each output is calculated based on its corresponding input and all the previous outputs.</p>
    <p class="normal">One common use case for this type of RNN is time series forecasting, where we want to perform rolling prediction at every time step based on the current and previously observed data. Here are some examples of time series forecasting where we can leverage many-to-many (synced) RNNs:</p>
    <ul>
      <li class="bulletList">Product sales each day for a store</li>
      <li class="bulletList">The daily closing price of a stock</li>
      <li class="bulletList">Power <a id="_idIndexMarker1209"/>consumption of a <a id="_idIndexMarker1210"/>factory each hour</li>
    </ul>
    <p class="normal">They are also widely used in solving NLP problems, including PoS tagging, named-entity recognition, and real-time speech recognition.</p>
    <h2 class="heading-2" id="_idParaDest-294">Many-to-many (unsynced) RNNs</h2>
    <p class="normal">Sometimes, we only <a id="_idIndexMarker1211"/>want to generate the output sequence <em class="italic">after</em> we’ve<a id="_idIndexMarker1212"/> processed the entire input sequence. This is the <strong class="keyWord">unsynced</strong> version of a many-to-many RNN.</p>
    <p class="normal">Refer to the following diagram for the general structure of a many-to-many (unsynced) RNN:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, diagram, white, font  Description automatically generated" src="../Images/B21047_12_08.png"/></figure>
    <p class="packt_figref">Figure 12.8: General form of a many-to-many (unsynced) RNN</p>
    <p class="normal">Note that the length of the output sequence (T<em class="italic">y</em> in the preceding diagram) can be different from that of the input sequence (T<em class="italic">x</em> in the preceding diagram). This provides us with some flexibility.</p>
    <p class="normal">This type of <a id="_idIndexMarker1213"/>RNN is the go-to model for machine translation. In <a id="_idIndexMarker1214"/>French-English translation, for example, the model first reads a complete sentence in French and then produces a translated sentence in English. Multi-step ahead forecasting is another popular example: sometimes, we are asked to predict sales for multiple days in the future when given data from the past month.</p>
    <p class="normal">You have now learned about four types of RNN based on the model’s input and output.</p>
    <p class="normal">Wait, what about one-to-one RNNs? There is no such thing. One-to-one is just a regular feedforward model.</p>
    <p class="normal">We will be applying some of these types of RNN to solve projects, including sentiment analysis and word generation, later in this chapter. Now, let’s figure out how an RNN model is trained.</p>
    <h1 class="heading-1" id="_idParaDest-295">Training an RNN model</h1>
    <p class="normal">To explain how we optimize<a id="_idIndexMarker1215"/> the weights (parameters) of an RNN, we first annotate the weights and the data on the network, as follows:</p>
    <ul>
      <li class="bulletList"><em class="italic">U</em> denotes the weights connecting the input layer and the hidden layer.</li>
      <li class="bulletList"><em class="italic">V</em> denotes the weights between the hidden layer and the output layer. Note here that we use only one recurrent layer for simplicity.</li>
      <li class="bulletList"><em class="italic">W</em> denotes the weights of the recurrent layer; that is, the feedback layer.</li>
      <li class="bulletList"><em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub> denotes the inputs at time step <em class="italic">t</em>.</li>
      <li class="bulletList"><em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub> denotes the hidden state at time step <em class="italic">t</em>.</li>
      <li class="bulletList"><em class="italic">h</em><sub class="subscript-italic" style="font-style: italic;">t</sub> denotes the outputs at time step <em class="italic">t</em>.</li>
    </ul>
    <p class="normal">Next, we unfold the simple RNN model over three time steps: <em class="italic">t</em> − 1, <em class="italic">t</em>, and <em class="italic">t</em> + 1, as follows:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, diagram  Description automatically generated" src="../Images/B21047_12_09.png"/></figure>
    <p class="packt_figref">Figure 12.9: Unfolding a recurrent layer</p>
    <p class="normal">We describe the mathematical relationships between the layers as follows:</p>
    <ul>
      <li class="bulletList">We let <em class="italic">a</em> denote the activation function for the hidden layer. In RNNs, we usually choose tanh or ReLU as the activation function for the hidden layers.</li>
      <li class="bulletList">Given the current input, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, and the previous hidden state, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>, we compute the current hidden state, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, by <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub> = <em class="italic">a</em>(<em class="italic">Ux</em><sub class="subscript-italic" style="font-style: italic;">t</sub> + <em class="italic">Ws</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>).</li>
    </ul>
    <div class="note-one">
      <p class="normal">Feel free to read <em class="chapterRef">Chapter 6</em>, <em class="italic">Predicting Stock Prices with Artificial Neural Networks</em>, again to brush up on your knowledge of neural networks.</p>
    </div>
    <ul>
      <li class="bulletList">In a similar manner, we compute <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub> based on</li>
    </ul>
    <p class="center"><em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-2</sub>:<em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-1</sub>=a(<em class="italic">Ux</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-1</sub>+<em class="italic">Ws</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-2</sub>)</p>
    <ul>
      <li class="bulletList">We repeat this until <em class="italic">s</em><sub class="subscript">1</sub>, which depends on:</li>
    </ul>
    <p class="center"><em class="italic">s</em><sub class="subscript">0</sub>:<em class="italic">s</em><sub class="subscript">1</sub>=a(<em class="italic">Ux</em><sub class="subscript">1</sub>+<em class="italic">Ws</em><sub class="subscript">0</sub>)</p>
    <p class="normal-one">We usually set <em class="italic">s</em><sub class="subscript">0</sub> to all zeros.</p>
    <ul>
      <li class="bulletList">We let <em class="italic">g</em> denote the <a id="_idIndexMarker1216"/>activation function for the output layer. It can be a sigmoid function if we want to perform binary classification, a softmax function for multi-class classification, and a simple linear function (that is, no activation) for regression.</li>
      <li class="bulletList">Finally, we compute the output at time step <em class="italic">t</em>:</li>
    </ul>
    <p class="center"><em class="italic">h</em><sub class="subscript">t</sub>:<em class="italic">h</em><sub class="subscript">t</sub>=<em class="italic">g</em>(<em class="italic">Vs</em><sub class="subscript-italic" style="font-style: italic;">t</sub>)</p>
    <p class="normal">With the dependency in hidden states over time steps (that is, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub> depends on <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub> depends on <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−2</sub>, and so on), the recurrent layer brings memory to the network, which captures and retains information from all the previous time steps.</p>
    <p class="normal">As we did for traditional neural networks, we apply the backpropagation algorithm to optimize all the weights, <em class="italic">U</em>, <em class="italic">V</em>, and <em class="italic">W</em>, in RNNs. However, as you may have noticed, the output at a time step is indirectly dependent on all the previous time steps (<em class="italic">h</em><sup class="superscript-italic" style="font-style: italic;">t</sup> depends on <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, while <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub> depends on all the previous ones). Hence, we need to compute the loss over all previous <em class="italic">t</em>-1 time steps, besides the current time step. Consequently, the gradients of the weights are calculated this way. For example, if we want to compute the gradients at time step <em class="italic">t</em> = 4, we need to backpropagate the previous four time steps (<em class="italic">t</em> = 3, <em class="italic">t</em> = 2, <em class="italic">t</em> = 1, <em class="italic">t</em> = 0) and sum up the gradients <a id="_idIndexMarker1217"/>over these five time steps. This version of the backpropagation algorithm is called <strong class="keyWord">Back Propagation Through Time</strong> (<strong class="keyWord">BPTT</strong>).</p>
    <p class="normal">The recurrent architecture enables RNNs to capture information from the very beginning of the input sequence. This advances the predictive capability of sequence learning. You may be wondering whether vanilla RNNs can handle long sequences. They can in theory, but not in practice due to<a id="_idIndexMarker1218"/> the <strong class="keyWord">vanishing gradient</strong> problem. A vanishing gradient means the gradient will become vanishingly small over long time steps, which prevents the weight from updating. I will explain this in detail in the next section, as well as introduce a variant architecture, LSTM, that helps solve this issue.</p>
    <h1 class="heading-1" id="_idParaDest-296">Overcoming long-term dependencies with LSTM</h1>
    <p class="normal">Let’s start <a id="_idIndexMarker1219"/>with the vanishing gradient issue in vanilla RNNs. Where does it come from? Recall that during backpropagation, the gradient decays along with each time step in the RNN (that is, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub>=<em class="italic">a</em>(<em class="italic">Ux</em><sub class="subscript-italic" style="font-style: italic;">t</sub>+<em class="italic">Ws</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>); early elements in a long input sequence will have little contribution to the computation of the current gradient. This means that vanilla RNNs can only capture the temporal dependencies within a short time window. However, dependencies between time steps that are far away are sometimes critical signals to the prediction. RNN variants, including LSTM<a id="_idIndexMarker1220"/> and <strong class="keyWord">gated recurrent units</strong> (<strong class="keyWord">GRUs</strong>), are specifically designed to solve problems that require learning long-term dependencies.</p>
    <div class="note">
      <p class="normal">We will be focusing on LSTM in this book as it is a lot more popular than GRU. LSTM was introduced a decade earlier and is more mature than GRU. If you are interested in learning more about GRU and its applications, feel free to check out<em class="italic"> Hands-On Deep Learning Architectures with Python</em> by Yuxi Hayden Liu (Packt Publishing).</p>
    </div>
    <p class="normal">In LSTM, we use a grating mechanism to handle long-term dependencies. Its magic comes from a memory unit and three information gates built on top of the recurrent cell. The word “gate” is taken from the logic gate<a id="_idIndexMarker1221"/> in a circuit (<a href="https://en.wikipedia.org/wiki/Logic_gate"><span class="url">https://en.wikipedia.org/wiki/Logic_gate</span></a>). It is basically a sigmoid function whose output value ranges from <code class="inlineCode">0</code> to <code class="inlineCode">1</code>. <code class="inlineCode">0</code> represents the “off” logic, while <code class="inlineCode">1</code> represents the “on” logic.</p>
    <p class="normal">The LSTM version of the recurrent cell is depicted in the following diagram, right after the vanilla version for comparison:</p>
    <figure class="mediaobject"><img alt="A picture containing diagram, text, technical drawing, plan  Description automatically generated" src="../Images/B21047_12_10.png"/></figure>
    <p class="packt_figref">Figure 12.10: Recurrent cell in vanilla RNNs versus LSTM RNNs</p>
    <p class="normal">Let’s look at the <a id="_idIndexMarker1222"/>LSTM recurrent cell in detail from left to right:</p>
    <ul>
      <li class="bulletList"><em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">t</sub> is the <strong class="keyWord">memory unit</strong>. It <a id="_idIndexMarker1223"/>memorizes information from the very beginning of the input sequence.</li>
      <li class="bulletList"><em class="italic">f</em> stands for the <strong class="keyWord">forget gate</strong>. It determines how much information from the previous memory state, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>, to forget, or, in other words, how much information to pass forward. Let <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">f</sup> denote the weights between the forget gate and the previous hidden state, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>, and <em class="italic">U</em><sup class="superscript-italic" style="font-style: italic;">f</sup> denote the weights between the forget gate and the current input, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>.</li>
      <li class="bulletList"><em class="italic">i</em> represents <a id="_idIndexMarker1224"/>the <strong class="keyWord">input gate</strong>. It controls how much information from the current input to put through. <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">i</sup> and <em class="italic">U</em><sup class="superscript-italic" style="font-style: italic;">i</sup> are the weights connecting the input gate to the previous hidden state, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>, and the current input, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, respectively.</li>
      <li class="bulletList"><em class="italic">tanh</em> is simply the activation function for the hidden state. It acts as the <em class="italic">a</em> in the vanilla RNN. Its output is computed based on the current input, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, along with the associated weights, <em class="italic">U</em><sup class="superscript-italic" style="font-style: italic;">c</sup>, the previous hidden state, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>, and the corresponding weights, <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">c</sup>.</li>
      <li class="bulletList"><code class="inlineCode">o</code> serves as the <strong class="keyWord">output gate</strong>. It defines <a id="_idIndexMarker1225"/>how much information is extracted from the internal memory for the output of the entire recurrent cell. As always, <em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">o</sup> and <em class="italic">U</em><sup class="superscript-italic" style="font-style: italic;">o</sup> are the associated weights for the previous hidden state and current input, respectively.</li>
    </ul>
    <p class="normal">We describe the<a id="_idIndexMarker1226"/> relationship between these components as follows:</p>
    <ul>
      <li class="bulletList">The output of the forget gate, <em class="italic">f</em>, at time step <em class="italic">t</em> is computed as </li>
    </ul>
    <p class="center"><em class="italic">f</em>=<em class="italic">sigmoid</em>(<em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">f</sup><em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>+<em class="italic">U</em><sup class="superscript-italic" style="font-style: italic;">f</sup><em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>)</p>
    <ul>
      <li class="bulletList">The output of the input gate, <em class="italic">i</em>, at time step <em class="italic">t</em> is computed as </li>
    </ul>
    <p class="center"><em class="italic">i</em>=<em class="italic">sigmoid</em>(<em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">i</sup><em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>+<em class="italic">U</em><sup class="superscript-italic" style="font-style: italic;">i</sup><em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>)</p>
    <ul>
      <li class="bulletList">The output of the tanh activation, <em class="italic">c’</em>, at time step <em class="italic">t</em> is computed as </li>
    </ul>
    <p class="center"><em class="italic">c’</em>=<em class="italic">tanh</em>(<em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">c</sup><em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>+<em class="italic">U</em><sup class="superscript-italic" style="font-style: italic;">c</sup><em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>)</p>
    <ul>
      <li class="bulletList">The output of the output gate, <em class="italic">o</em>, at time step <em class="italic">t</em> is computed as </li>
    </ul>
    <p class="center"><em class="italic">o</em>=<em class="italic">sigmoid</em>(<em class="italic">W</em><sup class="superscript-italic" style="font-style: italic;">o</sup><em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>+<em class="italic">U</em><sup class="superscript-italic" style="font-style: italic;">o</sup><em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>)</p>
    <ul>
      <li class="bulletList">The memory unit, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, at time step <em class="italic">t</em> is updated using <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">t</sub>=<em class="italic">f</em>.*<em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>+<em class="italic">i</em>.*<em class="italic">c’</em> (here, the operator .* denotes element-wise multiplication). Again, the output of a sigmoid function has a value from 0 to 1. Hence, the forget gate, <em class="italic">f</em>, and input gate, <em class="italic">i</em>, control how much of the previous memory, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>, and the current memory input, <em class="italic">c’</em>, to carry forward, respectively.</li>
      <li class="bulletList">Finally, we update the hidden state, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, at time step <em class="italic">t</em> by <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">t</sub>=<em class="italic">o</em>.*<em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">t </sub>. Here, the output gate, <em class="italic">o</em>, governs how much of the updated memory unit, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, will be used as the output of the entire cell.</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">LSTM is often considered the default choice for RNN models in practice due to its ability to effectively capture long-term dependencies in sequential data while mitigating the vanishing gradient problem. However, GRUs are also commonly used depending on the specific task and dataset characteristics. The choice between LSTM and GRU depends on the following factors:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Model complexity</strong>: LSTMs typically have more parameters than GRUs due to their additional gating mechanisms. If you have limited computational resources or are working with smaller datasets, GRUs may be more suitable due to their simpler architecture.</li>
        <li class="bulletList"><strong class="keyWord">Training speed</strong>: GRUs are generally faster to train than LSTMs. If training time is a concern, GRUs might be a better choice.</li>
        <li class="bulletList"><strong class="keyWord">Performance</strong>: LSTMs tend to have better performance on tasks that require modeling long-term dependencies in sequential data. If your task involves capturing complex temporal patterns and you’re concerned about overfitting, LSTMs might be preferable.</li>
      </ul>
    </div>
    <p class="normal">As always, we <a id="_idIndexMarker1227"/>apply the <strong class="keyWord">BPTT</strong> algorithm to train all the weights in LSTM RNNs, including four sets each of weights, <em class="italic">U</em> and <em class="italic">W</em>, associated with three gates and the tanh activation function. By learning these weights, the LSTM network explicitly models long-term dependencies in an efficient way. Hence, LSTM is the go-to or default RNN model in practice.</p>
    <p class="normal">Next, you will learn how to use LSTM RNNs to solve real-world problems. We will start by categorizing movie review sentiment.</p>
    <h1 class="heading-1" id="_idParaDest-297">Analyzing movie review sentiment with RNNs</h1>
    <p class="normal">So, here comes our first <a id="_idIndexMarker1228"/>RNN project: movie review <a id="_idIndexMarker1229"/>sentiment. We’ll use the IMDb (<a href="https://www.imdb.com/"><span class="url">https://www.imdb.com/</span></a>) movie review dataset (<a href="https://ai.stanford.edu/~amaas/data/sentiment/"><span class="url">https://ai.stanford.edu/~amaas/data/sentiment/</span></a>) as an example. It contains 25,000 highly popular movie reviews for training and another 25,000 for testing. Each review is labeled as 1 (positive) or 0 (negative). We’ll build our RNN-based movie sentiment classifier in the following three sections: <em class="italic">Analyzing and preprocessing the movie review data, Developing a simple LSTM network, </em>and<em class="italic"> Boosting the performance with multiple LSTM layers</em>.</p>
    <h2 class="heading-2" id="_idParaDest-298">Analyzing and preprocessing the data</h2>
    <p class="normal">We’ll start<a id="_idIndexMarker1230"/> with data analysis and <a id="_idIndexMarker1231"/>preprocessing, as follows:</p>
    <ol>
      <li class="numberedList" value="1">PyTorch’s <code class="inlineCode">torchtext</code> has a built-in IMDb dataset, so first, we load the dataset:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torchtext.datasets </span><span class="hljs-con-keyword">import</span><span class="language-python"> IMDB</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dataset = </span><span class="hljs-con-built_in">list</span><span class="language-python">(IMDB(split=</span><span class="hljs-con-string">'train'</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_dataset = </span><span class="hljs-con-built_in">list</span><span class="language-python">(IMDB(split=</span><span class="hljs-con-string">'test'</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-built_in">len</span><span class="language-python">(train_dataset), </span><span class="hljs-con-built_in">len</span><span class="language-python">(test_dataset))</span>
25000 25000
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We just load 25,000 training samples and 25,000 test samples.</p>
    <p class="normal-one">If you encounter any errors while running the code, consider installing the <code class="inlineCode">torchtext</code> and <code class="inlineCode">portalocker</code> packages. You could use the following commands for installation via <code class="inlineCode">conda</code>:</p>
    <pre class="programlisting con-one"><code class="hljs-con">conda install -c torchtext
conda install -c conda-forge portalocker
</code></pre>
    <p class="normal-one">Or, via <code class="inlineCode">pip</code>:</p>
    <pre class="programlisting con-one"><code class="hljs-con">pip install portalocker
</code></pre>
    <ol>
      <li class="numberedList" value="2">Now, let’s explore the vocabulary within the training set:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> re</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> collections </span><span class="hljs-con-keyword">import</span><span class="language-python"> Counter, OrderedDict</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">tokenizer</span><span class="language-python">(</span><span class="hljs-con-params">text</span><span class="language-python">):</span>
        text = re.sub('&lt;[^&gt;]*&gt;', '', text)
        emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text.lower())
        text = re.sub('[\W]+', ' ', text.lower()) +\
                              ' '.join(emoticons).replace('-', '')
        tokenized = text.split()
        return tokenized
<span class="hljs-con-meta">&gt;&gt;&gt;</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">token_counts = Counter()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_labels = []</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> label, line </span><span class="hljs-con-keyword">in</span><span class="language-python"> train_dataset:</span>
        train_labels.append(label)
        tokens = tokenizer(line)
        token_counts.update(tokens)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">'</span><span class="hljs-con-string">Vocab-size:'</span><span class="language-python">, </span><span class="hljs-con-built_in">len</span><span class="language-python">(token_counts))</span>
Vocab-size: 75977
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(Counter(train_labels))</span>
Counter({1: 12500, 2: 12500})
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, we define a function to extract tokens (words, in our case) from a given document (movie review, in our case). It first removes HTML-like tags, then extracts and standardizes emoticons, removes non-alphanumeric characters, and tokenizes the text into a list of words for further processing. We store the tokens and their occurrences in the <code class="inlineCode">Counter</code> object <code class="inlineCode">token_counts</code>.</p>
    <p class="normal-one">As evident, the training set comprises approximately 76,000 unique words, and it exhibits a perfect balance with an equal count of positive (labeled as “<code class="inlineCode">2</code>") and negative (labeled as “<code class="inlineCode">1</code>") samples.</p>
    <ol>
      <li class="numberedList" value="3">We will <a id="_idIndexMarker1232"/>feed the word tokens into an embedding layer, <code class="inlineCode">nn.Embedding</code>. The embedding layer requires integer input because it’s specifically designed <a id="_idIndexMarker1233"/>to handle discrete categorical data, such as word indices, and transform them into continuous representations that a neural network can work with and learn from. Therefore, we need to first encode each token into a unique integer as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torchtext.vocab </span><span class="hljs-con-keyword">import</span><span class="language-python"> vocab</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sorted_by_freq_tuples = </span><span class="hljs-con-built_in">sorted</span><span class="language-python">(token_counts.items(), key=</span><span class="hljs-con-keyword">lambda</span><span class="language-python"> x: x[</span><span class="hljs-con-number">1</span><span class="language-python">],</span>
                                   reverse=True)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">ordered_dict = OrderedDict(sorted_by_freq_tuples)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vocab_mapping = vocab(ordered_dict)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We use the <code class="inlineCode">vocab</code> module in PyTorch to create a vocabulary (token mapping) based on the frequency of words in the corpus. But this vocabulary is not complete yet. Let’s see why in the next two steps.</p>
    <ol>
      <li class="numberedList" value="4">When examining the document lengths within the training set, you’ll notice that they range from 10 to 2,498 words. It’s common practice to apply padding to sequences to ensure uniform length during batch processing. So, we insert the special token, <code class="inlineCode">"&lt;pad&gt;"</code>, representing padding into the vocabulary mapping at index <code class="inlineCode">0</code> as a <strong class="keyWord">placeholder</strong>:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vocab_mapping.insert_token(</span><span class="hljs-con-string">"&lt;pad&gt;"</span><span class="language-python">, </span><span class="hljs-con-number">0</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">We also need to handle unseen words during inference. Similar to the previous step, we insert the special token <code class="inlineCode">"&lt;unk&gt;"</code> (short for “unknown”) into the vocabulary mapping at index <code class="inlineCode">1</code>. The token represents out-of-vocabulary <a id="_idIndexMarker1234"/>words or <a id="_idIndexMarker1235"/>tokens that are not found in the training data:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vocab_mapping.insert_token(</span><span class="hljs-con-string">"&lt;unk&gt;"</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vocab_mapping.set_default_index(</span><span class="hljs-con-number">1</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We also set the default vocabulary mapping to <code class="inlineCode">1</code>. This means <code class="inlineCode">"&lt;unk&gt;"</code> (index 1) is used as the default index for unseen or out-of-vocabulary words.</p>
    <p class="normal-one">Let’s take a look at the following examples showing the mappings of given words, including an unseen one:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">([vocab_mapping[token] </span><span class="hljs-con-keyword">for</span><span class="language-python"> token </span><span class="hljs-con-keyword">in</span><span class="language-python"> [</span><span class="hljs-con-string">'this'</span><span class="language-python">, </span><span class="hljs-con-string">'is'</span><span class="language-python">, </span><span class="hljs-con-string">'an'</span><span class="language-python">, </span><span class="hljs-con-string">'example'</span><span class="language-python">]])</span>
[11, 7, 35, 462]
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">([vocab_mapping[token] </span><span class="hljs-con-keyword">for</span><span class="language-python"> token </span><span class="hljs-con-keyword">in</span><span class="language-python"> [</span><span class="hljs-con-string">'this'</span><span class="language-python">, </span><span class="hljs-con-string">'is'</span><span class="language-python">, </span><span class="hljs-con-string">'example2'</span><span class="language-python">]])</span>
[11, 7, 1]
</code></pre>
    <p class="normal-one">By now, we have the complete vocabulary mapping.</p>
    <div class="note-one">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">Using special tokens like <code class="inlineCode">&lt;pad&gt;</code> and <code class="inlineCode">&lt;unk&gt;</code> in RNNs is a common practice for handling variable-length sequences and out-of-vocabulary words. Here are some best practices for their usage:</p>
      <ul>
        <li class="bulletList">Use <code class="inlineCode">&lt;pad&gt;</code> tokens to pad sequences to a fixed length. This ensures that all input sequences have the same length, which is necessary for efficient batch processing in neural networks. Pad sequences at the end rather than the beginning to preserve the order of the input data. When tokenizing text data, assign a unique integer index to the <code class="inlineCode">&lt;pad&gt;</code> token and ensure that it corresponds to a vector of zeros in the embedding matrix.</li>
        <li class="bulletList">Use <code class="inlineCode">&lt;unk&gt;</code> tokens to represent out-of-vocabulary words that are not present in the vocabulary of the model. During inference, replace any words that are not present in the vocabulary with the <code class="inlineCode">&lt;unk&gt;</code> token to ensure that the model can process the input.</li>
        <li class="bulletList">Exclude <code class="inlineCode">&lt;pad&gt;</code> tokens from contributing to the loss during training to avoid skewing the learning process.</li>
        <li class="bulletList">Monitor the distribution of <code class="inlineCode">&lt;unk&gt;</code> tokens in the dataset to assess the prevalence of out-of-vocabulary words and adjust the vocabulary size accordingly.</li>
      </ul>
    </div>
    <ol>
      <li class="numberedList" value="6">Next, we define<a id="_idIndexMarker1236"/> the function defining <a id="_idIndexMarker1237"/>how batches of samples should be collated:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span> torch
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span> torch.nn as nn
<span class="hljs-con-meta">&gt;&gt;&gt;</span> device = torch.device(<span class="hljs-con-string">"cuda"</span> <span class="hljs-con-keyword">if</span> torch.cuda.is_available() <span class="hljs-con-keyword">else</span> <span class="hljs-con-string">"cpu"</span>)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> text_transform = lambda x: [vocab[token] <span class="hljs-con-keyword">for</span> token <span class="hljs-con-keyword">in</span> tokenizer(x)]
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">collate_batch</span><span class="language-python">(</span><span class="hljs-con-params">batch</span><span class="language-python">):</span>
        label_list, text_list, lengths = [], [], []
        for _label, _text in batch:
            label_list.append(1. if _label == 2 else 0.)
            processed_text = [vocab_mapping[token] for token in tokenizer(_text)] 
            text_list.append(torch.tensor(processed_text, dtype=torch.int64))
            lengths.append(len(processed_text))
        label_list = torch.tensor(label_list)
        lengths = torch.tensor(lengths)
        padded_text_list = nn.utils.rnn.pad_sequence(
                                text_list, batch_first=True)
        return padded_text_list.to(device), label_list.to(device),
                                                      lengths.to(device)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Besides <a id="_idIndexMarker1238"/>generating inputs and label outputs as we used to do, we also generate the length of individual samples in a given batch. Note that we convert the positive label from the raw 2 to 1 here, for label standardization and loss function<a id="_idIndexMarker1239"/> compatibility for binary classification. The length information is used for handling variable-length sequences efficiently. Take a small batch of four samples and examine the processed batch:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torch.utils.data </span><span class="hljs-con-keyword">import</span><span class="language-python"> DataLoader</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">dataloader = DataLoader(train_dataset, batch_size=</span><span class="hljs-con-number">4</span><span class="language-python">, shuffle=</span><span class="hljs-con-literal">True</span><span class="language-python">,</span>
                            collate_fn=collate_batch)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">text_batch, label_batch, length_batch = </span><span class="hljs-con-built_in">next</span><span class="language-python">(</span><span class="hljs-con-built_in">iter</span><span class="language-python">(dataloader))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(text_batch)</span>
tensor([[   46,     8,   287,    21,    16,     2,    76,  3987,     3,   226, 10,   381,     2,   461,    14,    65,     9,  1208,    17,     8, 13,   856,     2,   156,    70,   398,    50,    32,  2338,    67, 103,     6,   110,    19,     9,     2,   130,     2,   153,    12, 14,    65,  1002,    14,     4,  1143,   226,     6,  1061,    31, 2,  1317,   293,    10,    61,   542,  1459,    24,     6,   105,
<span class="hljs-con-meta">...</span>
<span class="hljs-con-meta">...</span>
<span class="hljs-con-meta">...</span>
0,     0,     0,     0,     0,     0,     0,     0,     0,     0, 0,     0,     0,     0,     0,     0,     0,     0,     0,     0, 0,     0,     0,     0,     0,     0,     0]], device='cuda:0')
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(label_batch)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tensor([</span><span class="hljs-con-number">0.</span><span class="language-python">, </span><span class="hljs-con-number">1.</span><span class="language-python">, </span><span class="hljs-con-number">1.</span><span class="language-python">, </span><span class="hljs-con-number">0.</span><span class="language-python">], device=</span><span class="hljs-con-string">'cuda:0'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(length_batch)</span>
tensor([106,  76, 247, 158], device='cuda:0')
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(text_batch.shape)</span>
torch.Size([4, 247])
</code></pre>
    <p class="normal-one">You can see the processed text sequences have been standardized to a length of 247 tokens, with the first, second, and fourth samples padded with 0s.</p>
    <ol>
      <li class="numberedList" value="7">Finally, we<a id="_idIndexMarker1240"/> batch the<a id="_idIndexMarker1241"/> training and testing set:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">batch_size = </span><span class="hljs-con-number">32</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dl = DataLoader(train_dataset, batch_size=batch_size, </span>
<span class="language-python">                          shuffle=</span><span class="hljs-con-literal">True</span><span class="language-python">, collate_fn=collate_batch)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_dl = DataLoader(test_dataset, batch_size=batch_size,</span>
                         shuffle=False, collate_fn=collate_batch)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The<a id="_idIndexMarker1242"/> generated data loaders are ready to use for sentiment prediction.</p>
    <p class="normal">Let’s move on to building an LSTM network.</p>
    <h2 class="heading-2" id="_idParaDest-299">Building a simple LSTM network</h2>
    <p class="normal">Now that the<a id="_idIndexMarker1243"/> training and testing data loaders are ready, we can build our first RNN model with an embedding layer that encodes the input word tokens, and an LSTM layer followed by a fully connected layer:</p>
    <ol>
      <li class="numberedList" value="1">First, we define the network hyperparameters, including the input dimension and the embedding dimension of the embedding layer:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vocab_size = </span><span class="hljs-con-built_in">len</span><span class="language-python">(vocab_mapping)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">embed_dim = </span><span class="hljs-con-number">32</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We also define the number of hidden nodes in the LSTM layer and the fully connected layer:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">rnn_hidden_dim = </span><span class="hljs-con-number">50</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">fc_hidden_dim = </span><span class="hljs-con-number">32</span>
</code></pre>
    <ol>
      <li class="numberedList" value="2">Next, we build our RNN model class:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">class</span><span class="language-python"> </span><span class="hljs-con-title">RNN</span><span class="language-python">(nn.Module):</span>
        def __init__(self, vocab_size, embed_dim, rnn_hidden_dim, fc_hidden_dim):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size,
                                          embed_dim,
                                          padding_idx=0)
            self.rnn = nn.LSTM(embed_dim, rnn_hidden_dim,
                               batch_first=True)
            self.fc1 = nn.Linear(rnn_hidden_dim, fc_hidden_dim)
            self.relu = nn.ReLU()
            self.fc2 = nn.Linear(fc_hidden_dim, 1)
            self.sigmoid = nn.Sigmoid()
        def forward(self, text, lengths):
            out = self.embedding(text)
            out = nn.utils.rnn.pack_padded_sequence(
                                            out, lengths.cpu().numpy(),
                                          enforce_sorted=False,
                                          batch_first=True)
            out, (hidden, cell) = self.rnn(out)
            out = hidden[-1, :, :]
            out = self.fc1(out)
            out = self.relu(out)
            out = self.fc2(out)
            out = self.sigmoid(out)
            return out
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The <code class="inlineCode">nn.Embedding</code> layer is used to convert input word indices into dense word embeddings. The <code class="inlineCode">padding_idx</code> parameter is set to 0, indicating that padding tokens should be ignored during embedding.</p>
    <p class="normal-one">The recurrent layer, <code class="inlineCode">nn.LSTM</code>, takes the embedded input sequence and processes it sequentially. <code class="inlineCode">batch_first=True</code> means that the input has a batch size as the first dimension.</p>
    <p class="normal-one">The fully <a id="_idIndexMarker1244"/>connected<a id="_idIndexMarker1245"/> hidden layer, <code class="inlineCode">fc1</code>, follows the LSTM layer, and the ReLU activation is applied to the output of the fully connected layer.</p>
    <p class="normal-one">The final layer has a single output because this model is used for binary classification (sentiment analysis).</p>
    <p class="normal-one">In the forward pass method, <code class="inlineCode">pack_padded_sequence</code> is used to pack and pad sequences for efficient processing in the LSTM layer. The packed sequence is passed through the LSTM layer, and the final hidden state (<code class="inlineCode">hidden[-1, :, :]</code>) is extracted.</p>
    <ol>
      <li class="numberedList" value="3">We then create an instance of the LSTM model with the specific hyperparameters we defined earlier. We also ensure that the model is placed on the specified computing device (GPU if available) for training and inference:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = RNN(vocab_size, embed_dim, rnn_hidden_dim, fc_hidden_dim)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = model.to(device)</span>
</code></pre>
      </li>
      <li class="numberedList">As for the loss function, we use <code class="inlineCode">nn.BCELoss()</code> since it is a binary classification problem. We also set the corresponding optimizer and try with a learning rate of <code class="inlineCode">0.003</code> as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">loss_fn = nn.BCELoss()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">0.003</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Now, we <a id="_idIndexMarker1246"/>define a <a id="_idIndexMarker1247"/>training function responsible for training the model for one iteration:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">train</span><span class="language-python">(</span><span class="hljs-con-params">model, dataloader, optimizer</span><span class="language-python">):</span>
        model.train()
        total_acc, total_loss = 0, 0
        for text_batch, label_batch, length_batch in dataloader:
            optimizer.zero_grad()
            pred = model(text_batch, length_batch)[:, 0]
            loss = loss_fn(pred, label_batch)
            loss.backward()
            optimizer.step()
            total_acc += ((pred&gt;=0.5).float() == label_batch)
                                                  .float().sum().item()
            total_loss += loss.item()*label_batch.size(0)
        total_loss /= len(dataloader.dataset)
        total_acc /= len(train_dl.dataset)
        print(f'Epoch {epoch+1} - loss: {total_loss:.4f} - accuracy:
               {total_acc:.4f}')
</code></pre>
      </li>
    </ol>
    <p class="normal-one">It also displays the training loss and accuracy at the end of an iteration.</p>
    <ol>
      <li class="numberedList" value="6">We then train the model for 10 iterations:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">num_epochs = </span><span class="hljs-con-number">10</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(num_epochs):</span>
        train(model, train_dl, optimizer)
Epoch 1 - loss: 0.5899 - accuracy: 0.6707
Epoch 2 - loss: 0.4354 - accuracy: 0.8019
Epoch 3 - loss: 0.2762 - accuracy: 0.8888
Epoch 4 - loss: 0.1766 - accuracy: 0.9341
Epoch 5 - loss: 0.1215 - accuracy: 0.9563
Epoch 6 - loss: 0.0716 - accuracy: 0.9761
Epoch 7 - loss: 0.0417 - accuracy: 0.9868
Epoch 8 - loss: 0.0269 - accuracy: 0.9912
Epoch 9 - loss: 0.0183 - accuracy: 0.9943
Epoch 10 - loss: 0.0240 - accuracy: 0.9918
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The training accuracy is close to 100% after 10 iterations.</p>
    <ol>
      <li class="numberedList" value="7">Finally, we<a id="_idIndexMarker1248"/> evaluate the<a id="_idIndexMarker1249"/> performance on the test set:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">evaluate</span><span class="language-python">(</span><span class="hljs-con-params">model, dataloader</span><span class="language-python">):</span>
        model.eval()
        total_acc = 0
        with torch.no_grad():
        for text_batch, label_batch, lengths in dataloader:
            pred = model(text_batch, lengths)[:, 0]
            total_acc += ((pred&gt;=0.5).float() == label_batch)
                                                 .float().sum().item()
print(f'Accuracy on test set: {100 * total_acc/len(dataloader.dataset)} %')
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">evaluate(model, test_dl)</span>
Accuracy on test set: 86.1 %
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We obtained a test accuracy of 86%.</p>
    <h2 class="heading-2" id="_idParaDest-300">Stacking multiple LSTM layers</h2>
    <p class="normal">We can also<a id="_idIndexMarker1250"/> stack two (or more) recurrent layers. The following diagram shows how two recurrent layers can be <a id="_idIndexMarker1251"/>stacked:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, diagram, line  Description automatically generated" src="../Images/B21047_12_11.png"/></figure>
    <p class="packt_figref">Figure 12.11: Unfolding two stacked recurrent layers</p>
    <p class="normal">In PyTorch, stacking multiple RNN layers is simple. Using LSTM as an example once more, it suffices to specify the number of LSTM layers in the <code class="inlineCode">num_layers</code> argument:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nn.LSTM(embed_dim, rnn_hidden_dim, num_layers=</span><span class="hljs-con-number">2</span><span class="language-python">, batch_first=</span><span class="hljs-con-literal">True</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">In this example, we stack two LSTM layers. Feel free to experiment with a multi-layer RNN model<a id="_idIndexMarker1252"/> and see whether you can beat the previous single-layered model.</p>
    <p class="normal">With that, we’ve just finished the review sentiment classification project using RNNs. In the next project, we will revisit stock price prediction and solve it using RNNs.</p>
    <h1 class="heading-1" id="_idParaDest-301">Revisiting stock price forecasting with LSTM</h1>
    <p class="normal">Recall in <em class="chapterRef">Chapter 6</em><em class="italic">, Predicting Stock Prices with Artificial Neural Networks</em>, we derived features from past <a id="_idIndexMarker1253"/>prices and performance<a id="_idIndexMarker1254"/> within a specific time step and then trained a standard neural network. In this instance, we will utilize RNNs as the sequential model and harness features from five consecutive time steps rather than just one. Let’s examine the process in the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Initially, we load the stock data, create the features and labels, and then split it into training and test sets, mirroring our approach in <em class="chapterRef">Chapter 6</em>:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_raw = pd.read_csv(</span><span class="hljs-con-string">'19900101_20230630.csv'</span><span class="language-python">, index_col=</span><span class="hljs-con-string">'Date'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data = generate_features(data_raw)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">start_train = </span><span class="hljs-con-string">'1990-01-01'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">end_train = </span><span class="hljs-con-string">'2022-12-31'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">start_test = </span><span class="hljs-con-string">'2023-01-01'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">end_test = </span><span class="hljs-con-string">'2023-06-30'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_train = data.loc[start_train:end_train]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_train = data_train.drop(</span><span class="hljs-con-string">'close'</span><span class="language-python">, axis=</span><span class="hljs-con-number">1</span><span class="language-python">).values</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_train = data_train[</span><span class="hljs-con-string">'close'</span><span class="language-python">].values</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_test = data.loc[start_test:end_test]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_test = data_test.drop(</span><span class="hljs-con-string">'close'</span><span class="language-python">, axis=</span><span class="hljs-con-number">1</span><span class="language-python">).values</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_test = data_test[</span><span class="hljs-con-string">'close'</span><span class="language-python">].values</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, we reuse the feature and label generation function, <code class="inlineCode">generate_features</code>, defined in <em class="chapterRef">Chapter 6</em>. Similarly, we scale the feature space using <code class="inlineCode">StandardScaler</code> and covert data into <code class="inlineCode">FloatTensor</code>:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.preprocessing </span><span class="hljs-con-keyword">import</span><span class="language-python"> StandardScaler</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">scaler = StandardScaler()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_scaled_train = torch.FloatTensor(scaler.fit_transform(X_train))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_scaled_test = torch.FloatTensor(scaler.transform(X_test))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_train_torch = torch.FloatTensor(y_train)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_test_torch = torch.FloatTensor(y_test)</span>
</code></pre>
    <ol>
      <li class="numberedList" value="2">Next, we define a function to create sequences:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">create_sequences</span><span class="language-python">(</span><span class="hljs-con-params">data, labels, seq_length</span><span class="language-python">):</span>
        sequences = []
        for i in range(len(data) - seq_length):
            seq = data[i:i+seq_length]
            label = labels[i+seq_length-1]
            sequences.append((seq, label))
        return sequences
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">seq_length = </span><span class="hljs-con-number">5</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sequence_train = create_sequences(X_scaled_train, y_train_torch, seq_length)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sequence_test = create_sequences(X_scaled_test, y_test_torch, seq_length)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, every generated sequence comprises two components: the input sequence, which encompasses features from five successive days, and the label, representing the price of the last day in that five-day period. We generate sequences for the training and test sets respectively.</p>
    <ol>
      <li class="numberedList" value="3">Subsequently, we establish a data loader for the training sequences in preparation for model construction and training:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">batch_size = </span><span class="hljs-con-number">128</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dl = DataLoader(sequence_train, batch_size=batch_size,</span>
<span class="language-python">                          shuffle=</span><span class="hljs-con-literal">True</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We set 128 as <a id="_idIndexMarker1255"/>the batch <a id="_idIndexMarker1256"/>size in this project.</p>
    <ol>
      <li class="numberedList" value="4">Now, we define an RNN model with a two-layered LSTM followed by a fully connected layer and an output layer for regression:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">class</span><span class="language-python"> </span><span class="hljs-con-title">RNN</span><span class="language-python">(nn.Module):</span>
        def __init__(self, input_dim, rnn_hidden_dim, fc_hidden_dim):
            super().__init__()
            self.rnn = nn.LSTM(input_dim, rnn_hidden_dim, 2,
                               batch_first=True)
            self.fc1 = nn.Linear(rnn_hidden_dim, fc_hidden_dim)
            self.relu = nn.ReLU()
            self.fc2 = nn.Linear(fc_hidden_dim, 1)
        def forward(self, x):
            out, (hidden, cell) = self.rnn(x)
            out = hidden[-1, :, :]
            out = self.fc1(out)
            out = self.relu(out)
            out = self.fc2(out)
            return out
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The LSTM layer captures sequential dependencies in the input data, and the fully connected layers perform the final regression.</p>
    <ol>
      <li class="numberedList" value="5">Next, we initiate a model after specifying the input dimension and hidden layer dimensions, and use MSE as the loss function:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">rnn_hidden_dim = </span><span class="hljs-con-number">16</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">fc_hidden_dim = </span><span class="hljs-con-number">16</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = RNN(X_train.shape[</span><span class="hljs-con-number">1</span><span class="language-python">], rnn_hidden_dim, fc_hidden_dim)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">device = torch.device(</span><span class="hljs-con-string">"cuda"</span><span class="language-python"> </span><span class="hljs-con-keyword">if</span><span class="language-python"> torch.cuda.is_available() </span><span class="hljs-con-keyword">else</span><span class="language-python"> </span><span class="hljs-con-string">"cpu"</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = model.to(device)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">loss_fn = nn.MSELoss()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">0.01</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Small to medium values (like 16) are often used as starting points for hidden dimensions in RNNs, for computational efficiency. The chosen optimizer (Adam) and learning rate (<code class="inlineCode">0.01</code>) are hyperparameters that can be tuned for better performance.</p>
    <ol>
      <li class="numberedList" value="6">Next, we<a id="_idIndexMarker1257"/> train the model <a id="_idIndexMarker1258"/>for 1000 iterations as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">train</span><span class="language-python">(</span><span class="hljs-con-params">model, dataloader, optimizer</span><span class="language-python">):</span>
        model.train()
        total_loss = 0
        for seq, label in dataloader:
            optimizer.zero_grad()
            pred = model(seq.to(device))[:, 0]
            loss = loss_fn(pred, label.to(device))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()*label.size(0)
        return total_loss/len(dataloader.dataset)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">num_epochs = </span><span class="hljs-con-number">1000</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(num_epochs):</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">    loss = train(model, train_dl, optimizer)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">    </span><span class="hljs-con-keyword">if</span><span class="language-python"> epoch % </span><span class="hljs-con-number">100</span><span class="language-python"> == </span><span class="hljs-con-number">0</span><span class="language-python">:</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">        </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Epoch </span><span class="hljs-con-subst">{epoch+</span><span class="hljs-con-number">1</span><span class="hljs-con-subst">}</span><span class="hljs-con-string"> - loss: </span><span class="hljs-con-subst">{loss:</span><span class="hljs-con-number">.4</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
Epoch 1 - loss: 24611083.8868
Epoch 101 - loss: 5483.5394
Epoch 201 - loss: 11613.8535
Epoch 301 - loss: 4459.1431
Epoch 401 - loss: 4646.8745
Epoch 501 - loss: 4046.1726
Epoch 601 - loss: 3583.5710
Epoch 701 - loss: 2846.1768
Epoch 801 - loss: 2417.1702
Epoch 901 - loss: 2814.3970
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The MSE during training is displayed every 100 iterations.</p>
    <ol>
      <li class="numberedList" value="7">Finally, we<a id="_idIndexMarker1259"/> apply the trained <a id="_idIndexMarker1260"/>model on the test set and evaluate the performance:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions, y = [], []</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> seq, label </span><span class="hljs-con-keyword">in</span><span class="language-python"> sequence_test:</span>
        with torch.no_grad():
            pred = model.cpu()(seq.view(1, seq_length, X_test.shape[1]))[:, 0]
            predictions.append(pred)
            y.append(label)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.metrics </span><span class="hljs-con-keyword">import</span><span class="language-python"> mean_squared_error, mean_absolute_error, r2_score</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'R^2: </span><span class="hljs-con-subst">{r2_score(y, predictions):</span><span class="hljs-con-number">.3</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
R^2: 0.897
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We are able to obtain an <em class="italic">R</em><sup class="superscript">2</sup> of <code class="inlineCode">0.9</code> on the test set. You may observe that this doesn’t outperform our previous standard neural network. The reason is that we have a relatively small training dataset of only eight thousand samples. RNNs typically require a larger dataset to excel.</p>
    <p class="normal">The two RNN models we’ve explored up to this point followed the many-to-one structure. In our upcoming project, we’ll create an RNN using the many-to-many structure, and the objective is to generate a “novel.”</p>
    <h1 class="heading-1" id="_idParaDest-302">Writing your own War and Peace with RNNs</h1>
    <p class="normal">In this<a id="_idIndexMarker1261"/> project, we’ll work on an interesting<a id="_idIndexMarker1262"/> language modeling problem – text generation.</p>
    <p class="normal">An RNN-based text generator can write anything, depending on what text we feed it. The training text can be from a novel such as <em class="italic">A Game of Thrones</em>, a poem from Shakespeare, or the movie scripts for <em class="italic">The Matrix</em>. The artificial text that’s generated should read similarly (but not identically) to the original one if the model is well trained. In this section, we are going to write our own <em class="italic">War and Peace</em> with RNNs, a novel written by the Russian author Leo Tolstoy. Feel free to train your own RNNs on any of your favorite books.</p>
    <p class="normal">We will start with data acquisition and analysis before constructing the training set. After that, we will build and train an RNN model for text generation.</p>
    <h2 class="heading-2" id="_idParaDest-303">Acquiring and analyzing the training data</h2>
    <p class="normal">I <a id="_idIndexMarker1263"/>recommend downloading text<a id="_idIndexMarker1264"/> data for training from books that are not currently protected by copyright. Project Gutenberg (<a href="https://www.gutenberg.org"><span class="url">www.gutenberg.org</span></a>) is a great place for this. It provides over 60,000 free e-books whose copyright has expired.</p>
    <p class="normal">The original work, <em class="italic">War and Peace</em>, can be downloaded from <a href="http://www.gutenberg.org/ebooks/2600"><span class="url">http://www.gutenberg.org/ebooks/2600</span></a>, but note that there will be some cleanup, such as removing the extra beginning section “<em class="italic">The Project Gutenberg EBook</em>,” the table of contents, and the extra appendix “<em class="italic">End of the Project Gutenberg EBook of War and Peace</em>” of the plain text UTF-8 file (<a href="http://www.gutenberg.org/files/2600/2600-0.txt"><span class="url">http://www.gutenberg.org/files/2600/2600-0.txt</span></a>), required. So, instead of doing this, we will download the cleaned text file directly from <a href="https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt"><span class="url">https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt</span></a>. Let’s get started:</p>
    <ol>
      <li class="numberedList" value="1">First, we read the file and convert the text into lowercase:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">with</span><span class="language-python"> </span><span class="hljs-con-built_in">open</span><span class="language-python">(</span><span class="hljs-con-string">'warpeace_input.txt'</span><span class="language-python">, </span><span class="hljs-con-string">'r'</span><span class="language-python">, encoding=</span><span class="hljs-con-string">"utf8"</span><span class="language-python">) </span><span class="hljs-con-keyword">as</span><span class="language-python"> fp:</span>
        raw_text = fp.read()
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">raw_text = raw_text.lower()</span>
</code></pre>
      </li>
      <li class="numberedList">Then, we take a quick look at the training text data by printing out the first 200 characters:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(raw_text[:</span><span class="hljs-con-number">200</span><span class="language-python">])</span>
"well, prince, so genoa and lucca are now just family estates of the
buonapartes. but i warn you, if you don't tell me that this means war,
if you still try to defend the infamies and horrors perpetr
</code></pre>
      </li>
      <li class="numberedList">Next, we count the number of unique words:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">all_words = raw_text.split()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">unique_words = </span><span class="hljs-con-built_in">list</span><span class="language-python">(</span><span class="hljs-con-built_in">set</span><span class="language-python">(all_words))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Number of unique words: </span><span class="hljs-con-subst">{</span><span class="hljs-con-built_in">len</span><span class="hljs-con-subst">(unique_words)}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
Number of unique words: 39830
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Then, we count the total number of characters:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">n_chars = </span><span class="hljs-con-built_in">len</span><span class="language-python">(raw_text)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Total characters: </span><span class="hljs-con-subst">{n_chars}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
Total characters: 3196213
</code></pre>
    <ol>
      <li class="numberedList" value="4">From these 3 million characters, we obtain the unique characters, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">chars = </span><span class="hljs-con-built_in">sorted</span><span class="language-python">(</span><span class="hljs-con-built_in">list</span><span class="language-python">(</span><span class="hljs-con-built_in">set</span><span class="language-python">(raw_text)))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vocab_size = </span><span class="hljs-con-built_in">len</span><span class="language-python">(chars)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Total vocabulary (unique characters): </span><span class="hljs-con-subst">{vocab_size}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
Total vocabulary (unique characters): 57
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(chars)</span>
['\n', ' ', '!', '"', "'", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'ä', 'é', 'ê', '\ufeff']
</code></pre>
      </li>
    </ol>
    <p class="normal">The raw <a id="_idIndexMarker1265"/>training text is made up of 57 <a id="_idIndexMarker1266"/>unique characters and close to 40,000 unique words. Generating words, which requires computing 40,000 probabilities at one step, is far more difficult than generating characters, which requires computing only 57 probabilities at one step. Hence, we treat a character as a token, and the vocabulary here is composed of 57 characters.</p>
    <p class="normal">So, how can we feed the characters to the RNN model and generate output characters? Let’s see in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-304">Constructing the training set for the RNN text generator</h2>
    <p class="normal">Recall that in a synced “many-to-many” RNN, the network takes in a sequence and simultaneously <a id="_idIndexMarker1267"/>produces a sequence; the model captures the relationships among the elements in a sequence and reproduces a new sequence based on the learned patterns. As for our text generator, we can feed in fixed-length sequences of characters and let it generate sequences of the same length, where each output sequence is one character shifted from its input sequence. The <a id="_idIndexMarker1268"/>following example will help you understand this better.</p>
    <p class="normal">Say that we have a raw text sample, “<code class="inlineCode">learning</code>,” and we want the sequence length to be 5. Here, we can have an input sequence, “<code class="inlineCode">learn</code>,” and an output sequence, “<code class="inlineCode">earni</code>.” We can put them into the network as follows:</p>
    <figure class="mediaobject"><img alt="A diagram of a flowchart  Description automatically generated with low confidence" src="../Images/B21047_12_12.png"/></figure>
    <p class="packt_figref">Figure 12.12: Feeding a training set (“learn,” “earni”) to the RNN</p>
    <p class="normal">We’ve just <a id="_idIndexMarker1269"/>constructed a training sample <code class="inlineCode">("learn</code>,” “<code class="inlineCode">earni</code>"). Similarly, to construct training samples from the entire original text, first, we need to split the original text into fixed-length sequences, <em class="italic">X</em>; then, we need to ignore the first character of the original text and split shift it into sequences of the same length, <em class="italic">Y</em>. A sequence from <em class="italic">X</em> is the input of a training sample, while the <a id="_idIndexMarker1270"/>corresponding sequence from <em class="italic">Y</em> is the output of the sample. Let’s say we have a raw text sample, “machine learning by example,” and we set the sequence length to 5. We will construct the following training samples:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, number  Description automatically generated" src="../Images/B21047_12_13.png"/></figure>
    <p class="packt_figref">Figure 12.13: Training samples constructed from “machine learning by example”</p>
    <p class="normal">Here, <img alt="" role="presentation" src="../Images/Icon.png"/> denotes space. Note that the remaining subsequence, “le,” is not long enough, so we simply ditch it.</p>
    <p class="normal">We also need to one-hot encode the input and output characters since neural network models only take in numerical data. We simply map the 57 unique characters to indices from 0 to 56, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">index_to_char = </span><span class="hljs-con-built_in">dict</span><span class="language-python">((i, c) </span><span class="hljs-con-keyword">for</span><span class="language-python"> i, c </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">enumerate</span><span class="language-python">(chars))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">char_to_index = </span><span class="hljs-con-built_in">dict</span><span class="language-python">((c, i) </span><span class="hljs-con-keyword">for</span><span class="language-python"> i, c </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">enumerate</span><span class="language-python">(chars))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(char_to_index)</span>
{'\n': 0, ' ': 1, '!': 2, '"': 3, "'": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'a': 26, 'b': 27, 'c': 28, 'd': 29, 'e': 30, 'f': 31, 'g': 32, 'h': 33, 'i': 34, 'j': 35, 'k': 36, 'l': 37, 'm': 38, 'n': 39, 'o': 40, 'p': 41, 'q': 42, 'r': 43, 's': 44, 't': 45, 'u': 46, 'v': 47, 'w': 48, 'x': 49, 'y': 50, 'z': 51, 'à': 52, 'ä': 53, 'é': 54, 'ê': 55, '\ufeff': 56}
</code></pre>
    <p class="normal">For<a id="_idIndexMarker1271"/> instance, the character <code class="inlineCode">c</code> becomes a vector of length 57 with <code class="inlineCode">1</code> in index 28 and <code class="inlineCode">0</code>s in all other indices; the character <code class="inlineCode">h</code> becomes a vector of length 57 with <code class="inlineCode">1</code> in index 33 and <code class="inlineCode">0</code>s in all other indices.</p>
    <p class="normal">Now that the character<a id="_idIndexMarker1272"/> lookup dictionary is ready, we can construct the entire training set, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> numpy </span><span class="hljs-con-keyword">as</span><span class="language-python"> np</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">text_encoded = np.array(char_to_index[ch] </span><span class="hljs-con-keyword">for</span><span class="language-python"> ch </span><span class="hljs-con-keyword">in</span><span class="language-python"> raw_text],</span>
                            dtype=np.int32)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">seq_length = </span><span class="hljs-con-number">40</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">chunk_size = seq_length + </span><span class="hljs-con-number">1</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">text_chunks = np.array([text_encoded[i:i+chunk_size]</span>
                     for i in range(len(text_encoded)-chunk_size+1)])
</code></pre>
    <p class="normal">Here, we set the sequence length to <code class="inlineCode">40</code> and obtain training samples of a length of <code class="inlineCode">41</code> where the first 40 elements represent the input, and the last 40 elements represent the target.</p>
    <p class="normal">Next, we initialize the training dataset object and data loader, which will be used for model training:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torch.utils.data </span><span class="hljs-con-keyword">import</span><span class="language-python"> Dataset</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">class</span><span class="language-python"> </span><span class="hljs-con-title">SeqDataset</span><span class="language-python">(</span><span class="hljs-con-title">Dataset</span><span class="language-python">):</span>
    def __init__(self, text_chunks):
        self.text_chunks = text_chunks
    def __len__(self):
        return len(self.text_chunks)
    def __getitem__(self, idx):
        text_chunk = self.text_chunks[idx]
        return text_chunk[:-1].long(), text_chunk[1:].long()
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">seq_dataset = SeqDataset(torch. from_numpy (text_chunks))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">batch_size = </span><span class="hljs-con-number">64</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=</span><span class="hljs-con-literal">True</span><span class="language-python">,</span>
                       drop_last=True)
</code></pre>
    <p class="normal">We just create a data loader in batches of 64 sequences, shuffle the data at the beginning of each epoch, and drop any remaining data points that don’t fit into a complete batch.</p>
    <p class="normal">We finally got the training set ready and it is time to build and fit the RNN model. Let’s do this in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-305">Building and training an RNN text generator</h2>
    <p class="normal">We first <a id="_idIndexMarker1273"/>build the RNN model as<a id="_idIndexMarker1274"/> follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">class</span><span class="language-python"> </span><span class="hljs-con-title">RNN</span><span class="language-python">(nn.Module):</span>
    def __init__(self, vocab_size, embed_dim, rnn_hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.rnn_hidden_dim = rnn_hidden_dim
        self.rnn = nn.LSTM(embed_dim, rnn_hidden_dim,
                           batch_first=True)
        self.fc = nn.Linear(rnn_hidden_dim, vocab_size)
    def forward(self, x, hidden, cell):
        out = self.embedding(x).unsqueeze(1)
        out, (hidden, cell) = self.rnn(out, (hidden, cell))
        out = self.fc(out).reshape(out.size(0), -1)
        return out, hidden, cell
    def init_hidden(self, batch_size):
        hidden = torch.zeros(1, batch_size, self.rnn_hidden_dim)
        cell = torch.zeros(1, batch_size, self.rnn_hidden_dim)
        return hidden, cell
</code></pre>
    <p class="normal">This class defines a sequence-to-sequence model that takes tokenized input, converts token indices into dense vector representation with an embedding layer, processes the dense vectors through an LSTM layer, and generates logits for the next token in the sequence.</p>
    <p class="normal">In this class, the <code class="inlineCode">init_hidden</code> method initializes the hidden state and cell state of the LSTM. It takes <code class="inlineCode">batch_size</code> as a parameter, which is used to determine the batch size for the initial states. Two tensors are created: <code class="inlineCode">hidden</code> and <code class="inlineCode">cell</code>, both initialized with zeros. The <code class="inlineCode">forward</code> method receives two additional inputs, <code class="inlineCode">hidden</code> and <code class="inlineCode">cell</code>, which correspond to the many-to-many architecture of our RNN model.</p>
    <p class="normal">One more thing to note, we use logits as outputs of the model here instead of probabilities, as we will sample from the predicted logits to generate new sequences of characters.</p>
    <p class="normal">Now, let’s <a id="_idIndexMarker1275"/>train the RNN model we just <a id="_idIndexMarker1276"/>defined as follows:</p>
    <ol>
      <li class="numberedList" value="1">First, we specify the embedding dimension and the size of the LSTM hidden layer, and initiate the RNN model object:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">embed_dim = </span><span class="hljs-con-number">256</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">rnn_hidden_dim = </span><span class="hljs-con-number">512</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = RNN(vocab_size, embed_dim, rnn_hidden_dim)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = model.to(device)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model</span>
RNN(
  (embedding): Embedding(57, 256)
  (rnn): LSTM(256, 512, batch_first=True)
  (fc): Linear(in_features=512, out_features=57, bias=True)
)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">A relatively high embedding dimension (like 256) allows for capturing richer semantic information about words. This can be beneficial for tasks like text generation. However, excessively high dimensions can increase computational cost and might lead to overfitting. 256 provides a good balance between these factors.</p>
    <p class="normal-one">Text generation often requires the model to learn long-term dependencies between words in a sequence. A hidden layer size of 512 offers a good capacity to capture these complex relationships.</p>
    <ol>
      <li class="numberedList" value="2">The next task is to define a loss function and an optimizer. In the case of multiclass classification, where there is a single logit output for each target character, we utilize <code class="inlineCode">CrossEntropyLoss</code> as the appropriate loss function:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">loss_fn = nn.CrossEntropyLoss()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">0.003</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Now, we<a id="_idIndexMarker1277"/> train the model for 10,000 epochs. In each epoch, we train our many-to-many RNN on one training batch selected from the data loader, and we display the <a id="_idIndexMarker1278"/>training loss for every 500 epochs:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">num_epochs = </span><span class="hljs-con-number">10000</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(num_epochs):</span>
        hidden, cell = model.init_hidden(batch_size)
        seq_batch, target_batch = next(iter(seq_dl))
        seq_batch = seq_batch.to(device)
        target_batch = target_batch.to(device)
        optimizer.zero_grad()
        loss = 0
        for c in range(seq_length):
            pred, hidden, cell = model(seq_batch[:, c],
                                       hidden.to(device),
                                       cell.to(device))
            loss += loss_fn(pred, target_batch[:, c])
        loss.backward()
        optimizer.step()
        loss = loss.item()/seq_length
        if epoch % 500 == 0:
            print(f'Epoch {epoch} - loss: {loss:.4f}')
Epoch 0 - loss: 4.0255
Epoch 500 - loss: 1.4560
Epoch 1000 - loss: 1.2794
<span class="hljs-con-meta">...</span>
8500 loss: - 1.2557
Epoch 9000 - loss: 1.2014
Epoch 9500 - loss: 1.2442
</code></pre>
      </li>
    </ol>
    <p class="normal-one">For each element in a given sequence, we feed the recurrent layer with the previous hidden state along with the current input.</p>
    <ol>
      <li class="numberedList" value="4">Model <a id="_idIndexMarker1279"/>training is complete, and now it’s time to assess its performance. We can generate text by providing a few starting words, for instance:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torch.distributions.categorical </span><span class="hljs-con-keyword">import</span><span class="language-python"> Categorical</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">generate_text</span><span class="language-python">(</span><span class="hljs-con-params">model, starting_str, len_generated_text=</span><span class="hljs-con-number">500</span><span class="language-python">):</span>
    encoded_input = torch.tensor([char_to_index[s] for s in starting_str])
    encoded_input = torch.reshape(encoded_input, (1, -1))
    generated_str = starting_str
    model.eval()
    hidden, cell = model.init_hidden(1)
    for c in range(len(starting_str)-1):
        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) 
    last_char = encoded_input[:, -1]
        for _ in range(len_generated_text):
            logits, hidden, cell = model(last_char.view(1), hidden, cell)
            logits = torch.squeeze(logits, 0)
            last_char = Categorical(logits=logits).sample()
            generated_str += str(index_to_char[last_char.item()])
        return generated_str
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.to(</span><span class="hljs-con-string">'cpu'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(generate_text(model, </span><span class="hljs-con-string">'the emperor'</span><span class="language-python">, </span><span class="hljs-con-number">500</span><span class="language-python">))</span>
the emperor!" said he.
"finished! it's all with moscow, it's not get bald hills!" he added the civer with whom and desire to change. they really asked the imperor's field!" she said. alpaty. there happed the cause of the longle matestood itself. "the mercy tiresist between paying so impressions, and till the staff offsicilling petya, the chief dear body, returning quite dispatchma--he turned and ecstatically. "ars doing her dome." said rostov, and the general feelings of the bottom would be the pickled ha
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We generate a 500-character text beginning with our given input “the emperor.” Specifically, we first initialize the hidden and cell state for the RNN model. This is required to start generating text. Then, in the <code class="inlineCode">for</code> loop, we iterate over the characters in the starting text except the last one. For each character in the input, we pass it through the model, updating the hidden and cell states. To <a id="_idIndexMarker1280"/>generate the next character index, we predict the logits for all possible characters and sample it based on the logits utilizing a <code class="inlineCode">Categorical</code> distribution. With that, we’ve successfully used a many-to-many type of RNN to<a id="_idIndexMarker1281"/> generate text.</p>
    <p class="normal-one">Feel free to tweak the model so that the RNN-based text generator can write a more realistic and interesting version of <em class="italic">War and Peace</em>.</p>
    <p class="normal">An RNN with a many-to-many structure is a type of sequence-to-sequence (seq2seq) model that takes in a sequence and outputs another sequence. A typical example is machine translation, where a sequence of words from one language is transformed into a sequence in another language. The state-of-the-art seq2seq model is the <strong class="keyWord">Transformer</strong> model, and it was developed by Google Brain. We will discuss it in the next chapter.</p>
    <h1 class="heading-1" id="_idParaDest-306">Summary</h1>
    <p class="normal">In this chapter, we worked on three NLP projects: sentiment analysis, stock price prediction, and text generation using RNNs. We started with a detailed explanation of the recurrent mechanism and different RNN structures for different forms of input and output sequences. You also learned how LSTM improves vanilla RNNs.</p>
    <p class="normal">In the next chapter, we will focus on the Transformer, a recent state-of-the-art sequential learning model, and generative models.</p>
    <h1 class="heading-1" id="_idParaDest-307">Exercises</h1>
    <ol>
      <li class="numberedList" value="1">Use a bi-directional recurrent layer (it is easy enough to learn about it by yourself) and apply it to the sentiment analysis project. Can you beat what we achieved? Hint: set the <code class="inlineCode">bidirectional</code> argument to <code class="inlineCode">True</code> in the LSTM layer.</li>
      <li class="numberedList">Feel free to fine-tune the hyperparameters in the text generator, and see whether you can generate a more realistic and interesting version of <em class="italic">War and Peace</em>.</li>
      <li class="numberedList">Can you train an RNN model on any of your favorite books in order to write your own version?</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-308">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/yuxi"><span class="url">https://packt.link/yuxi</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code187846872178698968.png"/></p>
  </div>
</body></html>
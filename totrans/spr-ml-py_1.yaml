- en: First Step Towards Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, we will learn about the implementation of many of the common
    machine learning algorithms you interact with in your daily life. There will be
    plenty of math, theory, and tangible code examples to satisfy even the biggest
    machine learning junkie and, hopefully, you''ll pick up some useful Python tricks
    and practices along the way. We are going to start off with a very brief introduction
    to supervised learning, sharing a real-life machine learning demo; getting our
    Anaconda environment setup done; learning how to measure the slope of a curve,
    Nd-curve, and multiple functions; and finally, we''ll discuss how we know whether
    or not a model is good. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An example of supervised learning in action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hill climbing and loss functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and data splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need to install the following software, if you haven''t
    already done so:'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code files for this chapter can be found at [https:/​/​github.​com/​PacktPublishing/
  prefs: []
  type: TYPE_NORMAL
- en: Supervised-Machine-Learning-with-Python](https://github.com/PacktPublishing/Supervised-Machine-Learning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: An example of supervised learning in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will take a look at what we can do with supervised machine learning.
    With the following Terminal prompt, we will launch a new Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we are inside this top-level, `Hands-on-Supervised-Machine-Learning-with-Python-master`
    home directory, we will go directly inside the `examples` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b307221-119e-4772-a9d0-44d3186979f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that our only Notebook in here is `1.1 Supervised Learning Demo.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c853b1ee-ae6e-4447-b516-ffc646c710d0.png)'
  prefs: []
  type: TYPE_IMG
- en: We have the supervised learning demo Jupyter Notebook. We are going to be using
    a UCI dataset called the `Spam` dataset. This is a list of different emails that
    contain different features that correspond to spam or not spam. We want to build
    a machine learning algorithm that can predict whether or not we have an email
    coming in that is going to be spam. This could be extremely helpful for you if
    you're running your own email server.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the first function in the following code is simply a request''s get function.
    You should already have the dataset, which is already sitting inside the `examples`
    directory. But in case you don''t, you can go ahead and run the following code.
    You can see that we already have `spam.csv`, so we''re not going to download it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use the `pandas` library. This is a data analysis library from
    Python. You can install it when we go through the next stage, which is the environment
    setup. This library is a data frame data structure that is a kind of native Python,
    which we will use as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to lay out our data in the following format. We can use all
    sorts of different statistical functions that are nice to use when you''re doing
    machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9205d451-daa7-42ca-ae8f-075ae077fe9e.png)'
  prefs: []
  type: TYPE_IMG
- en: If some of this terminology is not familiar to you, don't panic yet—we will
    learn about these terminologies in detail over the course of the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'For `train_test_split`, we will take the `df` dataset and split it into two
    parts: train set and test set. In addition to that, we have the target, which
    is a `01` variable that indicates true or false for spam or not spam. We will
    split that as well, which includes the corresponding vector of true or false labels.
    By splitting the labels, we get `3680` training samples and `921` test samples,
    file as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we have a lot more training samples than test samples, which is
    important for fitting our models. We will learn about this later in the book.
    So, don't worry too much about what's going on here, as this is all just for demo
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we have the `packtml` library. This is the actual package
    that we are building, which is a classification and regression tree classifier.
    `CARTClassifier` is simply a generalization of a decision tree for both regression
    and classification purposes. Everything we fit here is going to be a supervised
    machine learning algorithm that we build from scratch. This is one of the classifiers
    that we are going to build in this book. We also have this utility function for
    plotting a learning curve. This is going to take our train set and break it into
    different folds for cross-validation. We will fit the training set in different
    stages of numbers of training samples, so we can see how the learning curve converges
    between the train and validation folds, which determines how our algorithm is
    learning, essentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We will go ahead and run the preceding code and plot how the algorithm has learned
    across the different sizes of our training set. You can see we're going to fit
    it for 4 different training set sizes at 3 folds of cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what we''re actually doing is fitting 12 separate models, which will take
    a few seconds to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1923903f-00ea-423c-985e-9142be846bd2.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding output, we can see our **Training score** and our **Validation
    score**. The **Training score** diminishes as it learns to generalize, and our
    **Validation score** increases as it learns to generalize from the training set
    to the validation set. So, our accuracy is hovering right around 92-93% on our
    validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the hyperparameters from the very best one here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn about logistic regression, which is another
    classification model that we''re going to build from scratch. We will go ahead
    and fit the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is much faster than the decision tree. In the following output, you can
    see that we converge a lot more around the 92.5% range. This looks a little more
    consistent than our decision tree, but it doesn''t perform quite well enough on
    the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa022926-38db-42f3-8dc2-f205f5120b4f.png)'
  prefs: []
  type: TYPE_IMG
- en: In the following screenshot, there are encoded records of spam emails. We will
    see how this encoding performs on an email that we can read and validate. So,
    if you have visited the UCI link that was included at the top of the Jupyter Notebook,
    it will provide a description of all the features inside the dataset. We have
    a lot of different features here that are counting the ratio of particular words
    to the number of words in the entire email. Some of those words might be free
    and some credited. We also have a couple of other features that are counting character
    frequencies, the number of exclamation points, and the number of concurrent capital
    runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if you have a really highly capitalized set of words, we have all these
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/278f0298-97cf-4a1e-abd2-79fc4470b12d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following screenshot, we will create two emails. The first email is
    very obviously spam. Even if anyone gets this email, no one will respond to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The second email looks less like spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/239df941-2895-4c5e-b9ab-d7e4d5304eca.png)'
  prefs: []
  type: TYPE_IMG
- en: The model that we have just fit is going to look at both of the emails and encode
    the features, and will classify which is, and which is not, spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function is going to encode those emails into the features we
    discussed. Initially, we''re going to use a `Counter` function as an object, and
    tokenize our emails. All we''re doing is splitting our email into a list of words,
    and then the words can be split into a list of characters. Later, we''ll count
    the characters and words so that we can generate our features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'So, all those features that we have up at the beginning tell us what words
    we''re interested in counting. We can see that the original dataset is interested
    in counting words such as address, email, business, and credit, and then, for
    our characters, we''re looking for opened and closed parentheses and dollar signs
    (which are quite relevant to our spam emails). So, we''re going to count all of
    those shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/defb48de-1917-450d-8b29-3bfd7edc6f46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Apply the ratio and keep track of the total number of `capital_runs`, computing
    the mean average, maximum, and minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the preceding code, we get the following output. This is going
    to encode our emails. This is just simply a vector of all the different features.
    It should be about 50 characters long:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49f17f3e-ff3a-4a64-9765-05e471651690.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we fit the preceding values into our models, we will see whether our model
    is any good. So, ideally, we will see that the actual fake email is predicted
    to be fake, and the actual real email is predicted to be real. So, if the emails
    are predicted as fake, our spam prediction is indeed spam for both the decision
    tree and the logistic regression. Our true email is not spam, which perhaps is
    even more important, because we don''t want to filter real email into the spam
    folder. So, you can see that we fitted some pretty good models here that apply
    to something that we would visually inspect as true spam or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8aa61090-9474-4640-bf82-5c88c68fb8c6.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a demo of the actual algorithms that we're going to build from scratch
    in this book, and can be applied to real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will go ahead and get our environment set up. Now that we have walked through
    the preceding example, let's go ahead and get our Anaconda environment set up.
    Among other things, Anaconda is a dependency management tool that will allow us
    to control specific versioning of each of the packages that we want to use. We
    will go to the Anaconda website through this link, [https://www.anaconda.com/download/](https://www.anaconda.com/download/),
    and click on the Download tab.
  prefs: []
  type: TYPE_NORMAL
- en: The package that we're building is not going to work with Python 2.7\. So, once
    you have Anaconda, we will perform a live coding example of an actual package
    setup, as well as the environment setup that's included in the `.yml` file that
    we built.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have Anaconda set up inside the home directory, we are going to use
    the `environment.yml` file. You can see that the name of the environment we''re
    going to create is `packt-sml` for supervised machine learning. We will need NumPy,
    SciPy, scikit-learn, and pandas. These are all scientific computing and data analysis
    libraries. Matplotlib is what we were using to plot those plots inside the Jupyter
    Notebook, so you''re going to need all those plots. The `conda` package makes
    it really easy to build this environment. All we have to do is type `conda env
    create` and then `-f` to point it to the file, go to `Hands-on-Supervised-Machine-Learning-with-Python-master`,
    and we''re going to use the `environment.yml` as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As this is the first time you're creating this, it will create a large script
    that will download everything you need. Once you have created your environment,
    you need to activate it. So, on a macOS or a Linux machine, we will type `source
    activate packt-sml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re on Windows, simply type `activate packt-sml`, which will activate
    that environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/205cbcc3-10ea-48a9-a83b-76fcf196cffc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to build the package, we will type the `cat setup.py` command. We
    can inspect this quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at this `setup.py`. Basically, this is just using setup tools to
    install the package. In the following screenshot, we see all the different sub
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c57903bf-a407-461b-93d6-f2798ddf73a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will build the package by typing the `python setup.py install` command.
    Now, when we go into Python and try to import `packtml`, we get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/273e6b37-dfe5-4dbd-ae08-a34b4635bd8b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we have installed the environment and built the package. In
    the next section, we will start covering some of the theory behind supervised
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will formally define what machine learning is and, specifically,
    what supervised machine learning is.
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of AI, everything was a rules engine. The programmer wrote
    the function and the rules, and the computer simply followed them. Modern-day
    AI is more in line with machine learning, which teaches a computer to write its
    own functions. Some may contest that oversimplification of the concept, but, at
    its core, this is largely what machine learning is all about.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to look at a quick example of what machine learning is and what
    it is not. Here, we''re using scikit-learn''s datasets, submodule to create two
    objects and variables, also known as covariance or features, which are along the
    column axis. `y` is a vector with the same number of values as there are rows
    in `X`. In this case, `y` is a class label. For the sake of an example, `y` here
    could be a binary label corresponding to a real-world occurrence, such as the
    malignancy of a tumor. `X` is then a matrix of attributes that describe `y`. One
    feature could be the diameter of the tumor, and another could indicate its density.
    The preceding explanation can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A rules engine, by our definition, is simply business logic. It can be as simple
    or as complex as you need it to be, but the programmer makes the rules. In this
    function, we''re going to evaluate our `X` matrix by returning `1`, or `true`,
    where the sums over the rows are greater than `0`. Even though there''s some math
    involved here, there is still a rules engine, because we, the programmers, defined
    a rule. So, we could theoretically get into a gray area, where the rule itself
    was discovered via machine learning. But, for the sake of argument, let''s take
    an example that the head surgeon arbitrarily picks `0` as our threshold, and anything
    above that is deemed as cancerous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as mentioned before, our rules engine can be as simple or as complex as
    we want it to be. Here, we''re not only interested in `row_sums`, but we have
    several criteria to meet in order to deem something cancerous. The minimum value
    in the row must be less than `-1.5`, in addition to one or more of the following
    three criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: The row sum exceeds `0`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the rows is evenly divisible by `0.5`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum value of the row is greater than `1.5`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, even though our math is a little more complex here, we''re still just building
    a rules engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s say that our surgeon understands and realizes they''re not the
    math or programming whiz that they thought they were. So, they hire programmers
    to build them a machine learning model. The model itself is a function that discovers
    parameters that complement a decision function, which is essentially the function
    the machine itself learned. So, parameters are things we''ll discuss in our next
    [Chapter 2](b51fa9b6-9158-4bcf-9998-18a8f91d3d06.xhtml), *Implementing Parametric
    Models,* which are parametric models. So, what''s happening behind the scenes
    when we invoke the `fit` method is that the model learns the characteristics and
    patterns of the data, and how the `X` matrix describes the `y` vector. Then, when
    we call the `predict` function, it applies its learned decision function to the
    input data to make an educated guess:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now we''re at a point where we need to define specifically what supervised
    learning is. Supervised learning is precisely the example we just described previously.
    Given our matrix of examples, *X*, in a vector of corresponding labels, *y*, that learns
    a function which approximates the value of *y* or ![](img/ec167647-fd63-4575-af06-3c72ac2af74b.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bae7a15-da5a-4a4b-ab78-6e5a9587ff51.png)'
  prefs: []
  type: TYPE_IMG
- en: There are other forms of machine learning that are not supervised, known as
    **unsupervised machine learning**. These do not have labels and are more geared
    toward pattern recognition tasks. So, what makes something supervised is the presence
    of labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our previous example, when we invoke the `fit` method, we learn
    our new decision function and then, when we call `predict`, we''re approximating
    the new `y` values. So, the output is this ![](img/68d94b46-bc5d-4c32-9c89-4805452ca273.png) we
    just looked at:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4430bb58-e5a2-4710-8fb3-a957fc2e1684.png)'
  prefs: []
  type: TYPE_IMG
- en: Supervised learning learns a function from labelled samples that approximates
    future `y` values. At this point, you should feel comfortable explaining the abstract
    concept—just the high-level idea of what supervised machine learning is.
  prefs: []
  type: TYPE_NORMAL
- en: Hill climbing and loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we got comfortable with the idea of supervised machine
    learning. Now, we will learn how exactly a machine learns underneath the hood.
    This section is going to examine a common optimization technique used by many
    machine learning algorithms, called **hill climbing**. It is predicated on the
    fact that each problem has an ideal state and a way to measure how close or how
    far we are from that. It is important to note that not all machine learning algorithms
    use this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we'll cover loss functions, and then, prior to diving into hill climbing
    and descent, we'll take a quick math refresher.
  prefs: []
  type: TYPE_NORMAL
- en: There's going to be some math in this lesson, and while we try to shy away from
    the purely theoretical concepts, this is something that we simply have to get
    through in order to understand the guts of most of these algorithms. There will
    be a brief applied section at the end. Don't panic if you can't remember some
    of the calculus; just simply try to grasp what is happening behind the black box.
  prefs: []
  type: TYPE_NORMAL
- en: So, as mentioned before, a machine learning algorithm has to measure how close
    it is to some objective. We define this as a cost function, or a loss function.
    Sometimes, we hear it referred to as an objective function. Although not all machine
    learning algorithms are designed to directly minimize a loss function, we're going
    to learn the rule here rather than the exception. The point of a loss function
    is to determine the goodness of a model fit. It is typically evaluated over the
    course of a model's learning procedure and converges when the model has maximized
    its learning capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical loss function computes a scalar value which is given by the true
    labels and the predicted labels. That is, given our actual *y* and our predicted
    *y*, which is ![](img/6adc46e5-4e64-4050-b55c-5c1d8e0e8e55.png). This notation
    might be cryptic, but all it means is that some function, *L*, which we''re going
    to call our loss function, is going to accept the ground truth, which is *y* and
    the predictions, ![](img/6fa4fd62-405e-4353-b7bf-35e9022c2f5a.png), and return
    some scalar value. The typical formula for the loss function is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18aa419c-7dd3-45df-8bcf-d7724f238d73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, I''ve listed several common loss functions here, which may or may not look
    familiar. **Sum of Squared Error** (**SSE**) is a metric that we''re going to
    be using for our regression models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc07628b-c403-4b88-8337-300ee6bcf651.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Cross entropy is a very commonly used classification metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29b4930c-d8a3-4fd2-b805-b7a4466b5e58.png)'
  prefs: []
  type: TYPE_IMG
- en: In the following diagram, the *L* function on the left is simply indicating
    that it is our loss function over *y* and ![](img/ba99764f-61f6-4266-806b-7acbb95ed809.png) given
    parameter theta. So, for any algorithm, we want to find the set of the theta parameters
    that minimize the loss. That is, if we're predicting the cost of a house, for
    example, we may want to estimate the cost per square foot as accurately as possible
    so as to minimize how wrong we are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters are often in a much higher dimensional space than can be represented
    visually. So, the big question we''re concerned with is the following: How can
    we minimize the cost? It is typically not feasible for us to attempt every possible
    value to determine the true minimum of a problem. So, we have to find a way to
    descend this nebulous hill of loss. The tough part is that, at any given point,
    we don''t know whether the curve goes up or down without some kind of evaluation.
    And that''s precisely what we want to avoid, because it''s very expensive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dfdcaa5-bf54-47ff-a8af-dbfc885a2b29.png)'
  prefs: []
  type: TYPE_IMG
- en: We can describe this problem as waking up in a pitch-black room with an uneven
    floor and trying to find the lowest point in the room. You don't know how big
    the room is. You don't know how deep or how high it gets. Where do you step first?
    One thing we can do is to examine exactly where we stand and determine which direction
    around us slopes downward. To do that, we have to measure the slope of the curve.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the slope of a curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a quick refresher on scalar derivatives. To compute the slope
    at any given point, the standard way is to typically measure the slope of the
    line between the point we''re interested in and some secant point, which we''ll
    call delta *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ec22951-f7ed-4b45-bc08-48fba2dd67ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the distance between *x* and its neighbor delta *x* approaches *0*, or as
    our limit approaches *0*, we arrive at the slope of the curve. This is given by
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30cef7ab-b444-4823-aacf-64bdda6d6016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are several different notations that you may be familiar with. One is
    *f* prime of *x*. The slope of a constant is *0*. So, if *f(x)* is *9*, in other
    words, if *y* is simply *9*, it never changes. There is no slope. So, the slope
    is *0*, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dcce59e-5617-411b-921e-e5497a64a9b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also see the power law in effect here in the second example. This will
    come in useful later on. If we multiply the variable by the power, and decrement
    the power by one, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22f03da9-d95c-4c95-9857-020d6fe1b9f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Measuring the slope of an Nd-curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to measure the slope of a vector or a multi-dimensional surface, we
    will introduce the idea of partial derivatives, which are simply derivatives with
    respect to a variable, with all the other variables held as constants. So, our
    solution is a vector of dimension *k*, where *k* is the number of variables that
    our function takes. In this case, we have *x* and *y*. Each respective position
    in the vector that we solve is a derivative with respect to the corresponding
    function's positional variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a conceptual level, what we''re doing is we''re holding one of the variables
    still and changing the other variables around it to see how the slope changes.
    Our denominator''s notation indicates which variable we''re measuring the slope
    with, with respect to that point. So, in this case, the first position, *d(x)*,
    is showing that we''re taking the partial derivative of function *f* with respect
    to *x*, where we hold *y* constant. And then, likewise, in the second one, we''re
    taking the derivative of function f with respect to *y*, holding *x* constant.
    So, what we get in the end is called a gradient, which is a super keyword. It
    is simply just a vector of partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62ac4d23-5f7d-460a-858a-bdd93f6f3646.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/e86981ff-c747-49ba-a53d-8ee22588f525.png)'
  prefs: []
  type: TYPE_IMG
- en: Measuring the slope of multiple functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We want to get really complicated, though, and measure the slopes of multiple
    functions at the same time. All we''ll end up with is a matrix of gradients along
    the rows. In the following formula, we can see the solution that we just solved
    from the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38704fda-a85a-4fec-9e1c-4fbbcc29ff32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next formula, we have introduced this new function, called *g*. We see
    the gradient for function *g*, with each position corresponding to the partial
    derivative with respect to the variables *x* and *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a984ad0-25f6-4b2f-8613-03876048985a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we stack these together into a matrix, what we get is a Jacobian. You
    don''t need to solve this, but you should understand that what we''re doing is
    taking the slope of a multi-dimensional surface. You can treat it as a bit of
    a black box as long as you understand that. This is exactly how we''re computing
    the gradient and the Jacobian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5f178d0-8f97-4450-8043-e9fb391a1bfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Hill climbing and descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will go back to our example—the lost hill that we looked at. We want to find
    a way to select a set of theta parameters that is going to minimize our loss function, *L*.
    As we've already established, we need to climb or descend the hill, and understand
    where we are with respect to our neighboring points without having to compute
    everything. To do that, we need to be able to measure the slope of the curve with
    respect to the theta parameters. So, going back to our house example, as mentioned
    before, we want to know how much correct the incremental value of cost per square
    foot makes. Once we know that, we can start taking directional steps toward finding
    the best estimate. So, if you make a bad guess, you can turn around and go in
    exactly the other direction. So, we can either climb or descend the hill depending
    on our metric, which allows us to optimize the parameters of a function that we
    want to learn irrespective of how the function itself performs. This is a layer
    of abstraction. This optimization process is called gradient descent, and it supports many
    of the machine learning algorithms that we will discuss in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows a simple example of how we can measure the gradient
    of a matrix with respect to theta. This example is actually a simplified snippet
    of the learning component of logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: At the very top, we randomly initialize `X` and `y`, which is not part of the
    algorithm. So, `x` here is the sigmoid function, also called the **logistic function**.
    The word logistic comes from logistic progression. This is a necessary transformation
    that is applied in logistic regression. Just understand that we have to apply
    that; it's part of the function. So, we initialize our `theta` vector, with respect
    to which we're going to compute our gradient as zeros. Again, all of them are
    zeros. Those are our parameters. Now, for each iteration, we're going to get our
    ![](img/c700ee8d-f92f-44b3-9ac0-54c1cb781e40.png), which is our estimated `y`,
    if you recall. We get that by taking the dot product of our `X` matrix against
    our theta parameters, pushed through that logistic function, `h`, which is our
    ![](img/9fa6e3b4-1d08-487f-b97c-17fbef8dfa08.png).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we want to compute the gradient of that dot product between the residuals
    and the input matrix, `X`, of our predictors. The way we compute our residuals
    is simply `y` minus ![](img/038cd708-4e73-4af7-a532-e5f7a70612f5.png), which gives
    the residuals. Now, we have our ![](img/e627b377-2a40-4d23-9e9d-409a0db456e6.png).
    How do we get the gradient? The gradient is just the dot product between the input
    matrix, `X`, and those residuals. We will use that gradient to determine which
    direction we need to step in. The way we do that is we add the gradient to our
    theta vector. Lambda regulates how quickly we step up or down that gradient. So,
    it's our learning rate. If you think of it as a step size—going back to that dark
    room example—if it's too large, it's easy to overstep the lowest point. But if
    it's too small, you're going to spend forever inching around the room. So, it's
    a bit of a balancing act, but it allows us to regulate the pace at which we update
    our theta values and descend our gradient. Again, this algorithm is something
    we will cover in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get the output of the preceding code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This example demonstrates how our gradient or slope actually changes as we adjust
    our coefficients and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to evaluate our models and learn the cryptic
    `train_test_split`.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and data splitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will define what it means to evaluate a model, best practices
    for gauging the advocacy of a model, how to split your data, and several considerations
    that you'll have to make when preparing your split.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand some core best practices of machine learning.
    One of our primary tasks as ML practitioners is to create a model that is effective
    for making predictions on new data. But how do we know that a model is good? If
    you recall from the previous section, we defined supervised learning as simply
    a task that learns a function from labelled data such that we can approximate
    the target of the new data. Therefore, we can test our model's effectiveness.
    We can determine how it performs on data that is never seen—just like it's taking
    a test.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-sample versus in-sample evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say we are training a small machine which is a simple classification
    task. Here''s some nomenclature you''ll need: the in-sample data is the data the
    model learns from and the out-of-sample data is the data the model has never seen
    before. One of the pitfalls many new data scientists make is that they measure
    their model''s effectiveness on the same data that the model learned from. What
    this ends up doing is rewarding the model''s ability to memorize, rather than
    its ability to generalize, which is a huge difference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take a look at the two examples here, the first presents a sample that
    the model learned from, and we can be reasonably confident that it''s going to
    predict one, which would be correct. The second example presents a new sample,
    which appears to resemble more of the zero class. Of course, the model doesn''t
    know that. But a good model should be able to recognize and generalize this pattern,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fbc7355-5703-4c20-86a5-84d65ef157cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, now the question is how we can ensure both in-sample and out-of-sample
    data for the model to prove its worth. Even more precisely, our out-of-sample
    data needs to be labeled. New or unlabeled data won''t suffice because we have
    to know the actual answer in order to determine how correct the model is. So,
    one of the ways we can handle this in machine learning is to split our data into
    two parts: a training set and a testing set. The training set is what our model
    will learn on; the testing set is what our model will be evaluated on. How much
    data you have matters a lot. In fact, in the next sections, when we discuss the
    bias-variance trade-off, you''ll see how some models require much more data to
    learn than others do.'
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to keep in mind is that if some of the distributions of your variables
    are highly skewed, or you have rare categorical levels embedded throughout, or
    even class imbalance in your `y` vector, you may end up getting a bad split. As
    an example, let's say you have a binary feature in your `X` matrix that indicates
    the presence of a very rare sensor for some event that occurs every 10,000 occurrences.
    If you randomly split your data and all of the positive sensor events are in your
    test set, then your model will learn from the training data that the sensor is
    never tripped and may deem that as an unimportant variable when, in reality, it
    could be hugely important, and hugely predictive. So, you can control these types
    of issues with stratification.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting made easy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we have a simple snippet that demonstrates how we can use the scikit-learn
    library to split our data into training and test sets. We''re loading the data
    in from the datasets module and passing both `X` and `y` into the split function.
    We should be familiar with loading the data up. We have the `train_test_split`
    function from the `model_selection` submodule in `sklearn`. This is going to take
    any number of arrays. So, 20% is going to be `test_size`, and the remaining 80%
    of that data will be training. We define `random_state`, so that our split can
    be reproducible if we ever have to prove exactly how we got this split. There''s
    also the `stratify` keyword, which we''re not using here, which can be used to
    `stratify` a split for rare features or an imbalanced `y` vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced supervised learning, got our environment put
    together, and learned about hill climbing and model evaluation. At this point,
    you should understand the abstract conceptual underpinnings of what makes a machine
    learn. It's all about optimizing a number of loss functions. In the next chapter,
    we'll jump into parametric models and even code some popular algorithms from scratch.
  prefs: []
  type: TYPE_NORMAL

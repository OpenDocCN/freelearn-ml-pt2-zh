["```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import silhouette_score\n    from scipy.spatial.distance import cdist\n    iris = pd.read_csv('iris_data.csv', header=None)\n    iris.columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'species']\n    ```", "```py\n    X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n    y = iris['species']\n    ```", "```py\n    X.head()\n    ```", "```py\n    def k_means(X, K):\n    #Keep track of history so you can see k-means in action\n        centroids_history = []\n        labels_history = []\n        rand_index = np.random.choice(X.shape[0], K)  \n        centroids = X[rand_index]\n        centroids_history.append(centroids)\n        while True:\n    # Euclidean distances are calculated for each point relative to centroids, #and then np.argmin returns\n    # the index location of the minimal distance - which cluster a point    is #assigned to\n            labels = np.argmin(cdist(X, centroids), axis=1)\n            labels_history.append(labels)\n    #Take mean of points within clusters to find new centroids:\n            new_centroids = np.array([X[labels == i].mean(axis=0)\n                                    for i in range(K)])\n            centroids_history.append(new_centroids)\n\n            # If old centroids and new centroids no longer change, k-means is complete and end. Otherwise continue\n            if np.all(centroids == new_centroids):\n                break\n            centroids = new_centroids\n\n        return centroids, labels, centroids_history, labels_history\n    ```", "```py\n    X_mat = X.values\n    ```", "```py\n    centroids, labels, centroids_history, labels_history = k_means(X_mat, 3)\n    ```", "```py\n    print(labels)\n    ```", "```py\n    plt.scatter(X['SepalLengthCm'], X['SepalWidthCm'])\n    plt.title('Iris - Sepal Length vs Width')\n    plt.show()\n    ```", "```py\n    plt.scatter(X['SepalLengthCm'], X['SepalWidthCm'], c=labels, cmap='tab20b')\n    plt.title('Iris - Sepal Length vs Width - Clustered')\n    plt.show()\n    ```", "```py\n    # Calculate Silhouette Score\n    silhouette_score(X[['SepalLengthCm','SepalWidthCm']], labels)\n    ```", "```py\n    from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n    from sklearn.datasets import make_blobs\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    # Generate a random cluster dataset to experiment on. X = coordinate points, y = cluster labels (not needed)\n    X, y = make_blobs(n_samples=1000, centers=8, n_features=2, random_state=800)\n    # Visualize the data\n    plt.scatter(X[:,0], X[:,1])\n    plt.show()\n    ```", "```py\n    methods = ['centroid', 'single', 'complete', 'average', 'weighted']\n    ```", "```py\n    for method in methods:\n        distances = linkage(X, method=method, metric=\"euclidean\")\n        clusters = fcluster(distances, 3, criterion=\"distance\") \n        plt.title('linkage: ' + method)\n        plt.scatter(X[:,0], X[:,1], c=clusters, cmap='tab20b')\n        plt.show()\n    ```", "```py\n    from sklearn.cluster import KMeans\n    from sklearn.cluster import AgglomerativeClustering\n    from sklearn.metrics import silhouette_score\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    ```", "```py\n    wine_df = pd.read_csv(\"wine_data.csv\")\n    print(wine_df.head)\n    ```", "```py\n    plt.scatter(wine_df.values[:,0], wine_df.values[:,1])\n    plt.title(\"Wine Dataset\")\n    plt.xlabel(\"OD Reading\")\n    plt.ylabel(\"Proline\")\n    plt.show()\n    ```", "```py\n    km = KMeans(3)\n    km_clusters = km.fit_predict(wine_df)\n    ```", "```py\n    ac = AgglomerativeClustering(3, linkage='average')\n    ac_clusters = ac.fit_predict(wine_df)\n    ```", "```py\n    plt.scatter(wine_df.values[:,0], wine_df.values[:,1], c=km_clusters)\n    plt.title(\"Wine Clusters from Agglomerative Clustering\")\n    plt.xlabel(\"OD Reading\")\n    plt.ylabel(\"Proline\")\n    plt.show()\n    ```", "```py\n    plt.scatter(wine_df.values[:,0], wine_df.values[:,1], c=ac_clusters)\n    plt.title(\"Wine Clusters from Agglomerative Clustering\")\n    plt.xlabel(\"OD Reading\")\n    plt.ylabel(\"Proline\")\n    plt.show()\n    ```", "```py\n    print(\"Silhouette Scores for Wine Dataset:\\n\")\n    print(\"k-means Clustering: \", silhouette_score(X[:,11:13], km_clusters))\n    print(\"Agg Clustering: \", silhouette_score(X[:,11:13], ac_clusters))\n    ```", "```py\n    from sklearn.cluster import DBSCAN\n    from sklearn.datasets import make_blobs\n    import matplotlib.pyplot as plt\n    import numpy as np\n    %matplotlib inline\n    X_blob, y_blob = make_blobs(n_samples=500, centers=4, n_features=2, random_state=800)\n    ```", "```py\n    plt.scatter(X_blob[:,0], X_blob[:,1])\n    plt.show()\n    ```", "```py\n    def scratch_DBSCAN(x, eps, min_pts):\n        \"\"\"\n        param x (list of vectors): your dataset to be clustered\n        param eps (float): neigborhood radius threshold\n        param min_pts (int): minimum number of points threshold for a nieghborhood to be a cluster\n        \"\"\"\n         # Build a label holder that is comprised of all 0s\n        labels = [0]* x.shape[0]\n        # Arbitrary starting \"current cluster\" ID    \n        C = 0\n\n        # For each point p in x...\n        # ('p' is the index of the datapoint, rather than the datapoint itself.)\n        for p in range(0, x.shape[0]):\n\n            # Only unvisited points can be evaluated as neighborhood centers\n            if not (labels[p] == 0):\n                continue\n\n            # Find all of p's neighbors.\n            neighbors = neighborhood_search(x, p, eps)\n\n            # If there are not enough neighbor points, then it is classified as noise (-1).\n            # Otherwise we can use this point as a neighborhood cluster\n            if len(neighbors) < min_pts:\n                labels[p] = -1    \n            else: \n                C += 1\n                neighbor_cluster(x, labels, p, neighbors, C, eps, min_pts)\n\n        return labels\n    def neighbor_cluster(x, labels, p, neighbors, C, eps, min_pts):\n        # Assign the cluster label to original point\n        labels[p] = C\n\n        # Look at each neighbor of p (by index, not the points themselves) and evaluate\n        i = 0\n        while i < len(neighbors):    \n\n            # Get the next point from the queue.        \n            potential_neighbor_ix = neighbors[i]\n\n            # If potential_neighbor_ix is noise from previous runs, we can assign it to current cluster\n            if labels[potential_neighbor_ix] == -1:\n                labels[potential_neighbor_ix] = C\n\n            # Otherwise, if potential_neighbor_ix is unvisited, we can add it to current cluster\n            elif labels[potential_neighbor_ix] == 0:\n                labels[potential_neighbor_ix] = C\n\n                # Further find neighbors of potential neighbor\n                potential_neighbors_cluster = neighborhood_search(x, potential_neighbor_ix, eps)\n\n                if len(potential_neighbors_cluster) >= min_pts:\n                    neighbors = neighbors + potential_neighbors_cluster      \n\n            # Evaluate next neighbor\n            i += 1        \n    def neighborhood_search(x, p, eps):\n        neighbors = []\n\n        # For each point in the dataset...\n        for potential_neighbor in range(0, x.shape[0]):\n\n            # If a nearby point falls below the neighborhood radius threshold, add to neighbors list\n            if np.linalg.norm(x[p] - x[potential_neighbor]) < eps:\n                neighbors.append(potential_neighbor)\n\n        return neighbors\n    ```", "```py\n    labels = scratch_DBSCAN(X_blob, 0.6, 5)\n    ```", "```py\n    plt.scatter(X_blob[:,0], X_blob[:,1], c=labels)\n    plt.title(\"DBSCAN from Scratch Performance\")\n    plt.show()\n    ```", "```py\n    from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n    from sklearn.metrics import silhouette_score\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    ```", "```py\n    # Load Wine data set\n    wine_df = pd.read_csv(\"../CH2/wine_data.csv\")\n    # Show sample of data set\n    print(wine_df.head())\n    ```", "```py\n    plt.scatter(wine_df.values[:,0], wine_df.values[:,1])\n    plt.title(\"Wine Dataset\")\n    plt.xlabel(\"OD Reading\")\n    plt.ylabel(\"Proline\")\n    plt.show()\n    ```", "```py\n    # Generate clusters from K-Means\n    km = KMeans(3)\n    km_clusters = km.fit_predict(wine_df)\n    # Generate clusters using Agglomerative Hierarchical Clustering\n    ac = AgglomerativeClustering(3, linkage='average')\n    ac_clusters = ac.fit_predict(wine_df)\n    ```", "```py\n    db_param_options = [[20,5],[25,5],[30,5],[25,7],[35,7],[35,3]]\n    for ep,min_sample in db_param_options:\n        # Generate clusters using DBSCAN\n        db = DBSCAN(eps=ep, min_samples = min_sample)\n        db_clusters = db.fit_predict(wine_df)\n        print(\"Eps: \", ep, \"Min Samples: \", min_sample)\n        print(\"DBSCAN Clustering: \", silhouette_score(wine_df, db_clusters))\n    ```", "```py\n    # Generate clusters using DBSCAN\n    db = DBSCAN(eps=35, min_samples = 3)\n    db_clusters = db.fit_predict(wine_df)\n    ```", "```py\n    plt.title(\"Wine Clusters from K-Means\")\n    plt.scatter(wine_df['OD_read'], wine_df['Proline'], c=km_clusters,s=50, cmap='tab20b')\n    plt.show()\n    plt.title(\"Wine Clusters from Agglomerative Clustering\")\n    plt.scatter(wine_df['OD_read'], wine_df['Proline'], c=ac_clusters,s=50, cmap='tab20b')\n    plt.show()\n    plt.title(\"Wine Clusters from DBSCAN\")\n    plt.scatter(wine_df['OD_read'], wine_df['Proline'], c=db_clusters,s=50, cmap='tab20b')\n    plt.show()\n    ```", "```py\n    # Calculate Silhouette Scores\n    print(\"Silhouette Scores for Wine Dataset:\\n\")\n    print(\"K-Means Clustering: \", silhouette_score(wine_df, km_clusters))\n    print(\"Agg Clustering: \", silhouette_score(wine_df, ac_clusters))\n    print(\"DBSCAN Clustering: \", silhouette_score(wine_df, db_clusters))\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    ```", "```py\n    df = pd.read_csv('iris-data.csv')\n    df = df[['Sepal Length', 'Sepal Width']]\n    df.head()\n    ```", "```py\n    cov = np.cov(df.values.T)\n    cov\n    ```", "```py\n    model = PCA(n_components=1)\n    sklearn_pca = model.fit_transform(df.values)\n    ```", "```py\n    eigenvectors, eigenvalues, _ = np.linalg.svd(cov, full_matrices=False)\n    P = eigenvectors[0]\n    manual_pca = P.dot(df.values.T)\n    ```", "```py\n    plt.figure(figsize=(10, 7));\n    plt.plot(sklearn_pca, label='Scikit-learn PCA');\n    plt.plot(manual_pca, label='Manual PCA', linestyle='--');\n    plt.xlabel('Sample');\n    plt.ylabel('Transformed Value');\n    plt.legend();\n    ```", "```py\n    model.components_\n    ```", "```py\n    array([[ 0.99693955, -0.07817635]])\n    ```", "```py\n    P\n    ```", "```py\n    array([-0.99693955,  0.07817635])\n    ```", "```py\n    manual_pca *= -1\n    plt.figure(figsize=(10, 7));\n    plt.plot(sklearn_pca, label='Scikit-learn PCA');\n    plt.plot(manual_pca, label='Manual PCA', linestyle='--');\n    plt.xlabel('Sample');\n    plt.ylabel('Transformed Value');\n    plt.legend();\n    ```", "```py\n    mean_vals = np.mean(df.values, axis=0)\n    offset_vals = df.values - mean_vals\n    manual_pca = P.dot(offset_vals.T)\n    ```", "```py\n    manual_pca *= -1\n    ```", "```py\n    plt.figure(figsize=(10, 7));\n    plt.plot(sklearn_pca, label='Scikit-learn PCA');\n    plt.plot(manual_pca, label='Manual PCA', linestyle='--');\n    plt.xlabel('Sample');\n    plt.ylabel('Transformed Value');\n    plt.legend();\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from mpl_toolkits.mplot3d import Axes3D # Required for 3D plotting\n    ```", "```py\n    df = pd.read_csv('iris-data.csv')[['Sepal Length', 'Sepal Width', 'Petal Width']]\n    df.head()\n    ```", "```py\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(df['Sepal Length'], df['Sepal Width'], df['Petal Width']);\n    ax.set_xlabel('Sepal Length (mm)');\n    ax.set_ylabel('Sepal Width (mm)');\n    ax.set_zlabel('Petal Width (mm)');\n    ax.set_title('Expanded Iris Dataset');\n    ```", "```py\n    model = PCA()\n    ```", "```py\n    model.fit(df.values)\n    ```", "```py\n    model.explained_variance_ratio_\n    ```", "```py\n    array([0.8004668 , 0.14652357, 0.05300962])\n    ```", "```py\n    model = PCA(n_components=2)\n    ```", "```py\n    data_transformed = model.fit_transform(df.values)\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.scatter(data_transformed[:,0], data_transformed[:,1]);\n    ```", "```py\n    data_restored = model.inverse_transform(data_transformed)\n    ```", "```py\n    fig = plt.figure(figsize=(10, 14))\n    # Original Data\n    ax = fig.add_subplot(211, projection='3d')\n    ax.scatter(df['Sepal Length'], df['Sepal Width'], df['Petal Width'], label='Original Data');\n    ax.set_xlabel('Sepal Length (mm)');\n    ax.set_ylabel('Sepal Width (mm)');\n    ax.set_zlabel('Petal Width (mm)');\n    ax.set_title('Expanded Iris Dataset');\n    # Transformed Data\n    ax = fig.add_subplot(212, projection='3d')\n    ax.scatter(data_restored[:,0], data_restored[:,1], data_restored[:,2], label='Restored Data');\n    ax.set_xlabel('Sepal Length (mm)');\n    ax.set_ylabel('Sepal Width (mm)');\n    ax.set_zlabel('Petal Width (mm)');\n    ax.set_title('Restored Iris Dataset');\n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    ```", "```py\n    plt.rc('text', usetex=True)\n    ```", "```py\n    def relu(x):\n        return np.max((0, x))\n    ```", "```py\n    theta = 1\n    x = np.linspace(-5, 5, 100)\n    x\n    ```", "```py\n    y = [relu(_x * theta) for _x in x]\n    ```", "```py\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111)\n    ax.plot(x, y)\n    ax.set_xlabel('$x$', fontsize=22);\n    ax.set_ylabel('$h(x\\Theta)$', fontsize=22);\n    ax.spines['left'].set_position(('data', 0));\n    ax.spines['top'].set_visible(False);\n    ax.spines['right'].set_visible(False);\n    ax.tick_params(axis='both', which='major', labelsize=22)\n    ```", "```py\n    theta = 5\n    y_2 = [relu(_x * theta) for _x in x]\n    ```", "```py\n    theta = 0.2\n    y_3 = [relu(_x * theta) for _x in x]\n    ```", "```py\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111)\n    ax.plot(x, y, label='$\\Theta=1$');\n    ax.plot(x, y_2, label='$\\Theta=5$', linestyle=':');\n    ax.plot(x, y_3, label='$\\Theta=0.2$', linestyle='--');\n    ax.set_xlabel('$x\\Theta$', fontsize=22);\n    ax.set_ylabel('$h(x\\Theta)$', fontsize=22);\n    ax.spines['left'].set_position(('data', 0));\n    ax.spines['top'].set_visible(False);\n    ax.spines['right'].set_visible(False);\n    ax.tick_params(axis='both', which='major', labelsize=22);\n    ax.legend(fontsize=22);\n    ```", "```py\n    import pickle\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from keras.models import Sequential\n    from keras.layers import Dense\n    ```", "```py\n    with open('mnist.pkl', 'rb') as f:\n        data = pickle.load(f)\n\n    images = data['images']\n    labels = data['labels']\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    for i in range(10):\n        plt.subplot(2, 5, i + 1)\n        plt.imshow(images[i], cmap='gray')\n        plt.title(labels[i])\n        plt.axis('off')\n    ```", "```py\n    one_hot_labels = np.zeros((images.shape[0], 10))\n    for idx, label in enumerate(labels):\n        one_hot_labels[idx, label] = 1\n\n    one_hot_labels\n    ```", "```py\n    images = images.reshape((-1, 28 ** 2))\n    images = images / 255.\n    ```", "```py\n    model = Sequential([\n        Dense(600, input_shape=(784,), activation='relu'),\n        Dense(10, activation='softmax'),\n    ])\n    ```", "```py\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n    ```", "```py\n    model.fit(images, one_hot_labels, epochs=20)\n    ```", "```py\n    import pickle\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from keras.models import Model\n    from keras.layers import Input, Dense\n    ```", "```py\n    with open('mnist.pkl', 'rb') as f:\n        images = pickle.load(f)['images']\n    ```", "```py\n    images = images.reshape((-1, 28 ** 2))\n    images = images / 255.\n    ```", "```py\n    input_stage = Input(shape=(784,))\n    encoding_stage = Dense(100, activation='relu')(input_stage)\n    decoding_stage = Dense(784, activation='sigmoid')(encoding_stage)\n    autoencoder = Model(input_stage, decoding_stage)\n    ```", "```py\n    autoencoder.compile(loss='binary_crossentropy',\n                  optimizer='adadelta')\n    ```", "```py\n    autoencoder.fit(images, images, epochs=100)\n    ```", "```py\n    encoder_output = Model(input_stage, encoding_stage).predict(images[:5])\n    ```", "```py\n    encoder_output = encoder_output.reshape((-1, 10, 10)) * 255\n    ```", "```py\n    decoder_output = autoencoder.predict(images[:5])\n    ```", "```py\n    decoder_output = decoder_output.reshape((-1, 28, 28)) * 255\n    ```", "```py\n    images = images.reshape((-1, 28, 28))\n    plt.figure(figsize=(10, 7))\n    for i in range(5):\n        plt.subplot(3, 5, i + 1)\n        plt.imshow(images[i], cmap='gray')\n        plt.axis('off')\n        plt.subplot(3, 5, i + 6)\n        plt.imshow(encoder_output[i], cmap='gray')\n        plt.axis('off')   \n\n        plt.subplot(3, 5, i + 11)\n        plt.imshow(decoder_output[i], cmap='gray')\n        plt.axis('off')    \n    ```", "```py\n    import pickle\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from keras.models import Model\n    from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n    ```", "```py\n    with open('mnist.pkl', 'rb') as f:\n        images = pickle.load(f)['images']\n    ```", "```py\n    images = images / 255.\n    ```", "```py\n    images = images.reshape((-1, 28, 28, 1))\n    ```", "```py\n    input_layer = Input(shape=(28, 28, 1,))\n    ```", "```py\n    hidden_encoding = Conv2D(\n        16, # Number of layers or filters in the weight matrix\n        (3, 3), # Shape of the weight matrix\n        activation='relu',\n        padding='same', # How to apply the weights to the images\n    )(input_layer)\n    ```", "```py\n    encoded = MaxPooling2D((2, 2))(hidden_encoding)\n    ```", "```py\n    hidden_decoding = Conv2D(\n        16, # Number of layers or filters in the weight matrix\n        (3, 3), # Shape of the weight matrix\n        activation='relu',\n        padding='same', # How to apply the weights to the images\n    )(encoded)\n    ```", "```py\n    upsample_decoding = UpSampling2D((2, 2))(hidden_decoding)\n    ```", "```py\n    decoded = Conv2D(\n        1, # Number of layers or filters in the weight matrix\n        (3, 3), # Shape of the weight matrix\n        activation='sigmoid',\n        padding='same', # How to apply the weights to the images\n    )(upsample_decoding)\n    ```", "```py\n    autoencoder = Model(input_layer, decoded)\n    ```", "```py\n    autoencoder.summary()\n    ```", "```py\n    autoencoder.compile(loss='binary_crossentropy',\n                  optimizer='adadelta')\n    ```", "```py\n    autoencoder.fit(images, images, epochs=20)\n    ```", "```py\n    encoder_output = Model(input_layer, encoded).predict(images[:5])\n    ```", "```py\n    encoder_output = encoder_output.reshape((-1, 14 * 14, 16))\n    ```", "```py\n    decoder_output = autoencoder.predict(images[:5])\n    ```", "```py\n    decoder_output = decoder_output.reshape((-1, 28, 28))\n    ```", "```py\n    images = images.reshape((-1, 28, 28))\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    for i in range(5):\n        plt.subplot(3, 5, i + 1)\n        plt.imshow(images[i], cmap='gray')\n        plt.axis('off')\n\n        plt.subplot(3, 5, i + 6)\n        plt.imshow(encoder_output[i], cmap='gray')\n        plt.axis('off')   \n\n        plt.subplot(3, 5, i + 11)\n        plt.imshow(decoder_output[i], cmap='gray')\n        plt.axis('off')        \n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    ```", "```py\n    df = pd.read_csv('wine.data', header=None)\n    df.head()\n    ```", "```py\n    labels = df[0]\n    del df[0]\n    ```", "```py\n    model_pca = PCA(n_components=6)\n    wine_pca = model_pca.fit_transform(df)\n    ```", "```py\n    np.sum(model_pca.explained_variance_ratio_)\n    ```", "```py\n    0.99999314824536\n    ```", "```py\n    tsne_model = TSNE(random_state=0, verbose=1)\n    tsne_model\n    ```", "```py\n    wine_tsne = tsne_model.fit_transform(wine_pca.reshape((len(wine_pca), -1)))\n    ```", "```py\n    wine_tsne.shape\n    ```", "```py\n    (172, 8)\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.scatter(wine_tsne[:,0], wine_tsne[:,1]);\n    plt.title('Low Dimensional Representation of Wine');\n    plt.show()\n    ```", "```py\n    MARKER = ['o', 'v', '^',]\n    plt.figure(figsize=(10, 7))\n    plt.title('Low Dimensional Representation of Wine');\n    for i in range(1, 4):\n        selections = wine_tsne[labels == i]\n        plt.scatter(selections[:,0], selections[:,1], marker=MARKER[i-1], label=f'Wine {i}', s=30);\n        plt.legend();\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    ```", "```py\n    df = pd.read_csv('wine.data', header=None)\n    df.head()\n    ```", "```py\n    labels = df[0]\n    del df[0]\n    ```", "```py\n    model_pca = PCA(n_components=6)\n    wine_pca = model_pca.fit_transform(df)\n    wine_pca = wine_pca.reshape((len(wine_pca), -1))\n    ```", "```py\n    MARKER = ['o', 'v', '^',]\n    for perp in [1, 5, 20, 30, 80, 160, 320]:\n        tsne_model = TSNE(random_state=0, verbose=1, perplexity=perp)\n        wine_tsne = tsne_model.fit_transform(wine_pca)\n        plt.figure(figsize=(10, 7))\n        plt.title(f'Low Dimensional Representation of Wine. Perplexity {perp}');\n        for i in range(1, 4):\n            selections = wine_tsne[labels == i]\n            plt.scatter(selections[:,0], selections[:,1], marker=MARKER[i-1], label=f'Wine {i}', s=30);\n            plt.legend();\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    ```", "```py\n    df = pd.read_csv('wine.data', header=None)\n    df.head()\n    ```", "```py\n    labels = df[0]\n    del df[0]\n    ```", "```py\n    model_pca = PCA(n_components=6)\n    wine_pca = model_pca.fit_transform(df)\n    wine_pca = wine_pca.reshape((len(wine_pca), -1))\n    ```", "```py\n    MARKER = ['o', 'v', '1', 'p' ,'*', '+', 'x', 'd', '4', '.']\n    for iterations in [250, 500, 1000]:\n        model_tsne = TSNE(random_state=0, verbose=1, n_iter=iterations, n_iter_without_progress=iterations)\n        mnist_tsne = model_tsne.fit_transform(mnist_pca)\n    ```", "```py\n        plt.figure(figsize=(10, 7))\n        plt.title(f'Low Dimensional Representation of MNIST (iterations = {iterations})');\n        for i in range(10):\n            selections = mnist_tsne[mnist['labels'] == i]\n            plt.scatter(selections[:,0], selections[:,1], alpha=0.2, marker=MARKER[i], s=5);\n            x, y = selections.mean(axis=0)\n            plt.text(x, y, str(i), fontdict={'weight': 'bold', 'size': 30}) \n    ```", "```py\n    import langdetect\n    import matplotlib.pyplot\n    import nltk\n    import numpy\n    import pandas\n    import pyLDAvis\n    import pyLDAvis.sklearn\n    import regex\n    import sklearn\n    ```", "```py\n    path = '<Path>/latimeshealth.txt'\n    df = pandas.read_csv(path, sep=\"|\", header=None)\n    df.columns = [\"id\", \"datetime\", \"tweettext\"]\n    ```", "```py\n    def dataframe_quick_look(df, nrows):\n    print(\"SHAPE:\\n{shape}\\n\".format(shape=df.shape))\n    print(\"COLUMN NAMES:\\n{names}\\n\".format(names=df.columns))\n    print(\"HEAD:\\n{head}\\n\".format(head=df.head(nrows)))\n    dataframe_quick_look(df, nrows=2)\n    ```", "```py\n    raw = df['tweettext'].tolist()\n    print(\"HEADLINES:\\n{lines}\\n\".format(lines=raw[:5]))\n    print(\"LENGTH:\\n{length}\\n\".format(length=len(raw)))\n    ```", "```py\n    def do_language_identifying(txt):\n        \ttry:\n               the_language = langdetect.detect(txt)\n        \texcept:\n            \tthe_language = 'none'\n        \treturn the_language\n    def do_lemmatizing(wrd):\n        \tout = nltk.corpus.wordnet.morphy(wrd)\n        \treturn (wrd if out is None else out)\n    def do_tweet_cleaning(txt):\n    # identify language of tweet\n    # return null if language not english\n        \tlg = do_language_identifying(txt)\n        \tif lg != 'en':\n            \treturn None\n    # split the string on whitespace\n        \tout = txt.split(' ')\n    # identify screen names\n    # replace with SCREENNAME\n        \tout = ['SCREENNAME' if i.startswith('@') else i for i in out]\n    # identify urls\n    # replace with URL\n        \tout = ['URL' if bool(regex.search('http[s]?://', i)) else i for i in out]\n          # remove all punctuation\n        \tout = [regex.sub('[^\\\\w\\\\s]|\\n', '', i) for i in out]\n          # make all non-keywords lowercase\n        \tkeys = ['SCREENNAME', 'URL']\n        \tout = [i.lower() if i not in keys else i for i in out]\n          # remove keywords\n        \tout = [i for i in out if i not in keys]\n          # remove stopwords\n        \tlist_stop_words = nltk.corpus.stopwords.words('english')\n        \tlist_stop_words = [regex.sub('[^\\\\w\\\\s]', '', i) for i in list_stop_words]\n        \tout = [i for i in out if i not in list_stop_words]\n          # lemmatizing\n        \tout = [do_lemmatizing(i) for i in out]\n          # keep words 4 or more characters long\n        \tout = [i for i in out if len(i) >= 5]\n        \treturn out\n    ```", "```py\n    clean = list(map(do_tweet_cleaning, raw))\n    ```", "```py\n    clean = list(filter(None.__ne__, clean))\n    print(\"HEADLINES:\\n{lines}\\n\".format(lines=clean[:5]))\n    print(\"LENGTH:\\n{length}\\n\".format(length=len(clean)))\n    ```", "```py\n    clean_sentences = [\" \".join(i) for i in clean]\n    print(clean_sentences[0:10])\n    ```", "```py\n    number_words = 10\n    number_docs = 10\n    number_features = 1000\n    ```", "```py\n    vectorizer1 = sklearn.feature_extraction.text.CountVectorizer(\n        analyzer=»word»,\n        max_df=0.95, \n        min_df=10, \n        max_features=number_features\n    )\n    clean_vec1 = vectorizer1.fit_transform(clean_sentences)\n    print(clean_vec1[0])\n    feature_names_vec1 = vectorizer1.get_feature_names()\n    ```", "```py\n    (0, 320)    1\n    ```", "```py\n    def perplexity_by_ntopic(data, ntopics):\n        output_dict = {\n            «Number Of Topics\": [], \n            «Perplexity Score»: []\n        }\n        for t in ntopics:\n            lda = sklearn.decomposition.LatentDirichletAllocation(\n                n_components=t,\n                learning_method=\"online\",\n                random_state=0\n            )\n            lda.fit(data)\n            output_dict[\"Number Of Topics\"].append(t)\n            output_dict[\"Perplexity Score\"].append(lda.perplexity(data))\n        output_df = pandas.DataFrame(output_dict)\n        index_min_perplexity = output_df[\"Perplexity Score\"].idxmin()\n        output_num_topics = output_df.loc[\n            index_min_perplexity,  # index\n            «Number Of Topics\"  # column\n        ]\n        return (output_df, output_num_topics)\n    df_perplexity, optimal_num_topics = perplexity_by_ntopic(\n        clean_vec1, \n        ntopics=[i for i in range(1, 21) if i % 2 == 0]\n    )\n    print(df_perplexity)\n    ```", "```py\n    lda = sklearn.decomposition.LatentDirichletAllocation(\n        n_components=optimal_num_topics,\n        learning_method=\"online\",\n        random_state=0\n    )\n    lda.fit(clean_vec1)\n    ```", "```py\n    def get_topics(mod, vec, names, docs, ndocs, nwords):\n        # word to topic matrix\n        W = mod.components_\n        W_norm = W / W.sum(axis=1)[:, numpy.newaxis]\n        # topic to document matrix\n        H = mod.transform(vec)\n        W_dict = {}\n        H_dict = {}\n        for tpc_idx, tpc_val in enumerate(W_norm):\n            topic = «Topic{}\".format(tpc_idx)\n            # formatting w\n            W_indices = tpc_val.argsort()[::-1][:nwords]\n            W_names_values = [\n                (round(tpc_val[j], 4), names[j]) \n                for j in W_indices\n            ]\n            W_dict[topic] = W_names_values\n            # formatting h\n            H_indices = H[:, tpc_idx].argsort()[::-1][:ndocs]\n            H_names_values = [\n            (round(H[:, tpc_idx][j], 4), docs[j]) \n                for j in H_indices\n            ]\n            H_dict[topic] = H_names_values\n        W_df = pandas.DataFrame(\n            W_dict, \n            index=[\"Word\" + str(i) for i in range(nwords)]\n        )\n        H_df = pandas.DataFrame(\n            H_dict,\n            index=[\"Doc\" + str(i) for i in range(ndocs)]\n        )\n        return (W_df, H_df)\n    W_df, H_df = get_topics(\n        mod=lda,\n        vec=clean_vec1,\n        names=feature_names_vec1,\n        docs=raw,\n        ndocs=number_docs, \n        nwords=number_words\n    )\n    print(W_df)\n    ```", "```py\n    print(H_df)\n    ```", "```py\n    lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, vectorizer1, R=10)\n    pyLDAvis.display(lda_plot)\n    ```", "```py\n    vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer(\n        analyzer=\"word\",\n        max_df=0.5, \n        min_df=20, \n        max_features=number_features,\n        smooth_idf=False\n    )\n    clean_vec2 = vectorizer2.fit_transform(clean_sentences)\n    print(clean_vec2[0])\n    feature_names_vec2 = vectorizer2.get_feature_names()\n    ```", "```py\n    nmf = sklearn.decomposition.NMF(\n        n_components=optimal_num_topics,\n        init=\"nndsvda\",\n        solver=\"mu\",\n        beta_loss=\"frobenius\",\n        random_state=0, \n        alpha=0.1, \n        l1_ratio=0.5\n    )\n    nmf.fit(clean_vec2)\n    ```", "```py\n    W_df, H_df = get_topics(\n        mod=nmf,\n        vec=clean_vec2,\n        names=feature_names_vec2,\n        docs=raw,\n        ndocs=number_docs, \n        nwords=number_words\n    )\n    print(W_df)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    import mlxtend.frequent_patterns\n    import mlxtend.preprocessing\n    import numpy\n    import pandas\n    online = pandas.read_excel(\n        io=\"Online Retail.xlsx\", \n        sheet_name=\"Online Retail\", \n        header=0\n    )\n    ```", "```py\n    online['IsCPresent'] = (\n        online['InvoiceNo']\n        .astype(str)\n        .apply(lambda x: 1 if x.find('C') != -1 else 0)\n    )\n    online1 = (\n        online\n        .loc[online[\"Quantity\"] > 0]\n        .loc[online['IsCPresent'] != 1]\n        .loc[:, [\"InvoiceNo\", \"Description\"]]\n        .dropna()\n    )\n    invoice_item_list = []\n    for num in list(set(online1.InvoiceNo.tolist())):\n        tmp_df = online1.loc[online1['InvoiceNo'] == num]\n        tmp_items = tmp_df.Description.tolist()\n        invoice_item_list.append(tmp_items)\n    ```", "```py\n    online_encoder = mlxtend.preprocessing.TransactionEncoder()\n    online_encoder_array = online_encoder.fit_transform(invoice_item_list)\n    online_encoder_df = pandas.DataFrame(\n        online_encoder_array, \n        columns=online_encoder.columns_\n    )\n    online_encoder_df.loc[\n        20125:20135, \n        online_encoder_df.columns.tolist()[100:110]\n    ]\n    ```", "```py\n    mod_colnames_minsupport = mlxtend.frequent_patterns.apriori(\n        online_encoder_df, \n        min_support=0.01,\n        use_colnames=True\n    )\n    mod_colnames_minsupport.loc[0:6]\n    ```", "```py\n    mod_colnames_minsupport[\n        mod_colnames_minsupport['itemsets'] == frozenset(\n            {'10 COLOUR SPACEBOY PEN'}\n        )\n    ]\n    ```", "```py\n    mod_colnames_minsupport['length'] = (\n        mod_colnames_minsupport['itemsets'].apply(lambda x: len(x))\n    )\n    mod_colnames_minsupport[\n        (mod_colnames_minsupport['length'] == 2) & \n        (mod_colnames_minsupport['support'] >= 0.02) &\n        (mod_colnames_minsupport['support'] < 0.021)\n    ]\n    ```", "```py\n    mod_colnames_minsupport.hist(\"support\", grid=False, bins=30)\n    plt.title(\"Support\")\n    ```", "```py\n    rules = mlxtend.frequent_patterns.association_rules(\n        mod_colnames_minsupport, \n        metric=\"confidence\",\n        min_threshold=0.6, \n        support_only=False\n    )\n    rules.loc[0:6]\n    ```", "```py\n    print(\"Number of Associations: {}\".format(rules.shape[0]))\n    ```", "```py\n    rules.plot.scatter(\"support\", \"confidence\", alpha=0.5, marker=\"*\")\n    plt.xlabel(\"Support\")\n    plt.ylabel(\"Confidence\")\n    plt.title(\"Association Rules\")\n    plt.show()\n    ```", "```py\n    rules.hist(\"lift\", grid=False, bins=30)\n    plt.title(\"Lift\")\n    ```", "```py\nrules.hist(\"leverage\", grid=False, bins=30)\nplt.title(\"Leverage\")\n```", "```py\nplt.hist(\n    rules[numpy.isfinite(rules['conviction'])].conviction.values, \n    bins = 30\n)\nplt.title(\"Conviction\")\n```", "```py\n    get_ipython().run_line_magic('matplotlib', 'inline')\n    import matplotlib.pyplot as plt\n    import numpy\n    import pandas\n    import seaborn\n    import sklearn.datasets\n    import sklearn.model_selection\n    import sklearn.neighbors\n    seaborn.set()\n    ```", "```py\n    rand = numpy.random.RandomState(100)\n    vals = rand.randn(1000)  # standard normal\n    vals[375:] += 3.5\n    ```", "```py\n    fig, ax = plt.subplots(figsize=(14, 10))\n    ax.hist(vals, bins=50, density=True, label='Sampled Values')\n    ax.plot(vals, -0.005 - 0.01 * numpy.random.random(len(vals)), '+k', label='Individual Points')\n    ax.legend(loc='upper right')\n    ```", "```py\n    bandwidths = 10 ** numpy.linspace(-1, 1, 100)\n    grid = sklearn.model_selection.GridSearchCV(\n        estimator=sklearn.neighbors.KernelDensity(kernel=\"gaussian\"),\n        param_grid={\"bandwidth\": bandwidths},\n        cv=10\n    )\n    grid.fit(vals[:, None])\n    ```", "```py\n    best_bandwidth = grid.best_params_[\"bandwidth\"]\n    print(\n        \"Best Bandwidth Value: {}\"\n        .format(best_bandwidth)\n    )\n    ```", "```py\n    fig, ax = plt.subplots(figsize=(14, 10))\n    ax.hist(vals, bins=50, density=True, alpha=0.75, label='Sampled Values')\n    x_vec = numpy.linspace(-4, 8, 10000)[:, numpy.newaxis]\n    log_density = numpy.exp(grid.best_estimator_.score_samples(x_vec))\n    ax.plot(\n         x_vec[:, 0], log_density, \n         '-', linewidth=4, label='Kernel = Gaussian'\n    )\n    ax.legend(loc='upper right')\n    ```", "```py\n    base_path = (\n        \"~/Documents/packt/unsupervised-learning-python/\"\n        \"lesson-9-hotspot-models/metro-jul18-dec18/\"\n        \"{yr_mon}/{yr_mon}-metropolitan-street.csv\"\n    )\n    print(base_path)\n    yearmon_list = [\n        \"2018-0\" + str(i) if i <= 9 else \"2018-\" + str(i) \n        for i in range(7, 13)\n    ]\n    print(yearmon_list)\n    data_yearmon_list = []\n    for idx, i in enumerate(yearmon_list):\n        df = pandas.read_csv(\n            base_path.format(yr_mon=i), \n            header=0\n        )\n\n        data_yearmon_list.append(df)\n\n        if idx == 0:\n            print(\"Month: {}\".format(i))\n            print(\"Dimensions: {}\".format(df.shape))\n            print(\"Head:\\n{}\\n\".format(df.head(2)))\n    london = pandas.concat(data_yearmon_list)\n    ```", "```py\n    print(\n        \"Dimensions - Full Data:\\n{}\\n\"\n        .format(london.shape)\n    )\n    print(\n        \"Unique Months - Full Data:\\n{}\\n\"\n        .format(london[\"Month\"].unique())\n    )\n    print(\n        \"Number of Unique Crime Types - Full Data:\\n{}\\n\"\n        .format(london[\"Crime type\"].nunique())\n    )\n    print(\n        \"Unique Crime Types - Full Data:\\n{}\\n\"\n        .format(london[\"Crime type\"].unique())\n    )\n    print(\n        \"Count Occurrences Of Each Unique Crime Type - Full Type:\\n{}\\n\"\n        .format(london[\"Crime type\"].value_counts())\n    )\n    ```", "```py\n    london_subset = london[[\"Month\", \"Longitude\", \"Latitude\", \"Crime type\"]]\n    london_subset.head(5)\n    ```", "```py\n    crime_bicycle_jul = london_subset[\n        (london_subset[\"Crime type\"] == \"Bicycle theft\") & \n        (london_subset[\"Month\"] == \"2018-07\")\n    ]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", crime_bicycle_jul, kind=\"kde\")\n    ```", "```py\n    crime_bicycle_sept = london_subset[\n        (london_subset[\"Crime type\"] == \"Bicycle theft\") & \n        (london_subset[\"Month\"] == \"2018-09\")\n    ]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", crime_bicycle_sept, kind=\"kde\")\n    ```", "```py\n    crime_bicycle_dec = london_subset[\n        (london_subset[\"Crime type\"] == \"Bicycle theft\") & \n        (london_subset[\"Month\"] == \"2018-12\")\n    ]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", crime_bicycle_dec, kind=\"kde\")\n    ```", "```py\n    crime_shoplift_aug = london_subset[\n        (london_subset[\"Crime type\"] == \"Shoplifting\") & \n        (london_subset[\"Month\"] == \"2018-08\")\n    ]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", crime_shoplift_aug, kind=\"kde\")\n    ```", "```py\n    crime_shoplift_oct = london_subset[\n        (london_subset[\"Crime type\"] == \"Shoplifting\") & \n        (london_subset[\"Month\"] == \"2018-10\")\n    ]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", crime_shoplift_oct, kind=\"kde\")\n    ```", "```py\n    crime_shoplift_nov = london_subset[\n        (london_subset[\"Crime type\"] == \"Shoplifting\") & \n        (london_subset[\"Month\"] == \"2018-11\")\n    ]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", crime_shoplift_nov, kind=\"kde\")\n    ```", "```py\n    crime_burglary_jul = london_subset[\n        (london_subset[\"Crime type\"] == \"Burglary\") & \n        (london_subset[\"Month\"] == \"2018-07\")\n    ]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", crime_burglary_jul, kind=\"kde\")\n    ```", "```py\ncrime_burglary_oct = london_subset[\n    (london_subset[\"Crime type\"] == \"Burglary\") & \n    (london_subset[\"Month\"] == \"2018-10\")\n]\nseaborn.jointplot(\"Longitude\", \"Latitude\", crime_burglary_oct, kind=\"kde\")\n```", "```py\ncrime_burglary_dec = london_subset[\n    (london_subset[\"Crime type\"] == \"Burglary\") & \n    (london_subset[\"Month\"] == \"2018-12\")\n]\nseaborn.jointplot(\"Longitude\", \"Latitude\", crime_burglary_dec, kind=\"kde\")\n```"]
["```py\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\ndf = pd.read_csv('student-por.csv')\ndf.head()\n```", "```py\ndf = pd.read_csv('student-por.csv', sep=';')\ndf.head()\n```", "```py\ndf.isnull().sum()\nschool        0\nsex           1\nage           1\naddress       0\n…\nhealth        0\nabsences      0\nG1            0\nG2            0\nG3            0\ndtype: int64\n```", "```py\ndf[df.isna().any(axis=1)]\n```", "```py\npd.options.display.max_columns = None\n```", "```py\ndf[df.isna().any(axis=1)]\n```", "```py\ndf['age'].fillna(-999.0)\n```", "```py\ndf['sex'] = df['sex'].fillna(df['sex'].mode())\ndf['guardian'] = df['guardian'].fillna(df['guardian'].mode())\n```", "```py\ndf.head()\n```", "```py\n    categorical_columns = df.columns[df.dtypes==object].tolist()\n    ```", "```py\n    from sklearn.preprocessing import OneHotEncoder\n    ohe = OneHotEncoder()\n    ```", "```py\n    hot = ohe.fit_transform(df[categorical_columns])\n    ```", "```py\n    print(hot)\n    ```", "```py\n      (0, 0)\t\t1.0\n      (0, 2)\t\t1.0\n      (0, 5)\t\t1.0\n      (0, 6)\t\t1.0\n      (0, 8)\t\t1.0\n    …  \n    0 have been skipped. For instance, the 0th row and the 1st column, denoted by (0, 1), has a value of 0.0 in the dense matrix, but it's skipped over in the one-hot matrix.\n    ```", "```py\nhot\n```", "```py\n<649x43 sparse matrix of type '<class 'numpy.float64'>'\n\twith 11033 stored elements in Compressed Sparse Row format>\n```", "```py\ncold_df = df.select_dtypes(exclude=[\"object\"])\ncold_df.head()\n```", "```py\n    from scipy.sparse import csr_matrix\n    cold = csr_matrix(cold_df)\n    ```", "```py\n    from scipy.sparse import hstack\n    final_sparse_matrix = hstack((hot, cold))\n    ```", "```py\n    final_df = pd.DataFrame(final_sparse_matrix.toarray())\n    final_df.head()\n    ```", "```py\nclass YourClass(TransformerMixin):\n    def __init__(self):\n        None\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        # insert code to transform X\n        return X\n```", "```py\n    from sklearn.base import TransformerMixin \n    class NullValueImputer(TransformerMixin):\n    ```", "```py\n    def __init__(self):\n    None\n    ```", "```py\n    def fit(self, X, y=None):\n    return self\n    ```", "```py\n    def transform(self, X, y=None):\n    ```", "```py\n    for column in X.columns.tolist():\n    ```", "```py\n        if column in X.columns[X.dtypes==object].tolist():\n    ```", "```py\n            X[column] = X[column].fillna(X[column].mode())\n    ```", "```py\n        else:\n            X[column]=X[column].fillna(-999.0)\n          return X\n    ```", "```py\ndf = pd.read_csv('student-por.csv', sep=';')\nnvi = NullValueImputer().fit_transform(df)\nnvi.head()\n```", "```py\n    class SparseMatrix(TransformerMixin):\n    ```", "```py\n    def __init__(self):\n        \t\tNone\n    ```", "```py\n    def fit(self, X, y=None):\n        \t\treturn self\n    ```", "```py\n    def transform(self, X, y=None):\n    ```", "```py\n        \t\tcategorical_columns= X.columns[X.dtypes==object].tolist()\n    ```", "```py\n        \t\tohe = OneHotEncoder() \n    ```", "```py\n    hot = ohe.fit_transform(X[categorical_columns])\n    ```", "```py\n    cold_df = X.select_dtypes(exclude=[\"object\"])\n    ```", "```py\n            \tcold = csr_matrix(cold_df)\n    ```", "```py\n             final_sparse_matrix = hstack((hot, cold))\n    ```", "```py\n             final_csr_matrix = final_sparse_matrix.tocsr()\n             return final_csr_matrix\n    ```", "```py\n    sm = SparseMatrix().fit_transform(nvi)\n    print(sm)\n    ```", "```py\n      (0, 0)\t1.0\n      (0, 2)\t1.0\n      (0, 5)\t1.0\n      (0, 6)\t1.0\n      (0, 8)\t1.0\n      (0, 10)\t1.0\n      :\t:\n      (648, 53)\t4.0\n      (648, 54)\t5.0\n      (648, 55)\t4.0\n      (648, 56)\t10.0\n      (648, 57)\t11.0\n      (648, 58)\t11.0\n    ```", "```py\n    sm_df = pd.DataFrame(sm.toarray())\n    sm_df.head()\n    ```", "```py\n    df = pd.read_csv('student-por.csv', sep=';')\n    ```", "```py\n    y = df.iloc[:, -1]\n    X = df.iloc[:, :-3]\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n    ```", "```py\n    from sklearn.pipeline import Pipeline\n    ```", "```py\n    data_pipeline = Pipeline([('null_imputer', NullValueImputer()), ('sparse', SparseMatrix())])\n    ```", "```py\n    X_train_transformed = data_pipeline.fit_transform(X_train)\n    ```", "```py\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom xgboost import XGBRegressor\n```", "```py\ny_train.value_counts()\n```", "```py\n11    82\n10    75\n13    58\n12    53\n14    42\n15    36\n9     29\n16    27\n8     26\n17    24\n18    14\n0     10\n7      7\n19     1\n6      1\n5      1\n```", "```py\n    kfold = KFold(n_splits=5, shuffle=True, random_state=2)\n    ```", "```py\n    def cross_val(model):\n        scores = cross_val_score(model, X_train_transformed, y_train, scoring='neg_root_mean_squared_error', cv=kfold)\n        rmse = (-scores.mean())\n        return rmse\n    ```", "```py\n    cross_val(XGBRegressor(missing=-999.0))\n    ```", "```py\n    2.9702248207546296\n    ```", "```py\n    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_train_transformed, y_train, random_state=2)\n    ```", "```py\n    def n_estimators(model):\n        eval_set = [(X_test_2, y_test_2)]\n        eval_metric=\"rmse\"\n        model.fit(X_train_2, y_train_2, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=100)\n        y_pred = model.predict(X_test_2)\n        rmse = MSE(y_test_2, y_pred)**0.5\n        return rmse  \n    ```", "```py\n    n_estimators(XGBRegressor(n_estimators=5000, missing=-999.0))\n    ```", "```py\n    [128]\tvalidation_0-rmse:3.10450\n    [129]\tvalidation_0-rmse:3.10450\n    [130]\tvalidation_0-rmse:3.10450\n    [131]\tvalidation_0-rmse:3.10450\n    Stopping. Best iteration:\n    [31]\tvalidation_0-rmse:3.09336\n    ```", "```py\n    3.0933612343143153\n    ```", "```py\ndef grid_search(params, reg=XGBRegressor(missing=-999.0)):\n    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=kfold)\n    grid_reg.fit(X_train_transformed, y_train)\n    best_params = grid_reg.best_params_\n    print(\"Best params:\", best_params)\n    best_score = np.sqrt(-grid_reg.best_score_)\n    print(\"Best score:\", best_score)\n```", "```py\n    grid_search(params={'max_depth':[1, 2, 3, 4, 6, 7, 8], \n                        'n_estimators':[31]})\n    ```", "```py\n    Best params: {'max_depth': 1, 'n_estimators': 31}\n    Best score: 2.6634430373079425\n    ```", "```py\n    grid_search(params={'max_depth':[1, 2, 3], \n                        'min_child_weight':[1,2,3,4,5], \n                        'n_estimators':[31]})\n    ```", "```py\n    Best params: {'max_depth': 1, 'min_child_weight': 1, 'n_estimators': 31}\n    Best score: 2.6634430373079425\n    ```", "```py\n    grid_search(params={'max_depth':[2],\n                        'min_child_weight':[2,3],\n                        'subsample':[0.5, 0.6, 0.7, 0.8, 0.9],\n                       'n_estimators':[31, 50]})\n    ```", "```py\n    Best params: {'max_depth': 1, 'min_child_weight': 2, 'n_estimators': 50, 'subsample': 0.9}\n    Best score: 2.665209161229433\n    ```", "```py\n    grid_search(params={'max_depth':[1],\n                        'min_child_weight':[1, 2, 3], \n                        'subsample':[0.6, 0.7, 0.8], \n                        'colsample_bytree':[0.5, 0.6, 0.7, 0.8, 0.9, 1],\n                       'n_estimators':[50]})\n    ```", "```py\n    Best params: {'colsample_bytree': 0.9, 'max_depth': 1, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 0.8}\n    Best score: 2.659649642579931\n    ```", "```py\n     grid_search(params={'max_depth':[1],\n                        'min_child_weight':[3], \n                        'subsample':[.8], \n                        'colsample_bytree':[0.9],\n                        'colsample_bylevel':[0.6, 0.7, 0.8, 0.9, 1],\n                        'colsample_bynode':[0.6, 0.7, 0.8, 0.9, 1],\n                        'n_estimators':[50]})\n    ```", "```py\n    Best params: {'colsample_bylevel': 0.9, 'colsample_bynode': 0.8, 'colsample_bytree': 0.9, 'max_depth': 1, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 0.8}\n    Best score: 2.64172735526102\n    ```", "```py\nX_test_transformed = data_pipeline.fit_transform(X_test)\n```", "```py\nmodel = XGBRegressor(max_depth=2, min_child_weight=3, subsample=0.9, colsample_bytree=0.8, gamma=2, missing=-999.0)\nmodel.fit(X_train_transformed, y_train)\ny_pred = model.predict(X_test_transformed)\nrmse = MSE(y_pred, y_test)**0.5\nrmse\n```", "```py\n2.7908972630881435\n```", "```py\nmodel = XGBRegressor(max_depth=1,\n                       min_child_weight=5,\n                       subsample=0.6, \n                       colsample_bytree=0.9, \n                       colsample_bylevel=0.9,\n                       colsample_bynode=0.8,\n                     n_estimators=50,\n                       missing=-999.0)\nmodel.fit(X_train_transformed, y_train)\ny_pred = model.predict(X_test_transformed)\nrmse = MSE(y_pred, y_test)**0.5\nrmse\n```", "```py\n2.730601403138633\n```", "```py\nfull_pipeline = Pipeline([('null_imputer', NullValueImputer()),  ('sparse', SparseMatrix()), \n('xgb', XGBRegressor(max_depth=1, min_child_weight=5, subsample=0.6, colsample_bytree=0.9, colsample_bylevel=0.9, colsample_bynode=0.8, missing=-999.0))]) \n```", "```py\nfull_pipeline.fit(X, y)\n```", "```py\nnew_data = X_test\nfull_pipeline.predict(new_data)\n```", "```py\narray([13.55908  ,  8.314051 , 11.078157 , 14.114085 , 12.2938385, 11.374797 , 13.9611025, 12.025812 , 10.80344  , 13.479145 , 13.02319  ,  9.428679 , 12.57761  , 12.405045 , 14.284043 , 8.549758 , 10.158956 ,  9.972576 , 15.502667 , 10.280028 , ...\n```", "```py\nnp.round(full_pipeline.predict(new_data))\n```", "```py\narray([14.,  8., 11., 14., 12., 11., 14., 12., 11., 13., 13.,  9., 13., 12., 14.,  9., 10., 10., 16., 10., 13., 13.,  7., 12.,  7.,  8., 10., 13., 14., 12., 11., 12., 15.,  9., 11., 13., 12., 11.,  8.,\n...\n11., 13., 12., 13.,  9., 13., 10., 14., 12., 15., 15., 11., 14., 10., 14.,  9.,  9., 12., 13.,  9., 11., 14., 13., 11., 13., 13., 13., 13., 11., 13., 14., 15., 13.,  9., 10., 13.,  8.,  8., 12., 15., 14., 13., 10., 12., 13.,  9.], dtype=float32)\n```", "```py\nnew_df = pd.read_csv('student-por.csv')\nnew_X = df.iloc[:, :-3]\nnew_y = df.iloc[:, -1]\nnew_model = full_pipeline.fit(new_X, new_y)\n```", "```py\nmore_new_data = X_test[:25]\nnp.round(new_model.predict(more_new_data))\n```", "```py\narray([14.,  8., 11., 14., 12., 11., 14., 12., 11., 13., 13.,  9., 13., 12., 14.,  9., 10., 10., 16., 10., 13., 13.,  7., 12.,  7.],\n      dtype=float32)\n```", "```py\nsingle_row = X_test[:1]\nsingle_row_plus = pd.concat([single_row, X_test[:25]])\nprint(np.round(new_model.predict(single_row_plus))[:1])\n```", "```py\n[14.]\n```"]
<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer197">
			<h1 id="_idParaDest-261"><a id="_idTextAnchor260"/>Chapter 12: Automating Machine Learning Workflows</h1>
			<p>In the previous chapter, you learned how to deploy machine learning models in different configurations, using both the <strong class="bold">SageMaker SDK</strong> and the <strong class="source-inline">boto3</strong> SDK. We used their APIs in <strong class="bold">Jupyter</strong> <strong class="bold">Notebooks</strong> – the preferred way to experiment and iterate quickly. </p>
			<p>However, running notebooks for production tasks is not a good idea. Even if your code has been carefully tested, what about monitoring, logging, creating other AWS resources, handling errors, rolling back, and so on? Doing all of this right would require a lot of extra work and code, opening the possibility for more bugs. A more industrial approach is required.</p>
			<p>In this chapter, you'll first learn how to provision SageMaker resources with <strong class="bold">AWS</strong> <strong class="bold">CloudFormation</strong> and <strong class="bold">AWS</strong> <strong class="bold">Cloud Development Kit</strong> (<strong class="bold">CDK</strong>) – two AWS services purposely built to bring repeatability, predictability, and robustness. You'll see how you can preview changes before applying them, in order to avoid uncontrolled and potentially destructive operations.</p>
			<p>Then, you'll learn how to automate end-to-end machine learning workflows with two other services – <strong class="bold">AWS</strong> <strong class="bold">Step Functions</strong> and <strong class="bold">Amazon</strong> <strong class="bold">SageMaker Pipelines</strong>. You'll see how to build workflows with simple APIs, and how to visualize results in <strong class="bold">SageMaker Studio</strong>.</p>
			<p>In this chapter, we'll cover the following topics:</p>
			<ul>
				<li>Automating with AWS CloudFormation</li>
				<li>Automating with AWS CDK</li>
				<li>Building end-to-end workflows with AWS Step Functions</li>
				<li>Building end-to-end workflows with Amazon SageMaker Pipelines</li>
			</ul>
			<h1 id="_idParaDest-262"><a id="_idTextAnchor261"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you haven't got one already, please point your browser at <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create one. You should also familiarize yourself with the AWS free tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the <strong class="bold">AWS</strong> <strong class="bold">Command Line Interface</strong> (<strong class="bold">CLI</strong>) for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).</p>
			<p>You will need a working <strong class="bold">Python</strong> 3.x environment. Installing the <strong class="bold">Anaconda</strong> distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory, but strongly encouraged, as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>Code examples included in this book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor262"/>Automating with AWS CloudFormation</h1>
			<p>AWS CloudFormation has long<a id="_idIndexMarker1336"/> been the preferred way to automate infrastructure builds and <a id="_idIndexMarker1337"/>operations on AWS (<a href="https://aws.amazon.com/cloudformation">https://aws.amazon.com/cloudformation</a>). You could certainly write a book <a id="_idIndexMarker1338"/>on the topic, but we'll stick to the basics in this section.</p>
			<p>The first step in using CloudFormation is to write a template – that is, a <strong class="bold">JSON</strong> or <strong class="bold">YAML</strong> text file describing the <strong class="bold">resources</strong> that you want to build, such as an <strong class="bold">EC2</strong> instance or an <strong class="bold">S3</strong> bucket. Resources are available for almost all AWS services, and SageMaker is no exception. If we look at <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_SageMaker.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_SageMaker.html</a>, we see that we can create SageMaker Studio applications, deploy endpoints, and more.</p>
			<p>A template can (and should) include parameters and outputs. The former help make templates as generic as possible. The latter provide information that can be used by downstream applications, such as endpoint URLs or bucket names.</p>
			<p>Once you've written your template file, you pass it to CloudFormation to create a <strong class="bold">stack</strong> – that is, a collection of AWS resources. CloudFormation will parse the template and create all resources automatically. Dependencies are also managed automatically, and resources will be created in the correct order. If a stack can't be created correctly, CloudFormation will roll it back, deleting resources that have been built so far.</p>
			<p>A stack can be updated by applying a newer template revision. CloudFormation will analyze changes, and will create, delete, update, or replace resources accordingly. Thanks to <strong class="bold">change sets</strong>, you can verify<a id="_idIndexMarker1339"/> changes before they are performed, and then decide whether to proceed or not.</p>
			<p>Of course, a stack can be<a id="_idIndexMarker1340"/> deleted, and CloudFormation<a id="_idIndexMarker1341"/> will automatically tear down all its resources, which is a great way to clean up your builds without leaving any cruft behind.</p>
			<p>Let's run a first example, where we deploy a model to a real-time endpoint.</p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor263"/>Writing a template</h2>
			<p>This stack will be equivalent to<a id="_idIndexMarker1342"/> calling the <strong class="source-inline">boto3</strong> API we studied in <a href="B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Deploying Machine Learning Models</em>: <strong class="source-inline">create_model()</strong>, <strong class="source-inline">create_endpoint_configuration()</strong>, and <strong class="source-inline">create_endpoint()</strong>. Accordingly, we'll define three CloudFormation resources (a model, an endpoint configuration, and an endpoint) and their parameters:</p>
			<ol>
				<li>Creating a new YAML file named <strong class="source-inline">endpoint-one-model.yml</strong>, we first define the input parameters for the stack in the <strong class="source-inline">Parameters</strong> section. Each parameter has a name, a description, and a type. Optionally, we can provide default values:<p class="source-code">AWSTemplateFormatVersion: 2010-09-09</p><p class="source-code">Parameters:</p><p class="source-code">    ModelName:</p><p class="source-code">        Description: Model name</p><p class="source-code">        Type: String</p><p class="source-code">    ModelDataUrl:</p><p class="source-code">        Description: Location of model artifact</p><p class="source-code">        Type: String</p><p class="source-code">    ContainerImage:</p><p class="source-code">        Description: Container used to deploy the model</p><p class="source-code">        Type: String</p><p class="source-code">    InstanceType:</p><p class="source-code">        Description: Instance type</p><p class="source-code">        Type: String</p><p class="source-code">        Default: ml.m5.large</p><p class="source-code">    InstanceCount:</p><p class="source-code">        Description: Instance count</p><p class="source-code">        Type: String</p><p class="source-code">        Default: 1</p><p class="source-code">    RoleArn:</p><p class="source-code">        Description: Execution Role ARN</p><p class="source-code">        Type: String</p></li>
				<li>In the <strong class="source-inline">Resources</strong> section, we define a model resource, using the <strong class="source-inline">Ref</strong> built-in function to reference the<a id="_idIndexMarker1343"/> appropriate input parameters:<p class="source-code">Resources:</p><p class="source-code">    Model:</p><p class="source-code">        Type: "AWS::SageMaker::Model"</p><p class="source-code">        Properties:</p><p class="source-code">            Containers:</p><p class="source-code">                -</p><p class="source-code">                    Image: !Ref ContainerImage</p><p class="source-code">                    ModelDataUrl: !Ref ModelDataUrl</p><p class="source-code">            ExecutionRoleArn: !Ref RoleArn</p><p class="source-code">            ModelName: !Ref ModelName</p></li>
				<li>We then define an endpoint configuration resource. We use the <strong class="source-inline">GetAtt</strong> built-in function to get the name of the model resource. Of course, this requires that the model resource already exists, and CloudFormation will make sure that resources<a id="_idIndexMarker1344"/> are created in the right order:<p class="source-code">    EndpointConfig:</p><p class="source-code">        Type: "AWS::SageMaker::EndpointConfig"</p><p class="source-code">        Properties:</p><p class="source-code">            ProductionVariants:</p><p class="source-code">                -</p><p class="source-code">                 ModelName: !GetAtt Model.ModelName</p><p class="source-code">                 VariantName: variant-1</p><p class="source-code">                 InitialInstanceCount: !Ref InstanceCount</p><p class="source-code">                 InstanceType: !Ref InstanceType</p><p class="source-code">                 InitialVariantWeight: 1.0</p></li>
				<li>Finally, we define an endpoint resource. Likewise, we use <strong class="source-inline">GetAtt</strong> to find the name of the endpoint configuration:<p class="source-code">    Endpoint:</p><p class="source-code">        Type: "AWS::SageMaker::Endpoint"</p><p class="source-code">        Properties:</p><p class="source-code">            EndpointConfigName: !GetAtt </p><p class="source-code">            EndpointConfig.EndpointConfigName</p></li>
				<li>In the <strong class="source-inline">Outputs</strong> section, we<a id="_idIndexMarker1345"/> return the CloudFormation identifier of the endpoint, as well as its name:<p class="source-code">Outputs:</p><p class="source-code">    EndpointId:</p><p class="source-code">        Value: !Ref Endpoint</p><p class="source-code">    EndpointName:</p><p class="source-code">        Value: !GetAtt Endpoint.EndpointName</p></li>
			</ol>
			<p>Now that the template is complete (<strong class="source-inline">endpoint-one-model.yml</strong>), we can create a stack.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please make sure that your IAM role has permission to invoke CloudFormation APIs. If not, please<a id="_idIndexMarker1346"/> add the <strong class="source-inline">AWSCloudFormationFullAccess</strong> managed policy to the role.</p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor264"/>Deploying a model to a real-time endpoint</h2>
			<p>Let's<a id="_idIndexMarker1347"/> use <a id="_idIndexMarker1348"/>the <strong class="source-inline">boto3</strong> API to create a <a id="_idIndexMarker1349"/>stack deploying a <strong class="bold">TensorFlow</strong> model. We'll reuse a model trained with <strong class="bold">Keras</strong> on <strong class="bold">Fashion MNIST</strong>:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">As our template is completely region-independent, you can use any region that you want. Just make sure that you have trained a model there, and that you're using the appropriate container image.</p>
			<ol>
				<li value="1">We'll need <strong class="source-inline">boto3</strong> clients for SageMaker and CloudFormation:<p class="source-code">import boto3</p><p class="source-code">sm = boto3.client('sagemaker')</p><p class="source-code">cf = boto3.client('cloudformation')</p></li>
				<li>We describe the training job to find the location of its artifact, and its execution role:<p class="source-code">training_job = </p><p class="source-code">    'tensorflow-training-2021-05-28-14-25-57-394'</p><p class="source-code">job = sm.describe_training_job(</p><p class="source-code">      TrainingJobName=training_job)</p><p class="source-code">model_data_url =    </p><p class="source-code">    job['ModelArtifacts']['S3ModelArtifacts']</p><p class="source-code">role_arn = job['RoleArn']</p></li>
				<li>We <a id="_idIndexMarker1350"/>set<a id="_idIndexMarker1351"/> the container to <a id="_idIndexMarker1352"/>use for deployment. In some cases, this is unnecessary, as the same container is used for training and deployment. For <strong class="bold">TensorFlow</strong> and other frameworks, SageMaker uses two different containers. You can find more information at <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md">https://github.com/aws/deep-learning-containers/blob/master/available_images.md</a>:<p class="source-code">container_image = '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-cpu-py36-ubuntu18.04'</p></li>
				<li>Then, we read our template, create a new stack, and pass the required parameters:<p class="source-code">import time</p><p class="source-code">timestamp = time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">stack_name='endpoint-one-model-'+timestamp</p><p class="source-code">with open('endpoint-one-model.yml', 'r') as f:</p><p class="source-code">  response = cf.create_stack(</p><p class="source-code">      StackName=stack_name,</p><p class="source-code">      TemplateBody=f.read(),</p><p class="source-code">      Parameters=[</p><p class="source-code">           { "ParameterKey":"ModelName",      </p><p class="source-code">             "ParameterValue":training_job+</p><p class="source-code">                              '-'+timestamp },</p><p class="source-code">           { "ParameterKey":"ContainerImage",  </p><p class="source-code">             "ParameterValue":container_image },</p><p class="source-code">           { "ParameterKey":"ModelDataUrl",   </p><p class="source-code">             "ParameterValue":model_data_url },</p><p class="source-code">           { "ParameterKey":"RoleArn",       </p><p class="source-code">             "ParameterValue":role_arn }</p><p class="source-code">      ]</p><p class="source-code">)</p></li>
				<li>Jumping to the CloudFormation console, we see that the stack is being created, as shown in<a id="_idIndexMarker1353"/> the <a id="_idIndexMarker1354"/>following <a id="_idIndexMarker1355"/>screenshot. Notice that resources are created in the right order: model, endpoint configuration, and endpoint:<div id="_idContainer174" class="IMG---Figure"><img src="Images/B17705_12_1.jpg" alt="Figure 12.1 – Viewing stack creation&#13;&#10;" width="874" height="610"/></div><p class="figure-caption">Figure 12.1 – Viewing stack creation</p><p>As we <a id="_idIndexMarker1356"/>would<a id="_idIndexMarker1357"/> expect, we also see the endpoint in SageMaker Studio, as shown in the following screenshot:</p><div id="_idContainer175" class="IMG---Figure"><img src="Images/B17705_12_2.jpg" alt="Figure 12.2 – Viewing endpoint creation&#13;&#10;" width="853" height="368"/></div><p class="figure-caption">Figure 12.2 – Viewing endpoint creation</p></li>
				<li>Once the<a id="_idIndexMarker1358"/> stack <a id="_idIndexMarker1359"/>creation is complete, we <a id="_idIndexMarker1360"/>can use its output to find the name of the endpoint:<p class="source-code">response = cf.describe_stacks(StackName=stack_name)</p><p class="source-code">print(response['Stacks'][0]['StackStatus'])</p><p class="source-code">for o in response['Stacks'][0]['Outputs']:</p><p class="source-code">    if o['OutputKey']=='EndpointName':</p><p class="source-code">         endpoint_name = o['OutputValue']</p><p class="source-code">print(endpoint_name)</p><p>This prints out the stack status and the endpoint name autogenerated by CloudFormation:</p><p><strong class="bold">CREATE_COMPLETE</strong></p><p><strong class="bold">Endpoint-MTaOIs4Vexpt</strong></p></li>
				<li>We can test the endpoint as usual. Then, we can delete the stack and its resources:<p class="source-code">cf.delete_stack(StackName=stack_name)</p></li>
			</ol>
			<p>However, let's <a id="_idIndexMarker1361"/>not <a id="_idIndexMarker1362"/>delete<a id="_idIndexMarker1363"/> the stack right away. Instead, we're going to update it using a change set.</p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor265"/>Modifying a stack with a change set</h2>
			<p>Here, we're going to<a id="_idIndexMarker1364"/> update the number of instances backing the endpoint:</p>
			<ol>
				<li value="1">We create a new change set using the same template and parameters, except <strong class="source-inline">InstanceCount</strong>, which we set to <strong class="source-inline">2</strong>:<p class="source-code">response = cf.create_change_set(</p><p class="source-code">    StackName=stack_name,</p><p class="source-code">    ChangeSetName='add-instance',</p><p class="source-code">    UsePreviousTemplate=True,</p><p class="source-code">    Parameters=[</p><p class="source-code">      { "ParameterKey":"InstanceCount", </p><p class="source-code">        "ParameterValue": "2" },</p><p class="source-code">      { "ParameterKey":"ModelName",</p><p class="source-code">        "UsePreviousValue": True },</p><p class="source-code">      { "ParameterKey":"ContainerImage",</p><p class="source-code">        "UsePreviousValue": True },</p><p class="source-code">      { "ParameterKey":"ModelDataUrl",</p><p class="source-code">        "UsePreviousValue": True },</p><p class="source-code">      { "ParameterKey":"RoleArn",</p><p class="source-code">        "UsePreviousValue": True }</p><p class="source-code">    ]</p><p class="source-code">)</p></li>
				<li>We see details <a id="_idIndexMarker1365"/>on the change set in the CloudFormation console, as shown in the next screenshot. We could also use the <strong class="source-inline">describe_change_set()</strong> API:<div id="_idContainer176" class="IMG---Figure"><img src="Images/B17705_12_3.jpg" alt="Figure 12.3 – Viewing a change set&#13;&#10;" width="979" height="425"/></div><p class="figure-caption">Figure 12.3 – Viewing a change set</p><p>This tells us that the endpoint configuration and the endpoint need to be modified, and possibly replaced. As we already know from <a href="B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Deploying Machine Learning Models</em>, a new endpoint will be created and applied in a non-disruptive <a id="_idIndexMarker1366"/>fashion to the existing endpoint.</p><p class="callout-heading">Note</p><p class="callout">When working with CloudFormation, it's critical that you understand the <strong class="bold">replacement policy</strong> for your resources. Details are available in the documentation for each resource type.</p></li>
				<li>By clicking on the <strong class="bold">Execute</strong> button, we execute the change set. We could also use the <strong class="source-inline">execute_change_set()</strong> API. As expected, the endpoint is immediately updated, as shown in the following screenshot:<div id="_idContainer177" class="IMG---Figure"><img src="Images/B17705_12_4.jpg" alt="Figure 12.4 – Updating the endpoint&#13;&#10;" width="849" height="386"/></div><p class="figure-caption">Figure 12.4 – Updating the endpoint</p></li>
				<li>Once the update is complete, we<a id="_idIndexMarker1367"/> see the sequence of events in the CloudFormation console, as shown in the next screenshot. A new endpoint configuration has been created and applied to the endpoint. The previous endpoint configuration has been deleted:<div id="_idContainer178" class="IMG---Figure"><img src="Images/B17705_12_5.jpg" alt="Figure 12.5 – Updating the stack&#13;&#10;" width="951" height="604"/></div><p class="figure-caption">Figure 12.5 – Updating the stack</p></li>
				<li>We can check that the endpoint is now backed by two instances:<p class="source-code">r = sm.describe_endpoint(EndpointName=endpoint_name)</p><p class="source-code">print r(['ProductionVariants'][0]</p><p class="source-code">        ['CurrentInstanceCount'])</p><p>This prints out the number of instances backing the Production Variant:</p><p class="source-code"><strong class="bold">2</strong></p></li>
			</ol>
			<p>Let's keep working with change <a id="_idIndexMarker1368"/>sets and add a second production variant to the endpoint.</p>
			<h2 id="_idParaDest-267"><a id="_idTextAnchor266"/>Adding a second production variant to the endpoint</h2>
			<p>Our initial template only defined a <a id="_idIndexMarker1369"/>single production variant. We'll update it and add another one (<strong class="source-inline">endpoint-two-models.yml</strong>):</p>
			<ol>
				<li value="1">In the <strong class="source-inline">Parameters</strong> section, we add entries for a second model:<p class="source-code">    ModelName2:</p><p class="source-code">       Description: Second model name</p><p class="source-code">       Type: String</p><p class="source-code">    ModelDataUrl2:</p><p class="source-code">       Description: Location of second model artifact</p><p class="source-code">       Type: String</p><p class="source-code">    VariantWeight2:</p><p class="source-code">       Description: Weight of second model</p><p class="source-code">       Type: String</p><p class="source-code">    Default: 0.0</p></li>
				<li>We do the same in the <strong class="source-inline">Resources</strong> section:<p class="source-code">    Model2:</p><p class="source-code">       Type: "AWS::SageMaker::Model"</p><p class="source-code">       Properties:</p><p class="source-code">           Containers:</p><p class="source-code">               - </p><p class="source-code">                   Image: !Ref ContainerImage</p><p class="source-code">                   ModelDataUrl: !Ref ModelDataUrl2</p><p class="source-code">       ExecutionRoleArn: !Ref RoleArn</p><p class="source-code">       ModelName: !Ref ModelName2</p></li>
				<li>Moving back to our notebook, we get information on another training job. We then create a change set, reading the updated template and passing all required parameters:<p class="source-code">training_job_2 = 'tensorflow-training-2020-06-08-07-32-18-734'</p><p class="source-code">job_2=sm.describe_training_job(</p><p class="source-code">      TrainingJobName=training_job_2)</p><p class="source-code">model_data_url_2=</p><p class="source-code">      job_2['ModelArtifacts']['S3ModelArtifacts']</p><p class="source-code">with open('endpoint-two-models.yml', 'r') as f:</p><p class="source-code">    response = cf.create_change_set(</p><p class="source-code">        StackName=stack_name,</p><p class="source-code">        ChangeSetName='add-model',</p><p class="source-code">        TemplateBody=f.read(),</p><p class="source-code">        Parameters=[</p><p class="source-code">             { "ParameterKey":"ModelName",      </p><p class="source-code">               "UsePreviousValue": True },</p><p class="source-code">             { "ParameterKey":"ModelDataUrl",   </p><p class="source-code">              "UsePreviousValue": True },</p><p class="source-code">            { "ParameterKey":"ContainerImage", </p><p class="source-code">              "UsePreviousValue": True },</p><p class="source-code">            { "ParameterKey":"RoleArn",        </p><p class="source-code">              "UsePreviousValue": True }, </p><p class="source-code">            { "ParameterKey":"ModelName2",     </p><p class="source-code">              "ParameterValue": training_job_2+'-</p><p class="source-code">                                '+timestamp},</p><p class="source-code">            { "ParameterKey":"ModelDataUrl2",  </p><p class="source-code">               "ParameterValue": model_data_url_2 }</p><p class="source-code">        ]</p><p class="source-code">    )</p></li>
				<li>Looking at the CloudFormation<a id="_idIndexMarker1370"/> console, we see the changes caused by the change set. Create a new model and modify the endpoint configuration and the endpoint:<div id="_idContainer179" class="IMG---Figure"><img src="Images/B17705_12_6.jpg" alt="Figure 12.6 – Viewing the change set&#13;&#10;" width="1064" height="405"/></div><p class="figure-caption">Figure 12.6 – Viewing the change set</p></li>
				<li>We execute the <a id="_idIndexMarker1371"/>change set. Once it's complete, we see that the endpoint now supports two production variants. Note that the instance count is back to its initial value, as we defined it as <strong class="source-inline">1</strong> in the updated template:</li>
			</ol>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="Images/B17705_12_7.jpg" alt="Figure 12.7 – Viewing production variants&#13;&#10;" width="1071" height="141"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.7 – Viewing production variants</p>
			<p>The new production variant has a weight of <strong class="source-inline">0</strong>, so it won't be used for prediction. Let's see how we can gradually<a id="_idIndexMarker1372"/> introduce<a id="_idIndexMarker1373"/> it using <strong class="bold">canary deployment</strong>.</p>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor267"/>Implementing canary deployment</h2>
			<p>Canary deployment is a popular<a id="_idIndexMarker1374"/> technique for gradual application deployment (<a href="https://martinfowler.com/bliki/CanaryRelease.html">https://martinfowler.com/bliki/CanaryRelease.html</a>), and it can <a id="_idIndexMarker1375"/>also be used for machine learning models. </p>
			<p>Simply put, we'll use a <a id="_idIndexMarker1376"/>series of stack updates to gradually increase the weight of the second production variant in 10% increments, until it completely replaces the first production variant. We'll <a id="_idIndexMarker1377"/>also create a CloudWatch alarm monitoring the latency of the second production variant – if the alarm is triggered, the change set will be rolled back:</p>
			<ol>
				<li value="1">We create a CloudWatch alarm monitoring the 60-second average latency of the second production variant. We set the threshold at 500 milliseconds:<p class="source-code">cw = boto3.client('cloudwatch')</p><p class="source-code">alarm_name = 'My_endpoint_latency'</p><p class="source-code">response = cw.put_metric_alarm(</p><p class="source-code">    AlarmName=alarm_name,</p><p class="source-code">    ComparisonOperator='GreaterThanThreshold',</p><p class="source-code">    EvaluationPeriods=1,</p><p class="source-code">    MetricName='ModelLatency',</p><p class="source-code">    Namespace='AWS/SageMaker',</p><p class="source-code">    Period=60,</p><p class="source-code">    Statistic='Average',</p><p class="source-code">    Threshold=500000.0,</p><p class="source-code">    AlarmDescription=</p><p class="source-code">        '1-minute average latency exceeds 500ms',</p><p class="source-code">    Dimensions=[</p><p class="source-code">        { 'Name': 'EndpointName', </p><p class="source-code">          'Value': endpoint_name },</p><p class="source-code">        { 'Name': 'VariantName', </p><p class="source-code">          'Value': 'variant-2' }</p><p class="source-code">    ],</p><p class="source-code">    Unit='Microseconds'</p><p class="source-code">)</p></li>
				<li>We find the ARN of the alarm:<p class="source-code">response = cw.describe_alarms(AlarmNames=[alarm_name])</p><p class="source-code">for a in response['MetricAlarms']:</p><p class="source-code">    if a['AlarmName'] == alarm_name:</p><p class="source-code">        alarm_arn = a['AlarmArn']</p></li>
				<li>Then, we loop <a id="_idIndexMarker1378"/>over weights and update<a id="_idIndexMarker1379"/> the stack. Change sets are unnecessary here, as we know exactly what's going to happen from a resource perspective. We set <a id="_idIndexMarker1380"/>our CloudWatch alarm as a <strong class="bold">rollback trigger</strong>, giving it five minutes to go off after each update before moving on to the next:<p class="source-code">for w in list(range(10,110,10)):</p><p class="source-code">    response = cf.update_stack(</p><p class="source-code">        StackName=stack_name,</p><p class="source-code">        UsePreviousTemplate=True,</p><p class="source-code">        Parameters=[</p><p class="source-code">            { "ParameterKey":"ModelName",      </p><p class="source-code">              "UsePreviousValue": True },</p><p class="source-code">            { "ParameterKey":"ModelDataUrl",</p><p class="source-code">              "UsePreviousValue": True },</p><p class="source-code">            { "ParameterKey":"ContainerImage", </p><p class="source-code">              "UsePreviousValue": True },</p><p class="source-code">            { "ParameterKey":"RoleArn",   </p><p class="source-code">              "UsePreviousValue": True }, </p><p class="source-code">            { "ParameterKey":"ModelName2",</p><p class="source-code">              "UsePreviousValue": True },</p><p class="source-code">            { "ParameterKey":"ModelDataUrl2",</p><p class="source-code">              "UsePreviousValue": True },</p><p class="source-code">            { "ParameterKey":"VariantWeight",</p><p class="source-code">              "ParameterValue": str(100-w) },</p><p class="source-code">            { "ParameterKey":"VariantWeight2", </p><p class="source-code">              "ParameterValue": str(w) }</p><p class="source-code">        ],</p><p class="source-code">        RollbackConfiguration={</p><p class="source-code">            'RollbackTriggers': [</p><p class="source-code">               { 'Arn': alarm_arn,: </p><p class="source-code">                 'AWS::CloudWatch::Alarm' }</p><p class="source-code">            ],</p><p class="source-code">            'MonitoringTimeInMinutes': 5</p><p class="source-code">        }</p><p class="source-code">    )</p><p class="source-code">    waiter = cf.get_waiter('stack_update_complete')</p><p class="source-code">    waiter.wait(StackName=stack_name)</p><p class="source-code">    print("Sending %d% of traffic to new model" % w)</p></li>
			</ol>
			<p>That's all it takes. Pretty cool, don't you think?</p>
			<p>This cell will run for a couple of hours, so don't stop it. In another notebook, the next step is to start sending some traffic to the endpoint. For the sake of brevity, I won't include the code, which is identical to the one we used in <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services with Built-in Frameworks</em>. You'll find the notebook in the GitHub repository for this book (<strong class="source-inline">Chapter12/cloudformation/Predict Fashion MNIST images.ipynb</strong>).</p>
			<p>Now, all we have to do is<a id="_idIndexMarker1381"/> sit back, have a cup of tea, and enjoy the fact that our model is being deployed safely and automatically. As endpoint updates are seamless, client applications won't notice a<a id="_idIndexMarker1382"/> thing. </p>
			<p>After a couple of hours, deployment is complete. The next screenshot shows invocations for both variants over time. As we can see, traffic was gradually shifted from the first variant to the second one:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="Images/B17705_12_8.jpg" alt="Figure 12.8 – Monitoring canary deployment&#13;&#10;" width="1650" height="559"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.8 – Monitoring canary deployment</p>
			<p>Latency stayed well under our<a id="_idIndexMarker1383"/> 500-millisecond limit, and the alarm wasn't triggered, as shown in the next screenshot:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="Images/B17705_12_9.jpg" alt="Figure 12.9 – Viewing the CloudWatch alarm&#13;&#10;" width="1650" height="542"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.9 – Viewing the CloudWatch alarm</p>
			<p>This example can<a id="_idIndexMarker1384"/> serve as a starting point for your own deployments. For example, you could add an alarm monitoring <strong class="source-inline">4xx</strong> or <strong class="source-inline">5xx</strong> HTTP errors. You could also monitor a business metric directly impacted by prediction latency and accuracy, such as click-through rate, conversion rate, and so on. A useful thing<a id="_idIndexMarker1385"/> to add would be an alarm notification (email, SMS, or even a Lambda function) in order to trigger downstream <a id="_idIndexMarker1386"/>actions, should model deployment fail. The possibilities are endless!</p>
			<p>When you're done, <em class="italic">don't forget to delete the stack</em>, either in the CloudFormation console or with the <strong class="source-inline">delete_stack()</strong> API. This will automatically clean up all AWS resources created by the stack.</p>
			<p><strong class="bold">Blue-green deployment</strong> is another popular technique. Let's see how we can implement it on SageMaker.</p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor268"/>Implementing blue-green deployment</h2>
			<p>Blue-green<a id="_idIndexMarker1387"/> deployment<a id="_idIndexMarker1388"/> requires two <a id="_idIndexMarker1389"/>production environments (<a href="https://martinfowler.com/bliki/BlueGreenDeployment.html">https://martinfowler.com/bliki/BlueGreenDeployment.html</a>):</p>
			<ul>
				<li>The live production<a id="_idIndexMarker1390"/> environment (<strong class="source-inline">blue</strong>) running version <strong class="source-inline">n</strong></li>
				<li>A copy of this environment (<strong class="source-inline">green</strong>) running version <strong class="source-inline">n+1</strong></li>
			</ul>
			<p>Let's look at two possible scenarios, which we could implement using the same APIs we've<a id="_idIndexMarker1391"/> used <a id="_idIndexMarker1392"/>for canary deployment.</p>
			<h3>Implementing blue-green deployment with a single endpoint</h3>
			<p>Starting from an <a id="_idIndexMarker1393"/>existing endpoint<a id="_idIndexMarker1394"/> running the current version of the model, we would carry out the following steps:</p>
			<ol>
				<li value="1">Create a new endpoint configuration with two production variants: one for the current model and one for the new model. Initial weights would be set to <strong class="source-inline">1</strong> and <strong class="source-inline">0</strong> respectively.</li>
				<li>Apply it to the endpoint.</li>
				<li>Run tests on the new production variant, selecting it explicitly with the <strong class="source-inline">TargetVariant</strong> parameter in <strong class="source-inline">invoke_endpoint()</strong>.</li>
				<li>When tests are satisfactory, update weights to <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. This will seamlessly switch traffic to the new model. If anything goes wrong, revert the weights to <strong class="source-inline">1</strong> and <strong class="source-inline">0</strong>.</li>
				<li>When the deployment is complete, update the endpoint to delete the first production variant.</li>
			</ol>
			<p>This is a simple and robust solution. However, updating <a id="_idIndexMarker1395"/>an endpoint takes several minutes, making the whole process not as quick as one may want. Let's see how we can fix this problem by using two endpoints.</p>
			<h3>Implementing blue-green deployment with two endpoints</h3>
			<p>Starting from an existing <a id="_idIndexMarker1396"/>endpoint <a id="_idIndexMarker1397"/>running the current version of the model, we would implement the following steps:</p>
			<ol>
				<li value="1">Create a second endpoint running the new version of the model.</li>
				<li>Run tests on this new endpoint.</li>
				<li>When the tests are satisfactory, switch all traffic to the new endpoint. This could be achieved in different ways; for example, updating a parameter in your business application, or updating a private DNS entry. If anything goes wrong, revert to the previous setting.</li>
				<li>When the deployment is complete, delete the old endpoint.</li>
			</ol>
			<p>This setup is a little more complex, but it lets you switch instantly from one model version to the next, both for deployments and rollbacks.</p>
			<p>CloudFormation is a fantastic tool for automation, and any time spent learning it will pay dividends. Yet some AWS users prefer writing code to writing templates, which is why we introduced the CDK.</p>
			<h1 id="_idParaDest-270"><a id="_idTextAnchor269"/>Automating with AWS CDK</h1>
			<p>AWS CDK is a multi-language SDK that lets you write code to<a id="_idIndexMarker1398"/> define AWS infrastructure (<a href="https://github.com/aws/aws-cdk">https://github.com/aws/aws-cdk</a>). Using the CDK CLI, you can then provision this infrastructure, using CloudFormation under the hood.</p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor270"/>Installing the CDK</h2>
			<p>The CDK<a id="_idIndexMarker1399"/> is natively implemented with <strong class="bold">Node.js</strong>, so please <a id="_idIndexMarker1400"/>make sure that the <strong class="source-inline">npm</strong> tool is installed on your machine (<a href="https://www.npmjs.com/get-npm">https://www.npmjs.com/get-npm</a>). </p>
			<p>Installing the CDK is then as simple as this:</p>
			<p class="source-code">$ npm i -g aws-cdk</p>
			<p class="source-code">$ cdk --version</p>
			<p class="source-code">1.114.0 (build 7e41b6b)</p>
			<p>Let's create <a id="_idIndexMarker1401"/>a CDK application and deploy an endpoint.</p>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor271"/>Creating a CDK application</h2>
			<p>We'll deploy<a id="_idIndexMarker1402"/> the same model that we deployed with CloudFormation. I'll use Python, and you could also use <strong class="bold">JavaScript</strong>, <strong class="bold">TypeScript</strong>, <strong class="bold">Java</strong>, and .<strong class="bold">NET</strong>. API documentation is available at <a href="https://docs.aws.amazon.com/cdk/api/latest/python/">https://docs.aws.amazon.com/cdk/api/latest/python/</a>: </p>
			<ol>
				<li value="1">First, we create a Python application named <strong class="source-inline">endpoint</strong>:<p class="source-code"><strong class="bold">$ mkdir cdk</strong></p><p class="source-code"><strong class="bold">$ cd cdk</strong></p><p class="source-code"><strong class="bold">$ cdk init --language python --app endpoint</strong></p></li>
				<li>This automatically creates a virtual environment, which we need to activate:<p class="source-code"><strong class="bold">$ source .venv/bin/activate</strong></p></li>
				<li>This also creates a default <strong class="source-inline">app.py</strong> file for our CDK code, a <strong class="source-inline">cdk.json</strong> file for application configuration, and a <strong class="source-inline">requirements.txt</strong> file to install dependencies. Instead, we'll use the files present in the GitHub repository:</li>
				<li>In the <strong class="source-inline">requirements.txt</strong> file, we install the CDK package for S3 and SageMaker. Each service requires a different package. For example, we would add <strong class="source-inline">aws_cdk.aws_s3</strong> for S3:<p class="source-code">-e .</p><p class="source-code">aws_cdk.aws_s3</p><p class="source-code">aws_cdk.aws_sagemaker</p></li>
				<li>We then install requirements as usual:<p class="source-code"><strong class="bold">$ pip install -r requirements.txt</strong></p></li>
				<li>In the <strong class="source-inline">cdk.json</strong> file, we store the application context. Namely, key-value pairs that <a id="_idIndexMarker1403"/>can be read by the application for configuration (<a href="https://docs.aws.amazon.com/cdk/latest/guide/context.html">https://docs.aws.amazon.com/cdk/latest/guide/context.html</a>):<p class="source-code">{</p><p class="source-code">  "app": "python3 app.py",</p><p class="source-code">  "context": {</p><p class="source-code">    "role_arn": "arn:aws:iam::123456789012:role/Sagemaker-fullaccess"</p><p class="source-code">    "model_name": "tf2-fmnist",</p><p class="source-code">    "epc_name": "tf2-fmnist-epc",</p><p class="source-code">    "ep_name": "tf2-fmnist-ep",</p><p class="source-code">    "image": "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1-cpu",</p><p class="source-code">    "model_data_url": "s3://sagemaker-us-east-1-123456789012/keras2-fashion-mnist/output/tensorflow-training-2020-06-08-07-46-04-367/output/model.tar.gz"</p><p class="source-code">    "instance_type": "ml.t2.xlarge",</p><p class="source-code">    "instance_count": 1</p><p class="source-code">  }</p><p class="source-code">}</p><p>This is the preferred way to pass values to your application. You should manage this file with version control in order to keep track of how stacks were built.</p></li>
				<li>We can view the context of our application with the <strong class="source-inline">cdk context</strong> command:</li>
			</ol>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="Images/B17705_12_10.jpg" alt="Figure 12.10 – Viewing CDK context&#13;&#10;" width="1650" height="572"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.10 – Viewing CDK context</p>
			<p>Now, we need <a id="_idIndexMarker1404"/>to write the actual application.</p>
			<h2 id="_idParaDest-273"><a id="_idTextAnchor272"/>Writing a CDK application</h2>
			<p>All code goes<a id="_idIndexMarker1405"/> in the <strong class="source-inline">app.py</strong> file, which we implement in the following steps:</p>
			<ol>
				<li value="1">We import the required packages:<p class="source-code">import time</p><p class="source-code">from aws_cdk import (</p><p class="source-code">    aws_sagemaker as sagemaker,</p><p class="source-code">    core</p><p class="source-code">)</p></li>
				<li>We extend the <strong class="source-inline">core.Stack</strong> class to create our own stack:<p class="source-code">class SagemakerEndpoint(core.Stack):</p><p class="source-code"> def __init__(self, app: core.App, id: str, **kwargs) -&gt; None:</p><p class="source-code">     timestamp = </p><p class="source-code">         '-'+time.strftime(</p><p class="source-code">                 "%Y-%m-%d-%H-%M-%S",time.gmtime())</p><p class="source-code">     super().__init__(app, id, **kwargs)</p></li>
				<li>We add <a id="_idIndexMarker1406"/>a <strong class="source-inline">CfnModel</strong> object, reading the appropriate context values:<p class="source-code">     model = sagemaker.CfnModel(</p><p class="source-code">         scope = self,</p><p class="source-code">         id="my_model",</p><p class="source-code">         execution_role_arn= </p><p class="source-code">             self.node.try_get_context("role_arn"),</p><p class="source-code">         containers=[{ </p><p class="source-code">           "image": </p><p class="source-code">             self.node.try_get_context("image"),</p><p class="source-code">           "modelDataUrl":                  </p><p class="source-code">             self.node.try_get_context("model_data_url")</p><p class="source-code">         }],           </p><p class="source-code">         model_name= self.node.try_get_context(</p><p class="source-code">                     "model_name")+timestamp</p><p class="source-code">     )</p></li>
				<li>We add a <strong class="source-inline">CfnEndpointConfig</strong> object, using the built-in <strong class="source-inline">get_att()</strong> function to associate it to the model. This creates a dependency that CloudFormation will use to build resources in the right order:<p class="source-code">     epc = sagemaker.CfnEndpointConfig(</p><p class="source-code">          scope=self,</p><p class="source-code">          id="my_epc",</p><p class="source-code">          production_variants=[{</p><p class="source-code">              "modelName": core.Fn.get_att(</p><p class="source-code">                               model.logical_id, </p><p class="source-code">                               'ModelName'</p><p class="source-code">                           ).to_string(),</p><p class="source-code">              "variantName": "variant-1",</p><p class="source-code">              "initialVariantWeight": 1.0,</p><p class="source-code">              "initialInstanceCount": 1,</p><p class="source-code">              "instanceType": </p><p class="source-code">                  self.node.try_get_context(</p><p class="source-code">                  "instance_type")</p><p class="source-code">          }],</p><p class="source-code">          endpoint_config_name=                   </p><p class="source-code">                  self.node.try_get_context("epc_name")</p><p class="source-code">                  +timestamp</p><p class="source-code">    )</p></li>
				<li>We <a id="_idIndexMarker1407"/>add a <strong class="source-inline">CfnEndpoint</strong> object, using the built-in <strong class="source-inline">get_att()</strong> function to associate it to the endpoint configuration:<p class="source-code">     ep = sagemaker.CfnEndpoint(</p><p class="source-code">         scope=self,</p><p class="source-code">         id="my_ep",</p><p class="source-code">         endpoint_config_name=</p><p class="source-code">             core.Fn.get_att(</p><p class="source-code">                 epc.logical_id,</p><p class="source-code">                 'EndpointConfigName'</p><p class="source-code">             ).to_string(),</p><p class="source-code">         endpoint_name=</p><p class="source-code">             self.node.try_get_context("ep_name")</p><p class="source-code">             +timestamp</p><p class="source-code">     )</p></li>
				<li>Finally, we <a id="_idIndexMarker1408"/>instantiate the application:<p class="source-code">app = core.App()</p><p class="source-code">SagemakerEndpoint(</p><p class="source-code">    app, </p><p class="source-code">    "SagemakerEndpoint", </p><p class="source-code">    env={'region': 'eu-west-1'}</p><p class="source-code">)</p><p class="source-code">app.synth()</p></li>
			</ol>
			<p>Our code is complete!</p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor273"/>Deploying a CDK application</h2>
			<p>We can<a id="_idIndexMarker1409"/> now deploy the endpoint:</p>
			<ol>
				<li value="1">We can list the available stacks:<p class="source-code"><strong class="bold">$ cdk list</strong></p><p class="source-code"><strong class="bold">SagemakerEndpointEU</strong></p></li>
				<li>We can also see the actual CloudFormation template. It should be extremely similar to the template we wrote in the previous section:<p class="source-code"><strong class="bold">$ cdk synth SagemakerEndpointEU</strong></p></li>
				<li>Deploying the stack is equally simple, as shown in the next screenshot:<div id="_idContainer184" class="IMG---Figure"><img src="Images/B17705_12_11.jpg" alt="Figure 12.11 – Deploying an endpoint&#13;&#10;" width="610" height="119"/></div><p class="figure-caption">Figure 12.11 – Deploying an endpoint</p></li>
				<li>Looking at CloudFormation, we see that the stack is created using a change set. A few minutes later, the endpoint is in service.</li>
				<li>Editing <strong class="source-inline">app.py</strong>, we<a id="_idIndexMarker1410"/> set the initial instance count to <strong class="source-inline">2</strong>. We then ask CDK to deploy the stack, but without executing the change set, as shown in the next screenshot:<div id="_idContainer185" class="IMG---Figure"><img src="Images/B17705_12_12.jpg" alt="Figure 12.12 – Creating a change set&#13;&#10;" width="789" height="122"/></div><p class="figure-caption">Figure 12.12 – Creating a change set</p></li>
				<li>If we're happy with the change set, we can execute it in the CloudFormation console, or run the previous command again without <strong class="source-inline">--no-execute</strong>. As expected, and as shown in the next screenshot, the endpoint is updated:<div id="_idContainer186" class="IMG---Figure"><img src="Images/B17705_12_13.jpg" alt="Figure 12.13 – Updating the endpoint&#13;&#10;" width="608" height="119"/></div><p class="figure-caption">Figure 12.13 – Updating the endpoint</p></li>
				<li>When we're done, we can destroy the stack:<p class="source-code"><strong class="bold">$ cdk destroy SagemakerEndpointEU</strong></p></li>
			</ol>
			<p>As you can see, the CDK is an interesting alternative to writing templates directly, while still benefiting<a id="_idIndexMarker1411"/> from the rigor and the robustness of CloudFormation.</p>
			<p>One thing we haven't done yet is to automate an end-to-end workflow, from training to deployment. Let's do this with AWS Step Functions.</p>
			<h1 id="_idParaDest-275"><a id="_idTextAnchor274"/>Building end-to-end workflows with AWS Step Functions</h1>
			<p><strong class="bold">AWS Step Functions</strong> let you define and run <a id="_idIndexMarker1412"/>workflows based<a id="_idIndexMarker1413"/> on <strong class="bold">state machines </strong>(<a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a>). A<a id="_idIndexMarker1414"/> state machine is a combination of steps, which can be sequential, parallel, or conditional. Each step <a id="_idIndexMarker1415"/>receives an input from its predecessor, performs an operation, and passes the output to its successor. Step Functions are integrated with many AWS services, such as Amazon SageMaker, <strong class="bold">AWS</strong> <strong class="bold">Lambda</strong>, container services, <strong class="bold">Amazon</strong> <strong class="bold">DynamoDB</strong>, <strong class="bold">Amazon</strong> <strong class="bold">EMR</strong>, <strong class="bold">AWS</strong> <strong class="bold">Glue</strong>, and more.</p>
			<p>State machines can be defined using JSON and the <strong class="bold">Amazon States Language</strong>, and you can visualize them in the service console. State machine execution is fully managed, so you don't need to provision any infrastructure to run.</p>
			<p>When it comes to<a id="_idIndexMarker1416"/> SageMaker, Step Functions has a dedicated Python SDK, oddly named the <strong class="bold">Data Science SDK</strong> (<a href="https://github.com/aws/aws-step-functions-data-science-sdk-python">https://github.com/aws/aws-step-functions-data-science-sdk-python</a>).</p>
			<p>Let's run an example where we automate training<a id="_idIndexMarker1417"/> and deployment <a id="_idIndexMarker1418"/>for a <strong class="bold">scikit-learn</strong> model <a id="_idIndexMarker1419"/>trained <a id="_idIndexMarker1420"/>on the <strong class="bold">Boston Housing</strong> dataset.</p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor275"/>Setting up permissions</h2>
			<p>First, please<a id="_idIndexMarker1421"/> make sure that the IAM role for your user or for your notebook instance has permission to invoke Step Functions APIs. If not, please add the <strong class="source-inline">AWSStepFunctionsFullAccess</strong> managed policy to the role.</p>
			<p>Then, we need to create a service role for Step Functions, allowing it to invoke AWS APIs on our behalf:</p>
			<ol>
				<li value="1">Starting from the IAM console (<a href="https://console.aws.amazon.com/iam/home#/roles">https://console.aws.amazon.com/iam/home#/roles</a>), we click on <strong class="bold">Create role</strong>.</li>
				<li>We select <strong class="bold">AWS service</strong> and <strong class="bold">Step Functions</strong>. </li>
				<li>We click through the next screens until we can enter the role name. Let's call it <strong class="source-inline">StepFunctionsWorkflowExecutionRole</strong>, and click on <strong class="bold">Create role</strong>.</li>
				<li>Selecting this role, we click on its <strong class="bold">Permission</strong> tab, then on <strong class="bold">Add inline policy</strong>.</li>
				<li>Selecting the JSON tab, we replace the empty policy with the content of the <strong class="source-inline">Chapter12/step_functions/service-role-policy.json</strong> file, and we click on <strong class="bold">Review policy</strong>.</li>
				<li>We name the policy <strong class="source-inline">StepFunctionsWorkflowExecutionPolicy</strong> and click on <strong class="bold">Create policy</strong>. </li>
				<li>We write down the ARN on the role, and we close the IAM console.</li>
			</ol>
			<p>The setup<a id="_idIndexMarker1422"/> is now complete. Now, let's create a workflow.</p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor276"/>Implementing our first workflow</h2>
			<p>In this<a id="_idIndexMarker1423"/> workflow, we'll go through the following step sequence: train the model, create it, use it for a batch transform, create an endpoint configuration, and deploy the model to an endpoint:</p>
			<ol>
				<li value="1">We upload the training set to S3, as well as a test set where we removed the target attribute. We'll use the latter for a batch transform:<p class="source-code">import sagemaker</p><p class="source-code">import pandas as pd</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()   </p><p class="source-code">prefix = 'sklearn-boston-housing-stepfunc'</p><p class="source-code">training_data = sess.upload_data(</p><p class="source-code">    path='housing.csv', </p><p class="source-code">    key_prefix=prefix + "/training")</p><p class="source-code">data = pd.read_csv('housing.csv')</p><p class="source-code">data.drop(['medv'], axis=1, inplace=True)</p><p class="source-code">data.to_csv('test.csv', index=False, header=False)</p><p class="source-code">batch_data = sess.upload_data(</p><p class="source-code">    path='test.csv', </p><p class="source-code">    key_prefix=prefix + "/batch")</p></li>
				<li>We configure our estimator as usual:<p class="source-code">from sagemaker.sklearn import SKLearn</p><p class="source-code">output = 's3://{}/{}/output/'.format(bucket,prefix)</p><p class="source-code">sk = SKLearn(</p><p class="source-code">    entry_point='sklearn-boston-housing.py',</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    framework_version='0.23-1',</p><p class="source-code">    train_instance_count=1,</p><p class="source-code">    train_instance_type='ml.m5.large',</p><p class="source-code">    output_path=output,</p><p class="source-code">    hyperparameters={</p><p class="source-code">        'normalize': True,</p><p class="source-code">        'test-size': 0.1</p><p class="source-code">    }</p><p class="source-code">)</p></li>
				<li>We<a id="_idIndexMarker1424"/> also define the transformer that we'll use for batch transform:<p class="source-code">sk_transformer = sk.transformer(</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large')</p></li>
				<li>We import the Step Functions objects required by the workflow. You can find the API documentation at <a href="https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/">https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/</a>:<p class="source-code">import stepfunctions</p><p class="source-code">from stepfunctions import steps</p><p class="source-code">from stepfunctions.steps import TrainingStep, ModelStep, TransformStep</p><p class="source-code">from stepfunctions.inputs import ExecutionInput</p><p class="source-code">from stepfunctions.workflow import Workflow</p></li>
				<li>We define the input of the workflow. We'll pass it a training job name, a model name, and an endpoint name:<p class="source-code">execution_input = ExecutionInput(schema={</p><p class="source-code">    'JobName': str,</p><p class="source-code">    'ModelName': str,</p><p class="source-code">    'EndpointName': str}</p><p class="source-code">)</p></li>
				<li>The first step <a id="_idIndexMarker1425"/>of the workflow is the training step. We pass it the estimator, the location of the dataset in S3, and a training job name:<p class="source-code">from sagemaker.inputs import TrainingInput</p><p class="source-code">training_step = TrainingStep(</p><p class="source-code">  'Train Scikit-Learn on the Boston Housing dataset',</p><p class="source-code">  estimator=sk,</p><p class="source-code">  data={'training': TrainingInput(</p><p class="source-code">       training_data,content_type='text/csv')},</p><p class="source-code">  job_name=execution_input['JobName']</p><p class="source-code">)</p></li>
				<li>The next step is the model creation step. We pass it the location of the model trained in the previous step, and a model name:<p class="source-code">model_step = ModelStep(</p><p class="source-code">    'Create the model in SageMaker',</p><p class="source-code">    model=training_step.get_expected_model(),</p><p class="source-code">    model_name=execution_input['ModelName']</p><p class="source-code">)</p></li>
				<li>The next step is running a batch transform on the test dataset. We pass the <strong class="source-inline">transformer</strong> <a id="_idIndexMarker1426"/>object, the location of the test dataset in S3, and its content type:<p class="source-code">transform_step = TransformStep(</p><p class="source-code">    'Transform the dataset in batch mode',</p><p class="source-code">    transformer=sk_transformer,</p><p class="source-code">    job_name=execution_input['JobName'],    </p><p class="source-code">    model_name=execution_input['ModelName'],</p><p class="source-code">    data=batch_data,</p><p class="source-code">    content_type='text/csv'</p><p class="source-code">)</p></li>
				<li>The next step is creating the endpoint configuration:<p class="source-code">endpoint_config_step = EndpointConfigStep(</p><p class="source-code">    "Create an endpoint configuration for the model",</p><p class="source-code">    endpoint_config_name=execution_input['ModelName'],</p><p class="source-code">    model_name=execution_input['ModelName'],</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large'</p><p class="source-code">)</p></li>
				<li>The last step is creating the endpoint:<p class="source-code">endpoint_step = EndpointStep(</p><p class="source-code">    "Create an endpoint hosting the model",</p><p class="source-code">    endpoint_name=execution_input['EndpointName'],</p><p class="source-code">    endpoint_config_name=execution_input['ModelName']</p><p class="source-code">)</p></li>
				<li>Now that all steps have been defined, we <a id="_idIndexMarker1427"/>chain them in sequential order:<p class="source-code">workflow_definition = Chain([</p><p class="source-code">    training_step,</p><p class="source-code">    model_step,</p><p class="source-code">    transform_step,</p><p class="source-code">    endpoint_config_step,</p><p class="source-code">    endpoint_step</p><p class="source-code">])</p></li>
				<li>We now build our workflow, using the workflow definition and the input definition:<p class="source-code">import time</p><p class="source-code">timestamp = time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">workflow_execution_role = "arn:aws:iam::0123456789012:role/</p><p class="source-code">StepFunctionsWorkflowExecutionRole"</p><p class="source-code">workflow = Workflow(</p><p class="source-code">    name='sklearn-boston-housing-workflow1-{}'</p><p class="source-code">         .format(timestamp),</p><p class="source-code">    definition=workflow_definition,</p><p class="source-code">    role=workflow_execution_role,</p><p class="source-code">    execution_input=execution_input</p><p class="source-code">)</p></li>
				<li>We can visualize the state machine, an easy way to check that we built it as expected, as shown <a id="_idIndexMarker1428"/>in the next screenshot:<p class="source-code">workflow.render_graph(portrait=True)</p><div id="_idContainer187" class="IMG---Figure"><img src="Images/B17705_12_14.jpg" alt="Figure 12.14 – Viewing the state machine&#13;&#10;" width="475" height="596"/></div><p class="figure-caption">Figure 12.14 – Viewing the state machine</p></li>
				<li>­­We create the workflow:<p class="source-code">workflow.create()</p></li>
				<li>It's visible in the Step Functions console, as shown in the following screenshot. We can see both its graphical representation and its JSON definition based on the Amazon States Language. We could edit the workflow as well if needed:<div id="_idContainer188" class="IMG---Figure"><img src="Images/B17705_12_15.jpg" alt="Figure 12.15 – Viewing the state machine in the console&#13;&#10;" width="1365" height="649"/></div><p class="figure-caption">Figure 12.15 – Viewing the state machine in the console</p></li>
				<li>We <a id="_idIndexMarker1429"/>run the workflow:<p class="source-code">execution = workflow.execute(</p><p class="source-code"> inputs={</p><p class="source-code">   'JobName': 'sklearn-boston-housing-{}'</p><p class="source-code">              .format(timestamp),</p><p class="source-code">   'ModelName': 'sklearn-boston-housing-{}'</p><p class="source-code">                .format(timestamp),</p><p class="source-code">   'EndpointName': 'sklearn-boston-housing-{}'</p><p class="source-code">                   .format(timestamp)</p><p class="source-code"> }</p><p class="source-code">)</p></li>
				<li>We can track its progress with <strong class="source-inline">render_progress()</strong> and the <strong class="source-inline">list_events()</strong> API. We can also see it in the console, as shown in the next screenshot. Note that we also see the input and output of each step, which is a great way to troubleshoot problems:<div id="_idContainer189" class="IMG---Figure"><img src="Images/B17705_12_16.jpg" alt="Figure 12.16 – Running the state machine&#13;&#10;" width="1265" height="564"/></div><p class="figure-caption">Figure 12.16 – Running the state machine</p></li>
				<li>When the workflow is complete, you can test the endpoint as usual. <em class="italic">Don't forget to delete it in the SageMaker console when you're done</em>.</li>
			</ol>
			<p>This example <a id="_idIndexMarker1430"/>shows how simple it is to build a SageMaker workflow with this SDK. Still, we could improve it by running batch transform and endpoint creation in parallel. </p>
			<h2 id="_idParaDest-278"><a id="_idTextAnchor277"/>Adding parallel execution to a workflow</h2>
			<p>The next<a id="_idIndexMarker1431"/> screenshot shows the workflow we're going to build. The steps themselves are exactly the same. We're only going to modify the way they're chained:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="Images/B17705_12_17.jpg" alt="Figure 12.17 – Viewing the parallel state machine&#13;&#10;" width="1060" height="755"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.17 – Viewing the parallel state machine</p>
			<p>We will get started using the following steps:</p>
			<ol>
				<li value="1">Our <a id="_idIndexMarker1432"/>workflow has two branches – one for batch transform and one for the endpoint:<p class="source-code">batch_branch = Chain([</p><p class="source-code">  transform_step</p><p class="source-code">])</p><p class="source-code">endpoint_branch = Chain([</p><p class="source-code">  endpoint_config_step,</p><p class="source-code">  endpoint_step</p><p class="source-code">]) </p></li>
				<li>We create a <strong class="source-inline">Parallel</strong> step in order to allow parallel execution of these two branches:<p class="source-code">parallel_step = Parallel('Parallel execution')</p><p class="source-code">parallel_step.add_branch(batch_branch)</p><p class="source-code">parallel_step.add_branch(endpoint_branch)</p></li>
				<li>We put everything together:<p class="source-code">workflow_definition = Chain([</p><p class="source-code">    training_step,</p><p class="source-code">    model_step,</p><p class="source-code">    parallel_step</p><p class="source-code">])</p></li>
			</ol>
			<p>That's it! We <a id="_idIndexMarker1433"/>can now create and run this workflow just like in the previous example.</p>
			<p>Looking at the Step Functions console, we see that the workflow does run the two branches in parallel. There is a minor problem, however. The endpoint creation step is shown as complete, although the endpoint is still being created. You can see in the SageMaker console that the endpoint is listed as <strong class="source-inline">Creating</strong>. This could cause a problem if a client application tried to invoke the endpoint right after the workflow completes.</p>
			<p>Let's improve this by adding an extra step, waiting for the endpoint to be in service. We can easily do this with a Lambda function, allowing us to run our own code anywhere in a workflow.</p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor278"/>Adding a Lambda function to a workflow</h2>
			<p>If you've<a id="_idIndexMarker1434"/> never<a id="_idIndexMarker1435"/> looked at <strong class="bold">AWS</strong> <strong class="bold">Lambda</strong> (<a href="https://aws.amazon.com/lambda">https://aws.amazon.com/lambda</a>), you're missing out! Lambda is at the core of serverless architectures, where you can write and deploy short functions running on fully managed infrastructure. These functions can be triggered by all sorts of AWS events, and they can also be invoked on demand.</p>
			<h3>Setting up permissions</h3>
			<p>Creating a Lambda<a id="_idIndexMarker1436"/> function is simple. The only prerequisite is to <a id="_idIndexMarker1437"/>create an <strong class="bold">execution role</strong> – that is, an IAM role that gives the function permission to invoke other AWS services. Here, we only need permission for the <strong class="source-inline">DescribeEndpoint</strong> API, as well as permission to create a log in CloudWatch. Let's use the <strong class="source-inline">boto3</strong> API for this. You can find more information at <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html">https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html</a>:</p>
			<ol>
				<li value="1">We first define a <strong class="bold">trust policy</strong> for the role, allowing<a id="_idIndexMarker1438"/> it to be assumed by the Lambda service:<p class="source-code">{</p><p class="source-code">  "Version": "2012-10-17",</p><p class="source-code">  "Statement": [{</p><p class="source-code">    "Effect": "Allow",</p><p class="source-code">    "Principal": {</p><p class="source-code">      "Service": "lambda.amazonaws.com"</p><p class="source-code">    },</p><p class="source-code">    "Action": "sts:AssumeRole"</p><p class="source-code">  }]</p><p class="source-code">}</p></li>
				<li>We create a role and <a id="_idIndexMarker1439"/>attach the trust policy to it:<p class="source-code">iam = boto3.client('iam')</p><p class="source-code">with open('trust-policy.json') as f:</p><p class="source-code">    policy = f.read()</p><p class="source-code">    role_name = 'lambda-role-sagemaker-describe-endpoint'</p><p class="source-code">response = iam.create_role(</p><p class="source-code">    RoleName=role_name,</p><p class="source-code">    AssumeRolePolicyDocument=policy,</p><p class="source-code">    Description='Allow function to invoke all SageMaker APIs'</p><p class="source-code">)</p><p class="source-code">role_arn = response['Role']['Arn']</p></li>
				<li>We define<a id="_idIndexMarker1440"/> a policy listing the APIs that are allowed:<p class="source-code">{</p><p class="source-code">  "Version": "2012-10-17",</p><p class="source-code">  "Statement": [</p><p class="source-code">    {</p><p class="source-code">      "Effect": "Allow",</p><p class="source-code">      "Action": "sagemaker:DescribeEndpoint",</p><p class="source-code">      "Resource": "*"</p><p class="source-code">    },</p><p class="source-code">    {</p><p class="source-code">      "Effect": "Allow",</p><p class="source-code">      "Action": [</p><p class="source-code">          "logs:CreateLogGroup",</p><p class="source-code">          "logs:CreateLogStream",</p><p class="source-code">          "logs:PutLogEvents"</p><p class="source-code">      ],</p><p class="source-code">      "Resource": "*"</p><p class="source-code">     }</p><p class="source-code">  ]</p><p class="source-code">}</p></li>
				<li>We create the policy and add it to the role:<p class="source-code">with open('policy.json') as f:</p><p class="source-code">    policy = f.read()</p><p class="source-code">policy_name = 'Sagemaker-describe-endpoint'</p><p class="source-code">response = iam.create_policy(</p><p class="source-code">    PolicyName=policy_name,</p><p class="source-code">    PolicyDocument=policy,</p><p class="source-code">    Description='Allow the DescribeEndpoint API'</p><p class="source-code">)</p><p class="source-code">policy_arn = response['Policy']['Arn']</p><p class="source-code">response = iam.attach_role_policy(</p><p class="source-code">    RoleName=role_name,</p><p class="source-code">    PolicyArn=policy_arn</p><p class="source-code">)</p></li>
			</ol>
			<p>The IAM setup is<a id="_idIndexMarker1441"/> now complete. </p>
			<h3>Writing a Lambda function</h3>
			<p>We can now write a<a id="_idIndexMarker1442"/> short Lambda function. It receives a JSON event as input, which stores the ARN of the endpoint being created by the <strong class="source-inline">EndpointStep</strong> step. It simply extracts the endpoint name from the ARN, creates a <strong class="source-inline">boto3</strong> waiter, and waits until the endpoint is in service. The following screenshot shows the code in the Lambda console:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="Images/B17705_12_18.jpg" alt="Figure 12.18 – Our Lambda function&#13;&#10;" width="1649" height="885"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.18 – Our Lambda function</p>
			<p>Let's deploy this function:</p>
			<ol>
				<li value="1">We create a <a id="_idIndexMarker1443"/>deployment package for the Lambda function and upload it to S3:<p class="source-code"><strong class="bold">$ zip -9 lambda.zip lambda.py</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp lambda.zip s3://my-bucket</strong></p></li>
				<li>We create the function with a timeout of 15 minutes, the longest possible runtime for a Lambda function. Endpoints are typically deployed in less than 10 minutes, so this should be more than enough:<p class="source-code">lambda_client = boto3.client('lambda')</p><p class="source-code">response = lambda_client.create_function(</p><p class="source-code">    FunctionName='sagemaker-wait-for-endpoint',</p><p class="source-code">    Role=role_arn,</p><p class="source-code">    Runtime='python3.6',</p><p class="source-code">    Handler='lambda.lambda_handler',</p><p class="source-code">    Code={</p><p class="source-code">        'S3Bucket': bucket_name,</p><p class="source-code">        'S3Key': 'lambda.zip'</p><p class="source-code">    },</p><p class="source-code">    Description='Wait for endpoint to be in service',</p><p class="source-code">    Timeout=900,</p><p class="source-code">    MemorySize=128</p><p class="source-code">)</p></li>
				<li>Now that the Lambda<a id="_idIndexMarker1444"/> function has been created, we can easily add it to the existing workflow. We define a <strong class="source-inline">LambdaStep</strong> and add it to the endpoint branch. Its payload is the endpoint ARN, extracted from the output of the <strong class="source-inline">EndpointStep</strong>:<p class="source-code">lambda_step = LambdaStep(</p><p class="source-code">    'Wait for endpoint to be in service',</p><p class="source-code">    parameters={</p><p class="source-code">        'FunctionName': 'sagemaker-wait-for-endpoint',</p><p class="source-code">        'Payload': {"EndpointArn.$": "$.EndpointArn"}</p><p class="source-code">    },</p><p class="source-code">    timeout_seconds=900</p><p class="source-code">)</p><p class="source-code">endpoint_branch = steps.Chain([</p><p class="source-code">    endpoint_config_step,</p><p class="source-code">    endpoint_step,</p><p class="source-code">    lambda_step</p><p class="source-code">])</p></li>
				<li>Running the workflow again, we see in the following screenshot that this new step receives the endpoint ARN as input and waits for the endpoint to be in service:</li>
			</ol>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="Images/B17705_12_19.jpg" alt="Figure 12.19 – Running the state machine with Lambda&#13;&#10;" width="535" height="457"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.19 – Running the state machine with Lambda</p>
			<p>There are many other <a id="_idIndexMarker1445"/>ways you can use Lambda functions with SageMaker. You can extract training metrics, predict test sets on an endpoint, and more. The possibilities are endless.</p>
			<p>Now, let's automate end-to-end workflows with Amazon SageMaker Pipelines.</p>
			<h1 id="_idParaDest-280"><a id="_idTextAnchor279"/>Building end-to-end workflows with Amazon SageMaker Pipelines</h1>
			<p><strong class="bold">Amazon SageMaker Pipelines</strong> lets <a id="_idIndexMarker1446"/>us create <a id="_idIndexMarker1447"/>and run end-to-end machine learning <strong class="bold">workflows</strong> based on SageMaker steps for training, tuning, batch transform, and processing scripts, using SageMaker APIs SDK that are very similar to the ones we used in Step Functions.</p>
			<p>Compared to Step Functions, SageMaker Pipelines adds the following features:</p>
			<ul>
				<li>The ability to write, run, visualize and manage your workflows directly in SageMaker Studio, without having to jump to the AWS console. </li>
				<li>A <strong class="bold">model registry</strong>, which<a id="_idIndexMarker1448"/> makes it easier to manage model versions, deploy only approved versions, and track <strong class="bold">lineage</strong>. </li>
				<li><strong class="bold">MLOps templates</strong> – a collection of CloudFormation templates published via <strong class="bold">AWS Service Catalog</strong> that <a id="_idIndexMarker1449"/>help you automate the deployment of your models. Built-in templates are provided, and you can add your own. You (or your<a id="_idIndexMarker1450"/> Ops team) can <a id="_idIndexMarker1451"/>learn more at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html</a>.<p class="callout-heading">Note</p><p class="callout">One thing that SageMaker Pipelines lacks is integration with other AWS services. At the time of writing, SageMaker Pipelines only supports <strong class="bold">SQS</strong>, whereas Step Functions supports many compute and big data services. With SageMaker Pipelines, the assumption is either that your training data has already been processed, or that you'll process it with SageMaker Processing steps.</p></li>
			</ul>
			<p>Now that we know what SageMaker Pipelines is, let's run a complete example based on the Amazon Reviews dataset and the BlazingText algorithm we used in <a href="B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 6</em></a>, <em class="italic">Training Natural Language Processing Models</em>, and <a href="B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Training Techniques</em>, putting together many of the services we learned about so far. Our pipeline will contain the following steps:</p>
			<ul>
				<li>A processing step, where<a id="_idIndexMarker1452"/> we prepare the dataset with <strong class="bold">SageMaker Processing</strong>.</li>
				<li>An ingestion <a id="_idIndexMarker1453"/>step, where we load the processed data set in <strong class="bold">SageMaker Feature Store</strong>.</li>
				<li>A dataset building step, where we use <strong class="bold">Amazon Athena</strong> to query the offline store and save a dataset to S3.</li>
				<li>A training step, where we train a BlazingText model on the dataset.</li>
				<li>A model creation step, where we save the trained model as a SageMaker model.</li>
				<li>A model registration step, where we add the model to the SageMaker Pipelines model registry.</li>
			</ul>
			<p>In real life, you should not initially worry about automation. You should first experiment with Jupyter Notebooks and iterate on all these steps. Then, as the project matures, you should start automating each step, eventually assembling them as a pipeline.</p>
			<p>My recommendation is to first automate each processing step, with individual SageMaker Processing jobs. Not only will this come in handy in the development phase, but it will also create a simple and step-by-step path to full automation. Indeed, once steps run fine with SageMaker Processing, it takes little effort to combine them with SageMaker Pipelines. In fact, you can use the exact same Python script. You'll only have to write code with the Pipelines SDK. As you'll see in a minute, it's very similar to the Processing SDK.</p>
			<p>This is the approach I've followed with the following example. In the GitHub repository, you'll find <a id="_idIndexMarker1454"/>SageMaker <a id="_idIndexMarker1455"/>Processing notebooks for the data processing, ingestion, and dataset building steps, as well as another notebook for the end-to-end workflow. Here, we'll focus on the latter. Let's get started!</p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor280"/>Defining workflow parameters</h2>
			<p>Just like <a id="_idIndexMarker1456"/>CloudFormation templates, you can (and should) define parameters in your workflows. This makes them easier to reuse in other projects. Parameters can be strings, integers, and floats, with an optional default value.</p>
			<ol>
				<li value="1">We create parameters for the AWS region and for the instances we'd like to use for processing and training:<p class="source-code">from sagemaker.workflow.parameters import ParameterInteger, ParameterString</p><p class="source-code">region = ParameterString(</p><p class="source-code">    name='Region',</p><p class="source-code">    default_value='eu-west-1')</p><p class="source-code">processing_instance_type = ParameterString(</p><p class="source-code">    name='ProcessingInstanceType',</p><p class="source-code">    default_value='ml.m5.4xlarge')</p><p class="source-code">processing_instance_count = ParameterInteger(</p><p class="source-code">    name='ProcessingInstanceCount',</p><p class="source-code">    default_value=1)</p><p class="source-code">training_instance_type = ParameterString(</p><p class="source-code">    name='TrainingInstanceType',</p><p class="source-code">    default_value='ml.p3.2xlarge')</p><p class="source-code">training_instance_count = ParameterInteger(</p><p class="source-code">    name='TrainingInstanceCount',</p><p class="source-code">    default_value=1)</p></li>
				<li>We also create <a id="_idIndexMarker1457"/>parameters for the location of input data, the model name, and the model status to set in the model registry (more on this later).<p class="source-code">input_data = ParameterString(name='InputData')</p><p class="source-code">model_name = ParameterString(name='ModelName')</p><p class="source-code">model_approval_status = ParameterString(</p><p class="source-code">    name='ModelApprovalStatus',</p><p class="source-code">    default_value='PendingManualApproval')</p></li>
			</ol>
			<p>Now, let's define the data processing step.</p>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor281"/>Processing the dataset with SageMaker Processing</h2>
			<p>We reuse<a id="_idIndexMarker1458"/> the processing script we <a id="_idIndexMarker1459"/>wrote in <a href="B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 6</em></a> (<strong class="source-inline">preprocessing.py</strong>). </p>
			<ol>
				<li value="1">We create a <strong class="source-inline">SKLearnProcessor</strong> object with the parameters we just defined:<p class="source-code">from sagemaker.sklearn.processing import SKLearnProcessor</p><p class="source-code">sklearn_processor = SKLearnProcessor(</p><p class="source-code">    framework_version='0.23-1',</p><p class="source-code">    role=role,</p><p class="source-code">    instance_type=processing_instance_type,</p><p class="source-code">    instance_count=processing_instance_count)</p></li>
				<li>We then define the data processing step. Remember that it creates two outputs: one in BlazingText format, and one for ingestion in SageMaker Feature Store. As mentioned earlier, the SageMaker Pipelines syntax is extremely close to the SageMaker Processing syntax (inputs, outputs, and arguments).<p class="source-code">from sagemaker.workflow.steps import ProcessingStep</p><p class="source-code">from sagemaker.processing import ProcessingInput, ProcessingOutput</p><p class="source-code">step_process = ProcessingStep(</p><p class="source-code">    name='process-customer-reviews'</p><p class="source-code">    processor=sklearn_processor,</p><p class="source-code">    inputs=[</p><p class="source-code">        ProcessingInput(source=input_data, </p><p class="source-code">            destination="/opt/ml/processing/input")],</p><p class="source-code">    outputs=[</p><p class="source-code">        ProcessingOutput(output_name='bt_data',</p><p class="source-code">            source='/opt/ml/processing/output/bt'),</p><p class="source-code">        ProcessingOutput(output_name='fs_data',</p><p class="source-code">            source='/opt/ml/processing/output/fs')],</p><p class="source-code">    code='preprocessing.py',</p><p class="source-code">    job_arguments=[</p><p class="source-code">        '--filename', </p><p class="source-code">        'amazon_reviews_us_Camera_v1_00.tsv.gz',</p><p class="source-code">        '--library', </p><p class="source-code">        'spacy']</p><p class="source-code">)</p></li>
			</ol>
			<p>Now, let's <a id="_idIndexMarker1460"/>define<a id="_idIndexMarker1461"/> the ingestion step.</p>
			<h2 id="_idParaDest-283"><a id="_idTextAnchor282"/>Ingesting the dataset in SageMaker Feature Store with SageMaker Processing</h2>
			<p>We<a id="_idIndexMarker1462"/> reuse the <a id="_idIndexMarker1463"/>processing script we wrote in <a href="B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 10</em></a> (<strong class="source-inline">ingesting.py</strong>).</p>
			<ol>
				<li value="1">We first define a name for the feature group:<p class="source-code">feature_group_name = 'amazon-reviews-feature-group-' + strftime('%d-%H-%M-%S', gmtime())</p></li>
				<li>We then define a processing step, setting the data input to the output of the first processing job. To illustrate step chaining, we define an output pointing to a file saved by the script, which contains the name of the feature group:<p class="source-code">step_ingest = ProcessingStep(</p><p class="source-code">    name='ingest-customer-reviews',</p><p class="source-code">    processor=sklearn_processor,</p><p class="source-code">    inputs=[</p><p class="source-code">       ProcessingInput(</p><p class="source-code">       source=</p><p class="source-code">        step_process.properties.ProcessingOutputConfig</p><p class="source-code">        .Outputs['fs_data'].S3Output.S3Uri,</p><p class="source-code">       destination="/opt/ml/processing/input")],</p><p class="source-code">    outputs = [</p><p class="source-code">       ProcessingOutput(</p><p class="source-code">       output_name='feature_group_name',</p><p class="source-code">       source='/opt/ml/processing/output/')],</p><p class="source-code">    code='ingesting.py',</p><p class="source-code">    job_arguments=[</p><p class="source-code">       '--region', region,</p><p class="source-code">       '--bucket', bucket,</p><p class="source-code">       '--role', role,</p><p class="source-code">       '--feature-group-name', feature_group_name,</p><p class="source-code">       '--max-workers', '32']</p><p class="source-code">)</p></li>
			</ol>
			<p>Now, let's <a id="_idIndexMarker1464"/>take care<a id="_idIndexMarker1465"/> of the dataset building step.</p>
			<h2 id="_idParaDest-284"><a id="_idTextAnchor283"/>Building a dataset with Amazon Athena and SageMaker Processing</h2>
			<p>We<a id="_idIndexMarker1466"/> reuse <a id="_idIndexMarker1467"/>the <a id="_idIndexMarker1468"/>processing script <a id="_idIndexMarker1469"/>we wrote in <a href="B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 10</em></a> (<strong class="source-inline">querying.py</strong>). </p>
			<p>We set the input to the output of the ingestion step, in order to retrieve the name of the <a id="_idIndexMarker1470"/>feature<a id="_idIndexMarker1471"/> group. We <a id="_idIndexMarker1472"/>also define two <a id="_idIndexMarker1473"/>outputs for the training and validation datasets:</p>
			<p class="source-code">step_build_dataset = ProcessingStep(</p>
			<p class="source-code">    name='build-dataset',</p>
			<p class="source-code">    processor=sklearn_processor,</p>
			<p class="source-code">    inputs=[</p>
			<p class="source-code">      ProcessingInput(</p>
			<p class="source-code">        source=</p>
			<p class="source-code">          step_ingest.properties.ProcessingOutputConfig</p>
			<p class="source-code">          .Outputs['feature_group_name'].S3Output.S3Uri,</p>
			<p class="source-code">        destination='/opt/ml/processing/input')],</p>
			<p class="source-code">    outputs=[</p>
			<p class="source-code">      ProcessingOutput(</p>
			<p class="source-code">        output_name='training',</p>
			<p class="source-code">        source='/opt/ml/processing/output/training'),</p>
			<p class="source-code">      ProcessingOutput(</p>
			<p class="source-code">        output_name='validation',               </p>
			<p class="source-code">        source='/opt/ml/processing/output/validation')],</p>
			<p class="source-code">      code='querying.py',</p>
			<p class="source-code">      job_arguments=[</p>
			<p class="source-code">        '--region', region,</p>
			<p class="source-code">        '--bucket', bucket,]</p>
			<p class="source-code">)</p>
			<p>Now, let's move on to the training step.</p>
			<h2 id="_idParaDest-285"><a id="_idTextAnchor284"/>Training a model</h2>
			<p>No<a id="_idIndexMarker1474"/> surprises here:</p>
			<ol>
				<li value="1">We define an <strong class="source-inline">Estimator</strong> module for this job:<p class="source-code">container = image_uris.retrieve(</p><p class="source-code">    'blazingtext', </p><p class="source-code">    str(region))     # region is a ParameterString</p><p class="source-code">prefix = 'blazing-text-amazon-reviews'</p><p class="source-code">s3_output = 's3://{}/{}/output/'.format(bucket, prefix)</p><p class="source-code">bt = Estimator(container,</p><p class="source-code">               role,</p><p class="source-code">               instance_count=training_instance_count, </p><p class="source-code">               instance_type=training_instance_type,</p><p class="source-code">               output_path=s3_output)</p><p class="source-code">bt.set_hyperparameters(mode='supervised')</p></li>
				<li>We then define the training step, passing the training and validation datasets as inputs:<p class="source-code">from sagemaker.workflow.steps import TrainingStep</p><p class="source-code">from sagemaker.inputs import TrainingInput</p><p class="source-code">step_train = TrainingStep(</p><p class="source-code">    name='train-blazing-text',</p><p class="source-code">    estimator=bt,</p><p class="source-code">    inputs={</p><p class="source-code">      'train': TrainingInput(s3_data=</p><p class="source-code">step_build_dataset.properties.ProcessingOutputConfig</p><p class="source-code">.Outputs['training'].S3Output.S3Uri,</p><p class="source-code">      content_type='text/plain'),</p><p class="source-code">       'validation': TrainingInput(s3_data=</p><p class="source-code">step_build_dataset.properties.ProcessingOutputConfig</p><p class="source-code">.Outputs['validation'].S3Output.S3Uri,</p><p class="source-code">      content_type='text/plain')</p><p class="source-code">    }</p><p class="source-code">)</p></li>
			</ol>
			<p>Now, let's take <a id="_idIndexMarker1475"/>care of the model creation and model registration steps (the last ones in the pipeline).</p>
			<h2 id="_idParaDest-286"><a id="_idTextAnchor285"/>Creating and registering a model in SageMaker Pipelines</h2>
			<p>Once<a id="_idIndexMarker1476"/> the<a id="_idIndexMarker1477"/> model<a id="_idIndexMarker1478"/> has been trained, we<a id="_idIndexMarker1479"/> need to create it as a SageMaker model and register it in the model registry.</p>
			<ol>
				<li value="1">We create the model, passing the location of the training container and of the model artifact:<p class="source-code">from sagemaker.model import Model</p><p class="source-code">from sagemaker.workflow.steps import CreateModelStep</p><p class="source-code">model = Model(</p><p class="source-code">    image_uri=container,</p><p class="source-code">    model_data=step_train.properties</p><p class="source-code">               .ModelArtifacts.S3ModelArtifacts,</p><p class="source-code">    sagemaker_session=session,</p><p class="source-code">    name=model_name,   # workflow parameter</p><p class="source-code">    role=role)</p><p class="source-code">step_create_model = CreateModelStep(</p><p class="source-code">    name='create-model',</p><p class="source-code">    model=model,</p><p class="source-code">    inputs=None)</p></li>
				<li>We then register the model in the model registry, passing the list of allowed instance<a id="_idIndexMarker1480"/> types<a id="_idIndexMarker1481"/> for <a id="_idIndexMarker1482"/>deployment, as<a id="_idIndexMarker1483"/> well as the approval status. We associate it to a model package group that will hold this model, as well as further versions we train in the future:<p class="source-code">from sagemaker.workflow.step_collections import RegisterModel</p><p class="source-code">step_register = RegisterModel(</p><p class="source-code">    name='register-model',</p><p class="source-code">    estimator=bt,</p><p class="source-code">    model_data=step_train.properties.ModelArtifacts</p><p class="source-code">               .S3ModelArtifacts,</p><p class="source-code">    content_types=['text/plain'],</p><p class="source-code">    response_types=['application/json'],</p><p class="source-code">    inference_instances=['ml.t2.medium'],</p><p class="source-code">    transform_instances=['ml.m5.xlarge'],</p><p class="source-code">    model_package_group_name='blazing-text-on-amazon-customer-reviews-package',</p><p class="source-code">    approval_status=model_approval_status</p><p class="source-code">)</p></li>
			</ol>
			<p>All the steps are now defined, so let's assemble them in a pipeline.</p>
			<h2 id="_idParaDest-287"><a id="_idTextAnchor286"/>Creating a pipeline</h2>
			<p>We simply put<a id="_idIndexMarker1484"/> together all the steps and their parameters. Then, we create the pipeline (or update it if it existed previously):</p>
			<p class="source-code">from sagemaker.workflow.pipeline import Pipeline</p>
			<p class="source-code">pipeline_name = 'blazing-text-amazon-customer-reviews'</p>
			<p class="source-code">pipeline = Pipeline(</p>
			<p class="source-code">    name=pipeline_name,</p>
			<p class="source-code">    parameters=[region, processing_instance_type, processing_instance_count, training_instance_type, training_instance_count, model_approval_status, input_data, model_name],</p>
			<p class="source-code">    steps=[step_process, step_ingest, step_build_dataset, step_train, step_create_model, step_register])</p>
			<p class="source-code">pipeline.upsert(role_arn=role)</p>
			<p>We're all set. Let's run our pipeline!</p>
			<h2 id="_idParaDest-288"><a id="_idTextAnchor287"/>Running a pipeline</h2>
			<p>It takes a<a id="_idIndexMarker1485"/> single line of code to fire up a pipeline execution: </p>
			<ol>
				<li value="1">We assign values to the data location and model name parameters (the other ones have default values):<p class="source-code">execution = pipeline.start(</p><p class="source-code">    parameters=dict(</p><p class="source-code">        InputData=input_data_uri,</p><p class="source-code">        ModelName='blazing-text-amazon-reviews')</p><p class="source-code">)</p></li>
				<li>In SageMaker Studio, we go <strong class="bold">SageMaker resources</strong> / <strong class="bold">Pipelines</strong>, and we see the pipeline executing, as shown in the next screenshot:<div id="_idContainer193" class="IMG---Figure"><img src="Images/B17705_12_20.jpg" alt="Figure 12.20 – Executing a pipeline&#13;&#10;" width="1086" height="423"/></div><p class="figure-caption">Figure 12.20 – Executing a pipeline</p><p>After an <a id="_idIndexMarker1486"/>hour and a half, the pipeline is complete, as shown in the next screenshot:</p><div id="_idContainer194" class="IMG---Figure"><img src="Images/B17705_12_21.jpg" alt="Figure 12.21 – Visualizing a pipeline&#13;&#10;" width="907" height="598"/></div><p class="figure-caption">Figure 12.21 – Visualizing a pipeline</p></li>
				<li>Finally, for each step of the pipeline, we can see the lineage of all artifacts: <p class="source-code">from sagemaker.lineage.visualizer import LineageTableVisualizer</p><p class="source-code">viz = LineageTableVisualizer(session)</p><p class="source-code">for execution_step in reversed(execution.list_steps()):</p><p class="source-code">    print(execution_step)</p><p class="source-code">display(viz.show(</p><p class="source-code">    pipeline_execution_step=execution_step))</p><p>For example, the <a id="_idIndexMarker1487"/>output for the training step is shown in the next image. We see exactly which datasets and which container were used to train the model:</p></li>
			</ol>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="Images/B17705_12_22.jpg" alt="Figure 12.22 – Viewing the lineage for the training step&#13;&#10;" width="901" height="261"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.22 – Viewing the lineage for the training step</p>
			<p>Let's see how we can deploy this model.</p>
			<h2 id="_idParaDest-289"><a id="_idTextAnchor288"/>Deploying a model from the model registry</h2>
			<p>Going<a id="_idIndexMarker1488"/> to <strong class="bold">SageMaker resources</strong> / <strong class="bold">Model registry</strong>, we <a id="_idIndexMarker1489"/>also see that the model has been registered in the model registry, as shown in the next screenshot. If we train further versions of the model, they will also appear here:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="Images/B17705_12_23.jpg" alt="Figure 12.23 – Viewing a model in the model registry&#13;&#10;" width="973" height="493"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.23 – Viewing a model in the model registry</p>
			<p>As its status is <strong class="source-inline">Pending</strong>, it can't be deployed for now. We need to change it to <strong class="source-inline">Approved</strong> in<a id="_idIndexMarker1490"/> order to allow deployment. This is a safe way<a id="_idIndexMarker1491"/> to guarantee that only good models are deployed, once all appropriate tests have been performed.</p>
			<p>We right-click on the model and select <strong class="bold">Update model version status</strong>. We then set the model status to <strong class="source-inline">Approved</strong>. We also note the model ARN, which is visible in the <strong class="bold">Settings</strong> tab.</p>
			<p>Now, we can deploy and test the model:</p>
			<ol>
				<li value="1">Back in our Jupyter Notebook, we create a <strong class="source-inline">ModelPackage</strong> object pointing at the model version we'd like to deploy:<p class="source-code">from sagemaker import ModelPackage</p><p class="source-code">model_package_arn = 'arn:aws:sagemaker:eu-west-1:123456789012:model-package/blazing-text-on-amazon-customer-reviews-package/1'</p><p class="source-code">model = sagemaker.ModelPackage(</p><p class="source-code">    role = role,</p><p class="source-code">    model_package_arn = model_package_arn)</p></li>
				<li>We call <strong class="source-inline">deploy()</strong> as usual:<p class="source-code">model.deploy(</p><p class="source-code">    initial_instance_count = 1,</p><p class="source-code">    instance_type = 'ml.t2.medium',</p><p class="source-code">    endpoint_name='blazing-text-on-amazon-reviews')</p></li>
				<li>We <a id="_idIndexMarker1492"/>create a <strong class="source-inline">Predictor</strong> and send a test <a id="_idIndexMarker1493"/>sample for prediction:<p class="source-code">from sagemaker.predictor import Predictor</p><p class="source-code">bt_predictor = Predictor(</p><p class="source-code">    endpoint_name='blazing-text-on-amazon-reviews',</p><p class="source-code">    serializer=</p><p class="source-code">        sagemaker.serializers.JSONSerializer(),       </p><p class="source-code">    deserializer=</p><p class="source-code">        sagemaker.deserializers.JSONDeserializer())</p><p class="source-code">instances = [' I really love this camera , it takes amazing pictures . ']</p><p class="source-code">payload = {'instances': instances, </p><p class="source-code">           'configuration': {'k': 3}}</p><p class="source-code">response = bt_predictor.predict(payload)</p><p class="source-code">print(response)</p><p>This prints out the probabilities for all three classes:</p><p class="source-code"><strong class="bold">[{'label': ['__label__positive__', '__label__neutral__', '__label__negative__'],</strong></p><p class="source-code"><strong class="bold">'prob': [0.9999945163726807, 2.51355941145448e-05, 1.0307396223652177e-05]},</strong></p></li>
				<li>Once we're done, we can delete the endpoint. <p class="callout-heading">Note</p><p class="callout">For a full clean-up, you should also delete the pipeline, the feature store, and the model package group. You'll find a clean-up notebook in the GitHub repository.</p></li>
			</ol>
			<p>As you can see, SageMaker Pipelines provides you with robust and powerful tools to build, run, and track end-to-end machine learning workflows. These tools are nicely integrated<a id="_idIndexMarker1494"/> in<a id="_idIndexMarker1495"/> SageMaker Studio, which should help you to be more productive and get high-quality models in production quicker</p>
			<h1 id="_idParaDest-290"><a id="_idTextAnchor289"/>Summary</h1>
			<p>In this chapter, you first learned how to deploy and update endpoints with AWS CloudFormation. You also saw how it can be used to implement canary deployment and blue-green deployment.</p>
			<p>Then, you learned about the AWS CDK, an SDK specifically built to easily generate and deploy CloudFormation templates using a variety of programming languages. </p>
			<p>Finally, you built complete end-to-end machine learning workflows with AWS Step Functions and Amazon SageMaker Pipelines.</p>
			<p>In the next and final chapter, you'll learn about additional SageMaker capabilities that help you optimize the cost and performance of predictions.</p>
		</div>
	</div></body></html>
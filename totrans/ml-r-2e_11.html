<html><head></head><body><div class="chapter" title="Chapter&#xA0;11.&#xA0;Improving Model Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Improving Model Performance</h1></div></div></div><p>When a sports team falls short of meeting its goal—whether the goal is to obtain an Olympic gold medal, a league championship, or a world record time—it must search for possible improvements. Imagine that you're the team's coach. How would you spend your practice sessions? Perhaps you'd direct the athletes to train harder or train differently in order to maximize every bit of their potential. Or, you might emphasize better teamwork, utilizing the athletes' strengths and weaknesses more smartly.</p><p>Now imagine that you're training a world champion machine learning algorithm. Perhaps you hope to compete <a id="id893" class="indexterm"/>in data mining competitions such as those posted on Kaggle (<a class="ulink" href="http://www.kaggle.com/competitions">http://www.kaggle.com/competitions</a>). Maybe you simply need to improve business results. Where <a id="id894" class="indexterm"/>do you begin? Although the context differs, the strategies one uses to improve sports team performance can also be used to improve the performance of statistical learners.</p><p>As the coach, it is your job to find the combination of training techniques and teamwork skills that allow you to meet your performance goals. This chapter builds upon the material covered throughout this book to introduce a set of techniques for improving the predictive performance of machine learners. You will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to automate model performance tuning by systematically searching for the optimal set of training conditions</li><li class="listitem" style="list-style-type: disc">The methods for combining models into groups that use teamwork to tackle tough learning tasks</li><li class="listitem" style="list-style-type: disc">How to apply a variant of decision trees, which has quickly become popular due to its impressive performance</li></ul></div><p>None of these methods will be successful for every problem. Yet, looking at the winning entries to machine learning competitions, you'll likely find that at least one of them has been employed. To be competitive, you too will need to add these skills to your repertoire.</p><div class="section" title="Tuning stock models for better performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec47"/>Tuning stock models for better performance</h1></div></div></div><p>Some learning <a id="id895" class="indexterm"/>problems are well-suited to the stock models presented in the previous chapters. In such cases, it may not be necessary to spend much time iterating and refining the model; it may perform well enough as it is. On the other hand, some problems are inherently more difficult. The underlying concepts to be learned may be extremely complex, requiring an understanding of many subtle relationships, or it may be affected by random variation, making it difficult to define the signal within the noise.</p><p>Developing models that perform extremely well on difficult problems is every bit an art as it is a science. Sometimes a bit of intuition is helpful when trying to identify areas where performance can be improved. In other cases, finding improvements will require a brute-force, trial and error approach. Of course, the process of searching numerous possible improvements can be aided by the use of automated programs.</p><p>In <a class="link" href="ch05.html" title="Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules">Chapter 5</a>, <span class="emphasis"><em>Divide and Conquer – Classification Using Decision Trees and Rules</em></span>, we attempted a difficult problem: identifying loans that were likely to enter into default. Although we were able to use performance tuning methods to obtain a respectable classification accuracy of about 82 percent, upon a more careful examination in <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span>, we realized that the high accuracy was a bit misleading. In spite of the reasonable accuracy, the kappa statistic was only about 0.28, which suggested that the model was actually performing somewhat poorly. In this section, we'll revisit the credit scoring model to see whether we can improve the results.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip128"/>Tip</h3><p>To follow along with the examples, download the <code class="literal">credit.csv</code> file from the Packt Publishing website and save it to your R working directory. Load the file into R using the command <code class="literal">credit &lt;- read.csv("credit.csv")</code>.</p></div></div><p>You will recall that we first used a stock C5.0 decision tree to build the classifier for the credit data. We then attempted to improve its performance by adjusting the <code class="literal">trials</code> parameter to increase the number of boosting iterations. By increasing the number of iterations from the default of 1 up to the value of 10, we were able to increase the model's accuracy. This process of adjusting the model options to identify the best fit is called <span class="strong"><strong>parameter </strong></span><a id="id896" class="indexterm"/>
<span class="strong"><strong>tuning</strong></span>.</p><p>Parameter tuning is not limited to decision trees. For instance, we tuned k-NN models when we searched for the best value of <span class="emphasis"><em>k</em></span>. We also tuned neural networks and support vector machines as we adjusted the number of nodes or hidden layers, or chose different kernel functions. Most machine learning algorithms allow the adjustment of at least one parameter, and the most sophisticated models offer a large number of ways to tweak the model fit. Although <a id="id897" class="indexterm"/>this allows the model to be tailored closely to the learning task, the complexity of all the possible options can be daunting. A more systematic approach is warranted.</p><div class="section" title="Using caret for automated parameter tuning"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec114"/>Using caret for automated parameter tuning</h2></div></div></div><p>Rather than <a id="id898" class="indexterm"/>choosing arbitrary <a id="id899" class="indexterm"/>values for each of the model's parameters—a task that is not only tedious, but also somewhat unscientific—it is better to conduct a search through many possible parameter values to find the best combination.</p><p>The <code class="literal">caret</code> package, which we used extensively in <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span>, provides tools to assist with automated parameter tuning. The core functionality is provided by a <code class="literal">train()</code> function that serves as a standardized interface for over 175 different machine learning models for both classification and regression tasks. By using this function, it is possible to automate the search for optimal models using a choice of evaluation methods and metrics.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip129"/>Tip</h3><p>Do not feel overwhelmed by the large number of models—we've already covered many of them in the earlier chapters. Others are simple variants or extensions of the base concepts. Given what you've learned so far, you should be confident that you have the ability to understand all of the available methods.</p></div></div><p>Automated parameter <a id="id900" class="indexterm"/>tuning requires you to consider three questions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What type of machine learning model (and specific implementation) should be trained on the data?</li><li class="listitem" style="list-style-type: disc">Which model parameters can be adjusted, and how extensively should they be tuned to find the optimal settings?</li><li class="listitem" style="list-style-type: disc">What criteria should be used to evaluate the models to find the best candidate?</li></ul></div><p>Answering the first question involves finding a well-suited match between the machine learning task and one of the 175 models. Obviously, this requires an understanding of the breadth and depth of machine learning models. It can also help to work through a process of elimination. Nearly half of the models can be eliminated depending on whether the task is classification or numeric prediction; others can be excluded based on the format of the data or the need to avoid black box models, and so on. In any case, there's also no reason you can't try several approaches and compare the best results of each.</p><p>Addressing the second question is a matter largely dictated by the choice of model, since each algorithm utilizes <a id="id901" class="indexterm"/>a unique set of parameters. The available <a id="id902" class="indexterm"/>tuning parameters for the predictive models covered in this book are listed in the following table. Keep in mind that although some models have additional options not shown, only those listed in the <a id="id903" class="indexterm"/>table are supported by <code class="literal">caret</code> for automatic tuning.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Model</p>
</th><th style="text-align: left" valign="bottom">
<p>Learning Task</p>
</th><th style="text-align: left" valign="bottom">
<p>Method name</p>
</th><th style="text-align: left" valign="bottom">
<p>Parameters</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>k-Nearest Neighbors</p>
</td><td style="text-align: left" valign="top">
<p>Classification</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">knn</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">k</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Naive Bayes</p>
</td><td style="text-align: left" valign="top">
<p>Classification</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">nb</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">fL</code>, <code class="literal">usekernel</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Decision Trees</p>
</td><td style="text-align: left" valign="top">
<p>Classification</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">C5.0</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">model</code>, <code class="literal">trials</code>, <code class="literal">winnow</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>OneR Rule Learner</p>
</td><td style="text-align: left" valign="top">
<p>Classification</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">OneR</code>
</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>RIPPER Rule Learner</p>
</td><td style="text-align: left" valign="top">
<p>Classification</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">JRip</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">NumOpt</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Linear Regression</p>
</td><td style="text-align: left" valign="top">
<p>Regression</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">lm</code>
</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Regression Trees</p>
</td><td style="text-align: left" valign="top">
<p>Regression</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">rpart</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">cp</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Model Trees</p>
</td><td style="text-align: left" valign="top">
<p>Regression</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">M5</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">pruned</code>, <code class="literal">smoothed</code>, <code class="literal">rules</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Neural Networks</p>
</td><td style="text-align: left" valign="top">
<p>Dual use</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">nnet</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">size</code>, <code class="literal">decay</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Support Vector Machines (Linear Kernel)</p>
</td><td style="text-align: left" valign="top">
<p>Dual use</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">svmLinear</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">C</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Support Vector Machines (Radial Basis Kernel)</p>
</td><td style="text-align: left" valign="top">
<p>Dual use</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">svmRadial</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">C, sigma</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Random Forests</p>
</td><td style="text-align: left" valign="top">
<p>Dual use</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">rf</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">mtry</code>
</p>
</td></tr></tbody></table></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip130"/>Tip</h3><p>For a <a id="id904" class="indexterm"/>complete list of the models and corresponding tuning parameters covered by <code class="literal">caret</code>, refer to the table provided by package author Max Kuhn at <a class="ulink" href="http://topepo.github.io/caret/modelList.html">http://topepo.github.io/caret/modelList.html</a>.</p></div></div><p>If you ever forget the tuning parameters for a particular model, the <code class="literal">modelLookup()</code> function can be used to find them. Simply supply the method name, as illustrated here for the C5.0 model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; modelLookup("C5.0")</strong></span>
<span class="strong"><strong>  model parameter                 label forReg forClass probModel</strong></span>
<span class="strong"><strong>1  C5.0    trials # Boosting Iterations  FALSE     TRUE      TRUE</strong></span>
<span class="strong"><strong>2  C5.0     model            Model Type  FALSE     TRUE      TRUE</strong></span>
<span class="strong"><strong>3  C5.0    winnow                Winnow  FALSE     TRUE      TRUE</strong></span>
</pre></div><p>The goal of automatic tuning is to search a set of candidate models comprising a matrix, or <span class="strong"><strong>grid</strong></span>, of parameter combinations. Because it is impractical to search every conceivable combination, only a subset of possibilities is used to construct the grid. By default, <code class="literal">caret</code> searches at most three values for each of the <span class="emphasis"><em>p</em></span> parameters. This means that at most <span class="emphasis"><em>3^p</em></span> <a id="id905" class="indexterm"/>candidate models will be tested. For example, by default, the automatic tuning of k-Nearest Neighbors will compare <span class="emphasis"><em>3^1 = 3</em></span> candidate models with <code class="literal">k=5</code>, <code class="literal">k=7</code>, and <code class="literal">k=9</code>. Similarly, tuning a decision tree will result in a comparison of up to 27 different candidate models, comprising the grid of <span class="emphasis"><em>3^3 = 27</em></span> combinations of <code class="literal">model</code>, <code class="literal">trials</code>, and <code class="literal">winnow</code> settings. In practice, however, only 12 models are actually tested. This is because the <code class="literal">model</code> and <code class="literal">winnow</code> parameters <a id="id906" class="indexterm"/>can only take two values (<code class="literal">tree</code> versus <code class="literal">rules</code> and <code class="literal">TRUE</code> versus <code class="literal">FALSE</code>, respectively), which makes the grid size <span class="emphasis"><em>3 * 2 * 2 = 12</em></span>.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip131"/>Tip</h3><p>Since the default search grid may not be ideal for your learning problem, <code class="literal">caret</code> allows you to provide a custom search grid defined by a simple command, which we will cover later.</p></div></div><p>The third and final step in automatic model tuning involves identifying the best model among the candidates. This uses the methods discussed in <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span>, such as the choice of resampling strategy for creating training and test datasets and the use of model performance statistics to measure the predictive accuracy.</p><p>All of the resampling strategies and many of the performance statistics we've learned are supported by <code class="literal">caret</code>. These include statistics such as accuracy and kappa (for classifiers) and R-squared or RMSE (for numeric models). Cost-sensitive measures such as sensitivity, specificity, and area under the ROC curve (AUC) can also be used, if desired.</p><p>By default, <code class="literal">caret</code> will select the candidate model with the largest value of the desired performance measure. As this practice sometimes results in the selection of models that achieve marginal performance improvements via large increases in model complexity, alternative model selection functions are provided.</p><p>Given the wide variety of options, it is helpful that many of the defaults are reasonable. For instance, <code class="literal">caret</code> will use prediction accuracy on a bootstrap sample to choose the best performer for <a id="id907" class="indexterm"/>classification models. Beginning with these default values, we can then tweak the <code class="literal">train()</code> function to design a <a id="id908" class="indexterm"/>wide variety of experiments.</p><div class="section" title="Creating a simple tuned model"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec63"/>Creating a simple tuned model</h3></div></div></div><p>To illustrate the <a id="id909" class="indexterm"/>process of tuning a model, let's begin by observing what happens when we attempt to tune the credit scoring model using the <code class="literal">caret</code> package's default settings. From there, we will adjust the options to our liking.</p><p>The simplest way to tune a learner requires you to only specify a model type via the <code class="literal">method</code> parameter. Since we used C5.0 decision trees previously with the credit model, we'll continue our work by optimizing this learner. The basic <code class="literal">train()</code> command for tuning a C5.0 decision tree using the default settings is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; m &lt;- train(default ~ ., data = credit, method = "C5.0")</strong></span>
</pre></div><p>First, the <code class="literal">set.seed()</code> function is used to initialize R's random number generator to a set starting position. You may recall that we used this function in several prior chapters. By setting the <code class="literal">seed</code> parameter (in this case to the arbitrary number 300), the random numbers will follow a predefined sequence. This allows simulations that use random sampling to be repeated with identical results—a very helpful feature if you are sharing code or attempting to replicate a prior result.</p><p>Next, we define a tree as <code class="literal">default ~ .</code> using the R formula interface. This models loan default status (<code class="literal">yes</code> or <code class="literal">no</code>) using all of the other features in the <code class="literal">credit</code> data frame. The parameter <code class="literal">method = "C5.0"</code> tells <code class="literal">caret</code> to use the C5.0 decision tree algorithm.</p><p>After you've entered the preceding command, there may be a significant delay (dependent upon your computer's capabilities) as the tuning process occurs. Even though this is a fairly small dataset, a substantial amount of calculation must occur. R must repeatedly generate random samples of data, build decision trees, compute performance statistics, and evaluate the result.</p><p>The result of the experiment is saved in an object named <code class="literal">m</code>. If you would like to examine the object's contents, the <code class="literal">str(m)</code> command will list all the associated data, but this can be quite overwhelming. Instead, simply type the name of the object for a condensed summary of the results. For instance, typing <code class="literal">m</code> yields the following output (note that labels have been added for clarity):</p><div class="mediaobject"><img src="graphics/B03905_11_01.jpg" alt="Creating a simple tuned model"/></div><p>The labels highlight <a id="id910" class="indexterm"/>four main components in the output:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>A brief description of the input dataset</strong></span>: If you are familiar with your data and have applied the <code class="literal">train()</code> function correctly, this information should not be surprising.</li><li class="listitem"><span class="strong"><strong>A report of the preprocessing and resampling methods applied</strong></span>: Here, we see that 25 bootstrap samples, each including 1,000 examples, were used to train the models.</li><li class="listitem"><span class="strong"><strong>A list of the candidate models evaluated</strong></span>: In this section, we can confirm that 12 different models were tested, based on the combinations of three C5.0 tuning parameters—<code class="literal">model</code>, <code class="literal">trials</code>, and <code class="literal">winnow</code>. The average and standard deviation of the accuracy and kappa statistics for each candidate model are also shown.</li><li class="listitem"><span class="strong"><strong>The choice of the best model</strong></span>: As the footnote describes, the model with the largest accuracy was selected. This was the model that used a decision tree with 20 trials and the setting <code class="literal">winnow = FALSE</code>.</li></ol></div><p>After identifying the best model, the <code class="literal">train()</code> function uses its tuning parameters to build a model on the full input dataset, which is stored in the <code class="literal">m</code> list object as <code class="literal">m$finalModel</code>. In most cases, you will not <a id="id911" class="indexterm"/>need to work directly with the <code class="literal">finalModel</code> sub-object. Instead, simply use the <code class="literal">predict()</code> function with the <code class="literal">m</code> object as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; p &lt;- predict(m, credit)</strong></span>
</pre></div><p>The resulting vector of predictions works as expected, allowing us to create a confusion matrix that compares the predicted and actual values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(p, credit$default)</strong></span>

<span class="strong"><strong>p      no yes</strong></span>
<span class="strong"><strong>  no  700   2</strong></span>
<span class="strong"><strong>  yes   0 298</strong></span>
</pre></div><p>Of the 1,000 examples used for training the final model, only two were misclassified. However, it is very important to note that since the model was built on both the training and test data, this accuracy is optimistic and thus, should not be viewed as indicative of performance on unseen data. The bootstrap estimate of 73 percent (shown in the summary output) is a more realistic estimate of future performance.</p><p>Using the <code class="literal">train()</code> and <code class="literal">predict()</code> functions also offers a couple of benefits in addition to the automatic parameter tuning.</p><p>First, any data preparation steps applied by the <code class="literal">train()</code> function will be similarly applied to the data used for generating predictions. This includes transformations such as centering and scaling as well as imputation of missing values. Allowing <code class="literal">caret</code> to handle the data preparation will ensure that the steps that contributed to the best model's performance will remain in place when the model is deployed.</p><p>Second, the <code class="literal">predict()</code> function provides a standardized interface for obtaining predicted class values and class probabilities, even for model types that ordinarily would require additional steps to obtain this information. The predicted classes are provided by default:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head(predict(m, credit))</strong></span>
<span class="strong"><strong>[1] no  yes no  no  yes no</strong></span>
<span class="strong"><strong>Levels: no yes</strong></span>
</pre></div><p>To obtain the estimated probabilities for each class, use the <code class="literal">type = "prob"</code> parameter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head(predict(m, credit, type = "prob"))</strong></span>
<span class="strong"><strong>         no        yes</strong></span>
<span class="strong"><strong>1 0.9606970 0.03930299</strong></span>
<span class="strong"><strong>2 0.1388444 0.86115561</strong></span>
<span class="strong"><strong>3 1.0000000 0.00000000</strong></span>
<span class="strong"><strong>4 0.7720279 0.22797208</strong></span>
<span class="strong"><strong>5 0.2948062 0.70519385</strong></span>
<span class="strong"><strong>6 0.8583715 0.14162851</strong></span>
</pre></div><p>Even in cases where the <a id="id912" class="indexterm"/>underlying model refers to the prediction probabilities using a different string (for example, <code class="literal">"raw"</code> for a <code class="literal">naiveBayes</code> model), the <code class="literal">predict()</code> function will translate <code class="literal">type = "prob"</code> to the appropriate string behind the scenes.</p></div><div class="section" title="Customizing the tuning process"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec64"/>Customizing the tuning process</h3></div></div></div><p>The decision tree <a id="id913" class="indexterm"/>we created previously demonstrates the <code class="literal">caret</code> package's ability to produce an optimized model with minimal intervention. The default settings allow optimized models to be created easily. However, it is also possible to change the default settings to something more specific to a learning task, which may assist with unlocking the upper echelon of performance.</p><p>Each step in the model selection process can be customized. To illustrate this flexibility, let's modify our work on the credit decision tree to mirror the process we had used in <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span>. If you remember, we had estimated the kappa statistic using 10-fold cross-validation. We'll do the same here, using kappa to optimize the boosting parameter of the decision tree. Note that decision tree boosting was previously covered in <a class="link" href="ch05.html" title="Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules">Chapter 5</a>, <span class="emphasis"><em>Divide and Conquer – Classification Using Decision Trees and Rules</em></span>, and will also be covered in greater detail later this chapter.</p><p>The <code class="literal">trainControl()</code> function is used to create a set of configuration options known as a <span class="strong"><strong>control object</strong></span>, which <a id="id914" class="indexterm"/>guides the <code class="literal">train()</code> function. These options allow for the management of model evaluation criteria such as the resampling strategy and the measure used for choosing the best model. Although this function can be used to modify nearly every aspect of a tuning experiment, we'll focus on the two important parameters: <code class="literal">method</code> and <code class="literal">selectionFunction</code>.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip132"/>Tip</h3><p>If you're eager for more details, you can use the <code class="literal">?trainControl</code> command for a list of all the parameters.</p></div></div><p>For the <code class="literal">trainControl()</code> function, the <code class="literal">method</code> parameter is used to set the resampling method, such as holdout sampling or k-fold cross-validation. The following table lists the possible method types as well as any additional parameters for adjusting the sample size and number of iterations. Although the default options for these resampling methods follow popular <a id="id915" class="indexterm"/>convention, you may choose to adjust these depending upon the size of your dataset and the complexity of your model.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Resampling method</p>
</th><th style="text-align: left" valign="bottom">
<p>Method name</p>
</th><th style="text-align: left" valign="bottom">
<p>Additional options and default values</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Holdout sampling</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">LGOCV</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">p = 0.75</code> (training data proportion)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>k-fold cross-validation</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">cv</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">number = 10</code> (number of folds)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Repeated k-fold cross-validation</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">repeatedcv</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">number = 10</code> (number of folds)</p>
<p>
<code class="literal">repeats = 10</code> (number of iterations)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Bootstrap sampling</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">boot</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">number = 25</code> (resampling iterations)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.632 bootstrap</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">boot632</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">number = 25</code> (resampling iterations)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Leave-one-out cross-validation</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">LOOCV</code>
</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td></tr></tbody></table></div><p>The <code class="literal">selectionFunction</code> parameter is used to specify the function that will choose the optimal model among the various candidates. Three such functions are included. The <code class="literal">best</code> function simply chooses the candidate with the best value on the specified performance measure. This is used by default. The other two functions are used to choose the most parsimonious, or simplest, model that is within a certain threshold of the best model's performance. The <code class="literal">oneSE</code> function chooses the simplest candidate within one standard error of the best performance, and <code class="literal">tolerance</code> uses the simplest candidate within a user-specified percentage.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip133"/>Tip</h3><p>Some subjectivity is involved with the <code class="literal">caret</code> package's ranking of models by simplicity. For information on how models are ranked, see the help page for the selection functions by typing <code class="literal">?best</code> at the R command prompt.</p></div></div><p>To create a control object named <code class="literal">ctrl</code> that uses 10-fold cross-validation and the <code class="literal">oneSE</code> selection function, use the following command (note that <code class="literal">number = 10</code> is included only for clarity; since this is the default value for <code class="literal">method = "cv"</code>, it could have been omitted):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; ctrl &lt;- trainControl(method = "cv", number = 10,</strong></span>
<span class="strong"><strong>                       selectionFunction = "oneSE")</strong></span>
</pre></div><p>We'll use the result of this function shortly.</p><p>In the meantime, the next step in defining our experiment is to create the grid of parameters to optimize. The grid must include a column named for each parameter in the desired model, prefixed by a period. It must also include a row for each desired combination of parameter values. Since we are using a C5.0 decision tree, this means we'll need columns named <code class="literal">.model</code>, <code class="literal">.trials</code>, and <code class="literal">.winnow</code>. For other machine learning models, refer to the table presented earlier in this chapter or use the <code class="literal">modelLookup()</code> function to lookup the parameters as described previously.</p><p>Rather than filling <a id="id916" class="indexterm"/>this data frame cell by cell—a tedious task if there are many possible combinations of parameter values—we can use the <code class="literal">expand.grid()</code> function, which creates data frames from the combinations of all the values supplied. For example, suppose we would like to hold constant <code class="literal">model = "tree"</code> and <code class="literal">winnow = "FALSE"</code> while searching eight different values of trials. This can be created as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; grid &lt;- expand.grid(.model = "tree",</strong></span>
<span class="strong"><strong>                      .trials = c(1, 5, 10, 15, 20, 25, 30, 35),</strong></span>
<span class="strong"><strong>                      .winnow = "FALSE")</strong></span>
</pre></div><p>The resulting grid data frame contains <span class="emphasis"><em>1 * 8 * 1 = 8</em></span> rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; grid</strong></span>
<span class="strong"><strong>  .model .trials .winnow</strong></span>
<span class="strong"><strong>1   tree       1   FALSE</strong></span>
<span class="strong"><strong>2   tree       5   FALSE</strong></span>
<span class="strong"><strong>3   tree      10   FALSE</strong></span>
<span class="strong"><strong>4   tree      15   FALSE</strong></span>
<span class="strong"><strong>5   tree      20   FALSE</strong></span>
<span class="strong"><strong>6   tree      25   FALSE</strong></span>
<span class="strong"><strong>7   tree      30   FALSE</strong></span>
<span class="strong"><strong>8   tree      35   FALSE</strong></span>
</pre></div><p>The <code class="literal">train()</code> function will build a candidate model for evaluation using each row's combination of model parameters.</p><p>Given this search grid and the control list created previously, we are ready to run a thoroughly customized <code class="literal">train()</code> experiment. As we did earlier, we'll set the random seed to the arbitrary number <code class="literal">300</code> in order to ensure repeatable results. But this time, we'll pass our control object and tuning grid while adding a parameter <code class="literal">metric = "Kappa"</code>, indicating the statistic to be used by the model evaluation function—in this case, <code class="literal">"oneSE"</code>. The full command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; m &lt;- train(default ~ ., data = credit, method = "C5.0",</strong></span>
<span class="strong"><strong>             metric = "Kappa",</strong></span>
<span class="strong"><strong>             trControl = ctrl,</strong></span>
<span class="strong"><strong>             tuneGrid = grid)</strong></span>
</pre></div><p>This results in an object that we can view by typing its name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; m</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03905_11_02.jpg" alt="Customizing the tuning process"/></div><p>Although much of the output is similar to the automatically tuned model, there are a few differences of note. As 10-fold cross-validation was used, the sample size to build each candidate model was reduced to 900 rather than the 1,000 used in the bootstrap. As we requested, eight candidate models were tested. Additionally, because <code class="literal">model</code> and <code class="literal">winnow</code> were held constant, their values are no longer shown in the results; instead, they are listed as a footnote.</p><p>The best model here <a id="id917" class="indexterm"/>differs quite significantly from the prior trial. Before, the best model used <code class="literal">trials = 20</code>, whereas here, it used <code class="literal">trials = 1</code>. This seemingly odd finding is due to the fact that we used the <code class="literal">oneSE</code> rule rather the <code class="literal">best</code> rule to select the optimal model. Even though the 35-trial model offers the best raw performance according to kappa, the 1-trial model offers nearly the same performance with a much simpler form. Not only are simple models more computationally efficient, but they also reduce the chance of overfitting the training data.</p></div></div></div></div>
<div class="section" title="Improving model performance with meta-learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec48"/>Improving model performance with meta-learning</h1></div></div></div><p>As an alternative to <a id="id918" class="indexterm"/>increasing the performance of a single model, it is possible to combine several models to form a <a id="id919" class="indexterm"/>powerful team. Just as the best sports teams have players with complementary rather than overlapping skillsets, some of the best machine learning algorithms utilize teams of complementary models. Since a model brings a unique bias to a learning task, it may readily learn one subset of examples, but have trouble with another. Therefore, by intelligently using the talents of several diverse team members, it is possible to create a strong team of multiple weak learners.</p><p>This technique of combining and managing the predictions of multiple models falls into a wider set of <a id="id920" class="indexterm"/>
<span class="strong"><strong>meta-learning</strong></span> methods defining techniques that involve learning how to learn. This includes anything from simple algorithms that gradually improve performance by iterating over design decisions—for instance, the automated parameter tuning used earlier in this chapter—to highly complex algorithms that use concepts borrowed from evolutionary biology and genetics for self-modifying and adapting to learning tasks.</p><p>For the remainder of this chapter, we'll focus on meta-learning only as it pertains to modeling a relationship between the predictions of several models and the desired outcome. The teamwork-based techniques covered here are quite powerful, and are used quite often to build more effective classifiers.</p><div class="section" title="Understanding ensembles"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec115"/>Understanding ensembles</h2></div></div></div><p>Suppose you were a <a id="id921" class="indexterm"/>contestant on a television trivia show that allowed you to choose a panel of five friends to assist you with answering the final question for the million-dollar prize. Most people would try to stack the panel with a diverse set of subject matter experts. A panel containing professors of literature, science, history, and art, along with a current pop-culture expert would be a safely well-rounded group. Given their breadth of knowledge, it would be unlikely to find a question that stumps the group.</p><p>The meta-learning approach that utilizes a similar principle of creating a varied team of experts is known as an <span class="strong"><strong>ensemble</strong></span>. All the ensemble methods are based on the idea that by combining multiple weaker learners, a stronger learner is created. The various ensemble methods can be distinguished, in large part, by the answers to these two questions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How are the weak learning models chosen and/or constructed?</li><li class="listitem" style="list-style-type: disc">How are the weak learners' predictions combined to make a single final prediction?</li></ul></div><p>When answering these questions, it can be helpful to imagine the ensemble in terms of the following process diagram; nearly all ensemble approaches follow this pattern:</p><div class="mediaobject"><img src="graphics/B03905_11_03.jpg" alt="Understanding ensembles"/></div><p>First, input <a id="id922" class="indexterm"/>training data is used to build a number of models. The <span class="strong"><strong>allocation function</strong></span> dictates how much of the training data each model receives. Do they each receive the full training dataset or merely a sample? Do they each receive every feature or a subset?</p><p>Although the ideal ensemble includes a diverse set of models, the allocation function can increase diversity by artificially varying the input data to bias the resulting learners, even if they are the same type. For instance, it might use bootstrap sampling to construct unique training datasets or pass on a different subset of features or examples to each model. On the other hand, if the ensemble already includes a diverse set of algorithms—such as a neural network, a decision tree, and a k-NN classifier—the allocation function might pass the data on to each algorithm relatively unchanged.</p><p>After the models are constructed, they can be used to generate a set of predictions, which must be managed in some <a id="id923" class="indexterm"/>way. The <span class="strong"><strong>combination function</strong></span> governs how disagreements among the predictions are reconciled. For example, the ensemble might use a majority vote to determine the final prediction, or it could use a more complex strategy such as weighting each model's votes based on its prior performance.</p><p>Some ensembles even utilize another model to learn a combination function from various combinations of predictions. For example, suppose that when <span class="emphasis"><em>M1</em></span> and <span class="emphasis"><em>M2</em></span> both vote yes, the actual class value is usually no. In this case, the ensemble could learn to ignore the vote of <span class="emphasis"><em>M1</em></span> and <span class="emphasis"><em>M2</em></span> when they agree. This process of using the predictions of several models to train a final arbiter <a id="id924" class="indexterm"/>model is known as <span class="strong"><strong>stacking</strong></span>.</p><div class="mediaobject"><img src="graphics/B03905_11_04.jpg" alt="Understanding ensembles"/></div><p>One of the benefits of using ensembles is that they may allow you to spend less time in pursuit of a single best model. Instead, you can train a number of reasonably strong candidates and combine them. Yet, convenience isn't the only reason why ensemble-based methods continue to rack up wins in machine learning competitions; ensembles also offer a number of <a id="id925" class="indexterm"/>performance advantages over single models:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Better generalizability to future problems</strong></span>: As the opinions of several learners are incorporated into a single final prediction, no single bias is able to dominate. This reduces the chance of overfitting to a learning task.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Improved performance on massive or miniscule datasets</strong></span>: Many models run into memory or complexity limits when an extremely large set of features or examples are used, making it more efficient to train several small models than a single full model. Conversely, ensembles also do well on the smallest datasets because resampling methods such as bootstrapping are inherently a part of many ensemble designs. Perhaps most importantly, it is often possible to train an ensemble in parallel using distributed computing methods.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The ability to synthesize data from distinct domains</strong></span>: Since there is no one-size-fits-all learning algorithm, the ensemble's ability to incorporate evidence from multiple types of learners is increasingly important as complex phenomena rely on data drawn from diverse domains.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>A more nuanced understanding of difficult learning tasks</strong></span>: Real-world phenomena are <a id="id926" class="indexterm"/>often extremely complex with many interacting intricacies. Models that divide the task into smaller portions are likely to more accurately capture subtle patterns that a single global model might miss.</li></ul></div><p>None of these benefits <a id="id927" class="indexterm"/>would be very helpful if you weren't able to easily apply ensemble methods in R, and there are many packages available to do just that. Let's take a look at several of the most popular ensemble methods and how they can be used to improve the performance of the credit model we've been working on.</p></div><div class="section" title="Bagging"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec116"/>Bagging</h2></div></div></div><p>One of the first <a id="id928" class="indexterm"/>ensemble methods to gain widespread acceptance <a id="id929" class="indexterm"/>used a technique called <span class="strong"><strong>bootstrap aggregating</strong></span> or <a id="id930" class="indexterm"/>
<span class="strong"><strong>bagging</strong></span> for short. As described by Leo Breiman in 1994, bagging generates a number of training datasets by bootstrap sampling the original training data. These datasets are then used to generate a set of models using a single learning algorithm. The models' predictions are combined using voting (for classification) or averaging (for numeric prediction).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note39"/>Note</h3><p>For additional information on bagging, refer to Breiman L. <span class="emphasis"><em>Bagging predictors</em></span>. Machine Learning. 1996; 24:123-140.</p></div></div><p>Although bagging is a relatively simple ensemble, it can perform quite well as long as it is used with relatively <span class="strong"><strong>unstable</strong></span> learners, that is, those generating models that tend to change substantially when the input data changes only slightly. Unstable models are essential in order to ensure the ensemble's diversity in spite of only minor variations between the bootstrap training datasets. For this reason, bagging is often used with decision trees, which have the tendency to vary dramatically given minor changes in the input data.</p><p>The <code class="literal">ipred</code> package offers a classic implementation of bagged decision trees. To train the model, the <code class="literal">bagging()</code> function works similar to many of the models used previously. The <code class="literal">nbagg</code> parameter is used to control the number of decision trees voting in the ensemble (with a default value of <code class="literal">25</code>). Depending on the difficulty of the learning task and the amount of training data, increasing this number may improve the model's performance up to a limit. The downside is that this comes at the expense of additional computational expense because a large number of trees may take some time to train.</p><p>After installing the <code class="literal">ipred</code> package, we can create the ensemble as follows. We'll stick to the default value of 25 decision trees:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(ipred)</strong></span>
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; mybag &lt;- bagging(default ~ ., data = credit, nbagg = 25)</strong></span>
</pre></div><p>The resulting model <a id="id931" class="indexterm"/>works as expected with the <code class="literal">predict()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_pred &lt;- predict(mybag, credit)</strong></span>
<span class="strong"><strong>&gt; table(credit_pred, credit$default)</strong></span>
<span class="strong"><strong>           </strong></span>
<span class="strong"><strong>credit_pred  no yes</strong></span>
<span class="strong"><strong>        no  699   2</strong></span>
<span class="strong"><strong>        yes   1 298</strong></span>
</pre></div><p>Given the preceding results, the model seems to have fit the training data extremely well. To see how this translates into future performance, we can use the bagged trees with 10-fold CV using the <code class="literal">train()</code> function in the <code class="literal">caret</code> package. Note that the method name for the <code class="literal">ipred</code> bagged trees function is <code class="literal">treebag</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; ctrl &lt;- trainControl(method = "cv", number = 10)</strong></span>
<span class="strong"><strong>&gt; train(default ~ ., data = credit, method = "treebag",</strong></span>
<span class="strong"><strong>         trControl = ctrl)</strong></span>

<span class="strong"><strong>Bagged CART </strong></span>

<span class="strong"><strong>1000 samples</strong></span>
<span class="strong"><strong>  16 predictor</strong></span>
<span class="strong"><strong>   2 classes: 'no', 'yes' </strong></span>

<span class="strong"><strong>No pre-processing</strong></span>
<span class="strong"><strong>Resampling: Cross-Validated (10 fold) </strong></span>

<span class="strong"><strong>Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... </strong></span>

<span class="strong"><strong>Resampling results</strong></span>

<span class="strong"><strong>  Accuracy  Kappa      Accuracy SD  Kappa SD  </strong></span>
<span class="strong"><strong>  0.735     0.3297726  0.03439961   0.08590462</strong></span>
</pre></div><p>The kappa statistic of 0.33 for this model suggests that the bagged tree model performs at least as well as the best C5.0 decision tree we tuned earlier in this chapter. This illustrates the power of ensemble methods; a set of simple learners working together can outperform very sophisticated models.</p><p>To get beyond bags of decision trees, the <code class="literal">caret</code> package also provides a more general <code class="literal">bag()</code> function. It includes native support for a handful of models, though it can be adapted to other types with a bit of additional effort. The <code class="literal">bag()</code> function uses a control object to configure the <a id="id932" class="indexterm"/>bagging process. It requires the specification of three <a id="id933" class="indexterm"/>functions: one for fitting the model, one for making predictions, and one for aggregating the votes.</p><p>For example, suppose we wanted to create a bagged support vector machine model, using the <a id="id934" class="indexterm"/>
<code class="literal">ksvm()</code> function in the <code class="literal">kernlab</code> package we used in <a class="link" href="ch07.html" title="Chapter 7. Black Box Methods – Neural Networks and Support Vector Machines">Chapter 7</a>, <span class="emphasis"><em>Black Box Methods – Neural Networks and Support Vector Machines</em></span>. The <code class="literal">bag()</code> function requires us to provide functionality for training the SVMs, making predictions, and counting votes.</p><p>Rather than writing <a id="id935" class="indexterm"/>these ourselves, the <code class="literal">caret</code> package's built-in <code class="literal">svmBag</code> list object supplies three functions we can use for this purpose:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(svmBag)</strong></span>
<span class="strong"><strong>List of 3</strong></span>
<span class="strong"><strong> $ fit      :function (x, y, ...)  </strong></span>
<span class="strong"><strong> $ pred     :function (object, x)  </strong></span>
<span class="strong"><strong> $ aggregate:function (x, type = "class")</strong></span>
</pre></div><p>By looking at the <code class="literal">svmBag$fit</code> function, we see that it simply calls the <code class="literal">ksvm()</code> function from the <code class="literal">kernlab</code> package and returns the result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; svmBag$fit</strong></span>
<span class="strong"><strong>function (x, y, ...) </strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>    library(kernlab)</strong></span>
<span class="strong"><strong>    out &lt;- ksvm(as.matrix(x), y, prob.model = is.factor(y), ...)</strong></span>
<span class="strong"><strong>    out</strong></span>
<span class="strong"><strong>}</strong></span>
<span class="strong"><strong>&lt;environment: namespace:caret&gt;</strong></span>
</pre></div><p>The <code class="literal">pred</code> and <code class="literal">aggregate</code> <a id="id936" class="indexterm"/>functions for <code class="literal">svmBag</code> are also similarly straightforward. By studying these functions and creating your own in the same format, it is possible to use bagging with any machine learning algorithm you would like.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip134"/>Tip</h3><p>The <code class="literal">caret</code> package also includes example objects for bags of naive Bayes models (<code class="literal">nbBag</code>), decision trees (<code class="literal">ctreeBag</code>), and neural networks (<code class="literal">nnetBag</code>).</p></div></div><p>Applying the three functions in the <code class="literal">svmBag</code> list, we can create a bagging control object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; bagctrl &lt;- bagControl(fit = svmBag$fit, </strong></span>
<span class="strong"><strong>                        predict = svmBag$pred,</strong></span>
<span class="strong"><strong>                        aggregate = svmBag$aggregate)</strong></span>
</pre></div><p>By using this with the <code class="literal">train()</code> function and the training control object (<code class="literal">ctrl</code>), defined earlier, we can evaluate the <a id="id937" class="indexterm"/>bagged SVM model as follows (note that the <code class="literal">kernlab</code> package is required for this to work; you will need to install it if you have not done so previously):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; svmbag &lt;- train(default ~ ., data = credit, "bag",</strong></span>
<span class="strong"><strong>                  trControl = ctrl, bagControl = bagctrl)</strong></span>
<span class="strong"><strong>&gt; svmbag</strong></span>

<span class="strong"><strong>Bagged Model</strong></span>
<span class="strong"><strong>1000 samples</strong></span>
<span class="strong"><strong>  16 predictors</strong></span>
<span class="strong"><strong>   2 classes: 'no', 'yes' </strong></span>

<span class="strong"><strong>No pre-processing</strong></span>
<span class="strong"><strong>Resampling: Cross-Validation (10 fold) </strong></span>

<span class="strong"><strong>Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... </strong></span>

<span class="strong"><strong>Resampling results</strong></span>

<span class="strong"><strong>  Accuracy  Kappa      Accuracy SD  Kappa SD </strong></span>
<span class="strong"><strong>  0.728     0.2929505  0.04442222   0.1318101</strong></span>

<span class="strong"><strong>Tuning parameter 'vars' was held constant at a value of 35</strong></span>
</pre></div><p>Given that the kappa statistic is below 0.30, it seems that the bagged SVM model performs worse than the bagged decision tree model. It's worth pointing out that the standard deviation of the kappa statistic is fairly large compared to the bagged decision tree model. This <a id="id938" class="indexterm"/>suggests that the performance varies <a id="id939" class="indexterm"/>substantially among the folds in the cross-validation. Such variation may imply that the performance might be improved further by upping the number of models in the ensemble.</p></div><div class="section" title="Boosting"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec117"/>Boosting</h2></div></div></div><p>Another common <a id="id940" class="indexterm"/>ensemble-based method is called <span class="strong"><strong>boosting</strong></span> because it <a id="id941" class="indexterm"/>boosts the performance of weak learners to attain the performance of stronger learners. This method is based largely on the work of Robert Schapire and Yoav Freund, who have published extensively on the topic.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note40"/>Note</h3><p>For additional information on boosting, refer to Schapire RE, Freund Y. <span class="emphasis"><em>Boosting: Foundations and Algorithms</em></span>. Cambridge, MA, The MIT Press; 2012.</p></div></div><p>Similar to bagging, boosting uses ensembles of models trained on resampled data and a vote to determine the final prediction. There are two key distinctions. First, the resampled datasets in boosting are constructed specifically to generate complementary learners. Second, rather than giving each learner an equal vote, boosting gives each learner's vote a weight based on its past performance. Models that perform better have greater influence over the ensemble's final prediction.</p><p>Boosting will result in performance that is often quite better and certainly no worse than the best of the models in the ensemble. Since the models in the ensemble are built to be complementary, it is possible to increase ensemble performance to an arbitrary threshold simply by adding additional classifiers to the group, assuming that each classifier performs better than random chance. Given the obvious utility of this finding, boosting is thought to be one of the most significant discoveries in machine learning.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip135"/>Tip</h3><p>Although boosting can create a model that meets an arbitrarily low error rate, this may not always be reasonable in practice. For one, the performance gains are incrementally smaller as additional learners are added, making some thresholds practically infeasible. Additionally, the pursuit of pure accuracy may result in the model being overfitted to the training data and not generalizable to unseen data.</p></div></div><p>A boosting algorithm <a id="id942" class="indexterm"/>called <span class="strong"><strong>AdaBoost</strong></span> or <span class="strong"><strong>adaptive boosting</strong></span> was proposed by <a id="id943" class="indexterm"/>Freund and Schapire in 1997. The algorithm is based on the idea of generating weak learners that iteratively learn a larger portion of the difficult-to-classify examples by paying more attention (that is, giving more weight) to frequently misclassified examples.</p><p>Beginning from an <a id="id944" class="indexterm"/>unweighted dataset, the first classifier attempts to model the outcome. Examples that the classifier predicted correctly will be less likely to appear in the training dataset for the following classifier, and conversely, the difficult-to-classify examples will appear more frequently. As additional rounds of weak learners are added, they are trained on data with successively more difficult examples. The <a id="id945" class="indexterm"/>process continues until the desired overall error rate is reached or performance no longer improves. At that point, each classifier's vote is weighted according to its accuracy on the training data on which it was built.</p><p>Though boosting principles can be applied to nearly any type of model, the principles are most commonly used with decision trees. We already used boosting in this way in <a class="link" href="ch05.html" title="Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules">Chapter 5</a>, <span class="emphasis"><em>Divide and Conquer – Classification Using Decision Trees and Rules</em></span>, as a method to improve the performance of a C5.0 decision tree.</p><p>The <span class="strong"><strong>AdaBoost.M1</strong></span> algorithm provides another tree-based implementation of AdaBoost for classification. The <a id="id946" class="indexterm"/>AdaBoost.M1 algorithm can be found in the <code class="literal">adabag</code> package.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note41"/>Note</h3><p>For more information about the <code class="literal">adabag</code> package, refer to Alfaro E, Gamez M, Garcia N. <span class="emphasis"><em>adabag – an R package for classification with boosting and bagging</em></span>. Journal of Statistical Software. 2013; 54:1-35.</p></div></div><p>Let's create an <code class="literal">AdaBoost.M1</code> classifier for the credit data. The general syntax for this algorithm is similar to other modeling techniques:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; m_adaboost &lt;- boosting(default ~ ., data = credit)</strong></span>
</pre></div><p>As usual, the <code class="literal">predict()</code> function is applied to the resulting object to make predictions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; p_adaboost &lt;- predict(m_adaboost, credit)</strong></span>
</pre></div><p>Departing from convention, rather than returning a vector of predictions, this returns an object with information about the model. The predictions are stored in a sub-object called <code class="literal">class</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head(p_adaboost$class)</strong></span>
<span class="strong"><strong>[1] "no"  "yes" "no"  "no"  "yes" "no"</strong></span>
</pre></div><p>A confusion matrix can be found in the <code class="literal">confusion</code> sub-object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; p_adaboost$confusion</strong></span>
<span class="strong"><strong>               Observed Class</strong></span>
<span class="strong"><strong>Predicted Class  no yes</strong></span>
<span class="strong"><strong>            no  700   0</strong></span>
<span class="strong"><strong>            yes   0 300</strong></span>
</pre></div><p>Did you notice that the AdaBoost model made no mistakes? Before you get your hopes up, remember that the preceding confusion matrix is based on the model's performance on the training data. Since boosting allows the error rate to be reduced to an arbitrarily low level, the learner simply continued until it made no more errors. This likely resulted in overfitting on the training dataset.</p><p>For a more accurate <a id="id947" class="indexterm"/>assessment of performance on unseen data, we need to use another evaluation method. The <code class="literal">adabag</code> package provides a simple function to use 10-fold CV:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; adaboost_cv &lt;- boosting.cv(default ~ ., data = credit)</strong></span>
</pre></div><p>Depending on your <a id="id948" class="indexterm"/>computer's capabilities, this may take some time to run, during which it will log each iteration to screen. After it completes, we can view a more reasonable confusion matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; adaboost_cv$confusion</strong></span>
<span class="strong"><strong>               Observed Class</strong></span>
<span class="strong"><strong>Predicted Class  no yes</strong></span>
<span class="strong"><strong>            no  594 151</strong></span>
<span class="strong"><strong>            yes 106 149</strong></span>
</pre></div><p>We can find the kappa statistic using the <code class="literal">vcd</code> package as described in <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(vcd)</strong></span>
<span class="strong"><strong>&gt; Kappa(adaboost_cv$confusion)</strong></span>
<span class="strong"><strong>               value       ASE</strong></span>
<span class="strong"><strong>Unweighted 0.3606965 0.0323002</strong></span>
<span class="strong"><strong>Weighted   0.3606965 0.0323002</strong></span>
</pre></div><p>With a kappa of about 0.36, this is our best-performing credit scoring model yet. Let's see how it compares to one last ensemble method.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip136"/>Tip</h3><p>The AdaBoost.M1 algorithm can be tuned in <code class="literal">caret</code> by specifying <code class="literal">method = "AdaBoost.M1"</code>.</p></div></div></div><div class="section" title="Random forests"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec118"/>Random forests</h2></div></div></div><p>Another <a id="id949" class="indexterm"/>ensemble-based method called <span class="strong"><strong>random forests</strong></span> (or <span class="strong"><strong>decision </strong></span><a id="id950" class="indexterm"/>
<span class="strong"><strong>tree forests</strong></span>) focuses only on ensembles of decision trees. This <a id="id951" class="indexterm"/>method was championed by Leo Breiman and Adele Cutler, and combines the base principles of bagging with random feature selection to add additional diversity to the decision tree models. After the ensemble of trees (the forest) is generated, the model uses a vote to combine the trees' predictions.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note42"/>Note</h3><p>For more detail on how random forests are constructed, refer to Breiman L. <span class="emphasis"><em>Random Forests</em></span>. Machine Learning. 2001; 45:5-32.</p></div></div><p>Random forests combine <a id="id952" class="indexterm"/>versatility and power into a single machine learning approach. As the ensemble uses only a small, random portion of the full feature set, random forests can handle extremely large datasets, where the so-called "curse of dimensionality" might cause other models to fail. At the same time, its error rates for most learning tasks are on par with nearly any other method.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip137"/>Tip</h3><p>Although the term "Random Forests" is trademarked by Breiman and Cutler, the term is <a id="id953" class="indexterm"/>sometimes used colloquially to refer to any type of decision tree ensemble. A pedant would use the more general term "decision tree forests" except when referring to the specific implementation by Breiman and Cutler.</p></div></div><p>It's worth noting that relative to other ensemble-based methods, random forests are quite competitive and offer key advantages relative to the competition. For instance, random forests tend to be easier to use and less prone to overfitting. The following table lists the general strengths and weaknesses of random forest models:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strengths</p>
</th><th style="text-align: left" valign="bottom">
<p>Weaknesses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An all-purpose <a id="id954" class="indexterm"/>model that performs well on most problems</li><li class="listitem" style="list-style-type: disc">Can handle noisy or missing data as well as categorical or continuous features</li><li class="listitem" style="list-style-type: disc">Selects only <a id="id955" class="indexterm"/>the most important features</li><li class="listitem" style="list-style-type: disc">Can be used on data with an extremely large number of features or examples</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Unlike a decision tree, the model is not easily interpretable</li><li class="listitem" style="list-style-type: disc">May require some work to tune the model to the data</li></ul></div>
</td></tr></tbody></table></div><p>Due to their power, versatility, and ease of use, random forests are quickly becoming one of the most popular <a id="id956" class="indexterm"/>machine learning methods. Later on in this chapter, we'll <a id="id957" class="indexterm"/>compare a random forest model head-to-head against <a id="id958" class="indexterm"/>the boosted C5.0 tree.</p><div class="section" title="Training random forests"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec65"/>Training random forests</h3></div></div></div><p>Though there <a id="id959" class="indexterm"/>are several packages to create random forests in R, the <code class="literal">randomForest</code> package is perhaps the implementation that is most faithful to the specification by Breiman and Cutler, and is also supported by <code class="literal">caret</code> for automated tuning. The syntax for training this model is as follows:</p><div class="mediaobject"><img src="graphics/B03905_11_05.jpg" alt="Training random forests"/></div><p>By default, the <code class="literal">randomForest()</code> function creates an ensemble of 500 trees that consider <code class="literal">sqrt(p)</code> random <a id="id960" class="indexterm"/>features at each split, where <code class="literal">p</code> is the number of features in the training dataset and <code class="literal">sqrt()</code> refers to R's square root function. Whether or not these default parameters are appropriate depends on the nature of the learning task and training data. Generally, more complex learning problems and larger datasets (either more features or more examples) work better with a larger number of trees, though this needs to be balanced with the computational expense of training more trees.</p><p>The goal of using a large number of trees is to train enough so that each feature has a chance to appear in several models. This is the basis of the <code class="literal">sqrt(p)</code> default value for the <code class="literal">mtry</code> parameter; using this value limits the features sufficiently so that substantial random variation occurs from tree-to-tree. For example, since the credit data has 16 features, each tree would be limited to splitting on four features at any time.</p><p>Let's see how the default <code class="literal">randomForest()</code> parameters work with the credit data. We'll train the model just as <a id="id961" class="indexterm"/>we did with other learners. Again, the <code class="literal">set.seed()</code> function ensures that the result can be replicated:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(randomForest)</strong></span>
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; rf &lt;- randomForest(default ~ ., data = credit)</strong></span>
</pre></div><p>To look at a summary of the model's performance, we can simply type the resulting object's name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; rf</strong></span>

<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong> randomForest(formula = default ~ ., data = credit) </strong></span>
<span class="strong"><strong>               Type of random forest: classification</strong></span>
<span class="strong"><strong>                     Number of trees: 500</strong></span>
<span class="strong"><strong>No. of variables tried at each split: 4</strong></span>

<span class="strong"><strong>        OOB estimate of error rate: 23.8%</strong></span>
<span class="strong"><strong>Confusion matrix:</strong></span>
<span class="strong"><strong>     no yes class.error</strong></span>
<span class="strong"><strong>no  640  60  0.08571429</strong></span>
<span class="strong"><strong>yes 178 122  0.59333333</strong></span>
</pre></div><p>The output notes that the random forest included 500 trees and tried four variables at each split, just as we expected. At first glance, you might be alarmed at the seemingly poor performance according to the confusion matrix—the error rate of 23.8 percent is far worse than the resubstitution error of any of the other ensemble methods so far. However, this confusion matrix does not show resubstitution error. Instead, it reflects the <span class="strong"><strong>out-of-bag error rate</strong></span> (listed in the output as <code class="literal">OOB estimate of error rate</code>), which unlike resubstitution error, is <a id="id962" class="indexterm"/>an unbiased estimate of the test set error. This means that it should be a fairly reasonable estimate of future performance.</p><p>The out-of-bag estimate is computed during the construction of the random forest. Essentially, any example not selected for a single tree's bootstrap sample can be used to test the model's performance on unseen data. At the end of the forest construction, the predictions for each example each time it was held out are tallied, and a vote is taken to determine the final <a id="id963" class="indexterm"/>prediction for the example. The total error rate of such predictions becomes the out-of-bag error rate.</p></div><div class="section" title="Evaluating random forest performance"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec66"/>Evaluating random forest performance</h3></div></div></div><p>As mentioned <a id="id964" class="indexterm"/>previously, the <code class="literal">randomForest()</code> function is supported by <code class="literal">caret</code>, which allows us to optimize the model while, at the same time, calculating performance measures beyond the out-of-bag error rate. To make things interesting, let's compare an auto-tuned random forest to the best auto-tuned boosted C5.0 model we've developed. We'll treat this experiment as if we were hoping to identify a candidate model for submission to a machine learning competition.</p><p>We must first load <code class="literal">caret</code> and set our training control options. For the most accurate comparison of model performance, we'll use repeated 10-fold cross-validation, or 10-fold CV repeated 10 times. This means that the models will take a much longer time to build and will be more computationally intensive to evaluate, but since this is our final comparison we should be <span class="emphasis"><em>very</em></span> sure that we're making the right choice; the winner of this showdown will be our only entry into the machine learning competition.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
<span class="strong"><strong>&gt; ctrl &lt;- trainControl(method = "repeatedcv",</strong></span>
<span class="strong"><strong>                       number = 10, repeats = 10)</strong></span>
</pre></div><p>Next, we'll set up the tuning grid for the random forest. The only tuning parameter for this model is <code class="literal">mtry</code>, which defines how many features are randomly selected at each split. By default, we know that the random forest will use <code class="literal">sqrt(16)</code>, or four features per tree. To be thorough, we'll also test values half of that, twice that, as well as the full set of 16 features. Thus, we need to create a grid with values of <code class="literal">2</code>, <code class="literal">4</code>, <code class="literal">8</code>, and <code class="literal">16</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; grid_rf &lt;- expand.grid(.mtry = c(2, 4, 8, 16))</strong></span>
</pre></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip138"/>Tip</h3><p>A random forest that considers the full set of features at each split is essentially the same as a bagged decision tree model.</p></div></div><p>We can supply the resulting grid to the <code class="literal">train()</code> function with the <code class="literal">ctrl</code> object as follows. We'll use the kappa metric to select the best model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; m_rf &lt;- train(default ~ ., data = credit, method = "rf",</strong></span>
<span class="strong"><strong>                metric = "Kappa", trControl = ctrl,</strong></span>
<span class="strong"><strong>                tuneGrid = grid_rf)</strong></span>
</pre></div><p>The preceding command may take some time to complete as it has quite a bit of work to do! When it finishes, we'll compare that to a boosted tree using <code class="literal">10</code>, <code class="literal">20</code>, <code class="literal">30</code>, and <code class="literal">40</code> iterations:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; grid_c50 &lt;- expand.grid(.model = "tree",</strong></span>
<span class="strong"><strong>                          .trials = c(10, 20, 30, 40),</strong></span>
<span class="strong"><strong>                          .winnow = "FALSE")</strong></span>
<span class="strong"><strong>&gt; set.seed(300)</strong></span>
<span class="strong"><strong>&gt; m_c50 &lt;- train(default ~ ., data = credit, method = "C5.0",</strong></span>
<span class="strong"><strong>                 metric = "Kappa", trControl = ctrl,</strong></span>
<span class="strong"><strong>                 tuneGrid = grid_c50)</strong></span>
</pre></div><p>When the C5.0 <a id="id965" class="indexterm"/>decision tree finally completes, we can compare the two approaches side-by-side. For the random forest model, the results are:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; m_rf</strong></span>

<span class="strong"><strong>Resampling results across tuning parameters:</strong></span>

<span class="strong"><strong>  mtry  Accuracy  Kappa      Accuracy SD  Kappa SD  </strong></span>
<span class="strong"><strong>   2    0.7247    0.1284142  0.01690466   0.06364740</strong></span>
<span class="strong"><strong>   4    0.7499    0.2933332  0.02989865   0.08768815</strong></span>
<span class="strong"><strong>   8    0.7539    0.3379986  0.03107160   0.08353988</strong></span>
<span class="strong"><strong>  16    0.7556    0.3613151  0.03379439   0.08891300</strong></span>
</pre></div><p>For the boosted C5.0 model, the results are:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; m_c50</strong></span>

<span class="strong"><strong>Resampling results across tuning parameters:</strong></span>

<span class="strong"><strong>  trials  Accuracy  Kappa      Accuracy SD  Kappa SD  </strong></span>
<span class="strong"><strong>  10      0.7325    0.3215655  0.04021093   0.09519817</strong></span>
<span class="strong"><strong>  20      0.7343    0.3268052  0.04033333   0.09711408</strong></span>
<span class="strong"><strong>  30      0.7381    0.3343137  0.03672709   0.08942323</strong></span>
<span class="strong"><strong>  40      0.7388    0.3335082  0.03934514   0.09746073</strong></span>
</pre></div><p>With a kappa of about 0.361, the random forest model with <code class="literal">mtry = 16</code> was the winner among these eight models. It was higher than the best C5.0 decision tree, which had a kappa of about 0.334, and slightly higher than the <code class="literal">AdaBoost.M1</code> model with a kappa of about 0.360. Based on these results, we would submit the random forest as our final model. Without actually evaluating the model on the competition data, we have no way of knowing for sure <a id="id966" class="indexterm"/>whether it will end up winning, but given our performance estimates, it's the safer bet. With a bit of luck, perhaps we'll come away with the prize.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec49"/>Summary</h1></div></div></div><p>After reading this chapter, you should now know the base techniques that are used to win data mining and machine learning competitions. Automated tuning methods can assist with squeezing every bit of performance out of a single model. On the other hand, performance gains are also possible by creating groups of machine learning models that work together.</p><p>Although this chapter was designed to help you prepare competition-ready models, note that your fellow competitors have access to the same techniques. You won't be able to get away with stagnancy; therefore, continue to add proprietary methods to your bag of tricks. Perhaps you can bring unique subject-matter expertise to the table, or perhaps your strengths include an eye for detail in data preparation. In any case, practice makes perfect, so take advantage of open competitions to test, evaluate, and improve your own machine learning skillset.</p><p>In the next chapter—the last in this book—we'll take a bird's eye look at ways to apply machine learning to some highly specialized and difficult domains using R. You'll gain the knowledge needed to apply machine learning to tasks at the cutting edge of the field.</p></div></body></html>
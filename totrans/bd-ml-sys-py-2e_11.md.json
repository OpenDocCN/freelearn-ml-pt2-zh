["```py\n>>> from scipy.stats import pearsonr\n>>> pearsonr([1,2,3], [1,2,3.1])\n>>> (0.99962228516121843, 0.017498096813278487)\n>>> pearsonr([1,2,3], [1,20,6])\n>>> (0.25383654128340477, 0.83661493668227405)\n\n```", "```py\n>>> from sklearn.feature_selection import RFE\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.datasets import make_classification\n>>> X,y = make_classification(n_samples=100, n_features=10, n_informative=3, random_state=0)\n>>> clf = LogisticRegression()\n>>> clf.fit(X, y)\n>>> selector = RFE(clf, n_features_to_select=3)\n>>> selector = selector.fit(X, y)\n>>> print(selector.support_)\n[False  True False  True False False False False  True False]\n>>> print(selector.ranking_)\n[4 1 3 1 8 5 7 6 1 2]\n\n```", "```py\n>>> x1 = np.arange(0, 10, .2)\n>>> x2 = x1+np.random.normal(loc=0, scale=1, size=len(x1))\n>>> X = np.c_[(x1, x2)]\n>>> good = (x1>5) | (x2>5) # some arbitrary classes\n>>> bad = ~good # to make the example look good\n\n```", "```py\n>>> from sklearn import linear_model, decomposition, datasets\n>>> pca = decomposition.PCA(n_components=1)\n\n```", "```py\n>>> Xtrans = pca.fit_transform(X)\n\n```", "```py\n>>> print(pca.explained_variance_ratio_)\n>>> [ 0.96393127]\n\n```", "```py\n>>> from sklearn import lda\n>>> lda_inst = lda.LDA(n_components=1)\n>>> Xtrans = lda_inst.fit_transform(X, good)\n\n```", "```py\n>>> X = np.c_[np.ones(5), 2 * np.ones(5), 10 * np.ones(5)].T\n>>> print(X)\n[[  1\\.   1\\.   1\\.   1\\.   1.]\n [  2\\.   2\\.   2\\.   2\\.   2.]\n [ 10\\.  10\\.  10\\.  10\\.  10.]]\n\n```", "```py\n>>> from sklearn import manifold\n>>> mds = manifold.MDS(n_components=3)\n>>> Xtrans = mds.fit_transform(X)\n\n```"]
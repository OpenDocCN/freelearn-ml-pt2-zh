- en: Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network – multilayer perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking with a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks and deep learning have been incredibly popular recently as they
    have solved tough problems and perhaps have become a significant part of the public
    face of artificial intelligence. Let's explore the feed-forward neural networks
    available in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With scikit-learn, you can explore the perceptron classifier and relate it to
    other classification procedures within scikit-learn. Additionally, perceptrons
    are the building blocks of neural networks, which are a very prominent part of
    machine learning, particularly computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get started. The process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the UCI diabetes classification dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into training and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import a perceptron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate the perceptron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then train the perceptron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try the perceptron on the testing set or preferably compute `cross_val_score`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the UCI diabetes dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You have loaded `X`, the set of input features, and `y`, the variable we desired
    to predict. Split `X` and `y` into testing and training sets. Do this by stratifying
    the target set, keeping the classes in balanced proportions in both training and
    testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scale the set of features. Perform the scaling operation on the training set
    only and then continue with the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate and train the perceptron on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Measure the cross-validation score. Pass `roc_auc` as the cross-validation
    scoring mechanism. Additionally, use a stratified k-fold by setting `cv=skf`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Measure the performance on the test set. Import two metrics, `accuracy_score`
    and `roc_auc_score`, from the `sklearn.metrics` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The test finished relatively quickly. It performed okay, a bit worse than logistic
    regression, which was 75% accurate (this is an estimate; we cannot compare the
    logistic regression from any previous chapter because the training/testing split
    is different).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron is a simplification of a neuron in the brain. In the following
    diagram, the perceptron receives inputs *x[1],* and *x[2]* from the left. A bias
    term*, w*[0]*,* and weights *w[1]* and *w[2]* are computed. The terms *x*[i] and
    *w*[i] form a linear function. This linear function is then passed to an activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following activation function, if the sum of the dot product of the
    weight and input vector is less than zero, an individual row is classified as
    0; otherwise it is classified as 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56833da2-8df6-4a37-a6cb-5133ddeda3c3.png)'
  prefs: []
  type: TYPE_IMG
- en: This happens in a single epoch, or iteration, passing through the perceptron.
    The process repeats through several iterations and weights are readjusted each
    time, minimizing the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: With regard to perceptrons and the current state of neural networks, they work
    well as researchers have tried many things. In practice, they currently work well
    with the computational power available now.
  prefs: []
  type: TYPE_NORMAL
- en: As computing power keeps increasing, neural networks and perceptrons become
    capable of solving increasingly difficult problems and training times keep decreasing
    and decreasing.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try running a grid search by varying the perceptron's hyperparameters. Some
    notable parameters include regularization parameters, `penalty` and `alpha`, `class_weight`,
    and `max_iter`. The `class_weight` parameter deals well with unbalanced classes
    by giving more weight to the underrepresented classes. The `max_iter` parameter refers
    to the maximum number of passes through the perceptron. In general, the higher
    its value the better, so we set it to 50\. (Note that this is the code for scikit-learn
    0.19.0\. In scikit-learn 0.18.1, use the `n_iter` parameter instead of the `max_iter`
    parameter.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Try the following grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the best parameters and the best score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Varying hyperparameters using cross-validation has improved the results. Now
    try to use bagging with a set of perceptrons as follows. Start by noticing and
    picking the best perceptron from the perceptron grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the new cross-validation score and best parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Thus, a bag of perceptrons scores better than a single perceptron for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network – multilayer perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using a neural network in scikit-learn is straightforward and proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale the data with a standard scaler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do a hyperparameter search. Begin by varying the alpha parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the medium-sized California housing dataset that we used in [Chapter 9](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml),
    *Tree Algorithms and Ensembles*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Bin the target variable so that the target train set and target test set are
    a bit more similar. Then use a stratified train/test split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Begin by scaling the input variables. Train the scaler only on the training
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, perform the scaling on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, perform a randomized search (or grid search if you prefer) to find
    a reasonable value for `alpha`, one that scores well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the context of neural networks, the single perceptrons look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a494cfb8-d5cd-4eea-8df9-c8190c0558f0.png)'
  prefs: []
  type: TYPE_IMG
- en: The output is a function of a sum of the dot product of weights and inputs.
    The function *f* is the activation function and can be a sigmoid curve, for example.
    In the neural network, hyperparameter activation refers to this function. In scikit-learn,
    there are the options of identity, logistic, tanh, and relu, where logistic is
    the sigmoid curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole network looks like this (the following is a diagram from the scikit
    documentation at [http://scikit-learn.org/stable/modules/neural_networks_supervised.html](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1eefe899-179d-46b0-81eb-c8adbda3965c.png)'
  prefs: []
  type: TYPE_IMG
- en: It is instructive to use a neural network on a dataset we are familiar with,
    the California housing dataset. The California housing dataset seemed to favor
    nonlinear algorithms, particularly trees and ensembles of trees. Trees did well
    on the dataset and established a benchmark as to how well algorithms can do on
    that dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, neural networks did okay but not nearly as well as gradient boosting
    machines. Additionally, they were computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Philosophical thoughts on neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks are mathematically universal function approximators and can
    learn any function. Also, the hidden layers are often interpreted as the network
    learning the intermediate steps of a process without a human having to program
    the intermediate steps. This can come from convolutional neural networks in computer
    vision, where it is easy to see how the neural network figures out each layer.
  prefs: []
  type: TYPE_NORMAL
- en: These facts make an interesting mental image and can be applied to other estimators.
    Many people do not tend to think of random forests as trees figuring out processes
    tree level by tree level, or tree by tree (perhaps because their structure is
    not as organized and random forests do not invoke visualizations of the biological
    brain). In more practical detail, if you wanted to organize random forests, you
    can limit their depth or perhaps use gradient boosting machines.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the hard facts present or not in the idea of a neural network
    truly being intelligent, it is helpful to carry around such mental images as the
    field progresses and machines become smarter and smarter. Carry the idea around,
    yet focus on the results as they are; that's what machine learning is about now.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking with a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The two most common meta-learning methods are bagging and boosting. Stacking
    is less widely used; yet it is powerful because one can combine models of different
    types. All three methods create a stronger estimator from a set of not-so-strong
    estimators. We tried the stacking procedure in [Chapter 9](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml),
    *Tree Algorithms and Ensembles*. Here, we try it with a neural network mixed with
    other models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process for stacking is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the dataset into training and testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the training set into two sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train base learners on the first part of the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions using the base learners on the second part of the training
    set. Store these prediction vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the stored prediction vectors as inputs and the target variable as output.
    Train a higher level learner (note that we are still on the second part of the
    training set).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, you can view the results of the overall process on the test set
    (note that you cannot select a model by viewing results on the test set).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the California housing dataset and the libraries we have been using:
    `numpy`, `pandas`, and `matplotlib`. It is a medium-sized dataset but is large
    relative to the other scikit-learn datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Bin the target variable to increase the balance in splitting the dataset in
    regards to the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the dataset `X` and `y` into three sets. `X_1` and `X_stack` refer to
    the input variables of the first and second training sets, respectively. `y_1`
    and `y_stack` refer to the output target variables of the first and second training
    sets respectively. The test set consists of `X_test_prin` and `y_test_prin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Another option is to use `StratifiedShuffleSplit` from the `model_selection`
    module in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to use three base regressors: a neural network, a single gradient
    boosting machine, and a bag of gradient boosting machines.'
  prefs: []
  type: TYPE_NORMAL
- en: First base model – neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add a neural network by performing a cross-validated grid search on the first
    training set: `X_1`, the inputs, and `y_1` the target set. This will find the
    best parameters of the neural network for this dataset. We are only varying the
    `alpha` parameter in this example. Do not forget to scale the inputs or else the
    network will not run well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'View the best parameters and the best score of the grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Pickle the neural network that performed the best during the grid search. This
    will save the training we have done so that we do not have to keep doing it several
    times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Second base model – gradient boost ensemble
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform a randomized grid search on gradient-boosted trees:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'View the best score and parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Increase the number of estimators and train:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Pickle the estimator. For convenience and reusability, the pickling code is
    wrapped into a single function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Third base model – bagging regressor of gradient boost ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, perform a small grid search for a bag of gradient-boosted trees. It is
    hard to know from a theoretical viewpoint whether this type of ensemble will do
    well. For the purpose of stacking, it will do well enough if it is not too correlated
    with the other base estimators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'View the best parameters and score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Pickle the best estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Some functions of the stacker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use functions similar to [Chapter 9](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml),
    *Tree Algorithms and Ensembles*. The `handle_X_set` function creates a dataframe
    of the prediction vectors on the `X_stack` set. Conceptually, it refers to the
    fourth step of predictions on the second part of the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you pickled the files previously and want to start at this step, unpickle
    the files. The following files are loaded with the correct filenames and variable
    names to perform the `handle_X_set` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a dataframe of predictions using the `handle_X_set` function. Print
    the Pearson correlation between the prediction vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Meta-learner – extra trees regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to [Chapter 9](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml), *Tree Algorithms
    and Ensembles*, train an extra tree regressor on the dataframe of predictions.
    Use `y_stack` as the target vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'View the best parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the extra trees regressor but increase the number of estimators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'View the `final_etr` estimator''s cross-validation performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'View the performance on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Perhaps we can increase the results even further. Place the training columns
    alongside the prediction vectors. Start by modifying the functions we have been
    using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Continue the training of the extra trees regressor as before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We continue as we did previously. View the best parameters and train a model
    with more estimators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'View cross-validation performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We included the original input columns in the training of the high-level learner
    of the stacker. The cross-validation performance has increased to 0.8297 from
    0.8221\. Thus, we conclude that the model that includes the input columns is the
    best model. Now, after we have selected this model as the final best model, we
    look at the performance of the estimator on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After trying out neural networks on scikit-learn, you can try packages such
    as `skflow`, which borrows the syntax of scikit-learn yet utilizes Google's powerful
    open source TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: In regards to stacking, you can try cross-validation performance and prediction
    on the whole training set `X_train_prin`, instead of splitting it into two parts,
    `X_1` and `X_stack`.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of packages in data science borrow heavily from either scikit-learn's
    or R's syntaxes.
  prefs: []
  type: TYPE_NORMAL

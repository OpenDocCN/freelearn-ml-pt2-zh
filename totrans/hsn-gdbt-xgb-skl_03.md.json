["```py\n    import pandas as pd\n    import numpy as np\n    import warnings\n    warnings.filterwarnings('ignore')\n    ```", "```py\n    df_census = pd.read_csv('census_cleaned.csv')\n    ```", "```py\n    X = df_census.iloc[:,:-1]\n    y = df_census.iloc[:,-1]\n    ```", "```py\n    from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n    ```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n```", "```py\n    clf = DecisionTreeClassifier(random_state=2)\n    ```", "```py\n    clf.fit(X_train, y_train)\n    ```", "```py\n    y_pred = clf.predict(X_test)\n    ```", "```py\n    accuracy_score(y_pred, y_test) \n    ```", "```py\n    0.8131679154894976\n    ```", "```py\n    df_bikes = pd.read_csv('bike_rentals_cleaned.csv')X_bikes = df_bikes.iloc[:,:-1]y_bikes = df_bikes.iloc[:,-1]\n    ```", "```py\n    from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import cross_val_score\n    ```", "```py\n    reg = DecisionTreeRegressor(random_state=2)\n    scores = cross_val_score(reg, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=5)\n    ```", "```py\nreg = DecisionTreeRegressor()reg.fit(X_train, y_train)y_pred = reg.predict(X_train)\nfrom sklearn.metrics import mean_squared_error reg_mse = mean_squared_error(y_train, y_pred)reg_rmse = np.sqrt(reg_mse)reg_rmse\n```", "```py\n0.0\n```", "```py\nfrom sklearn.model_selection import GridSearchCV params = {'max_depth':[None,2,3,4,6,8,10,20]}\n```", "```py\nreg = DecisionTreeRegressor(random_state=2)grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)grid_reg.fit(X_train, y_train)\n```", "```py\nbest_params = grid_reg.best_params_print(\"Best params:\", best_params)\n```", "```py\nBest params: {'max_depth': 6}\n```", "```py\nbest_score = np.sqrt(-grid_reg.best_score_)print(\"Training score: {:.3f}\".format(best_score))\n```", "```py\nTraining score: 951.938\n```", "```py\nbest_model = grid_reg.best_estimator_\ny_pred = best_model.predict(X_test) \nrmse_test = mean_squared_error(y_test, y_pred)**0.5\nprint('Test score: {:.3f}'.format(rmse_test))\n```", "```py\nTest score: 864.670\n```", "```py\ndef grid_search(params, reg=DecisionTreeRegressor(random_state=2)):\n    grid_reg = GridSearchCV(reg, params,   \n    scoring='neg_mean_squared_error', cv=5, n_jobs=-1):\n    grid_reg.fit(X_train, y_train)\n       best_params = grid_reg.best_params_    print(\"Best params:\", best_params)    best_score = np.sqrt(-grid_reg.best_score_)    print(\"Training score: {:.3f}\".format(best_score))\n    y_pred = grid_reg.predict(X_test)    rmse_test = mean_squared_error(y_test, y_pred)**0.5\n    print('Test score: {:.3f}'.format(rmse_test))\n```", "```py\nX_train.shape\n```", "```py\n(548, 12)\n```", "```py\ngrid_search(params={'min_samples_leaf':[1, 2, 4, 6, 8, 10, 20, 30]})\n```", "```py\nBest params: {'min_samples_leaf': 8}\nTraining score: 896.083\nTest score: 855.620\n```", "```py\ngrid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})\n```", "```py\nBest params: {'max_depth': 6, 'min_samples_leaf': 2}\nTraining score: 870.396\nTest score: 913.000\n```", "```py\ngrid_search(params={'max_depth':[6,7,8,9,10],'min_samples_leaf':[3,5,7,9]})\n```", "```py\nBest params: {'max_depth': 9, 'min_samples_leaf': 7}\nTraining score: 888.905\nTest score: 878.538\n```", "```py\ndf_heart = pd.read_csv('heart_disease.csv')df_heart.head()\n```", "```py\nX = df_heart.iloc[:,:-1]y = df_heart.iloc[:,-1]from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n```", "```py\nmodel = DecisionTreeClassifier(random_state=2)\nscores = cross_val_score(model, X, y, cv=5)\nprint('Accuracy:', np.round(scores, 2))\nprint('Accuracy mean: %0.2f' % (scores.mean()))\nAccuracy: [0.74 0.85 0.77 0.73 0.7 ]\n```", "```py\nAccuracy mean: 0.76\n```", "```py\ndef randomized_search_clf(params, runs=20, clf=DecisionTreeClassifier(random_state=2)):    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs,    cv=5, n_jobs=-1, random_state=2)    rand_clf.fit(X_train, y_train)\n    best_model = rand_clf.best_estimator_\n    best_score = rand_clf.best_score_  \n    print(\"Training score: {:.3f}\".format(best_score))\n    y_pred = best_model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print('Test score: {:.3f}'.format(accuracy))\n    return best_model\n```", "```py\nrandomized_search_clf(params={'criterion':['entropy', 'gini'],'splitter':['random', 'best'], 'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01],'min_samples_split':[2, 3, 4, 5, 6, 8, 10],'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],'max_features':['auto', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],'max_depth':[None, 2,4,6,8],'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]})\nTraining score: 0.798\nTest score: 0.855\nDecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=8, max_features=0.8, max_leaf_nodes=45, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.04, min_samples_split=10,min_weight_fraction_leaf=0.05, presort=False, random_state=2, splitter='best')\n```", "```py\nrandomized_search_clf(params={'max_depth':[None, 6, 7],'max_features':['auto', 0.78], 'max_leaf_nodes':[45, None], 'min_samples_leaf':[1, 0.035, 0.04, 0.045, 0.05],'min_samples_split':[2, 9, 10],'min_weight_fraction_leaf': [0.0, 0.05, 0.06, 0.07],}, runs=100)\nTraining score: 0.802\nTest score: 0.868\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,max_features=0.78, max_leaf_nodes=45, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.045, min_samples_split=9, min_weight_fraction_leaf=0.06, presort=False, random_state=2, splitter='best')\n```", "```py\nmodel = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7, max_features=0.78, max_leaf_nodes=45, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.045, min_samples_split=9, min_weight_fraction_leaf=0.06, presort=False, random_state=2, splitter='best')\nscores = cross_val_score(model, X, y, cv=5)\nprint('Accuracy:', np.round(scores, 2))\nprint('Accuracy mean: %0.2f' % (scores.mean()))\nAccuracy: [0.82 0.9  0.8  0.8  0.78]\n```", "```py\nAccuracy mean: 0.82\n```", "```py\nbest_clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=9,max_features=0.8, max_leaf_nodes=47,min_impurity_decrease=0.0, min_impurity_split=None,min_samples_leaf=1, min_samples_split=8,min_weight_fraction_leaf=0.05, presort=False,random_state=2, splitter='best')\nbest_clf.fit(X, y)\n```", "```py\nbest_clf.feature_importances_\narray([0.04826754, 0.04081653, 0.48409586, 0.00568635, 0.        , 0., 0., 0.00859483, 0., 0.02690379, 0., 0.18069065, 0.20494446])\n```", "```py\nfeature_dict = dict(zip(X.columns, best_clf.feature_importances_))\n# Import operator import operator\nSort dict by values (as list of tuples)sorted(feature_dict.items(), key=operator.itemgetter(1), reverse=True)[0:3]\n[('cp', 0.4840958610240171),\n ('thal', 0.20494445570568706),\n ('ca', 0.18069065321397942)]\n```"]
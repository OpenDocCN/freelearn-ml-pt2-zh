- en: Chapter 4. Probabilistic Learning – Classification Using Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a meteorologist provides a weather forecast, precipitation is typically
    described with terms such as "70 percent chance of rain." Such forecasts are known
    as probability of precipitation reports. Have you ever considered how they are
    calculated? It is a puzzling question, because in reality, either it will rain
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: Weather estimates are based on probabilistic methods or those concerned with
    describing uncertainty. They use data on past events to extrapolate future events.
    In the case of weather, the chance of rain describes the proportion of prior days
    to similar measurable atmospheric conditions in which precipitation occurred.
    A 70 percent chance of rain implies that in 7 out of the 10 past cases with similar
    conditions, precipitation occurred somewhere in the area.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the Naive Bayes algorithm, which uses probabilities in
    much the same way as a weather forecast. While studying this method, you will
    learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic principles of probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specialized methods and data structures needed to analyze text data with
    R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to employ Naive Bayes to build an SMS junk message filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you've taken a statistics class before, some of the material in this chapter
    may be a review. Even so, it may be helpful to refresh your knowledge on probability,
    as these principles are the basis of how Naive Bayes got such a strange name.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic statistical ideas necessary to understand the Naive Bayes algorithm
    have existed for centuries. The technique descended from the work of the 18^(th)
    century mathematician Thomas Bayes, who developed foundational principles to describe
    the probability of events, and how probabilities should be revised in the light
    of additional information. These principles formed the foundation for what are
    now known as **Bayesian methods**.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover these methods in greater detail later on. But, for now, it suffices
    to say that a probability is a number between 0 and 1 (that is, between 0 percent
    and 100 percent), which captures the chance that an event will occur in the light
    of the available evidence. The lower the probability, the less likely the event
    is to occur. A probability of 0 indicates that the event will definitely not occur,
    while a probability of 1 indicates that the event will occur with 100 percent
    certainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classifiers based on Bayesian methods utilize training data to calculate an
    observed probability of each outcome based on the evidence provided by feature
    values. When the classifier is later applied to unlabeled data, it uses the observed
    probabilities to predict the most likely class for the new features. It''s a simple
    idea, but it results in a method that often has results on par with more sophisticated
    algorithms. In fact, Bayesian classifiers have been used for:'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification, such as junk e-mail (spam) filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intrusion or anomaly detection in computer networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnosing medical conditions given a set of observed symptoms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, Bayesian classifiers are best applied to problems in which the information
    from numerous attributes should be considered simultaneously in order to estimate
    the overall probability of an outcome. While many machine learning algorithms
    ignore features that have weak effects, Bayesian methods utilize all the available
    evidence to subtly change the predictions. If large number of features have relatively
    minor effects, taken together, their combined impact could be quite large.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of Bayesian methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before jumping into the Naive Bayes algorithm, it's worth spending some time
    defining the concepts that are used across Bayesian methods. Summarized in a single
    sentence, Bayesian probability theory is rooted in the idea that the estimated
    likelihood of an **event**, or a potential outcome, should be based on the evidence
    at hand across multiple **trials**, or opportunities for the event to occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table illustrates events and trials for several real-world outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Event | Trial |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Heads result | Coin flip |'
  prefs: []
  type: TYPE_TB
- en: '| Rainy weather | A single day |'
  prefs: []
  type: TYPE_TB
- en: '| Message is spam | Incoming e-mail message |'
  prefs: []
  type: TYPE_TB
- en: '| Candidate becomes president | Presidential election |'
  prefs: []
  type: TYPE_TB
- en: '| Win the lottery | Lottery ticket |'
  prefs: []
  type: TYPE_TB
- en: Bayesian methods provide insights into how the probability of these events can
    be estimated from the observed data. To see how, we'll need to formalize our understanding
    of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The probability of an event is estimated from the observed data by dividing
    the number of trials in which the event occurred by the total number of trials.
    For instance, if it rained 3 out of 10 days with similar conditions as today,
    the probability of rain today can be estimated as *3 / 10 = 0.30* or 30 percent.
    Similarly, if 10 out of 50 prior email messages were spam, then the probability
    of any incoming message being spam can be estimated as *10 / 50 = 0.20* or 20
    percent.
  prefs: []
  type: TYPE_NORMAL
- en: To denote these probabilities, we use notation in the form *P(A)*, which signifies
    the probability of event *A*. For example, *P(rain) = 0.30* and *P(spam) = 0.20*.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of all the possible outcomes of a trial must always sum to 1,
    because a trial always results in some outcome happening. Thus, if the trial has
    two outcomes that cannot occur simultaneously, such as rainy versus sunny or spam
    versus ham (nonspam), then knowing the probability of either outcome reveals the
    probability of the other. For example, given the value *P(spam) = 0.20*, we can
    calculate *P(ham) = 1 – 0.20 = 0.80*. This concludes that spam and ham are **mutually
    exclusive and exhaustive** events, which implies that they cannot occur at the
    same time and are the only possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Because an event cannot simultaneously happen and not happen, an event is always
    mutually exclusive and exhaustive with its **complement**, or the event comprising
    of the outcomes in which the event of interest does not happen. The complement
    of event *A* is typically denoted *A^c* or *A'*. Additionally, the shorthand notation
    *P(¬A)* can used to denote the probability of event *A* not occurring, as in *P(¬spam)
    = 0.80*. This notation is equivalent to *P(A^c)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate events and their complements, it is often helpful to imagine
    a two-dimensional space that is partitioned into probabilities for each event.
    In the following diagram, the rectangle represents the possible outcomes for an
    e-mail message. The circle represents the 20 percent probability that the message
    is spam. The remaining 80 percent represents the complement *P(¬spam)* or the
    messages that are not spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding probability](img/B03905_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding joint probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Often, we are interested in monitoring several nonmutually exclusive events
    for the same trial. If certain events occur with the event of interest, we may
    be able to use them to make predictions. Consider, for instance, a second event
    based on the outcome that an e-mail message contains the word Viagra. In most
    cases, this word is likely to appear only in a spam message; its presence in an
    incoming e-mail is therefore a very strong piece of evidence that the message
    is spam. The preceding diagram, updated for this second event, might appear as
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding joint probability](img/B03905_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice in the diagram that the Viagra circle does not completely fill the spam
    circle, nor is it completely contained by the spam circle. This implies that not
    all spam messages contain the word Viagra and not every e-mail with the word Viagra
    is spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'To zoom in for a closer look at the overlap between the spam and Viagra circles,
    we''ll employ a visualization known as a **Venn diagram**. First used in the late
    19th century by John Venn, the diagram uses circles to illustrate the overlap
    between sets of items. In most Venn diagrams, the size of the circles and the
    degree of the overlap is not meaningful. Instead, it is used as a reminder to
    allocate probability to all possible combinations of events:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding joint probability](img/B03905_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We know that 20 percent of all messages were spam (the left circle) and 5 percent
    of all messages contained the word Viagra (the right circle). We would like to
    quantify the degree of overlap between these two proportions. In other words,
    we hope to estimate the probability that both *P(spam)* and *P(Viagra)* occur,
    which can be written as *P(spam ∩ Viagra)*. The upside down 'U' symbol signifies
    the **intersection** of the two events; the notation *A ∩ B* refers to the event
    in which both *A* and *B* occur.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating *P(spam ∩ Viagra)* depends on the **joint probability** of the two
    events or how the probability of one event is related to the probability of the
    other. If the two events are totally unrelated, they are called **independent
    events**. This is not to say that independent events cannot occur at the same
    time; event independence simply implies that knowing the outcome of one event
    does not provide any information about the outcome of the other. For instance,
    the outcome of a heads result on a coin flip is independent from whether the weather
    is rainy or sunny on any given day.
  prefs: []
  type: TYPE_NORMAL
- en: If all events were independent, it would be impossible to predict one event
    by observing another. In other words, **dependent events** are the basis of predictive
    modeling. Just as the presence of clouds is predictive of a rainy day, the appearance
    of the word Viagra is predictive of a spam e-mail.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding joint probability](img/B03905_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Calculating the probability of dependent events is a bit more complex than for
    independent events. If *P(spam)* and *P(Viagra)* were independent, we could easily
    calculate *P(spam ∩ Viagra)*, the probability of both events happening at the
    same time. Because 20 percent of all the messages are spam, and 5 percent of all
    the e-mails contain the word Viagra, we could assume that 1 percent of all messages
    are spam with the term Viagra. This is because *0.05 * 0.20 = 0.01*. More generally,
    for independent events *A* and *B*, the probability of both happening can be expressed
    as *P(A ∩ B) = P(A) * P(B)*.
  prefs: []
  type: TYPE_NORMAL
- en: This said, we know that *P(spam)* and *P(Viagra)* are likely to be highly dependent,
    which means that this calculation is incorrect. To obtain a reasonable estimate,
    we need to use a more careful formulation of the relationship between these two
    events, which is based on advanced Bayesian methods.
  prefs: []
  type: TYPE_NORMAL
- en: Computing conditional probability with Bayes' theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The relationships between dependent events can be described using **Bayes''
    theorem**, as shown in the following formula. This formulation provides a way
    of thinking about how to revise an estimate of the probability of one event in
    light of the evidence provided by another event:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing conditional probability with Bayes'' theorem](img/B03905_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The notation *P(A|B)* is read as the probability of event *A*, given that event
    *B* occurred. This is known as **conditional probability**, since the probability
    of *A* is dependent (that is, conditional) on what happened with event *B*. Bayes'
    theorem tells us that our estimate of *P(A|B)* should be based on *P(A ∩ B)*,
    a measure of how often *A* and *B* are observed to occur together, and *P(B)*,
    a measure of how often *B* is observed to occur in general.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem states that the best estimate of *P(A|B)* is the proportion of
    trials in which *A* occurred with *B* out of all the trials in which *B* occurred.
    In plain language, this tells us that if we know event *B* occurred, the probability
    of event *A* is higher the more often that *A* and *B* occur together each time
    *B* is observed. In a way, this adjusts *P(A ∩ B)* for the probability of *B*
    occurring; if *B* is extremely rare, *P(B)* and *P(A ∩ B)* will always be small;
    however, if *A* and *B* almost always happen together, *P(A|B)* will be high regardless
    of the probability of *B*.
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition, *P(A ∩ B) = P(A|B) * P(B)*, a fact that can be easily derived
    by applying a bit of algebra to the previous formula. Rearranging this formula
    once more with the knowledge that *P(A ∩ B) = P(B ∩ A)* results in the conclusion
    that *P(A ∩ B) = P(B|A) * P(A)*, which we can then use in the following formulation
    of Bayes'' theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing conditional probability with Bayes'' theorem](img/B03905_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In fact, this is the traditional way in which Bayes' theorem has been specified,
    for reasons that will become clear as we apply it to machine learning. First,
    to better understand how Bayes' theorem works in practice, let's revisit our hypothetical
    spam filter.
  prefs: []
  type: TYPE_NORMAL
- en: Without knowledge of an incoming message's content, the best estimate of its
    spam status would be *P(spam)*, the probability that any prior message was spam,
    which we calculated previously to be 20 percent. This estimate is known as the
    **prior probability**.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you obtained additional evidence by looking more carefully at the
    set of previously received messages to examine the frequency that the term Viagra
    appeared. The probability that the word Viagra was used in previous spam messages,
    or *P(Viagra|spam)*, is called the **likelihood**. The probability that Viagra
    appeared in any message at all, or *P(Viagra)*, is known as the **marginal likelihood**.
  prefs: []
  type: TYPE_NORMAL
- en: 'By applying Bayes'' theorem to this evidence, we can compute a **posterior
    probability** that measures how likely the message is to be spam. If the posterior
    probability is greater than 50 percent, the message is more likely to be spam
    than ham and it should perhaps be filtered. The following formula shows how Bayes''
    theorem is applied to the evidence provided by the previous e-mail messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing conditional probability with Bayes'' theorem](img/B03905_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate these components of Bayes'' theorem, it helps to construct a **frequency
    table** (shown on the left in the following diagram) that records the number of
    times Viagra appeared in spam and ham messages. Just like a two-way cross-tabulation,
    one dimension of the table indicates levels of the class variable (spam or ham),
    while the other dimension indicates levels for features (Viagra: yes or no). The
    cells then indicate the number of instances having the particular combination
    of class value and feature value. The frequency table can then be used to construct
    a **likelihood table**, as shown on right in the following diagram. The rows of
    the likelihood table indicate the conditional probabilities for Viagra (yes/no),
    given that an e-mail was either spam or ham:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing conditional probability with Bayes'' theorem](img/B03905_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The likelihood table reveals that *P(Viagra=Yes|spam) = 4/20 = 0.20*, indicating
    that the probability is 20 percent that a message contains the term Viagra, given
    that the message is spam. Additionally, since *P(A ∩ B) = P(B|A) * P(A)*, we can
    calculate *P(spam ∩ Viagra)* as *P(Viagra|spam) * P(spam) = (4/20) * (20/100)
    = 0.04*. The same result can be found in the frequency table, which notes that
    4 out of the 100 messages were spam with the term Viagra. Either way, this is
    four times greater than the previous estimate of 0.01 we calculated as *P(A ∩
    B) = P(A) * P(B)* under the false assumption of independence. This, of course,
    illustrates the importance of Bayes' theorem while calculating joint probability.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the posterior probability, *P(spam|Viagra)*, we simply take *P(Viagra|spam)
    * P(spam) / P(Viagra)* or *(4/20) * (20/100) / (5/100) = 0.80*. Therefore, the
    probability is 80 percent that a message is spam, given that it contains the word
    Viagra. In light of this result, any message containing this term should probably
    be filtered.
  prefs: []
  type: TYPE_NORMAL
- en: This is very much how commercial spam filters work, although they consider a
    much larger number of words simultaneously while computing the frequency and likelihood
    tables. In the next section, we'll see how this concept is put to use when additional
    features are involved.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Naive** **Bayes** algorithm describes a simple method to apply Bayes''
    theorem to classification problems. Although it is not the only machine learning
    method that utilizes Bayesian methods, it is the most common one. This is particularly
    true for text classification, where it has become the de facto standard. The strengths
    and weaknesses of this algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Simple, fast, and very effective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does well with noisy and missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires relatively few examples for training, but also works well with very
    large numbers of examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to obtain the estimated probability for a prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Relies on an often-faulty assumption of equally important and independent features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not ideal for datasets with many numeric features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimated probabilities are less reliable than the predicted classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm is named as such because it makes some "naive" assumptions
    about the data. In particular, Naive Bayes assumes that all of the features in
    the dataset are equally important and independent. These assumptions are rarely
    true in most real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you were attempting to identify spam by monitoring e-mail messages,
    it is almost certainly true that some features will be more important than others.
    For example, the e-mail sender may be a more important indicator of spam than
    the message text. Additionally, the words in the message body are not independent
    from one another, since the appearance of some words is a very good indication
    that other words are also likely to appear. A message with the word Viagra will
    probably also contain the words prescription or drugs.
  prefs: []
  type: TYPE_NORMAL
- en: However, in most cases when these assumptions are violated, Naive Bayes still
    performs fairly well. This is true even in extreme circumstances where strong
    dependencies are found among the features. Due to the algorithm's versatility
    and accuracy across many types of conditions, Naive Bayes is often a strong first
    candidate for classification learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The exact reason why Naive Bayes works well in spite of its faulty assumptions
    has been the subject of much speculation. One explanation is that it is not important
    to obtain a precise estimate of probability, so long as the predictions are accurate.
    For instance, if a spam filter correctly identifies spam, does it matter whether
    it was 51 percent or 99 percent confident in its prediction? For one discussion
    of this topic, refer to: Domingos P, Pazzani M. On the optimality of the simple
    Bayesian classifier under zero-one loss. *Machine Learning*. 1997; 29:103-130.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification with Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s extend our spam filter by adding a few additional terms to be monitored
    in addition to the term Viagra: Money, Groceries, and Unsubscribe. The Naive Bayes
    learner is trained by constructing a likelihood table for the appearance of these
    four words (labeled *W[1]*, *W[2]*, *W[3]*, and *W[4]*), as shown in the following
    diagram for 100 e-mails:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification with Naive Bayes](img/B03905_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As new messages are received, we need to calculate the posterior probability
    to determine whether they are more likely to be spam or ham, given the likelihood
    of the words found in the message text. For example, suppose that a message contains
    the terms Viagra and Unsubscribe, but does not contain either Money or Groceries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Bayes'' theorem, we can define the problem as shown in the following
    formula. It captures the probability that a message is spam, given that *Viagra
    = Yes*, *Money = No*, *Groceries = No*, and *Unsubscribe = Yes*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification with Naive Bayes](img/B03905_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a number of reasons, this formula is computationally difficult to solve.
    As additional features are added, tremendous amounts of memory are needed to store
    probabilities for all of the possible intersecting events; imagine the complexity
    of a Venn diagram for the events for four words, let alone for hundreds or more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The work becomes much easier if we can exploit the fact that Naive Bayes assumes
    independence among events. Specifically, it assumes **class-conditional independence**,
    which means that events are independent so long as they are conditioned on the
    same class value. Assuming conditional independence allows us to simplify the
    formula using the probability rule for independent events, which states that *P(A
    ∩ B) = P(A) * P(B)*. Because the denominator does not depend on the class (spam
    or ham), it is treated as a constant value and can be ignored for the time being.
    This means that the conditional probability of spam can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification with Naive Bayes](img/B03905_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the probability that the message is ham can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification with Naive Bayes](img/B03905_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the equals symbol has been replaced by the proportional-to symbol
    (similar to a sideways, open-ended '8') to indicate the fact that the denominator
    has been omitted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the values in the likelihood table, we can start filling numbers in these
    equations. The overall likelihood of spam is then:'
  prefs: []
  type: TYPE_NORMAL
- en: (4/20) * (10/20) * (20/20) * (12/20) * (20/100) = 0.012
  prefs: []
  type: TYPE_NORMAL
- en: 'While the likelihood of ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: (1/80) * (66/80) * (71/80) * (23/80) * (80/100) = 0.002
  prefs: []
  type: TYPE_NORMAL
- en: Because *0.012/0.002 = 6*, we can say that this message is six times more likely
    to be spam than ham. However, to convert these numbers into probabilities, we
    need to perform one last step to reintroduce the denominator that had been excluded.
    Essentially, we must rescale the likelihood of each outcome by dividing it by
    the total likelihood across all possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, the probability of spam is equal to the likelihood that the message
    is spam divided by the likelihood that the message is either spam or ham:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.012/(0.012 + 0.002) = 0.857
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the probability of ham is equal to the likelihood that the message
    is ham divided by the likelihood that the message is either spam or ham:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.002/(0.012 + 0.002) = 0.143
  prefs: []
  type: TYPE_NORMAL
- en: Given the pattern of words found in this message, we expect that the message
    is spam with 85.7 percent probability and ham with 14.3 percent probability. Because
    these are mutually exclusive and exhaustive events, the probabilities sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Naive Bayes classification algorithm we used in the preceding example can
    be summarized by the following formula. The probability of level *L* for class
    *C*, given the evidence provided by features *F[1]* through *F[n]*, is equal to
    the product of the probabilities of each piece of evidence conditioned on the
    class level, the prior probability of the class level, and a scaling factor *1/Z*,
    which converts the likelihood values into probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification with Naive Bayes](img/B03905_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although this equation seems intimidating, as the prior example illustrated,
    the series of steps is fairly straightforward. Begin by building a frequency table,
    use this to build a likelihood table, and multiply the conditional probabilities
    according to the Naive Bayes' rule. Finally, divide by the total likelihood to
    transform each class likelihood into a probability. After attempting this calculation
    a few times by hand, it will become second nature.
  prefs: []
  type: TYPE_NORMAL
- en: The Laplace estimator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we employ Naive Bayes in more complex problems, there are some nuances
    to consider. Suppose that we received another message, this time containing all
    four terms: Viagra, Groceries, Money, and Unsubscribe. Using the Naive Bayes algorithm
    as before, we can compute the likelihood of spam as:'
  prefs: []
  type: TYPE_NORMAL
- en: (4/20) * (10/20) * (0/20) * (12/20) * (20/100) = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'The likelihood of ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: (1/80) * (14/80) * (8/80) * (23/80) * (80/100) = 0.00005
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the probability of spam is:'
  prefs: []
  type: TYPE_NORMAL
- en: 0/(0 + 0.00005) = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability of ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.00005/(0 + 0\. 0.00005) = 1
  prefs: []
  type: TYPE_NORMAL
- en: These results suggest that the message is spam with 0 percent probability and
    ham with 100 percent probability. Does this prediction make sense? Probably not.
    The message contains several words usually associated with spam, including Viagra,
    which is rarely used in legitimate messages. It is therefore very likely that
    the message has been incorrectly classified.
  prefs: []
  type: TYPE_NORMAL
- en: This problem might arise if an event never occurs for one or more levels of
    the class. For instance, the term Groceries had never previously appeared in a
    spam message. Consequently, *P(spam|groceries) = 0%*.
  prefs: []
  type: TYPE_NORMAL
- en: Because probabilities in the Naive Bayes formula are multiplied in a chain,
    this 0 percent value causes the posterior probability of spam to be zero, giving
    the word Groceries the ability to effectively nullify and overrule all of the
    other evidence. Even if the e-mail was otherwise overwhelmingly expected to be
    spam, the absence of the word Groceries in spam will always veto the other evidence
    and result in the probability of spam being zero.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to this problem involves using something called the **Laplace estimator**,
    which is named after the French mathematician Pierre-Simon Laplace. The Laplace
    estimator essentially adds a small number to each of the counts in the frequency
    table, which ensures that each feature has a nonzero probability of occurring
    with each class. Typically, the Laplace estimator is set to 1, which ensures that
    each class-feature combination is found in the data at least once.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Laplace estimator can be set to any value and does not necessarily even
    have to be the same for each of the features. If you were a devoted Bayesian,
    you could use a Laplace estimator to reflect a presumed prior probability of how
    the feature relates to the class. In practice, given a large enough training dataset,
    this step is unnecessary and the value of 1 is almost always used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this affects our prediction for this message. Using a Laplace
    value of 1, we add one to each numerator in the likelihood function. The total
    number of 1 values must also be added to each conditional probability denominator.
    The likelihood of spam is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: (5/24) * (11/24) * (1/24) * (13/24) * (20/100) = 0.0004
  prefs: []
  type: TYPE_NORMAL
- en: 'The likelihood of ham is:'
  prefs: []
  type: TYPE_NORMAL
- en: (2/84) * (15/84) * (9/84) * (24/84) * (80/100) = 0.0001
  prefs: []
  type: TYPE_NORMAL
- en: This means that the probability of spam is 80 percent, and the probability of
    ham is 20 percent, which is a more plausible result than the one obtained when
    the term Groceries alone determined the result.
  prefs: []
  type: TYPE_NORMAL
- en: Using numeric features with Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because Naive Bayes uses frequency tables to learn the data, each feature must
    be categorical in order to create the combinations of class and feature values
    comprising of the matrix. Since numeric features do not have categories of values,
    the preceding algorithm does not work directly with numeric data. There are, however,
    ways that this can be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: One easy and effective solution is to **discretize** numeric features, which
    simply means that the numbers are put into categories known as **bins**. For this
    reason, discretization is also sometimes called **binning**. This method is ideal
    when there are large amounts of training data, a common condition while working
    with Naive Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: There are several different ways to discretize a numeric feature. Perhaps the
    most common is to explore the data for natural categories or **cut points** in
    the distribution of data. For example, suppose that you added a feature to the
    spam dataset that recorded the time of night or day the e-mail was sent, from
    0 to 24 hours past midnight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depicted using a histogram, the time data might look something like the following
    diagram. In the early hours of the morning, the message frequency is low. The
    activity picks up during business hours and tapers off in the evening. This seems
    to create four natural bins of activity, as partitioned by the dashed lines indicating
    places where the numeric data are divided into levels of a new nominal feature,
    which could then be used with Naive Bayes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using numeric features with Naive Bayes](img/B03905_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Keep in mind that the choice of four bins was somewhat arbitrary based on the
    natural distribution of data and a hunch about how the proportion of spam might
    change throughout the day. We might expect that spammers operate in the late hours
    of the night or they may operate during the day, when people are likely to check
    their e-mail. This said, to capture these trends, we could have just as easily
    used three bins or twelve.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If there are no obvious cut points, one option will be to discretize the feature
    using quantiles. You could divide the data into three bins with tertiles, four
    bins with quartiles, or five bins with quintiles.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind is that discretizing a numeric feature always results
    in a reduction of information as the feature's original granularity is reduced
    to a smaller number of categories. It is important to strike a balance here. Too
    few bins can result in important trends being obscured. Too many bins can result
    in small counts in the Naive Bayes frequency table, which can increase the algorithm's
    sensitivity to noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: Example – filtering mobile phone spam with the Naive Bayes algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the worldwide use of mobile phones has grown, a new avenue for electronic
    junk mail has opened for disreputable marketers. These advertisers utilize Short
    Message Service (SMS) text messages to target potential consumers with unwanted
    advertising known as SMS spam. This type of spam is particularly troublesome because,
    unlike e-mail spam, many cellular phone users pay a fee per SMS received. Developing
    a classification algorithm that could filter SMS spam would provide a useful tool
    for cellular phone providers.
  prefs: []
  type: TYPE_NORMAL
- en: Since Naive Bayes has been used successfully for e-mail spam filtering, it seems
    likely that it could also be applied to SMS spam. However, relative to e-mail
    spam, SMS spam poses additional challenges for automated filters. SMS messages
    are often limited to 160 characters, reducing the amount of text that can be used
    to identify whether a message is junk. The limit, combined with small mobile phone
    keyboards, has led many to adopt a form of SMS shorthand lingo, which further
    blurs the line between legitimate messages and spam. Let's see how a simple Naive
    Bayes classifier handles these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To develop the Naive Bayes classifier, we will use data adapted from the SMS
    Spam Collection at [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To read more about how the SMS Spam Collection was developed, refer to: Gómez
    JM, Almeida TA, Yamakami A. On the validity of a new SMS spam collection. *Proceedings
    of the 11^(th) IEEE International Conference on Machine Learning and Applications*.
    2012.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset includes the text of SMS messages along with a label indicating
    whether the message is unwanted. Junk messages are labeled spam, while legitimate
    messages are labeled ham. Some examples of spam and ham are shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sample SMS ham | Sample SMS spam |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Better. Made up for Friday and stuffed myself like a pig yesterday. Now I feel
    bleh. But, at least, its not writhing pain kind of bleh.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If he started searching, he will get job in few days. He has great potential
    and talent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I got another job! The one at the hospital, doing data analysis or something,
    starts on Monday! Not sure when my thesis will finish.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations ur awarded 500 of CD vouchers or 125 gift guaranteed & Free
    entry 2 100 wkly draw txt MUSIC to 87066.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: December only! Had your mobile 11mths+? You are entitled to update to the latest
    colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valentines Day Special! Win over £1000 in our quiz and take your partner on
    the trip of a lifetime! Send GO to 83600 now. 150 p/msg rcvd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding messages, did you notice any distinguishing characteristics
    of spam? One notable characteristic is that two of the three spam messages use
    the word "free," yet the word does not appear in any of the ham messages. On the
    other hand, two of the ham messages cite specific days of the week, as compared
    to zero in spam messages.
  prefs: []
  type: TYPE_NORMAL
- en: Our Naive Bayes classifier will take advantage of such patterns in the word
    frequency to determine whether the SMS messages seem to better fit the profile
    of spam or ham. While it's not inconceivable that the word "free" would appear
    outside of a spam SMS, a legitimate message is likely to provide additional words
    explaining the context. For instance, a ham message might state "are you free
    on Sunday?" Whereas, a spam message might use the phrase "free ringtones." The
    classifier will compute the probability of spam and ham, given the evidence provided
    by all the words in the message.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step towards constructing our classifier involves processing the raw
    data for analysis. Text data are challenging to prepare, because it is necessary
    to transform the words and sentences into a form that a computer can understand.
    We will transform our data into a representation known as **bag-of-words**, which
    ignores word order and simply provides a variable indicating whether the word
    appears at all.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data used here has been modified slightly from the original in order to
    make it easier to work with in R. If you plan on following along with the example,
    download the `sms_spam.csv` file from the Packt website and save it in your R
    working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll begin by importing the CSV data and saving it in a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `str()` function, we see that the `sms_raw` data frame includes 5,559
    total SMS messages with two features: `type` and `text`. The SMS type has been
    coded as either `ham` or `spam`. The `text` element stores the full raw SMS text.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `type` element is currently a character vector. Since this is a categorical
    variable, it would be better to convert it into a factor, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Examining this with the `str()` and `table()` functions, we see that `type`
    has now been appropriately recoded as a factor. Additionally, we see that 747
    (about 13 percent) of SMS messages in our data were labeled as spam, while the
    others were labeled as ham:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For now, we will leave the message text alone. As you will learn in the next
    section, processing the raw SMS messages will require the use of a new set of
    powerful tools designed specifically to process text data.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – cleaning and standardizing text data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SMS messages are strings of text composed of words, spaces, numbers, and punctuation.
    Handling this type of complex data takes a lot of thought and effort. One needs
    to consider how to remove numbers and punctuation; handle uninteresting words
    such as *and*, *but*, and *or*; and how to break apart sentences into individual
    words. Thankfully, this functionality has been provided by the members of the
    R community in a text mining package titled `tm`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `tm` package was originally created by Ingo Feinerer as a dissertation
    project at the Vienna University of Economics and Business. To learn more, see:
    Feinerer I, Hornik K, Meyer D. Text Mining Infrastructure in R. *Journal of Statistical
    Software*. 2008; 25:1-54.'
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package can be installed via the `install.packages("tm")` command and
    loaded with the `library(tm)` command. Even if you already have it installed,
    it may be worth re-running the install process to ensure that your version is
    up-to-date, as the `tm` package is still being actively developed. This occasionally
    results in changes to its functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter was written and tested using tm version 0.6-2, which was current
    as of July 2015\. If you see differences in the output or if the code does not
    work, you may be using a different version. The Packt Publishing support page
    for this book will post solutions for future `tm` packages if significant changes
    are noted.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in processing text data involves creating a **corpus**, which
    is a collection of text documents. The documents can be short or long, from individual
    news articles, pages in a book or on the web, or entire books. In our case, the
    corpus will be a collection of SMS messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create a corpus, we''ll use the `VCorpus()` function in the `tm`
    package, which refers to a volatile corpus—volatile as it is stored in memory
    as opposed to being stored on disk (the `PCorpus()` function can be used to access
    a permanent corpus stored in a database). This function requires us to specify
    the source of documents for the corpus, which could be from a computer''s filesystem,
    a database, the Web, or elsewhere. Since we already loaded the SMS message text
    into R, we''ll use the `VectorSource()` reader function to create a source object
    from the existing `sms_raw$text` vector, which can then be supplied to `VCorpus()`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The resulting corpus object is saved with the name `sms_corpus`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By specifying an optional `readerControl` parameter, the `VCorpus()` function
    offers functionality to import text from sources such as PDFs and Microsoft Word
    files. To learn more, examine the *Data Import* section in the `tm` package vignette
    using the `vignette("tm")` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'By printing the corpus, we see that it contains documents for each of the 5,559
    SMS messages in the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the `tm` corpus is essentially a complex list, we can use list operations
    to select documents in the corpus. To receive a summary of specific messages,
    we can use the `inspect()` function with list operators. For example, the following
    command will view a summary of the first and second SMS messages in the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the actual message text, the `as.character()` function must be applied
    to the desired messages. To view one message, use the `as.character()` function
    on a single list element, noting that the double-bracket notation is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To view multiple documents, we''ll need to use `as.character()` on several
    items in the `sms_corpus` object. To do so, we''ll use the `lapply()` function,
    which is a part of a family of R functions that applies a procedure to each element
    of an R data structure. These functions, which include `apply()` and `sapply()`
    among others, are one of the key idioms of the R language. Experienced R coders
    use these much like the way `for` or `while` loops are used in other programming
    languages, as they result in more readable (and sometimes more efficient) code.
    The `lapply()` command to apply `as.character()` to a subset of corpus elements
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As noted earlier, the corpus contains the raw text of 5,559 text messages. In
    order to perform our analysis, we need to divide these messages into individual
    words. But first, we need to clean the text, in order to standardize the words,
    by removing punctuation and other characters that clutter the result. For example,
    we would like the strings *Hello*!, *HELLO*, and *hello* to be counted as instances
    of the same word.
  prefs: []
  type: TYPE_NORMAL
- en: The `tm_map()` function provides a method to apply a transformation (also known
    as mapping) to a `tm` corpus. We will use this function to clean up our corpus
    using a series of transformations and save the result in a new object called `corpus_clean`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first order of business will be to standardize the messages to use only
    lowercase characters. To this end, R provides a `tolower()` function that returns
    a lowercase version of text strings. In order to apply this function to the corpus,
    we need to use the `tm` wrapper function `content_transformer()` to treat `tolower()`
    as a transformation function that can be used to access the corpus. The full command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether the command worked as advertised, let''s inspect the first
    message in the original corpus and compare it to the same in the transformed corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As expected, uppercase letters have been replaced by lowercase versions of the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `content_transformer()` function can be used to apply more sophisticated
    text processing and cleanup processes, such as `grep` pattern matching and replacement.
    Simply write a custom function and wrap it before applying via `tm_map()` as done
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue our cleanup by removing numbers from the SMS messages. Although
    some numbers may provide useful information, the majority would likely be unique
    to individual senders and thus will not provide useful patterns across all messages.
    With this in mind, we''ll strip all the numbers from the corpus as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the preceding code did not use the `content_transformer()` function.
    This is because `removeNumbers()` is built into `tm` along with several other
    mapping functions that do not need to be wrapped. To see the other built-in transformations,
    simply type `getTransformations()`.
  prefs: []
  type: TYPE_NORMAL
- en: Our next task is to remove filler words such as *to*, *and*, *but*, and *or*
    from our SMS messages. These terms are known as **stop words** and are typically
    removed prior to text mining. This is due to the fact that although they appear
    very frequently, they do not provide much useful information for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than define a list of stop words ourselves, we'll use the `stopwords()`
    function provided by the `tm` package. This function allows us to access various
    sets of stop words, across several languages. By default, common English language
    stop words are used. To see the default list, type `stopwords()` at the command
    line. To see the other languages and options available, type `?stopwords` for
    the documentation page.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even within a single language, there is no single definitive list of stop words.
    For example, the default English list in `tm` includes about 174 words while another
    option includes 571 words. You can even specify your own list of stop words if
    you prefer. Regardless of the list you choose, keep in mind the goal of this transformation,
    which is to eliminate all useless data while keeping as much useful information
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stop words alone are not a useful transformation. What we need is a way
    to remove any words that appear in the stop words list. The solution lies in the
    `removeWords()` function, which is a transformation included with the `tm` package.
    As we have done before, we''ll use the `tm_map()` function to apply this mapping
    to the data, providing the `stopwords()` function as a parameter to indicate exactly
    the words we would like to remove. The full command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Since `stopwords()` simply returns a vector of stop words, had we chosen so,
    we could have replaced it with our own vector of words to be removed. In this
    way, we could expand or reduce the list of stop words to our liking or remove
    a completely different set of words entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with our cleanup process, we can also eliminate any punctuation
    from the text messages using the built-in `removePunctuation()` transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `removePunctuation()` transformation strips punctuation characters from
    the text blindly, which can lead to unintended consequences. For example, consider
    what happens when it is applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As shown, the lack of blank space after the ellipses has caused the words *hello*
    and *world* to be joined as a single word. While this is not a substantial problem
    for our analysis, it is worth noting for the future.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To work around the default behavior of `removePunctuation()`, simply create
    a custom function that replaces rather than removes punctuation characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Essentially, this uses R's `gsub()` function to substitute any punctuation characters
    in `x` with a blank space. The `replacePunctuation()` function can then be used
    with `tm_map()` as with other transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Another common standardization for text data involves reducing words to their
    root form in a process called **stemming**. The stemming process takes words like
    *learned*, *learning*, and *learns*, and strips the suffix in order to transform
    them into the base form, *learn*. This allows machine learning algorithms to treat
    the related terms as a single concept rather than attempting to learn a pattern
    for each variant.
  prefs: []
  type: TYPE_NORMAL
- en: The `tm` package provides stemming functionality via integration with the `SnowballC`
    package. At the time of this writing, `SnowballC` was not installed by default
    with `tm`. Do so with `install.packages("SnowballC")` if it is not installed already.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `SnowballC` package is maintained by Milan Bouchet-Valat and provides an
    R interface to the C-based `libstemmer` library, which is based on M.F. Porter's
    "Snowball" word stemming algorithm, a widely used open source stemming method.
    For more detail, see [http://snowball.tartarus.org](http://snowball.tartarus.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SnowballC` package provides a `wordStem()` function, which for a character
    vector, returns the same vector of terms in its root form. For example, the function
    correctly stems the variants of the word *learn*, as described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to apply the `wordStem()` function to an entire corpus of text documents,
    the `tm` package includes a `stemDocument()` transformation. We apply this to
    our corpus with the `tm_map()` function exactly as done earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you receive an error message while applying the `stemDocument()` transformation,
    please confirm that you have the `SnowballC` package installed. If after installing
    the package you still encounter the message that `all scheduled cores encountered
    errors`, you can also try forcing the `tm_map()` command to a single core, by
    adding an additional parameter to specify `mc.cores=1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After removing numbers, stop words, and punctuation as well as performing stemming,
    the text messages are left with the blank spaces that previously separated the
    now-missing pieces. The final step in our text cleanup process is to remove additional
    whitespace, using the built-in `stripWhitespace()` transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows the first three messages in the SMS corpus before
    and after the cleaning process. The messages have been limited to the most interesting
    words, and punctuation and capitalization have been removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '| SMS messages before cleaning | SMS messages after cleaning |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – splitting text documents into words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the data are processed to our liking, the final step is to split the
    messages into individual components through a process called **tokenization**.
    A token is a single element of a text string; in this case, the tokens are words.
  prefs: []
  type: TYPE_NORMAL
- en: As you might assume, the `tm` package provides functionality to tokenize the
    SMS message corpus. The `DocumentTermMatrix()` function will take a corpus and
    create a data structure called a **Document Term Matrix** (**DTM**) in which rows
    indicate documents (SMS messages) and columns indicate terms (words).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `tm` package also provides a data structure for a **Term Document Matrix**
    (**TDM**), which is simply a transposed DTM in which the rows are terms and the
    columns are documents. Why the need for both? Sometimes, it is more convenient
    to work with one or the other. For example, if the number of documents is small,
    while the word list is large, it may make sense to use a TDM because it is generally
    easier to display many rows than to display many columns. This said, the two are
    often interchangeable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each cell in the matrix stores a number indicating a count of the times the
    word represented by the column appears in the document represented by the row.
    The following illustration depicts only a small portion of the DTM for the SMS
    corpus, as the complete matrix has 5,559 rows and over 7,000 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data preparation – splitting text documents into words](img/B03905_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The fact that each cell in the table is zero implies that none of the words
    listed on the top of the columns appear in any of the first five messages in the
    corpus. This highlights the reason why this data structure is called a **sparse
    matrix**; the vast majority of the cells in the matrix are filled with zeros.
    Stated in real-world terms, although each message must contain at least one word,
    the probability of any one word appearing in a given message is small.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a DTM sparse matrix, given a `tm` corpus, involves a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This will create an `sms_dtm` object that contains the tokenized corpus using
    the default settings, which apply minimal processing. The default settings are
    appropriate because we have already prepared the corpus manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if we hadn''t performed the preprocessing, we could do so
    here by providing a list of `control` parameter options to override the defaults.
    For example, to create a DTM directly from the raw, unprocessed SMS corpus, we
    can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This applies the same preprocessing steps to the SMS corpus in the same order
    as done earlier. However, comparing `sms_dtm` to `sms_dtm2`, we see a slight difference
    in the number of terms in the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The reason for this discrepancy has to do with a minor difference in the ordering
    of the preprocessing steps. The `DocumentTermMatrix()` function applies its cleanup
    functions to the text strings only after they have been split apart into words.
    Thus, it uses a slightly different stop words removal function. Consequently,
    some words split differently than when they are cleaned before tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To force the two prior document term matrices to be identical, we can override
    the default stop words function with our own that uses the original replacement
    function. Simply replace `stopwords = TRUE` with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The differences between these two cases illustrate an important principle of
    cleaning text data: the order of operations matters. With this in mind, it is
    very important to think through how early steps in the process are going to affect
    later ones. The order presented here will work in many cases, but when the process
    is tailored more carefully to specific datasets and use cases, it may require
    rethinking. For example, if there are certain terms you hope to exclude from the
    matrix, consider whether you should search for them before or after stemming.
    Also, consider how the removal of punctuation—and whether the punctuation is eliminated
    or replaced by blank space—affects these steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – creating training and test datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our data prepared for analysis, we now need to split the data into training
    and test datasets, so that once our spam classifier is built, it can be evaluated
    on data it has not previously seen. But even though we need to keep the classifier
    blinded as to the contents of the test dataset, it is important that the split
    occurs after the data have been cleaned and processed; we need exactly the same
    preparation steps to occur on both the training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll divide the data into two portions: 75 percent for training and 25 percent
    for testing. Since the SMS messages are sorted in a random order, we can simply
    take the first 4,169 for training and leave the remaining 1,390 for testing. Thankfully,
    the DTM object acts very much like a data frame and can be split using the standard
    `[row, col]` operations. As our DTM stores SMS messages as rows and words as columns,
    we must request a specific range of rows and all columns for each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'For convenience later on, it is also helpful to save a pair of vectors with
    labels for each of the rows in the training and testing matrices. These labels
    are not stored in the DTM, so we would need to pull them from the original `sms_raw`
    data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that the subsets are representative of the complete set of SMS data,
    let''s compare the proportion of spam in the training and test data frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Both the training data and test data contain about 13 percent spam. This suggests
    that the spam messages were divided evenly between the two datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing text data – word clouds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **word cloud** is a way to visually depict the frequency at which words appear
    in text data. The cloud is composed of words scattered somewhat randomly around
    the figure. Words appearing more often in the text are shown in a larger font,
    while less common terms are shown in smaller fonts. This type of figures grew
    in popularity recently, since it provides a way to observe trending topics on
    social media websites.
  prefs: []
  type: TYPE_NORMAL
- en: The `wordcloud` package provides a simple R function to create this type of
    diagrams. We'll use it to visualize the types of words in SMS messages, as comparing
    the clouds for spam and ham will help us gauge whether our Naive Bayes spam filter
    is likely to be successful. If you haven't already done so, install and load the
    package by typing `install.packages("wordcloud")` and `library(wordcloud)` at
    the R command line.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `wordcloud` package was written by Ian Fellows. For more information on
    this package, visit his blog at [http://blog.fellstat.com/?cat=11](http://blog.fellstat.com/?cat=11).
  prefs: []
  type: TYPE_NORMAL
- en: 'A word cloud can be created directly from a `tm` corpus object using the syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This will create a word cloud from our prepared SMS corpus. Since we specified
    `random.order = FALSE`, the cloud will be arranged in a nonrandom order with higher
    frequency words placed closer to the center. If we do not specify `random.order`,
    the cloud would be arranged randomly by default. The `min.freq` parameter specifies
    the number of times a word must appear in the corpus before it will be displayed
    in the cloud. Since a frequency of 50 is about 1 percent of the corpus, this means
    that a word must be found in at least 1 percent of the SMS messages to be included
    in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might get a warning message noting that R was unable to fit all of the words
    in the figure. If so, try increasing `min.freq` to reduce the number of words
    in the cloud. It might also help to use the `scale` parameter to reduce the font
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting word cloud should appear similar to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing text data – word clouds](img/B03905_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A perhaps more interesting visualization involves comparing the clouds for SMS
    spam and ham. Since we did not construct separate corpora for spam and ham, this
    is an appropriate time to note a very helpful feature of the `wordcloud()` function.
    Given a vector of raw text strings, it will automatically apply common text preparation
    processes before displaying the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use R''s `subset()` function to take a subset of the `sms_raw` data
    by the SMS `type`. First, we''ll create a subset where the message `type` is `spam`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll do the same thing for the `ham` subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be careful to note the double equals sign. Like many programming languages,
    R uses `==` to test equality. If you accidently use a single equals sign, you'll
    end up with a subset much larger than you expected!
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have two data frames, `spam` and `ham`, each with a `text` feature containing
    the raw text strings for SMSes. Creating word clouds is as simple as before. This
    time, we''ll use the `max.words` parameter to look at the 40 most common words
    in each of the two sets. The scale parameter allows us to adjust the maximum and
    minimum font size for words in the cloud. Feel free to adjust these parameters
    as you see fit. This is illustrated in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting word clouds are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing text data – word clouds](img/B03905_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Do you have a hunch about which one is the spam cloud and which represents ham?
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because of the randomization process, each word cloud may look slightly different.
    Running the `wordcloud()` function several times allows you to choose the cloud
    that is the most visually appealing for presentation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: As you probably guessed, the spam cloud is on the left. Spam messages include
    words such as *urgent*, *free*, *mobile*, *claim*, and *stop*; these terms do
    not appear in the ham cloud at all. Instead, ham messages use words such as *can*,
    *sorry*, *need*, and *time*. These stark differences suggest that our Naive Bayes
    model will have some strong key words to differentiate between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation – creating indicator features for frequent words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step in the data preparation process is to transform the sparse matrix
    into a data structure that can be used to train a Naive Bayes classifier. Currently,
    the sparse matrix includes over 6,500 features; this is a feature for every word
    that appears in at least one SMS message. It's unlikely that all of these are
    useful for classification. To reduce the number of features, we will eliminate
    any word that appear in less than five SMS messages, or in less than about 0.1
    percent of the records in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding frequent words requires use of the `findFreqTerms()` function in the
    `tm` package. This function takes a DTM and returns a character vector containing
    the words that appear for at least the specified number of times. For instance,
    the following command will display the words appearing at least five times in
    the `sms_dtm_train` matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the function is a character vector, so let''s save our frequent
    words for later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'A peek into the contents of the vector shows us that there are 1,136 terms
    appearing in at least five SMS messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to filter our DTM to include only the terms appearing in a specified
    vector. As done earlier, we''ll use the data frame style `[row, col]` operations
    to request specific portions of the DTM, noting that the columns are named after
    the words the DTM contains. We can take advantage of this to limit the DTM to
    specific words. Since we want all the rows, but only the columns representing
    the words in the `sms_freq_words` vector, our commands are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The training and test datasets now include 1,136 features, which correspond
    to words appearing in at least five messages.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes classifier is typically trained on data with categorical features.
    This poses a problem, since the cells in the sparse matrix are numeric and measure
    the number of times a word appears in a message. We need to change this to a categorical
    variable that simply indicates yes or no depending on whether the word appears
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following defines a `convert_counts()` function to convert counts to `Yes`/`No`
    strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: By now, some of the pieces of the preceding function should look familiar. The
    first line defines the function. The `ifelse(x > 0, "Yes", "No")` statement transforms
    the values in `x`, so that if the value is greater than `0`, then it will be replaced
    by `"Yes"`, otherwise it will be replaced by a `"No"` string. Lastly, the newly
    transformed `x` vector is returned.
  prefs: []
  type: TYPE_NORMAL
- en: We now need to apply `convert_counts()` to each of the columns in our sparse
    matrix. You may be able to guess the R function to do exactly this. The function
    is simply called `apply()` and is used much like `lapply()` was used previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `apply()` function allows a function to be used on each of the rows or
    columns in a matrix. It uses a `MARGIN` parameter to specify either rows or columns.
    Here, we''ll use `MARGIN = 2`, since we''re interested in the columns (`MARGIN
    = 1` is used for rows). The commands to convert the training and test matrices
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The result will be two character type matrixes, each with cells indicating `"Yes"`
    or `"No"` for whether the word represented by the column appears at any point
    in the message represented by the row.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have transformed the raw SMS messages into a format that can be
    represented by a statistical model, it is time to apply the Naive Bayes algorithm.
    The algorithm will use the presence or absence of words to estimate the probability
    that a given SMS message is spam.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes implementation we will employ is in the `e1071` package. This
    package was developed in the statistics department of the Vienna University of
    Technology (TU Wien), and includes a variety of functions for machine learning.
    If you have not done so already, be sure to install and load the package using
    the `install.packages("e1071")` and `library(e1071)` commands before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many machine learning approaches are implemented in more than one R package,
    and Naive Bayes is no exception. One other option is `NaiveBayes()` in the `klaR`
    package, which is nearly identical to the one in the `e1071` package. Feel free
    to use whichever option you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the k-NN algorithm we used for classification in the previous chapter,
    a Naive Bayes learner is trained and used for classification in separate stages.
    Still, as shown in the following table, these steps are is fairly straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 3 – training a model on the data](img/B03905_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To build our model on the `sms_train` matrix, we''ll use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `sms_classifier` object now contains a `naiveBayes` classifier object that
    can be used to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate the SMS classifier, we need to test its predictions on unseen messages
    in the test data. Recall that the unseen message features are stored in a matrix
    named `sms_test`, while the class labels (spam or ham) are stored in a vector
    named `sms_test_labels`. The classifier that we trained has been named `sms_classifier`.
    We will use this classifier to generate predictions and then compare the predicted
    values to the true values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict()` function is used to make the predictions. We will store these
    in a vector named `sms_test_pred`. We will simply supply the function with the
    names of our classifier and test dataset, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare the predictions to the true values, we''ll use the `CrossTable()`
    function in the `gmodels` package, which we used previously. This time, we''ll
    add some additional parameters to eliminate unnecessary cell proportions and use
    the `dnn` parameter (dimension names) to relabel the rows and columns, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 4 – evaluating model performance](img/B03905_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the table, we can see that a total of only *6 + 30 = 36* of the 1,390
    SMS messages were incorrectly classified (2.6 percent). Among the errors were
    6 out of 1,207 ham messages that were misidentified as spam, and 30 of the 183
    spam messages were incorrectly labeled as ham. Considering the little effort we
    put into the project, this level of performance seems quite impressive. This case
    study exemplifies the reason why Naive Bayes is the standard for text classification;
    directly out of the box, it performs surprisingly well.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the six legitimate messages that were incorrectly classified
    as spam could cause significant problems for the deployment of our filtering algorithm,
    because the filter could cause a person to miss an important text message. We
    should investigate to see whether we can slightly tweak the model to achieve better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that we didn't set a value for the Laplace estimator while
    training our model. This allows words that appeared in zero spam or zero ham messages
    to have an indisputable say in the classification process. Just because the word
    "ringtone" only appeared in the spam messages in the training data, it does not
    mean that every message with this word should be classified as spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll build a Naive Bayes model as done earlier, but this time set `laplace
    = 1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll compare the predicted classes to the actual classifications
    using a cross tabulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 5 – improving model performance](img/B03905_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Adding the Laplace estimator reduced the number of false positives (ham messages
    erroneously classified as spam) from six to five and the number of false negatives
    from 30 to 28\. Although this seems like a small change, it's substantial considering
    that the model's accuracy was already quite impressive. We'd need to be careful
    before tweaking the model too much in order to maintain the balance between being
    overly aggressive and overly passive while filtering spam. Users would prefer
    that a small number of spam messages slip through the filter than an alternative
    in which ham messages are filtered too aggressively.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about classification using Naive Bayes. This algorithm
    constructs tables of probabilities that are used to estimate the likelihood that
    new examples belong to various classes. The probabilities are calculated using
    a formula known as Bayes' theorem, which specifies how dependent events are related.
    Although Bayes' theorem can be computationally expensive, a simplified version
    that makes so-called "naive" assumptions about the independence of features is
    capable of handling extremely large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes classifier is often used for text classification. To illustrate
    its effectiveness, we employed Naive Bayes on a classification task involving
    spam SMS messages. Preparing the text data for analysis required the use of specialized
    R packages for text processing and visualization. Ultimately, the model was able
    to classify over 97 percent of all the SMS messages correctly as spam or ham.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine two more machine learning methods. Each
    performs classification by partitioning data into groups of similar values.
  prefs: []
  type: TYPE_NORMAL

- en: Cross-Validation and Post-Model Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a model with cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balanced cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation with ShuffleSplit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid search with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomized search with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using dummy estimators to compare results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection on L1 norms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting models with joblib or pickle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is perhaps the most important chapter. The fundamental question addressed
    in this chapter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we select a model that predicts well?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the purpose of cross-validation, regardless of what the model is. This
    is slightly different from traditional statistics, which is perhaps more concerned
    with how we understand a phenomenon better. (Why would I limit my quest for understanding?
    Well, because there is more and more data, we cannot necessarily look at it all,
    reflect upon it, and create a theoretical model.)
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is concerned with prediction and how a machine learning algorithm
    processes new unseen data and arrives at predictions. Even if it does not seem
    like traditional statistics, you can use interpretation and domain understanding
    to create new columns (features) and make even better predictions. You can use
    traditional statistics to create new columns.
  prefs: []
  type: TYPE_NORMAL
- en: Very early in the book, we started with training/testing splits. Cross-validation
    is the iteration of many crucial training and testing splits to maximize prediction
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter examines the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation schemes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid searches—what are the best parameters within an estimator?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics that compare `y_test` with `y_pred`—the real target set versus the predicted
    target set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following line contains the cross-validation scheme `cv = 10` for the scoring
    mechanism `neg_log_lost`, which is built from the `log_loss` metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Part of the power of scikit-learn is including so much information in a single
    line. Additionally, we will also see a dummy estimator, have a look at feature
    selection, and save trained models. These methods are what really make machine
    learning what it is.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a model with cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw automatic cross-validation, the `cross_val_score` function, in [Chapter
    1](9a5af114-e518-47ef-ac63-edf9ae69384c.xhtml), *High-Performance Machine Learning
    – NumPy*. This will be very similar, except we will use the last two columns of
    the iris dataset as the data. The purpose of this section is to select the best
    model we can.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting, we will define the best model as the one that scores the highest.
    If there happens to be a tie, we will choose the model that has the best score
    with the least volatility.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the last two features (columns) of the iris dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the data into training and testing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate two **k-nearest neighbors** (**KNN**) algorithms, with three and
    five neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Score both algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the model that scores the best
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Start by loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and testing. The samples are stratified, the default
    throughout the book. Stratified means that the proportions of the target variable
    are the same in both the training and testing sets (also, `random_state` is set
    to `7`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, instantiate two nearest-neighbor algorithms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, score both algorithms using `cross_val_score`. View `kn_3_scores`, a list
    of scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'View `kn_5_scores`, the other list of scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'View basic statistics of both lists. View the means:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'View the spreads, looking at the standard deviations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Overall, `kn_5`, when the algorithm is set to five neighbors, performs a little
    better than three neighbors, yet it is less stable (its scores are a bit more
    all over the place).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now do the final step: select the model that scores the highest. We
    select `kn_5` because it scores the highest. (This model has the highest score
    under cross-validation. Note that the scores involved are the nearest neighbors
    default accuracy score: the proportion of correct classifications divided by all
    of the classifications attempted.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is an example of 4-fold cross-validation because in the `cross_val_score`
    function, `cv = 4`. We split the training data, or **CV Set** (`X_train`), into
    four parts, or folds. We iterate by rotating each fold as the testing set. At
    first, fold 1 is the testing set while folds 2, 3, and 4 are together the training
    set. Then fold 2 is the testing set while folds 1, 3, and 4 are the training set.
    We do this procedure with folds 3 and 4 as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f4fc2e4-4e98-4165-9467-bafb11dff24c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we split the dataset into folds, we score the algorithm four times:'
  prefs: []
  type: TYPE_NORMAL
- en: We train one of the nearest neighbors algorithm on folds 2, 3, and 4.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we predict on fold 1, the test fold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We measure the classification accuracy: compare the test fold with the predicted
    results on that fold. This is the first of the classification scores on the list.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process is performed four times. The final output is a list of four scores.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we did the whole process twice, once for `kn_3` and once for `kn_5`,
    and produced two lists to select the best model. The module we imported from is
    called `model_selection` because it is helping us select the best model.
  prefs: []
  type: TYPE_NORMAL
- en: K-fold cross validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the quest to find the best model, you can view the indices of cross-validation
    folds and see what data is in each fold.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a toy dataset that is very small:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How to do it..
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import `KFold` and select the number of splits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You can iterate through the generator and print out the indices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can see, for example, how in the first round there are two testing indices,
    `0` and `1`. `[0 1]` constitutes the first fold. `[2 3 4 5 6 7]` are folds 2,
    3, and 4 put together.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also view the number of splits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The number of splits is `4`, which we set when we instantiated the `KFold` class.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want, you can view the data in the folds themselves. Store the generator
    as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, `indices_list` is a list of tuples. View the information for the fourth
    fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This information matches the information from the preceding printout, except
    it is in the form of a tuple of two NumPy arrays. View the actual data from the
    fourth fold. View the training data for the fourth fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'View the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Balanced cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While splitting the different folds in various datasets, you might wonder:
    couldn''t the different sets in each fold of k-fold cross-validation be very different?
    The distributions could be very different in each fold, and these differences
    could lead to volatility in the scores.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a solution for this, using stratified cross-validation. The subsets
    of the dataset will look like smaller versions of the whole dataset (at least
    in the target variable).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a toy dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we perform 4-fold cross-validation on this miniature toy dataset, each of
    the four testing folds will have only one value for the target. This can be remedied
    using `StratifiedKFold`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Print out the indices of the folds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Observe that the `split` method of the `skf` class, the stratified k-fold split,
    has two arguments, `X` and `y`. It tries to distribute the target `y` with the
    same distribution in each of the fold sets. In this case, every subset has 50%
    `1` and 50% `2`, just like the whole target set `y`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can use `StratifiedShuffleSplit` to reshuffle the stratified fold. Note
    that this does not try to make four folds with testing sets that are mutually
    exclusive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The splits are not splits of the dataset but iterations of a random procedure,
    each one with a training set size of 75% of the whole dataset and a testing set
    size of 25%. All of the iterations are stratified.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation with ShuffleSplit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ShuffleSplit is one of the simplest cross-validation techniques. Using this
    cross-validation technique will simply take a sample of the data for the number
    of iterations specified.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ShuffleSplit is a simple validation technique. We'll specify the total elements
    in the dataset, and it will take care of the rest. We'll walk through an example
    of estimating the mean of a univariate dataset. This is similar to resampling,
    but it'll illustrate why we want to use cross-validation while showing cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to create the dataset. We''ll use NumPy to create a dataset
    in which we know the underlying mean. We''ll sample half of the dataset to estimate
    the mean and see how close it is to the underlying mean. Generate a normally distributed
    random sample with a mean of 1,000 and a scale (standard deviation) of 10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/201288c4-5c4a-4e64-bb84-2d2395182bc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Estimate the mean of half of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also get the mean of the whole dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It is not 1,000 because random points were selected to create the dataset.
    To observe the behavior of `ShuffleSplit`, write the following and make a plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/7de5d845-f9e0-4854-ba09-c8bc4998efea.png)'
  prefs: []
  type: TYPE_IMG
- en: The estimated mean keeps getting closer to the data's mean of 999.55177343767843
    and then plateaus at being 0.1 away from the data's mean. It is a bit closer than
    the estimate of the mean, with half the dataset, to the mean of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Time series cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn can perform cross-validation for time series data such as stock
    market data. We will do so with a time series split, as we would like the model
    to predict the future, not have an information data leak from the future.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create the indices for a time series split. Start by creating a small
    toy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now create a time series split object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also save the indices by creating a list of tuples from the generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can create rolling windows with NumPy or pandas as well. The main requirement
    of time series cross-validation is that the test set appears after the training
    set in time; otherwise, you would be predicting the past from the future.
  prefs: []
  type: TYPE_NORMAL
- en: Time series cross-validation is interesting because depending on the dataset,
    the influence of time varies. Sometimes, you do not have to put data rows in time-sequential
    order, yet you can never assume you know the future in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of the model selection and cross-validation chapter we tried
    to select the best nearest-neighbor model for the two last features of the iris
    dataset. We will refocus on that now with `GridSearchCV` in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, load the last two features of the iris dataset. Split the data into
    training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instantiate a nearest neighbors classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare a parameter grid, which is necessary for a grid search. A parameter
    grid is a dictionary with the parameter setting you would like to try:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a grid search passing the following as arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The estimator
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameter grid
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A type of cross-validation, `cv=10`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the grid search estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'View the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first chapter, we tried the brute force method, that is, scanning for
    the best score with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The problem with this approach is that it is more time consuming and error prone,
    especially if there are more parameters involved or additional transformations,
    such as those involved in using pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the grid search and brute force approaches both scan all of the possible
    values of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Randomized search with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a practical standpoint, `RandomizedSearchCV` is more important than a regular
    grid search. This is because with a medium amount of data, or with a model involving
    a few parameters, it is too computationally expensive to try every parameter combination
    involved in a complete grid search.
  prefs: []
  type: TYPE_NORMAL
- en: Computational resources are probably better spent stratifying sampling very
    well, or improving randomization procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As before, load the last two features of the iris dataset. Split the data into
    training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instantiate a nearest neighbors classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare a parameter distribution, which is necessary for a randomized grid
    search. A parameter distribution is a dictionary with the parameter setting you
    would like to try randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a randomized grid search passing the following as arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The estimator
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameter distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A type of cross-validation, `cv=10`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of times to run the procedure, `n_iter`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the randomized grid search estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'View the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we actually ran a grid search through all six of the parameters.
    You could have scanned a larger parameter space, however:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Try timing this procedure with IPython:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Time the grid search procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the grid search''s best parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'It turns out that 3-nearest neighbors scores the same as 16-nearest neighbors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we got the same score in one-third of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Whether to use randomized search or not is a decision you have to make on a
    case-by-case basis. You should use a randomized search to try to get a feel for
    an algorithm. It is possible that it performs poorly no matter what the parameters
    are, so then you can move on to a different algorithm. If the algorithm performs
    very well, you can use a complete grid search to find the best parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, instead of focusing on exhaustive searches, you could bag, stack,
    or mix a set of reasonably good algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Classification metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier in the chapter, we explored choosing the best of a few nearest neighbors
    instances based on the number of neighbors, `n_neighbors`, parameter. This is
    the main parameter in nearest neighbors classification: classify a point based
    on the label of KNN. So, for 3-nearest neighbors, classify a point based on the
    label of the three nearest points. Take a majority vote of the three nearest points.'
  prefs: []
  type: TYPE_NORMAL
- en: The classification metric in this case was the internal metric `accuracy_score`,
    which is defined as the number of classifications that were correct divided by
    the total number of classifications. There are alternate metrics, and we will
    explore them here.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start, load the Pima diabetes dataset from the UCI repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To review the previous section, run a randomized search using the KNN algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then display the best accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, look at the confusion matrix on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The confusion matrix gives more specific information about how the model performed.
    There were 27 times when the model predicted someone did not have diabetes even
    though they did. This is a more serious mistake than the 16 people thought to
    have diabetes that really did not.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this situation, we want to maximize sensitivity or recall. While examining
    linear models, we looked at the definition of recall, or sensitivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1a676a1-db39-4460-b560-18ba206d5bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the sensitivity score in this case is 27/ (27 + 27) = 0.5\. With scikit-learn,
    we can conveniently compute this as follows.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import from the metrics module `recall_score`. Measure the sensitivity of the
    set using `y_test` and `y_pred`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We recovered the recall score we computed by hand beforehand. In the randomized
    search, we could have used the `recall_score` to find the nearest neighbor instance
    with the highest recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `make_scorer` and use the function with two arguments, `recall_score`
    and `greater_is_better`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now perform a randomized grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now look at the highest score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the recall score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'It is the same as before. In the randomized search you could have tried the
    `roc_auc_score`, the ROC area under the curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You could design your own scorer for classification. Suppose that you are an
    insurance company and that you have associated costs for each cell in the confusion
    matrix. The relative costs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8334234-cca1-421f-8d45-e157d3765ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The cost for the confusion matrix we were looking at can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now add up the total cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now place it in a scorer and run the grid search. The argument within the scorer
    `greater_is_better` is set to `False`, because costs should be as low as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The score is negative because when the `greater_is_better` argument in the `make_scorer`
    function is false, the score is multiplied by `-1`. The grid search attempts to
    maximize this score, thereby minimizing the absolute value of the score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost on the test set is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: While looking at this number, do not forget to look at the number of individuals
    involved in the test set, which is 154\. The average cost per person is about
    $21.8.
  prefs: []
  type: TYPE_NORMAL
- en: Regression metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation with a regression metric is straightforward with scikit-learn.
    Either import a score function from `sklearn.metrics` and place it within a `make_scorer`
    function, or you could create a custom scorer for a particular data science problem.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load a dataset that utilizes a regression metric. We will load the Boston housing
    dataset and split it into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We do not know much about the dataset. We can try a quick grid search using
    a high variance algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Try a different model, this time a linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Both regressors, by default, measure `r2_score`, R-squared, so the linear model
    is far better. Try a different complex model, an ensemble of trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The ensemble performs even better. You can try a random forest as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we could focus on making gradient boosting better with the current score
    mechanism by maximizing the internal R-squared gradient boosting scorer. Try one
    or two randomized searches. This is a second one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimizing for R-squared returned the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The trees in the gradient boost have a depth of three.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Make a scoring function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a scikit-scorer with that function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a grid search to find the best gradient boost parameters to minimize the
    error function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the mean percentage error scoring function with the Numba **just-in-time**
    (**JIT**) compiler. The original NumPy function looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s rewrite the function using the Numba JIT compiler to make things a bit
    faster. You can write C-like code, indexing arrays by location using Numba:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now make a scorer. The lower the score the better, unlike R-squared, in which
    higher is better:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run a grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Using this metric, the best score corresponds to a gradient boost with trees
    of depth one.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring the performance of a clustering algorithm is a little trickier than
    classification or regression, because clustering is unsupervised machine learning.
    Thankfully, scikit-learn comes equipped to help us with this as well in a very
    straightforward manner.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To measure clustering performance, start by loading the iris dataset. We will
    relabel the iris flowers as two types: type 0 is whenever the target is 0 and
    type 1 is when the target is 1 or 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instantiate a k-means algorithm and train it. Since the algorithm is a clustering
    one, do not use the target in the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now import everything necessary to score k-means through cross-validation.
    We will use the `adjusted_rand_score` clustering performance metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Scoring a clustering algorithm is very similar to scoring a classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Using dummy estimators to compare results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe is about creating fake estimators; this isn't the pretty or exciting
    stuff, but it is worthwhile having a reference point for the model you'll eventually
    build.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Create some random data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the various dummy estimators.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll perform these two steps for regression data and classification data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we''ll create the random data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the estimator will predict by just taking the mean of the values
    and outputting it multiple times::'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: There are other two other strategies we can try. We can predict a supplied constant
    (refer to `constant=None` in the first command block in this recipe). We can also
    predict the median value. Supplying a constant will only be considered if strategy
    is constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We actually have four options for classifiers. These strategies are similar
    to the continuous case, it''s just slanted toward classification problems:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's always good to test your models against the simplest models, and that's
    exactly what the dummy estimators give you. For example, imagine a fraud model.
    In this model, only 5% of the dataset is fraudulent. Therefore, we can probably
    fit a pretty good model just by never guessing that the data is fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create this model by using the stratified strategy using the following
    command. We can also get a good example of why class imbalance causes problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: We were actually correct very often, but that's not the point. The point is
    that this is our baseline. If we cannot create a model for fraud that is more
    accurate than this, then it isn't worth our time.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe, along with the two following it, will be centered around automatic
    feature selection. I like to think of this as the feature analog of parameter
    tuning. In the same way that we cross-validate to find an appropriately general
    parameter, we can find an appropriately general subset of features. This will
    involve several different methods.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest idea is univariate selection. The other methods involve working
    with a combination of features.
  prefs: []
  type: TYPE_NORMAL
- en: An added benefit of feature selection is that it can ease the burden on the
    data collection. Imagine that you have built a model on a very small subset of
    the data. If all goes well, you might want to scale up to predict the model on
    the entire subset of data. If this is the case, you can ease the engineering effort
    of data collection at that scale.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With univariate feature selection, scoring functions will come to the forefront
    again. This time, they will define the comparable measure by which we can eliminate
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll fit a regression model with around 10,000 features,
    but only 1,000 points. We''ll walk through the various univariate feature selection
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the data, we will compare the features that are included with
    the various methods. This is actually a very common situation when you're dealing
    with text analysis or some areas of bioinformatics.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to import the `feature_selection` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `f` is the `f` score associated with each linear model fit with just
    one of the features. We can then compare these features and based on this comparison,
    we can cull features. `p` is the `p` value associated with the `f` value. In statistics,
    the `p` value is the probability of a value more extreme than the current value
    of the test statistic. Here, the `f` value is the test statistic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, many of the `p` values are quite large. We want the `p` values
    to be quite small. So, we can grab NumPy out of our toolbox and choose all the
    `p` values less than `.05`. These will be the features we''ll use for our analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we're actually keeping a relatively large number of features.
    Depending on the context of the model, we can tighten this `p` value. This will
    lessen the number of features kept.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is using the `VarianceThreshold` object. We've learned a bit
    about it, but it's important to understand that our ability to fit models is largely
    based on the variance created by features. If there is no variance, then our features
    cannot describe the variation in the dependent variable. A nice feature of this,
    as per the documentation, is that because it does not use the outcome variable,
    it can be used for unsupervised cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to set the threshold for which we eliminate features. In order
    to do that, we just take the median of the feature variances and supply that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we have eliminated roughly half the features, more or less what
    we would expect.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, all these methods work by fitting a basic model with a single feature.
    Depending on whether we have a classification problem or a regression problem,
    we can use the appropriate scoring function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a smaller problem and visualize how feature selection will eliminate
    certain features. We''ll use the same scoring function from the first example,
    but just 20 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s plot the p values of the features. We can see which features will
    be eliminated and which will be kept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a8f4c0f9-e43a-4714-8c0b-05890091b2d1.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, many of the features won't be kept, but several will be.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection on L1 norms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to work with some ideas that are similar to those we saw in the
    recipe on LASSO regression. In that recipe, we looked at the number of features
    that had zero coefficients. Now we're going to take this a step further and use
    the sparseness associated with L1 norms to pre-process the features.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use the diabetes dataset to fit a regression. First, we'll fit a basic
    linear regression model with a ShuffleSplit cross-validation. After we do that,
    we'll use LASSO regression to find the coefficients that are zero when using an
    L1 penalty. This hopefully will help us to avoid overfitting (when the model is
    too specific to the data it was trained on). To put this another way, the model,
    if it overfits, does not generalize well to outside data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a basic linear regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use feature selection to remove uninformative features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refit the linear regression and check to see how well it fits compared with
    the fully featured model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s get the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the `LinearRegression` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also import from the metrics module the `mean_squared_error` function
    and `make_scorer` wrapper. From the `model_selection` module, import the `ShuffleSplit`
    cross-validation scheme and the `cross_val_score` cross-validation scorer. Go
    ahead and score the function with the `mean_squared_error` metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'So now that we have the regular fit, let''s check it after eliminating any
    features with a zero for the coefficient. Let''s fit the LASSO regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll remove the first feature. I''ll use a NumPy array to represent the columns
    that are to be included in the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Okay, so now we''ll fit the model with the specific features (see the columns
    in the following code block):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: The score afterwards is not much better than the score before, even though we
    eliminated an uninformative feature. We will see an additional example in the
    *There's more...* section.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we''re going to create a regression dataset with many uninformative
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `ShuffleSplit` instance with 10 iterations, `n_splits=10`. Measure
    the cross-validation score of plain linear regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate `LassoCV` to eliminate uninformative columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Eliminate uninformative columns. Look at the final scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: The fit is a lot better at the end after we removed uninformative features.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting models with joblib or pickle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we're going to show how you can keep your model around for later
    use. For example, you might want to actually use a model to predict an outcome
    and automatically make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a dataset and train a classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Save the training work the classifier has done with joblib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Opening the saved model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the model with joblib. Make a prediction with a set of inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: We did not have to train the model again, and have saved a lot of training time.
    We simply reloaded it with joblib and made a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also use the `cPickle` module in Python 2.x or the `pickle` module
    in Python 3.x. Personally, I use this module for several types of Python classes
    and objects:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin by importing `pickle`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `dump()` module method. It has three arguments: the data being saved,
    the file it is being saved to, and the pickle protocol. The following saves the
    trained tree to the `dtree.save` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Open `dtree.save` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'View the tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE

<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predict whether Your Content Will Go Viral</h1>
                </header>
            
            <article>
                
<p><span class="HeaderFooterPACKT">Like many great things, this all begins with a bet. It was 2001, and Jonah Peretti, a graduate student at MIT at the time, was procrastinating. Instead of writing his thesis, he had decided to take up Nike on their offer to personalize a pair of sneakers. Under a recently launched program, anyone could do so from their website, NIKEiD. The only problem, at least from Nike's point of view, was that emblazoning them with the word <em>sweatshop</em>, as Peretti had requested, was a non-starter. Peretti, in a series of emails, demurred pointing out that in no way did the word fall into any of the categories of objectionable terms that would result in his personalization request being rejected.</span></p>
<p><span class="HeaderFooterPACKT">Peretti, believing others might find the back-and-forth with Nike's customer service representatives amusing as well, forwarded them to a number of close friends. Within days, the emails had found their way into inboxes across the world. Major media outlets, such as Time, Salon, The Guardian, and even the Today Show, had picked up on it. Peretti was at the center of a viral sensation.</span></p>
<p><span class="HeaderFooterPACKT">But the question that began nagging at Peretti was, could this sort of thing be replicated? His friend, Cameron Marlow, had been preparing to write his PhD thesis on viral phenomena, and was adamant that such things were far too complex for anyone to engineer. And it is here that the bet comes into play. Marlow wagered that Peretti could not repeat the success he had enjoyed with that original set of emails with Nike.</span></p>
<p><span class="HeaderFooterPACKT">Fast forward 15 years, and Jonah Peretti leads the website whose name has become synonymous with virality—BuzzFeed. With more than 77 million unique visitors in 2015, it ranked higher than the New York Times in total reach. I think it's safe to say that Peretti won that bet.</span></p>
<p><span class="HeaderFooterPACKT">But how exactly did Peretti do it? How did he piece together the secret formula for creating content that spreads like wildfire? In this chapter, we'll attempt to unravel some of these mysteries. We'll examine some of the most shared content and attempt to find the common elements that differentiate it from the content people were less willing to share.</span></p>
<p><span class="HeaderFooterPACKT">The following topics will be covered in this chapter:</span></p>
<ul>
<li>What does research tell us about virality?</li>
<li>Sourcing shared counts and content</li>
<li>Exploring the features of shareability</li>
<li>Building a predictive content scoring model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What does research tell us about virality?</h1>
                </header>
            
            <article>
                
<p>Understanding sharing behavior is big business. As consumers become increasingly blind to traditional advertising year on year, the push is on to go beyond simple pitches to tell engaging stories. And increasingly, the success of these endeavors is measured in social shares. Why go to so much trouble? Because, as a brand, every share I receive represents another consumer I've reached—all without spending an additional cent.</p>
<p>Because of this value, several researchers have examined sharing behavior in the hope of understanding what motivates it. Among the reasons researchers have found are the following:</p>
<ul>
<li>To provide practical value to others (an altruistic motive)</li>
<li>To associate ourselves with certain ideas and concepts (an identity motive)</li>
<li>To bond with others around a common emotion (a communal motive)</li>
</ul>
<p>With regard to the last motive, one particularly well-designed study looked at the 7,000 pieces of content from the New York Times to examine the effect of emotion on sharing. They found that simple emotional sentiment was not enough to explain sharing behavior, but when combined with emotional arousal, the explanatory power was greater.</p>
<p>For example, while sadness has a strong negative valence, it is considered to be a low arousal state. Anger, on the other hand, has a negative valence, which is paired with a high arousal state. As such, stories that sadden the reader tend to generate far fewer stories than anger-inducing stories. Is it any wonder then that much of the <em>fake news</em> that plays such a large part in politics these days comes in this form? Following image shows the same result:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-522 image-border" src="assets/9b9b8c3b-d282-4c1f-890b-1b728d14e6cf.png" style="width:33.17em;height:38.50em;"/></p>
<div class="packt_figref">Figure taken from <em>What Makes Online Content Viral?</em> by Jonah Berger and Katherine L. Milkman, Journal of Marketing Research, available at: http://jonahberger.com/wp-content/uploads/2013/02/ViralityB.pdf</div>
<p>This covers the motivational aspects, but if we hold those factors constant, how do other attributes affect the virality of a piece of content? Some of these factors could include the following: headline wording, headline length, headline parts of speech, content length, social network of post, the topic, the timeliness of the subject matter, and so on. Without a doubt, a person could spend their entire life studying this phenomenon. For now, however, we'll just spend the next 30 or so pages doing so. From there, you can decide whether you'd like to take it further.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sourcing shared counts and content</h1>
                </header>
            
            <article>
                
<p>Before we can begin exploring which features make content shareable, we need to get our hands on a fair amount of content, as well as data on how often it's shared. Unfortunately, securing this type of data has gotten more difficult in the last few years. In fact, when the first edition of this book came out in 2016, this data was easily obtainable. But today, there appears to be no free sources of this type of data, though if you are willing to pay, you can still find it.</p>
<p>Fortunately for us, I have a dataset that was collected from a now defunct website, <kbd><span class="URLPACKT">ruzzit.com</span></kbd>. This site, when it was active, tracked the most shared content over time, which is exactly what we require for this project:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1004 image-border" src="assets/bca0d0d0-360c-413e-a842-42d21fb7502f.png" style="width:44.25em;height:23.33em;"/></p>
<p>We'll begin by loading our imports into our notebook, as we always do, and then load in the data. This particular data is in the form of a JSON file. We can read it in using the pandas <kbd>read_json()</kbd> method, as demonstrated in the following code block:</p>
<pre><strong>import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
%matplotlib inline 
 
dfc = pd.read_json('viral_dataset.json') 
dfc.reset_index(drop=True, inplace=True) 
dfc</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-524 image-border" src="assets/89f77732-6fef-499e-b962-3e4066a30c3e.png" style="width:41.42em;height:22.25em;"/></p>
<p>Let's take a look at the columns of this dataset to better understand what we'll be working with:</p>
<pre><strong>dfc.columns</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-525 image-border" src="assets/65c84592-3c86-4e83-8d6d-788da676f882.png" style="width:101.50em;height:10.83em;"/></p>
<p>Now, let's walk through what each of these columns represents:</p>
<ul>
<li><kbd>title</kbd>: The title of the article</li>
<li><kbd>link</kbd>: The <kbd>ruzzit.com</kbd> link</li>
<li><kbd>bb</kbd>: The number of Facebook likes</li>
<li><kbd>lnkdn</kbd>: The number of LinkedIn shares</li>
<li><kbd>pins</kbd>: The number of Pinterest pins</li>
<li><kbd>date</kbd>: The date of the article</li>
<li><kbd>redirect</kbd>: The link to the original article</li>
<li><kbd>pg_missing</kbd>: A field that describes whether that page is available</li>
<li><kbd>img_link</kbd>: The link to the image for the article</li>
<li><kbd>json_data</kbd>: Additional data pertaining to the article</li>
<li><kbd>site</kbd>: The domain the article is hosted on</li>
<li><kbd>img_count</kbd>: The number of images contained in the article</li>
<li><kbd>entities</kbd>: Person-, place-, and thing-related features of the article</li>
<li><kbd>html</kbd>: The body of the article</li>
<li><kbd>text</kbd>: The text of the body of the article</li>
</ul>
<p>Another feature that will be instructive is the word count of each article. We don't have that in our data currently, so let's go ahead and create a function that will provide this for us:</p>
<pre><strong>def get_word_count(x): 
    if not x is None: 
        return len(x.split(' ')) 
    else: 
        return None 
 
dfc['word_count'] = dfc['text'].map(get_word_count) 
dfc</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-526 image-border" src="assets/efe55afa-952b-41db-b88c-7a3f2dae6855.png" style="width:147.50em;height:38.17em;"/></p>
<p>Let's add more features. We'll add the most prominent color of the first image on the page. The colors for each image are listed by RGB value in the JSON data, so we can extract it from there:</p>
<pre><strong>import matplotlib.colors as mpc 
 
def get_rgb(x): 
    try: 
        if x.get('images'): 
            main_color = x.get('images')[0].get('colors')[0].get('color') 
            return main_color 
    except: 
        return None 
 
def get_hex(x): 
    try: 
        if x.get('images'): 
            main_color = x.get('images')[0].get('colors')[0].get('color') 
            return mpc.rgb2hex([(x/255) for x in main_color]) 
    except: 
        return None</strong> 
<strong> 
dfc['main_hex'] = dfc['json_data'].map(get_hex) 
dfc['main_rgb'] = dfc['json_data'].map(get_rgb) 
 
dfc</strong> </pre>
<p>The preceding code <span>generates </span>the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-527 image-border" src="assets/e8be8d5f-ad65-4e2d-8fbb-ddf8ec321b25.png" style="width:19.33em;height:14.25em;"/></p>
<p>We've pulled the most prominent color from the first image as an RGB value, but we have also transformed that into a hex value. We'll use that later when we examine the image colors.</p>
<p>With our data now ready, we can begin to perform our analysis. We're going to attempt to find what makes content highly shareable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the features of shareability</h1>
                </header>
            
            <article>
                
<p>The stories we have collected here represent roughly the 500 most shared pieces of content in 2015 and early 2016. We're going to try to deconstruct these articles to find the common traits that make them so shareable. We'll begin by looking at the image data.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring image data</h1>
                </header>
            
            <article>
                
<p>Let's begin by looking at the number of images included with each story. We'll run a value count and then plot the numbers:</p>
<pre><strong>dfc['img_count'].value_counts().to_frame('count')</strong> </pre>
<p>This should display an output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-528 image-border" src="assets/d6207023-2df1-40aa-a943-0b94c084e7bd.png" style="width:7.67em;height:15.92em;"/></p>
<p>Now, let's plot that same information:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(8,6)) 
y = dfc['img_count'].value_counts().sort_index() 
x = y.sort_index().index 
plt.bar(x, y, color='k', align='center') 
plt.title('Image Count Frequency', fontsize=16, y=1.01) 
ax.set_xlim(-.5,5.5) 
ax.set_ylabel('Count') 
ax.set_xlabel('Number of Images')</strong> </pre>
<p>This code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-529 image-border" src="assets/420c6673-50df-4242-bbff-e11f21996146.png" style="width:37.17em;height:29.33em;"/></p>
<p>Already, I'm surprised by the numbers. The vast majority of stories have five pictures in them, while those stories that have either one or no pictures at all are quite rare.</p>
<p>Hence, we can see that people tend to share content with lots of images. Now, let's take a look at the most common colors in those images:</p>
<pre><strong>mci = dfc['main_hex'].value_counts().to_frame('count') 
 
mci</strong> </pre>
<p>This code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-530 image-border" src="assets/eb32ef86-1152-4fdf-8a25-5b11cac04f3d.png" style="width:9.50em;height:24.42em;"/></p>
<p>I don't know about you, but this isn't extremely helpful given that I don't see hex values as colors. We can, however, use a new feature in pandas called conditional formatting to help us out:</p>
<pre><strong>mci['color'] = ' ' 
 
def color_cells(x): 
    return 'background-color: ' + x.index 
 
mci.style.apply(color_cells, subset=['color'], axis=0) 
 
mci</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-531 image-border" src="assets/ebb9a741-3b26-4c14-9955-65c53d80d847.png" style="width:12.67em;height:23.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p><span>This certainly helps, but the colors are so granular that we have over 450</span> unique<span> colors in total.</span> Let's use a bit of clustering to get <span>this down to a more manageable range. Since we have the RBG values for each color, we can create a three-dimensional</span> space <span>to cluster them using the k-means algorithm. I won't go into the details of the algorithm here, but it is a fairly simple iterative algorithm based</span> upon <span>generating clusters by measuring the distance to centers and repeating. The algorithm does require us to select the</span> <em>k</em><span>, or the number of clusters we expect. Because RGB ranges from 0 to 256, we'll use the square root of 256, which is 16. That should give us a manageable number while retaining the characteristics of our palette.</span></p>
<p>First, we'll split our RGB values into individual columns:</p>
<pre><strong>def get_csplit(x): 
    try: 
        return x[0], x[1], x[2] 
    except: 
        return None, None, None 
 
dfc['reds'], dfc['greens'], dfc['blues'] = zip(*dfc['main_rgb'].map(get_csplit))</strong> </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Next, we'll use this to run our k-means model and retrieve the center values:</p>
<pre><strong>from sklearn.cluster import KMeans 
 
clf = KMeans(n_clusters=16) 
clf.fit(dfc[['reds', 'greens', 'blues']].dropna()) 
 
clusters = pd.DataFrame(clf.cluster_centers_, columns=['r', 'g', 'b']) 
 
clusters</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-532 image-border" src="assets/99774024-acd8-44f3-afc6-9911ae7f455c.png" style="width:20.25em;height:24.08em;"/></p>
<p>Now, we have the sixteen most popular dominant colors from the first image in each picture. Let's check whether they are using our pandas <kbd>DataFrame.style()</kbd> method and the function we created previously to color our cells. We'll need to set our index equal to the hex value of the three columns to use our <kbd>color_cells</kbd> function, so we'll do that as well:</p>
<pre><strong>def hexify(x): 
    rgb = [round(x['r']), round(x['g']), round(x['b'])] 
    hxc = mpc.rgb2hex([(x/255) for x in rgb]) 
    return hxc 
 
clusters.index = clusters.apply(hexify, axis=1) 
 
clusters['color'] = ' ' 
 
clusters.style.apply(color_cells, subset=['color'], axis=0)</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-533 image-border" src="assets/b49f4800-869d-4ba3-b975-48fc31cfe1b2.png" style="width:20.33em;height:31.58em;"/></p>
<p>So there you have it; those are the most common colors you will see (at least for the first image) in the most frequently shared content. This is a bit more on the drab side than I had expected as the first several all seem to be shades of beige and gray.</p>
<p>Now, let's move on and examine the headlines of our stories.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the headlines</h1>
                </header>
            
            <article>
                
<p>Let's start by creating a function we can use to examine the most common tuples. We'll set it up so that we can use it later on the body text as well. We'll do this using the Python <strong>Natural Language Toolkit</strong> (<span><strong>NLTK</strong>)</span> library. This can be pip installed if you don't have it currently:</p>
<pre><strong>from nltk.util import ngrams 
from nltk.corpus import stopwords 
import re 
 
def get_word_stats(txt_series, n, rem_stops=False): 
    txt_words = [] 
    txt_len = [] 
    for w in txt_series: 
        if w is not None: 
            if rem_stops == False: 
                word_list = [x for x in ngrams(re.findall('[a-z0-9\']+', w.lower()), n)] 
            else: 
                word_list = [y for y in ngrams([x for x in re.findall('[a-z0-9\']+', w.lower())\ 
                                                if x not in stopwords.words('english')], n)] 
            word_list_len = len(list(word_list)) 
            txt_words.extend(word_list) 
            txt_len.append(word_list_len) 
    return pd.Series(txt_words).value_counts().to_frame('count'), pd.DataFrame(txt_len, columns=['count']) </strong></pre>
<p>There is a lot in there, so let's unpack it. We created a function that takes in a series, an integer, and a Boolean value. The integer determines the <em>n</em> we'll use for n-gram parsing, while the Boolean determines whether or not we exclude stop words. The function returns the number of tuples per row and the frequency for each tuple.</p>
<p>Let's run it on our headlines, while retaining the stop words. We'll begin with just single words:</p>
<pre><strong>hw,hl = get_word_stats(dfc['title'], 1, 0) 
 
hl</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-534 image-border" src="assets/2b68bf0e-a421-4183-8ac8-162b1fa0612f.png" style="width:7.50em;height:27.00em;"/></p>
<p>Now, we have the word count for each headline. Let's see what the stats on this look like:</p>
<pre><strong>hl.describe()</strong> </pre>
<p><span class="fontstyle0">This code generates the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-535 image-border" src="assets/8a848a0e-9844-406f-9848-57aba1428cfc.png" style="width:9.67em;height:16.42em;"/></p>
<p>We can see that the median headline length for our viral stories comes in at exactly 11 words. Let's take a look at the most frequently used words:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-536 image-border" src="assets/94de7857-688e-4dcc-af0f-68970d4e398e.png" style="width:9.42em;height:19.83em;"/></p>
<p>That is not exactly useful, but is in keeping with what we might expect. Now, let's take a look at the same information for bi-grams:</p>
<pre><strong>hw,hl = get_word_stats(dfc['title'], 2, 0) 
 
hw</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-537 image-border" src="assets/3b85a514-c0c3-4493-bf6c-804bb0a83c1a.png" style="width:11.92em;height:18.42em;"/></p>
<p>This is definitely more interesting. We can start to see some of the components of the headlines over and over again. The two that stand out are <kbd>(donald, trump)</kbd> and <kbd>(dies, at)</kbd>. Trump makes sense as he said some headline-grabbing statements during the election, but I was surprised by the <em>dies</em> headlines. I took a look at the headlines, and apparently a number of high-profile people died in the year in question, so that also makes sense.</p>
<p>Now, let's run this with the stop words removed:</p>
<pre><strong>hw,hl = get_word_stats(dfc['title'], 2, 1) 
 
hw</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-538 image-border" src="assets/e644b50e-4c13-449f-87e8-70d0bf290465.png" style="width:14.67em;height:23.67em;"/></p>
<p>Again, we can see many things we might expect. It looks like if we changed how we parsed numbers (replacing each of them with a single identifier like number), we would likely see more of these bubble up. I'll leave that as an exercise to the reader, if you'd like to attempt that.</p>
<p>Now, let's take a look at tri-grams:</p>
<pre><strong>hw,hl = get_word_stats(dfc['title'], 3, 0)</strong> </pre>
<p><span class="fontstyle0">This code generates the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-539 image-border" src="assets/9d842935-566f-41b8-9acd-1dd4dfcc85c2.png" style="width:17.83em;height:27.00em;"/></p>
<p>It seems that the more words we include, the more the headlines come to resemble the classic BuzzFeed prototype. In fact, let's see whether that's the case. We haven't looked at which sites produce the most viral stories; let's see whether BuzzFeed leads the charts:</p>
<pre><strong>dfc['site'].value_counts().to_frame()</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1005 image-border" src="assets/ca85393f-97c5-4256-a0cf-771359f589e1.png" style="width:20.25em;height:36.25em;"/></p>
<p>We can clearly see that BuzzFeed dominates the list. In a distant second place, we can see The Huffington Post, which incidentally is another site that Jonah Peretti worked for. It appears that studying the science of virality can pay big dividends.</p>
<p>So far, we have examined images and headlines. Now, let's move on to examining the full text of the stories.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the story content</h1>
                </header>
            
            <article>
                
<p>In the last section, we created a function to examine the common n-grams that are found in the headlines of our stories. Now, let's apply that to explore the full content of our stories.</p>
<p>We'll start by exploring bi-grams with the stop words removed. Since headlines are so short compared to the body of the stories, it makes sense to look at them with the stop words intact, although within the story, it typically makes sense to eliminate them:</p>
<pre><strong>hw,hl = get_word_stats(dfc['text'], 2, 1) 
 
hw</strong> </pre>
<p>This <span>generates </span>the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-541 image-border" src="assets/134d9d43-1e9e-4c0d-9f18-fc64adf7b4be.png" style="width:13.67em;height:38.17em;"/></p>
<p>Interestingly, we can see that the frivolity we saw in the headlines has completely disappeared. The text is now filled with content discussing terrorism, politics, and race relations.</p>
<p>How is it possible that the headlines are light-hearted, while the text is dark and controversial? I would suggest that this is because articles such as <em>13 Puppies Who Look Like Elvis</em> are going to have substantially less text than <em>The History of the Islamic State</em>.</p>
<p>Let's take a look at one more. We'll evaluate the tri-grams for the story bodies:</p>
<pre><strong>hw,hl = get_word_stats(dfc['text'], 3, 1) 
 
hw</strong> </pre>
<p><span class="fontstyle0">This code generates the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-542 image-border" src="assets/7c15e854-45cf-4047-9aa0-8b7fc40323a0.png" style="width:20.08em;height:34.50em;"/></p>
<p>We appear to have suddenly entered the land of advertising and social pandering. With that, let's move on to building a predictive model for content scoring.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a predictive content scoring model</h1>
                </header>
            
            <article>
                
<p>Let's use what we have learned to create a model that can estimate the share counts for a given piece of content. We'll use the features we have already created, along with a number of additional ones.</p>
<p>Ideally, we would have a much larger sample of content—especially content that had more typical share counts—but we'll have to make do with what we have here.</p>
<p>We're going to be using an algorithm called <strong>random forest regression</strong>. In previous chapters, we looked at a more typical implementation of random forests that is based on classification, but here we're going to attempt to predict the share counts. We could consolidate our share classes into ranges, but it is preferable to use regression when dealing with continuous variables, which is what we're working with here.</p>
<p>To begin, we'll create a bare-bones model. We'll use the number of images, the site, and the word count. We'll train our model in terms of the number of Facebook likes. We're also going to be splitting our data into two sets: a training set and a test set.</p>
<p>First, we'll import the scikit-learn library, and then we'll prepare our data by removing the rows with nulls, resetting our index, and finally splitting the frame into our training and test set:</p>
<pre><strong>from sklearn.ensemble import RandomForestRegressor 
 
all_data = dfc.dropna(subset=['img_count', 'word_count']) 
all_data.reset_index(inplace=True, drop=True) 
 
train_index = [] 
test_index = [] 
for i in all_data.index: 
    result = np.random.choice(2, p=[.65,.35]) 
    if result == 1: 
        test_index.append(i) 
    else: 
        train_index.append(i)</strong> </pre>
<p>We used a random number generator with a probability set for approximately two-thirds and one-third to determine which row items (based on their <kbd>index</kbd>) would be placed in each set. Setting the probabilities like this ensures that we get approximately twice the number of rows in our training set compared to the test set. We can see this in the following code:</p>
<pre><strong>print('test length:', len(test_index), '\ntrain length:', len(train_index))</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-543 image-border" src="assets/4a1c3eea-65ba-4512-b7f8-98619dfc242d.png" style="width:12.08em;height:3.17em;"/></p>
<p>Now, we'll continue with preparing our data. Next, we need to set up categorical encoding for our sites. Currently, our DataFrame has the name for each site represented with a string. We need to use dummy encoding. This creates a column for each site, and if the row has that particular site, then that column will be filled with a <kbd>1</kbd>, while all the other columns for sites will be coded with a <kbd>0</kbd>. Let's do that now:</p>
<pre><strong>sites = pd.get_dummies(all_data['site']) 
 
sites</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1006 image-border" src="assets/e33a452e-3b1d-4e81-bcad-405e5ca0f5ab.png" style="width:48.42em;height:19.08em;"/></p>
<p>You can see from the preceding output how the dummy encoding appears.</p>
<p>We'll now continue:</p>
<pre><strong>y_train = all_data.iloc[train_index]['fb'].astype(int) 
X_train_nosite = all_data.iloc[train_index][['img_count', 'word_count']] 
 
X_train = pd.merge(X_train_nosite, sites.iloc[train_index], left_index=True, right_index=True) 
 
y_test = all_data.iloc[test_index]['fb'].astype(int) 
X_test_nosite = all_data.iloc[test_index][['img_count', 'word_count']] 
 
X_test = pd.merge(X_test_nosite, sites.iloc[test_index], left_index=True, right_index=True)</strong> </pre>
<p>With that, we've set up our <kbd>X_test</kbd>, <kbd>X_train</kbd>, <kbd>y_test</kbd>, and <kbd>y_train</kbd> variables. Now, we're going to use our training data to build our model:</p>
<pre><strong>clf = RandomForestRegressor(n_estimators=1000) 
clf.fit(X_train, y_train)</strong> </pre>
<p>With those two lines of code, we have trained our model. Let's use it to predict the Facebook likes for our test set:</p>
<pre><strong>y_actual = y_test 
deltas = pd.DataFrame(list(zip(y_pred, y_actual, (y_pred - y_actual)/(y_actual))), columns=['predicted', 'actual', 'delta']) 
 
deltas</strong> </pre>
<p>This code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-545 image-border" src="assets/652a88fe-3dc0-4869-9bbf-bae54f746b38.png" style="width:16.42em;height:22.33em;"/></p>
<p>Here, we can see the predicted values, the actual value, and the difference as a percentage  side by side. Let's take a look at the descriptive stats for this:</p>
<pre><strong>deltas['delta'].describe()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-546 image-border" src="assets/85467ba6-ebe3-4d00-bb24-7d9c290c189a.png" style="width:18.00em;height:11.67em;"/></p>
<p>This looks amazing. Our median error is 0! Well, unfortunately, this is a particularly useful bit of information as errors are on both sides—positive and negative—and tend to average out, which is what we can see here. Let's look at a more informative metric to evaluate our model. We're going to look at root mean square error as a percentage of the actual mean.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>To illustrate why this is more useful, let's run the following scenario on two sample series:</p>
<pre><strong>a = pd.Series([10,10,10,10]) 
b = pd.Series([12,8,8,12]) 
 
np.sqrt(np.mean((b-a)**2))/np.mean(a)</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-547 image-border" src="assets/3e6c455f-9fd7-426a-ad2c-83d1f49e5d69.png" style="width:4.00em;height:2.17em;"/></p>
<p>Now, compare that to the mean:</p>
<pre><strong>(b-a).mean()</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-548 image-border" src="assets/8601f8fe-5653-4dea-acef-3aa10929af7a.png" style="width:3.33em;height:2.17em;"/></p>
<p class="mce-root"/>
<p>Clearly, the latter is the more meaningful statistic. Now, let's run it for our model:</p>
<pre><strong>np.sqrt(np.mean((y_pred-y_actual)**2))/np.mean(y_actual)</strong> </pre>
<p>This generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-549 image-border" src="assets/31be237c-21be-42c2-8426-e4c19010c4ea.png" style="width:13.75em;height:2.75em;"/></p>
<p>Suddenly, our awesome model looks a lot less awesome. Let's take a look at some of the predictions our model made versus the actual values that can be seen in the data:</p>
<pre><strong>deltas[['predicted','actual']].iloc[:30,:].plot(kind='bar', figsize=(16,8))</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-550 image-border" src="assets/651e35ce-564f-46c1-9ab5-84c5368adffc.png" style="width:144.17em;height:71.67em;"/></p>
<p>Based on what we can see here, the model—at least for this sample—tends to modestly underpredict the virality of the typical article, but then heavily underpredicts the virality for a small number. Let's see what those are:</p>
<pre><strong>all_data.loc[test_index[:30],['title', 'fb']]</strong><strong>.reset_index(drop=True)</strong> </pre>
<p class="mce-root"/>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-551 image-border" src="assets/dc0d413e-750f-4ab1-80ac-4ed042511fca.png" style="width:105.67em;height:98.67em;"/></p>
<p>From the preceding output, we can see that an article on <em>Malala</em> and an article on <em>a husband complaining about how much his stay-at-home wife costs him</em> greatly overshot the predicted numbers of our model. Both would seem to have high emotional valence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding new features to our model</h1>
                </header>
            
            <article>
                
<p>Now, let's add another feature to our model. Let's see whether adding the counts for words will help our model. We'll use a <kbd>CountVectorizer</kbd> to do this. Much like what we did with the site names, we'll be transforming individual words and n-grams into features:</p>
<pre><strong>from sklearn.feature_extraction.text import CountVectorizer 
 
vect = CountVectorizer(ngram_range=(1,3)) <br/>X_titles_all = vect.fit_transform(all_data['title']) 
 
X_titles_train = X_titles_all[train_index] 
X_titles_test = X_titles_all[test_index] 
 
X_test = pd.merge(X_test, pd.DataFrame(X_titles_test.toarray(), index=X_test.index), left_index=True, right_index=True) 
 
X_train = pd.merge(X_train, pd.DataFrame(X_titles_train.toarray(), index=X_train.index), left_index=True, right_index=True)</strong> </pre>
<p>In the preceding lines, we have joined our existing features to our new n-gram features. Let's train our model and see whether we have any improvement:</p>
<pre><strong>clf.fit(X_train, y_train) 
 
y_pred = clf.predict(X_test) 
 
deltas = pd.DataFrame(list(zip(y_pred, y_actual, (y_pred - y_actual)/(y_actual))), columns=['predicted', 'actual', 'delta']) 
 
deltas</strong> </pre>
<p>This code <span>generates </span>the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-552 image-border" src="assets/6334cdc0-b783-401e-b824-f4f149adc245.png" style="width:14.50em;height:20.67em;"/></p>
<p>And if we check our error <span>again</span>, we will see the following:</p>
<pre><strong>np.sqrt(np.mean((y_pred-y_actual)**2))/np.mean(y_actual)</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-553 image-border" src="assets/4f1d6244-a80d-4cfa-8456-5544081a5801.png" style="width:11.92em;height:1.75em;"/></p>
<p>So it appears that we have a modestly improved model. Let's add one more feature to our model—the word count of the title:</p>
<pre><strong>all_data = all_data.assign(title_wc = all_data['title'].map(lambda x: len(x.split(' ')))) 
 
X_train = pd.merge(X_train, all_data[['title_wc']], left_index=True, right_index=True) 
 
X_test = pd.merge(X_test, all_data[['title_wc']], left_index=True, right_index=True) 
 
clf.fit(X_train, y_train) 
 
y_pred = clf.predict(X_test) 
 
np.sqrt(np.mean((y_pred-y_actual)**2))/np.mean(y_actual)</strong> </pre>
<p>This code <span>generates the following output</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-554 image-border" src="assets/5f26bec4-8c20-45ec-b2ec-44f44c37328f.png" style="width:12.00em;height:2.08em;"/></p>
<p>It appears that each feature has modestly improved our model. There are certainly more features we could add to it. For example, we could add the day of the week and the hour of the posting, we could determine whether the article is a listicle by running a regex on the headline, or we could examine the sentiment of each article. But this only just touches on the features that could be important for modeling virality. We would certainly need to go much further to continue reducing the number of errors in our model.</p>
<p>I should also note that we have done only the most cursory testing of our model. Each measurement should be run multiple times to get a more accurate representation of the actual error rate. It is possible that there is no statistically discernible difference between our last two models since we only performed one test.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we examined what the common features of viral content are and how we can build a model to predict virality using a random forest regression. We also learned how to combine multiple types of features and how to split our model into training and test sets.</p>
<p>Hopefully, you will take what you've learned here to build the next viral empire. If that doesn't work out, perhaps the next chapter on mastering the stock market will.</p>


            </article>

            
        </section>
    </body></html>
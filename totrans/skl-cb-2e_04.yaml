- en: Linear Models with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a line through data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting a line through data with machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the linear regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ridge regression to overcome linear regression's shortfalls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the ridge regression parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using sparsity to regularize models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking a more fundamental approach to regularization with LARS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I conjecture that we are built to perceive linear functions very well. They
    are very easy to visualize, interpret, and explain. Linear regression is very
    old and was probably the first statistical model.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take a machine learning approach to linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this chapter, similar to the chapter on dimensionality reduction and
    PCA, involves selecting the best features using linear models. Even if you decide
    not to perform regression for predictions with linear models, you can select the
    most powerful features.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that linear models provide a lot of the intuition behind the use of
    many machine learning algorithms. For example, RBF-kernel SVMs have smooth boundaries,
    which when looked at up close, look like a line. Thus, SVMs are often easy to
    explain if, in the background, you remember your linear model intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a line through data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will start with some basic modeling with linear regression. Traditional
    linear regression is the first, and therefore, probably the most fundamental model—a
    straight line through data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, it is familiar to a lot of the population: a change in one input
    variable proportionally changes the output variable. It is important that many
    people will have seen it in school, or in a newspaper data graphic, or in a presentation
    at work, and so on, as it will be easy for you to explain to colleagues and investors.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Boston dataset is perfect to play around with regression. The Boston dataset
    has the median home price of several areas in Boston. It also has other factors
    that might impact housing prices, for example, crime rate. First, import the `datasets`
    model, then we can load the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Actually, using linear regression in scikit-learn is quite simple. The API for
    linear regression is basically the same API you're now familiar with from the
    previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the `LinearRegression` object and create an object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s as easy as passing the independent and dependent variables to the
    `fit` method of `LinearRegression`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to get the predictions, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You have obtained the predictions produced by linear regression. Now, explore
    the `LinearRegression` class a bit more. Look at the residuals, the difference
    between the real target set and the predicted target set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0af36255-f283-4278-97a1-dc8e72212c2d.png)'
  prefs: []
  type: TYPE_IMG
- en: A common pattern to express the coefficients of features and their names is
    `zip(boston.feature_names, lr.coef_)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the coefficients of the linear regression by typing `lr.coef_`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So, going back to the data, we can see which factors have a negative relationship
    with the outcome, and also the factors that have a positive relationship. For
    example, and as expected, an increase in the per capita crime rate by town has
    a negative relationship with the price of a home in Boston. The per capita crime
    rate is the first coefficient in the regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also look at the intercept, the predicted value of the target, when
    all input variables are zero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you forget the names of the coefficients or intercept attributes, type `dir(lr)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For many scikit-learn predictors, parameters that consist of a word followed
    by a single `_`, such as `coef_` or `intercept_`, are of special interest. Using
    the `dir` command is a good way to check what is available within the scikit predictor
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic idea of linear regression is to find the set of coefficients of *v*
    that satisfy *y = Xv*, where *X* is the data matrix. It's unlikely that, for the
    given values of *X*, we will find a set of coefficients that exactly satisfy the
    equation; an error term gets added if there is an inexact specification or measurement
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the equation becomes *y = Xv + e*, where *e* is assumed to be normally
    distributed and independent of the *X* values. Geometrically, this means that
    the error terms are perpendicular to *X*. This is beyond the scope of this book,
    but it might be worth proving E(*Xv*) = 0 for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `LinearRegression` object can automatically normalize (or scale) inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Fitting a line through data with machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression with machine learning involves testing the linear regression
    algorithm on unseen data. Here, we will perform 10-fold cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the set into 10 parts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train on 9 of the parts and test on the one left over
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat this 10 times so that every part gets to be a test set once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As in the previous section, load the dataset you want to apply linear regression
    to, in this case, the Boston housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps involved in performing linear regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `LinearRegression` object and create an object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass the independent and dependent variables to the `fit` method of `LinearRegression`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to get the 10-fold cross-validated predictions, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the residuals, the difference between the real data and the predictions,
    they are more normally distributed compared to the residuals in the previous section
    of linear regression without cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the new residuals through cross-validation. The normal distribution
    is more symmetric than it was previously without cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ff855a90-134d-47fc-8495-d3099653b95f.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating the linear regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll look at how well our regression fits the underlying data.
    We fitted a regression in the last recipe, but didn't pay much attention to how
    well we actually did it. The first question after we fitted the model was clearly,
    how well does the model fit? In this recipe, we'll examine this question.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's use the `lr` object and Boston dataset—reach back into your code from
    the *Fitting a line through data* recipe. The `lr` object will have a lot of useful
    methods now that the model has been fit.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start within IPython with several imports, including `numpy`, `pandas`, and
    `matplotlib` for visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It is worth looking at a Q-Q plot. We''ll use `scipy` here because it has a
    built-in probability plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the probability plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a51e9fc-0f65-42bf-84f3-dc9f5278eedc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Type `tuple_out[1]` and you will get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is a tuple of the form *(slope, intercept, r)*, where *slope* and *intercept*
    come from the least-squares fit and *r* is the square root of the coefficient
    of determination.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the skewed values we saw earlier are a bit clearer. We can also look at
    some other metrics of the fit; **mean squared error** (**MSE**) and **mean absolute
    deviation** (**MAD**) are two common metrics. Let's define each one in Python
    and use them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have seen the formulas in NumPy to get the errors, you can also
    use the `sklearn.metrics` module to get the errors quickly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The formula for MSE is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaacc3fe-3001-4375-9d72-48862ed03c4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It takes each predicted value''s deviance from the actual value, squares it,
    and then averages all the squared terms. This is actually what we optimized to
    find the best set of coefficients for linear regression. The Gauss-Markov theorem
    actually guarantees that the solution to linear regression is the best in the
    sense that the coefficients have the smallest expected squared error and are unbiased.
    In the next recipe, we''ll look at what happens when we''re OK with our coefficients
    being biased. MAD is the expected error for the absolute errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1559fcfa-4342-4a4a-bf5c-44e51a33b72d.png)'
  prefs: []
  type: TYPE_IMG
- en: MAD isn't used when fitting linear regression, but it's worth taking a look
    at. Why? Think about what each one is doing and which errors are more important
    in each case. For example, with MSE, the larger errors get penalized more than
    the other terms because of the square term. Outliers have the potential to skew
    results substantially.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One thing that''s been glossed over a bit is the fact that coefficients themselves
    are random variables, therefore, they have a distribution. Let''s use bootstrapping
    to look at the distribution of the coefficient for the crime rate. Bootstrapping
    is a very common technique to get an understanding of the uncertainty of an estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can look at the distribution of the coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the histogram that gets generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d793d37-edad-4092-b816-524ee5c5470d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We might also want to look at the bootstrapped confidence interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This is interesting; there's actually reason to believe that the crime rate
    might not have an impact on home prices. Notice how zero is within the **confidence
    interval** (**CI**) between -0.18 and 0.03, which means that it may not play a
    role. It's also worth pointing out that bootstrapping can potentially lead to
    better estimations for coefficients, because the bootstrapped mean with convergence
    to the true mean is faster than finding the coefficient using regular estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Using ridge regression to overcome linear regression's shortfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll learn about ridge regression. It is different from vanilla
    linear regression; it introduces a regularization parameter to shrink coefficients.
    This is useful when the dataset has collinear factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ridge regression is actually so powerful in the presence of collinearity that
    you can model polynomial features: vectors *x*, *x*²,* x*³, ... which are highly
    collinear and correlated.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's load a dataset that has a low effective rank and compare ridge regression
    with linear regression by way of the coefficients. If you're not familiar with
    rank, it's the smaller of the linearly independent columns and the linearly independent
    rows. One of the assumptions of linear regression is that the data matrix is full
    rank.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, use `make_regression` to create a simple dataset with three predictors,
    but an `effective_rank` of `2`. Effective rank means that, although technically
    the matrix is full rank, many of the columns have a high degree of collinearity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s take a look at regular linear regression with bootstrapping from
    the previous chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the coefficients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/33cc9289-5560-49b7-b2d5-be71519cf3aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Perform the same procedure with ridge regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a70ee4ac-6a94-41f1-8f06-83a51681dbba.png)'
  prefs: []
  type: TYPE_IMG
- en: Don't let the similar width of the plots fool you; the coefficients for ridge
    regression are much closer to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the average spread between the coefficients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: So, on average, the coefficients for linear regression are much higher than
    the ridge regression coefficients. This difference is the bias in the coefficients
    (forgetting, for a second, the potential bias of the linear regression coefficients).
    So then, what is the advantage of ridge regression?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, let''s look at the variance of our coefficients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The variance has been dramatically reduced. This is the bias-variance trade-off
    that is so often discussed in machine learning. The next recipe will introduce
    how to tune the regularization parameter in ridge regression, which is at the
    heart of this trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the ridge regression parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you start using ridge regression to make predictions or learn about relationships
    in the system you're modeling, you'll start thinking about the choice of alpha.
  prefs: []
  type: TYPE_NORMAL
- en: For example, using ordinary least squares (**OLS**) regression might show a
    relationship between two variables; however, when regularized by an alpha, the
    relationship is no longer significant. This can be a matter of whether a decision
    needs to be taken.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Through cross-validation, we will tune the alpha parameter of ridge regression.
    If you remember, in ridge regression, the gamma parameter is typically represented
    as alpha in scikit-learn when calling `RidgeRegression`; so, the question that
    arises is what is the best alpha? Create a regression dataset, and then let''s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the `linear_models` module, there is an object called `RidgeCV`, which stands
    for ridge cross-validation. This performs a cross-validation similar to **leave-one-out
    cross-validation** (**LOOCV**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, it''s going to train the model for all samples except one.
    It''ll then evaluate the error in predicting this one test case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After we fit the regression, the alpha attribute will be the best alpha choice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous example, it was the first choice. We might want to hone in
    on something around `.1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We could continue this hunt, but hopefully, the mechanics are clear.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mechanics might be clear, but we should talk a little more about why and
    define what was meant by best. At each step in the cross-validation process, the
    model scores an error against the test sample. By default, it's essentially a
    squared error.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can force the `RidgeCV` object to store the cross-validation values; this
    will let us visualize what it''s doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we test a bunch of points (50 in total) between `0.01` and
    `1`. Since we passed `store_cv_values` as `True`, we can access these values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we had 100 values in the initial regression and tested 50 different alpha
    values. We now have access to the errors of all 50 values. So, we can now find
    the smallest mean error and choose it as alpha:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This matches the best value found by the `rcv3` instance of the class `RidgeCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s also worthwhile visualizing what''s going on. In order to do that, we''ll
    plot the mean for all 50 test alphas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0cb03c53-80ff-4cb2-96e8-6babd594555e.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to use our own scoring function, we can do that as well. Since we
    looked up MAD before, let''s use it to score the differences. First, we need to
    define our loss function. We will import it from `sklearn.metrics`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After we define the loss function, we can employ the `make_scorer` function
    in `sklearn`. This will take care of standardizing our function so that scikit''s
    objects know how to use it. Also, because this is a loss function and not a score
    function, the lower the better, and thus the need to let `sklearn` flip the sign
    to turn this from a maximization problem into a minimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Continue as before to find the minimum negative MAD score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the lowest score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'It occurs at the alpha of `0.01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Bayesian ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Additionally, scikit-learn contains Bayesian ridge regression, which allows
    for easy estimates of confidence intervals. (Note that obtaining the following Bayesian
    ridge confidence intervals specifically requires scikit-learn Version 0.19.0 or
    higher.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a line with a slope of `3` and no intercept for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Import, instantiate, and fit the Bayesian ridge model. Note that the one-dimensional
    `X` and `y` variables have to be reshaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the following to get the error estimates on the regularized linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the results. The noisy data is the blue dots and the green lines approximate
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/260b30a5-7c91-4a82-b2ef-d503cc670c5c.png)'
  prefs: []
  type: TYPE_IMG
- en: As a final aside on Bayesian ridge, you can perform hyperparameter optimization
    on the parameters `alpha_1`, `alpha_2`, `lambda_1`, and `lambda_2` using a cross-validated
    grid search.
  prefs: []
  type: TYPE_NORMAL
- en: Using sparsity to regularize models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **least absolute shrinkage and selection operator** (**LASSO**) method is
    very similar to ridge regression and **l****east angle regression** (**LARS**).
    It's similar to ridge regression in the sense that we penalize our regression
    by an amount, and it's similar to LARS in that it can be used as a parameter selection,
    typically leading to a sparse vector of coefficients. Both LASSO and LARS get
    rid of a lot of the features of the dataset, which is something you might or might
    not want to do depending on the dataset and how you apply it. (Ridge regression,
    on the other hand, preserves all features, which allows you to model polynomial
    functions or complex functions with correlated features.)
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be clear, LASSO regression is not a panacea. There can be computation consequences
    to using LASSO regression. As we'll see in this recipe, we'll use a loss function
    that isn't differential, and therefore requires special, and more importantly,
    performance-impairing workarounds.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps involved in performing LASSO regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back to the trusty `make_regression` function and create a dataset
    with the same parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to import the `Lasso` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'LASSO contains many parameters, but the most interesting parameter is `alpha`.
    It scales the penalization term of the `Lasso` method. For now, leave it as one.
    As an aside, and much like ridge regression, if this term is zero, LASSO is equivalent
    to linear regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, let''s see how many of the coefficients remain nonzero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: None of our coefficients turn out to be zero, which is what we expected. Actually,
    if you run this, you might get a warning from scikit-learn that advises you to
    choose `LinearRegression`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LASSO cross-validation – LASSOCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Choosing the most appropriate lambda is a critical problem. We can specify
    the lambda ourselves or use cross-validation to find the best choice given the
    data at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The LASSOCV will have, as an attribute, the most appropriate lambda. scikit-learn
    mostly uses alpha in its notation, but the literature uses lambda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of coefficients can be accessed in the regular manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Letting LASSOCV choose the appropriate best fit leaves us with `15` nonzero
    coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: LASSO for feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LASSO can often be used for feature selection for other methods. For example,
    you might run LASSO regression to get the appropriate number of features, and
    then use those features in another algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the features we want, create a masking array based on the columns that
    aren''t zero, and then filter out the nonzero columns to keep the features we
    want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Taking a more fundamental approach to regularization with LARS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To borrow from Gilbert Strang's evaluation of the Gaussian elimination, LARS
    is an idea you probably would've considered eventually had it not already been
    discovered by Efron, Hastie, Johnstone, and Tibshirani in their work [1].
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LARS is a regression technique that is well suited to high-dimensional problems,
    that is, *p >> n*, where *p* denotes the columns or features and *n* is the number
    of samples.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, import the necessary objects. The data we use will have 200 data points
    and 500 features. We''ll also choose low noise and a small number of informative
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we used 10 informative features, let''s also specify that we want 10
    nonzero coefficients in LARS. We will probably not know the exact number of informative
    features beforehand, but it''s useful for learning purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then verify that LARS returns the correct number of nonzero coefficients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The question then is why it is more useful to use a smaller number of features.
    To illustrate this, let''s hold out half of the data and train two LARS models,
    one with 12 nonzero coefficients and another with no predetermined amount. We
    use 12 here because we might have an idea of the number of important features,
    but we might not be sure of the exact number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to see how well each feature fits the unknown data, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Look again if you missed it; the error on the test set was clearly very high.
    Herein lies the problem with high-dimensional datasets; given a large number of
    features, it's typically not too difficult to get a model of good fit on the train
    sample, but overfitting becomes a huge problem.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LARS works by iteratively choosing features that are correlated with the residuals.
    Geometrically, correlation is effectively the least angle between the feature
    and the residuals; this is how LARS got its name.
  prefs: []
  type: TYPE_NORMAL
- en: 'After choosing the first feature, LARS will continue to move in the least angle
    direction until a different feature has the same amount of correlation with the
    residuals. Then, LARS will begin to move in the combined direction of both features.
    To visualize this, consider the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ff7d0c4-56ec-4028-b183-b1db7a6f4f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we move along **x1** until we get to the point where the pull on **x1**
    by **y** is the same as the pull on **x2** by **y**. When this occurs, we move
    along the path that is equal to the angle between **x1** and **x2** divided by
    two.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Much in the same way as we used cross-validation to tune ridge regression,
    we can do the same with LARS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Using cross-validation will help us to determine the best number of nonzero
    coefficients to use. Here, it turns out to be as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani, *Least
    angle regression*, The Annals of Statistics 32(2) 2004: pp. 407–499, doi:10.1214/009053604000000067,
    MR2060166.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL

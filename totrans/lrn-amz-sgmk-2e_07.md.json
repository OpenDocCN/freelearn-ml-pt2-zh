["```py\n1023  5  prefix/image2753.jpg\n38    6  another_prefix/image72.jpg\n983   2  yet_another_prefix/image863.jpg\n```", "```py\n    $ kaggle competitions download -c dogs-vs-cats\n    $ sudo yum -y install zip unzip\n    $ unzip dogs-vs-cats.zip\n    $ unzip train.zip\n    ```", "```py\n    $ cd train\n    $ mkdir dog cat\n    $ find . -name 'dog.*' -exec mv {} dog \\;\n    $ find . -name 'cat.*' -exec mv {} cat \\;\n    ```", "```py\n    $ mkdir -p val/dog val/cat\n    $ ls dog | sort -R | tail -1250 | while read file;\n    do mv dog/$file val/dog; done\n    $  ls cat | sort -R | tail -1250 | while read file;\n    do mv cat/$file val/cat; done\n    ```", "```py\n    $ mkdir train\n    $ mv dog cat train\n    ```", "```py\n    $ du -h\n    33M     ./val/dog\n    28M     ./val/cat\n    60M     ./val\n    289M    ./train/dog\n    248M    ./train/cat\n    537M    ./train\n    597M    .\n    ```", "```py\n    $ wget https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py\n    $ sudo yum -y install python-devel python-pip opencv opencv-devel opencv-python\n    $ pip3 install mxnet opencv-python\n    ```", "```py\n    dogscats-train.lst and dogscats-val.lst files. Their three columns are a unique image identifier, the class label (0 for cats, 1 for dogs), and the image path, as follows:\n\n    ```", "```py\n\n    ```", "```py\n    $ mkdir train_lst val_lst\n    $ mv dogscats-train.lst train_lst\n    $ mv dogscats-val.lst val_lst\n    ```", "```py\n    $ du -h\n    33M     ./val/dog\n    28M     ./val/cat\n    60M     ./val\n    700K    ./train_lst\n    80K     ./val_lst\n    289M    ./train/dog\n    248M    ./train/cat\n    537M    ./train\n    597M    .\n    ```", "```py\n    $ aws s3 sync . \n      s3://sagemaker-eu-west-1-123456789012/dogscats-images/input/\n    ```", "```py\n{\n   \"file\": \" my-prefix/my-picture.jpg\",\n   \"image_size\": [{\"width\": 512,\"height\": 512,\"depth\": 3}],\n   \"annotations\": [\n      {\n       \"class_id\": 1, \n       \"left\": 67, \"top\": 121, \"width\": 61, \"height\": 128\n      },\n      {\n       \"class_id\": 5, \n       \"left\": 324, \"top\": 134, \"width\": 112, \"height\": 267\n      }\n   ],\n   \"categories\": [\n      { \"class_id\": 1, \"name\": \"pizza\" },\n      { \"class_id\": 5, \"name\": \"beer\" }\n   ]\n}\n```", "```py\n├── train\n│   ├── image001.png\n│   ├── image007.png\n│   └── image042.png\n├── train_annotation\n│   ├── image001.png\n│   ├── image007.png\n│   └── image042.png \n├── validation\n│   ├── image059.png\n│   ├── image062.png\n│   └── image078.png\n└── validation_annotation\n│   ├── image059.png\n│   ├── image062.png\n│   └── image078.png\n```", "```py\n    $ wget https://data.deepai.org/PascalVOC2012.zip\n    $ unzip PascalVOC2012.zip \n    ```", "```py\n    $ mkdir input\n    $ cd input\n    $ mkdir train validation train_annotation validation_annotation\n    ```", "```py\n    $ for file in 'cat ../ImageSets/Segmentation/train.txt | xargs'; do cp ../JPEGImages/$file\".jpg\" train; done\n    ```", "```py\n    $ for file in 'cat ../ImageSets/Segmentation/val.txt | xargs'; do cp ../JPEGImages/$file\".jpg\" validation; done\n    $ for file in 'cat ../ImageSets/Segmentation/train.txt | xargs'; do cp ../SegmentationClass/$file\".png\" train_annotation; done\n    $ for file in 'cat ../ImageSets/Segmentation/val.txt | xargs'; do cp ../SegmentationClass/$file\".png\" validation_annotation; done\n    ```", "```py\n    $ for dir in train train_annotation validation validation_annotation; do find $dir -type f | wc -l; done\n    ```", "```py\n    1464\n    1464\n    1449\n    1449\n    ```", "```py\n    $ aws s3 sync . s3://sagemaker-eu-west-1-123456789012/pascalvoc-segmentation/input/\n    ```", "```py\n    $ cd train\n    $ mkdir dog cat\n    $ find . -name 'dog.*' -exec mv {} dog \\;\n    $ find . -name 'cat.*' -exec mv {} cat \\;\n    ```", "```py\n    $ python3 im2rec.py --list --recursive --train-ratio 0.9 dogscats .\n    ```", "```py\n    .rec) containing the packed images, and two index files (.idx) containing the offsets of these images inside the record files:\n\n    ```", "```py\n\n    ```", "```py\n    $ aws s3 cp dogscats_train.rec s3://sagemaker-eu-west-1-123456789012/dogscats/input/train/\n    $ aws s3 cp dogscats_val.rec s3://sagemaker-eu-west-1-123456789012/dogscats/input/validation/\n    ```", "```py\n<annotation>\n        <folder>VOC2007</folder>\n        <filename>003988.jpg</filename>\n        . . .\n        <object>\n                <name>chair</name>\n                <pose>Unspecified</pose>\n                <truncated>1</truncated>\n                <difficult>0</difficult>\n                <bndbox>\n                    <xmin>1</xmin>\n                    <ymin>222</ymin>\n                    <xmax>117</xmax>\n                    <ymax>336</ymax>\n                </bndbox>\n        </object>\n        <object>\n                <name>chair</name>\n                <pose>Unspecified</pose>\n                <truncated>1</truncated>\n                <difficult>1</difficult>\n                <bndbox>\n                    <xmin>429</xmin>\n                    <ymin>216</ymin>\n                    <xmax>448</xmax>\n                    <ymax>297</ymax>\n                </bndbox>\n        </object>\n        <object>\n                <name>chair</name>\n                <pose>Unspecified</pose>\n                <truncated>0</truncated>\n                <difficult>1</difficult>\n                <bndbox>\n                    <xmin>281</xmin>\n                    <ymin>149</ymin>\n                    <xmax>317</xmax>\n                    <ymax>211</ymax>\n                </bndbox>\n        </object>\n</annotation>\n```", "```py\n9404 2 6  8.0000  0.0022  0.6607  0.2612  1.0000  0.0000 8.0000  0.9576  0.6429  1.0000  0.8839  1.0000 8.0000  0.6272  0.4435  0.7076  0.6280  1.0000 VOC2007/JPEGImages/003988.jpg \n```", "```py\n    $ sudo yum -y install git python3-devel python3-pip opencv opencv-devel opencv-python\n    $ pip3 install mxnet opencv-python --user\n    $ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/ec2-user/.local/lib/python3.7/site-packages/mxnet/\n    $ sudo ldconfig\n    ```", "```py\n    $ mkdir pascalvoc\n    $ cd pascalvoc\n    $ wget https://data.deepai.org/PascalVOC2012.zip\n    $ wget https://data.deepai.org/PASCALVOC2007.zip\n    $ unzip PascalVOC2012.zip\n    $ unzip PASCALVOC2007.zip \n    $ mv VOC2012 VOCtrainval_06-Nov-2007/VOCdevkit\n    ```", "```py\n    $ git clone --single-branch --branch v1.4.x https://github.com/apache/incubator-mxnet\n    ```", "```py\n    $ cd VOCtrainval_06-Nov-2007\n    $ python3 ../incubator-mxnet/example/ssd/tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --root VOCdevkit --target VOCdevkit/train.lst\n    $ mv VOCdevkit/train.* ..\n    ```", "```py\n    $ cd ../VOCtest_06-Nov-2007\n    $ python3 ../incubator-mxnet/example/ssd/tools/prepare_dataset.py --dataset pascal --year 2007 --set test --root VOCdevkit --target VOCdevkit/val.lst\n    $ mv VOCdevkit/val.* ..\n    $ cd ..\n    ```", "```py\n    train.idx  train.lst  train.rec  \n    val.idx  val.lst  val.rec  \n    ```", "```py\n    $ aws s3 cp train.rec s3://sagemaker-eu-west-1-123456789012/pascalvoc/input/train/\n    $ aws s3 cp val.rec s3://sagemaker-eu-west-1-123456789012/pascalvoc/input/validation/\n    ```", "```py\n{\"source-ref\":\"s3://julien-sagemaker-book/chapter2/cat/cat1.jpg\",\n\"my-cat-job-ref\":\"s3://julien-sagemaker-book/chapter2/cat/output/my-cat-job/annotations/consolidated-annotation/output/0_2020-04-21T13:48:00.091190.png\",\n\"my-cat-job-ref-metadata\":{\n  \"internal-color-map\":{\n   \"0\":{\"class-name\":\"BACKGROUND\",\"hex-color\": \"#ffffff\", \n        \"confidence\": 0.8054600000000001}, \n   \"1\":{\"class-name\":\"cat\",\"hex-color\": \"#2ca02c\", \n        \"confidence\":0.8054600000000001}\n}, \n\"type\":\"groundtruth/semantic-segmentation\",\n\"human-annotated\":\"yes\",\n\"creation-date\":\"2020-04-21T13:48:00.562419\",\n\"job-name\":\"labeling-job/my-cat-job\"}}\n```", "```py\ntraining_data_channel = sagemaker.s3_input(\n    s3_data=augmented_manifest_file_path, \n    s3_data_type='AugmentedManifestFile',\n    attribute_names=['source-ref', 'my-job-cat-ref'])\n```", "```py\n    import sagemaker\n    sess = sagemaker.Session()\n    bucket = sess.default_bucket()\n    prefix = 'dogscats-images'\n    s3_train_path = \n      's3://{}/{}/input/train/'.format(bucket, prefix)\n    s3_val_path = \n      's3://{}/{}/input/val/'.format(bucket, prefix)\n    s3_train_lst_path = \n      's3://{}/{}/input/train_lst/'.format(bucket, prefix)\n    s3_val_lst_path = \n      's3://{}/{}/input/val_lst/'.format(bucket, prefix)\n    s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    region_name = sess.boto_session.boto_region_name\n    container = retrieve('image-classification', region)\n    ic = sagemaker.estimator.Estimator(container,\n                  sagemaker.get_execution_role(),\n                  instance_count=1,\n                  instance_type='ml.p3.2xlarge', \n                  output_path=s3_output)\n    ```", "```py\n    ic.set_hyperparameters(num_layers=18,\n                           use_pretrained_model=0,\n                           num_classes=2,\n                           num_training_samples=22500,\n                           resize=224,\n                           mini_batch_size=128,\n                           epochs=10)\n    ```", "```py\n    from sagemaker import TrainingInput\n    train_data = TrainingInput (\n        s3_train_path,                              \n        content_type='application/x-image')                                       \n    val_data = TrainingInput (\n        s3_val_path,                                                                  \n        content_type='application/x-image')\n    train_lst_data = TrainingInput (\n        s3_train_lst_path,                                  \n        content_type='application/x-image')\n    val_lst_data = TrainingInput (\n        s3_val_lst_path,                                    \n        content_type='application/x-image')                                      \n    s3_channels = {'train': train_data, \n                   'validation': val_data,\n                   'train_lst': train_lst_data, \n                   'validation_lst': val_lst_data}\n    ```", "```py\n    ic.fit(inputs=s3_channels)\n    ```", "```py\n    Searching for .lst files in /opt/ml/input/data/train_lst.\n    Creating record files for dogscats-train.lst\n    Done creating record files...\n    Searching for .lst files in /opt/ml/input/data/validation_lst.\n    Creating record files for dogscats-val.lst\n    Done creating record files...\n    ```", "```py\n    Epoch[0] Time cost=22.337\n    Epoch[0] Validation-accuracy=0.605859\n    ```", "```py\n    ic_predictor = ic.deploy(initial_instance_count=1,\n                             instance_type='ml.t2.medium')\n    ```", "```py\n    import boto3, json\n    import numpy as np\n    with open('test.jpg', 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    runtime = boto3.Session().client(\n        service_name='runtime.sagemaker')\n    response = runtime.invoke_endpoint(\n        EndpointName=ic_predictor.endpoint_name,                                  \n        ContentType='application/x-image',\n        Body=payload)\n    result = response['Body'].read()\n    result = json.loads(result)\n    index = np.argmax(result)\n    print(result[index], index)\n    ```", "```py\n    0.9999721050262451 1\n    ```", "```py\n    ic_predictor.delete_endpoint()\n    ```", "```py\nfrom sagemaker import TrainingInput\nprefix = 'dogscats'\ns3_train_path=\n  's3://{}/{}/input/train/'.format(bucket, prefix)\ns3_val_path=\n  's3://{}/{}/input/validation/'.format(bucket, prefix)\ntrain_data = TrainingInput(\n    s3_train_path,\n    content_type='application/x-recordio')\nvalidation_data = TrainingInput(\n    s3_val_path,                                        \n    content_type='application/x-recordio')\n```", "```py\n    %%sh\n    wget http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec\n    wget http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec\n    ```", "```py\n    import sagemaker\n    session = sagemaker.Session()\n    bucket = session.default_bucket()\n    prefix = 'caltech256/'\n    s3_train_path = session.upload_data(\n        path='caltech-256-60-train.rec',\n        bucket=bucket, key_prefix=prefix+'input/train')\n    s3_val_path = session.upload_data(\n        path='caltech-256-60-val.rec',\n        bucket=bucket, key_prefix=prefix+'input/validation')\n    ```", "```py\n    ic.set_hyperparameters(num_layers=50,\n                           use_pretrained_model=1,\n                           num_classes=257,\n                           num_training_samples=15240,\n                           learning_rate=0.001,\n                           epochs=5)\n    ```", "```py\n    Epoch[4] Validation-accuracy=0.8119\n    ```", "```py\n    import sagemaker\n    sess = sagemaker.Session()\n    bucket = sess.default_bucket()\n    prefix = 'pascalvoc'\n    s3_train_data = 's3://{}/{}/input/train'.format(bucket, prefix)\n    s3_validation_data = 's3://{}/{}/input/validation'.format(bucket, prefix)\n    s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    region = sess.boto_region_name\n    container = retrieve('object-detection', region)\n    ```", "```py\n    od = sagemaker.estimator.Estimator(\n             container,\n             sagemaker.get_execution_role(),\n             instance_count=1,\n             instance_type='ml.p3.2xlarge',\n             output_path=s3_output_location)\n    ```", "```py\n    od.set_hyperparameters(base_network='resnet-50',\n                           use_pretrained_model=1,\n                           num_classes=20,\n                           num_training_samples=16551,\n                           epochs=30)\n    ```", "```py\n    from sagemaker.session import TrainingInput\n    train_data = TrainingInput (\n          s3_train_data,\n          content_type='application/x-recordio')\n    validation_data = TrainingInput (\n          s3_validation_data, \n          content_type='application/x-recordio')\n    data_channels = {'train': train_data, \n                     'validation': validation_data}\n    od.fit(inputs=data_channels)\n    ```", "```py\n    od_predictor = od.deploy(\n        initial_instance_count = 1, \n        instance_type= 'ml.c5.2xlarge')\n    ```", "```py\n    import boto3,json\n    with open('test.jpg', 'rb') as image:\n        payload = image.read()\n        payload = bytearray(payload)\n    runtime = boto3.Session().client(\n        service_name='runtime.sagemaker')\n    response = runtime.invoke_endpoint(\n        EndpointName=od_predictor.endpoint_name,                                  \n        ContentType='image/jpeg',\n        Body=payload)\n    response = response['Body'].read()\n    response = json.loads(response)\n    ```", "```py\n    {'prediction': \n    [[14.0, 0.7515302300453186, 0.39770469069480896, 0.37605002522468567, 0.5998836755752563, 1.0], \n    [14.0, 0.6490200161933899, 0.8020403385162354, 0.2027685046195984, 0.9918708801269531, 0.8575668931007385]\n    ```", "```py\n    od_predictor.delete_endpoint()\n    ```", "```py\n    import sagemaker\n    sess = sagemaker.Session()\n    bucket = sess.default_bucket()  \n    prefix = 'pascalvoc-segmentation'\n    s3_train_data = 's3://{}/{}/input/train'.format(bucket, prefix)\n    s3_validation_data = 's3://{}/{}/input/validation'.format(bucket, prefix)\n    s3_train_annotation_data = 's3://{}/{}/input/train_annotation'.format(bucket, prefix)\n    s3_validation_annotation_data = 's3://{}/{}/input/validation_annotation'.format(bucket, prefix)\n    s3_output_location = \n    's3://{}/{}/output'.format(bucket, prefix)\n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    container = retrieve('semantic-segmentation', region)\n    seg = sagemaker.estimator.Estimator(\n              container,\n              sagemaker.get_execution_role(),                                     \n              instance_count = 1,\n              instance_type = 'ml.p3.2xlarge',\n              output_path = s3_output_location)\n    ```", "```py\n    seg.set_hyperparameters(backbone='resnet-50',\n                            algorithm='fcn',\n                            use_pretrained_model=True,\n                            num_classes=21,\n                            num_training_samples=1464,\n                            epochs=30)\n    ```", "```py\n    from sagemaker import TrainingInput\n    train_data = TrainingInput(\n        s3_train_data, \n        content_type='image/jpeg')\n    validation_data = TrainingInput(\n        s3_validation_data,\n        content_type='image/jpeg')\n    train_annotation = TrainingInput(\n        s3_train_annotation_data,\n        content_type='image/png')\n    validation_annotation = TrainingInput(\n        s3_validation_annotation_data,  \n        content_type='image/png')\n    data_channels = {\n      'train': train_data,\n      'validation': validation_data,\n      'train_annotation': train_annotation,           \n      'validation_annotation':validation_annotation\n    }\n    seg.fit(inputs=data_channels)\n    ```", "```py\n    seg_predictor = seg.deploy(\n        initial_instance_count=1, \n        instance_type='ml.c5.2xlarge')\n    ```", "```py\n    !wget -O test.jpg https://bit.ly/3yhXB9l \n    filename = 'test.jpg'\n    with open(filename, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    runtime = boto3.Session().client(\n        service_name='runtime.sagemaker')\n    response = runtime.invoke_endpoint(\n        EndpointName=od_predictor.endpoint_name,\n        ContentType='image/jpeg',\n        Body=payload)\n    response = response['Body'].read()\n    response = json.loads(response)\n    ```", "```py\n    import PIL\n    from PIL import Image\n    import numpy as np\n    import io\n    num_classes = 21\n    mask = np.array(Image.open(io.BytesIO(response)))\n    plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='gray_r')\n    plt.show()\n    ```", "```py\n    response = runtime.invoke_endpoint(\n        EndpointName=seg_predictor.endpoint_name,\n        ContentType='image/jpeg',\n        Accept='application/x-protobuf',\n        Body=payload)\n    result = response['Body'].read()\n    seg_predictor.accept = 'application/x-protobuf'\n    response = seg_predictor.predict(img)\n    results_file = 'results.rec'\n    with open(results_file, 'wb') as f:\n        f.write(response)\n    ```", "```py\n    mask = np.reshape(np.array(values), shape)\n    pixel_probs = mask[0,:,0,0]\n    print(pixel_probs)\n    print(np.argmax(pixel_probs))\n    ```", "```py\n    [9.68291104e-01 3.72813665e-04 8.14868137e-04 1.22414716e-03\n     4.57380433e-04 9.95167647e-04 4.29908326e-03 7.52388616e-04\n     1.46311778e-03 2.03254796e-03 9.60668200e-04 1.51833100e-03\n     9.39570891e-04 1.49350625e-03 1.51627266e-03 3.63648031e-03\n     2.17934581e-03 7.69103528e-04 3.05095245e-03 2.10589729e-03\n     1.12741732e-03]\n    0\n    ```", "```py\n    seg_predictor.delete_endpoint()\n    ```"]
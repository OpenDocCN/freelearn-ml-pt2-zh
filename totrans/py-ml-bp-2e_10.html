<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Build a Recommendation Engine</h1>
                </header>
            
            <article>
                
<p><span class="HeaderFooterPACKT">Like so many things, it was born of frustration and stiff cocktails. It was a Saturday, and the two young men were once again stuck without a date for the night. As they sat pouring drinks and sharing laments, the two Harvard freshmen began to flesh out an idea. What if, instead of relying on random chance to meet the right girl, they could use a computer algorithm?</span></p>
<p><span class="HeaderFooterPACKT">The key to matching people up, they felt, would be to create a set of questions that provided the sort of information everyone is really looking for on those first awkward dates. By matching people using these questionnaires, you could eliminate dates that could best be avoided. The process would be super efficient.</span></p>
<p><span class="HeaderFooterPACKT">The idea was to market their new service to college students in Boston and around the country. And in short order, that's exactly what they did.</span></p>
<p><span class="HeaderFooterPACKT">Soon after, the digital matchmaking service they built went on to become a huge success. It received national media attention and generated tens of thousands of matches over the course of the next few years. </span><span class="HeaderFooterPACKT">The company was so successful, in fact, it was eventually bought out by a larger company that wanted to use its technology.</span></p>
<p><span class="HeaderFooterPACKT">If you think I'm talking about <strong>OkCupid</strong>, you would be wrong—and off by about 40 years. The company I'm speaking of did all of this beginning in 1965<span>—</span>a time when computing matches was done using punch cards on an IBM 1401 mainframe. It also took three days just to run the computations.</span></p>
<p><span class="HeaderFooterPACKT">But oddly enough, there's a connection between OkCupid and its 1965 precursor, Compatibility Research, Inc. The co-founder of Compatibility Research is Jeff Tarr, whose daughter, Jennifer Tarr, is the wife of OkCupid's co-founder Chris Coyne. Small world indeed.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="HeaderFooterPACKT">But why is any of this relevant to a chapter on building a recommendation engine? Because it's quite likely that this was in fact the first one. And while most people tend to think of recommendation engines as tools for finding closely related products or music and movies they're likely to appreciate, the original incarnation was to find potential mates. And as a model for thinking about how these systems work, it provides a good frame of reference.</span></p>
<p><span class="HeaderFooterPACKT">In this chapter, we're going to explore the different varieties of recommendation systems. We'll see how they're implemented commercially and how they work. Finally, we'll implement our own recommendation engine for finding GitHub repositories.</span></p>
<p><span class="HeaderFooterPACKT">We'll cover the following topics in this chapter:</span></p>
<ul>
<li>Collaborative filtering</li>
<li>Content-based filtering</li>
<li>Hybrid systems</li>
<li>Building a recommendation engine</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collaborative filtering</h1>
                </header>
            
            <article>
                
<p>In early 2012, a story broke about a man who had come into a Target store in Minneapolis to complain about a book of coupons sent to his home. He was in fact quite incensed about these coupons, which had been addressed to his daughter, a high school student at the time. And although it might seem like an odd reaction to a potential money-saving opportunity, learning that the coupons were exclusively for products such as prenatal vitamins, diapers, baby formula, cribs, and so on might change your view.</p>
<p>The manager, upon hearing the complaint, apologized profusely. He felt bad enough, in fact, that he called several days later to follow up and explain how this could have happened. But before the manager was able to even begin his apology, the father began to apologize to the manager. As it turned out, his daughter was in fact pregnant and her shopping habits had given her away.</p>
<p>The algorithm that gave her away was likely based<span>—</span>at least in part<span>—</span>on one type of algorithm used in recommendation engines called <strong>collaborative filtering</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">So, what's collaborative filtering?</h1>
                </header>
            
            <article>
                
<p>Collaborative filtering is based on the idea that, somewhere out there in the world, you have a taste doppelganger<span>—</span>someone who shares the same notions about how good <em>Star Wars</em> is and how awful <em>Love Actually</em> is.</p>
<p class="mce-root"/>
<p>The idea is that you've rated some set of items in a way that's very similar to the way this other person, this doppelganger, has rated them, but then each of you has rated additional items that the other hasn't. Because you've established that your tastes are similar, recommendations can be generated from the items your doppelganger has rated highly but which you haven't rated and vice versa. It's in a way much like digital matchmaking, but with the outcome being songs or products you would like, rather than actual people.</p>
<p>So, in the case of our pregnant high schooler, when she bought the right combination of unscented lotions, cotton balls, and vitamin supplements, she likely found herself paired up with people who went on to buy cribs and diapers at some point later.</p>
<p>Let's go through an example to see how this works in practice.</p>
<p>We'll start with what's called a <strong>utility matrix</strong>. This is similar to a <strong>term-document matrix</strong> but, instead of terms and documents, we'll be representing products and users.</p>
<p>Here we'll assume that we have customers <em>A-D</em> and a set of products that they've rated on a scale from 0 to 5:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Customer</strong></td>
<td>
<p><strong>Snarky's Potato Chips</strong></p>
</td>
<td>
<p><strong>SoSo Smooth</strong><br/>
<strong>Lotion</strong></p>
</td>
<td>
<p><strong>Duffly</strong><br/>
<strong>Beer</strong></p>
</td>
<td>
<p><strong>BetterTap</strong><br/>
<strong>Water</strong></p>
</td>
<td>
<p><strong>XXLargeLivin'</strong><br/>
<strong>Football Jersey</strong></p>
</td>
<td>
<p><strong>Snowy</strong><br/>
<strong>Cotton</strong><br/>
<strong>Balls</strong></p>
</td>
<td>
<p><strong>Disposos'</strong><br/>
<strong>Diapers</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>A</strong></p>
</td>
<td>
<p>4</p>
</td>
<td/>
<td>
<p>5</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>5</p>
</td>
<td/>
<td/>
</tr>
<tr>
<td>
<p><strong>B</strong></p>
</td>
<td/>
<td>
<p>4</p>
</td>
<td/>
<td>
<p>4</p>
</td>
<td/>
<td>
<p>5</p>
</td>
<td/>
</tr>
<tr>
<td>
<p><strong>C</strong></p>
</td>
<td>
<p>2</p>
</td>
<td/>
<td>
<p>2</p>
</td>
<td/>
<td>
<p>1</p>
</td>
<td/>
<td/>
</tr>
<tr>
<td>
<p><strong>D</strong></p>
</td>
<td/>
<td>
<p>5</p>
</td>
<td/>
<td>
<p>3</p>
</td>
<td/>
<td>
<p>5</p>
</td>
<td>
<p>4</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>We've seen previously that, when we want to find similar items, we could use cosine similarity. Let's try that here. We'll find the user most like user <em>A</em>. Because we have a sparse vector containing many unrated items, we'll have to input something for those missing values. We'll just go with 0 here. We'll start by comparing user <em>A</em> to user <em>B</em>:</p>
<pre><strong>from sklearn.metrics.pairwise import cosine_similarity 
cosine_similarity(np.array([4,0,5,3,5,0,0]).reshape(1,-1),\ 
                  np.array([0,4,0,4,0,5,0]).reshape(1,-1))</strong> </pre>
<p>The previous code results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-673 image-border" src="assets/5672e7bc-e966-477e-9ab0-95cdadabfaec.png" style="width:12.75em;height:2.08em;"/></p>
<p>As you can see, the two don't have a high similarity rating, which makes sense as they have no ratings in common.</p>
<p class="mce-root"/>
<p>Let's now look at user C compared to user <em>A</em>:</p>
<pre><strong>cosine_similarity(np.array([4,0,5,3,5,0,0]).reshape(1,-1),\ 
                  np.array([2,0,2,0,1,0,0]).reshape(1,-1))</strong> </pre>
<p>The previous code results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-674 image-border" src="assets/608f4fa0-0045-464c-b0a4-493eaa5c34ad.png" style="width:14.67em;height:1.67em;"/></p>
<p>Here, we see that they have a high similarity rating (remember 1 is perfect similarity), despite the fact they rated the same products very differently. Why are we getting these results? The problem lies with our choice of using 0 for the unrated products. It's registering as strong (negative) agreement on those unrated products. 0 isn't neutral in this case.</p>
<p>So, how can we fix this?</p>
<p>What we can do instead of just using 0 for the missing values is to re-center each user's ratings so that the mean rating is 0, or neutral. We do this by taking each user rating and subtracting the mean for all ratings of that user. For example, for user <em>A</em>, the mean is 17/4, or 4.25. We then subtract that from every individual rating that user <em>A</em> provided.</p>
<p>Once that's been done, we then continue on to find the mean for every other user and subtract it from each of their ratings until every user has been processed.</p>
<p>This procedure will result in a table like the following. You will notice each user row sums to 0 (ignore the rounding issues here):</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 1%"><strong>Customers</strong></td>
<td style="width: 18%">
<p><strong>Snarky's Potato Chips</strong></p>
</td>
<td style="width: 15%">
<p><strong>SoSo Smooth</strong><br/>
<strong>Lotion</strong></p>
</td>
<td style="width: 10%">
<p><strong>Duffly</strong><br/>
<strong>Beer</strong></p>
</td>
<td style="width: 13%">
<p><strong>BetterTap</strong><br/>
<strong>Water</strong></p>
</td>
<td style="width: 17%">
<p><strong>XXLargeLivin'</strong><br/>
<strong>Football Jersey</strong></p>
</td>
<td style="width: 7%">
<p><strong>Snowy</strong><br/>
<strong>Cotton</strong><br/>
<strong>Balls</strong></p>
</td>
<td style="width: 11%">
<p><strong>Disposos'</strong><br/>
<strong>Diapers</strong></p>
</td>
</tr>
<tr>
<td style="width: 1%">
<p><strong>A</strong></p>
</td>
<td style="width: 18%">
<p>-.25</p>
</td>
<td style="width: 15%"/>
<td style="width: 10%">
<p>.75</p>
</td>
<td style="width: 13%">
<p>-1.25</p>
</td>
<td style="width: 17%">
<p>.75</p>
</td>
<td style="width: 7%"/>
<td style="width: 11%"/>
</tr>
<tr>
<td style="width: 1%">
<p><strong>B</strong></p>
</td>
<td style="width: 18%"/>
<td style="width: 15%">
<p>-.33</p>
</td>
<td style="width: 10%"/>
<td style="width: 13%">
<p>-.33</p>
</td>
<td style="width: 17%"/>
<td style="width: 7%">
<p>.66</p>
</td>
<td style="width: 11%"/>
</tr>
<tr>
<td style="width: 1%">
<p><strong>C</strong></p>
</td>
<td style="width: 18%">
<p>.33</p>
</td>
<td style="width: 15%"/>
<td style="width: 10%">
<p>.33</p>
</td>
<td style="width: 13%"/>
<td style="width: 17%">
<p>-.66</p>
</td>
<td style="width: 7%"/>
<td style="width: 11%"/>
</tr>
<tr>
<td style="width: 1%">
<p><strong>D</strong></p>
</td>
<td style="width: 18%"/>
<td style="width: 15%">
<p>.75</p>
</td>
<td style="width: 10%"/>
<td style="width: 13%">
<p>-1.25</p>
</td>
<td style="width: 17%"/>
<td style="width: 7%">
<p>.75</p>
</td>
<td style="width: 11%">
<p>-.25</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Let's now try our cosine similarity on our newly centered data. We'll do user <em>A</em> compared to user <em>B</em> and <em>C</em> again.</p>
<p class="mce-root"/>
<p>First, let's compare user <em>A</em> to user <em>B</em>:</p>
<pre><strong>cosine_similarity(np.array([-.25,0,.75,-1.25,.75,0,0])\ 
                  .reshape(1,-1),\ 
                  np.array([0,-.33,0,-.33,0,.66,0])\ 
                  .reshape(1,-1)) </strong></pre>
<p>The preceding code results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-675 image-border" src="assets/0f996df4-2bae-40a3-8a4d-e42e49ed0faa.png" style="width:15.67em;height:2.00em;"/></p>
<p>Now let's try between users <em>A</em> and <em>C</em>:</p>
<pre><strong>cosine_similarity(np.array([-.25,0,.75,-1.25,.75,0,0])\ 
                  .reshape(1,-1),\ 
                  np.array([.33,0,.33,0,-.66,0,0])\ 
                  .reshape(1,-1))</strong> </pre>
<p>The preceding code results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-676 image-border" src="assets/be39db7c-65c3-435f-8a0e-e1def4064051.png" style="width:15.08em;height:2.00em;"/></p>
<p>What we can see is that the similarity between <em>A</em> and <em>B</em> increased slightly, while the similarity between <em>A</em> and <em>C</em> decreased dramatically. This is exactly as we would hope.</p>
<p>This centering process, besides helping us deal with missing values, also has the side benefit of helping us to deal with difficult or easy raters since now everyone is centered around a mean of 0. This formula, you may notice, is equivalent to the Pearson correlation coefficient and, just like with that coefficient, the values fall between <kbd>-1</kbd> and <kbd>1</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting the rating for the product</h1>
                </header>
            
            <article>
                
<p>Let's now take this framework and use it to predict the rating for a product. We'll limit our example to three users, person <em>X</em>, person <em>Y</em>, and person <em>Z</em>. We'll predict the rating of a product that person <em>X</em> hasn't rated, but that persons <em>Y</em> and <em>Z</em>, who are very similar to <em>X</em>, have rated.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We'll start with our base ratings for each user, as shown in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 11.708%">
<p><strong>Customers</strong></p>
</td>
<td style="width: 18.3196%">
<p><strong>Snarky's Potato Chips</strong></p>
</td>
<td style="width: 15.1515%">
<p><strong>SoSo Smooth</strong><br/>
<strong>Lotion</strong></p>
</td>
<td style="width: 6.88705%">
<p><strong>Duffly</strong><br/>
<strong>Beer</strong></p>
</td>
<td style="width: 10.8815%">
<p><strong>BetterTap</strong><br/>
<strong>Water</strong></p>
</td>
<td style="width: 15.7025%">
<p><strong>XXLargeLivin'</strong><br/>
<strong>Football Jersey</strong></p>
</td>
<td style="width: 7.43802%">
<p><strong>Snowy</strong><br/>
<strong>Cotton</strong><br/>
<strong>Balls</strong></p>
</td>
<td style="width: 10.6061%">
<p><strong>Disposos'</strong><br/>
<strong>Diapers</strong></p>
</td>
</tr>
<tr>
<td style="width: 11.708%">
<p><strong>X</strong></p>
</td>
<td style="width: 18.3196%"/>
<td style="width: 15.1515%">
<p>4</p>
</td>
<td style="width: 6.88705%"/>
<td style="width: 10.8815%">
<p>3</p>
</td>
<td style="width: 15.7025%"/>
<td style="width: 7.43802%">
<p>4</p>
</td>
<td style="width: 10.6061%"/>
</tr>
<tr>
<td style="width: 11.708%">
<p><strong>Y</strong></p>
</td>
<td style="width: 18.3196%"/>
<td style="width: 15.1515%">
<p>3.5</p>
</td>
<td style="width: 6.88705%"/>
<td style="width: 10.8815%">
<p>2.5</p>
</td>
<td style="width: 15.7025%"/>
<td style="width: 7.43802%">
<p>4</p>
</td>
<td style="width: 10.6061%">
<p>4</p>
</td>
</tr>
<tr>
<td style="width: 11.708%">
<p><strong>Z</strong></p>
</td>
<td style="width: 18.3196%"/>
<td style="width: 15.1515%">
<p>4</p>
</td>
<td style="width: 6.88705%"/>
<td style="width: 10.8815%">
<p>3.5</p>
</td>
<td style="width: 15.7025%"/>
<td style="width: 7.43802%">
<p>4.5</p>
</td>
<td style="width: 10.6061%">
<p>4.5</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Next, we'll center the ratings:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 1%">
<p><strong>Customers</strong></p>
</td>
<td style="width: 20%">
<p><strong>Snarky's Potato Chips</strong></p>
</td>
<td style="width: 15.1322%">
<p><strong>SoSo Smooth</strong><br/>
<strong>Lotion</strong></p>
</td>
<td style="width: 10%">
<p><strong>Duffly</strong><br/>
<strong>Beer</strong></p>
</td>
<td style="width: 11%">
<p><strong>BetterTap</strong><br/>
<strong>Water</strong></p>
</td>
<td style="width: 17%">
<p><strong>XXLargeLivin'</strong><br/>
<strong>Football Jersey</strong></p>
</td>
<td style="width: 7%">
<p><strong>Snowy</strong><br/>
<strong>Cotton</strong><br/>
<strong>Balls</strong></p>
</td>
<td style="width: 11%">
<p><strong>Disposos'</strong><br/>
<strong>Diapers</strong></p>
</td>
</tr>
<tr>
<td style="width: 1%">
<p><strong>X</strong></p>
</td>
<td style="width: 20%"/>
<td style="width: 15.1322%">
<p>.33</p>
</td>
<td style="width: 10%"/>
<td style="width: 11%">
<p>-.66</p>
</td>
<td style="width: 17%"/>
<td style="width: 7%">
<p>.33</p>
</td>
<td style="width: 11%">
<p>?</p>
</td>
</tr>
<tr>
<td style="width: 1%">
<p><strong>Y</strong></p>
</td>
<td style="width: 20%"/>
<td style="width: 15.1322%">
<p>0</p>
</td>
<td style="width: 10%"/>
<td style="width: 11%">
<p>-1</p>
</td>
<td style="width: 17%"/>
<td style="width: 7%">
<p>.5</p>
</td>
<td style="width: 11%">
<p>.5</p>
</td>
</tr>
<tr>
<td style="width: 1%">
<p><strong>Z</strong></p>
</td>
<td style="width: 20%"/>
<td style="width: 15.1322%">
<p>-.125</p>
</td>
<td style="width: 10%"/>
<td style="width: 11%">
<p>-.625</p>
</td>
<td style="width: 17%"/>
<td style="width: 7%">
<p>.375</p>
</td>
<td style="width: 11%">
<p>.375</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Now, we'd like to know what rating user <em>X</em> might be likely to give <strong>Disposos' Diapers</strong>. Using the ratings from user <em>Y</em> and user <em>Z</em>, we can calculate this by taking the weighted average according to their centered cosine similarity.</p>
<p>Let's first get that figure:</p>
<pre><strong>user_x = [0,.33,0,-.66,0,33,0] 
user_y = [0,0,0,-1,0,.5,.5] 
 
cosine_similarity(np.array(user_x).reshape(1,-1),\ 
                  np.array(user_y).reshape(1,-1))</strong> </pre>
<p>The preceding code results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-677 image-border" src="assets/a5611a3e-2da1-4821-a782-a64680fd5565.png" style="width:15.83em;height:2.00em;"/></p>
<p>Now, let's get that figure for user <em>Z</em>:</p>
<pre><strong>user_x = [0,.33,0,-.66,0,33,0] 
user_z = [0,-.125,0,-.625,0,.375,.375] 
 
cosine_similarity(np.array(user_x).reshape(1,-1),\ 
                  np.array(user_z).reshape(1,-1))</strong> </pre>
<p class="mce-root"/>
<p>The preceding code results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-678 image-border" src="assets/2bf43f2d-e87c-46aa-a3c4-048480370cb7.png" style="width:14.67em;height:2.08em;"/></p>
<p>So, now we have a figure for the similarity between user <em>X</em> and user <em>Y</em> (<kbd>0.42447212</kbd>) and user <em>Z</em> (<kbd>0.46571861</kbd>).</p>
<p>Putting it all together, we weight each users rating by their similarity to <em>X</em>, and then divide by the total similarity, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>(.42447212 * (4) + .46571861 * (4.5) ) / (.42447212 + .46571861) = 4.26</em></p>
<p>And we can see that the expected rating of user <em>X</em> for <strong>Disposos' Diapers</strong> is 4.26. (Better send a coupon!)</p>
<p>Now, so far, we've looked only at user-to-user collaborative filtering, but there's another method we can use. In practice, this method outperforms user-to-user filtering; it's called <strong>item-to-item filtering</strong>. Here's how the method works: rather than match each user up with other similar users based on their past ratings, each rated item is compared against all other items to find the most similar ones, again using centered cosine similarity.</p>
<p>Let's take a look at how this would work.</p>
<p>Again we have a utility matrix; this time, we'll look at users' ratings of songs. The users are along the columns and the songs are along the rows, shown as follows:</p>
<table border="1" style="border-collapse: collapse;width: 99.7249%">
<tbody>
<tr>
<td style="width: 15.427%">
<p><strong>Entity</strong></p>
</td>
<td style="width: 14.7383%">
<p><strong>U1</strong></p>
</td>
<td style="width: 16.8044%">
<p><strong>U2</strong></p>
</td>
<td style="width: 16.1157%">
<p><strong>U3</strong></p>
</td>
<td style="width: 17.4931%">
<p><strong>U4</strong></p>
</td>
<td style="width: 16.8044%">
<p><strong>U5</strong></p>
</td>
</tr>
<tr>
<td style="width: 15.427%">
<p><strong>S1</strong></p>
</td>
<td style="width: 14.7383%">
<p>2</p>
</td>
<td style="width: 16.8044%"/>
<td style="width: 16.1157%">
<p>4</p>
</td>
<td style="width: 17.4931%"/>
<td style="width: 16.8044%">
<p>5</p>
</td>
</tr>
<tr>
<td style="width: 15.427%">
<p><strong>S2</strong></p>
</td>
<td style="width: 14.7383%"/>
<td style="width: 16.8044%">
<p>3</p>
</td>
<td style="width: 16.1157%"/>
<td style="width: 17.4931%">
<p>3</p>
</td>
<td style="width: 16.8044%"/>
</tr>
<tr>
<td style="width: 15.427%">
<p><strong>S3</strong></p>
</td>
<td style="width: 14.7383%">
<p>1</p>
</td>
<td style="width: 16.8044%"/>
<td style="width: 16.1157%">
<p>5</p>
</td>
<td style="width: 17.4931%"/>
<td style="width: 16.8044%">
<p>4</p>
</td>
</tr>
<tr>
<td style="width: 15.427%">
<p><strong>S4</strong></p>
</td>
<td style="width: 14.7383%"/>
<td style="width: 16.8044%">
<p>4</p>
</td>
<td style="width: 16.1157%">
<p>4</p>
</td>
<td style="width: 17.4931%">
<p>4</p>
</td>
<td style="width: 16.8044%"/>
</tr>
<tr>
<td style="width: 15.427%">
<p><strong>S5</strong></p>
</td>
<td style="width: 14.7383%">
<p>3</p>
</td>
<td style="width: 16.8044%"/>
<td style="width: 16.1157%"/>
<td style="width: 17.4931%"/>
<td style="width: 16.8044%">
<p>5</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Now, suppose we would like to know the rating that user 3 will assign to song 5. Instead of looking for similar users, we'll look for songs that are similar based upon how they were rated across the users.</p>
<p>Let's see an example.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>First, we start by centering each song row, and calculating the cosine similarity for each versus our target row, which is <em>S5</em>, shown as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 7%"><strong>Entity</strong></td>
<td style="width: 11%">
<p><strong>U1</strong></p>
</td>
<td style="width: 11%">
<p><strong>U2</strong></p>
</td>
<td style="width: 11%">
<p><strong>U3</strong></p>
</td>
<td style="width: 11.8143%">
<p><strong>U4</strong></p>
</td>
<td style="width: 10%">
<p><strong>U5</strong></p>
</td>
<td style="width: 16%">
<p><strong>CntrdCoSim</strong></p>
</td>
</tr>
<tr>
<td style="width: 7%">
<p><strong>S1</strong></p>
</td>
<td style="width: 11%">
<p>-1.66</p>
</td>
<td style="width: 11%"/>
<td style="width: 11%">
<p>.33</p>
</td>
<td style="width: 11.8143%"/>
<td style="width: 10%">
<p>1.33</p>
</td>
<td style="width: 16%">
<p>.98</p>
</td>
</tr>
<tr>
<td style="width: 7%">
<p><strong>S2</strong></p>
</td>
<td style="width: 11%"/>
<td style="width: 11%">
<p>0</p>
</td>
<td style="width: 11%"/>
<td style="width: 11.8143%">
<p>0</p>
</td>
<td style="width: 10%"/>
<td style="width: 16%">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 7%">
<p><strong>S3</strong></p>
</td>
<td style="width: 11%">
<p>-2.33</p>
</td>
<td style="width: 11%"/>
<td style="width: 11%">
<p>1.66</p>
</td>
<td style="width: 11.8143%"/>
<td style="width: 10%">
<p>.66</p>
</td>
<td style="width: 16%">
<p>.72</p>
</td>
</tr>
<tr>
<td style="width: 7%">
<p><strong>S4</strong></p>
</td>
<td style="width: 11%"/>
<td style="width: 11%">
<p>0</p>
</td>
<td style="width: 11%">
<p>0</p>
</td>
<td style="width: 11.8143%">
<p>0</p>
</td>
<td style="width: 10%"/>
<td style="width: 16%">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 7%">
<p><strong>S5</strong></p>
</td>
<td style="width: 11%">
<p>-1</p>
</td>
<td style="width: 11%"/>
<td style="width: 11%">
<p>?</p>
</td>
<td style="width: 11.8143%"/>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 16%">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>You can see the far right column has been calculated with the centered cosine similarity for each row versus row <em>S5</em>.</p>
<p>We next need to select a number, <em>k</em>, that's the number of the nearest neighbors we'll use to rate songs for user 3. We use <em>k = 2</em> in our simple example.</p>
<p>You can see that song <em>S1</em> and song <em>S3</em> are the most similar, so we'll use those two along with the ratings user 3 had for <em>S1</em> and <em>S3</em> (4 and 5, respectively).</p>
<p>Let's now calculate the rating:</p>
<p class="CDPAlignCenter CDPAlign"><em>(.98 * (4) + .72 * (5)) / (.98 + .72) = 4.42</em></p>
<p>So, based on this item-to-item collaborative filtering, we can see user 3 is likely to rate song <em>S5</em> very highly at 4.42 from our calculations.</p>
<p>Earlier, I said that user-to-user filtering is less effective than item-to-item filtering. Why might that be?</p>
<p>There's a good chance you have friends who really enjoy some of things that you enjoy as well, but then each of you has other areas of interest that the other has absolutely no interest in.</p>
<p>For example, perhaps you both love <em>Game of Thrones</em>, but your friend also loves Norwegian death metal. You, however, would rather be dead than listen to Norwegian death metal. If you're similar in many ways—excluding the death metal<span>—</span>with user-to-user recommendations, you're still going to see a lot of recommendations for bands with names that include words such as <em>flaming</em>, <em>axe</em>, <em>skull</em>, and <em>bludgeon</em>. With item-to-item filtering, most likely, you would be spared those suggestions.</p>
<p class="mce-root"/>
<p>So far, we've looked at users and items as a single entity when making comparisons, but now let's move on to look at another method that decomposes our users and items into what might be called <strong>feature baskets</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Content-based filtering</h1>
                </header>
            
            <article>
                
<p>As a musician himself, Tim Westergren had spent years on the road listening to other talented musicians, wondering why they could never get ahead. Their music was good—just as good as anything you might hear on the radio—and yet, somehow, they just never caught their big break. He imagined it must be because their music just never got in front of enough of the right people.</p>
<p>Tim eventually quit his job as a musician and took another job as a composer for movie scores. It was there that he began to think of each piece of music as having a distinct structure that could be decomposed into constituent parts<span>—</span>a form of musical DNA.</p>
<p>After giving it some thought, he began to consider creating a company around this idea of building a <strong>musical genome</strong>. He ran the concept by one of his friends, who had previously created and sold a company. The friend loved Tim's idea. So much so, in fact, that he began helping him to write a business plan and gather the initial funding round for the project. It was a go.</p>
<p>Over the next several years, they employed a small army of musicians who meticulously codified almost 400 distinct musical features for over a million pieces of music. Each feature was rated on a 0 to 5 point scale by hand (or maybe by ear is a better way to say it). Each three- or four-minute song took nearly a half hour to classify.</p>
<p>The features included things such as how gravelly the lead singers' voice was or how many beats per minute the tempo was. It took nearly a year for their first prototype to be completed. Built entirely in Excel using a VBA macro, it took nearly four minutes just to return a single recommendation. But in the end, it worked and it worked well.</p>
<p>That company is now known as Pandora music, and chances are you've either heard of it or used its products as it has millions of daily users around the world. It's without a doubt a triumphant example of content-based filtering.</p>
<p>Rather than treat each song as a single indivisible unit, as in content-based filtering, the songs become feature vectors that can be compared using our friend cosine similarity.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Another benefit is that not only are the songs subject to being decomposed into feature vectors, but the listeners can be as well. Each listener's taste profile becomes a vector in this space so that measurements can be made between their taste profiles and the songs themselves.</p>
<p>For Tim Westergren, this was the magic, because rather than rely on the popularity of the music like so many recommendations are, the recommendations from this system were made based upon the inherent structural similarity. Maybe you've never heard of song <em>X</em>, but if you like song <em>Y</em>, then you should like song <em>X</em> because it's <em>genetically</em> almost identical. That's content-based filtering.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hybrid systems</h1>
                </header>
            
            <article>
                
<p>We've now looked at the two primary forms of recommender systems, but you should know that, in any large-scale production environment, you're likely to see recommendations that leverage both of these. This is known as a <strong>hybrid system</strong>, and the reason hybrid systems are preferred is that they help eliminate the drawbacks that can be present when using either system alone. The two systems together create a more robust solution.</p>
<p>Let's examine the pros and cons of each type.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collaborative filtering</h1>
                </header>
            
            <article>
                
<p>The pros of collaborative filtering are as follows:</p>
<ul>
<li>There's no need to hand-craft features</li>
</ul>
<p>The cons are as follows:</p>
<ul>
<li>Doesn't work well without a large number of items and users</li>
<li>Sparsity when the number of items far exceeds the number that could be purchased</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Content-based filtering</h1>
                </header>
            
            <article>
                
<p><span>The pros of content-based filtering are as follows:</span></p>
<ul>
<li>It doesn't require a large number of users</li>
</ul>
<p><span>The cons are as follows:</span></p>
<ul>
<li>Defining the right features can be a challenge</li>
<li>Lack of serendipity</li>
</ul>
<p>As you can see, content-based filtering is a better choice when you haven't built up a large user base but, as you grow, adding on collaborative filtering can help introduce more serendipity into the recommendations.</p>
<p>Now that you're familiar with the types and inner-workings of recommendation engines, let's begin constructing one of our own.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a recommendation engine</h1>
                </header>
            
            <article>
                
<p>One thing I love to stumble upon is a really useful GitHub repository. You can find repositories that contain everything from hand-curated tutorials on machine learning to libraries that will save you dozens of lines of code when using <strong>Elasticsearch</strong>. The trouble is, finding these libraries is far more difficult than it should be. Fortunately, we now have the knowledge to leverage the GitHub API in a way that will help us to find these code gems.</p>
<p>We're going to be using the GitHub API to create a recommendation engine based on collaborative filtering. The plan is to get all of the repos<span>itories</span> I've starred over time and to then get all of the creators of those repos<span>itories</span> to find what repos<span>itories</span> they've starred. Once that's done, we'll find which users are most similar to me (or you, if you're running this for your own repository, which I suggest). Once we have the most similar users, we can use the repos<span>itories</span> they've starred and that I haven't to generate a set of recommendations.</p>
<p>Let's get started:</p>
<ol>
<li>We'll import the libraries we'll need:</li>
</ol>
<pre style="padding-left: 60px"><strong>import pandas as pd 
import numpy as np 
import requests 
import json</strong> </pre>
<ol start="2">
<li>You'll need to have opened an account with GitHub and to have starred a number of repos<span>itories</span> for this to work for your GitHub handle, but you won't actually need to sign up for the developer program. You can get an authorization token from your profile, which will allow you to use the API. You can also get it to work with this code, but the limits are too restrictive to make it usable for our example.</li>
</ol>
<ol start="3">
<li>To create a token for use with the API, go to the following URL at <a href="https://github.com/settings/tokens"><span class="URLPACKT">https://github.com/settings/tokens</span></a>. There, you will see a button in the upper-right corner like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-989 image-border" src="assets/6093907c-b461-403b-b45a-1e54166869ad.png" style="width:162.50em;height:33.17em;"/></p>
<ol start="4">
<li>You'll need to click on that <span class="packt_screen">Generate new token</span> button. Once you've done that, you need to select the permissions, I chose just <span class="packt_screen">public_repo</span>. Then, finally, copy the token it gives you for use in the following code. Be sure to enclose both in quotes:</li>
</ol>
<pre style="padding-left: 60px"><strong>myun = YOUR_GITHUB_HANDLE 
mypw = YOUR_PERSONAL_TOKEN</strong> </pre>
<ol start="5">
<li>We'll create the function that will pull the names of every repository you've starred:</li>
</ol>
<pre style="padding-left: 60px"><strong>my_starred_repos = [] 
def get_starred_by_me(): 
    resp_list = [] 
    last_resp = '' 
    first_url_to_get = 'https://api.github.com/user/starred' 
    first_url_resp = requests.get(first_url_to_get, auth=(myun,mypw)) 
    last_resp = first_url_resp 
    resp_list.append(json.loads(first_url_resp.text)) 
     
    while last_resp.links.get('next'): 
        next_url_to_get = last_resp.links['next']['url'] 
        next_url_resp = requests.get(next_url_to_get, auth=(myun,mypw)) 
        last_resp = next_url_resp 
        resp_list.append(json.loads(next_url_resp.text)) 
         
    for i in resp_list: 
        for j in i: 
            msr = j['html_url'] 
            my_starred_repos.append(msr)</strong> </pre>
<p style="padding-left: 60px">There's a lot going on in there, but, essentially, we're querying the API to get our own starred repos<span>itories</span>. GitHub uses pagination rather than return everything in one call. Because of this, we'll need to check the <kbd>.links</kbd> that are returned from each response. As long as there is a next link to call, we'll continue to do so.</p>
<ol start="6">
<li>We just need to call that function we created:</li>
</ol>
<pre style="padding-left: 60px"><strong>get_starred_by_me()</strong> </pre>
<ol start="7">
<li>Then, we can see the full list of starred repos<span>itories</span>:</li>
</ol>
<pre style="padding-left: 60px"><strong>my_starred_repos</strong> </pre>
<p style="padding-left: 60px">The preceding code will result in output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-680 image-border" src="assets/d5f6c6d1-d138-4a60-8863-fce88149a145.png" style="width:42.92em;height:20.67em;"/></p>
<ol start="8">
<li>We need to parse out the user names for each of the libraries we starred so that we can retrieve the libraries they starred:</li>
</ol>
<pre style="padding-left: 60px"><strong>my_starred_users = [] 
for ln in my_starred_repos: 
    right_split = ln.split('.com/')[1] 
    starred_usr = right_split.split('/')[0] 
    my_starred_users.append(starred_usr) 
 
my_starred_users</strong> </pre>
<p style="padding-left: 60px">This will result in output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-681 image-border" src="assets/d61cd8d7-6229-474c-8904-b38d4e680787.png" style="width:13.08em;height:23.75em;"/></p>
<ol start="9">
<li>Now that we have the handles for all of the users we starred, we'll need to retrieve all of the repos<span>itories</span> they starred. The following function will do just that:</li>
</ol>
<pre style="padding-left: 60px"><strong>starred_repos = {k:[] for k in set(my_starred_users)} 
def get_starred_by_user(user_name): 
    starred_resp_list = [] 
    last_resp = '' 
    first_url_to_get = 'https://api.github.com/users/'+ user_name +'/starred' 
    first_url_resp = requests.get(first_url_to_get, auth=(myun,mypw)) 
    last_resp = first_url_resp 
    starred_resp_list.append(json.loads(first_url_resp.text)) 
     
    while last_resp.links.get('next'): 
        next_url_to_get = last_resp.links['next']['url'] 
        next_url_resp = requests.get(next_url_to_get, auth=(myun,mypw)) 
        last_resp = next_url_resp 
        starred_resp_list.append(json.loads(next_url_resp.text)) 
         
    for i in starred_resp_list: 
        for j in i: 
            sr = j['html_url'] 
            starred_repos.get(user_name).append(sr)</strong> </pre>
<p class="mce-root"/>
<p style="padding-left: 60px">This function works in nearly the same way as the function we called earlier, but calls a different endpoint. It'll add their starred repos<span>itories</span> to a dict we'll use later.</p>
<ol start="10">
<li>Let's call it now. It may take a few minutes to run, depending on the number of repos<span>itories</span> each user has starred. I actually had one that starred over 4,000 repos<span>itories</span>:</li>
</ol>
<pre style="padding-left: 60px"><strong>for usr in list(set(my_starred_users)): 
    print(usr) 
    try: 
        get_starred_by_user(usr) 
    except: 
        print('failed for user', usr)</strong> </pre>
<p style="padding-left: 60px">The preceding code will result in output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-682 image-border" src="assets/98699917-fcac-48cc-99db-1f272910481d.png" style="width:11.92em;height:22.50em;"/></p>
<p>Notice that I turned the list of starred users into a set before I called it. I noticed some duplication that resulted from starring multiple repos<span>itories</span> under one user handle, so it makes sense to follow these steps to reduce extra calls:</p>
<ol>
<li>We now need to build a feature set that includes all of the starred repos<span>itories</span> of everyone we have starred:</li>
</ol>
<pre style="padding-left: 60px"><strong>repo_vocab = [item for sl in list(starred_repos.values()) for item in sl] </strong></pre>
<ol start="2">
<li>We'll convert that into a set to remove duplicates that may be present from multiple users starring the same repos<span>itories</span>:</li>
</ol>
<pre style="padding-left: 60px"><strong>repo_set = list(set(repo_vocab))</strong> </pre>
<ol start="3">
<li>Let's see how many that produces:</li>
</ol>
<pre style="padding-left: 60px"><strong>len(repo_vocab)</strong> </pre>
<p style="padding-left: 60px">The preceding code should result in output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-990 image-border" src="assets/b1945589-999e-41b7-8532-67ae6570a9c7.png" style="width:5.17em;height:2.08em;"/></p>
<p style="padding-left: 60px">I had starred 170 repos<span>itories</span>, and together the users of those repos<span>itories</span> starred over 27,000 unique repos<span>itories</span>. You can imagine if we went one degree further out how many we might see.</p>
<p>Now that we have the full feature set, or repository vocabulary, we need to run every user to create a binary vector that contains a <kbd>1</kbd> for every repository they've starred and a <kbd>0</kbd> for every repository they haven't:</p>
<pre><strong>all_usr_vector = [] 
for k,v in starred_repos.items(): 
    usr_vector = [] 
    for url in repo_set: 
        if url in v: 
            usr_vector.extend([1]) 
        else: 
            usr_vector.extend([0]) 
    all_usr_vector.append(usr_vector)</strong> </pre>
<p>What we just did was check for every user whether they had starred every repository in our repository vocabulary. If they did, they received a <kbd>1</kbd>, if they didn't, they received a <kbd>0</kbd>.</p>
<p>At this point, we have a 27,098 item binary vector for each user<span>—</span>all 170 of them. Let's now put this into a <kbd>DataFrame</kbd>. The row index will be the user handles we starred, and the columns will be the repository vocabulary:</p>
<pre><strong>df = pd.DataFrame(all_usr_vector, columns=repo_set, index=starred_repos.keys()) 
 
df</strong> </pre>
<p class="mce-root"/>
<p>The preceding code will generate output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-684 image-border" src="assets/13164eb3-0140-47d5-b8d9-e0bb7ae78ff7.png" style="width:132.67em;height:51.67em;"/></p>
<p>Next, in order to compare ourselves to the other users, we need to add our own row to this frame. Here, I add my user handle, but you should add your own:</p>
<pre><strong>my_repo_comp = [] 
for i in df.columns: 
    if i in my_starred_repos: 
        my_repo_comp.append(1) 
    else: 
        my_repo_comp.append(0) 
 
mrc = pd.Series(my_repo_comp).to_frame('acombs').T 
 
mrc </strong></pre>
<p>The preceding code will generate output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-991 image-border" src="assets/88bf2a5c-5bd1-4ac6-b737-18bb530ac824.png" style="width:123.25em;height:10.75em;"/></p>
<p>We now need add the appropriate column names and to concatenate this to our other <kbd>DataFrame</kbd>:</p>
<pre><strong>mrc.columns = df.columns 
 
fdf = pd.concat([df, mrc]) 
 
fdf</strong> </pre>
<p>The preceding code will result in output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1192 image-border" src="assets/877967fd-8853-45f8-80e8-e32663d021a3.png" style="width:34.50em;height:19.92em;"/></p>
<p>You can see in the previous screenshot that I've been added into the <kbd>DataFrame</kbd>.</p>
<p>From here, we just need to calculate the similarity between ourselves and the other users we've starred. We'll do that now using the <kbd>pearsonr</kbd> function which we'll need to import from SciPy:</p>
<pre><strong>from scipy.stats import pearsonr 
 
sim_score = {} 
for i in range(len(fdf)): 
    ss = pearsonr(fdf.iloc[-1,:], fdf.iloc[i,:]) 
    sim_score.update({i: ss[0]}) 
 
sf = pd.Series(sim_score).to_frame('similarity') 
sf</strong> </pre>
<p>The preceding code will generate output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-688 image-border" src="assets/08e99592-0732-40f3-b1f3-5dba3b2360fa.png" style="width:7.83em;height:22.83em;"/></p>
<p>What we've just done is compare our vector, the last one in the <kbd>DataFrame</kbd>, to every other user's vector to generate a centered cosine similarity (Pearson correlation coefficient). Some values are by necessity <kbd>NaN</kbd>, as they've starred no repos<span>itories</span>, and hence result in division by zero in the calculation:</p>
<ol>
<li>Let's now sort these values to return the index of the users who are most similar:</li>
</ol>
<pre style="padding-left: 60px"><strong>sf.sort_values('similarity', ascending=False)</strong> </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The preceding code will result in output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-689 image-border" src="assets/4e0c87e0-5c41-4154-8298-c1b8c54a0d1c.png" style="width:7.67em;height:21.50em;"/></p>
<p style="padding-left: 60px">So there we have it, those are the most similar users, and hence the ones that we can use to recommend repos<span>itories</span> we might enjoy. Let's take a look at these users and what they have starred that we might like.</p>
<ol start="2">
<li>You can ignore that first user with a perfect similarity score; that's our own repository. Going down the list, the three nearest matches are user 6, user 42, and user 116. Let's look at each:</li>
</ol>
<pre style="padding-left: 60px"><strong>fdf.index[6]</strong> </pre>
<p style="padding-left: 60px">The preceding code will result in output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-992 image-border" src="assets/7e4949d8-0add-4d51-83f2-e9c940281dba.png" style="width:6.17em;height:2.58em;"/></p>
<ol start="3">
<li>Let's take a look at who this is and their repository. From <a href="https://github.com/cchi"><span class="URLPACKT">https://github.com/cchi</span></a><span>, I can see who the repository belongs to the following user:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-993 image-border" src="assets/4171a7e8-7065-45b6-98ce-4464443fc815.png" style="width:17.00em;height:29.50em;"/></p>
<p>This is actually Charles Chi, a former colleague of mine from Bloomberg, so this is no surprise. Let's see what he has starred:</p>
<ol>
<li>There are a couple of ways to do this; we can either use our code, or just click under their picture on stars. Let's do both for this one, just to compare and make sure everything matches up. First, let's do it via code:</li>
</ol>
<pre style="padding-left: 60px"><strong>fdf.iloc[6,:][fdf.iloc[6,:]==1]</strong> </pre>
<p style="padding-left: 60px">This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-994 image-border" src="assets/b2363349-f1b3-4e15-acab-0ad246dfbb49.png" style="width:39.08em;height:18.58em;"/></p>
<ol start="2">
<li>We see 30 starred repos<span>itories</span>. Let's compare those to the ones from GitHub's site:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-995 image-border" src="assets/a67bbc65-7c4e-4a76-9d6c-c8ffc54cfa5a.png" style="width:162.50em;height:92.67em;"/></p>
<ol start="3">
<li>Here we can see they're identical, and you'll notice you can ID the repos<span>itories</span> that we've both starred: they are the ones labelled <span class="packt_screen">Unstar</span>.</li>
<li>Unfortunately, with just 30 starred repos<span>itories</span>, there aren't a lot of repos<span>itories</span> to generate recommendations.</li>
<li>The next user in terms of similarity is 42, Artem Golubin:</li>
</ol>
<pre style="padding-left: 60px"><strong>fdf.index[42]</strong> </pre>
<p style="padding-left: 60px">The preceding code results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-694 image-border" src="assets/9e826b79-e99b-468e-9bf5-ea87d04475a1.png" style="width:7.17em;height:2.58em;"/></p>
<p style="padding-left: 60px">And his GitHub profile below:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-996 image-border" src="assets/2fd46a81-dbcc-4aec-9dae-74db19f2f4c1.png" style="width:17.58em;height:28.50em;"/></p>
<p style="padding-left: 60px">Here we see the repos<span>itories</span> he has starred:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-997 image-border" src="assets/688655ef-721a-4503-8cb7-d90fd6ca56c1.png" style="width:43.83em;height:18.17em;"/></p>
<ol start="6">
<li>Artem has starred over 500 repos<span>itories</span>, so there are definitely some recommendations to be found there.</li>
<li>And finally, let's look at the third most similar user:</li>
</ol>
<pre style="padding-left: 60px"><strong>fdf.index[116] </strong></pre>
<p style="padding-left: 60px">This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-697 image-border" src="assets/80d07cbf-9bc3-4acc-8fc5-026dec03b83a.png" style="width:8.00em;height:1.83em;"/></p>
<p style="padding-left: 60px">This user, Kevin Markham, has starred around 60 repos<span>itories</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-998 image-border" src="assets/d7801ab7-37c5-4a81-b8df-0bd6a2a39ef8.png" style="width:11.42em;height:17.17em;"/></p>
<p style="padding-left: 60px">We can see the starred repos<span>itories</span> in the following image:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-999 image-border" src="assets/2f7d43f9-fa57-4307-94f9-780a6c73fbdf.png" style="width:44.58em;height:18.42em;"/></p>
<p>This is definitely fertile ground for generating recommendations. Let's now do just that; let's use the links from these three to produce some recommendations:</p>
<ol>
<li>We need to gather the links to the repos<span>itories</span> they've starred and that I haven't. We'll create a <kbd>DataFrame</kbd> that has the repos<span>itories</span> I've starred as well as the three most similar users to me:</li>
</ol>
<pre style="padding-left: 60px"><strong>all_recs = fdf.iloc[[6,42,116,159],:] 
all_recs.T</strong> </pre>
<p style="padding-left: 60px">The preceding code will produce the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1000 image-border" src="assets/0c9c7b37-46ef-45c0-acd3-6edcdb94bb6a.png" style="width:32.58em;height:18.75em;"/></p>
<ol start="2">
<li>Don't worry if it looks like it's all zeros; this is a sparse matrix so most will be 0. Let's see whether there are any repos<span>itories</span> we've all starred:</li>
</ol>
<pre style="padding-left: 60px"><strong>all_recs[(all_recs==1).all(axis=1)]</strong> </pre>
<p style="padding-left: 60px">The preceding code will produce the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-701 image-border" src="assets/fa2dbf18-0845-4435-92e9-f9c1b1db9616.png" style="width:37.00em;height:7.58em;"/></p>
<ol start="3">
<li>As you can see, we all seem to love scikit-learn and machine learning repos<span>itories</span><span>—</span>no surprise there. Let's see what they might have all starred that I missed. We'll start by creating a frame that excludes me, and then we'll query it for commonly starred repos<span>itories</span>:</li>
</ol>
<pre style="padding-left: 60px"><strong>str_recs_tmp = all_recs[all_recs[myun]==0].copy() 
str_recs = str_recs_tmp.iloc[:,:-1].copy() 
str_recs[(str_recs==1).all(axis=1)]</strong> </pre>
<p style="padding-left: 60px">The preceding code produces the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-702 image-border" src="assets/aa86dd2f-5706-4d00-88d5-4c6329d650de.png" style="width:14.75em;height:2.50em;"/></p>
<ol start="4">
<li>Okay, so it looks like I haven't been missing anything super obvious. Let's see if there any repos<span>itories</span> that at least two out of three users starred. To find this, we'll just sum across the rows:</li>
</ol>
<pre style="padding-left: 60px"><strong>str_recs.sum(axis=1).to_frame('total').sort_values(by='total', ascending=False) </strong></pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The preceding code will result in output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-703 image-border" src="assets/9160ebd7-e513-4038-ad8a-8eff7cd4c253.png" style="width:27.75em;height:28.75em;"/></p>
<p>This looks promising. There are lots of good ML and AI repos<span>itories</span>, and I'm honestly ashamed I never starred <span class="packt_screen">fuzzywuzzy</span> as I use that quite frequently.</p>
<p>At this point, I have to say I'm impressed with the results. These are definitely repos<span>itories</span> that interest me, and I'll be checking them out.</p>
<p>So far, we've generated the recommendations using collaborative filtering and done some light additional filtering using aggregation. If we wanted to go further, we could order the recommendation based upon the total number of stars they received. This could be achieved by making another call to the GitHub API. There's an endpoint that provides this information.</p>
<p>Another thing we could do to improve the results is to add in a layer of content-based filtering. This is the hybridization step we discussed earlier. We would need to create a set of features from our own repository that was indicative of the types of things we would be interested in. One way to do this would be to create a feature set by tokenizing the names of the repos<span>itories</span> we have starred along with their descriptions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here's a look at my starred repos<span>itories</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1001 image-border" src="assets/f755b271-db4e-45e1-9e4f-8aa9779c097b.png" style="width:162.50em;height:65.33em;"/></p>
<p>As you might imagine, this would generate a set of word features that we could use to vet the repos<span>itories</span> of those we found using collaborative filtering. This would include a lot of words such as <em>Python</em>, <em>Machine Learning</em>, and <em>Data Science</em>. This would ensure that users who are less similar to ourselves are still providing recommendations that are based on our own interests. It would also reduce the serendipity of the recommendations, which is something to consider. Perhaps there's something unlike anything I have currently that I would love to see. It's certainly a possibility.</p>
<p>What would that content-based filtering step look like in terms of a DataFrame? The columns would be word features (n-grams) and the rows would be the repos<span>itories</span> generated from our collaborative filtering step. We would just run the similarity process once again using our own repository for comparison.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about recommendation engines. We learned about the two primary types of systems in use today: collaborative filtering and content-based filtering. We also learned how they can be used together to form a hybrid system. We also discussed the pros and cons of each type of system. And finally, we learned step by step how to build a recommendation engine from scratch using the GitHub API.</p>
<p>I hope you build your own recommendation engine using the guidance in this chapter, and I hope you find resources that are useful to you. I know I've found a number of things I will certainly be using. Best of luck to you on your journey!</p>


            </article>

            
        </section>
    </body></html>
<html><head></head><body>
		<div id="_idContainer051">
			<h1 id="_idParaDest-65"><em class="italic"><a id="_idTextAnchor070"/>Chapter 3</em>: Bagging with Random Forests</h1>
			<p>In this chapter, you will gain proficiency in building <strong class="bold">random forests</strong>, a leading competitor to XGBoost. Like XGBoost, random forests are ensembles of decision trees. The difference is that random forests combine trees via <strong class="bold">bagging</strong>, while XGBoost combines trees via <strong class="bold">boosting</strong>. Random forests are a viable alternative to XGBoost with advantages and limitations that are highlighted in this chapter. Learning about random forests is important because they provide valuable insights into the structure of tree-based ensembles (XGBoost), and they allow a deeper understanding of boosting in comparison and contrast with their own method of bagging.</p>
			<p>In this chapter, you will build and evaluate <strong class="bold">random forest classifiers</strong> and <strong class="bold">random forest regressors</strong>, gain mastery of random forest hyperparameters, learn about bagging in the machine learning landscape, and explore a case study that highlights some random forest limitations that spurred the development of gradient boosting (XGBoost).</p>
			<p>This chapter covers the following main topics:</p>
			<ul>
				<li><p>Bagging ensembles</p></li>
				<li><p>Exploring random forests </p></li>
				<li><p>Tuning random forest hyperparameters</p></li>
				<li><p>Pushing random forest boundaries – case study</p></li>
			</ul>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor071"/>Technical requirements</h1>
			<p>The code for this chapter is available at <a href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03">https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03</a></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor072"/>Bagging ensembles</h1>
			<p>In this section, you will <a id="_idIndexMarker185"/>learn why ensemble methods are usually superior to individual machine learning models. Furthermore, you will learn about the technique of bagging. Both are essential features of random forests.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor073"/>Ensemble methods</h2>
			<p>In machine learning, an ensemble method is a machine learning model that aggregates the predictions of <a id="_idIndexMarker186"/>individual models. Since ensemble methods combine the results of multiple models, they are less prone to error, and therefore tend to perform better.</p>
			<p>Imagine your goal is to determine whether a house will sell within the first month of being on the market. You run several machine learning algorithms and find that <strong class="bold">logistic regression</strong> gives 80% accuracy, <strong class="bold">decision trees</strong> 75% accuracy, and <strong class="bold">k-nearest neighbors</strong> 77% accuracy.</p>
			<p>One option is to use logistic regression, the most accurate model, as your final model. A more compelling option is to combine the predictions of each individual model.</p>
			<p>For classifiers, the standard option is to take the majority vote. If at least two of three models predict that a house will sell within the first month, the prediction is <em class="italic">YES</em>. Otherwise, it's <em class="italic">NO</em>.</p>
			<p>Overall accuracy is usually higher with ensemble methods. For a prediction to be wrong, it's not enough for one model to get it wrong; the majority of classifiers must get it wrong.</p>
			<p>Ensemble methods are generally classified into two types. The first type combines different machine learning models, such as scikit-learn's <strong class="source-inline">VotingClassifier</strong>, as chosen by the user. The second type of ensemble method combines many versions of the same model, as is the case with XGBoost and random forests.</p>
			<p>Random forests are among the most popular and widespread of all ensemble methods. The individual models of random forests are decision trees, the focus of the previous chapter, <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>, <em class="italic">Decision Trees in Depth</em>. A random forest may consist of hundreds or thousands of decision trees whose predictions are combined for the final result.</p>
			<p>Although random forests use majority rules for classifiers, and the average of all models for regressors, they also use a <a id="_idIndexMarker187"/>special method called bagging, short for bootstrap aggregation, to select individual trees.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor074"/>Bootstrap aggregation</h2>
			<p><strong class="bold">Bootstrapping</strong> means <a id="_idIndexMarker188"/>sampling with replacement.</p>
			<p>Imagine you have a bag of 20 shaded marbles. You are going to select 10 marbles, one at a time. Each time you select a marble, you put it back in the bag. This means that it's possible, though extremely unlikely, that you could pick the same marble 10 times.</p>
			<p>It's more likely that <a id="_idIndexMarker189"/>you will pick some marbles more than once, and some not at all.</p>
			<p>Here is a visual of the marbles:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B15551_03_01.jpg" alt="Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia Commons, <a href="https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg">https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg</a>)</p>
			<p>As you can see from the preceding diagram, bootstrap samples are achieved by sampling with replacement. If the marbles were not replaced, it would be impossible to obtain a sample with more black (<em class="italic">blue</em> in the original diagram) marbles than the original bag, as in the far-right box.</p>
			<p>When it comes to random forests, bootstrapping works under the hood. The bootstrapping occurs when each decision tree is made. If the decision trees all consisted of the same samples, the trees would give similar predictions making the aggregate result similar to the individual tree. Instead, with random forests, the trees are built using bootstrapping, usually with the same number of samples as in the original dataset. Mathematical estimations are that two-thirds of the samples for each tree are unique, and one-third include duplicates.</p>
			<p>After the bootstrapping phase of the model-build, each decision tree makes its own individual predictions. The <a id="_idIndexMarker190"/>result is a forest of trees whose predictions are aggregated into one final prediction using majority rules for classifiers and the average for regressors.</p>
			<p>In summary, a random forest aggregates the predictions of bootstrapped decision trees. This general ensemble method is known in machine learning as bagging.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor075"/>Exploring random forests</h1>
			<p>To get a better sense of <a id="_idIndexMarker191"/>how random forests work, let's build one using scikit-learn. </p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor076"/>Random forest classifiers</h2>
			<p>Let's use a random forest classifier to predict whether a user makes more or less than USD 50,000 using the census <a id="_idIndexMarker192"/>dataset we cleaned and scored in <a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Landscape</em>, and revisited in <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>, <em class="italic">Decision Trees in Depth</em>. We are going to use <strong class="source-inline">cross_val_score</strong> to ensure that our test results generalize well:</p>
			<p>The following steps build and score a random forest classifier using the census dataset:</p>
			<ol>
				<li value="1"><p>Import <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">RandomForestClassifier</strong>, and <strong class="source-inline">cross_val_score</strong> before silencing warnings:</p><p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">import warnings</p><p class="source-code">warnings.filterwarnings('ignore')</p></li>
				<li><p>Load the dataset <strong class="source-inline">census_cleaned.csv</strong> and split it into <strong class="source-inline">X</strong> (a predictor column) and <strong class="source-inline">y</strong> (a target column):</p><p class="source-code">df_census = pd.read_csv('census_cleaned.csv')</p><p class="source-code">X_census = df_census.iloc[:,:-1]</p><p class="source-code">y_census = df_census.iloc[:,-1]</p><p>With our imports and data ready to go, it's time to build a model.</p></li>
				<li><p>Next, we initialize the random forest classifier. In practice, ensemble algorithms work just like any other machine learning algorithm. A model is initialized, fit to the training data, and <a id="_idIndexMarker193"/>scored against the test data.</p><p>We initialize a random forest by setting the following hyperparameters in advance:</p><p>a) <strong class="source-inline">random_state=2</strong> to ensure that your results are consistent with ours.</p><p>b) <strong class="source-inline">n_jobs=-1</strong> to speed up computations by taking advantage of parallel processing. </p><p>c) <strong class="source-inline">n_estimators=10</strong>, a previous scikit-learn default sufficient to speed up computations and avoid ambiguity; new defaults have set <strong class="source-inline">n_estimators=100</strong>. <strong class="source-inline">n_esmitators</strong> will be explored in further detail in the next section:</p><p class="source-code">rf = RandomForestClassifier(n_estimators=10, random_state=2, n_jobs=-1)</p></li>
				<li><p>Now we'll use <strong class="source-inline">cross_val_score</strong>. <strong class="source-inline">Cross_val_score</strong> requires a model, predictor columns, and a target column as inputs. Recall that <strong class="source-inline">cross_val_score</strong> splits, fits, and scores the data:</p><p class="source-code">scores = cross_val_score(rf, X_census, y_census, cv=5)</p></li>
				<li><p>Display the results:</p><p class="source-code">print('Accuracy:', np.round(scores, 3))</p><p class="source-code">print('Accuracy mean: %0.3f' % (scores.mean()))</p><p class="source-code"><strong class="bold">Accuracy: [0.851 0.844 0.851 0.852 0.851]</strong></p><p class="source-code"><strong class="bold">Accuracy mean: 0.850</strong></p></li>
			</ol>
			<p>The default random forest classifier provides a better score for the census dataset than the decision tree in <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>, <em class="italic">Decision Trees in Depth</em> (81%), but not quite as good as XGBoost in <a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Landscape</em> (86%). Why does it perform better than individual decision trees?</p>
			<p>The improved performance is likely on account of the bagging method described in the previous section. With 10 trees in this forest (since <strong class="source-inline">n_estimators=10</strong>), each prediction is based on 10 decision trees instead of 1. The trees are bootstrapped, which increases diversity, and aggregated, which reduces variance.</p>
			<p>By default, random forest classifiers select from the square root of the total number of features when looking <a id="_idIndexMarker194"/>for a split. So, if there are 100 features (columns), each decision tree will only consider 10 features when choosing a split. Thus two trees with duplicate samples may give very different predictions due to the different splits. This is another way that random forests reduce variance.</p>
			<p>In addition to classification, random forests also work with regression.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor077"/>Random forest regressors </h2>
			<p>In a random forest regressor, the samples are bootstrapped, as with the random forest Classifier, but the max <a id="_idIndexMarker195"/>number of features is the total number of features instead of the square root. This change is due to experimental results (see <a href="https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf">https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf</a>).</p>
			<p>Furthermore, the final prediction is made by taking the average of the predictions of all the trees, instead of a majority rules vote.</p>
			<p>To see a random forest regressor in action, complete the following steps:</p>
			<ol>
				<li value="1"><p>Upload the bike rental dataset from <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>, <em class="italic">Decision Trees in Depth</em>, and pull up the first five rows for a refresher:</p><p class="source-code">df_bikes = pd.read_csv('bike_rentals_cleaned.csv')</p><p class="source-code">df_bikes.head()</p><p>The preceding code should result in the following output:</p><div id="_idContainer047" class="IMG---Figure"><img src="image/B15551_03_02.jpg" alt="Figure 3.2 – Bike rentals dataset – cleaned"/></div><p class="figure-caption">Figure 3.2 – Bike rentals dataset – cleaned</p></li>
				<li><p>Split the data into <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong>, the predictive and target columns:</p><p class="source-code">X_bikes = df_bikes.iloc[:,:-1]</p><p class="source-code">y_bikes = df_bikes.iloc[:,-1]</p></li>
				<li><p>Import the <a id="_idIndexMarker196"/>regressor, then initialize it using the same default hyperparameters, <strong class="source-inline">n_estimators=10</strong>, <strong class="source-inline">random_state=2</strong>, and <strong class="source-inline">n_jobs=-1</strong>:</p><p class="source-code">from sklearn.ensemble import RandomForestRegressor</p><p class="source-code">rf = RandomForestRegressor(n_estimators=10, random_state=2, n_jobs=-1)</p></li>
				<li><p>Now we need to use <strong class="source-inline">cross_val_score</strong>. Place the regressor, <strong class="source-inline">rf</strong>, along with predictor and target columns inside <strong class="source-inline">cross_val_score</strong>. Note that the negative mean squared error (<strong class="source-inline">'neg_mean_squared_error'</strong>) should be defined as the scoring parameter. Select 10 folds (<strong class="source-inline">cv=10</strong>):</p><p class="source-code">scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)</p></li>
				<li><p>Find <a id="_idIndexMarker197"/>and display the <strong class="bold">root mean squared error</strong> (<strong class="bold">RMSE</strong>):</p><p class="source-code">rmse = np.sqrt(-scores)</p><p class="source-code">print('RMSE:', np.round(rmse, 3))</p><p class="source-code">print('RMSE mean: %0.3f' % (rmse.mean()))</p><p>The output is as follows:</p><p class="source-code">RMSE: [ 801.486  579.987  551.347  846.698  895.05  1097.522   893.738  809.284  833.488 2145.046]</p><p class="source-code">RMSE mean: 945.365</p></li>
			</ol>
			<p>The random forest performs respectably, though not as well as other models that we have seen. We will <a id="_idIndexMarker198"/>further examine the bike rentals dataset in the case study later in this chapter to see why.</p>
			<p>Next, let's examine random forest hyperparameters in detail.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor078"/>Random forest hyperparameters</h1>
			<p>The range of random <a id="_idIndexMarker199"/>forest hyperparameters is large, unless one already has a working knowledge of decision tree hyperparameters, as covered in <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>, <em class="italic">Decision Trees in Depth</em>.</p>
			<p>In this section, we will go over additional random forest hyperparameters before grouping the hyperparameters that you have already seen. Many of these hyperparameters will be used by XGBoost. </p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor079"/>oob_score</h2>
			<p>Our first <a id="_idIndexMarker200"/>hyperparameter, and perhaps the most intriguing, is <strong class="source-inline">oob_score</strong>.</p>
			<p>Random forests select decision trees via bagging, meaning that samples are selected with replacement. After all of the samples have been chosen, some samples should remain that have not been chosen.</p>
			<p>It's possible to hold back these samples as the test set. After the model is fit on one tree, the model can immediately be scored against this test set. When the hyperparameter is set to <strong class="source-inline">oob_score=True</strong>, this is exactly what happens.</p>
			<p>In other words, <strong class="source-inline">oob_score</strong> provides a shortcut to get a test score. <strong class="source-inline">oob_score</strong> may be printed out immediately after the model has been fit. </p>
			<p>Let's use <strong class="source-inline">oob_score</strong> on the census dataset to see how it works in practice. Since we are using <strong class="source-inline">oob_score</strong> to test the model, it's not necessary to split the data into a training set and test set.</p>
			<p>The random forest may be initialized as usual with <strong class="source-inline">oob_score=True</strong>:</p>
			<p class="source-code">rf = RandomForestClassifier(oob_score=True, n_estimators=10, random_state=2, n_jobs=-1)</p>
			<p>Next, <strong class="source-inline">rf</strong> may be fit on the data:</p>
			<p class="source-code">rf.fit(X_census, y_census)</p>
			<p>Since <strong class="source-inline">oob_score=True</strong>, the score is available after the model has been fit. It may be accessed using the model attribute <strong class="source-inline">.oob_score_</strong> as follows (note the underscore after <strong class="source-inline">score</strong>):</p>
			<p class="source-code">rf.oob_score_</p>
			<p>The score is as follows:</p>
			<p class="source-code">0.8343109855348423</p>
			<p>As described <a id="_idIndexMarker201"/>previously, <strong class="source-inline">oob_score</strong> is created by scoring samples on individual trees excluded during the training phase. When the number of trees in the forest is small, as is the case with 10 estimators, there may not be enough test samples to maximize accuracy. </p>
			<p>More trees mean more samples, and often greater accuracy.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor080"/>n_estimators</h2>
			<p>Random <a id="_idIndexMarker202"/>forests are powerful when there are many trees in the forest. How many is enough? Recently, scikit-learn defaults changed from 10 to 100. While 100 trees may be enough to cut down on variance and obtain good scores, for larger datasets, 500 or more trees may be required.</p>
			<p>Let's start with <strong class="source-inline">n_estimators=50</strong> to see how <strong class="source-inline">oob_score</strong> changes:</p>
			<p class="source-code">rf = RandomForestClassifier(n_estimators=50, oob_score=True, random_state=2, n_jobs=-1)</p>
			<p class="source-code">rf.fit(X_census, y_census)</p>
			<p class="source-code">rf.oob_score_</p>
			<p>The score is as follows:</p>
			<p class="source-code">0.8518780135745216</p>
			<p>A definite improvement. What about 100 trees?</p>
			<p class="source-code">rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=2, n_jobs=-1)</p>
			<p class="source-code">rf.fit(X_census, y_census)</p>
			<p class="source-code">rf.oob_score_</p>
			<p>The score is as follows:</p>
			<p class="source-code">0.8551334418476091</p>
			<p>The <a id="_idIndexMarker203"/>gain is smaller. As <strong class="source-inline">n_estimators</strong> continues to rise, scores will eventually level off. </p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor081"/>warm_start</h2>
			<p>The <strong class="source-inline">warm_start</strong> hyperparameter is <a id="_idIndexMarker204"/>great for determining the number of trees in the forest (<strong class="source-inline">n_estimators</strong>). When <strong class="source-inline">warm_start=True</strong>, adding more trees does not require starting over from scratch. If you change <strong class="source-inline">n_estimators</strong> from 100 to 200, it may take twice as long to build the forest with 200 trees. When <strong class="source-inline">warm_start=True</strong>, the random forest with 200 trees does not start from scratch, but rather starts where the previous model stopped.</p>
			<p><strong class="source-inline">warm_start</strong> may be used to plot various scores with a range of <strong class="source-inline">n_estimators</strong>. </p>
			<p>As an example, the following code takes increments of 50 trees, starting with 50 and ending at 500, to display a range of scores. This code may take time to run as it is building 10 random forests by adding 50 new trees each round! The code is broken down in the following steps:</p>
			<ol>
				<li value="1"><p>Import matplotlib and seaborn, then set the seaborn dark grid with <strong class="source-inline">sns.set()</strong>:</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p><p class="source-code">sns.set()</p></li>
				<li><p>Initialize an empty list of scores and initialize a random forest classifier with 50 estimators, making sure that <strong class="source-inline">warm_start=True</strong> and <strong class="source-inline">oob_score=True</strong>:</p><p class="source-code">oob_scores = []</p><p class="source-code">rf = RandomForestClassifier(n_estimators=50, warm_start=True, oob_score=True, n_jobs=-1, random_state=2)</p></li>
				<li><p>Fit <strong class="source-inline">rf</strong> to the dataset, then append <strong class="source-inline">oob_score</strong> to the <strong class="source-inline">oob_scores</strong> list:</p><p class="source-code">rf.fit(X_census, y_census)</p><p class="source-code">oob_scores.append(rf.oob_score_)</p></li>
				<li><p>Prepare a list of estimators that contains the number of trees starting with 50:</p><p class="source-code">est = 50</p><p class="source-code">estimators=[est]</p></li>
				<li><p>Write <a id="_idIndexMarker205"/>a for loop that adds 50 trees each round. For each round, add 50 to <strong class="source-inline">est</strong>, append <strong class="source-inline">est</strong> to the <strong class="source-inline">estimators</strong> list, change <strong class="source-inline">n_estimators</strong> with <strong class="source-inline">rf.set_params(n_estimators=est)</strong>, fit the random forest on the data, then append the new <strong class="source-inline">oob_score_</strong>:</p><p class="source-code">for i in range(9):</p><p class="source-code">    est += 50</p><p class="source-code">    estimators.append(est)</p><p class="source-code">    rf.set_params(n_estimators=est)</p><p class="source-code">    rf.fit(X_census, y_census)</p><p class="source-code">    oob_scores.append(rf.oob_score_)</p></li>
				<li><p>For a nice display, show a larger graph, then plot the estimators and <strong class="source-inline">oob_scores</strong>. Add the appropriate labels, then save and show the graph:</p><p class="source-code">plt.figure(figsize=(15,7))</p><p class="source-code">plt.plot(estimators, oob_scores)</p><p class="source-code">plt.xlabel('Number of Trees')</p><p class="source-code">plt.ylabel('oob_score_')</p><p class="source-code">plt.title('Random Forest Warm Start', fontsize=15)</p><p class="source-code">plt.savefig('Random_Forest_Warm_Start', dpi=325)</p><p class="source-code">plt.show()</p><p>This <a id="_idIndexMarker206"/>generates the following graph:</p></li>
			</ol>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B15551_03_03.jpg" alt="Figure 3.3 – Random forest Warm Start – oob_score per number of trees"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Random forest Warm Start – oob_score per number of trees</p>
			<p>As you can see, the number of trees tends to peak at around 300. It's more costly and time-consuming to use more trees than 300, and the gains are minimal at best.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor082"/>bootstrap</h2>
			<p>Although random <a id="_idIndexMarker207"/>forests are traditionally bootstrapped, the <strong class="source-inline">bootstrap</strong> hyperparameter may be set to <strong class="source-inline">False</strong>. If <strong class="source-inline">bootstrap=False</strong>, <strong class="source-inline">oob_score</strong> cannot be included since <strong class="source-inline">oob_score</strong> is only possible when samples have been left out.</p>
			<p>We will not pursue this option, although it makes sense if underfitting occurs.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor083"/>Verbose</h2>
			<p>The <strong class="source-inline">verbose</strong> hyperparameter <a id="_idIndexMarker208"/>may be changed to a higher number to display more information when building a model. You may try it on your own for experimentation. When building large models, <strong class="source-inline">verbose=1</strong> may provide helpful information along the way.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor084"/>Decision Tree hyperparameters</h2>
			<p>The remaining <a id="_idIndexMarker209"/>hyperparameters all come from decision trees. It turns out that decision tree hyperparameters are not as significant within random forests since random forests cut down on variance by design.</p>
			<p>Here are decision tree hyperparameters grouped according to category for you to review.</p>
			<h3>Depth</h3>
			<p>The hyperparameters that fall under this category are:</p>
			<ul>
				<li><p><strong class="source-inline">max_depth</strong>: Always <a id="_idIndexMarker210"/>good to tune. Determines the number of times splits occur. Known as the length of the tree. A great way to reduce variance.</p></li>
			</ul>
			<h3>Splits</h3>
			<p>The hyperparameters that fall under this category are:</p>
			<ul>
				<li><p><strong class="source-inline">max_features</strong>: Limits the <a id="_idIndexMarker211"/>number of features to choose from when making splits.</p></li>
				<li><p><strong class="source-inline">min_samples_split</strong>: Increases the number of samples required for new splits.</p></li>
				<li><p><strong class="source-inline">min_impurity_decrease</strong>: Limits splits to decrease impurity greater than the set threshold.</p></li>
			</ul>
			<h3>Leaves</h3>
			<p>The hyperparameters that fall under this category are:</p>
			<ul>
				<li><p><strong class="source-inline">min_samples_leaf</strong>: Increases the <a id="_idIndexMarker212"/>minimum number of samples required for a node to be a leaf. </p></li>
				<li><p><strong class="source-inline">min_weight_fraction_leaf</strong>: The fraction of the total weights required to be a leaf. </p></li>
			</ul>
			<p>For more information on the preceding hyperparameters, check out the official random forest regressor documentation: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html</a></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor085"/>Pushing random forest boundaries – case study</h1>
			<p>Imagine you work for a bike rental company and your goal is to predict the number of bike rentals per <a id="_idIndexMarker213"/>day depending upon the weather, the time of day, the time of year, and the growth of the company.</p>
			<p>Earlier in this chapter, you implemented a random forest regressor with cross-validation to obtain an RMSE of 945 bikes. Your goal is to modify the random forest to obtain the lowest error score possible.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor086"/>Preparing the dataset</h2>
			<p>Earlier in this chapter, you <a id="_idIndexMarker214"/>downloaded the dataset <strong class="source-inline">df_bikes</strong> and split it into <strong class="source-inline">X_bikes</strong> and <strong class="source-inline">y_bikes</strong>. Now that you are doing some serious testing, you decide to split <strong class="source-inline">X_bikes</strong> and <strong class="source-inline">y_bikes</strong> into training sets and test sets as follows:</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor087"/>n_estimators</h2>
			<p>Start by choosing a reasonable value for <strong class="source-inline">n_estimators</strong>. Recall that <strong class="source-inline">n_estimators</strong> can be increased to <a id="_idIndexMarker215"/>improve accuracy at the cost of computational resources and time.</p>
			<p>The following is a graph of RMSE using the <strong class="source-inline">warm_start</strong> method for a variety of <strong class="source-inline">n_estimators</strong> using the same general code provided previously under the <em class="italic">warm_start</em> heading:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B15551_03_04.jpg" alt="Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees</p>
			<p>This graph is very interesting. The random forest provides the best score with 50 estimators. After 100 estimators, the error gradually starts to go up, a concept that will be revisited later.</p>
			<p>For now, it's sensible to use <strong class="source-inline">n_estimators=50</strong> as the starting point.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor088"/>cross_val_score</h2>
			<p>With errors ranging from 620 to 690 bike rentals according to the preceding graph, it's time to see how the dataset performs with <a id="_idIndexMarker216"/>cross-validation using <strong class="source-inline">cross_val_score</strong>. Recall that in cross-validation the purpose is to divide the samples into <em class="italic">k</em> different folds, and to use all samples as test sets over the different folds. Since all samples are used to test the model, <strong class="source-inline">oob_score</strong> will not work.</p>
			<p>The following code contains the same steps that you used earlier in the chapter:</p>
			<ol>
				<li value="1"><p>Initialize the model.</p></li>
				<li><p>Score the model, using <strong class="source-inline">cross_val_score</strong> with the model, predictor columns, target column, scoring, and the number of folds as parameters.</p></li>
				<li><p>Compute the RMSE.</p></li>
				<li><p>Display the cross-validation scores and the mean.</p></li>
			</ol>
			<p>Here is the code:</p>
			<p class="source-code">rf = RandomForestRegressor(n_estimators=50, warm_start=True, n_jobs=-1, random_state=2)</p>
			<p class="source-code">scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)</p>
			<p class="source-code">rmse = np.sqrt(-scores)</p>
			<p class="source-code">print('RMSE:', np.round(rmse, 3))</p>
			<p class="source-code">print('RMSE mean: %0.3f' % (rmse.mean()))</p>
			<p>The output is as follows:</p>
			<p class="source-code">RMSE: [ 836.482  541.898  533.086  812.782  894.877  881.117   794.103  828.968  772.517 2128.148]</p>
			<p class="source-code"><strong class="bold">RMSE mean: 902.398</strong></p>
			<p>This score is better than earlier in the chapter. Notice that the error in the last fold is much higher according to the last entry in the RMSE array. This could be due to errors within the data or outliers.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor089"/>Fine-tuning hyperparameters</h2>
			<p>It's time to create a grid of hyperparameters to fine-tune our model using <strong class="source-inline">RandomizedSearchCV</strong>. Here is a function that uses <strong class="source-inline">RandomizedSearchCV</strong> to display the RMSEs along <a id="_idIndexMarker217"/>with the mean score and best hyperparameters:</p>
			<p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p>
			<p class="source-code">def randomized_search_reg(params, runs=16, reg=RandomForestRegressor(random_state=2, n_jobs=-1)):</p>
			<p class="source-code">    rand_reg = RandomizedSearchCV(reg, params, n_iter=runs, scoring='neg_mean_squared_error', cv=10, n_jobs=-1, random_state=2)</p>
			<p class="source-code">    rand_reg.fit(X_train, y_train)</p>
			<p class="source-code">    best_model = rand_reg.best_estimator_</p>
			<p class="source-code">    best_params = rand_reg.best_params_</p>
			<p class="source-code">    print("Best params:", best_params)</p>
			<p class="source-code">    best_score = np.sqrt(-rand_reg.best_score_)</p>
			<p class="source-code">    print("Training score: {:.3f}".format(best_score))</p>
			<p class="source-code">    y_pred = best_model.predict(X_test)</p>
			<p class="source-code">    from sklearn.metrics import mean_squared_error as MSE</p>
			<p class="source-code">    rmse_test = MSE(y_test, y_pred)**0.5</p>
			<p class="source-code">    print('Test set score: {:.3f}'.format(rmse_test))</p>
			<p>Here is a starter's grid of hyperparameters placed inside the new <strong class="source-inline">randomized_search_reg</strong> function to obtain the first results:</p>
			<p class="source-code">randomized_search_reg(params={'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05],'min_samples_split':[2, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.1],'min_samples_leaf':[1,2,4,6,8,10,20,30],'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,2,4,6,8,10,20]})</p>
			<p>The output is as follows:</p>
			<p class="source-code">Best params: {'min_weight_fraction_leaf': 0.0, 'min_samples_split': 0.03, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.05, 'max_leaf_nodes': 25, 'max_features': 0.7, 'max_depth': None}</p>
			<p class="source-code"><strong class="bold">Training score: 759.076</strong></p>
			<p class="source-code"><strong class="bold">Test set score: 701.802</strong></p>
			<p>This is a <a id="_idIndexMarker218"/>major improvement. Let's see if we can do better by narrowing the range:</p>
			<p class="source-code">randomized_search_reg(params={'min_samples_leaf': [1,2,4,6,8,10,20,30], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4], 'max_depth':[None,2,4,6,8,10,20]})</p>
			<p>The output is as follows:</p>
			<p class="source-code">Best params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 10}</p>
			<p class="source-code"><strong class="bold">Training score: 679.052</strong></p>
			<p class="source-code"><strong class="bold">Test set score: 626.541</strong></p>
			<p>The score has improved yet again. </p>
			<p>Now let's increase the number of runs, and give more options for <strong class="source-inline">max_depth</strong>:</p>
			<p class="source-code">randomized_search_reg(params={'min_samples_leaf':[1,2,4,6,8,10,20,30],'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,4,6,8,10,12,15,20]}, runs=20)</p>
			<p>The output is as follows:</p>
			<p class="source-code">Best params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 12}</p>
			<p class="source-code"><strong class="bold">Training score: 675.128</strong></p>
			<p class="source-code"><strong class="bold">Test set score: 619.014</strong></p>
			<p>The score keeps getting better. At this point, it may be worth narrowing the ranges further, based upon the previous results:</p>
			<p class="source-code">randomized_search_reg(params={'min_samples_leaf':[1,2,3,4,5,6], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.08, 0.10, 0.12, 0.15], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,8,10,12,14,16,18,20]})</p>
			<p>The <a id="_idIndexMarker219"/>output is as follows:</p>
			<p class="source-code">Best params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.05, 'max_features': 0.7, 'max_depth': 18}</p>
			<p class="source-code"><strong class="bold">Training score: 679.595</strong></p>
			<p class="source-code"><strong class="bold">Test set score: 630.954</strong></p>
			<p>The test score has gone back up. Increasing <strong class="source-inline">n_estimators</strong> at this point could be a good idea. The more trees in the forest, the more potential there may be to realize small gains.</p>
			<p>We can also increase the number of runs to <strong class="source-inline">20</strong> to look for better hyperparameter combinations. Keep in mind that results are based on a randomized search, not a full grid search:</p>
			<p class="source-code">randomized_search_reg(params={'min_samples_leaf':[1,2,4,6,8,10,20,30], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,4,6,8,10,12,15,20],'n_estimators':[100]}, runs=20)</p>
			<p>The output is as follows: </p>
			<p class="source-code">Best params: {'n_estimators': 100, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 12}</p>
			<p class="source-code"><strong class="bold">Training score: 675.128</strong></p>
			<p class="source-code"><strong class="bold">Test set score: 619.014</strong></p>
			<p>This matches the best score achieved thus far. We could keep tinkering. It's possible with enough experimentation that the test score may drop to under 600 bikes. But we also seem to be peaking around the low 600 mark.</p>
			<p>Finally, let's <a id="_idIndexMarker220"/>place our best model in <strong class="source-inline">cross_val_score</strong> to see how the result compares with the original:</p>
			<p class="source-code">rf = RandomForestRegressor(n_estimators=100,  min_impurity_decrease=0.1, max_features=0.6, max_depth=12, warm_start=True, n_jobs=-1, random_state=2)</p>
			<p class="source-code">scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)</p>
			<p class="source-code">rmse = np.sqrt(-scores)</p>
			<p class="source-code">print('RMSE:', np.round(rmse, 3))</p>
			<p class="source-code">print('RMSE mean: %0.3f' % (rmse.mean()))</p>
			<p>The output is as follows:</p>
			<p class="source-code">RMSE: [ 818.354  514.173  547.392  814.059  769.54   730.025  831.376  794.634  756.83  1595.237]</p>
			<p class="source-code"><strong class="bold">RMSE mean: 817.162</strong></p>
			<p>The RMSE goes back up to <strong class="source-inline">817</strong>. The score is much better than <strong class="source-inline">903</strong>, but it's considerably worse than <strong class="source-inline">619</strong>. What's going on here?</p>
			<p>There may be an issue with the last split in <strong class="source-inline">cross_val_score</strong> since its score is twice as bad as the others. Let's see if shuffling the data does the trick. Scikit-learn has a shuffle module that may be imported from <strong class="source-inline">sklearn.utils</strong> as follows:</p>
			<p class="source-code">from sklearn.utils import shuffle</p>
			<p>Now we can shuffle the data as follows:</p>
			<p class="source-code">df_shuffle_bikes = shuffle(df_bikes, random_state=2)</p>
			<p>Now split the data into a new <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> and run <strong class="source-inline">RandomForestRegressor</strong> with <strong class="source-inline">cross_val_score</strong> again:</p>
			<p class="source-code">X_shuffle_bikes = df_shuffle_bikes.iloc[:,:-1]</p>
			<p class="source-code">y_shuffle_bikes = df_shuffle_bikes.iloc[:,-1]</p>
			<p class="source-code">rf = RandomForestRegressor(n_estimators=100,  min_impurity_decrease=0.1, max_features=0.6, max_depth=12, n_jobs=-1, random_state=2)</p>
			<p class="source-code">scores = cross_val_score(rf, X_shuffle_bikes, y_shuffle_bikes, scoring='neg_mean_squared_error', cv=10)</p>
			<p class="source-code">rmse = np.sqrt(-scores)</p>
			<p class="source-code">print('RMSE:', np.round(rmse, 3))</p>
			<p class="source-code">print('RMSE mean: %0.3f' % (rmse.mean()))</p>
			<p>The output is as follows:</p>
			<p class="source-code">RMSE: [630.093 686.673 468.159 526.676 593.033 724.575 774.402 672.63  760.253  616.797]</p>
			<p class="source-code"><strong class="bold">RMSE mean: 645.329</strong></p>
			<p>In the <a id="_idIndexMarker221"/>shuffled data, there is no issue with the last split, and the score is much higher, as expected.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor090"/>Random forest drawbacks</h2>
			<p>At the end of the day, the random forest is limited by its individual trees. If all trees make the same mistake, the <a id="_idIndexMarker222"/>random forest makes this mistake. There are scenarios, as is revealed in this case study before the data was shuffled, where random forests are unable to significantly improve upon errors due to challenges within the data that individual trees are unable to address.</p>
			<p>An ensemble method capable of improving upon initial shortcomings, an ensemble method that will learn from the mistakes of trees in future rounds, could be advantageous. Boosting was designed to learn from the mistakes of trees in early rounds. Boosting, in particular gradient boosting – the focus of the next chapter – addresses this topic. </p>
			<p>In closure, the following graph displays the results of the tuned random forest regressor and the default XGBoost regressor when increasing the number of trees in the bike rentals dataset if the data is not shuffled:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B15551_03_05.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Comparing the XGBoost default model with a tuned random forest</p>
			<p>As you can see, XGBoost <a id="_idIndexMarker223"/>does a much<a id="_idTextAnchor091"/> better job of learning as the number of trees increases. And the XGBoost model has not even been tuned!</p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor092"/>Summary</h1>
			<p>In this chapter, you learned about the importance of ensemble methods. In particular, you learned about bagging, the combination of bootstrapping, sampling with replacement, and aggregation, combining many models into one. You built random forest classifiers and regressors. You adjusted <strong class="source-inline">n_estimators</strong> with the <strong class="source-inline">warm_start</strong> hyperparameter and used <strong class="source-inline">oob_score_</strong> to find errors. Then you modified random forest hyperparameters to fine-tune models. Finally, you examined a case study where shuffling the data gave excellent results but adding more trees to the random forest did not result in any gains with the unshuffled data, as contrasted with XGBoost.</p>
			<p>In the next chapter, you will learn the fundamentals of boosting, an ensemble method that learns from its mistakes to improve upon accuracy as more trees are added. You will implement gradient boosting to make predictions, thereby setting the stage for Extreme gradient boosting, better known as XGBoost.</p>
		</div>
	</body></html>
- en: 'Chapter 4: Training Machine Learning Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how Amazon SageMaker Autopilot makes it
    easy to build, train, and optimize models automatically, without writing a line
    of machine learning code.
  prefs: []
  type: TYPE_NORMAL
- en: For problem types that are not supported by SageMaker Autopilot, the next best
    option is to use one of the algorithms already implemented in SageMaker and to
    train it on your dataset. These algorithms are referred to as **built-in algorithms**,
    and they cover many typical machine learning problems, from classification to
    time series to anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about built-in algorithms for supervised and
    unsupervised learning, what type of problems you can solve with them, and how
    to use them with the SageMaker SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the built-in algorithms in Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and deploying models with built-in algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the SageMaker SDK with built-in algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with more built-in algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you don't already have one, please point your browser to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to install and configure the AWS Command-Line Interface (CLI)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory, but
    strongly encouraged as it includes many projects that we will need (Jupyter, `pandas`,
    `numpy`, and more).
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the built-in algorithms in Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Built-in algorithms are machine learning algorithms implemented, and in some
    cases invented, by Amazon ([https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html)).
    They let you quickly train and deploy your own models without writing a line of
    machine learning code. Indeed, since the training and prediction code is readily
    available, you don't have to worry about implementing it, and you can focus on
    the machine learning problem at hand. As usual with SageMaker, infrastructure
    is fully managed, saving you even more time.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you'll learn about the built-in algorithms for traditional
    machine learning problems. Algorithms for computer vision and natural language
    processing will be covered in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Supervised learning focuses on problems that require a labeled dataset, such
    as regression or classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Learner** builds linear models to solve regression problems, as well
    as classification problems (binary or multi-class).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factorization Machines** builds linear models to solve regression problems,
    as well as classification problems (binary or multi-class). Factorization machines
    are a generalization of linear models, and they''re a good fit for high-dimension,
    sparse datasets, such as user-item interaction matrices in recommendation problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-nearest neighbors** (**KNN**) builds non-parametric models for regression
    and classification problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGBoost** builds models for regression, classification, and ranking problems.
    XGBoost is possibly the most widely used machine learning algorithm used today,
    and SageMaker uses the open source implementation available at [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepAR** builds forecasting models for multivariate time series. DeepAR is
    an Amazon-invented algorithm based on **Recurrent Neural Networks**, and you can
    read more about it at [https://arxiv.org/abs/1704.04110](https://arxiv.org/abs/1704.04110).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object2Vec** learns low-dimension embeddings from general-purpose high-dimensional
    objects. Object2Vec is an algorithm invented by Amazon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BlazingText** builds text classification models. This algorithm was invented
    by Amazon, and you can read more about it at [https://dl.acm.org/doi/10.1145/3146347.3146354](https://dl.acm.org/doi/10.1145/3146347.3146354).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unsupervised learning doesn''t require a labeled dataset, and includes problems
    such as clustering or anomaly detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '**K-means** builds clustering models. SageMaker uses a modified version of
    the web-scale k-means clustering algorithm ([https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**) builds dimensionality reduction
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Cut Forest** builds anomaly detection models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP Insights** builds models to identify usage patterns for IPv4 addresses.
    This comes in handy for monitoring, cybersecurity, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BlazingText** computes word vectors, a very useful representation for natural
    language processing tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll cover some of these algorithms in detail in the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A word about scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we dive into training and deploying models with the algorithms, you may
    wonder why you should use them instead of their counterparts in well-known libraries
    such as `scikit-learn` and `R`.
  prefs: []
  type: TYPE_NORMAL
- en: First, these algorithms have been implemented and tuned by Amazon teams, who
    are not exactly newcomers to machine learning! A lot of effort has been put into
    making sure that these algorithms run as fast as possible on AWS infrastructure,
    no matter what type of instance you use. In addition, many of these algorithms
    support **distributed training** out of the box, letting you split model training
    across a cluster of fully managed instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this, benchmarks indicate that these algorithms are generally 10
    times better than competing implementations. In many cases, they are also much
    more cost-effective. You can learn more about this at the following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS Tel Aviv Summit 2018: *Speed Up Your Machine Learning Workflows with Built-In
    Algorithms*: [https://www.youtube.com/watch?v=IeIUr78OrE0](https://www.youtube.com/watch?v=IeIUr78OrE0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elastic Machine Learning Algorithms in Amazon*, Liberty et al., SIGMOD''20:
    SageMaker: [https://www.amazon.science/publications/elastic-machine-learning-algorithms-in-amazon-sagemaker](https://www.amazon.science/publications/elastic-machine-learning-algorithms-in-amazon-sagemaker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, these algorithms benefit from all the features present in SageMaker,
    as you will find out by the end of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Training and deploying models with built-in algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon SageMaker lets you train and deploy models in many different configurations.
    Although it encourages best practices, it is a modular service that lets you do
    things your own way.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll first look at a typical end-to-end workflow, where we
    use SageMaker from data upload all the way to model deployment. Then, we'll discuss
    alternative workflows, and how you can cherry-pick the features that you need.
    Finally, we will take a look under the hood, and see what happens from an infrastructure
    perspective when we train and deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the end-to-end workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at a typical SageMaker workflow. You''ll see it again and again
    in our examples, as well as in the AWS notebooks available on GitHub ([https://github.com/awslabs/amazon-sagemaker-examples/](https://github.com/awslabs/amazon-sagemaker-examples/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`protobuf` ([https://developers.google.com/protocol-buffers](https://developers.google.com/protocol-buffers)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Configure the training job**: This is where you select the algorithm that
    you want to train with, set hyperparameters, and define infrastructure requirements
    for the training job.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Launch the training job**: This is where we pass the location of your dataset
    in S3\. Training takes place on managed infrastructure, created and provisioned
    automatically according to your requirements. Once training is complete, the **model
    artifact** is saved in S3\. The training infrastructure is terminated automatically,
    and you only pay for what you used.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deploy the model**: You can deploy a model either on a **real-time HTTPS
    endpoint** for live prediction or for **batch transform**. Again, you simply need
    to define infrastructure requirements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Predict data**: Either invoking a real-time endpoint or a batch transformer.
    As you would expect, infrastructure is managed here too. For production, you would
    also monitor the quality of data and predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Clean up!**: This involves taking the endpoint down, to avoid unnecessary
    charges.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding this workflow is critical in being productive with Amazon SageMaker.
    Fortunately, the SageMaker SDK has simple APIs that closely match these steps,
    so you shouldn't be confused about which one to use, or when to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start looking at the SDK, let's consider alternative workflows that
    could make sense in your business and technical environments.
  prefs: []
  type: TYPE_NORMAL
- en: Using alternative workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon SageMaker is a modular service that lets you work your way. Let's first
    consider a workflow where you would train on SageMaker and deploy on your own
    server, whatever the reasons may be.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting a model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Steps 1-3 would be the same as in the previous example, and then you would
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the training artifact from S3, which is materialized as a `model.tar.gz`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the model stored in the artifact.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On your own server, load the model with the appropriate machine learning library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`fastText` implementation available at [https://fasttext.cc/](https://fasttext.cc/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For all other models**: Use **Apache MXNet** ([https://mxnet.apache.org/](https://mxnet.apache.org/)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's see how you could import an existing model and deploy it on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Importing a model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The steps are equally simple:'
  prefs: []
  type: TYPE_NORMAL
- en: Package your model in a model artifact (`model.tar.gz`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the artifact to an S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the artifact as a SageMaker model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the model and predict.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is just a quick look. We'll run full examples for both workflows in [*Chapter
    11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237), *Deploying Machine Learning
    Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Using fully managed infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All SageMaker jobs run on managed infrastructure. Let's take a look under the
    hood and see what happens when we train and deploy models.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging algorithms in Docker containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All SageMaker algorithms must be packaged in **Docker** containers. Don''t
    worry, you don''t need to know much about Docker in order to use SageMaker. If
    you''re not familiar with it, I would recommend going through this tutorial to
    understand key concepts and tools: [https://docs.docker.com/get-started/](https://docs.docker.com/get-started/).
    It''s always good to know a little more than actually required!'
  prefs: []
  type: TYPE_NORMAL
- en: As you would expect, built-in algorithms are pre-packaged, and containers are
    readily available for training and deployment. They are hosted in **Amazon Elastic
    Container Registry** (**ECR**), AWS' Docker registry service ([https://aws.amazon.com/ecr/](https://aws.amazon.com/ecr/)).
    As ECR is a region-based service, you will find a collection of containers in
    each region where SageMaker is available.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the list of built-in algorithm containers at [https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html).
    For instance, the name of the container for the Linear Learner algorithm in the
    eu-west-1 region is `438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest`.
    These containers can only be pulled to SageMaker managed instances, so you won't
    be able to run them on your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the training infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you launch a training job, SageMaker fires up infrastructure according
    to your requirements (instance type and instance count).
  prefs: []
  type: TYPE_NORMAL
- en: Once a training instance is in service, it pulls the appropriate training container
    from ECR. Hyperparameters are applied to the algorithm, which also receives the
    location of your dataset. By default, the algorithm then copies the full dataset
    from S3 and starts training. If distributed training is configured, SageMaker
    automatically distributes dataset batches to the different instances in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Once training is complete, the model is packaged in a model artifact saved in
    S3\. Then, the training infrastructure is shut down automatically. Logs are available
    in **Amazon CloudWatch Logs**. Last but not least, you're only charged for the
    exact amount of training time.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the prediction infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you launch a deployment job, SageMaker once again creates infrastructure
    according to your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Let's focus on real-time endpoints for now, and not on batch transform.
  prefs: []
  type: TYPE_NORMAL
- en: Once an endpoint instance is in service, it pulls the appropriate prediction
    container from ECR and loads your model from S3\. Then, the HTTPS endpoint is
    provisioned and is ready for prediction within minutes.
  prefs: []
  type: TYPE_NORMAL
- en: If you configured the endpoint with several instances, load balancing and high
    availability are set up automatically. If you configured **Auto Scaling**, this
    is applied as well.
  prefs: []
  type: TYPE_NORMAL
- en: As you would expect, an endpoint stays up until it's deleted explicitly, either
    in the AWS Console or with a SageMaker API call. In the meantime, you will be
    charged for the endpoint, so **please make sure to delete endpoints that you don't
    need!**
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the big picture, let's start looking at the SageMaker
    SDK, and how we can use it to train and deploy models.
  prefs: []
  type: TYPE_NORMAL
- en: Using the SageMaker SDK with built-in algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Being familiar with the SageMaker SDK is important to making the most of SageMaker.
    You can find its documentation at [https://sagemaker.readthedocs.io](https://sagemaker.readthedocs.io).
  prefs: []
  type: TYPE_NORMAL
- en: Walking through a simple example is the best way to get started. In this section,
    we'll use the Linear Learner algorithm to train a regression model on the Boston
    Housing dataset ([https://www.kaggle.com/c/boston-housing](https://www.kaggle.com/c/boston-housing)).
    We'll proceed very slowly, leaving no stone unturned. Once again, these concepts
    are essential, so please take your time, and make sure you understand every step
    fully.
  prefs: []
  type: TYPE_NORMAL
- en: Reminder
  prefs: []
  type: TYPE_NORMAL
- en: 'I recommend that you follow along and run the code available in the companion
    GitHub repository. Every effort has been made to check all code samples present
    in the text. However, for those of you who have an electronic version, copying
    and pasting may have unpredictable results: formatting issues, weird quotes, and
    so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Built-in algorithms expect the dataset to be in a certain format, such as **CSV**,
    **protobuf**, or **libsvm**. Supported formats are listed in the algorithm documentation.
    For instance, Linear Learner supports CSV and RecordIO-wrapped protobuf ([https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our input dataset is already in the repository in CSV format, so let''s use
    that. The dataset preparation will be extremely simple, and we''ll run it manually:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `pandas`, we load the CSV dataset with pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we print the shape of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It contains 506 samples and 13 columns:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we display the first 5 lines of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the table visible in the following figure. For each house,
    we see 12 features, and a target attribute (`medv`) set to the median value of
    the house in thousands of dollars:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Viewing the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_04_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.1 – Viewing the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Reading the algorithm documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html)),
    we see that *Amazon SageMaker requires that a CSV file doesn''t have a header
    record and that the target variable is in the first column*. Accordingly, we move
    the `medv` column to the front of the dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A bit of `scikit-learn` magic helps split the dataframe up into two parts –
    90% for training, and 10% for validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We save these two splits to individual CSV files, without either an index or
    a header:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now need to upload these two files to S3\. We could use any bucket, and
    here we''ll use the default bucket conveniently created by SageMaker in the region
    we''re running in. We can find its name with the `sagemaker.Session.default_bucket()`
    API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we use the `sagemaker.Session.upload_data()` API to upload the two
    **CSV** files to the default bucket. Here, the training and validation datasets
    are made of a single file each, but we could upload multiple files if needed.
    For this reason, **we must upload the datasets under different S3 prefixes**,
    so that their files won''t be mixed up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The two S3 paths look like this. Of course, the account number in the default
    bucket name will be different:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that data is ready in S3, we can configure the training job.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a training job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Estimator` object (`sagemaker.estimator.Estimator`) is the cornerstone
    of model training. It lets you select the appropriate algorithm, define your training
    infrastructure requirements, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SageMaker SDK also includes algorithm-specific estimators, such as `sagemaker.LinearLearner`
    or `sagemaker.PCA`. I generally find them less flexible than the generic estimator
    (no CSV support, for one thing), and I don''t recommend using them. Using the
    `Estimator` object also lets you reuse your code across examples, as we will see
    in the next sections:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, we learned that SageMaker algorithms are packaged
    in Docker containers. Using `boto3` and the `image_uris.retrieve()` API, we can
    easily find the name of the Linear Learner algorithm in the region we''re running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we know the name of the container, we can configure our training job
    with the `Estimator` object. In addition to the container name, we also pass the
    IAM role that SageMaker instances will use, the instance type and instance count
    to use for training, as well as the output location for the model. `Estimator`
    will generate a training job automatically, and we could also set our own prefix
    with the `base_job_name` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: SageMaker supports plenty of different instance types, with some differences
    across AWS regions. You can find the full list at [https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html](https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Which one should we use here? Looking at the Linear Learner documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances)),
    we see that *you can train the Linear Learner algorithm on single- or multi-machine
    CPU and GPU instances*. Here, we''re working with a tiny dataset, so let''s select
    the smallest training instance available in our region: `ml.m5.large`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Checking the pricing page ([https://aws.amazon.com/sagemaker/pricing/](https://aws.amazon.com/sagemaker/pricing/)),
    we see that this instance costs $0.128 per hour in the eu-west-1 region (the one
    I'm using for this job).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we have to set `predictor_type`. It defines the type of problem that Linear
    Learner is training on (regression, binary classification, or multiclass classification).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Taking a deeper look, we see that the default value for `mini_batch_size` is
    1000: this isn''t going to work well with our 506-sample dataset, so let''s set
    it to 32\. We also learn that the `normalize_data` parameter is set to true by
    default, which makes it unnecessary to normalize data ourselves:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define the data channels: a channel is a named source of data passed
    to a SageMaker estimator. All built-in algorithms need at least a training channel,
    and many also accept additional channels for validation and testing. Here, we
    have two channels, which both provide data in CSV format. The `TrainingInput()`
    API lets us define their location, their format, whether they are compressed,
    and so on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By default, data served by a channel will be fully copied to each training instance,
    which is fine for small datasets. We'll study alternatives in [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Advanced Training Techniques*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Everything is now ready for training, so let's launch our job.
  prefs: []
  type: TYPE_NORMAL
- en: Launching a training job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All it takes is one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply pass a Python dictionary containing the two channels to the `fit()`
    API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Immediately, the training job starts:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As soon as the job is launched, it appears in the **SageMaker components and
    registries** | **Experiments and trials** panel. There, you can see all job metadata:
    the location of the dataset, hyperparameters, and more.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The training log is visible in the notebook, and it''s also stored in Amazon
    CloudWatch Logs, under the `/aws/sagemaker/TrainingJobs` prefix. Here are the
    first few lines, showing the infrastructure being provisioned, as explained earlier,
    in the *Using fully managed infrastructure* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the end of the training log, we see information on the **mean square error**
    (**MSE**) and loss metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once training is complete, the model is copied automatically to S3, and SageMaker
    tells us how long the job took:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at the output location in our S3 bucket, we see the model artifact:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We'll see in [*Chapter 11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237),
    *Deploying Machine Learning Models*, what's inside that artifact, and how to deploy
    the model outside of SageMaker. For now, let's deploy it to a real-time endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is my favorite part of SageMaker; we only need one line of code to deploy
    a model to an **HTTPS endpoint**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s good practice to create identifiable and unique endpoint names. We could
    also let SageMaker create one for us during deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the endpoint name is `linear-learner-demo-29-08-37-25`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We deploy the model using the `deploy()` API. As this is a test endpoint, we
    use the smallest endpoint instance available, `ml.t2.medium`. In the eu-west-1
    region, this will only cost us $0.07 per hour:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When the endpoint is created, we can see it in the **SageMaker components and
    registries** | **Endpoints** panel in SageMaker Studio.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A few minutes later, the endpoint is in service. We can use the `predict()`
    API to send it a CSV sample for prediction. We set serialization using built-in
    functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The prediction output tells us that this house should cost $30,173:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also predict multiple samples at a time:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now the prediction output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we're done working with the endpoint, **we shouldn't forget to delete it
    to avoid unnecessary charges**.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deleting an endpoint is as simple as calling the `delete_endpoint()` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'At the risk of repeating myself, the topics covered in this section are extremely
    important, so please make sure you''re completely familiar with them, as we''ll
    constantly use them in the rest of the book. Please spend some time reading the
    service and SDK documentation as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://sagemaker.readthedocs.io](https://sagemaker.readthedocs.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's explore other built-in algorithms. You'll see that the workflow and
    the code are very similar!
  prefs: []
  type: TYPE_NORMAL
- en: Working with more built-in algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the rest of this chapter, we will run more examples with built-in algorithms,
    both in supervised and unsupervised mode. This will help you become very familiar
    with the SageMaker SDK and learn how to solve actual machine learning problems.
    The following list shows some of these algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification with XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation with Factorization Machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction with PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection with Random Cut Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression with XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s train a model on the Boston Housing dataset with the **XGBoost** algorithm
    ([https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)). As we will
    see in [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending
    Machine Learning Services Using Built-In Frameworks* , SageMaker also supports
    XGBoost scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: We reuse the dataset preparation steps from the previous examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We find the name of the XGBoost container. As several versions are supported,
    we select the latest one (1.3.1 at the time of writing):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We configure the `Estimator` function. The code is strictly identical to the
    code used with `LinearLearner`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Taking a look at the hyperparameters ([https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)),
    we see that the only required one is `num_round`. As it''s not obvious which value
    to set, we''ll go for a large value, and we''ll also define the `early_stopping_rounds`
    parameter in order to avoid overfitting. Of course, we need to set the objective
    for a regression problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the training input, just like in the previous example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then launch the training job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The job only ran for 22 rounds, meaning that **early stopping** was triggered.
    Looking at the training log, we see that round #12 was actually the best one,
    with a **root mean square error** (**RMSE**) of 2.43126:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Deploying still takes one line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the model is deployed, we use the `predict()` API again to send it a CSV
    sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result tells us that this house should cost $23,754:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we delete the endpoint when we''re done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the SageMaker workflow is pretty simple and makes it easy to
    experiment quickly with different algorithms without having to rewrite all your
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the Factorization Machines algorithm. In the process, we will
    learn about the highly efficient RecordIO-wrapped protobuf format.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation with Factorization Machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Factorization Machines is a generalization of linear models ([https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)).
    It's well-suited for high-dimension sparse datasets, such as user-item interaction
    matrices for recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we're going to train a recommendation model based on the **MovieLens**
    dataset ([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset exists in several versions. To minimize training times, we'll use
    the 100k version. It contains 100,000 ratings (integer values from 1 to 5) assigned
    by 943 users to 1,682 movies. The dataset is already split for training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: As you know by now, training and deploying with SageMaker is very simple. Most
    of the code will be identical to the two previous examples, which is great! This
    lets us focus on understanding and preparing data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding sparse datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine building a matrix to store this dataset. It would have 943 lines (one
    per user) and 1,682 columns (one per movie). Cells would store the ratings. The
    following diagram shows a basic example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Sparse matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_04_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Sparse matrix
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the matrix would have 943*1,682=1,586,126 cells. However, as only 100,000
    ratings are present, 93.69% of cells would be empty. Storing our dataset this
    way would be extremely inefficient. It would needlessly consume RAM, storage,
    and network bandwidth to store and transfer lots of zero values!
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, things are much worse, as the algorithm expects the input dataset
    to look like in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Sparse matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_04_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – Sparse matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Why do we need to store data this way? The answer is simple: Factorization
    Machines is a **supervised learning** algorithm, so we need to train it on labeled
    samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding diagram, we see that each line represents a movie review.
    The matrix on the left stores its one-hot encoded features (users and movies),
    and the vector on the right stores its label. For instance, the last line tells
    us that user 4 has given movie 5 a "5" rating.
  prefs: []
  type: TYPE_NORMAL
- en: The size of this matrix is 100,000 lines by 2,625 columns (943 movies plus 1,682
    movies). The total number of cells is 262,500,000, which are only 0.076% full
    (200,000 / 262,500,000). If we used a 32-bit value for each cell, we would need
    almost a gigabyte of memory to store this matrix. This is horribly inefficient
    but still manageable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just for fun, let''s do the same exercise for the largest version of MovieLens,
    which has 25 million ratings, 62,000 movies, and 162,000 users. The matrix would
    have 25 million lines and 224,000 columns, for a total of 5,600,000,000,000 cells.
    Yes, that''s 5.6 trillion cells, and although they would be 99.999% empty, we
    would still need over 20 terabytes of RAM to store them. Ouch. If that''s not
    bad enough, consider recommendation models with millions of users and products:
    the numbers are mind-boggling!'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a plain matrix, we'll use a `SciPy` has exactly the object
    we need, named `lil_matrix` ([https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html)).
    This will help us to get rid of all these nasty zeros.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding protobuf and RecordIO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So how will we pass this sparse matrix to the SageMaker algorithm? As you would
    expect, we're going to serialize the object and store it in S3\. We're not going
    to use Python serialization, however. Instead, we're going to use `protobuf` ([https://developers.google.com/protocol-buffers/](https://developers.google.com/protocol-buffers/)),
    a popular and efficient serialization mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we''re going to store the protobuf-encoded data in a record format
    called **RecordIO** ([https://mxnet.apache.org/api/faq/recordio/](https://mxnet.apache.org/api/faq/recordio/)).
    Our dataset will be stored as a sequence of records in a single file. This has
    the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A single file is easier to move around: who wants to deal with thousands of
    individual files that can get lost or corrupted?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sequential file is faster to read, which makes the training process more efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sequence of records is easy to split for distributed training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't worry if you're not familiar with protobuf and RecordIO. The SageMaker
    SDK includes utility functions that hide their complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Factorization Machines model on MovieLens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will begin building the model using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Jupyter notebook, we first download and extract the MovieLens dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the dataset is ordered by user ID, we shuffle it as a precaution. Then,
    we take a look at the first few lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see four columns: the user ID, the movie ID, the rating, and a timestamp
    (which we''ll ignore in our model):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define sizing constants:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s write a function to load a dataset into a sparse matrix. Based
    on the previous explanation, we go through the dataset line by line. In the X
    matrix, we set the appropriate user and movie columns to `1`. We also store the
    rating in the Y vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then process the training and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We check that the shapes are what we expect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This displays the dataset shapes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s write a function that converts a dataset to RecordIO-wrapped `protobuf`,
    and uploads it to an S3 bucket. We first create an in-memory binary stream with
    `io.BytesIO()`. Then, we use the lifesaving `write_spmatrix_to_sparse_tensor()`
    function to write the sample matrix and the label vector to that buffer in `protobuf`
    format. Finally, we use `boto3` to upload the buffer to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Had our data been stored in a `numpy` array instead of `lilmatrix`, we would
    have used the `write_numpy_to_dense_tensor()` function instead. It has the same
    effect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We apply this function to both datasets, and we store their S3 paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Taking a look at the S3 bucket in a terminal, we see that the training dataset
    only takes 5.5 MB. The combination of sparse matrix, protobuf, and RecordIO has
    paid off:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'What comes next is SageMaker business as usual. We find the name of the Factorization
    Machines container, configure the `Estimator` function, and set the hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Looking at the documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html)),
    we see that the required hyperparameters are `feature_dim`, `predictor_type`,
    and `num_factors`. The default setting for `epochs` is `1`, which feels a little
    low, so we use `10` instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then launch the training job. Did you notice that we didn''t configure training
    inputs? We''re simply passing the location of the two `protobuf` files. As `protobuf`
    is the default format for Factorization Machines (as well as other built-in algorithms),
    we can save a step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the job is over, we deploy the model to a real-time endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll now send samples to the endpoint in JSON format (https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html#fm-inputoutput).
    For this purpose, we write a custom serializer to convert input data to JSON.
    The default JSON deserializer will be used automatically since we set the content
    type to `''application/json''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We send the first three samples of the test set for prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The prediction looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using this model, we could fill all the empty cells in the recommendation matrix.
    For each user, we would simply predict the score of all movies, and store, say,
    the top 50 movies. That information would be stored in a backend, and the corresponding
    metadata (title, genre, and so on) would be displayed to the user in a frontend
    application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we delete the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So far, we've only used supervised learning algorithms. In the next section,
    we'll move on to unsupervised learning with Principal Component Analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Using Principal Component Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`protobuf` dataset built in the Factorization Machines example. Its 2,625 columns
    are a good candidate for dimensionality reduction! We will use PCA by taking the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from the processed dataset, we configure `Estimator` for PCA. By now,
    you should (almost) be able to do this with your eyes closed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then set the hyperparameters. The required ones are the initial number of
    features, the number of principal components to compute, and the batch size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train and deploy the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we predict the first test sample, using the same serialization code as
    in the previous example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the 64 principal components of the test sample. In real life,
    we typically would process the dataset with this model, save the results, and
    use them to train a regression model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Don't forget to delete the endpoint when you're done. Then, let's run one more
    unsupervised learning example to conclude this chapter!
  prefs: []
  type: TYPE_NORMAL
- en: Detecting anomalies with Random Cut Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Random Cut Forest** (**RCF**) is an unsupervised learning algorithm for anomaly
    detection ([https://proceedings.mlr.press/v48/guha16.pdf](https://proceedings.mlr.press/v48/guha16.pdf)).
    We''re going to apply it to a subset of the household electric power consumption
    dataset ([https://archive.ics.uci.edu/ml/](https://archive.ics.uci.edu/ml/)),
    available in the GitHub repository for this book. Data is aggregated hourly over
    a period of a little less than a year (just under 8,000 values):'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Jupyter notebook, we load the dataset with `pandas`, and we display the
    first few lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As shown in the following screenshot, the dataset has three columns – an hourly
    timestamp, the power consumption value (in kilowatt-hours), and the client ID:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Viewing the columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_04_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.4 – Viewing the columns
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using `matplotlib`, we plot the dataset to get a quick idea of what it looks
    like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot is shown in the following diagram. We see three time series corresponding
    to three different clients:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Viewing the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_04_5.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.5 – Viewing the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are two issues with this dataset. First, it contains several time series:
    RCF can only train a model on a single series. Second, RCF requires `pandas` –
    we only keep the `"client_12"` time series, we multiply its values by 100, and
    cast them to the integer type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following diagram shows the first lines of the transformed dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.6 – The values of the first lines'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_04_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.6 – The values of the first lines
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We plot it again to check that it looks as expected. Note the large drop right
    after step 2000, highlighted by a box in the following screenshot. This is clearly
    an anomaly, and hopefully, our model will catch it:![Figure 4.7 – Viewing a single
    time series
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_04_7.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.7 – Viewing a single time series
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As in the previous examples, we save the dataset to a CSV file, which we upload
    to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we define the `label_size=1`). Even though the training channel never
    has labels, we still need to tell RCF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Second, the only `ShardedByS3Key`. This policy splits the dataset across the
    different instances in the training cluster, instead of sending them a full copy.
    We won''t run distributed training here, but we need to set that policy nonetheless:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The rest is business as usual: train and deploy! Once again, we reuse the code
    for the previous examples, and it''s almost unchanged:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a few minutes, the model is deployed. We convert the input time series
    to a Python list, and we send it to the endpoint for prediction. We use CSV and
    JSON, respectively, for serialization and deserialization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response contains the anomaly score for each value in the time series.
    It looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then convert this response to a Python list, and we then compute its mean
    and its standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We plot a subset of the time series and the corresponding scores. Let''s focus
    on the "[2000:2500]" interval, as this is where we saw a large drop. We also plot
    a line representing the mean plus three standard deviations (99.7% of the score
    distribution) – any score largely exceeding the line is likely to be an anomaly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The drop is clearly visible in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Zooming in on an anomaly'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_04_8.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.8 – Zooming in on an anomaly
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you can see on the following score plot, its score is sky high! Beyond a
    doubt, this value is an anomaly:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Viewing anomaly scores'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_04_9.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.9 – Viewing anomaly scores
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Exploring other intervals of the time series, we could certainly find more.
    Who said machine learning wasn't fun?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we delete the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Having gone through five complete examples, you should now be familiar with
    built-in algorithms, the SageMaker workflow, and the SDK. To fully master these
    topics, I would recommend experimenting with your datasets and running additional
    examples available at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, built-in algorithms are a great way to quickly train and deploy
    models without having to write any machine learning code.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned about the SageMaker workflow, and how to implement
    it with a handful of APIs from the SageMaker SDK, without ever worrying about
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'You learned how to work with data in CSV and RecordIO-wrapped protobuf format,
    the latter being the preferred format for large-scale training on bulky datasets.
    You also learned how to build models with important algorithms for supervised
    and unsupervised learning: Linear Learner, XGBoost, Factorization Machines, PCA,
    and Random Cut Forest.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to use additional built-in algorithms
    to build computer vision models.
  prefs: []
  type: TYPE_NORMAL

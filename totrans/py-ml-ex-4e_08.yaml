- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discovering Underlying Topics in the Newsgroups Dataset with Clustering and
    Topic Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we went through a text visualization using t-SNE. t-SNE,
    or any dimensionality reduction algorithm, is a type of unsupervised learning.
    In this chapter, we will be continuing our unsupervised learning journey, specifically
    focusing on clustering and topic modeling. We will start with how unsupervised
    learning learns without guidance and how it is good at discovering hidden information
    underneath data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will talk about clustering as an important branch of unsupervised learning,
    which identifies different groups of observations from data. For instance, clustering
    is useful for market segmentation, where consumers of similar behaviors are grouped
    into one segment for marketing purposes. We will perform clustering on the 20
    newsgroups text dataset and see what clusters will be produced.
  prefs: []
  type: TYPE_NORMAL
- en: Another unsupervised learning route we will take is topic modeling, which is
    the process of extracting themes hidden in the dataset. You will be amused by
    how many interesting themes we are able to mine from the 20 newsgroups dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Leaning without guidance – unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering newsgroups data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering underlying topics in newsgroups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning without guidance – unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we applied t-SNE to visualize the newsgroup text data,
    reduced to two dimensions. t-SNE, or dimensionality reduction in general, is a
    type of **unsupervised learning**. Instead of being guided by predefined labels
    or categories, such as a class or membership (classification), and a continuous
    value (regression), unsupervised learning identifies inherent structures or commonalities
    in the input data. Since there is no guidance in unsupervised learning, there
    is no clear answer on what is a right or wrong result. Unsupervised learning has
    the freedom to discover hidden information underneath input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to understand unsupervised learning is to think of going through
    many practice questions for an exam. In supervised learning, you are given answers
    to those practice questions. You basically figure out the relationship between
    the questions and answers and learn how to map the questions to the answers. Hopefully,
    you will do well in the actual exam in the end by giving the correct answers.
    However, in unsupervised learning, you are not provided with the answers to those
    practice questions. What you might do in this instance could include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping similar practice questions so that you can later study related questions
    together at one time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding questions that are highly repetitive so that you don’t have to waste
    time working out the answer for each one individually
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spotting rare questions so that you can be better prepared for them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the key chunk of each question by removing boilerplate text so you
    can cut to the point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will notice that the outcomes of all these tasks are pretty open-ended.
    They are correct as long as they are able to describe the commonality and the
    structure underneath the data.
  prefs: []
  type: TYPE_NORMAL
- en: Practice questions are the **features** in machine learning, which are also
    often called **attributes**, **observations**, or **predictive variables**. Answers
    to questions are the labels in machine learning, which are also called **targets**
    or **target variables**. Practice questions with answers provided are called **labeled
    data**, while practice questions without answers are called **unlabeled data**.
    Unsupervised learning works with unlabeled data and acts on that information without
    guidance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised learning can include the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering**: This means grouping data based on commonality, which is often
    used for exploratory data analysis. Grouping similar practice questions, as mentioned
    earlier, is an example of clustering. Clustering techniques are widely used in
    customer segmentation or for grouping similar online behaviors for a marketing
    campaign. We will learn more about the popular algorithm k-means clustering in
    this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Association**: This explores the co-occurrence of particular values of two
    or more features. Outlier detection (also called anomaly detection) is a typical
    case, where rare observations are identified. Spotting rare questions in the preceding
    example can be achieved using outlier detection techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Projection**: This maps the original feature space to a reduced dimensional
    space retaining or extracting a set of principal variables. Extracting the key
    chunk of practice questions is an example projection or, specifically, a dimensionality
    reduction. The t-SNE we learned about previously is a good example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning is extensively employed in the area of NLP mainly because
    of the difficulty of obtaining labeled text data. Unlike numerical data (such
    as house prices, stock data, and online click streams), labeling text can sometimes
    be subjective, manual, and tedious. Unsupervised learning algorithms that do not
    require labels become effective when it comes to mining text data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 7*, *Mining the 20 Newsgroups Dataset with Text Analysis Techniques*,
    you experienced using t-SNE to reduce the dimensionality of text data. Now, let’s
    explore text mining with clustering algorithms and topic modeling techniques.
    We will start with clustering the newsgroups data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The newsgroups data comes with labels, which are the categories of the newsgroups,
    and a number of categories that are closely related or even overlapping, for instance,
    the five computer groups: `comp.graphics`, `comp.os.ms-windows.misc`, `comp.sys.ibm.pc.hardware`,
    `comp.sys.mac.hardware`, and `comp.windows.x`, and the two religion-related ones:
    `alt.atheism` and `talk.religion.misc`.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now pretend we don’t know those labels or they don’t exist. Will samples
    from related topics be clustered together? We will now resort to the k-means clustering
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How does k-means clustering work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of the k-means algorithm is to partition the data into k groups based
    on feature similarities. *k* is a predefined property of a *k*-means clustering
    model. Each of the *k* clusters is specified by a centroid (center of a cluster)
    and each data sample belongs to the cluster with the nearest centroid. During
    training, the algorithm iteratively updates the *k* centroids based on the data
    provided. Specifically, it involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specifying k**: The algorithm needs to know how many clusters to generate
    as an end result.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Initializing centroids**: The algorithm starts with randomly selecting k
    samples from the dataset as centroids.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assigning clusters**: Now that we have *k* centroids, samples that share
    the same closest centroid constitute one cluster. *k* clusters are created as
    a result. Note that closeness is usually measured by the **Euclidean distance**.
    Other metrics can also be used, such as the **Manhattan distance** and **Chebyshev
    distance**, which are listed in the following table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B21047_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Distance metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '**Updating centroids**: For each cluster, we need to recalculate its center
    point, which is the mean of all the samples in the cluster. *k* centroids are
    updated to be the means of corresponding clusters. This is why the algorithm is
    called **k-means**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Repeating steps 3 and 4**: We keep repeating assigning clusters and updating
    centroids until the model converges when no or a small enough update of centroids
    can be done, or enough iterations have been completed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The outputs of a trained k-means clustering model include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The cluster ID of each training sample, ranging from 1 to *k*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* centroids, which can be used to cluster new samples—a new sample will belong
    to the cluster of the closest centroid'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easy to understand the k-means clustering algorithm and its implementation
    is also straightforward, as you will discover next.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing k-means from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the `iris` dataset from scikit-learn as an example. Let’s first
    load the data and visualize it. We herein only use two features out of the original
    four for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the dataset contains three iris classes, we plot it in three different
    colors, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output for the original data plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A chart of different colored dots  Description automatically generated](img/B21047_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Plot of the original iris dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we know nothing about the label *y*, we try to cluster the data into
    three groups, as there seem to be three clusters in the preceding plot (or you
    might say two, which we will come back to later). Let’s perform *step 1*, *specifying
    k*, and *step 2*, *initializing centroids*, by randomly selecting three samples
    as the initial centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We visualize the data (without labels anymore) along with the initial random
    centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the data, along with the initial random
    centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a line of dots  Description automatically generated](img/B21047_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Data points with random centroids'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we perform *step 3*, which entails assigning clusters based on the nearest
    centroids. First, we need to define a function calculating distance, which is
    measured by the Euclidean distance, as demonstrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we develop a function that assigns a sample to the cluster of the nearest
    centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With the clusters assigned, we perform *step 4*, which involves updating the
    centroids to the mean of all samples in the individual clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have *step 5*, which involves repeating *step 3* and *step 4* until
    the model converges and whichever of the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: Centroids move less than the pre-specified threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sufficient iterations have been taken
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We set the tolerance of the first condition and the maximum number of iterations
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the clusters’ starting values, along with the starting clusters
    for all samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With all the components ready, we can train the model iteration by iteration
    where it first checks convergence before performing *steps 3* and *4*, and then
    visualizes the latest centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the following outputs generated from the preceding commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 1**: Take a look at the following output of iteration 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot of centroids after iteration 1 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a map  Description automatically generated](img/B21047_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: k-means clustering result after the first round'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 2**: Take a look at the following output of iteration 2:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot of centroids after iteration 2 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a map  Description automatically generated](img/B21047_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: k-means clustering result after the second round'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 6**: Take a look at the following output of iteration 6 (we herein
    skip iterations 3 to 5 to avoid tedium):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot of centroids after iteration 6 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a map  Description automatically generated](img/B21047_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: k-means clustering result after the sixth round'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 7**: Take a look at the following output of iteration 7:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot of centroids after iteration 7 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a map  Description automatically generated](img/B21047_08_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: k-means clustering result after the seventh round'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model converges after seven iterations. The resulting centroids look promising,
    and we can also plot the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the end result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of different colored dots  Description automatically generated](img/B21047_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Data samples along with learned cluster centroids'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, samples around the same centroid form a cluster. After seven
    iterations (you might see slightly more or fewer iterations in your case if you
    change the random seed in `np.random.seed(0)`), the model converges and the centroids
    will no longer be updated.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing k-means with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having developed our own k-means clustering model, we will now discuss how
    to use scikit-learn for a quicker solution by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the `KMeans` class and initialize a model with three clusters,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `KMeans` class takes in the following important parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Constructor parameter** | **Default value** | **Example values** | **Description**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `n_clusters` | `8` | `3, 5, 10` | *k* clusters |'
  prefs: []
  type: TYPE_TB
- en: '| `max_iter` | `300` | `10, 100, 500` | Maximum number of iterations |'
  prefs: []
  type: TYPE_TB
- en: '| `tol` | `1e-4` | `1e-5, 1e-8` | Tolerance to declare convergence |'
  prefs: []
  type: TYPE_TB
- en: '| `random_state` | `None` | `0, 42` | Random seed for program reproducibility
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.1: Parameters of the KMeans class'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then fit the model on the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we can obtain the clustering results, including the clusters for
    data samples and centroids of individual clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we plot the clusters along with the centroids:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of different colored dots  Description automatically generated](img/B21047_08_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Data samples along with learned cluster centroids using scikit-learn'
  prefs: []
  type: TYPE_NORMAL
- en: We get a similar result to the previous one using the model we implemented from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the value of k
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s return to our earlier discussion on what the right value for *k* is. In
    the preceding example, it is more intuitive to set it to `3` since we know there
    are three classes in total. However, in most cases, we don’t know how many groups
    are sufficient or efficient, and meanwhile, the algorithm needs a specific value
    of *k* to start with. So, how can we choose the value of *k*? There is a famous
    heuristic approach called the **elbow method**.
  prefs: []
  type: TYPE_NORMAL
- en: In the elbow method, different values of *k* are chosen and corresponding models
    are trained; for each trained model, the **sum of squared errors**, or **SSE**
    (also called the **sum of within-cluster distances**), of centroids is calculated
    and is plotted against *k*. Note that for one cluster, the squared error (or the
    within-cluster distance) is computed as the sum of the squared distances from
    individual samples in the cluster to the centroid. The optimal *k* is chosen where
    the marginal drop of SSE starts to decrease dramatically. This means that further
    clustering does not provide any substantial gain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply the elbow method to the example we covered in the previous section
    (learning by example is what this book is all about). We perform k-means clustering
    under different values of *k* on the `iris` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the whole feature space and `k` ranges from `1` to `6`. Then, we train
    individual models and record the resulting SSE, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the SSE versus the various `k` ranges, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, line, rectangle, plot  Description automatically
    generated](img/B21047_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: k-means elbow – SSE versus k'
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the right similarity measure for distance calculation in k-means clustering
    depends on the nature of your data and the specific goals of your analysis. Some
    common similarity measures include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean distance**: This default measure is suitable for continuous data
    where the difference between feature values matters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manhattan distance (also known as L1 norm)**: This calculates the sum of
    the absolute differences between the coordinates of two points. It is suitable
    for high-dimensional data and when the dimensions are not directly comparable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cosine similarity**: This is useful for text data or data represented as
    vectors where the magnitude of the vectors is less important than the orientation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaccard similarity**: This measures the similarity between two sets by comparing
    their intersection to their union. It is commonly used for binary or categorical
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apparently, the elbow point is `k=3`, since the drop in SSE slows down dramatically
    right after `3`. Hence, `k=3` is an optimal solution in this case, which is consistent
    with the fact that there are three classes of flowers.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering newsgroups dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should now be very familiar with k-means clustering. Next, let’s see what
    we are able to mine from the newsgroups dataset using this algorithm. We will
    use all the data from four categories, `'alt.atheism'`, `'talk.religion.misc'`,
    `'comp.graphics'`, and `'sci.space'`, as an example. We will then use ChatGPT
    to describe the generated newsgroup clusters. ChatGPT can generate natural language
    descriptions of the clusters formed by k-means clustering. This can help in understanding
    the characteristics and themes of each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering newsgroups data using k-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first load the data from those newsgroups and preprocess it as we did in
    *Chapter 7*, *Mining the 20 Newsgroups Dataset with Text Analysis Techniques*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We then convert the cleaned text data into count vectors using `CountVectorizer`
    from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that the vectorizer we use here does not limit the number of features (word
    tokens), but the minimum and maximum document frequency (`min_df` and `max_df`),
    which are 2% and 50% of the dataset, respectively. The **document frequency**
    of a word is measured by the fraction of documents (samples) in the dataset that
    contain this word. This helps filter out rare or spurious terms that may not be
    relevant to the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the input data ready, we will now try to cluster them into four groups
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s do a quick check on the sizes of the resulting clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The clusters don’t look absolutely correct, with most samples (`3360` samples)
    congested in one big cluster (cluster 3). What could have gone wrong? It turns
    out that our count-based features are not sufficiently representative. A better
    numerical representation for text data is the **term frequency-inverse document
    frequency** (**tf-idf**). Instead of simply using the token count, or the so-called
    **term frequency** (**tf**), it assigns each term frequency a weighting factor
    that is inversely proportional to the document frequency. In practice, the **idf**
    factor of a term *t* in documents *D* is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_08_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n*[D] is the total number of documents, *n*[t] is the number of documents
    containing the term *t*, and *1* is added to avoid division by 0.
  prefs: []
  type: TYPE_NORMAL
- en: With the `idf` factor incorporated, the `tf-idf` representation diminishes the
    weight of common terms (such as *get* and *make*) and emphasizes terms that rarely
    occur but convey an important meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the `tf-idf` representation, we just need to replace `CountVectorizer`
    with `TfidfVectorizer` from scikit-learn as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The parameter `max_df` is used to ignore terms that have a document frequency
    higher than the given threshold. In this case, terms that appear in more than
    50% of the documents will be ignored during the vectorization process. `min_df`
    specifies the minimum document frequency required for a term to be included in
    the output. Terms that appear in fewer than two documents will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, redo feature extraction using the `tf-idf` vectorizer and the k-means
    clustering algorithm on the resulting feature space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The clustering result becomes more reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also take a closer look at the clusters by examining what they contain and
    the top 10 terms (the terms with the 10 highest tf-idf scores) representing each
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'From what we observe in the preceding results:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cluster_0` is obviously about space and includes almost all `sci.space` samples
    and related terms such as `orbit`, `moon`, `nasa`, `launch`, `shuttle`, and `space`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster_1` is more of a generic topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster_2` is more about computer graphics and related terms, such as `computer`,
    `program`, `file`, `graphic`, and `image`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster_3` is an interesting one, which successfully brings together two overlapping
    topics, atheism and religion, with key terms including `bible`, `believe`, `jesus`,
    `christian`, and `god`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feel free to try different values of `k`, or use the elbow method to find the
    optimal one (this is actually an exercise later in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: It is quite interesting to find key terms for each text group via clustering.
    It will be more fun if we can describe each cluster based on its key terms. Let’s
    see how we do so with ChatGPT in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Describing the clusters using GPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ChatGPT** ([https://chat.openai.com/](https://chat.openai.com/)) is an AI
    language model developed by **OpenAI** ([https://openai.com/](https://openai.com/)).
    It is part of the **Generative Pre-trained Transformer** (**GPT**) family of models,
    specifically based on GPT-3.5 (GPT-4 is in beta at the time of writing) architecture.
    ChatGPT is designed to engage in natural language conversations with users and
    provide human-like responses.'
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained on a vast amount of diverse text data from the internet,
    allowing it to understand and generate human-like text across a wide range of
    topics and contexts. ChatGPT can comprehend questions, prompts, and instructions
    given by users and generate coherent responses based on its training.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT has been used in various applications, including chatbots, virtual assistants,
    content generation, language translation, and more. Users interact with ChatGPT
    through API calls or interactive interfaces, and the model generates responses
    in real time. However, it is essential to note that while ChatGPT can produce
    impressive and contextually relevant responses, it may also occasionally generate
    incorrect or nonsensical answers due to the limitations of current language models.
    ChatGPT responses should be sense-checked to improve the quality and reliability
    of the generated text and minimize the risk of misinformation.
  prefs: []
  type: TYPE_NORMAL
- en: We will ask ChatGPT to describe the clusters we just generated in the following
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we obtain the top 100 terms as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After signing up (or logging in if you have an account) at [https://chat.openai.com](https://chat.openai.com),
    we ask ChatGPT to describe the topic based on these keywords using the prompt
    `Describe a common topic based on the following keywords:`. Refer to the following
    screenshot for the entire question and answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21047_08_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Asking ChatGPT to describe the topic of cluster 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'As ChatGPT pointed out correctly, `"the common topic revolves around the various
    aspects of space exploration, research, technology, and missions, with mentions
    of key players and celestial bodies in the field."` Feel free to repeat the same
    process for other clusters. You can also achieve the same using the ChatGPT API
    in Python by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the OpenAI library with `pip`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also do this with `conda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Generate an API key at [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
    Note that you will need to log in or sign up to do this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the library and set your API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function that allows you to obtain a response from ChatGPT:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `text-davinci-003` model. Check out the page at [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)
    for more information on the various models available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Query the API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will yield a response similar to what you previously read in the web interface.
    Note that the API call is subject to your plan quota.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, we have produced topical keywords by first grouping documents
    into clusters and subsequently extracting the top terms within each cluster. **Topic
    modeling** is another approach to produce topical keywords but in a much more
    direct way. It does not simply search for the key terms in individual clusters
    generated beforehand. What it does is directly extract collections of key terms
    from documents. You will see how this works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Density-Based Spatial Clustering of Applications with Noise** (**DBSCAN**)
    is another popular clustering algorithm used for identifying clusters in spatial
    data. Unlike centroid-based algorithms like k-means, DBSCAN does not require specifying
    the number of clusters in advance and can discover clusters of arbitrary shapes.
    It works by partitioning the dataset into clusters of contiguous high-density
    regions, separated by regions of low density, while also identifying outliers
    as noise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm requires two parameters: epsilon (![](img/B21047_08_002.png)),
    which defines the maximum distance between two samples for them to be considered
    as part of the same neighborhood, and `min_samples`, which specifies the minimum
    number of samples required to form a dense region.'
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN starts by randomly selecting a point and expanding its neighborhood to
    find all reachable points within ε distance. If the number of reachable points
    exceeds `min_samples`, the point is labeled as a core point and a new cluster
    is formed. The process is repeated recursively for all core points and their neighborhoods
    until all points are assigned to a cluster or labeled as noise.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering underlying topics in newsgroups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **topic model** is a type of statistical model for discovering the probability
    distributions of words linked to the topic. The topic in topic modeling does not
    exactly match the dictionary definition but corresponds to a nebulous statistical
    concept, which is an abstraction that occurs in a collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we read a document, we expect certain words appearing in the title or
    the body of the text to capture the semantic context of the document. An article
    about Python programming might have words such as *class* and *function*, while
    a story about snakes might have words such as *eggs* and *afraid*. Documents usually
    have multiple topics; for instance, this section is about three things: topic
    modeling, non-negative matrix factorization, and latent Dirichlet allocation,
    which we will discuss shortly. We can therefore define an additive model for topics
    by assigning different weights to topics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic modeling** is widely used for mining hidden semantic structures in
    given text data. There are two popular topic modeling algorithms—**non-negative
    matrix factorization** (**NMF**) and **latent Dirichlet allocation** (**LDA**).
    We will go through both of these in the next two sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling using NMF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Non-negative matrix factorization** (**NMF**) is a dimensionality reduction
    technique used for feature extraction and data representation. It factorizes a
    non-negative input matrix, **V**, into a product of two smaller matrices, **W**
    and **H**, in such a way that these three matrices have no negative values. These
    two lower-dimensional matrices represent features and their associated coefficients.
    In the context of NLP, these three matrices have the following meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: The input matrix **V** is the term count or tf-idf matrix of size *n* * *m*,
    where *n* is the number of documents or samples, and *m* is the number of terms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first decomposition output matrix **W** is the feature matrix of size *t*
    * *m*, where *t* is the number of topics specified. Each row of **W** represents
    a topic with each element in the row representing the rank of a term in the topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second decomposition output matrix **H** is the coefficient matrix of size
    *n* * *t*. Each row of **H** represents a document, with each element in the row
    representing the weight of a topic within the document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to derive the computation of **W** and **H** is beyond the scope of this
    book. However, you can refer to the following example to get a better sense of
    how NMF works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, diagram, screenshot, line  Description automatically
    generated](img/B21047_08_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Example of matrix W and matrix H derived from an input matrix
    V'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in reading more about NMF, feel free to check out the
    original paper *Generalized Nonnegative Matrix Approximations with Bregman Divergences*,
    by Inderjit S. Dhillon and Suvrit Sra, in NIPS 2005.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now apply NMF to our newsgroups data. Scikit-learn has a nice module
    for decomposition that includes NMF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify 20 topics (`n_components`) as an example. Important parameters of
    the model are included in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Constructor parameter** | **Default value** | **Example values** | **Description**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `n_components` | `None` | `5`, `10`, `20` | Number of components—in the context
    of topic modeling, this corresponds to the number of topics. If `None`, it becomes
    the number of input features. |'
  prefs: []
  type: TYPE_TB
- en: '| `max_iter` | `200` | `100`, `200` | Maximum number of iterations. |'
  prefs: []
  type: TYPE_TB
- en: '| `tol` | `1e-4` | `1e-5`, `1e-8` | Tolerance to declare convergence. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.2: Parameters of the NMF class'
  prefs: []
  type: TYPE_NORMAL
- en: 'We used the term matrix as input to the NMF model, but you could also use the
    `tf-idf` one instead. Now, fit the NMF model, `nmf`, on the term matrix, `data_cv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can obtain the resulting topic feature rank **W** after the model is trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'For each topic, we display the top 10 terms based on their ranks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of interesting topics, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Computer graphics-related topics, such as `0`, `2`, `6`, and `8`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Space-related ones, such as `3`, `4`, and `9`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Religion-related ones, such as `5`, `7`, and `13`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some topics, such as `1` and `12`, are hard to interpret. This is totally fine
    since topic modeling is a kind of free-form learning.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling using LDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s explore another popular topic modeling algorithm, **Latent Dirichlet Allocation**
    (**LDA**). LDA is a generative probabilistic graphical model that explains each
    input document by means of a mixture of topics with certain probabilities. It
    assumes that each document is a mixture of multiple topics, and each topic is
    characterized by a specific word probability distribution. The algorithm iteratively
    assigns words in documents to topics and updates the topic distributions based
    on the observed word co-occurrences. Again, **topic** in topic modeling means
    a collection of words with a certain connection. In other words, LDA basically
    deals with two probability values, *P*(*term* V *topic*) and *P*(*topic* V *document*).
    This can be difficult to understand at the beginning. So, let’s start from the
    bottom, the end result of an LDA model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the following set of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s say we want two topics. The topics derived from these documents
    may appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we find how each document is represented by these two topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'After seeing a toy example, we come back to its learning procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the number of topics, *T*. Now we have topics 1, 2, …, and *T*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document, randomly assign one of the topics to each term in the document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document, calculate *P*(*topic* = *t* V *document*), which is the proportion
    of terms in the document that are assigned to the topic *t*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each topic, calculate *P*(*term* = *w* V *topic*), which is the proportion
    of term *w* among all terms that are assigned to the topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each term *w*, reassign its topic based on the latest probabilities *P*(*topic*
    = *t* V *document*) and *P*(*term* = *w* V *topic* = *t*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 3* to *5* under the latest topic distributions for each iteration.
    The training stops if the model converges or reaches the maximum number of iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LDA is trained in a generative manner, where it tries to abstract from the documents
    a set of hidden topics that are likely to generate a certain collection of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all this in mind, let’s see LDA in action. The LDA model is also included
    in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we specify 20 topics (`n_components`). The key parameters of the model
    are included in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Constructor parameter** | **Default value** | **Example values** | **Description**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `n_components` | `10` | `5, 10, 20` | Number of components—in the context
    of topic modeling, this corresponds to the number of topics. |'
  prefs: []
  type: TYPE_TB
- en: '| `learning_method` | `"batch"` | `"online", "batch"` | In `batch` mode, all
    training data is used for each update. In `online` mode, a mini-batch of training
    data is used for each update. In general, if the data size is large, the `online`
    mode is faster. |'
  prefs: []
  type: TYPE_TB
- en: '| `max_iter` | `10` | `10, 20` | Maximum number of iterations. |'
  prefs: []
  type: TYPE_TB
- en: '| `randome_state` | `None` | `0, 42` | Seed used by the random number generator.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.3: Parameters of the LatentDirichletAllocation class'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the input data to LDA, remember that LDA only takes in term counts as it
    is a probabilistic graphical model. This is unlike NMF, which can work with both
    the term count matrix and the tf-idf matrix as long as they are non-negative data.
    Again, we use the term matrix defined previously as input to the LDA model. Now,
    we fit the LDA model on the term matrix, `data_cv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can obtain the resulting topic term rank after the model is trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, for each topic, we display the top 10 terms based on their ranks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of interesting topics that we just mined, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Computer graphics-related topics, such as `2`, `5`, `6`, `8`, and `19`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Space-related ones, such as `10`, `11`, `12`, and `15`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Religion-related ones, such as `0` and `13`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also topics involving noise, for example, `9` and `16`, which may
    require some imagination to interpret. Once more, this observation is entirely
    expected, given that LDA or topic modeling, as mentioned before, falls under the
    category of free-form learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The project in this chapter was about finding hidden similarities underneath
    newsgroups data, be it semantic groups, themes, or word clouds. We started with
    what unsupervised learning does and the typical types of unsupervised learning
    algorithms. We then introduced unsupervised learning clustering and studied a
    popular clustering algorithm, k-means, in detail. We also explored using ChatGPT
    to describe the topics of individual clusters based on their keywords.
  prefs: []
  type: TYPE_NORMAL
- en: We also talked about tf-idf as a more efficient feature extraction tool for
    text data. After that, we performed k-means clustering on the newsgroups data
    and obtained four meaningful clusters. After examining the key terms in each resulting
    cluster, we went straight to extracting representative terms among original documents
    using topic modeling techniques. Two powerful topic modeling approaches, NMF and
    LDA, were discussed and implemented. Finally, we had some fun interpreting the
    topics we obtained from both methods.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have covered all the main categories of unsupervised learning, including
    dimensionality reduction, clustering, and topic modeling, which is also dimensionality
    reduction in a way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will talk about **Support Vector Machines** (**SVMs**)
    for face recognition. SVM is a popular choice for a wide range of classification
    and regression tasks, especially when dealing with complex decision boundaries.
    We will also cover another dimensionality reduction technique called principal
    component analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ask ChatGPT to describe other clusters we generated through k-means clustering.
    You may experiment with various prompts and discover intriguing information within
    these clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform k-means clustering on newsgroups data using different values of *k*,
    or use the elbow method to find the optimal one. See if you get better grouping
    results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try different numbers of topics in either NMF or LDA and see which one produces
    more meaningful topics in the end. This should be a fun exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you experiment with NMF or LDA on the entire 20 groups of newsgroups data?
    Are the resulting topics full of noise or gems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code187846872178698968.png)'
  prefs: []
  type: TYPE_IMG

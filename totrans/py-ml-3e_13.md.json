["```py\npip install tensorflow \n```", "```py\npip install tensorflow==[desired-version] \n```", "```py\n**pip install tensorflow-gpu** \n```", "```py\npython -c 'import tensorflow as tf; print(tf.__version__)' \n```", "```py\n>>> import tensorflow as tf\n>>> import numpy as np\n>>> np.set_printoptions(precision=3)\n>>> a = np.array([1, 2, 3], dtype=np.int32)\n>>> b = [4, 5, 6]\n>>> t_a = tf.convert_to_tensor(a)\n>>> t_b = tf.convert_to_tensor(b)\n>>> print(t_a)\n>>> print(t_b)\ntf.Tensor([1 2 3], shape=(3,), dtype=int32)\ntf.Tensor([4 5 6], shape=(3,), dtype=int32) \n```", "```py\n>>> t_ones = tf.ones((2, 3))\n>>> t_ones.shape\nTensorShape([2, 3]) \n```", "```py\n>>> t_ones.numpy()\narray([[1., 1., 1.],\n       [1., 1., 1.]], dtype=float32) \n```", "```py\n>>> const_tensor = tf.constant([1.2, 5, np.pi],\n...                            dtype=tf.float32)\n>>> print(const_tensor)\ntf.Tensor([1.2   5\\.    3.142], shape=(3,), dtype=float32) \n```", "```py\n>>> t_a_new = tf.cast(t_a, tf.int64)\n>>> print(t_a_new.dtype)\n<dtype: 'int64'> \n```", "```py\n    >>> t = tf.random.uniform(shape=(3, 5))\n    >>> t_tr = tf.transpose(t)\n    >>> print(t.shape, ' --> ', t_tr.shape)\n    (3, 5)  -->  (5, 3) \n    ```", "```py\n    >>> t = tf.zeros((30,))\n    >>> t_reshape = tf.reshape(t, shape=(5, 6))\n    >>> print(t_reshape.shape)\n    (5, 6) \n    ```", "```py\n    >>> t = tf.zeros((1, 2, 1, 4, 1))\n    >>> t_sqz = tf.squeeze(t, axis=(2, 4))\n    >>> print(t.shape, ' --> ', t_sqz.shape)\n    (1, 2, 1, 4, 1)  -->  (1, 2, 4) \n    ```", "```py\n>>> tf.random.set_seed(1)\n>>> t1 = tf.random.uniform(shape=(5, 2),\n...                        minval=-1.0, maxval=1.0)\n>>> t2 = tf.random.normal(shape=(5, 2),\n...                       mean=0.0, stddev=1.0) \n```", "```py\n>>> t3 = tf.multiply(t1, t2).numpy()\n>>> print(t3)\n[[-0.27  -0.874]\n [-0.017 -0.175]\n [-0.296 -0.139]\n [-0.727  0.135]\n [-0.401  0.004]] \n```", "```py\n>>> t4 = tf.math.reduce_mean(t1, axis=0)\n>>> print(t4)\ntf.Tensor([0.09  0.207], shape=(2,), dtype=float32) \n```", "```py\n>>> t5 = tf.linalg.matmul(t1, t2, transpose_b=True)\n>>> print(t5.numpy())\n[[-1.144  1.115 -0.87  -0.321  0.856]\n [ 0.248 -0.191  0.25  -0.064 -0.331]\n [-0.478  0.407 -0.436  0.022  0.527]\n [ 0.525 -0.234  0.741 -0.593 -1.194]\n [-0.099  0.26   0.125 -0.462 -0.396]] \n```", "```py\n>>> t6 = tf.linalg.matmul(t1, t2, transpose_a=True)\n>>> print(t6.numpy())\n[[-1.711  0.302]\n [ 0.371 -1.049]] \n```", "```py\n>>> norm_t1 = tf.norm(t1, ord=2, axis=1).numpy()\n>>> print(norm_t1)\n[1.046 0.293 0.504 0.96  0.383] \nthe  norm of t1 correctly, you can compare the results with the following NumPy function: np.sqrt(np.sum(np.square(t1), axis=1)).\n```", "```py\n    >>> tf.random.set_seed(1)\n    >>> t = tf.random.uniform((6,))\n    >>> print(t.numpy())\n    [0.165 0.901 0.631 0.435 0.292 0.643]\n    >>> t_splits = tf.split(t, num_or_size_splits=3)\n    >>> [item.numpy() for item in t_splits]\n    [array([0.165, 0.901], dtype=float32),\n     array([0.631, 0.435], dtype=float32),\n     array([0.292, 0.643], dtype=float32)] \n    ```", "```py\n    >>> tf.random.set_seed(1)\n    >>> t = tf.random.uniform((5,))\n    >>> print(t.numpy())\n    [0.165 0.901 0.631 0.435 0.292]\n    >>> t_splits = tf.split(t, num_or_size_splits=[3, 2])\n    >>> [item.numpy() for item in t_splits]\n    [array([0.165, 0.901, 0.631], dtype=float32),\n     array([0.435, 0.292], dtype=float32)] \n    ```", "```py\n>>> A = tf.ones((3,))\n>>> B = tf.zeros((2,))\n>>> C = tf.concat([A, B], axis=0)\n>>> print(C.numpy())\n[1\\. 1\\. 1\\. 0\\. 0.] \n```", "```py\n>>> A = tf.ones((3,))\n>>> B = tf.zeros((3,))\n>>> S = tf.stack([A, B], axis=1)\n>>> print(S.numpy())\n[[1\\. 0.]\n [1\\. 0.]\n [1\\. 0.]] \n```", "```py\n>>> a = [1.2, 3.4, 7.5, 4.1, 5.0, 1.0]\n>>> ds = tf.data.Dataset.from_tensor_slices(a)\n>>> print(ds)\n<TensorSliceDataset shapes: (), types: tf.float32> \n```", "```py\n>>> for item in ds:\n...     print(item)\ntf.Tensor(1.2, shape=(), dtype=float32)\ntf.Tensor(3.4, shape=(), dtype=float32)\ntf.Tensor(7.5, shape=(), dtype=float32)\ntf.Tensor(4.1, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32) \n```", "```py\n>>> ds_batch = ds.batch(3)\n>>> for i, elem in enumerate(ds_batch, 1):\n...    print('batch {}:'.format(i), elem.numpy())\nbatch 1: [1.2 3.4 7.5]\nbatch 2: [4.1 5\\.  1\\. ] \n```", "```py\n>>> tf.random.set_seed(1)\n>>> t_x = tf.random.uniform([4, 3], dtype=tf.float32)\n>>> t_y = tf.range(4) \n```", "```py\n>>> ds_x = tf.data.Dataset.from_tensor_slices(t_x)\n>>> ds_y = tf.data.Dataset.from_tensor_slices(t_y)\n>>>\n>>> ds_joint = tf.data.Dataset.zip((ds_x, ds_y))\n>>> for example in ds_joint:\n...     print('  x:', example[0].numpy(),\n...           '  y:', example[1].numpy())\n  x: [0.165 0.901 0.631]   y: 0\n  x: [0.435 0.292 0.643]   y: 1\n  x: [0.976 0.435 0.66 ]   y: 2\n  x: [0.605 0.637 0.614]   y: 3 \n```", "```py\n>>> ds_joint = tf.data.Dataset.from_tensor_slices((t_x, t_y))\n>>> for example in ds_joint:\n...     print('  x:', example[0].numpy(),\n...           '  y:', example[1].numpy())\n  x: [0.165 0.901 0.631]   y: 0\n  x: [0.435 0.292 0.643]   y: 1\n  x: [0.976 0.435 0.66 ]   y: 2\n  x: [0.605 0.637 0.614]   y: 3 \n```", "```py\n>>> ds_trans = ds_joint.map(lambda x, y: (x*2-1.0, y))\n>>> for example in ds_trans:\n...    print('  x:', example[0].numpy(),\n...          '  y:', example[1].numpy())\n  x: [-0.67   0.803  0.262]   y: 0\n  x: [-0.131 -0.416  0.285]   y: 1\n  x: [ 0.952 -0.13   0.32 ]   y: 2\n  x: [ 0.21   0.273  0.229]   y: 3 \n```", "```py\n>>> tf.random.set_seed(1)\n>>> ds = ds_joint.shuffle(buffer_size=len(t_x))\n>>> for example in ds:\n...     print('  x:', example[0].numpy(),\n...           '  y:', example[1].numpy())\n  x: [0.976 0.435 0.66 ]   y: 2\n  x: [0.435 0.292 0.643]   y: 1\n  x: [0.165 0.901 0.631]   y: 0\n  x: [0.605 0.637 0.614]   y: 3 \n```", "```py\n>>> ds = ds_joint.batch(batch_size=3,\n...                     drop_remainder=False)\n>>> batch_x, batch_y = next(iter(ds))\n>>> print('Batch-x:\\n', batch_x.numpy())\nBatch-x:\n[[0.165 0.901 0.631]\n [0.435 0.292 0.643]\n [0.976 0.435 0.66 ]]\n>>> print('Batch-y: ', batch_y.numpy())\nBatch-y: [0 1 2] \n```", "```py\n>>> ds = ds_joint.batch(3).repeat(count=2)\n>>> for i,(batch_x, batch_y) in enumerate(ds):\n...     print(i, batch_x.shape, batch_y.numpy())\n0 (3, 3) [0 1 2]\n1 (1, 3) [3]\n2 (3, 3) [0 1 2]\n3 (1, 3) [3] \n```", "```py\n>>> ds = ds_joint.repeat(count=2).batch(3)\n>>> for i,(batch_x, batch_y) in enumerate(ds):\n...     print(i, batch_x.shape, batch_y.numpy())\n0 (3, 3) [0 1 2]\n1 (3, 3) [3 0 1]\n2 (2, 3) [2 3] \n```", "```py\n## Order 1: shuffle -> batch -> repeat\n>>> tf.random.set_seed(1)\n>>> ds = ds_joint.shuffle(4).batch(2).repeat(3)\n>>> for i,(batch_x, batch_y) in enumerate(ds):\n...     print(i, batch_x.shape, batch_y.numpy())\n0 (2, 3) [2 1]\n1 (2, 3) [0 3]\n2 (2, 3) [0 3]\n3 (2, 3) [1 2]\n4 (2, 3) [3 0]\n5 (2, 3) [1 2] \n```", "```py\n## Order 2: batch -> shuffle -> repeat\n>>> tf.random.set_seed(1)\n>>> ds = ds_joint.batch(2).shuffle(4).repeat(3)\n>>> for i,(batch_x, batch_y) in enumerate(ds):\n...     print(i, batch_x.shape, batch_y.numpy())\n0 (2, 3) [0 1]\n1 (2, 3) [2 3]\n2 (2, 3) [0 1]\n3 (2, 3) [2 3]\n4 (2, 3) [2 3]\n5 (2, 3) [0 1] \n```", "```py\n>>> import pathlib\n>>> imgdir_path = pathlib.Path('cat_dog_images')\n>>> file_list = sorted([str(path) for path in\n...                     imgdir_path.glob('*.jpg')])\n['cat_dog_images/dog-03.jpg', 'cat_dog_images/cat-01.jpg', 'cat_dog_images/cat-02.jpg', 'cat_dog_images/cat-03.jpg', 'cat_dog_images/dog-01.jpg', 'cat_dog_images/dog-02.jpg'] \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> fig = plt.figure(figsize=(10, 5))\n>>> for i, file in enumerate(file_list):\n...     img_raw = tf.io.read_file(file)\n...     img = tf.image.decode_image(img_raw)\n...     print('Image shape: ', img.shape)\n...     ax = fig.add_subplot(2, 3, i+1)\n...     ax.set_xticks([]); ax.set_yticks([])\n...     ax.imshow(img)\n...     ax.set_title(os.path.basename(file), size=15)\n>>> plt.tight_layout()\n>>> plt.show()\nImage shape:  (900, 1200, 3)\nImage shape:  (900, 1200, 3)\nImage shape:  (900, 1200, 3)\nImage shape:  (900, 742, 3)\nImage shape:  (800, 1200, 3)\nImage shape:  (800, 1200, 3) \n```", "```py\n>>> labels = [1 if 'dog' in os.path.basename(file) else 0\n...           for file in file_list]\n>>> print(labels)\n[1, 0, 0, 0, 1, 1] \n```", "```py\n>>> ds_files_labels = tf.data.Dataset.from_tensor_slices(\n...                                   (file_list, labels))\n>>> for item in ds_files_labels:\n...     print(item[0].numpy(), item[1].numpy())\nb'cat_dog_images/dog-03.jpg' 1\nb'cat_dog_images/cat-01.jpg' 0\nb'cat_dog_images/cat-02.jpg' 0\nb'cat_dog_images/cat-03.jpg' 0\nb'cat_dog_images/dog-01.jpg' 1\nb'cat_dog_images/dog-02.jpg' 1 \n```", "```py\n>>> def load_and_preprocess(path, label):\n...     image = tf.io.read_file(path)\n...     image = tf.image.decode_jpeg(image, channels=3)\n...     image = tf.image.resize(image, [img_height, img_width])\n...     image /= 255.0\n...     return image, label\n>>> img_width, img_height = 120, 80\n>>> ds_images_labels = ds_files_labels.map(load_and_preprocess)\n>>>\n>>> fig = plt.figure(figsize=(10, 6))\n>>> for i,example in enumerate(ds_images_labels):\n...     ax = fig.add_subplot(2, 3, i+1)\n...     ax.set_xticks([]); ax.set_yticks([])\n...     ax.imshow(example[0])\n...     ax.set_title('{}'.format(example[1].numpy()),\n...                  size=15)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\npip install tensorflow-datasets \n```", "```py\n>>> import tensorflow_datasets as tfds\n>>> print(len(tfds.list_builders()))\n101\n>>> print(tfds.list_builders()[:5])\n['abstract_reasoning', 'aflw2k3d', 'amazon_us_reviews', 'bair_robot_pushing_small', 'bigearthnet'] \n```", "```py\n>>> celeba_bldr = tfds.builder('celeb_a')\n>>> print(celeba_bldr.info.features)\nFeaturesDict({'image': Image(shape=(218, 178, 3), dtype=tf.uint8), 'landmarks': FeaturesDict({'lefteye_x': Tensor(shape=(), dtype=tf.int64), 'lefteye_y': Tensor(shape=(), dtype=tf.int64), 'righteye_x': Tensor(shape=(), dtype=tf.int64), 'righteye_y': ...\n>>> print(celeba_bldr.info.features['image'])\nImage(shape=(218, 178, 3), dtype=tf.uint8)\n>>> print(celeba_bldr.info.features['attributes'].keys())\ndict_keys(['5_o_Clock_Shadow', 'Arched_Eyebrows', ...\n>>> print(celeba_bldr.info.citation)\n@inproceedings{conf/iccv/LiuLWT15,\n  added-at = {2018-10-09T00:00:00.000+0200},\n  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n  biburl = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},\n  booktitle = {ICCV},\n  crossref = {conf/iccv/2015},\n  ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},\n  interhash = {3f735aaa11957e73914bbe2ca9d5e702},\n  intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},\n  isbn = {978-1-4673-8391-2},\n  keywords = {dblp},\n  pages = {3730-3738},\n  publisher = {IEEE Computer Society},\n  timestamp = {2018-10-11T11:43:28.000+0200},\n  title = {Deep Learning Face Attributes in the Wild.},\n  url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},\n  year = 2015\n} \n```", "```py\n>>> celeba_bldr.download_and_prepare() \n```", "```py\n>>> datasets = celeba_bldr.as_dataset(shuffle_files=False)\n>>> datasets.keys()\ndict_keys(['test', 'train', 'validation']) \n```", "```py\n>>> ds_train = datasets['train']\n>>> assert isinstance(ds_train, tf.data.Dataset)\n>>> example = next(iter(ds_train))\n>>> print(type(example))\n<class 'dict'>\n>>> print(example.keys())\ndict_keys(['image', 'landmarks', 'attributes']) \n```", "```py\n>>> ds_train = ds_train.map(lambda item:\n...                (item['image'],\n...                tf.cast(item['attributes']['Male'], tf.int32))) \n```", "```py\n>>> ds_train = ds_train.batch(18)\n>>> images, labels = next(iter(ds_train))\n>>> print(images.shape, labels)\n(18, 218, 178, 3) tf.Tensor([0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1], shape=(18,), dtype=int32)\n>>> fig = plt.figure(figsize=(12, 8))\n>>> for i,(image,label) in enumerate(zip(images, labels)):\n...     ax = fig.add_subplot(3, 6, i+1)\n...     ax.set_xticks([]); ax.set_yticks([])\n...     ax.imshow(image)\n...     ax.set_title('{}'.format(label), size=15)\n>>> plt.show() \n```", "```py\n>>> mnist, mnist_info = tfds.load('mnist', with_info=True,\n...                               shuffle_files=False)\n>>> print(mnist_info)\ntfds.core.DatasetInfo(\n    name='mnist',\n    version=1.0.0,\n    description='The MNIST database of handwritten digits.',\n    urls=['https://storage.googleapis.com/cvdf-datasets/mnist/'],\n    features=FeaturesDict({\n        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10)\n    },\n    total_num_examples=70000,\n    splits={\n        'test': <tfds.core.SplitInfo num_examples=10000>,\n        'train': <tfds.core.SplitInfo num_examples=60000>\n    },\n    supervised_keys=('image', 'label'),\n    citation=\"\"\"\n        @article{lecun2010mnist,\n          title={MNIST handwritten digit database},\n          author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n          journal={ATT Labs [Online]. Availablist},\n          volume={2},\n          year={2010}\n        }\n\n    \"\"\",\n    redistribution_info=,\n)\n>>> print(mnist.keys())\ndict_keys(['test', 'train']) \n```", "```py\n>>> ds_train = mnist['train']\n>>> ds_train = ds_train.map(lambda item:\n...                         (item['image'], item['label']))\n>>> ds_train = ds_train.batch(10)\n>>> batch = next(iter(ds_train))\n>>> print(batch[0].shape, batch[1])\n(10, 28, 28, 1) tf.Tensor([8 4 7 7 0 9 0 3 3 3], shape=(10,), dtype=int64)\n>>> fig = plt.figure(figsize=(15, 6))\n>>> for i,(image,label) in enumerate(zip(batch[0], batch[1])):\n...     ax = fig.add_subplot(2, 5, i+1)\n...     ax.set_xticks([]); ax.set_yticks([])\n...     ax.imshow(image[:, :, 0], cmap='gray_r')\n...     ax.set_title('{}'.format(label), size=15)\n>>> plt.show() \n```", "```py\n>>> X_train = np.arange(10).reshape((10, 1))\n>>> y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3,\n...                     6.6, 7.4, 8.0, 9.0])\n>>> plt.plot(X_train, y_train, 'o', markersize=10)\n>>> plt.xlabel('x')\n>>> plt.ylabel('y')\n>>> plt.show() \n```", "```py\n>>> X_train_norm = (X_train - np.mean(X_train))/np.std(X_train)\n>>> ds_train_orig = tf.data.Dataset.from_tensor_slices(\n...                   (tf.cast(X_train_norm, tf.float32),\n...                    tf.cast(y_train, tf.float32))) \n```", "```py\n>>> class MyModel(tf.keras.Model):\n...     def __init__(self):\n...         super(MyModel, self).__init__()\n...         self.w = tf.Variable(0.0, name='weight')\n...         self.b = tf.Variable(0.0, name='bias')\n...\n...     def call(self, x):\n...         return self.w * x + self.b \n```", "```py\n>>> model = MyModel()\n>>> model.build(input_shape=(None, 1))\n>>> model.summary()\nModel: \"my_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```py\n>>> def loss_fn(y_true, y_pred):\n...     return tf.reduce_mean(tf.square(y_true - y_pred))\n>>> def train(model, inputs, outputs, learning_rate):\n...     with tf.GradientTape() as tape:\n...         current_loss = loss_fn(model(inputs), outputs)\n...     dW, db = tape.gradient(current_loss, [model.w, model.b])\n...     model.w.assign_sub(learning_rate * dW)\n...     model.b.assign_sub(learning_rate * db) \n```", "```py\n>>> tf.random.set_seed(1)\n>>> num_epochs = 200\n>>> log_steps = 100\n>>> learning_rate = 0.001\n>>> batch_size = 1\n>>> steps_per_epoch = int(np.ceil(len(y_train) / batch_size))\n>>> ds_train = ds_train_orig.shuffle(buffer_size=len(y_train))\n>>> ds_train = ds_train.repeat(count=None)\n>>> ds_train = ds_train.batch(1)\n>>> Ws, bs = [], []\n>>> for i, batch in enumerate(ds_train):\n...     if i >= steps_per_epoch * num_epochs:\n...         # break the infinite loop\n...         break\n...     Ws.append(model.w.numpy())\n...     bs.append(model.b.numpy())\n...\n...     bx, by = batch\n...     loss_val = loss_fn(model(bx), by)\n...\n...     train(model, bx, by, learning_rate=learning_rate)\n...     if i%log_steps==0:\n...         print('Epoch {:4d} Step {:2d} Loss {:6.4f}'.format(\n...               int(i/steps_per_epoch), i, loss_val))\nEpoch    0 Step  0 Loss 43.5600\nEpoch   10 Step 100 Loss 0.7530\nEpoch   20 Step 200 Loss 20.1759\nEpoch   30 Step 300 Loss 23.3976\nEpoch   40 Step 400 Loss 6.3481\nEpoch   50 Step 500 Loss 4.6356\nEpoch   60 Step 600 Loss 0.2411\nEpoch   70 Step 700 Loss 0.2036\nEpoch   80 Step 800 Loss 3.8177\nEpoch   90 Step 900 Loss 0.9416\nEpoch  100 Step 1000 Loss 0.7035\nEpoch  110 Step 1100 Loss 0.0348\nEpoch  120 Step 1200 Loss 0.5404\nEpoch  130 Step 1300 Loss 0.1170\nEpoch  140 Step 1400 Loss 0.1195\nEpoch  150 Step 1500 Loss 0.0944\nEpoch  160 Step 1600 Loss 0.4670\nEpoch  170 Step 1700 Loss 2.0695\nEpoch  180 Step 1800 Loss 0.0020\nEpoch  190 Step 1900 Loss 0.3612 \n```", "```py\n>>> print('Final Parameters: ', model.w.numpy(), model.b.numpy())\nFinal Parameters:  2.6576622 4.8798566\n>>> X_test = np.linspace(0, 9, num=100).reshape(-1, 1)\n>>> X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train)\n>>> y_pred = model(tf.cast(X_test_norm, dtype=tf.float32))\n>>> fig = plt.figure(figsize=(13, 5))\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> plt.plot(X_train_norm, y_train, 'o', markersize=10)\n>>> plt.plot(X_test_norm, y_pred, '--', lw=3)\n>>> plt.legend(['Training examples', 'Linear Reg.'], fontsize=15)\n>>> ax.set_xlabel('x', size=15)\n>>> ax.set_ylabel('y', size=15)\n>>> ax.tick_params(axis='both', which='major', labelsize=15)\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> plt.plot(Ws, lw=3)\n>>> plt.plot(bs, lw=3)\n>>> plt.legend(['Weight w', 'Bias unit b'], fontsize=15)\n>>> ax.set_xlabel('Iteration', size=15)\n>>> ax.set_ylabel('Value', size=15)\n>>> ax.tick_params(axis='both', which='major', labelsize=15)\n>>> plt.show() \n```", "```py\n>>> tf.random.set_seed(1)\n>>> model = MyModel()\n>>> model.compile(optimizer='sgd',\n...               loss=loss_fn,\n...               metrics=['mae', 'mse']) \n```", "```py\n>>> model.fit(X_train_norm, y_train,\n...           epochs=num_epochs, batch_size=batch_size,\n...           verbose=1)\nTrain on 10 samples\nEpoch 1/200\n10/10 [==============================] - 0s 4ms/sample - loss: 27.8578 - mae: 4.5810 - mse: 27.8578\nEpoch 2/200\n10/10 [==============================] - 0s 738us/sample - loss: 18.6640 - mae: 3.7395 - mse: 18.6640\n...\nEpoch 200/200\n10/10 [==============================] - 0s 1ms/sample - loss: 0.4139 - mae: 0.4942 - mse: 0.4139 \n```", "```py\n>>> iris, iris_info = tfds.load('iris', with_info=True)\n>>> print(iris_info) \n```", "```py\n>>> tf.random.set_seed(1)\n>>> ds_orig = iris['train']\n>>> ds_orig = ds_orig.shuffle(150, reshuffle_each_iteration=False)\n>>> ds_train_orig = ds_orig.take(100)\n>>> ds_test = ds_orig.skip(100) \n```", "```py\n>>> ds_train_orig = ds_train_orig.map(\n...     lambda x: (x['features'], x['label']))\n>>> ds_test = ds_test.map(\n...     lambda x: (x['features'], x['label'])) \n```", "```py\n>>> iris_model = tf.keras.Sequential([\n...         tf.keras.layers.Dense(16, activation='sigmoid',\n...                               name='fc1', input_shape=(4,)),\n...         tf.keras.layers.Dense(3, name='fc2',\n...                               activation='softmax')])\n>>> iris_model.summary()\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nfc1 (Dense)                  (None, 16)                80        \n_________________________________________________________________\nfc2 (Dense)                  (None, 3)                 51        \n=================================================================\nTotal params: 131\nTrainable params: 131\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```py\n>>> iris_model.compile(optimizer='adam',\n...                    loss='sparse_categorical_crossentropy',\n...                    metrics=['accuracy']) \n```", "```py\n>>> num_epochs = 100\n>>> training_size = 100\n>>> batch_size = 2\n>>> steps_per_epoch = np.ceil(training_size / batch_size)\n>>> ds_train = ds_train_orig.shuffle(buffer_size=training_size)\n>>> ds_train = ds_train.repeat()\n>>> ds_train = ds_train.batch(batch_size=batch_size)\n>>> ds_train = ds_train.prefetch(buffer_size=1000)\n>>> history = iris_model.fit(ds_train, epochs=num_epochs,\n...                          steps_per_epoch=steps_per_epoch,\n...                          verbose=0) \n```", "```py\n>>> hist = history.history\n>>> fig = plt.figure(figsize=(12, 5))\n>>> ax = fig.add_subplot(1, 2, 1)\n>>> ax.plot(hist['loss'], lw=3)\n>>> ax.set_title('Training loss', size=15)\n>>> ax.set_xlabel('Epoch', size=15)\n>>> ax.tick_params(axis='both', which='major', labelsize=15)\n>>> ax = fig.add_subplot(1, 2, 2)\n>>> ax.plot(hist['accuracy'], lw=3)\n>>> ax.set_title('Training accuracy', size=15)\n>>> ax.set_xlabel('Epoch', size=15)\n>>> ax.tick_params(axis='both', which='major', labelsize=15)\n>>> plt.show() \n```", "```py\n>>> results = iris_model.evaluate(ds_test.batch(50), verbose=0)\n>>> print('Test loss: {:.4f}   Test Acc.: {:.4f}'.format(*results))\nTest loss: 0.0692   Test Acc.: 0.9800 \n```", "```py\n>>> iris_model.save('iris-classifier.h5',\n...            overwrite=True,\n...            include_optimizer=True,\n...            save_format='h5') \n```", "```py\n>>> iris_model_new = tf.keras.models.load_model('iris-classifier.h5') \n```", "```py\n>>> results = iris_model_new.evaluate(ds_test.batch(33), verbose=0)\n>>> print('Test loss: {:.4f}   Test Acc.: {:.4f}'.format(*results))\nTest loss: 0.0692   Test Acc.: 0.9800 \n```", "```py\n>>> import numpy as np\n>>> X = np.array([1, 1.4, 2.5]) ## first value must be 1\n>>> w = np.array([0.4, 0.3, 0.5])\n>>> def net_input(X, w):\n...     return np.dot(X, w)\n>>> def logistic(z):\n...     return 1.0 / (1.0 + np.exp(-z))\n>>> def logistic_activation(X, w):\n...     z = net_input(X, w)\n...     return logistic(z)\n>>> print(‘P(y=1|x) = %.3f’ % logistic_activation(X, w))\nP(y=1|x) = 0.888 \n```", "```py\n>>> # W : array with shape = (n_output_units, n_hidden_units+1)\n>>> #     note that the first column are the bias units\n>>> W = np.array([[1.1, 1.2, 0.8, 0.4],\n...               [0.2, 0.4, 1.0, 0.2],\n...               [0.6, 1.5, 1.2, 0.7]])\n>>> # A : data array with shape = (n_hidden_units + 1, n_samples)\n>>> #     note that the first column of this array must be 1\n>>> A = np.array([[1, 0.1, 0.4, 0.6]])\n>>> Z = np.dot(W, A[0])\n>>> y_probas = logistic(Z)\n>>> print(‘Net Input: \\n’, Z)\nNet Input:\n[ 1.78  0.76  1.65]\n>>> print(‘Output Units:\\n’, y_probas)\nOutput Units:\n[ 0.85569687  0.68135373  0.83889105] \n```", "```py\n>>> y_class = np.argmax(Z, axis=0)\n>>> print(‘Predicted class label: %d’ % y_class)\nPredicted class label: 0 \n```", "```py\n>>> def softmax(z):\n...     return np.exp(z) / np.sum(np.exp(z))\n>>> y_probas = softmax(Z)\n>>> print(‘Probabilities:\\n’, y_probas)\nProbabilities:\n[ 0.44668973  0.16107406  0.39223621]\n>>> np.sum(y_probas)\n1.0 \n```", "```py\n>>> import tensorflow as tf\n>>> Z_tensor = tf.expand_dims(Z, axis=0)\n>>> tf.keras.activations.softmax(Z_tensor)\n<tf.Tensor: id=21, shape=(1, 3), dtype=float64, numpy=array([[0.44668973, 0.16107406, 0.39223621]])> \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> def tanh(z):\n...     e_p = np.exp(z)\n...     e_m = np.exp(-z)\n...     return (e_p - e_m) / (e_p + e_m)\n>>> z = np.arange(-5, 5, 0.005)\n>>> log_act = logistic(z)\n>>> tanh_act = tanh(z)\n>>> plt.ylim([-1.5, 1.5])\n>>> plt.xlabel(‘net input $z$’)\n>>> plt.ylabel(‘activation $\\phi(z)$’)\n>>> plt.axhline(1, color=’black’, linestyle=’:’)\n>>> plt.axhline(0.5, color=’black’, linestyle=’:’)\n>>> plt.axhline(0, color=’black’, linestyle=’:’)\n>>> plt.axhline(-0.5, color=’black’, linestyle=’:’)\n>>> plt.axhline(-1, color=’black’, linestyle=’:’)\n>>> plt.plot(z, tanh_act,\n...          linewidth=3, linestyle=’--’,\n...          label=’tanh’)\n>>> plt.plot(z, log_act,\n...          linewidth=3,\n...          label=’logistic’)\n>>> plt.legend(loc=’lower right’)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> np.tanh(z)\narray([-0.9999092 , -0.99990829, -0.99990737, ...,  0.99990644,\n        0.99990737,  0.99990829])\n>>> tf.keras.activations.tanh(z)\n<tf.Tensor: id=14, shape=(2000,), dtype=float64, numpy=\narray([-0.9999092 , -0.99990829, -0.99990737, ...,  0.99990644,\n        0.99990737,  0.99990829])> \n```", "```py\n>>> from scipy.special import expit\n>>> expit(z)\narray([0.00669285, 0.00672617, 0.00675966, ..., 0.99320669, 0.99324034,\n       0.99327383]) \n```", "```py\n>>> tf.keras.activations.sigmoid(z)\n<tf.Tensor: id=16, shape=(2000,), dtype=float64, numpy=\narray([0.00669285, 0.00672617, 0.00675966, ..., 0.99320669, 0.99324034,\n       0.99327383])> \n```", "```py\n>>> tf.keras.activations.tanh(z)\n<tf.Tensor: id=23, shape=(2000,), dtype=float64, numpy=array([0\\.   , 0\\.   , 0\\.   , ..., 4.985, 4.99 , 4.995])> \n```"]
<html><head></head><body>
		<div id="_idContainer014">
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>Preface</h1>
			<p>XGBoost is an industry-proven, open-source software library that provides a gradient boosting framework for scaling billions of data points quickly and efficiently.</p>
			<p>The book introduces machine learning and XGBoost in scikit-learn before building up to the theory behind gradient boosting. You’ll cover decision trees and analyze bagging in the machine learning context, learning hyperparameters that extend to XGBoost along the way. You’ll build gradient boosting models from scratch and extend gradient boosting to big data while recognizing speed limitations using timers. Details in XGBoost are explored with a focus on speed enhancements and deriving parameters mathematically. With the help of detailed case studies, you’ll practice building and fine-tuning XGBoost classifiers and regressors using scikit-learn and the original Python API. You'll leverage XGBoost hyperparameters to improve scores, correct missing values, scale imbalanced datasets, and fine-tune alternative base learners. Finally, you'll apply advanced XGBoost techniques like building non-correlated ensembles, stacking models, and preparing models for industry deployment using sparse matrices, customized transformers, and pipelines.</p>
			<p>By the end of the book, you’ll be able to build high-performing machine learning models using XGBoost with minimal errors and maximum speed.</p>
			<h2 id="_idParaDest-9"><a id="_idTextAnchor008"/>Who this book is for</h2>
			<p>This book is for data science professionals and enthusiasts, data analysts, and developers who want to build fast and accurate machine learning models that scale with big data. Proficiency in Python along with a basic understanding of linear algebra will help you to get the most out of this book.</p>
			<h2 id="_idParaDest-10"><a id="_idTextAnchor009"/>What this book covers</h2>
			<p><a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning Landscape</em>, presents XGBoost within the general context of machine learning by introducing linear regression and logistic regression before comparing results with XGBoost. <strong class="source-inline">pandas</strong> is introduced to preprocess raw data for machine learning by converting categorical columns and clearing null values in a variety of ways.</p>
			<p><a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>, <em class="italic">Decision Trees in Depth</em>, presents a detailed examination of decision tree hyperparameters that are used by XGBoost, along with a graphical and statistical analysis of variance and bias that highlights the importance of overfitting, a theme touched on throughout the book.</p>
			<p><a href="B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070"><em class="italic">Chapter 3</em></a>, <em class="italic">Bagging with Random Forests</em>, presents a general survey of random forests as an XGBoost competitor with a focus on bagging. Additional XGBoost hyperparameters shared with random forests such as <strong class="source-inline">n_esimtators</strong> and <strong class="source-inline">subsample</strong> are thoroughly covered.</p>
			<p><a href="B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a>, <em class="italic">From Gradient Boosting to XGBoost</em>, covers boosting fundamentals, building a booster from scratch in <strong class="source-inline">scikit-learn</strong>, fine-tuning new XGBoost hyperparameters such as <strong class="source-inline">eta</strong>, and comparing runtimes between gradient boosting and XGBoost to highlight XGBoost's impressive speed.</p>
			<p><a href="B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117"><em class="italic">Chapter 5</em></a>, <em class="italic">XGBoost Unveiled</em>, analyzes the mathematical derivations of XGBoost algorithms and features a historically relevant case study featuring XGBoost's role as the winning model in the Higgs Boson Kaggle Competition. Standard XGBoost parameters are discussed, base models are built, and the original Python API is covered.</p>
			<p><a href="B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136"><em class="italic">Chapter 6</em></a>, <em class="italic">XGBoost Hyperparameters</em>, covers all essential XGBoost hyperparameters, summarizes previous tree ensemble hyperparameters, and uses original grid search functions to fine-tune XGBoost models to optimize scores.</p>
			<p><a href="B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161"><em class="italic">Chapter 7</em></a>, <em class="italic">Discovering Exoplanets with XGBoost</em>, gives you the opportunity to discover exoplanets with XGBoost in a top-to-bottom case study. The pitfalls of imbalanced datasets are analyzed with the confusion matrix and classification report leading to different scoring metrics and the important XGBoost hyperparameter <strong class="source-inline">scale_pos_weight</strong>.</p>
			<p><a href="B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189"><em class="italic">Chapter 8</em></a>, <em class="italic">XGBoost Alternative Base Learners</em>, covers the full range of XGBoost boosters including <strong class="source-inline">gbtree</strong>, <strong class="source-inline">dart</strong>, and <strong class="source-inline">gblinear</strong> for regression and classification. Random forests are presented as base learners, and as XGBoost alternative models with the new <strong class="source-inline">XGBRFRegressor</strong> and <strong class="source-inline">XGBRFClassifier</strong> classes.</p>
			<p><a href="B15551_09_Final_NM_ePUB.xhtml#_idTextAnchor211"><em class="italic">Chapter 9</em></a>, <em class="italic">XGBoost Kaggle Masters</em>, presents tips and tricks that XGBoost Kaggle winners have used to win competitions such as advanced feature engineering, building non-correlated machine ensembles, and stacking.</p>
			<p><a href="B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230"><em class="italic">Chapter 10</em></a>, <em class="italic">XGBoost Model Deployment</em>, transforms raw data into XGBoost machine learning predictions through the use of customized transformers to handle mixed data and machine learning pipelines to make predictions on incoming data with a fine-tuned XGBoost model.</p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>To get the most out of this book</h1>
			<p>Readers should be proficient in Python at the level of slicing lists, writing your own functions, and using dot-notation. General familiarity with linear algebra at the level of accessing rows and columns in matrices will be sufficient. A background in pandas and machine learning is helpful but not required as all code and concepts are explained along the way.</p>
			<p>This book uses the latest versions of Python in Jupyter Notebook with the Anaconda distribution. Anaconda is highly recommended since all major data science libraries are included. It's worth updating Anaconda before getting started. The following section provides detailed instructions to set up your coding environment like ours.</p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Setting up your coding environment</h1>
			<p>The following table summarizes the essential software used in this book.</p>
			<div>
				<div id="_idContainer004" class="IMG---Figure">
					<img src="image/B15551_Preface_Table_1.jpg" alt=""/>
				</div>
			</div>
			<p>Here are instructions for uploading this software to your system.</p>
			<h2 id="_idParaDest-13"><a id="_idTextAnchor012"/>Anaconda</h2>
			<p>The data science libraries that you will need in this book along with Jupyter Notebooks, scikit-learn (sklearn), and Python may be installed together using Anaconda, which is recommended.</p>
			<p>Here are the steps to install Anaconda on your computer as of 2020:</p>
			<ol>
				<li><p>Go to <a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a>.</p></li>
				<li><p>Click <strong class="bold">Download</strong> on the following screen, which does not yet start the download, but presents you with a variety of options (see step 3):</p><div id="_idContainer005" class="IMG---Figure"><img src="image/B15551_Preface_01.jpg" alt=""/></div><p class="figure-caption">Figure 0.1 – Preparing to download Anaconda</p></li>
				<li><p>Select your installer. The <strong class="screen-inline">64-Bit Graphical Installer</strong> is recommended for Windows and Mac. Make sure that you select from the top two rows under Python 3.7 since Python 3.7 is used throughout this book:</p><div id="_idContainer006" class="IMG---Figure"><img src="image/B15551_Preface_02.jpg" alt=""/></div><p class="figure-caption">Figure 0.2 – Anaconda Installers</p></li>
				<li><p>After your download begins, continue with the prompts on your computer to complete the installation:</p><p class="callout-heading">Warning for Mac users </p><p class="callout">If you run into the error <strong class="bold">You cannot install Anaconda3 in this location</strong>, do not panic. Just click on the highlighted row <strong class="bold">Install for me only</strong> and the <strong class="bold">Continue</strong> button will present as an option.</p></li>
			</ol>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/B15551_Preface_03.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 0.3 – Warning for Mac Users – Just click Install for me only then Continue</p>
			<h2 id="_idParaDest-14"><a id="_idTextAnchor013"/>Using Jupyter notebooks</h2>
			<p>Now that you have Anaconda installed, you may open a Jupyter notebook to use Python 3.7. Here are the steps to open a Jupyter notebook:</p>
			<ol>
				<li value="1"><p>Click on <strong class="bold">Anaconda-Navigator</strong> on your computer.</p></li>
				<li><p>Click <strong class="bold">Launch</strong> under <strong class="bold">Jupyter Notebook</strong> as shown in the following screenshot:</p><div id="_idContainer008" class="IMG---Figure"><img src="image/B15551_Preface_04.jpg" alt=""/></div><p class="figure-caption">Figure 0.4 – Anaconda home screen</p><p>This should open a Jupyter notebook in a browser window. While Jupyter notebooks appear in web browsers for convenience, they are run on your personal computer, not online. Google Colab notebooks are an acceptable online alternative, but in this book, Jupyter notebooks are used exclusively.</p></li>
				<li><p>Select <strong class="bold">Python 3</strong> from the <strong class="bold">New</strong> tab present on the right side of your Jupyter notebook as shown in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B15551_Preface_05.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 0.5 – Jupyter notebook home screen</p>
			<p>This should bring you to the following screen:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B15551_Preface_06.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 0.6 – Inside a Jupyter notebook</p>
			<p>Congratulations! You are now ready to run Python code! Just type anything in the cell, such as <strong class="source-inline">print('hello xgboost!')</strong>, and press <em class="italic">Shift</em> + <em class="italic">Enter</em> to run the code.</p>
			<p class="callout-heading">Troubleshooting Jupyter notebooks</p>
			<p class="callout">If you have trouble running or installing Jupyter notebooks, please visit Jupyter's official troubleshooting guide: <a href="https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html">https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html</a>.</p>
			<h2 id="_idParaDest-15"><a id="_idTextAnchor014"/>XGBoost</h2>
			<p>At the time of writing, XGBoost is not yet included in Anaconda so it must be installed separately.</p>
			<p>Here are the steps for installing XGBoost on your computer:</p>
			<ol>
				<li value="1"><p>Go to <a href="https://anaconda.org/conda-forge/xgboost">https://anaconda.org/conda-forge/xgboost</a>. Here is what you should see:</p><div id="_idContainer011" class="IMG---Figure"><img src="image/B15551_Preface_07.jpg" alt=""/></div><p class="figure-caption">Figure 0.7 – Anaconda recommendations to install XGBoost</p></li>
				<li><p>Copy the first line of code in the preceding screenshot, as shown here:</p><div id="_idContainer012" class="IMG---Figure"><img src="image/B15551_Preface_08.jpg" alt=""/></div><p class="figure-caption">Figure 0.8 – Package installation</p></li>
				<li><p>Open the Terminal on your computer.</p><p>If you do not know where your Terminal is located, search <strong class="source-inline">Terminal</strong> for Mac and <strong class="source-inline">Windows Terminal</strong> for Windows. </p></li>
				<li><p>Paste the following code into your Terminal, press <em class="italic">Enter</em>, and follow any prompts: </p><p class="source-code">conda install -c conda-forge xgboost</p></li>
				<li><p>Verify that the installation has worked by opening a new Jupyter notebook as outlined in the previous section. Then enter <strong class="source-inline">import xgboost</strong> and press <em class="italic">Shift</em> + <em class="italic">Enter</em>. You should see the following:</p></li>
			</ol>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B15551_Preface_09.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 0.9 – Successful import of XGBoost in a Jupyter notebook</p>
			<p>If you got no errors, congratulations! You now have all the necessary technical requirements to run code in this book.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If you received errors trying to set up your coding environment, please go back through the previous steps, or consider reviewing the Anaconda error documentation presented here: <a href="https://docs.anaconda.com/anaconda/user-guide/troubleshooting/">https://docs.anaconda.com/anaconda/user-guide/troubleshooting/</a>. Previous users of Anaconda should update Anaconda by entering <strong class="source-inline">conda update conda</strong> in the Terminal. If you have trouble uploading XGBoost, see the official documentation at <a href="https://xgboost.readthedocs.io/en/latest/build.html">https://xgboost.readthedocs.io/en/latest/build.html</a>.</p>
			<h2 id="_idParaDest-16"><a id="_idTextAnchor015"/>Versions</h2>
			<p>Here is code that you may run in a Jupyter notebook to see what versions of the following software you are using:</p>
			<p class="source-code">import platform; print(platform.platform())</p>
			<p class="source-code">import sys; print("Python", sys.version)</p>
			<p class="source-code">import numpy; print("NumPy", numpy.__version__)</p>
			<p class="source-code">import scipy; print("SciPy", scipy.__version__)</p>
			<p class="source-code">import sklearn; print("Scikit-Learn", sklearn.__version__)</p>
			<p class="source-code">import xgboost; print("XGBoost", xgboost.__version__)</p>
			<p>Here are the versions used to generate code in this book:</p>
			<p class="source-code">Darwin-19.6.0-x86_64-i386-64bit</p>
			<p class="source-code">Python 3.7.7 (default, Mar 26 2020, 10:32:53) </p>
			<p class="source-code">[Clang 4.0.1 (tags/RELEASE_401/final)]</p>
			<p class="source-code">NumPy 1.19.1</p>
			<p class="source-code">SciPy 1.5.2</p>
			<p class="source-code">Scikit-Learn 0.23.2</p>
			<p class="source-code">XGBoost 1.2.0</p>
			<p>It's okay if you have different versions than ours. Software is updated all the time, and you may obtain better results by using newer versions when released. If you are using older versions, however, it's recommended that you update using Anaconda by running <strong class="source-inline">conda update conda</strong> in the terminal. You may also run <strong class="source-inline">conda update xgboost</strong> if you installed an older version of XGBoost previously and forged it with Anaconda as outlined in the previous section.</p>
			<h2 id="_idParaDest-17"><a id="_idTextAnchor016"/>Accessing code files</h2>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code via the GitHub repository (link available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.</strong></p>
			<p>The code bundle for the book is also hosted on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn">https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn</a>. In case there's an update to the code, it will be updated on the existing GitHub repository.</p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Download the color images</h1>
			<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here:</p>
			<p><a href="https://static.packt-cdn.com/downloads/9781839218354_ColorImages.pdf">https://static.packt-cdn.com/downloads/9781839218354_ColorImages.pdf</a>.</p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Conventions used</h1>
			<p>There are a number of text conventions used throughout this book.</p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: "The <strong class="source-inline">AdaBoostRegressor</strong> and <strong class="source-inline">AdaBoostClassifier</strong> algorithms may be downloaded from the <strong class="source-inline">sklearn.ensemble</strong> library and fit to any training set."</p>
			<p>A block of code is set as follows:</p>
			<p class="source-code">X_bikes = df_bikes.iloc[:,:-1]</p>
			<p class="source-code">y_bikes = df_bikes.iloc[:,-1]</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)</p>
			<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
			<p class="source-code">Stopping. Best iteration:</p>
			<p class="source-code">[1]	validation_0-error:0.118421</p>
			<p class="source-code"><strong class="bold">Accuracy: 88.16%</strong></p>
			<p class="callout-heading">Tips or important notes</p>
			<p class="callout">Appear like this.</p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>Get in touch</h1>
			<p>Feedback from our readers is always welcome.</p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, mention the book title in the subject of your message and email us at <a href="mailto:customercare@packtpub.com">customercare@packtpub.com</a>.</p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <a href="mailto:copyright@packt.com">copyright@packt.com</a> with a link to the material.</p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a>.</p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/>Reviews</h1>
			<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
			<p>For more information about Packt, please visit <a href="http://packt.com">packt.com</a>.</p>
		</div>
	</body></html>
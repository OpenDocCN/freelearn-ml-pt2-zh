- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going Deeper – The Mechanics of TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 13*, *Parallelizing Neural Network Training with TensorFlow*, we
    covered how to define and manipulate tensors and worked with the `tf.data` API
    to build input pipelines. We further built and trained a multilayer perceptron
    to classify the Iris dataset using the TensorFlow Keras API (`tf.keras`).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some hands-on experience with TensorFlow neural network (NN)
    training and machine learning, it's time to take a deeper dive into the TensorFlow
    library and explore its rich set of features, which will allow us to implement
    more advanced deep learning models in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use different aspects of TensorFlow's API to implement
    NNs. In particular, we will again use the Keras API, which provides multiple layers
    of abstraction to make the implementation of standard architectures very convenient.
    TensorFlow also allows us to implement custom NN layers, which is very useful
    in research-oriented projects that require more customization. Later in this chapter,
    we will implement such a custom layer.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the different ways of model building using the Keras API, we will
    also consider the classic **exclusive or** (**XOR**) problem. Firstly, we will
    build multilayer perceptrons using the `Sequential` class. Then, we will consider
    other methods, such as subclassing `tf.keras.Model` for defining custom layers.
    Finally, we will cover `tf.estimator`, a high-level TensorFlow API that encapsulates
    the machine learning steps from raw input to prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that we will cover are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and working with TensorFlow graphs and migration to TensorFlow
    v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Function decoration for graph compilation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with TensorFlow variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the classic XOR problem and understanding model capacity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building complex NN models using Keras' `Model` class and the Keras functional
    API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing gradients using automatic differentiation and `tf.GradientTape`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with TensorFlow Estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key features of TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow provides us with a scalable, multiplatform programming interface
    for implementing and running machine learning algorithms. The TensorFlow API has
    been relatively stable and mature since its 1.0 release in 2017, but it just experienced
    a major redesign with its recent 2.0 release in 2019, which we are using in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Since its initial release in 2015, TensorFlow has become the most widely adopted
    deep learning library. However, one of its main friction points was that it was
    built around static computation graphs. Static computation graphs have certain
    advantages, such as better graph optimizations behind the scenes and support for
    a wider range of hardware devices; however, static computation graphs require
    separate graph declaration and graph evaluation steps, which make it cumbersome
    for users to develop and work with NNs interactively.
  prefs: []
  type: TYPE_NORMAL
- en: Taking all the user feedback to heart, the TensorFlow team decided to make dynamic
    computation graphs the default in TensorFlow 2.0, which makes the development
    and training of NNs much more convenient. In the next section, we will cover some
    of the important changes from TensorFlow v1.x to v2\. Dynamic computation graphs
    allow for interleaving the graph declaration and graph evaluation steps such that
    TensorFlow 2.0 feels much more natural for Python and NumPy users compared to
    previous versions of TensorFlow. However, note that TensorFlow 2.0 still allows
    users to use the "old" TensorFlow v1.x API via the `tf.compat` submodule. This
    helps users to transition their code bases more smoothly to the new TensorFlow
    v2 API.
  prefs: []
  type: TYPE_NORMAL
- en: A key feature of TensorFlow, which was also noted in *Chapter 13*, *Parallelizing
    Neural Network Training with TensorFlow*, is its ability to work with single or
    multiple graphical processing units (GPUs). This allows users to train deep learning
    models very efficiently on large datasets and large-scale systems.
  prefs: []
  type: TYPE_NORMAL
- en: While TensorFlow is an open source library and can be freely used by everyone,
    its development is funded and supported by Google. This involves a large team
    of software engineers who expand and improve the library continuously. Since TensorFlow
    is an open source library, it also has strong support from other developers outside
    of Google, who avidly contribute and provide user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: This has made the TensorFlow library more useful to both academic researchers
    and developers. A further consequence of these factors is that TensorFlow has
    extensive documentation and tutorials to help new users.
  prefs: []
  type: TYPE_NORMAL
- en: Last, but not least, TensorFlow supports mobile deployment, which also makes
    it a very suitable tool for production.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow''s computation graphs: migrating to TensorFlow v2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow performs its computations based on a directed acyclic graph (DAG).
    In TensorFlow v1.x, such graphs could be explicitly defined in the low-level API,
    although this was not trivial for large and complex models. In this section, we
    will see how these graphs can be defined for a simple arithmetic computation.
    Then, we will see how to migrate a graph to TensorFlow v2, the **eager execution**
    and dynamic graph paradigm, as well as the function decoration for faster computations.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding computation graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TensorFlow relies on building a computation graph at its core, and it uses
    this computation graph to derive relationships between tensors from the input
    all the way to the output. Let''s say that we have rank 0 (scalar) tensors *a*,
    *b*, and *c* and we want to evaluate ![](img/B13208_14_001.png). This evaluation
    can be represented as a computation graph, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13208_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the computation graph is simply a network of nodes. Each node
    resembles an operation, which applies a function to its input tensor or tensors
    and returns zero or more tensors as the output. TensorFlow builds this computation
    graph and uses it to compute the gradients accordingly. In the next subsections,
    we will see some examples of creating a graph for this computation using TensorFlow
    v1.x and v2 styles.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a graph in TensorFlow v1.x
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the earlier version of the TensorFlow (v1.x) low-level API, this graph had
    to be explicitly declared. The individual steps for building, compiling, and evaluating
    such a computation graph in TensorFlow v1.x are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a new, empty computation graph
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add nodes (tensors and operations) to the computation graph
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluate (execute) the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new session
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the variables in the graph
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the computation graph in this session
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we take a look at the dynamic approach in TensorFlow v2, let''s look
    at a simple example that illustrates how to create a graph in TensorFlow v1.x
    for evaluating ![](img/B13208_14_002.png), as shown in the previous figure. The
    variables *a*, *b*, and *c* are scalars (single numbers), and we define these
    as TensorFlow constants. A graph can then be created by calling `tf.Graph()`.
    Variables, as well as computations, represent the nodes of the graph, which we
    will define as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we first defined graph `g` via `g=tf.Graph()`. Then, we added
    nodes to the graph, `g`, using `with g.as_default()`. However, note that if we
    do not explicitly create a graph, there is always a default graph to which variables
    and computations will be added automatically.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow v1.x, a session is an environment in which the operations and
    tensors of a graph can be executed. The `Session` class was removed from TensorFlow
    v2; However, for the time being, it is still available via the `tf.compat` submodule
    to allow compatibility with TensorFlow v1.x. A session object can be created by
    calling `tf.compat.v1.Session()`, which can receive an existing graph (here, `g`)
    as an argument, as in `Session(graph=g)`.
  prefs: []
  type: TYPE_NORMAL
- en: After launching a graph in a TensorFlow session, we can execute its nodes, that
    is, evaluate its tensors or execute its operators. Evaluating each individual
    tensor involves calling its `eval()` method inside the current session. When evaluating
    a specific tensor in the graph, TensorFlow has to execute all the preceding nodes
    in the graph until it reaches the given node of interest. In case there are one
    or more placeholder variables, we also need to provide values for those through
    the session's `run` method, as we will see later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Migrating a graph to TensorFlow v2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, let''s look at how this code can be migrated to TensorFlow v2\. TensorFlow
    v2 uses dynamic (as opposed to static) graphs by default (this is also called
    eager execution in TensorFlow), which allows us to evaluate an operation on the
    fly. Therefore, we do not have to explicitly create a graph and a session, which
    makes the development workflow much more convenient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading input data into a model: TensorFlow v1.x style'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another important improvement from TensorFlow v1.x to v2 is regarding how data
    can be loaded into our models. In TensorFlow v2, we can directly feed data in
    the form of Python variables or NumPy arrays. However, when using the TensorFlow
    v1.x low-level API, we had to create placeholder variables for providing input
    data to a model. For the preceding simple computation graph example, ![](img/B13208_14_003.png),
    let''s assume that *a*, *b*, and *c* are the input tensors of rank 0\. We can
    then define three placeholders, which we will then use to "feed" data to the model
    via a so-called `feed_dict` dictionary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading input data into a model: TensorFlow v2 style'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In TensorFlow v2, all this can simply be done by *defining a regular Python
    function* with `a`, `b`, and `c` as its input arguments, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to carry out the computation, we can simply call this function with `Tensor`
    objects as function arguments. Note that TensorFlow functions such as `add`, `subtract`,
    and `multiply` also allow us to provide inputs of higher ranks in the form of
    a TensorFlow `Tensor` object, a NumPy array, or possibly other Python objects,
    such as lists and tuples. In the following code example, we provide scalar inputs
    (rank 0), as well as rank 1 and rank 2 inputs, as lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this section, you saw how migrating to TensorFlow v2 makes the programming
    style simple and efficient by avoiding explicit graph and session creation steps.
    Now that we have seen how TensorFlow v1.x compares to TensorFlow v2, we will focus
    only on TensorFlow v2 for the remainder of this book. Next, we will take a deeper
    look into decorating Python functions into a graph that allows for faster computation.
  prefs: []
  type: TYPE_NORMAL
- en: Improving computational performance with function decorators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you saw in the previous section, we can easily write a normal Python function
    and utilize TensorFlow operations. However, computations via the eager execution
    (dynamic graph) mode are not as efficient as the static graph execution in TensorFlow
    v1.x. Thus, TensorFlow v2 provides a tool called AutoGraph that can automatically
    transform Python code into TensorFlow's graph code for faster execution. In addition,
    TensorFlow provides a simple mechanism for compiling a normal Python function
    to a static TensorFlow graph in order to make the computations more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this works in practice, let''s work with our previous `compute_z`
    function and annotate it for graph compilation using the `@tf.function` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we can use and call this function the same way as before, but now
    TensorFlow will construct a static graph based on the input arguments. Python
    supports dynamic typing and polymorphism, so we can define a function such as
    `def f(a, b): return a+b` and then call it using integer, float, list, or string
    inputs (recall that `a+b` is a valid operation for lists and strings). While TensorFlow
    graphs require static types and shapes, `tf.function` supports such a dynamic
    typing capability. For example, let''s call this function with the following inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the same outputs as before. Here, TensorFlow uses a tracing
    mechanism to construct a graph based on the input arguments. For this tracing
    mechanism, TensorFlow generates a tuple of keys based on the input signatures
    given for calling the function. The generated keys are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For `tf.Tensor` arguments, the key is based on their shapes and dtypes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Python types, such as lists, their `id()` is used to generate cache keys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Python primitive values, the cache keys are based on the input values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upon calling such a decorated function, TensorFlow will check whether a graph
    with the corresponding key has already been generated. If such a graph does not
    exist, TensorFlow will generate a new graph and store the new key. On the other
    hand, if we want to limit the way a function can be called, we can specify its
    input signature via a tuple of `tf.TensorSpec` objects when defining the function.
    For example, let''s redefine the previous function, `compute_z`, and specify that
    only rank 1 tensors of type `tf.int32` are allowed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can call this function using rank 1 tensors (or lists that can be converted
    to rank 1 tensors):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'However, calling this function using tensors with ranks other than 1 will result
    in an error since the rank will not match the specified input signature, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In this section, we learned how to annotate a normal Python function so that
    TensorFlow will compile it into a graph for faster execution. Next, we will look
    at TensorFlow variables: how to create them and how to use them.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Variable objects for storing and updating model parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We covered `Tensor` objects in *Chapter 13*, *Parallelizing Neural Network
    Training with TensorFlow*. In the context of TensorFlow, a `Variable` is a special
    `Tensor` object that allows us to store and update the parameters of our models
    during training. A `Variable` can be created by just calling the `tf.Variable`
    class on user-specified initial values. In the following code, we will generate
    `Variable` objects of type `float32`, `int32`, `bool`, and `string`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we always have to provide the initial values when creating a `Variable`.
    Variables have an attribute called `trainable`, which, by default, is set to `True`.
    Higher-level APIs such as Keras will use this attribute to manage the trainable
    variables and non-trainable ones. You can define a non-trainable `Variable` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The values of a `Variable` can be efficiently modified by running some operations
    such as `.assign()`, `.assign_add()` and related methods. Let''s take a look at
    some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: When the `read_value` argument is set to `True` (which is also the default),
    these operations will automatically return the new values after updating the current
    values of the `Variable`. Setting the `read_value` to `False` will suppress the
    automatic return of the updated value (but the `Variable` will still be updated
    in place). Calling `w.value()` will return the values in a tensor format. Note
    that we cannot change the shape or type of the `Variable` during assignment.
  prefs: []
  type: TYPE_NORMAL
- en: You will recall that for NN models, initializing model parameters with random
    weights is necessary to break the symmetry during backpropagation—otherwise, a
    multilayer NN would be no more useful than a single-layer NN like logistic regression.
    When creating a TensorFlow `Variable`, we can also use a random initialization
    scheme. TensorFlow can generate random numbers based on a variety of distributions
    via `tf.random` (see [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random)).
    In the following example, we will take a look at some standard initialization
    methods that are also available in Keras (see [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers)).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s look at how we can create a `Variable` with Glorot initialization,
    which is a classic random initialization scheme that was proposed by Xavier Glorot
    and Yoshua Bengio. For this, we create an operator called `init` as an object
    of class `GlorotNormal`. Then, we call this operator and provide the desired shape
    of the output tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use this operator to initialize a `Variable` of shape ![](img/B13208_14_004.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Xavier (or Glorot) initialization**'
  prefs: []
  type: TYPE_NORMAL
- en: In the early development of deep learning, it was observed that random uniform
    or random normal weight initialization could often result in a poor performance
    of the model during training.
  prefs: []
  type: TYPE_NORMAL
- en: In 2010, Glorot and Bengio investigated the effect of initialization and proposed
    a novel, more robust initialization scheme to facilitate the training of deep
    networks. The general idea behind Xavier initialization is to roughly balance
    the variance of the gradients across different layers. Otherwise, some layers
    may get too much attention during training while the other layers lag behind.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the research paper by Glorot and Bengio, if we want to initialize
    the weights from uniform distribution, we should choose the interval of this uniform
    distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_14_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B13208_14_006.png) is the number of input neurons that are multiplied
    by the weights, and ![](img/B13208_14_007.png) is the number of output neurons
    that feed into the next layer. For initializing the weights from Gaussian (normal)
    distribution, it is recommended that you choose the standard deviation of this
    Gaussian to be ![](img/B13208_14_008.png).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow supports Xavier initialization in both uniform and normal distributions
    of weights.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Glorot and Bengio's initialization scheme, including
    the mathematical derivation and proof, read their original paper (*Understanding
    the difficulty of deep feedforward neural networks*, *Xavier Glorot* and *Yoshua
    Bengio*, 2010), which is freely available at [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to put this into the context of a more practical use case, let''s see
    how we can define a `Variable` inside the base `tf.Module` class. We will define
    two variables: a trainable one and a non-trainable one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in this code example, subclassing the `tf.Module` class gives
    us direct access to all variables defined in a given object (here, an instance
    of our custom `MyModule` class) via the `.variables` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s look at using variables inside a function decorated with `tf.function`.
    When we define a TensorFlow `Variable` inside a normal function (not decorated),
    we might expect that a new `Variable` will be created and initialized each time
    the function is called. However, `tf.function` will try to reuse the `Variable`
    based on tracing and graph creation. Therefore, TensorFlow does not allow the
    creation of a `Variable` inside a decorated function and, as a result, the following
    code will raise an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'One way to avoid this problem is to define the `Variable` outside of the decorated
    function and use it inside the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Computing gradients via automatic differentiation and GradientTape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you already know, optimizing NNs requires computing the gradients of the
    cost with respect to the NN weights. This is required for optimization algorithms
    such as stochastic gradient descent (SGD). In addition, gradients have other applications,
    such as diagnosing the network to find out why an NN model is making a particular
    prediction for a test example. Therefore, in this section, we will cover how to
    compute gradients of a computation with respect to some variables.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the gradients of the loss with respect to trainable variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow supports *automatic differentiation*, which can be thought of as
    an implementation of the *chain rule* for computing gradients of nested functions.
    When we define a series of operations that results in some output or even intermediate
    tensors, TensorFlow provides a context for calculating gradients of these computed
    tensors with respect to its dependent nodes in the computation graph. In order
    to compute these gradients, we have to "record" the computations via `tf.GradientTape`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s work with a simple example where we will compute ![](img/B13208_14_009.png)
    and define the loss as the squared loss between the target and prediction, ![](img/B13208_14_010.png).
    In the more general case, where we may have multiple predictions and targets,
    we compute the loss as the sum of the squared error, ![](img/B13208_14_011.png).
    In order to implement this computation in TensorFlow, we will define the model
    parameters, *w* and *b*, as variables, and the input, *x* and *y*, as tensors.
    We will place the computation of *z* and the loss within the `tf.GradientTape`
    context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When computing the value *z*, we could think of the required operations, which
    we recorded to the "gradient tape," as a forward pass in an NN. We used `tape.gradient`
    to compute ![](img/B13208_14_012.png). Since this is a very simple example, we
    can obtain the derivatives, ![](img/B13208_14_013.png), symbolically to verify
    that the computed gradients match the results we obtained in the previous code
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Understanding automatic differentiation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic differentiation represents a set of computational techniques for
    computing derivatives or gradients of arbitrary arithmetic operations. During
    this process, gradients of a computation (expressed as a series of operations)
    are obtained by accumulating the gradients through repeated applications of the
    chain rule. To better understand the concept behind automatic differentiation,
    let''s consider a series of computations, ![](img/B13208_14_014.png), with input
    *x* and output *y*. This can be broken into a series of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_14_015.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B13208_14_016.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B13208_14_017.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B13208_14_018.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'The derivative ![](img/B13208_14_019.png) can be computed in two different
    ways: forward accumulation, which starts with ![](img/B13208_14_020.png), and
    reverse accumulation, which starts with ![](img/B13208_14_021.png) Note that TensorFlow
    uses the latter, reverse accumulation.'
  prefs: []
  type: TYPE_NORMAL
- en: Computing gradients with respect to non-trainable tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`tf.GradientTape` automatically supports the gradients for trainable variables.
    However, for non-trainable variables and other `Tensor` objects, we need to add
    an additional modification to the `GradientTape` called `tape.watch()` to monitor
    those as well. For example, if we are interested in computing ![](img/B13208_14_022.png),
    the code will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**Adversarial examples**'
  prefs: []
  type: TYPE_NORMAL
- en: Computing gradients of the loss with respect to the input example is used for
    generating *adversarial examples* (or *adversarial attacks*). In computer vision,
    adversarial examples are examples that are generated by adding some small imperceptible
    noise (or perturbations) to the input example, which results in a deep NN misclassifying
    them. Covering adversarial examples is beyond the scope of this book, but if you
    are interested, you can find the original paper by Christian Szegedy et al., titled
    *Intriguing properties of neural networks,* at [https://arxiv.org/pdf/1312.6199.pdf](https://arxiv.org/pdf/1312.6199.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Keeping resources for multiple gradient computations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we monitor the computations in the context of `tf.GradientTape`, by default,
    the tape will keep the resources only for a single gradient computation. For instance,
    after calling `tape.gradient()` once, the resources will be released and the tape
    will be cleared. Hence, if we want to compute more than one gradient, for example,
    both ![](img/B13208_14_023.png) and ![](img/B13208_14_024.png), we need to make
    the tape persistent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: However, keep in mind that this is only needed when we want to compute more
    than one gradient, as recording and keeping the gradient tape is less memory-efficient
    compared to releasing the memory after a single gradient computation. This is
    also why the default setting is `persistent=False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if we are computing gradients of a loss term with respect to the parameters
    of a model, we can define an optimizer and apply the gradients to optimize the
    model parameters using the `tf.keras` API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You will recall that the initial weight and bias unit were *w* = 1.0 and *b*
    = 0.5, and applying the gradients of the loss with respect to the model parameters
    changed the model parameters to *w* = 1.0056 and *b* = 0.504.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying implementations of common architectures via the Keras API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have already seen some examples of building a feedforward NN model (for
    instance, a multilayer perceptron) and defining a sequence of layers using Keras''
    `Sequential` class. Before we look at different approaches for configuring those
    layers, let''s briefly recap the basic steps by building a model with two densely
    (fully) connected layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We specified the input shape with `model.build()`, instantiating the variables
    after defining the model for that particular shape. The number of parameters of
    each layer is displayed: ![](img/B13208_14_025.png) for the first layer, and ![](img/B13208_14_026.png)
    for the second layer. Once variables (or model parameters) are created, we can
    access both trainable and non-trainable variables as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this case, each layer has a weight matrix called `kernel` as well as a bias
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s configure these layers, for example, by applying different activation
    functions, variable initializers, or regularization methods to the parameters.
    A comprehensive and complete list of available options for these categories can
    be found in the official documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing activation functions via `tf.keras.activations`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initializing the layer parameters via `tf.keras.initializers`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applying regularization to the layer parameters (to prevent overfitting) via
    `tf.keras.regularizers`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code example, we will configure the first layer by specifying
    initializers for the kernel and bias variables. Then, we will configure the second
    layer by specifying an L1 regularizer for the kernel (weight matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, in addition to configuring the individual layers, we can also
    configure the model when we compile it. We can specify the type of optimizer and
    the loss function for training, as well as which metrics to use for reporting
    the performance on the training, validation, and test datasets. Again, a comprehensive
    list of all available options can be found in the official documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimizers via `tf.keras.optimizers`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loss functions via `tf.keras.losses`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Performance metrics via `tf.keras.metrics`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choosing a loss function**'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the choices for optimization algorithms, SGD and Adam are the most
    widely used methods. The choice of loss function depends on the task; for example,
    you might use mean square error loss for a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: The family of cross-entropy loss functions supplies the possible choices for
    classification tasks, which are extensively discussed in *Chapter 15*, *Classifying
    Images with Deep Convolutional Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, you can use the techniques you have learned from previous chapters
    (for example, techniques for model evaluation from *Chapter 6*, *Learning Best
    Practices for Model Evaluation and Hyperparameter Tuning*) combined with the appropriate
    metrics for the problem. For example, precision and recall, accuracy, area under
    the curve (AUC), and false negative and false positive scores are appropriate
    metrics for evaluating classification models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will compile the model using the SGD optimizer, cross-entropy
    loss for binary classification, and a specific list of metrics, including accuracy,
    precision, and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: When we train this model by calling `model.fit(...)`, the history of the loss
    and the specified metrics for evaluating training and validation performance (if
    a validation dataset is used) will be returned, which can be used to diagnose
    the learning behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at a more practical example: solving the classic XOR classification
    problem using the Keras API. First, we will use the `tf.keras.Sequential()` class
    to build the model. Along the way, you will also learn about the capacity of a
    model for handling nonlinear decision boundaries. Then, we will cover other ways
    of building a model that will give us more flexibility and control over the layers
    of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Solving an XOR classification problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The XOR classification problem is a classic problem for analyzing the capacity
    of a model with regard to capturing the nonlinear decision boundary between two
    classes. We generate a toy dataset of 200 training examples with two features
    ![](img/B13208_14_027.png) drawn from a uniform distribution between ![](img/B13208_14_028.png).
    Then, we assign the ground truth label for training example *i* according to the
    following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_14_029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will use half of the data (100 training examples) for training and the remaining
    half for validation. The code for generating the data and splitting it into the
    training and validation datasets is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The code results in the following scatterplot of the training and validation
    examples, shown with different markers based on their class label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13208_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous subsection, we covered the essential tools that we need to
    implement a classifier in TensorFlow. We now need to decide what architecture
    we should choose for this task and dataset. As a general rule of thumb, the more
    layers we have, and the more neurons we have in each layer, the larger the capacity
    of the model will be. Here, the model capacity can be thought of as a measure
    of how readily the model can approximate complex functions. While having more
    parameters means the network can fit more complex functions, larger models are
    usually harder to train (and prone to overfitting). In practice, it is always
    a good idea to start with a simple model as a base line, for example, a single-layer
    NN like logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The total size of the parameters for this simple logistic regression model
    is 3: a weight matrix (or kernel) of size ![](img/B13208_14_030.png) and a bias
    vector of size 1\. After defining the model, we will compile the model and train
    it for 200 epochs using a batch size of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `model.fit()` returns a history of training epochs, which is useful
    for visual inspection after training. In the following code, we will plot the
    learning curves, including the training and validation loss, as well as their
    accuracies.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use the MLxtend library to visualize the validation data and the
    decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLxtend can be installed via `conda` or `pip` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will plot the training performance along with the decision
    region bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following figure, with three separate panels for the losses,
    accuracies, and the scatterplot of the validation examples, along with the decision
    boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13208_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, a simple model with no hidden layer can only derive a linear
    decision boundary, which is unable to solve the XOR problem. As a consequence,
    we can observe that the loss terms for both the training and the validation datasets
    are very high, and the classification accuracy is very low.
  prefs: []
  type: TYPE_NORMAL
- en: In order to derive a nonlinear decision boundary, we can add one or more hidden
    layers connected via nonlinear activation functions. The universal approximation
    theorem states that a feedforward NN with a single hidden layer and a relatively
    large number of hidden units can approximate arbitrary continuous functions relatively
    well. Thus, one approach for tackling the XOR problem more satisfactorily is to
    add a hidden layer and compare different numbers of hidden units until we observe
    satisfactory results on the validation dataset. Adding more hidden units would
    correspond to increasing the width of a layer.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can also add more hidden layers, which will make the model
    deeper. The advantage of making a network deeper rather than wider is that fewer
    parameters are required to achieve a comparable model capacity. However, a downside
    of deep (versus wide) models is that deep models are prone to vanishing and exploding
    gradients, which make them harder to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an exercise, try adding one, two, three, and four hidden layers, each with
    four hidden units. In the following example, we will take a look at the results
    of a feedforward NN with three hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can repeat the previous code for visualization, which produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13208_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we can see that the model is able to derive a nonlinear decision boundary
    for this data, and the model reaches 100 percent accuracy on the training dataset.
    The validation dataset's accuracy is 95 percent, which indicates that the model
    is slightly overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Making model building more flexible with Keras' functional API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, we used the Keras `Sequential` class to create a fully
    connected NN with multiple layers. This is a very common and convenient way of
    building models. However, it unfortunately doesn't allow us to create more complex
    models that have multiple input, output, or intermediate branches. That's where
    Keras' so-called functional API comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how the functional API can be used, we will implement the same
    architecture that we built using the objected-oriented (`Sequential`) approach
    in the previous section; however, this time, we will use the functional approach.
    In this approach, we first specify the input. Then, the hidden layers are constructed,
    with their outputs named `h1`, `h2`, and `h3`. For this problem, we use the output
    of each layer as the input to the succedent layer (note that if you are building
    more complex models that have multiple branches, this may not be the case, but
    it can still be done via the functional API). Finally, we specify the output as
    the final dense layer that receives `h3` as input. The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and training this model is similar to what we did previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Implementing models based on Keras' Model class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An alternative way to build complex models is by subclassing `tf.keras.Model`.
    In this approach, we create a new class derived from `tf.keras.Model` and define
    the function, `__init__()`, as a constructor. The `call()` method is used to specify
    the forward pass. In the constructor function, `__init__()`, we define the layers
    as attributes of the class so that they can be accessed via the `self` reference
    attribute. Then, in the `call()` method, we specify how these layers are to be
    used in the forward pass of the NN. The code for defining a new class that implements
    the previous model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we used the same output name, `h`, for all hidden layers. This makes
    the code more readable and easier to follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'A model class derived from `tf.keras.Model` through subclassing inherits general
    model attributes, such as `build()`, `compile()`, and `fit()`. Therefore, once
    we define an instance of this new class, we can compile and train it like any
    other model built by Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Writing custom Keras layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cases where we want to define a new layer that is not already supported by
    Keras, we can define a new class derived from the `tf.keras.layers.Layer` class.
    This is especially useful when designing a new layer or customizing an existing
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of implementing custom layers, let's consider a simple
    example. Imagine we want to define a new linear layer that computes ![](img/B13208_14_031.png),
    where ![](img/B13208_14_032.png) refers to a random variable as a noise variable.
    To implement this computation, we define a new class as a subclass of `tf.keras.layers.Layer`.
    For this new class, we have to define both the constructor `__init__()` method
    and the `call()` method. In the constructor, we define the variables and other
    required tensors for our customized layer. We have the option to create variables
    and initialize them in the constructor if the `input_shape` is given to the constructor.
    Alternatively, we can delay the variable initialization (for instance, if we do
    not know the exact input shape upfront) and delegate it to the `build()` method
    for late variable creation. In addition, we can define `get_config()` for serialization,
    which means that a model using our custom layer can be efficiently saved using
    TensorFlow's model saving and loading capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To look at a concrete example, we are going to define a new layer called `NoisyLinear`,
    which implements the computation ![](img/B13208_14_033.png), which was mentioned
    in the preceding paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Before we go a step further and use our custom `NoisyLinear` layer in a model,
    let's test it in the context of a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will define a new instance of this layer, initialize
    it by calling `.build()`, and execute it on an input tensor. Then, we will serialize
    it via `.get_config()` and restore the serialized object via `.from_config()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create a new model similar to the previous one for solving the
    XOR classification task. As before, we will use Keras'' `Sequential` class, but
    this time, we will use our `NoisyLinear` layer as the first hidden layer of the
    multilayer perceptron. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting figure will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13208_14_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, our goal was to learn how to define a new custom layer subclassed from
    `tf.keras.layers.Layer` and to use it as we would use any other standard Keras
    layer. Although, with this particular example, `NoisyLinear` did not help to improve
    the performance, please keep in mind that our objective was to mainly learn how
    to write a customized layer from scratch. In general, writing a new customized
    layer can be useful in other applications, for example, if you develop a new algorithm
    that depends on a new layer beyond the existing ones.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in this chapter, we have mostly focused on the low-level TensorFlow
    API. We used decorators to modify functions to compile the computational graphs
    explicitly for computational efficiency. Then, we worked with the Keras API and
    implemented feedforward NNs, to which we added customized layers. In this section,
    we will switch gears and work with TensorFlow Estimators. The `tf.estimator` API
    encapsulates the underlying steps in machine learning tasks, such as training,
    prediction (inference), and evaluation. Estimators are more encapsulated but also
    more scalable when compared to the previous approaches that we have covered in
    this chapter. Also, the `tf.estimator` API adds support for running models on
    multiple platforms without requiring major code changes, which makes them more
    suitable for the so-called "production phase" in industry applications. In addition,
    TensorFlow comes with a selection of off-the-shelf estimators for common machine
    learning and deep learning architectures that are useful for comparison studies,
    for example, to quickly assess whether a certain approach is applicable to a particular
    dataset or problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the remaining sections of this chapter, you will learn how to use such pre-made
    Estimators and how to create an Estimator from an existing Keras model. One of
    the essential elements of Estimators is defining the feature columns as a mechanism
    for importing data into an Estimator-based model, which we will cover in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Working with feature columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In machine learning and deep learning applications, we can encounter various
    different types of features: continuous, unordered categorical (nominal), and
    ordered categorical (ordinal). You will recall that in *Chapter 4*, *Building
    Good Training Datasets – Data Preprocessing*, we covered different types of features
    and learned how to handle each type. Note that while numeric data can be either
    continuous or discrete, in the context of the TensorFlow API, "numeric" data specifically
    refers to continuous data of the floating point type.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, feature sets are comprised of a mixture of different feature types.
    While TensorFlow Estimators were designed to handle all these different types
    of features, we must specify how each feature should be interpreted by the Estimator.
    For example, consider a scenario with a set of seven different features, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13208_14_06.png)'
  prefs: []
  type: TYPE_IMG
- en: The features shown in the figure (model year, cylinders, displacement, horsepower,
    weight, acceleration, and origin) were obtained from the Auto MPG dataset, which
    is a common machine learning benchmark dataset for predicting the fuel efficiency
    of a car in miles per gallon (MPG). The full dataset and its description are available
    from UCI's machine learning repository at [https://archive.ics.uci.edu/ml/datasets/auto+mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg).
  prefs: []
  type: TYPE_NORMAL
- en: We are going to treat five features from the Auto MPG dataset (number of cylinders,
    displacement, horsepower, weight, and acceleration) as "numeric" (here, continuous)
    features. The model year can be regarded as an ordered categorical (ordinal) feature.
    Lastly, the manufacturing origin can be regarded as an unordered categorical (nominal)
    feature with three possible discrete values, 1, 2, and 3, which correspond to
    the US, Europe, and Japan, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first load the data and apply the necessary preprocessing steps, such
    as partitioning the dataset into training and test datasets, as well as standardizing
    the continuous features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13208_14_07.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s group the rather fine-grained model year information into buckets
    to simplify the learning task for the model that we are going to train later.
    Concretely, we are going to assign each car into one of four "year" buckets, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_14_036.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the chosen intervals were selected arbitrarily to illustrate the
    concepts of "bucketing." In order to group the cars into these buckets, we will
    first define a numeric feature based on each original model year. Then, these
    numeric features will be passed to the `bucketized_column` function for which
    we will specify three interval cut-off values: [73, 76, 79]. The specified values
    include the right cut-off value. These cut-off values are used to specify half-closed
    intervals, for instance, ![](img/B13208_14_037.png), ![](img/B13208_14_038.png),
    ![](img/B13208_14_039.png), and ![](img/B13208_14_040.png). The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: For consistency, we added this bucketized feature column to a Python list, even
    though the list consists of only one entry. In the following steps, we will merge
    this list with the lists made from other features, which will then be provided
    as input to the TensorFlow Estimator-based model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will proceed with defining a list for the unordered categorical feature,
    `Origin`. In TensorFlow, there are different ways of creating a categorical feature
    column. If the data contains the category names (for example, in string format
    like "US," "Europe," and "Japan"), then we can use `tf.feature_column.categorical_column_with_vocabulary_list`
    and provide a list of unique, possible category names as input. If the list of
    possible categories is too large, for example, in a typical text analysis context,
    then we can use `tf.feature_column.categorical_column_with_vocabulary_file` instead.
    When using this function, we simply provide a file that contains all the categories/words
    so that we do not have to store a list of all possible words in memory. Moreover,
    if the features are already associated with an index of categories in the range
    [0, `num_categories`), then we can use the `tf.feature_column.categorical_column_with_identity`
    function. However, in this case, the feature `Origin` is given as integer values
    1, 2, 3 (as opposed to 0, 1, 2), which does not match the requirement for categorical
    indexing, as it expects the indices to start from 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code example, we will proceed with the vocabulary list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Certain Estimators, such as `DNNClassifier` and `DNNRegressor`, only accept
    so-called "dense columns." Therefore, the next step is to convert the existing
    categorical feature column to such a dense column. There are two ways to do this:
    using an embedding column via `embedding_column` or an indicator column via `indicator_column`.
    An indicator column converts the categorical indices to one-hot encoded vectors,
    for example, index 0 will be encoded as [1, 0, 0], index 1 will be encoded as
    [0, 1, 0], and so on. On the other hand, the embedding column maps each index
    to a vector of random number of the type `float`, which can be trained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the number of categories is large, using the embedding column with fewer
    dimensions than the number of categories can improve the performance. In the following
    code snippet, we will use the indicator column approach on the categorical feature
    in order to convert it into the dense format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have covered the most common approaches for creating feature
    columns that can be used with TensorFlow Estimators. However, there are several
    additional feature columns that we haven't discussed, including hashed columns
    and crossed columns. More information about these other feature columns can be
    found in the official TensorFlow documentation at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning with pre-made Estimators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, after constructing the mandatory feature columns, we can finally utilize
    TensorFlow''s Estimators. Using pre-made Estimators can be summarized in four
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Define an input function for data loading
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the dataset into feature columns
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate an Estimator (use a pre-made Estimator or create a new one, for
    example, by converting a Keras model into an Estimator)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the Estimator methods `train()`, `evaluate()`, and `predict()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continuing with the Auto MPG example from the previous section, we will apply
    these four steps to illustrate how we can use Estimators in practice. For the
    first step, we need to define a function that processes the data and returns a
    TensorFlow dataset consisting of a tuple that contains the input features and
    the labels (ground truth MPG values). Note that the features must be in a dictionary
    format, and the keys of the dictionary must match the feature columns' names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the first step, we will define the input function for the training
    data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we used `dict(train_x)` in this function to convert the pandas
    `DataFrame` object into a Python dictionary. Let''s load a batch from this dataset
    to see how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to define an input function for the test dataset that will be
    used for evaluation after model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, moving on to step 2, we need to define the feature columns. We have already
    defined a list containing the continuous features, a list for the bucketized feature
    column, and a list for the categorical feature column. We can now concatenate
    these individual lists to a single list containing all feature columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'For step 3, we need to instantiate a new Estimator. Since predicting MPG values
    is a typical regression problem, we will use `tf.estimator.DNNRegressor`. When
    instantiating the regression Estimator, we will provide the list of feature columns
    and specify the number of hidden units that we want to have in each hidden layer
    using the argument `hidden_units`. Here, we will use two hidden layers, where
    the first hidden layer has 32 units and the second hidden layer has 10 units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The other argument, `model_dir`, that we have provided specifies the directory
    for saving model parameters. One of the advantages of Estimators is that they
    automatically checkpoint the model during training, so that in case the training
    of the model crashes for an unexpected reason (like power failure), we can easily
    load the last saved checkpoint and continue training from there. The checkpoints
    will also be saved in the directory specified by `model_dir`. If we do not specify
    the `model_dir` argument, the Estimator will create a random temporary folder
    (for example, in the Linux operating system, a random folder in the `/tmp/` directory
    will be created), which will be used for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'After these three basic setup steps, we can finally use the Estimator for training,
    evaluation, and, eventually, prediction. The regressor can be trained by calling
    the `train()` method, for which we require the previously defined input function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `.train()` will automatically save the checkpoints during the training
    of the model. We can then reload the last checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in order to evaluate the predictive performance of the trained model,
    we can use the `evaluate()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to predict the target values on new data points, we can use the `predict()`
    method. For the purposes of this example, suppose that the test dataset represents
    a dataset of new, unlabeled data points in a real-world application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in a real-world prediction task, the input function will only need
    to return a dataset consisting of features, assuming that the labels are not available.
    Here, we will simply use the same input function that we used for evaluation to
    get the predictions for each example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'While the preceding code snippets conclude the illustration of the four steps
    that are required for using pre-made Estimators, for practice, let''s take a look
    at another pre-made Estimator: the boosted tree regressor, `tf.estimator.BoostedTreeRegressor`.
    Since, the input functions and the feature columns are already built, we just
    need to repeat steps 3 and 4\. For step 3, we will create an instance of `BoostedTreeRegressor`
    and configure it to have 200 trees.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision tree boosting**'
  prefs: []
  type: TYPE_NORMAL
- en: We already covered the ensemble algorithms, including boosting, in *Chapter
    7,* *Combining Different Models for Ensemble Learning*. The boosted tree algorithm
    is a special family of boosting algorithms that is based on the optimization of
    an arbitrary loss function. Feel free to visit [https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the boosted tree regressor achieves lower average loss than
    the `DNNRegressor`. For a small dataset like this, this is expected.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered the essential steps for using TensorFlow's Estimators
    for regression. In the next subsection, we will take a look at a typical classification
    example using Estimators.
  prefs: []
  type: TYPE_NORMAL
- en: Using Estimators for MNIST handwritten digit classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this classification problem, we are going to use the `DNNClassifier` Estimator
    provided by TensorFlow, which lets us implement a multilayer perceptron very conveniently.
    In the previous section, we covered the four essential steps for using the pre-made
    Estimators in detail, which we will need to repeat in this section. First, we
    are going to import the `tensorflow_datasets` (`tfds`) submodule, which we can
    use to load the MNIST dataset and specify the hyperparameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimator API and graph issues**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since parts of TensorFlow 2.0 are still a bit rough around the edges, you may
    encounter the following issue when executing the next code block: `RuntimeError:
    Graph is finalized and cannot be modified.` Currently, there is no good solution
    for this issue, and a suggested workaround is to restart your Python, IPython,
    or Jupyter Notebook session before executing the next code block.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The setup step includes loading the dataset and specifying hyperparameters
    (`BUFFER_SIZE` for shuffling the dataset, `BATCH_SIZE` for the size of mini-batches,
    and the number of training epochs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Note that `steps_per_epoch` determines the number of iterations in each epoch,
    which is needed for infinitely repeated datasets (as discussed in *Chapter 13*,
    *Parallelizing Neural Network Training with TensorFlow*. Next, we will define
    a helper function that will preprocess the input image and its label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the input image is originally of the type `''uint8''` (in the range [0,
    255]), we will use `tf.image.convert_image_dtype()` to convert its type to `tf.float32`
    (and thereby, within the range [0, 1]):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 1**: Define two input functions (one for training and one for evaluation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the dictionary of features has only one key, `'image-pixels'`. We
    will use this key in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**: Define the feature columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Note that here, we defined the feature columns of size 784 (that is, ![](img/B13208_14_041.png)),
    which is the size of the input MNIST images after they are flattened.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**: Create a new Estimator. Here, we specify two hidden layers: 32
    units in the first hidden layer and 16 units in the second.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also specify the number of classes (remember that MNIST consists of 10 different
    digits, 0-9) using the argument `n_classes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4**: Use the Estimator for training, evaluation, and prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: So far, you have learned how to use pre-made Estimators and apply them for preliminary
    assessment to see, for example, whether an existing model is suitable for a particular
    problem. Besides using pre-made Estimators, we can also create an Estimator by
    converting a Keras model to an Estimator, which we will do in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom Estimator from an existing Keras model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Converting a Keras model to an Estimator is useful in academia as well as industry
    for cases where you have developed a model and want to publish it or share the
    model with other members in your organization. Such a conversion allows us to
    access the strengths of Estimators, such as distributed training and automatic
    checkpointing. In addition, it will make it easy for others to use this model,
    and particularly to avoid confusions in interpreting the input features by specifying
    the feature columns and the input function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn how we can create our own Estimator from a Keras model, we will work
    with the previous XOR problem. First, we will regenerate the data and split it
    into training and validation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also build a Keras model that we want to convert to an Estimator later.
    We will define the model using the `Sequential` class as before. This time, we
    will also add an input layer defined as `tf.keras.layers.Input` to give a name
    to the input to this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will go through the four steps that we described in the previous subsection.
    Steps 1, 2, and 4 will be the same as the ones we used with the pre-made estimators.
    Note that the key name for the input features that we use in steps 1 and 2 must
    match with what we defined in the input layer of our model. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'For step 3, we will convert the model to an Estimator using `tf.keras.estimator.model_to_estimator`
    instead of instantiating one of the pre-made Estimators. Before converting the
    model, we first need to compile it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in step 4, we can train our model using the Estimator and evaluate
    it on the validation dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, converting a Keras model to an Estimator is very straightforward.
    Doing this allows us to easily benefit from the various Estimator strengths, such
    as distributed training and automatically saving the checkpoints during training.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered TensorFlow's most essential and useful features.
    We started by discussing the migration from TensorFlow v1.x to v2\. In particular,
    we used TensorFlow's dynamic computation graph approach, the so-called eager execution
    mode, which makes implementing computations more convenient compared to using
    static graphs. We also covered the semantics of defining TensorFlow `Variable`
    objects as model parameters, annotating Python functions using the `tf.function`
    decorator to improve computational efficiency via graph compilation.
  prefs: []
  type: TYPE_NORMAL
- en: After we considered the concept of computing partial derivatives and gradients
    of arbitrary functions, we covered the Keras API in more detail. It provides us
    with a user-friendly interface for building more complex deep NN models. Finally,
    we utilized TensorFlow's `tf.estimator` API to provide a consistent interface
    that is typically preferred in production environments. We concluded this chapter
    by converting a Keras model into a custom Estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the core mechanics of TensorFlow, the next chapter
    will introduce the concept behind **convolutional neural network** (**CNN**) architectures
    for deep learning. CNNs are powerful models and have shown great performance in
    the field of computer vision.
  prefs: []
  type: TYPE_NORMAL

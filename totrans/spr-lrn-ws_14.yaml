- en: 6\. Ensemble Modeling
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.01: Stacking with Standalone and Ensemble Algorithms'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import seaborn as sns
  prefs: []
  type: TYPE_NORMAL
- en: '%matplotlib inline'
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.metrics import mean_absolute_error
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.model_selection import KFold
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.linear_model import LinearRegression
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.tree import DecisionTreeRegressor
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.neighbors import KNeighborsRegressor
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.ensemble import GradientBoostingRegressor, \
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestRegressor
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the data:'
  prefs: []
  type: TYPE_NORMAL
- en: data = pd.read_csv('boston_house_prices.csv')
  prefs: []
  type: TYPE_NORMAL
- en: data.head()
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The preceding code snippet assumes that the dataset is presented in the same
    folder as that of the exercise notebook. However, if your dataset is present in
    the Datasets folder, you need to use the following code: data = pd.read_csv(''../Datasets/boston_house_prices.csv'')'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15: Top rows of the Boston housing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-J3QAVKGH.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.15: Top rows of the Boston housing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preprocess the dataset to remove null values to prepare the data for modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: check how many columns have less than 10 % null data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: perc_missing = data.isnull().mean()*100
  prefs: []
  type: TYPE_NORMAL
- en: cols = perc_missing[perc_missing < 10].index.tolist()
  prefs: []
  type: TYPE_NORMAL
- en: cols
  prefs: []
  type: TYPE_NORMAL
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16: Number of columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-ZQI9Z8VR.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.16: Number of columns'
  prefs: []
  type: TYPE_NORMAL
- en: 'And then fill in the missing values, if any:'
  prefs: []
  type: TYPE_NORMAL
- en: data_final = data.fillna(-1)
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the dataset into train and validation DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: train, val = train, val = train_test_split(data_final, \
  prefs: []
  type: TYPE_NORMAL
- en: test_size=0.2, \
  prefs: []
  type: TYPE_NORMAL
- en: random_state=11)
  prefs: []
  type: TYPE_NORMAL
- en: x_train = train.drop(columns=['PRICE'])
  prefs: []
  type: TYPE_NORMAL
- en: y_train = train['PRICE'].values
  prefs: []
  type: TYPE_NORMAL
- en: x_val = val.drop(columns=['PRICE'])
  prefs: []
  type: TYPE_NORMAL
- en: y_val = val['PRICE'].values
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize dictionaries in which to store the train and validation MAE values:'
  prefs: []
  type: TYPE_NORMAL
- en: train_mae_values, val_mae_values = {}, {}
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a decision tree (dt) model with the following hyperparameters and save
    the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: 'dt_params = {''criterion'': ''mae'', ''min_samples_leaf'': 15, \'
  prefs: []
  type: TYPE_NORMAL
- en: '''random_state'': 11}'
  prefs: []
  type: TYPE_NORMAL
- en: dt = DecisionTreeRegressor(**dt_params)
  prefs: []
  type: TYPE_NORMAL
- en: dt.fit(x_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: dt_preds_train = dt.predict(x_train)
  prefs: []
  type: TYPE_NORMAL
- en: dt_preds_val = dt.predict(x_val)
  prefs: []
  type: TYPE_NORMAL
- en: train_mae_values['dt'] = mean_absolute_error(y_true=y_train, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=dt_preds_train)
  prefs: []
  type: TYPE_NORMAL
- en: val_mae_values['dt'] = mean_absolute_error(y_true=y_val, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=dt_preds_val)
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a k-nearest neighbours (knn) model with the following hyperparameters
    and save the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: 'knn_params = {''n_neighbors'': 5}'
  prefs: []
  type: TYPE_NORMAL
- en: knn = KNeighborsRegressor(**knn_params)
  prefs: []
  type: TYPE_NORMAL
- en: knn.fit(x_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: knn_preds_train = knn.predict(x_train)
  prefs: []
  type: TYPE_NORMAL
- en: knn_preds_val = knn.predict(x_val)
  prefs: []
  type: TYPE_NORMAL
- en: train_mae_values['knn'] = mean_absolute_error(y_true=y_train, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=knn_preds_train)
  prefs: []
  type: TYPE_NORMAL
- en: val_mae_values['knn'] = mean_absolute_error(y_true=y_val, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=knn_preds_val)
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a random forest (rf) model with the following hyperparameters and save
    the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: 'rf_params = {''n_estimators'': 20, ''criterion'': ''mae'', \'
  prefs: []
  type: TYPE_NORMAL
- en: '''max_features'': ''sqrt'', ''min_samples_leaf'': 10, \'
  prefs: []
  type: TYPE_NORMAL
- en: '''random_state'': 11, ''n_jobs'': -1}'
  prefs: []
  type: TYPE_NORMAL
- en: rf = RandomForestRegressor(**rf_params)
  prefs: []
  type: TYPE_NORMAL
- en: rf.fit(x_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: rf_preds_train = rf.predict(x_train)
  prefs: []
  type: TYPE_NORMAL
- en: rf_preds_val = rf.predict(x_val)
  prefs: []
  type: TYPE_NORMAL
- en: train_mae_values['rf'] = mean_absolute_error(y_true=y_train, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=rf_preds_train)
  prefs: []
  type: TYPE_NORMAL
- en: val_mae_values['rf'] = mean_absolute_error(y_true=y_val, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=rf_preds_val)
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a gradient boosting regression (gbr) model with the following hyperparameters
    and save the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: 'gbr_params = {''n_estimators'': 20, ''criterion'': ''mae'', \'
  prefs: []
  type: TYPE_NORMAL
- en: '''max_features'': ''sqrt'', ''min_samples_leaf'': 10, \'
  prefs: []
  type: TYPE_NORMAL
- en: '''random_state'': 11}'
  prefs: []
  type: TYPE_NORMAL
- en: gbr = GradientBoostingRegressor(**gbr_params)
  prefs: []
  type: TYPE_NORMAL
- en: gbr.fit(x_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: gbr_preds_train = gbr.predict(x_train)
  prefs: []
  type: TYPE_NORMAL
- en: gbr_preds_val = gbr.predict(x_val)
  prefs: []
  type: TYPE_NORMAL
- en: train_mae_values['gbr'] = mean_absolute_error(y_true=y_train, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=gbr_preds_train)
  prefs: []
  type: TYPE_NORMAL
- en: val_mae_values['gbr'] = mean_absolute_error(y_true=y_val, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=gbr_preds_val)
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the training and validation datasets, with the four meta estimators
    having the same hyperparameters that were used in the previous steps. First, we
    build the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: 'num_base_predictors = len(train_mae_values) # 4'
  prefs: []
  type: TYPE_NORMAL
- en: x_train_with_metapreds = np.zeros((x_train.shape[0], \
  prefs: []
  type: TYPE_NORMAL
- en: x_train.shape[1]+num_base_predictors))
  prefs: []
  type: TYPE_NORMAL
- en: x_train_with_metapreds[:, :-num_base_predictors] = x_train
  prefs: []
  type: TYPE_NORMAL
- en: x_train_with_metapreds[:, -num_base_predictors:] = -1
  prefs: []
  type: TYPE_NORMAL
- en: kf = KFold(n_splits=5, random_state=11)
  prefs: []
  type: TYPE_NORMAL
- en: 'for train_indices, val_indices in kf.split(x_train):'
  prefs: []
  type: TYPE_NORMAL
- en: kfold_x_train, kfold_x_val = x_train.iloc[train_indices], \
  prefs: []
  type: TYPE_NORMAL
- en: x_train.iloc[val_indices]
  prefs: []
  type: TYPE_NORMAL
- en: kfold_y_train, kfold_y_val = y_train[train_indices], \
  prefs: []
  type: TYPE_NORMAL
- en: y_train[val_indices]
  prefs: []
  type: TYPE_NORMAL
- en: predictions = []
  prefs: []
  type: TYPE_NORMAL
- en: dt = DecisionTreeRegressor(**dt_params)
  prefs: []
  type: TYPE_NORMAL
- en: dt.fit(kfold_x_train, kfold_y_train)
  prefs: []
  type: TYPE_NORMAL
- en: predictions.append(dt.predict(kfold_x_val))
  prefs: []
  type: TYPE_NORMAL
- en: knn = KNeighborsRegressor(**knn_params)
  prefs: []
  type: TYPE_NORMAL
- en: knn.fit(kfold_x_train, kfold_y_train)
  prefs: []
  type: TYPE_NORMAL
- en: predictions.append(knn.predict(kfold_x_val))
  prefs: []
  type: TYPE_NORMAL
- en: gbr = GradientBoostingRegressor(**gbr_params)
  prefs: []
  type: TYPE_NORMAL
- en: rf.fit(kfold_x_train, kfold_y_train)
  prefs: []
  type: TYPE_NORMAL
- en: predictions.append(rf.predict(kfold_x_val))
  prefs: []
  type: TYPE_NORMAL
- en: gbr = GradientBoostingRegressor(**gbr_params)
  prefs: []
  type: TYPE_NORMAL
- en: gbr.fit(kfold_x_train, kfold_y_train)
  prefs: []
  type: TYPE_NORMAL
- en: predictions.append(gbr.predict(kfold_x_val))
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, preds in enumerate(predictions):'
  prefs: []
  type: TYPE_NORMAL
- en: x_train_with_metapreds[val_indices, -(i+1)] = preds
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: x_val_with_metapreds = np.zeros((x_val.shape[0], \
  prefs: []
  type: TYPE_NORMAL
- en: x_val.shape[1]+num_base_predictors))
  prefs: []
  type: TYPE_NORMAL
- en: x_val_with_metapreds[:, :-num_base_predictors] = x_val
  prefs: []
  type: TYPE_NORMAL
- en: x_val_with_metapreds[:, -num_base_predictors:] = -1
  prefs: []
  type: TYPE_NORMAL
- en: predictions = []
  prefs: []
  type: TYPE_NORMAL
- en: dt = DecisionTreeRegressor(**dt_params)
  prefs: []
  type: TYPE_NORMAL
- en: dt.fit(x_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: predictions.append(dt.predict(x_val))
  prefs: []
  type: TYPE_NORMAL
- en: knn = KNeighborsRegressor(**knn_params)
  prefs: []
  type: TYPE_NORMAL
- en: knn.fit(x_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: predictions.append(knn.predict(x_val))
  prefs: []
  type: TYPE_NORMAL
- en: gbr = GradientBoostingRegressor(**gbr_params)
  prefs: []
  type: TYPE_NORMAL
- en: rf.fit(x_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: predictions.append(rf.predict(x_val))
  prefs: []
  type: TYPE_NORMAL
- en: gbr = GradientBoostingRegressor(**gbr_params)
  prefs: []
  type: TYPE_NORMAL
- en: gbr.fit(x_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: predictions.append(gbr.predict(x_val))
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, preds in enumerate(predictions):'
  prefs: []
  type: TYPE_NORMAL
- en: x_val_with_metapreds[:, -(i+1)] = preds
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a linear regression (lr) model as the stacked model:'
  prefs: []
  type: TYPE_NORMAL
- en: lr = LinearRegression(normalize=True)
  prefs: []
  type: TYPE_NORMAL
- en: lr.fit(x_train_with_metapreds, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: lr_preds_train = lr.predict(x_train_with_metapreds)
  prefs: []
  type: TYPE_NORMAL
- en: lr_preds_val = lr.predict(x_val_with_metapreds)
  prefs: []
  type: TYPE_NORMAL
- en: train_mae_values['lr'] = mean_absolute_error(y_true=y_train, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=lr_preds_train)
  prefs: []
  type: TYPE_NORMAL
- en: val_mae_values['lr'] = mean_absolute_error(y_true=y_val, \
  prefs: []
  type: TYPE_NORMAL
- en: y_pred=lr_preds_val)
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualize the train and validation errors for each individual model and the
    stacked model:'
  prefs: []
  type: TYPE_NORMAL
- en: mae_scores = pd.concat([pd.Series(train_mae_values, name='train'), \
  prefs: []
  type: TYPE_NORMAL
- en: pd.Series(val_mae_values, name='val')], \
  prefs: []
  type: TYPE_NORMAL
- en: axis=1)
  prefs: []
  type: TYPE_NORMAL
- en: mae_scores
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17: Values of training and validation errors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-YV2QPP1E.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.17: Values of training and validation errors'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, plot the MAE scores on a bar plot using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: mae_scores.plot(kind='bar', figsize=(10,7))
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel('MAE')
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel('Model')
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: 'The final output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18: Visualization of training and validation errors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image-2OFCL0GD.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.18: Visualization of training and validation errors'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to https://packt.live/3fNqtMG.
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at https://packt.live/2Yn2VIl. You must
    execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL

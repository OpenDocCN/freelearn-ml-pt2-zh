- en: '4\. Supervised Learning Algorithms: Predicting Annual Income'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take a look at three different supervised learning
    algorithms used for classification. We will also solve a supervised learning classification
    problem using these algorithms and perform error analysis by comparing the results
    of the three different algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to identify the algorithm with
    the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the key steps involved in working with a
    supervised learning data problem. Those steps aim to create high-performing algorithms,
    as explained in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on applying different algorithms to a real-life dataset,
    with the underlying objective of applying the steps that we learned previously
    to choose the best-performing algorithm for the case study. Considering this,
    you will pre-process and analyze a dataset, and then create three models using
    different algorithms. These models will be compared to one another in order to
    measure their performance.
  prefs: []
  type: TYPE_NORMAL
- en: The Census Income dataset that we'll be using contains demographical and financial
    information, which can be used to try and predict the level of income of an individual.
    By creating a model capable of predicting this outcome for new observations, it
    will be possible to determine whether a person can be pre-approved to receive
    a loan.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-life applications are crucial for cementing knowledge. Therefore, this
    chapter consists of a real-life case study involving a classification task, where
    the key steps that you learned about in the previous chapter will be applied in
    order to select the best performing model.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this, the Census Income dataset will be used, which is available
    at the UC Irvine Machine Learning Repository.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that will be used in the following section, as well as in this chapter's
    activities, can be found in this book's GitHub repository at [https://packt.live/2xUGShx](https://packt.live/2xUGShx).
  prefs: []
  type: TYPE_NORMAL
- en: 'Citation: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the dataset from this book''s GitHub repository. Alternatively,
    toÂ download the dataset from the original source, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the following link: [http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, click the `Data Folder` link.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this chapter, the data available under `adult.data` will be used. Once you
    click this link, the download will be triggered. Save it as a `.csv` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Open the file and add header names over each column to make pre-processing easier.
    For instance, the first column should have the header `Age`, as per the features
    available in the dataset. These can be seen in the preceding link, under `Attribute
    Information`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Understanding the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build a model that fits the data accurately, it is important to understand
    the different details of the dataset, as mentioned in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the data that''s available is revised to understand the size of the
    dataset and the type of supervised learning task to be developed: classification
    or regression. Next, the purpose of the study should be clearly defined, even
    if it is obvious. For supervised learning, the purpose is closely linked to the
    class labels. Finally, each feature is analyzed so that we can be aware of their
    types for pre-processing purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: The Census Income dataset is a collection of demographical data on adults, which
    is an extract from the 1994 Census Database from the United States. For this chapter,
    only the data available under the `adult.data` link will be used. The dataset
    consists of 32,561 instances, 14 features, and 1 binary class label. Considering
    that the class label is discrete, our task is to achieve the classification of
    the different observations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The following exploration of the dataset does not require coding of any sort,
    but rather a simple evaluation by opening the dataset in Excel or a similar program.
  prefs: []
  type: TYPE_NORMAL
- en: Through a quick evaluation of the data, it is possible to observe that some
    features present missing values in the form of a question mark. This is common
    when dealing with datasets that are available online and should be handled by
    replacing the symbol with an empty value (not a space). Other common forms of
    missing values are the `NULL` value and a dash.
  prefs: []
  type: TYPE_NORMAL
- en: 'To edit missing value symbols in Excel, use the **Replace** functionality,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`?`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replace with**: Leave it blank (do not enter a space).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, once we import the dataset into the code, NumPy will be able to find
    the missing values so that it can handle them.
  prefs: []
  type: TYPE_NORMAL
- en: The prediction task for this dataset involves determining whether a person earns
    over 50K dollars a year. According to this, the two possible outcome labels are
    `>50K` (greater than 50K) or `<=50K` (less than, or equal to 50K).
  prefs: []
  type: TYPE_NORMAL
- en: 'A brief explanation of each of the features in the dataset is shown in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Dataset feature analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1: Dataset feature analysis'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Publisher''s Note: Gender and race would have impacted the earning potential
    of an individual at the date this study was conducted. However, for the purpose
    of this chapter, we have decided to exclude these categories from our exercises
    and activities.'
  prefs: []
  type: TYPE_NORMAL
- en: We recognize that due to biases and discriminatory practices, it is impossible
    to separate issues such as gender, race, and educational and vocational opportunities.
    The removal of certain features from our dataset in the pre-processing stage of
    these exercises is not intended to ignore the issues, nor the valuable work undertaken
    by organizations and individuals working in the civil rights sphere.
  prefs: []
  type: TYPE_NORMAL
- en: We strongly recommend that you consider the sociopolitical impacts of data and
    the way it is used, and also consider how past prejudices can be perpetuated by
    using historical data to introduce bias into new algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding table, it is possible to conclude the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Five features are not relevant to the study: `fnlwgt`, `education`, `relationship`,
    `race`, and `sex`. These features must be deleted from the dataset before we proceed
    with pre-processing and training the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of the remaining features, four are presented as qualitative values. Considering
    that many algorithms do not take qualitative features into account, the values
    should be represented in numerical form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the concepts that we learned about in the previous chapters, the preceding
    statements, as well as the pre-processing process for handling outliers and missing
    values, can be taken care of. The following steps explain the logic of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to import the dataset and drop the features that are irrelevant to
    the study.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should check for missing values. Considering the feature with the most missing
    values (`occupation`, with 1,843 missing values), there will be no need to delete
    or replace the missing values as they represent only 5% or less of the entire
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You must convert the qualitative values into their numeric representations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should check for outliers. Upon using three standard deviations to detect
    outliers, the feature with the maximum number of outliers is `capital-loss`, which
    contains 1,470 outliers. Again, the outliers represent less than 5% of the entire
    dataset, meaning they can be left untouched without impacting the result of the
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The preceding process will convert the original dataset into a new dataset
    with 32,561 instances (since no instances were deleted), but with 9 features and
    a class label. All values should be in their numerical forms. Save the pre-processed
    dataset into a file using pandas'' `to_csv` function, as per the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet takes the pre-processed data stored in a Pandas DataFrame
    and saves it into a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you perform the preceding pre-processing steps, as this is the
    dataset that will be used for training the models in the different activities
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To review these steps, visit the GitHub repository of this book, under the folder
    named `Chapter04`, in the file named `Census income dataset preprocessing`.
  prefs: []
  type: TYPE_NORMAL
- en: The NaÃ¯ve Bayes Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**NaÃ¯ve Bayes** is a classification algorithm based on **Bayes'' theorem**
    that *naÃ¯vely* assumes independence between features and assigns the same weight
    (degree of importance) to all features. This means that the algorithm assumes
    that no single feature correlates to or affects another. For example, although
    weight and height are somehow correlated when predicting a person''s age, the
    algorithm assumes that each feature is independent. Additionally, the algorithm
    considers all features equally important. For instance, even though an education
    degree may influence the earnings of a person to a greater degree than the number
    of children the person has, the algorithm still considers both features equally
    important.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes'' theorem is a mathematical formula that calculates conditional probabilities.
    To learn more about this theorem, visit the following URL: [https://plato.stanford.edu/entries/bayes-theorem/](https://plato.stanford.edu/entries/bayes-theorem/).'
  prefs: []
  type: TYPE_NORMAL
- en: Although real-life datasets contain features that are not equally important,
    nor independent, this algorithm is popular among scientists as it performs surprisingly
    well on large datasets. Also, due to the simplistic approach of the algorithm,
    it runs quickly, thus allowing it to be applied to problems that require predictions
    in real-time. Moreover, it is frequently used for text classification as it commonly
    outperforms more complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: How Does the NaÃ¯ve Bayes Algorithm Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The algorithm converts the input data into a summary of occurrences of each
    class label against each feature, which is then used to calculate the likelihood
    of one event (a class label), given a combination of features. Finally, this likelihood
    is normalized against the likelihood of the other class labels. The result is
    the probability of an instance belonging to each class label. The sum of the probabilities
    must be one, and the class label with a higher probability is the one that the
    algorithm chooses as the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take, for example, the data presented in the following tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Table A - Input data and Table B - Occurrence count](img/B15781_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Table A - Input data and Table B - Occurrence count'
  prefs: []
  type: TYPE_NORMAL
- en: Table A represents the data that is fed to the algorithm to build the model.
    Table B refer to the occurrence count that the algorithm uses implicitly to calculate
    the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the likelihood of an event occurring when given a set of features,
    the algorithm multiplies the probability of the event occurring, given each individual
    feature, by the probability of the occurrence of the event, independent of the
    rest of the features, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, *A*1 refers to an event (one of the class labels) and *E* represents the
    set of features, where *E*1 is the first feature and *E*n is the last feature
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The multiplication of these probabilities can only be made by assuming independence
    between features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding equation is calculated for all possible outcomes (all class labels),
    and then the normalized probability of each outcome is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: Formula to calculate normalized probability'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.3: Formula to calculate normalized probability'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example in *Figure 4.2*, given a new instance with weather equal to
    *sunny* and temperature equal to *cool*, the calculation of probabilities is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Calculation of the likelihood and probabilities for the example
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.4: Calculation of the likelihood and probabilities for the example
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the preceding equations, it is possible to conclude that the prediction
    should be *yes*.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to mention that for continuous features, the summary of occurrences
    is done by creating ranges. For instance, for a feature of price, the algorithm
    may count the number of instances with prices below 100K, as well as the instances
    with prices above 100K.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the algorithm may encounter some issues if one value of a feature
    is never associated with one of the outcomes. This is an issue mainly because
    the probability of the outcome given that feature will be zero, which influences
    the entire calculation. In the preceding example, for predicting the outcome of
    an instance with weather equal to *mild* and temperature equal to *cool*, the
    probability of *no*, given the set of features will be equal to zero, considering
    that the probability of *no*, given *mild* weather, computes to zero, since there
    are no occurrences of *mild* weather when the outcome is *no*.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, the **Laplace estimator** technique should be used. Here, the
    fractions representing the probability of the occurrence of an event given a feature,
    *P[A|E*1*]*, are modified by adding 1 to the numerator while also adding the number
    of possible values of that feature to the denominator.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, to perform a prediction for a new instance with weather equal
    to *mild* and temperature equal to *cool* using the Laplace estimator, this would
    be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: Calculation of the likelihood and probability using the Laplace
    estimator for the example dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.5: Calculation of the likelihood and probability using the Laplace
    estimator for the example dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the fraction that calculates the occurrences of *yes*, given *mild* weather,
    goes from 2/7 to 3/10, as a result of the addition of 1 to the numerator and 3
    (for *sunny*, *mild*, and *rainy*) to the denominator. The same goes for the other
    fractions that calculate the probability of the event, given a feature. Note that
    the fraction that calculates the probability of the event occurring independently
    of any feature is left unaltered.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, as you have learned so far, the scikit-learn library allows you
    to train models and then use them for predictions, without needing to hardcode
    the math.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.01: Applying the NaÃ¯ve Bayes Algorithm'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s apply the NaÃ¯ve Bayes algorithm to a Fertility dataset, which aims
    to determine whether the fertility level of an individual has been affected by
    their demographics, their environmental conditions, and their previous medical
    conditions. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For the exercises and activities within this chapter, you will need to have
    Python 3.7, NumPy, Jupyter, Pandas, and scikit-learn installed on your system.
  prefs: []
  type: TYPE_NORMAL
- en: Download the Fertility dataset from [http://archive.ics.uci.edu/ml/datasets/Fertility](http://archive.ics.uci.edu/ml/datasets/Fertility).
    Go to the link and click on `Data Folder`. Click on `fertility_Diagnosis.txt`,
    which will trigger the download. Save it as a `.csv` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The dataset is also available in this book's GitHub repository at [https://packt.live/39SsSSN](https://packt.live/39SsSSN).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It was downloaded from the UC Irvine Machine Learning Repository: David Gil,
    Jose Luis Girela, Joaquin De Juan, M. Jose Gomez-Torres, and Magnus Johnsson.
    *Predicting seminal quality with artificial intelligence methods*. Expert Systems
    with Applications.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook to implement this exercise. Import pandas, as well
    as the `GaussianNB` class from scikit-learn''s `naive_bayes` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the `.csv` file that you downloaded in the first step. Make sure that
    you add the `header` argument equal to `None` to the `read_csv` function, considering
    that the dataset does not contain a header row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into `X` and `Y`, considering that the class label is found
    under the column with an index equal to 9\. Use the following code to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `GaussianNB` class that we imported previously. Next, use the
    `fit` method to train the model using `X` and `Y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from running this script is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This states that the instantiation of the class was successful. The information
    inside the parentheses represents the values used for the arguments that the class
    accepts, which are the hyperparameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For instance, for the `GaussianNB` class, it is possible to set the prior probabilities
    to consider for each class label and a smoothing argument that stabilizes variance.
    Nonetheless, the model was initialized without setting any arguments, which means
    that it will use the default values for each argument, which is `None` for the
    case of `priors` and `1e-09` for the smoothing hyperparameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, perform a prediction using the model that you trained before, for
    a new instance with the following values for each feature: `â0.33`, `0.69`, `0`,
    `1`, `1`, `0`, `0.8`, `0`, `0.88`. Use the following code to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we feed the values inside of double square brackets, considering that
    the `predict` function takes in the values for prediction as an array of arrays,
    where the first set of arrays corresponds to the list of new instances to predict
    and the second array refers to the list of features for each instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output from the preceding code snippet is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The predicted class for that subject is equal to `N`, which means that the fertility
    of the subject has not been affected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Y2wW0c](https://packt.live/2Y2wW0c).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3e40LTt](https://packt.live/3e40LTt).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have successfully trained a NaÃ¯ve Bayes model and performed prediction on
    a new observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.01: Training a NaÃ¯ve Bayes Model for Our Census Income Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test different classification algorithms on a real-life dataset, consider
    the following scenario: you work for a bank, and they have decided to implement
    a model that is able to predict a person''s annual income and use that information
    to decide whether to approve a loan. You are given a dataset with 32,561 suitable
    observations, which you have already pre-processed. Your job is to train three
    different models on the dataset and determine which one best suits the case study.
    The first model to be built is a Gaussian NaÃ¯ve Bayes model. Use the following
    steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: In a Jupyter Notebook, import all the required elements to load and split the
    dataset, as well as to train a NaÃ¯ve Bayes algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the pre-processed Census Income dataset. Next, separate the features from
    the target by creating two variables, `X` and `Y`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The pre-processed Census Income dataset can be found in this book's GitHub repository
    at [https://packt.live/2JMhsFB](https://packt.live/2JMhsFB). It consists of the
    transformed Census Income dataset that was pre-processed at the beginning of this
    chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Divide the dataset into training, validation, and testing sets, using a split
    ratio of 10%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When all three sets are created from the same dataset, it is not required to
    create an additional train/dev set to measure data mismatch. Moreover, note that
    it is OK to try a different split ratio, considering that the percentages explained
    in the previous chapter are not set in stone. Even though they tend to work well,
    it is important that you embrace experimentation at different levels when building
    machine learning models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use the `fit` method to train a NaÃ¯ve Bayes model on the training sets (`X_train`
    and `Y_train`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, perform a prediction using the model that you trained previously,
    for a new instance with the following values for each feature: `39`, `6`, `13`,
    `4`, `0`, `2174`, `0`, `40`, `38`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The prediction for the individual should be equal to zero, meaning that the
    individual most likely has an income less than or equal to 50K.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use the same Jupyter Notebook for all the activities within this chapter so
    that you can perform a comparison of different models on the same dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 236.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Decision Tree Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **decision tree algorithm** performs classification based on a sequence
    that resembles a tree-like structure. It works by dividing the dataset into small
    subsets that serve as guides to develop the decision tree nodes. The nodes can
    be either decision nodes or leaf nodes, where the former represent a question
    or decision, andÂ the latter represent the decisions made or the final outcome.
  prefs: []
  type: TYPE_NORMAL
- en: How Does the Decision Tree Algorithm Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering what we just mentioned, decision trees continually split the dataset
    according to the parameters defined in the decision nodes. Decision nodes have
    branches coming out of them, where each decision node can have two or more branches.
    The branches represent the different possible answers that define the way in which
    the data is split.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider the following table, which shows whether a person has
    a pending student loan based on their age, highest education, and current income:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: Dataset for student loans'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.6: Dataset for student loans'
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible configuration for a decision tree built based on the preceding
    data is shown in the following diagram, where the light boxes represent the decision
    nodes, the arrows are the branches representing each answer to the decision node,
    and the dark boxes refer to the outcome for instances that follow the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: Data represented in a decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.7: Data represented in a decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: To perform the prediction, once the decision tree has been built, the model
    takes each instance and follows the sequence that matches the instance's features
    until it reaches a leaf, that is, the outcome. According to this, the classification
    process starts at the root node (the one on top) and continues along the branch
    that describes the instance. This process continues until a leaf node is reached,
    which represents the prediction for that instance.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a person *over 40 years old* with an income *below $150,000* and
    an education level of *bachelor* is likely to not have a student loan; hence,
    the class label assigned to it would be *No*.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees can handle both quantitative and qualitative features, considering
    that continuous features will be handled in ranges. Additionally, leaf nodes can
    handle categorical or continuous class labels; for categorical class labels, a
    classification is made, while for continuous class labels, the task to be handled
    is regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.02: Applying the Decision Tree Algorithm'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will apply the decision tree algorithm to the Fertility
    Dataset, with the objective of determining whether the fertility level of an individual
    is affected by their demographics, their environmental conditions, and their previous
    medical conditions. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook to implement this exercise and import `pandas`, as
    well as the `DecisionTreeClassifier` class from scikit-learn''s `tree` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `fertility_Diagnosis` dataset that you downloaded in *Exercise 4.01*,
    *Applying the NaÃ¯ve Bayes Algorithm*. Make sure that you add the `header` argument
    equal to `None` to the `read_csv` function, considering that the dataset does
    not contain a header row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into `X` and `Y`, considering that the class label is found
    under the column with the index equal to `9`. Use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `DecisionTreeClassifier` class. Next, use the `fit` function
    to train the model using `X` and `Y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Again, the output from running the preceding code snippet will appear. This
    output summarizes the conditions that define your model by printing the values
    that are used for every hyperparameter that the model uses, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since the model has been instantiated without setting any hyperparameters, the
    summary will show the default values that were used for each.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, perform a prediction by using the model that you trained before, for
    the same instances that we used in *Exercise 4.01*, *Applying the NaÃ¯ve Bayes
    Algorithm*: `â0.33`, `0.69`, `0`, `1`, `1`, `0`, `0.8`, `0`, `0.88`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following code to do so:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from the prediction is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Again, the model predicted that the fertility of the subject has not been affected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hDlvns](https://packt.live/3hDlvns).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3fsVw07](https://packt.live/3fsVw07).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have successfully trained a decision tree model and performed a prediction
    on new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.02: Training a Decision Tree Model for Our Census Income Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You continue to work on building a model that''s able to predict a person''s
    annual income. Using the pre-processed Census Income dataset, you have chosen
    to build a decision tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Jupyter Notebook that you used for the previous activity and import
    the decision tree algorithm from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model using the `fit` method on the `DecisionTreeClassifier` class
    from scikit-learn. To train the model, use the training set data from the previous
    activity (`X_train` and `Y_train`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, perform a prediction by using the model that you trained for a new
    instance with the following values for each feature: `39`, `6`, `13`, `4`, `0`,
    `2174`, `0`, `40`, `38`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The prediction for the individual should be equal to zero, meaning that the
    individual most likely has an income less than or equal to 50K.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 237.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Support Vector Machine Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Support Vector Machine** (**SVM**) algorithm is a classifier that finds
    the hyperplane that effectively separates the observations into their class labels.
    It starts by positioning each instance into a data space with *n* dimensions,
    where *n* represents the number of features. Next, it traces an imaginary line
    that clearly separates the instances belonging to a class label from the instances
    belonging to others.
  prefs: []
  type: TYPE_NORMAL
- en: A support vector refers to the coordinates of a given instance. According to
    this, the support vector machine is the boundary that effectively segregates the
    different support vectors in a data space.
  prefs: []
  type: TYPE_NORMAL
- en: For a two-dimensional data space, the hyperplane is a line that splits the data
    space into two sections, each one representing a class label.
  prefs: []
  type: TYPE_NORMAL
- en: How Does the SVM Algorithm Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following diagram shows a simple example of an SVM model. Both the triangles
    and circular data points represent the instances from the input dataset, where
    the shapes define the class label that each instance belongs to. The dashed line
    signifies the hyperplane that clearly segregates the data points, which is defined
    based on the data points' location in the data space. This line is used to classify
    unseen data, as represented by the square. This way, new instances that are located
    to the left of the line will be classified as triangles, while the ones to the
    right will be circles.
  prefs: []
  type: TYPE_NORMAL
- en: 'The larger the number of features, the more dimensions the data space will
    have, which will make visually representing the model impossible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Graphical example of an SVM model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.8: Graphical example of an SVM model'
  prefs: []
  type: TYPE_NORMAL
- en: Although the algorithm seems to be quite simple, its complexity is evident in
    the algorithm's methodology for drawing the appropriate hyperplane. This is because
    the model generalizes to hundreds of observations with multiple features.
  prefs: []
  type: TYPE_NORMAL
- en: 'To choose the right hyperplane, the algorithm follows the following rules,
    wherein *Rule 1* is more important than *Rule 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rule 1**: The hyperplane must maximize the correct classification of instances.
    This basically means that the best line is the one that effectively separates
    data points belonging to different class labels while keeping those that belong
    to the same one together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, in the following diagram, although both lines are able to separate
    most instances into their correct class labels, line A would be selected by the
    model as the one that segregates the classes better than line B, which fails to
    classify two data points:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9: Sample of hyperplanes that explain Rule 1'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15781_04_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.9: Sample of hyperplanes that explain Rule 1'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rule 2**: The hyperplane must maximize its distance to the nearest data point
    of either of the class labels, which is also known as the **margin**. This rule
    helps the model become more robust, which means that the model is able to generalize
    the input data so that it works efficiently on unseen data. This rule is especially
    important in preventing new instances from being mislabeled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, by looking at the following diagram, it is possible to conclude
    that both hyperplanes comply with *Rule 1*. Nevertheless, line A is selected,
    since it maximizes its distance to the nearest data points for both classes in
    comparison to the distance of line B to its nearest data point:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10: Sample of hyperplanes that explain Rule 2'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15781_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.10: Sample of hyperplanes that explain Rule 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the SVM algorithm uses a linear function to split the data points
    of the input data. However, this configuration can be modified by changing the
    kernel type of the algorithm. For example, consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For scikit-learn''s SVM algorithm, the kernel refers to the mathematical function
    to be used to split the data points, which can be linear, polynomial, or sigmoidal,
    among others. To learn more about the parameters for this algorithm, visit the
    following URL: [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: Sample observations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.11: Sample observations'
  prefs: []
  type: TYPE_NORMAL
- en: To segregate these observations, the model would have to draw a circle or another
    similar shape. The algorithm handles this by using kernels (mathematical functions)
    that can introduce additional features to the dataset in order to modify the distribution
    of data points into a form that allows a line to segregate them. There are several
    kernels available for this, and the selection of one should be done by trial and
    error so that you can find the one that best classifies the data that's available.
  prefs: []
  type: TYPE_NORMAL
- en: However, the default kernel for the SVM algorithm in scikit-learn is the **Radial
    Basis Function** (**RBF**) kernel. This is mainly because, based on several studies,
    this kernel has proved to work great for most data problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.03: Applying the SVM Algorithm'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will apply the SVM algorithm to the Fertility dataset.
    The idea, which is the same as in previous exercises, is to determine whether
    the fertility level of an individual is affected by their demographics, their
    environmental conditions, and their previous medical conditions. Follow these
    steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook to implement this exercise. Import pandas as well as
    the `SVC` class from scikit-learn''s `svm` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `fertility_Diagnosis` dataset that you downloaded in *Exercise 4.01*,
    *Applying the NaÃ¯ve Bayes Algorithm*. Make sure to add the `header = None` argument
    to the `read_csv` function, considering that the dataset does not contain a header
    row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into `X` and `Y`, considering that the class label is found
    under the column with the index equal to `9`. Use the following code to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate scikit-learn''s `SVC` class and use the `fit` function to train
    the model using `X` and `Y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Again, the output from running this code represents the summary of the model,
    along with its default hyperparameters, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, perform a prediction using the model that you trained previously,
    for the same instances that we used in *Exercise 4.01*, *Applying the NaÃ¯ve Bayes
    Algorithm*: â`0.33`, `0.69`, `0`, `1`, `1`, `0`, `0.8`, `0`, `0.88`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following code to do so:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Again, the model predicts the instance's class label as `N`, meaning that the
    fertility of the subject has not been affected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YyEMNX](https://packt.live/2YyEMNX).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Y3nIR2](https://packt.live/2Y3nIR2).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have successfully trained an SVM model and performed a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.03: Training an SVM Model for Our Census Income Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Continuing with your task of building a model that is capable of predicting
    a person''s annual income, the final algorithm that you want to train is the Support
    Vector Machine. Follow these steps to implement this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Jupyter Notebook that you used for the previous activity and import
    the SVM algorithm from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model using the `fit` method on the `SVC` class from scikit-learn.
    To train the model, use the training set data from the previous activity (`X_train`
    and `Y_train`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The process of training the SVC class using the `fit` method may take a while.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, perform a prediction using the model that you trained previously,
    for a new instance with the following values for each feature: `39`, `6`, `13`,
    `4`, `0`, `2174`, `0`, `40`, `38`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The prediction for the individual should be equal to zero, that is, the individual
    most likely has an income less than or equal to 50K.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 238.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Error Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explained the importance of error analysis. In this
    section, the different evaluation metrics will be calculated for all three models
    that were created in the previous activities so that we can compare them.
  prefs: []
  type: TYPE_NORMAL
- en: For learning purposes, we will compare the models using accuracy, precision,
    and recall metrics. This way, it will be possible to see that even though a model
    might be better in terms of one metric, it could be worse when measuring a different
    metric, which helps to emphasize the importance of choosing the right metric to
    measure your model according to the goal you wish to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy, Precision, and Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a quick reminder, in order to measure performance and perform error analysis,
    it is required that you use the `predict` method for the different sets of data
    (training, validation, and testing). The following code snippets present a clean
    way of measuring all three metrics on our three sets at once:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The following steps are to be performed after solving the activities of this
    chapter. This is mainly because the steps in this section correspond to a continuation
    of this chapter's activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the three metrics to be used are imported:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create two lists containing the different sets of data that will be
    used inside a `for` loop to perform the performance calculation on all sets of
    data for all models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A dictionary will be created, which will hold the value of each evaluation
    metric for each set of data for each model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A `for` loop is used to go through the different sets of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the metrics, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.12: Printing the metrics'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15781_04_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.12: Printing the metrics'
  prefs: []
  type: TYPE_NORMAL
- en: Inside the `for` loop, there are three blocks of code, one for each model we
    created in the previous activities. Each block of code performs the following
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: First, a prediction is made. The prediction is achieved by calling the `predict`
    method on the model and inputting a set of data. As this operation occurs inside
    a `for` loop, the prediction will occur for all sets of data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the calculation of all three metrics is done by comparing the ground truth
    data with the prediction that we calculated previously. The calculation is appended
    to the dictionary that was created previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding snippets, the following results are obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13: Performance results of all three models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_04_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.13: Performance results of all three models'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Review the code to arrive at these results, which can be found in this book's
    GitHub repository, under the folder named `Chapter04`, by opening the file named
    `Error analysis`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, the following inferences, in relation to selecting the best-fitted
    model, as well as with regard to the conditions that each model suffers from,
    will be done while considering only the values from the accuracy metric, assuming
    a Bayes error of close to 0 (meaning that the model could reach a maximum success
    rate of close to 1):'
  prefs: []
  type: TYPE_NORMAL
- en: Upon comparing the three accuracy scores of the NaÃ¯ve Bayes and the SVM models,
    it is possible to conclude that the models behave almost the same way for all
    three sets of data. This basically means that the models are generalizing the
    data from the training set, which allows them to perform well on unseen data.
    Nevertheless, the overall performance of the models is around 0.8, which is far
    from the maximum success rate. This means that the models may be suffering from
    high bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, the performance of the decision tree model, in terms of the accuracy
    of the training set, is closer to the maximum success rate. However, the model
    is suffering from a case of overfitting, considering that the accuracy level of
    the model on the validation set is much lower than its performance on the training
    set. According to this, it would be possible to address the overfitting issue
    by adding more data to the training set or by fine-tuning the hyperparameters
    of the model, which would help to bring up the accuracy level of the validation
    and testing sets. Pruning the tree can help an overfitted model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering this, the researcher now has the required information to select
    a model and work on improving the results to achieve the maximum possible performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, for learning purposes, let's compare the results of all the metrics for
    the decision tree model. Although the values for all three metrics prove the existence
    of overfitting, it is possible to observe that the degree of overfitting is much
    larger for the precision and recall metrics. Also, it is possible to conclude
    that the performance of the model on the training set measured by the recall metric
    is much lower, which means that the model is not as good at classifying positive
    labels. This means that if the purpose of the case study was to maximize the number
    of positive classifications, regardless of the classification of negative labels,
    the model would also need to improve its performance on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding comparison is done to show that the performance of the same model
    can vary if measured with a different metric. According to this, it is crucial
    to choose the metric of relevance for the case study.
  prefs: []
  type: TYPE_NORMAL
- en: Using the knowledge that you have gained from previous chapters, feel free to
    keep exploring the results shown in the preceding table.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the knowledge from previous chapters, we started this chapter by performing
    an analysis of the Census Income dataset, with the objective of understanding
    the data that's available and making decisions about the pre-processing process.
    Three supervised learning classification algorithmsâthe NaÃ¯ve Bayes algorithm,
    the Decision Tree algorithm, and the SVM algorithmâwere explained, and were applied
    to the previously pre-processed dataset to create models that generalized to the
    training data. Finally, we compared the performance of the three models on the
    Census Income dataset by calculating the accuracy, precision, and recall on the
    different sets of data (training, validation, and testing).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at **Artificial Neural Networks** (**ANNs**),
    their different types, and their advantages and disadvantages. We will also use
    an ANN to solve the same data problem that was discussed in this chapter, as well
    as to compare its performance with that of the other supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL

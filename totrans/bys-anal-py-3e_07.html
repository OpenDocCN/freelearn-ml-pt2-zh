<html><head></head><body>
<section id="chapter-8-gaussian-processes" class="level2 chapterHead" data-number="1.12">&#13;
<h1 class="chapterHead" data-number="1.12">ChapterÂ 8<br/>&#13;
<span id="x1-1560008"/>Gaussian Processes</h1>&#13;
<blockquote>&#13;
<p>Lonely? You have yourself. Your infinite selves. - Rick Sanchez (at least the one from dimension C-137)</p>&#13;
</blockquote>&#13;
<p>In the last chapter, we learned about the Dirichlet process, an infinite-dimensional generalization of the Dirichlet distribution that can be used to set a prior on an unknown continuous distribution. In this chapter, we will learn about the Gaussian process, an infinite-dimensional generalization of the Gaussian distribution that can be used to set a prior on unknown functions. Both the Dirichlet process and the Gaussian process are used in Bayesian statistics to build flexible models where the number of parameters is allowed to increase with the size of the data.</p>&#13;
<p>We will cover the following topics:</p>&#13;
<ul>&#13;
<li><p>Functions as probabilistic objects</p></li>&#13;
<li><p>Kernels</p></li>&#13;
<li><p>Gaussian processes with Gaussian likelihoods</p></li>&#13;
<li><p>Gaussian processes with non-Gaussian likelihoods</p></li>&#13;
<li><p>Hilbert space Gaussian process</p></li>&#13;
</ul>&#13;
<p><span id="x1-156001r366"/></p>&#13;
<section id="linear-models-and-non-linear-data" class="level3 sectionHead" data-number="1.12.1">&#13;
<h2 class="sectionHead" data-number="1.12.1">8.1 <span id="x1-1570001"/>Linear models and non-linear data</h2>&#13;
<p><span id="dx1-157001"/></p>&#13;
<p>In <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em> and <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em> we learned how to build models of the general form:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file216.jpg" class="math-display" alt="Î¸ = ğœ“ (Ï•(X )ğ›½ ) "/>&#13;
</div>&#13;
<p>Here, <em>Î¸</em> is a parameter for some probability distribution, for example, the mean of a Gaussian, the <em>p</em> parameter of the binomial, the rate of a Poisson, and so on. We call <img src="../media/phi.png" style="width: 0.8em; vertical-align: -0.30em;"/> the inverse link function and <img src="../media/phi.png" style="width: 0.8em; vertical-align: -0.30em;"/> is some other function we use to potentially transform the data, like a square root, a polynomial function, or something else.</p>&#13;
<p>Fitting, or learning, a Bayesian model can be seen as finding the posterior distribution of the weights <em>Î²</em>, and thus this is known as the weight view of approximating functions. As we already saw with polynomial and splines regression, by letting <img src="../media/phi.png" style="width: 0.8em; vertical-align: -0.30em;"/> be a non-linear function, we can map the inputs onto a <em>feature space</em>. We also saw that by using a polynomial of the proper degree, we can perfectly fit any function. But unless we apply some form of regularization, for example, using prior distributions, this will lead to models that memorize the data, or in other words models with very poor generalizing properties. We also mention that splines can be as flexible as polynomials but with better statistical properties. We will now discuss Gaussian processes, which provide a principled solution to modeling arbitrary functions by effectively letting the data decide on the complexity of the function, while avoiding, or at least minimizing, the chance of overfitting.</p>&#13;
<p>The following sections discuss Gaussian processes from a very practical point of view; we have avoided covering almost all the mathematics surrounding them. For a more formal explanation, you may read <em>Gaussian Processes for Machine Learning</em> by <a href="Bibliography.xhtml#Xrasmussen_2005">Rasmussen and Williams</a>Â [<a href="Bibliography.xhtml#Xrasmussen_2005">2005</a>]. <span id="x1-157002r369"/></p>&#13;
</section>&#13;
<section id="modeling-functions" class="level3 sectionHead" data-number="1.12.2">&#13;
<h2 class="sectionHead" data-number="1.12.2">8.2 <span id="x1-1580002"/>Modeling functions</h2>&#13;
<p><span id="dx1-158001"/></p>&#13;
<p>We will begin our discussion of Gaussian processes by first describing a way to represent functions as probabilistic objects. We may think of a function <em>f</em> as a mapping from a set of inputs <em>X</em> to a set of outputs <em>Y</em> . Thus, we can write:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file217.jpg" class="math-display" alt="Y = f(X ) "/>&#13;
</div>&#13;
<p>One very crude way to represent functions is by listing for each <em>x</em><sub><em>i</em></sub> value its corresponding <em>y</em><sub><em>i</em></sub> value as in <em>Table <a href="#x1-158002r1">8.1</a></em>. You may remember this way of representing functions from elementary school.</p>&#13;
<table id="TBL-12" class="tabular">&#13;
<tbody>&#13;
&#13;
<tr id="TBL-12-1-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-12-1-1" class="td11" style="text-align: center; white-space: nowrap;"><em>x</em></td>&#13;
<td id="TBL-12-1-2" class="td11" style="text-align: center; white-space: nowrap;"><em>y</em></td>&#13;
</tr>&#13;
&#13;
<tr id="TBL-12-2-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-12-2-1" class="td11" style="text-align: center; white-space: nowrap;">0.00</td>&#13;
<td id="TBL-12-2-2" class="td11" style="text-align: center; white-space: nowrap;">0.46</td>&#13;
</tr>&#13;
<tr id="TBL-12-3-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-12-3-1" class="td11" style="text-align: center; white-space: nowrap;">0.33</td>&#13;
<td id="TBL-12-3-2" class="td11" style="text-align: center; white-space: nowrap;">2.60</td>&#13;
</tr>&#13;
<tr id="TBL-12-4-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-12-4-1" class="td11" style="text-align: center; white-space: nowrap;">0.67</td>&#13;
<td id="TBL-12-4-2" class="td11" style="text-align: center; white-space: nowrap;">5.90</td>&#13;
</tr>&#13;
<tr id="TBL-12-5-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-12-5-1" class="td11" style="text-align: center; white-space: nowrap;">1.00</td>&#13;
<td id="TBL-12-5-2" class="td11" style="text-align: center; white-space: nowrap;">7.91</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="IMG---Caption"><span id="x1-158002r1"/> <span id="x1-158003"/><strong>TableÂ 8.1</strong>: A tabular representation of a function (sort of)</p>&#13;
<p>As a general case, the values of <em>X</em> and <em>Y</em> will live on the real line; thus, we can see a function as a (potentially) infinite and ordered list of paired values (<em>x</em><sub><em>i</em></sub><em>,y</em><sub><em>i</em></sub>). The order is important because, if we shuffle the values, we will get different functions.</p>&#13;
<p>Following this description, we can represent, numerically, any specific function we want. But what if we want to represent functions probabilistically? Well, we then need to encode a probabilitics mapping. Let me explain this; we can let each value be a random variable with some associated distribution. As working with Gaussians is usually convenient, letâ€™s say that they are distributed as a Gaussian with a given mean and variance. In this way, we no longer have the description of a single specific function, but the description of a family of distributions.</p>&#13;
<p>To make this discussion concrete, letâ€™s use some Python code to build and plot two examples of such functions:</p>&#13;
<p><span id="x1-158004r1"/> <span id="x1-158005"/><strong>CodeÂ 8.1</strong></p>&#13;
<pre id="listing-102" class="source-code"><code>np.random.seed(42)Â </code>&#13;
<code>x = np.linspace(0, 1, 10)Â </code>&#13;
<code>y = np.random.normal(0, 1, len(x))Â </code>&#13;
<code>plt.plot(x, y, 'o-', label='the first one')Â </code>&#13;
<code>y = np.zeros_like(x)Â </code>&#13;
<code>Â </code>&#13;
<code>for i in range(len(x)):Â </code>&#13;
<code>Â Â Â Â y[i] = np.random.normal(y[i-1], 1)Â </code>&#13;
<code>plt.plot(x, y, 'o-', label='the second one')Â </code>&#13;
<code>plt.legend()</code></pre>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file218.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-158016r1"/><strong>FigureÂ 8.1</strong>: Two dummy functions sampled from Gaussian distributions</p>&#13;
<p><em>Figure <a href="#x1-158016r1">8.1</a></em> shows that encoding functions using samples from Gaussian distributions is not that crazy or foolish, so we may be on the right track. Nevertheless, the approach used to generate <em>Figure <a href="#x1-158016r1">8.1</a></em> is limited and not sufficiently flexible.</p>&#13;
<p>While we expect real functions to have some structure or pattern, the way we express <code>the first one </code>function does not let us encode any relation between data points. In fact, each point is completely independent of the others, as we just get them as 10 independent samples from a common one-dimensional Gaussian distribution. For <code>the second one </code>function, we introduce some dependency. The mean of the point <em>y</em><sub><em>i</em>+1</sub> is the value <em>y</em><sub><em>i</em></sub>, thus we have some structure here. Nevertheless, we will see next that there is a more general approach to capturing dependencies, and not only between consecutive points.</p>&#13;
<p>Before continuing, let me stop for a moment and consider why weâ€™re using Gaussians and not any other probability distribution. First, by restricting ourselves to working with Gaussians, we do not lose any flexibility in specifying functions of different shapes, as each point has potentially its own mean and variance. Second, working with Gaussians is nice from a mathematical point of view. <span id="x1-158017r370"/></p>&#13;
</section>&#13;
<section id="multivariate-gaussians-and-functions" class="level3 sectionHead" data-number="1.12.3">&#13;
<h2 class="sectionHead" data-number="1.12.3">8.3 <span id="x1-1590003"/>Multivariate Gaussians and functions</h2>&#13;
<p><span id="dx1-159001"/></p>&#13;
<p>In <em>Figure <a href="#x1-158016r1">8.1</a></em>, we represented a function as a collection of samples from 1-dimensional Gaussian distributions. One alternative is to use an n-dimensional multivariate Gaussian distribution to get a sample vector of length <em>n</em>. Actually, you may want to try to reproduce <em>Figure <a href="#x1-158016r1">8.1</a></em> but replacing <code>np.random.normal(0, 1, len(x)) </code>with <code>np.random.multivariate_normal</code>, with a mean of <code>np.zeros_like(x) </code>and a standard deviation of <code>np.eye(len(x)</code>. The advantage of working with a Multivariate Normal is that we can use the covariance matrix to encode information about the function. For instance, by setting the covariance matrix to <code>np.eye(len(x))</code>, we are saying that each of the 10 points, where we are evaluating the function, has a variance of 1. We are also saying that the variance between them, that is, their covariances, is 0. In other words, they are independent. If we replace those zeros with other numbers, we could get covariances telling a different story.</p>&#13;
<p>I hope you are starting to get convinced that it is possible to use a multivariate Gaussian in order to represent functions. If thatâ€™s the case, then we just need to find a suitable covariance matrix. And thatâ€™s the topic of the next section. <span id="x1-159002r364"/></p>&#13;
<section id="covariance-functions-and-kernels" class="level4 subsectionHead" data-number="1.12.3.1">&#13;
<h3 class="subsectionHead" data-number="1.12.3.1">8.3.1 <span id="x1-1600001"/>Covariance functions and kernels</h3>&#13;
<p><span id="dx1-160001"/> <span id="dx1-160002"/> <span id="dx1-160003"/></p>&#13;
<p>In practice, covariance matrices are specified using functions known as kernels. Unfortunately, the term kernel is a very polysemic one, even in the statistical literature. An easy way to define a kernel is any function that returns a valid covariance matrix. But this is a tautological and not very intuitive definition. A more conceptual and useful definition is that a kernel defines a measure of similarity between data points in the input space, and this similarity determines how much influence one data point should have on predicting the value of another data point.</p>&#13;
<p>There are many useful kernels, a popular one being the exponentiated quadratic kernel:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file219.jpg" class="math-display" alt=" ( â€² 2) ğœ…(X,X â€²) = exp âˆ’ âˆ¥X--âˆ’-X-âˆ¥- 2â„“2 "/>&#13;
</div>&#13;
<p>Here, <span class="cmsy-10x-x-109">âˆ¥</span><strong>X</strong> <span class="cmsy-10x-x-109">âˆ’</span> <strong>X</strong><span class="cmsy-10x-x-109">â€²</span><span class="cmsy-10x-x-109">âˆ¥</span><sup>2</sup> is the squared Euclidean distance:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file220.jpg" class="math-display" alt="âˆ¥X âˆ’ Xâ€²âˆ¥2 = (X1 âˆ’ X1â€²)2 + (X2 âˆ’ Xâ€²2)2 + â‹…â‹…â‹…+ (Xn âˆ’ Xâ€²n)2 "/>&#13;
</div>&#13;
<p>For this kernel, we can see that we have a symmetric function that takes two inputs and returns a value of 0 if the inputs are the same, or positive otherwise. And thus we can interpret the output of the exponentiated quadratic kernel as a measure of similarity between the two inputs.</p>&#13;
<p>It may not be obvious at first sight, but the exponentiated quadratic kernel has a similar formula to the Gaussian distribution. For this reason, this kernel is also called the Gaussian kernel. The term <em>â„“</em> is known as the length scale (or bandwidth or variance) and controls the width of the kernel. In other words, it controls at what scale the <em>X</em> values are considered similar.</p>&#13;
<p>To better understand the role of kernels, I recommend you play with them. For instance, letâ€™s define a Python function to compute the exponentiated quadratic kernel:</p>&#13;
<p><span id="x1-160004r2"/> <span id="x1-160005"/><strong>CodeÂ 8.2</strong></p>&#13;
<pre id="listing-103" class="source-code"><code>def exp_quad_kernel(x, knots, â„“=1):Â </code>&#13;
<code>Â Â Â Â """exponentiated quadratic kernel"""Â </code>&#13;
<code>Â Â Â Â return np.array([np.exp(-(x-k)**2 / (2*â„“**2)) for k in knots])</code></pre>&#13;
<p><em>Figure <a href="#x1-160009r2">8.2</a></em> shows how a 4 <span class="cmsy-10x-x-109">Ã— </span>4 covariance matrix looks for different inputs. The input I chose is rather simple and consists of the values [<span class="cmsy-10x-x-109">âˆ’</span>1<em>,</em>0<em>,</em>1<em>,</em>2].</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file221.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-160009r2"/><strong>FigureÂ 8.2</strong>: Input values (left), covariance matrix (right)</p>&#13;
<p>On the left panel of <em>Figure <a href="#x1-160009r2">8.2</a></em>, we have the input values. These are the values on the x-axis, and we have labeled the points from 0 to 3. Thus, point 0 takes the value -1, point 1 takes 0, and so on. On the right panel, we have a heatmap representing the covariance matrix that we computed using the exponentiated quadratic kernel. The lighter the color, the larger the value of the covariance. As you can see, the heatmap is symmetric, with the diagonal taking the largest values. This makes sense when we realize that the value of each element in the covariance matrix is inversely proportional to the distance between the points, and the diagonal is the result of comparing each data point with itself. The smallest value is the one for the points 0 and 3, as they are the most distant points.</p>&#13;
<p>Once you understand this example, you should try it with other inputs. See exercise 1 at the end of this chapter and the accompanying notebook ( <a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>) for further practice.</p>&#13;
<p>Now that we have a better grasp of how to use a kernel to generate a covariance matrix, letâ€™s move one step further and use the covariance matrix to sample functions. As you can see in <em>Figure <a href="#x1-160010r3">8.3</a></em>, a Gaussian kernel implies a wide variety of functions with the parameter <em>â„“</em> controlling the smoothness of the functions. The larger the value of <em>â„“</em>, the smoother the function.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file222.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-160010r3"/><strong>FigureÂ 8.3</strong>: Realizations of a Gaussian kernel for four values of <em>â„“</em> (two realization per value of <em>â„“</em>)</p>&#13;
<div id="tcolobox-17" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Show me Your Friends and Iâ€™ll Show you your Future</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>The kernel translates the distance of the data points along the x axis to values of covariances for values of the expected function (on the y axis). Thus, the closest two points are on the x axis; the most similar we expect their values to be on the y axis.</p>&#13;
</div>&#13;
</div>&#13;
<p><span id="x1-160011r374"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="gaussian-processes" class="level3 sectionHead" data-number="1.12.4">&#13;
<h2 class="sectionHead" data-number="1.12.4">8.4 <span id="x1-1610004"/>Gaussian processes</h2>&#13;
<p><span id="dx1-161001"/></p>&#13;
<p>Now we are ready to understand what Gaussian processes (GPs) are and how they are used in practice. A somewhat formal definition of GPs, taken from Wikipedia, is as follows:</p>&#13;
<p>â€The collection of random variables indexed by time or space, such that every finite collection of those random variables has a MultivariateNormal distribution, i.e. every finite linear combination of them is normally distributed.â€</p>&#13;
<p>This is probably not a very useful definition, at least not at this stage of your learning path. The trick to understanding Gaussian processes is to realize that the concept of GP is a mental (and mathematical) scaffold, since, in practice, we do not need to directly work with this infinite mathematical object. Instead, we only evaluate the GPs at the points where we have data. By doing this, we collapse the infinite-dimensional GP into a finite multivariate Gaussian distribution with as many dimensions as data points. Mathematically, this collapse is achieved by marginalization over the infinitely unobserved dimensions. The theory assures us that it is OK to omit (actually marginalize over) all points, except the ones we are observing. It also guarantees that we will always get a multivariate Gaussian distribution. Thus, we can rigorously interpret <em>Figure <a href="#x1-160010r3">8.3</a></em> as actual samples from a Gaussian process!</p>&#13;
<p>So far we have focused on the covariance matrix of the MultivariateNormal and we have not discussed the mean. Setting the mean of a multivariate Gaussian at 0 is common practice when working with GPs, since they are flexible enough to model the mean arbitrarily well. But notice that there is no restriction in doing so. Actually, for some problems, you may want to model the mean parametrically and leave the GP to model the residuals.</p>&#13;
<div id="tcolobox-18" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>GPs are Prior Over Functions</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>Gaussian processes are prior distributions over functions in such a way that at each point that you evaluate a function, it places a Gaussian distribution with a given mean and variance. In practice, GPs are usually built using kernels, which turn distance on an x axis into similarities on the y axis.</p>&#13;
</div>&#13;
</div>&#13;
<p><span id="x1-161002r378"/></p>&#13;
</section>&#13;
<section id="gaussian-process-regression" class="level3 sectionHead" data-number="1.12.5">&#13;
<h2 class="sectionHead" data-number="1.12.5">8.5 <span id="x1-1620005"/>Gaussian process regression</h2>&#13;
<p><span id="dx1-162001"/></p>&#13;
<p>Letâ€™s assume we can model a variable <em>Y</em> as a function <em>f</em> of <em>X</em> plus some Gaussian noise:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file223.jpg" class="math-display" alt="Y âˆ¼ ğ’© (Î¼ = f(X ),Ïƒ = ğœ–) "/>&#13;
</div>&#13;
<p>If <em>f</em> is a linear function of <em>X</em>, then this assumption is essentially the same one we used in <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em> when we discussed simple linear regression. In this chapter, instead, we are going to use a more general expression for <em>f</em> by setting a prior over it. In that way, we will be able to get more complex functions than linear. If we decided to use Gaussian processes as this prior, then we can write:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file224.jpg" class="math-display" alt=" â€² f(X ) = ğ’¢ğ’« (Î¼X,ğœ…(X, X )) "/>&#13;
</div>&#13;
<p>Here, <span class="cmsy-10x-x-109"><img src="../media/GP.PNG" style="width:1.4em; vertical-align: -0.10em;"/></span> represents a Gaussian process with the mean function <em>Î¼</em><sub><em>X</em></sub> and covariance function <em>K</em>(<em>X,X</em><span class="cmsy-10x-x-109">â€²</span>). Even though in practice, we always work with finite objects, we used the word <strong>function</strong> to indicate that mathematically, the mean and covariance are infinite objects.</p>&#13;
<p>I mentioned before that working with Gaussians is nice. For instance, if the prior distribution is a GP and the likelihood is a Gaussian distribution, then the posterior is also a GP and we can compute it analytically. Additionally, its nice to have a Gaussian likelihood because we can marginalize out the GP, which hugely reduces the size of the parameter space we need to sample from. The GP module in PyMC takes advantage of this and then it has different implementations for Gaussian and non-Gaussian likelihoods. In the next sections, we will explore both. <span id="x1-162002r379"/></p>&#13;
</section>&#13;
<section id="gaussian-process-regression-with-pymc" class="level3 sectionHead" data-number="1.12.6">&#13;
<h2 class="sectionHead" data-number="1.12.6">8.6 <span id="x1-1630006"/>Gaussian process regression with PyMC</h2>&#13;
<p><span id="dx1-163001"/></p>&#13;
<p>The gray line in <em>Figure <a href="#x1-163002r4">8.4</a></em> is a sin function. We are going to assume we donâ€™t know this function and instead, all we have is a set of data points (dots). Then we use a Gaussian process to approximate the function that generated those data points.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file225.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-163002r4"/><strong>FigureÂ 8.4</strong>: Synthetic data (dots) generated from a known function (line)</p>&#13;
<p>GPs are implemented in PyMC as a series of Python classes that deviate a little bit from what we have seen in previous models; nevertheless, the code is still very <em>PyMConic</em>. I have added a few comments in the following code to guide you through the key steps of defining a GP with PyMC.</p>&#13;
<p><span id="x1-163003r3"/> <span id="x1-163004"/><strong>CodeÂ 8.3</strong></p>&#13;
<pre id="listing-104" class="source-code"><code># A one-dimensional column vector of inputs.Â </code>&#13;
<code>X = x[:, None]Â </code>&#13;
<code>Â </code>&#13;
<code>with pm.Model() as model_reg:Â </code>&#13;
<code>Â Â Â Â # hyperprior for lengthscale kernel parameterÂ </code>&#13;
<code>Â Â Â Â â„“Â = pm.InverseGamma("â„“", 7, 17)Â </code>&#13;
<code>Â Â Â Â # instanciate a covariance functionÂ </code>&#13;
<code>Â Â Â Â cov = pm.gp.cov.ExpQuad(1, ls=â„“)Â </code>&#13;
<code>Â Â Â Â # instanciate a GP priorÂ </code>&#13;
<code>Â Â Â Â gp = pm.gp.Marginal(cov_func=cov)Â </code>&#13;
<code>Â Â Â Â ÏƒÂ = pm.HalfNormal('Ïƒ', 25)Â </code>&#13;
<code>Â Â Â Â # Class representing that the observed data is a GP plus Gaussian noiseÂ </code>&#13;
<code>Â Â Â Â y_pred = gp.marginal_likelihood('y_pred', X=X, y=y, sigma=Ïƒ)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â idata_reg = pm.sample()</code></pre>&#13;
<p>Notice that instead of a Gaussian likelihood, we have used the <code>gp.marginal_likelihood </code>method. This method takes advantage of the fact that the posterior has a closed form, as explained in the previous section.</p>&#13;
<p>OK, now that we have computed the posterior, letâ€™s see how to get predictions of the mean fitted function. We can do this by computing the conditional distribution evaluated over new input locations using <code>gp.conditional</code>.</p>&#13;
<p><span id="x1-163020r4"/> <span id="x1-163021"/><strong>CodeÂ 8.4</strong></p>&#13;
<pre id="listing-105" class="source-code"><code>X_new = np.linspace(np.floor(x.min()), np.ceil(x.max()), 100)[:,None]Â </code>&#13;
<code>with model_reg:Â </code>&#13;
<code>Â Â Â Â f_pred = gp.conditional('f_pred', X_new)</code></pre>&#13;
<p>As a result, we get a new PyMC random variable, <code>f_pred</code>, which we can use to get samples from the posterior predictive distribution:</p>&#13;
<p><span id="x1-163025r5"/> <span id="x1-163026"/><strong>CodeÂ 8.5</strong></p>&#13;
<pre id="listing-106" class="source-code"><code>with model_reg:Â </code>&#13;
<code>Â Â Â Â idata_subset = idata_reg.sel(draw=slice(0, None, 100))Â </code>&#13;
<code>Â Â Â Â pred_samples = pm.sample_posterior_predictive(idata_subset,Â </code>&#13;
<code>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â var_names=["f_pred"])Â </code>&#13;
<code>Â </code>&#13;
<code>f_pred = (pred_samples.Â </code>&#13;
<code>Â Â Â Â Â Â Â Â Â posterior_predictive.stack(samples=("chain", "draw"))['f_pred'].Â </code>&#13;
<code>Â Â Â Â Â Â Â Â Â values)</code></pre>&#13;
<p>Now we can plot the fitted functions over the original data, to visually inspect how well they fit the data and the associated uncertainty in our predictions. As we did with linear models in <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>, we are going to show different ways to plot the same results. <em>Figure <a href="#x1-163035r5">8.5</a></em> shows lines from the fitted function.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file226.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-163035r5"/><strong>FigureÂ 8.5</strong>: Lines represent samples from the posterior mean of <code>model_reg</code></p>&#13;
<p>Alternatively, we can use the auxiliary function <code>pm.gp.util.plot_gp_dist </code>to get some nice plots as in <em>Figure <a href="#x1-163036r6">8.6</a></em>. In this plot, each band represents a different percentile, ranging from percentile 99 (lighter gray) to percentile 51 (darker gray).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file227.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-163036r6"/><strong>FigureÂ 8.6</strong>: Samples from the posterior of <code>model_reg </code>plotted using <code>plot_gp_dist </code>function</p>&#13;
<p>Yet another alternative is to compute the mean vector and standard deviation of the conditional distribution evaluated at a given point in the parameter space. In <em>Figure <a href="#x1-163037r7">8.7</a></em>, we use the mean (over the samples in the trace) for <em>â„“</em> and <img src="../media/e.png" style="width:0.75em; vertical-align: -0.10em;"/>. We can compute the mean and variance using the <code>gp.predict</code> method.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file228.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-163037r7"/><strong>FigureÂ 8.7</strong>: Posterior mean of <code>model_reg </code>with bands for 1 and 2 standard deviations</p>&#13;
<p>As we saw in <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>, we can use a linear model with a non-Gaussian likelihood and a proper inverse link function to extend the range of useful linear models. We can do the same for GPs. We can, for example, use a Poisson likelihood with an exponential inverse link function. For a model like this, the posterior is no longer analytically tractable, but, nevertheless, we can use numerical methods to approximate it. In the following sections, we will discuss these types of models. <span id="x1-163038r375"/></p>&#13;
<section id="setting-priors-for-the-length-scale" class="level4 subsectionHead" data-number="1.12.6.1">&#13;
<h3 class="subsectionHead" data-number="1.12.6.1">8.6.1 <span id="x1-1640001"/>Setting priors for the length scale</h3>&#13;
<p><span id="dx1-164001"/> <span id="dx1-164002"/></p>&#13;
<p>For length-scale parameters, priors avoiding zero usually work better. As we already saw, <em>â„“</em> controls the smoothness of the function, thus a value of 0 for <em>â„“</em> implies a non-smooth function; we will get a function like â€the first oneâ€ from <em>Figure <a href="#x1-158016r1">8.1</a></em>. But a far more important reason is that for values of <em>â„“</em> that are larger than 0 but still below the minimum spacing of the covariates, we can get some nasty effects. Essentially, below that point, the likelihood has no way to distinguish between different length scales, so all of them are equally good. This is a type of non-identifiability issue. As a result, we will have a GP that will tend to overfit and exactly interpolate between the input data. Additionally, the MCMC sampler will have a harder time, and we could get longer sampling times or simple unreliable samples. Something similar happens for values beyond the range of the data. If the range of your data is 10 and the value of <em>â„“ &gt;</em>= 10, this implies a flat function. And again beyond that point, you (and the likelihood) have no way of distinguishing between different values of the parameter. Thus even if you have no idea how smooth or wiggly your function is, you can still set a prior that avoids very low and very high values of <em>â„“</em>. For instance, to get the prior <code>pm.InverseGamma("</code><em>â„“</em><code>", 7, 17) </code>we ask PreliZ for the maximum entropy prior that has 0.95 of the mass between 1 and 5:</p>&#13;
<p><span id="x1-164003r6"/> <span id="x1-164004"/><strong>CodeÂ 8.6</strong></p>&#13;
<pre id="listing-107" class="source-code"><code>pz.maxent(pz.InverseGamma(), 1, 5, 0.95)</code></pre>&#13;
<p>The InverseGamma is a common choice. Like the Gamma, it allows us to set a prior that avoids 0, but unlike the Gamma, the InverseGamma has a lighter tail toward 0, or in other words, it allocates less mass for small values.</p>&#13;
<p>For the rest of this chapter, we will use the function <code>get_ig_params </code>to obtain weakly informative priors from the scale of the covariates. You will find the details in the accompanying code (<a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>), but essentially we are using the <code>maxent </code>function from PreliZ to set most of the prior mass in a range compatible with the range of the covariates. <span id="x1-164006r380"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="gaussian-process-classification" class="level3 sectionHead" data-number="1.12.7">&#13;
<h2 class="sectionHead" data-number="1.12.7">8.7 <span id="x1-1650007"/>Gaussian process classification</h2>&#13;
<p><span id="dx1-165001"/></p>&#13;
<p>In <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>, we saw how a linear model can be used to classify data. We used a Bernoulli likelihood with a logistic inverse link function. Then, we applied a boundary decision rule. In this section, we are going to do the same, but this time using a GP instead of a linear model. As we did with <code>model_lrs </code>from <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>, we are going to use the iris dataset with two classes, <code>setosa </code>and <code>versicolor</code>, and one predictor variable, the <code>sepal length</code>.</p>&#13;
<p>For this model, we cannot use the <code>pm.gp.Marginal </code>class, because that class is restricted to Gaussian likelihoods as it takes advantage of the mathematical tractability of the combination of a GP prior with a Gaussian likelihood. Instead, we need to use the more general class <code>pm.gp.Latent</code>.</p>&#13;
<p><span id="x1-165002r7"/> <span id="x1-165003"/><strong>CodeÂ 8.7</strong></p>&#13;
<pre id="listing-108" class="source-code"><code>with pm.Model() as model_iris:Â </code>&#13;
<code>Â Â Â Â â„“Â = pm.InverseGamma('â„“', *get_ig_params(x_1))Â </code>&#13;
<code>Â Â Â Â cov = pm.gp.cov.ExpQuad(1, â„“)Â </code>&#13;
<code>Â Â Â Â gp = pm.gp.Latent(cov_func=cov)Â </code>&#13;
<code>Â Â Â Â f = gp.prior("f", X=X_1)Â </code>&#13;
<code>Â Â Â Â # logistic inverse link function and Bernoulli likelihoodÂ </code>&#13;
<code>Â Â Â Â y_ = pm.Bernoulli("y", p=pm.math.sigmoid(f), observed=y)Â </code>&#13;
<code>Â Â Â Â idata_iris = pm.sample()</code></pre>&#13;
<p>As we can see, <em>Figure <a href="#x1-165012r8">8.8</a></em> looks pretty similar to <em>Figure <a href="CH04.xhtml#x1-85023r11">4.11</a></em>. Please take some time to compare these figures.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file229.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-165012r8"/><strong>FigureÂ 8.8</strong>: Logistic regression, result of <code>model_lrs</code></p>&#13;
<p>You probably have already noticed that the inferred function looks similar to a sigmoid curve, except for the tails that go up at lower values of <code>sepal_length</code>, and down at higher values of <code>sepal_length</code>. Why are we seeing this? Because when there is little or no data available, a GP posterior tends to revert to the GP prior. This makes sense if we think that in the absence of data, your posterior essentially becomes the prior.</p>&#13;
<p>If our only concern is the decision boundary, then the behavior at the tails may be irrelevant. But if we want to model the probabilities of belonging to setosa or versicolor at different values of <code>sepal_length</code>, we should do something to improve the model at the tails. One way to achieve this is to add more structure to the Gaussian process. One very nice feature of GP is that we can combine covariance functions. Hence, for the next model, we are going to combine three kernels: the exponential quadratic kernel, a linear kernel, and a white noise kernel.</p>&#13;
<p>The linear kernel will have the effect of making the tails go to 0 or 1 at the boundaries of the data. Additionally, we use the white noise kernel just as a trick to stabilize the computation of the covariance matrix. Kernels for Gaussian processes are restricted to guarantee the resulting covariance matrix is positive definite. Nevertheless, numerical errors can lead to violating this condition. One manifestation of this problem is that we get NaNs when computing posterior predictive samples of the fitted function. One way to mitigate this error is to stabilize the computation by adding some noise. As a matter of fact, PyMC already does something similar to this under the hood, but sometimes a little bit more noise is needed, as shown in the following code:</p>&#13;
<p><span id="x1-165013r8"/> <span id="x1-165014"/><strong>CodeÂ 8.8</strong></p>&#13;
<pre id="listing-109" class="source-code"><code>with pm.Model() as model_iris2:Â </code>&#13;
<code>Â Â Â Â â„“Â = pm.InverseGamma('â„“', *get_ig_params(x_1))Â </code>&#13;
<code>Â Â Â Â c = pm.Normal('c', x_1.min())Â </code>&#13;
<code>Â Â Â Â Ï„Â = pm.HalfNormal('Ï„', 5)Â </code>&#13;
<code>Â Â Â Â cov = (pm.gp.cov.ExpQuad(1, â„“) +Â </code>&#13;
<code>Â Â Â Â Â Â Â Â Â Â Â Ï„Â * pm.gp.cov.Linear(1, c) +Â </code>&#13;
<code>Â Â Â Â Â Â Â Â Â Â Â pm.gp.cov.WhiteNoise(1E-5))Â </code>&#13;
<code>Â Â Â Â gp = pm.gp.Latent(cov_func=cov)Â </code>&#13;
<code>Â Â Â Â f = gp.prior("f", X=X_1)Â </code>&#13;
<code>Â Â Â Â # logistic inverse link function and Bernoulli likelihoodÂ </code>&#13;
<code>Â Â Â Â y_ = pm.Bernoulli("y", p=pm.math.sigmoid(f), observed=y)Â </code>&#13;
<code>Â Â Â Â idata_iris2 = pm.sample()</code></pre>&#13;
<p>We can see the result of this model in <em>Figure <a href="#x1-165027r9">8.9</a></em>. Notice how this figure looks much more similar now to <em>Figure <a href="CH04.xhtml#x1-85023r11">4.11</a></em> than <em>Figure <a href="#x1-165012r8">8.8</a></em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file230.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-165027r9"/><strong>FigureÂ 8.9</strong>: Logistic regression, result of <code>model_lrs</code></p>&#13;
<p>The example discussed in this section has two main aims:</p>&#13;
<ul>&#13;
<li><p>Showing how we can easily combine kernels to get a more expressive model</p></li>&#13;
<li><p>Showing how we can <em>recover</em> a logistic regression using a Gaussian process</p></li>&#13;
</ul>&#13;
<p>Regarding the second point, logistic regression is indeed a special case of Gaussian processes, because a simple linear regression is just a particular case of a Gaussian process. In fact, many known models can be seen as special cases of GPs, or at least they are somehow connected to GPs. If you want to learn more about this, you can read Chapter 15 from Kevin Murphyâ€™s Machine Learning: A Probabilistic Perspective (first edition) [<a href="Bibliography.xhtml#Xpml0Book">Murphy</a>,Â <a href="Bibliography.xhtml#Xpml0Book">2012</a>], and also Chapter 18 from the second edition [<a href="Bibliography.xhtml#Xpml2Book">Murphy</a>,Â <a href="Bibliography.xhtml#Xpml2Book">2023</a>]. <span id="x1-165028r387"/></p>&#13;
<section id="gps-for-space-flu" class="level4 subsectionHead" data-number="1.12.7.1">&#13;
<h3 class="subsectionHead" data-number="1.12.7.1">8.7.1 <span id="x1-1660001"/>GPs for space flu</h3>&#13;
<p><span id="dx1-166001"/></p>&#13;
<p>In practice, it does not make too much sense to use a GP to model a problem we can just solve with a logistic regression. Instead, we want to use a GP to model more complex data that is not well captured with less flexible models. For instance, suppose we want to model the probability of getting a disease as a function of age. It turns out that very young and very old people have a higher risk than people of middle age. The dataset <code>space_flu.csv </code>is a synthetic dataset inspired by the previous description. <em>Figure <a href="#x1-166011r10">8.10</a></em> shows a plot of it.</p>&#13;
<p>Letâ€™s fit the following model and plot the results:</p>&#13;
<p><span id="x1-166002r9"/> <span id="x1-166003"/><strong>CodeÂ 8.9</strong></p>&#13;
<pre id="listing-110" class="source-code"><code>with pm.Model() as model_space_flu:Â </code>&#13;
<code>Â Â Â Â â„“Â = pm.InverseGamma('â„“', *get_ig_params(age))Â </code>&#13;
<code>Â Â Â Â cov = pm.gp.cov.ExpQuad(1, â„“) + pm.gp.cov.WhiteNoise(1E-5)Â </code>&#13;
<code>Â Â Â Â gp = pm.gp.Latent(cov_func=cov)Â </code>&#13;
<code>Â Â Â Â f = gp.prior('f', X=age)Â </code>&#13;
<code>Â Â Â Â y_ = pm.Bernoulli('y', p=pm.math.sigmoid(f), observed=space_flu)Â </code>&#13;
<code>Â Â Â Â idata_space_flu = pm.sample()</code></pre>&#13;
<p>Notice, as illustrated in <em>Figure <a href="#x1-166011r10">8.10</a></em>, that the GP can fit this space flu dataset very well, even when the data demands the function to be more complex than a logistic one. Fitting this data well will be impossible for a simple logistic regression, unless we introduce some ad hoc modifications to help it a little bit (see exercise 6 at the end of the chapter for a discussion of such modifications).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file231.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-166011r10"/><strong>FigureÂ 8.10</strong>: Logistic regression, result of <code>model_space_flu</code></p>&#13;
<p><span id="x1-166012r390"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="cox-processes" class="level3 sectionHead" data-number="1.12.8">&#13;
<h2 class="sectionHead" data-number="1.12.8">8.8 <span id="x1-1670008"/>Cox processes</h2>&#13;
<p><span id="dx1-167001"/></p>&#13;
<p>Now we are going to model count data. We will see two examples; one with a time-varying rate and one with a 2D spatially varying rate. To do this, we will use a Poisson likelihood and the rate will be modeled using a Gaussian process. Because the rate of the Poisson distribution is limited to positive values, we will use an exponential as the inverse link function, as we did for the NegativeBinomial regression from <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>.</p>&#13;
<p>We can think of a Poisson process as a distribution over collections of points in a given space where every finite collection of those random variables has a Poisson distribution. When the rate of the Poisson process is itself a stochastic process, such as, for example, a Gaussian process, then we have a Cox process. <span id="x1-167002r395"/></p>&#13;
<section id="coal-mining-disasters" class="level4 subsectionHead" data-number="1.12.8.1">&#13;
<h3 class="subsectionHead" data-number="1.12.8.1">8.8.1 <span id="x1-1680001"/>Coal mining disasters</h3>&#13;
<p><span id="dx1-168001"/></p>&#13;
<p>The first example is known as the coal mining disasters. This example consists of a record of coal-mining disasters in the UK from 1851 to 1962. The number of disasters is thought to have been affected by changes in safety regulations during this period. We want to model the rate of disasters as a function of time. Our dataset consists of a single column and each entry corresponds to the time a disaster happened. The model we will use to fit the data has the form:</p>&#13;
<div class="math-display">&#13;
<img src="../media/Formula_01.PNG" style="width: 15em;"/>&#13;
</div>&#13;
<p>As you can see, this is a Poisson regression problem. You may be wondering at this point how weâ€™re going to perform a regression if we only have a single column with just the date of the disasters. The answer is to discretize the data, just as if we were building a histogram. We are going to use the centers of the bins as the <em>X</em> variable and the counts per bin as the <em>Y</em> variable:</p>&#13;
<p><span id="x1-168002r10"/> <span id="x1-168003"/><strong>CodeÂ 8.10</strong></p>&#13;
<pre id="listing-111" class="source-code"><code># discretize dataÂ </code>&#13;
<code>years = int((coal_df.max() - coal_df.min()).iloc[0])Â </code>&#13;
<code>bins = years // 4Â </code>&#13;
<code>hist, x_edges = np.histogram(coal_df, bins=bins)Â </code>&#13;
<code># Compute the location of the centers of the discretized dataÂ </code>&#13;
<code>x_centers = x_edges[:-1] + (x_edges[1] - x_edges[0]) / 2Â </code>&#13;
<code># xdata needs to be 2D for BARTÂ </code>&#13;
<code>x_data = x_centers[:, None]Â </code>&#13;
<code># express data as the rate number of disasters per yearÂ </code>&#13;
<code>y_data = hist</code></pre>&#13;
<p>Now we define and solve the model with PyMC:</p>&#13;
<p><span id="x1-168014r11"/> <span id="x1-168015"/><strong>CodeÂ 8.11</strong></p>&#13;
<pre id="listing-112" class="source-code"><code>with pm.Model() as model_coal:Â </code>&#13;
<code>Â Â Â Â â„“Â = pm.InverseGamma('â„“', *get_ig_params(x_edges))Â </code>&#13;
<code>Â Â Â Â cov = pm.gp.cov.ExpQuad(1, ls=â„“) + pm.gp.cov.WhiteNoise(1E-5)Â </code>&#13;
<code>Â Â Â Â gp = pm.gp.Latent(cov_func=cov)Â </code>&#13;
<code>Â Â Â Â f = gp.prior('f', X=x_data)Â </code>&#13;
<code>Â Â Â Â y_pred = pm.Poisson('y_pred', mu=pm.math.exp(f), observed=y_data)Â </code>&#13;
<code>Â Â Â Â idata_coal = pm.sample()</code></pre>&#13;
<p><em>Figure <a href="#x1-168023r11">8.11</a></em> shows the median disaster rate as a function of time (white line). The bands describe the 50% HDI (darker) and the 94% HDI (lighter). At the bottom, the black markers indicate the moment of each disaster. As we can see, the rate of accidents decreases with time, except for a brief initial increase. The PyMC documentation includes the coal mining disaster but is modeled from a different perspective. I strongly recommend that you check that example as it is very useful on its own and is also useful to compare it with the approach we just implemented here.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file232.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-168023r11"/><strong>FigureÂ 8.11</strong>: Logistic regression, result of <code>model_coal</code></p>&#13;
<p>Notice that even when we binned the data, we obtained, as a result, a smooth curve. In this sense, we can see <code>model_coal </code>(and, in general, this type of model) as building a histogram and then smoothing it. <span id="x1-168024r399"/></p>&#13;
</section>&#13;
<section id="red-wood" class="level4 subsectionHead" data-number="1.12.8.2">&#13;
<h3 class="subsectionHead" data-number="1.12.8.2">8.8.2 <span id="x1-1690002"/>Red wood</h3>&#13;
<p><span id="dx1-169001"/></p>&#13;
<p>Letâ€™s apply the same approach we just did to a 2D spatial problem. We are going to use the redwood data as shown in <em>Figure <a href="#x1-169015r12">8.12</a></em>. This dataset (distributed with a GPL license) is from the GPstuff package. The dataset consists of the location of redwood trees over a given area. The motivation of the inference is to obtain a map of a rate, the number of trees in a given area.</p>&#13;
<p>As with the coal-mining disaster example, we need to discretize the data:</p>&#13;
<p><span id="x1-169002r12"/> <span id="x1-169003"/><strong>CodeÂ 8.12</strong></p>&#13;
<pre id="listing-113" class="source-code"><code># discretize spatial dataÂ </code>&#13;
<code>bins = 20Â </code>&#13;
<code>hist, x1_edges, x2_edges = np.histogram2d(Â </code>&#13;
<code>Â Â Â Â rw_df[1].values, rw_df[0].values, bins=bins)Â </code>&#13;
<code># compute the location of the centers of the discretized dataÂ </code>&#13;
<code>x1_centers = x1_edges[:-1] + (x1_edges[1] - x1_edges[0]) / 2Â </code>&#13;
<code>x2_centers = x2_edges[:-1] + (x2_edges[1] - x2_edges[0]) / 2Â </code>&#13;
<code># arrange xdata into proper shape for GPÂ </code>&#13;
<code>x_data = [x1_centers[:, None], x2_centers[:, None]]Â </code>&#13;
<code># arrange ydata into proper shape for GPÂ </code>&#13;
<code>y_data = hist.flatten().astype(int)</code></pre>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file233.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-169015r12"/><strong>FigureÂ 8.12</strong>: Redwood data</p>&#13;
<p>Notice that instead of doing a mesh grid, we treat <code>x1 </code>and <code>x2 </code>data as being distinct arrays. This allows us to build a covariance matrix independently for each coordinate, effectively reducing the size of the matrix needed to compute the GP. We then combine both matrices using the <code>LatentKron </code>class. It is important to note that this is not a numerical trick, but a mathematical property of the structure of this type of matrix, so we are not introducing any approximation or error in our model. We are just expressing it in a way that allows faster computations:</p>&#13;
<p><span id="x1-169016r13"/> <span id="x1-169017"/><strong>CodeÂ 8.13</strong></p>&#13;
<pre id="listing-114" class="source-code"><code>with pm.Model() as model_rw:Â </code>&#13;
<code>Â Â Â Â â„“Â = pm.InverseGamma('â„“', *get_ig_params(x_data), shape=2)Â </code>&#13;
<code>Â Â Â Â cov_func1 = pm.gp.cov.ExpQuad(1, ls=â„“[0])Â </code>&#13;
<code>Â Â Â Â cov_func2 = pm.gp.cov.ExpQuad(1, ls=â„“[1])Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â gp = pm.gp.LatentKron(cov_funcs=[cov_func1, cov_func2])Â </code>&#13;
<code>Â Â Â Â f = gp.prior('f', Xs=x_data)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â y = pm.Poisson('y', mu=pm.math.exp(f), observed=y_data)Â </code>&#13;
<code>Â Â Â Â idata_rw = pm.sample()</code></pre>&#13;
<p>In <em>Figure <a href="#x1-169028r13">8.13</a></em>, the darker the shade of gray, the higher the rate of trees. We may imagine that we are interested in finding high-growing rate zones, because we may be interested in how a wood is recovering from a fire, or maybe we are interested in some properties of the soil and we use the trees as a proxy.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file234.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-169028r13"/><strong>FigureÂ 8.13</strong>: Logistic regression, result of <code>model_rw</code></p>&#13;
<p><span id="x1-169029r398"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="regression-with-spatial-autocorrelation" class="level3 sectionHead" data-number="1.12.9">&#13;
<h2 class="sectionHead" data-number="1.12.9">8.9 <span id="x1-1700009"/>Regression with spatial autocorrelation</h2>&#13;
<p><span id="dx1-170001"/></p>&#13;
<p>The following example is taken from <em>Statistical Rethinking: A Bayesian Course with Examples in R and STAN, Second Edition by Richard McElreath, Copyright (2020) by Chapman and Hall/CRC. Reproduced by permission of Taylor &amp; Francis Group</em>. I strongly recommend reading this book, as you will find many good examples like this and very good explanations. The only <em>caveat</em> is that the book examples are in R/Stan, but donâ€™t worry and keep sampling; you will find the Python/PyMC version of those examples in the <a href="https://github.com/pymc-devs/pymc-resources" class="url">https://github.com/pymc-devs/pymc-resources</a> resources.</p>&#13;
<p>For this example we have 10 different island societies; for each one of them, we have the number of tools they use. Some theories predict that larger populations develop and sustain more tools than smaller populations. Thus, we have a regression problem where the dependent variable is the number of tools and the independent variable is the population. Because the number of tools is a count variable, we can use a Poisson distribution. Additionally, we have good theoretical reasons to think the logarithm of the population is a better variable than absolute size because what really matters (according to the theory) is the order of magnitude of the population.</p>&#13;
<p>So far, the model we have in mind is a Poisson regression, but here comes the interesting part. Another important factor affecting the number of tools is the contact rates among the island societies. One way to include the contact rate in our model is to gather information on how frequent these societies were in contact throughout history and to create a categorical variable such as low/high rate. Yet another way is to use the distance between societies as a proxy of the contact rate, since it is reasonable to assume that geographically close societies come into contact more often than distant ones.</p>&#13;
<p>The number of tools, the population size, and the coordinates are stored in the file <code>islands.csv </code>in the GitHub repo of this book (<a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>).</p>&#13;
<p>Omitting the priors, the model we are going to build is:</p>&#13;
<div class="math-display">&#13;
<img src="../media/Formula_02.PNG" style="width: 18em;"/>&#13;
</div>&#13;
<p>This model is a linear model plus a GP term. We use the linear part to model the effect of the logarithm of the population and the GP term to model the effect of the distance/contact rate. In this way, we will be effectively incorporating a measure of similarity in technology exposure (estimated from the distance matrix). Thus, instead of assuming the total number is just a consequence of population alone and independent from one society to the next, we will be modeling the number of tools in each society as a function of their spatial distribution.</p>&#13;
<p>The information about the spatial distribution is in terms of latitudes and longitudes, but the kernels in PyMC assume the distances are all Euclidean. This can be problematic. Probably the cleanest way to circumvent this issue is to work with a distance that takes into account that the islands are on an approximately spherical planet. For instance, we can use the haversine distance, which determines the great-circle distance between two points on a sphere given their longitudes and latitudes. The great-circle distance is the shortest distance between two points on the surface of a sphere, measured along the surface of the sphere. To use this distance, we need to create a new kernel as shown in the next code block. If you are not very familiar with classes in Python, you just need to know that what I did is copy the code for the <code>ExpQuad</code> from the PyMC code base and tweak it a little bit to create a new class, <code>ExpQuadHaversine</code>. The largest change is the addition of the function/method <code>haversine_distance</code>.</p>&#13;
<p><span id="x1-170002r14"/> <span id="x1-170003"/><strong>CodeÂ 8.14</strong></p>&#13;
<pre id="listing-115" class="source-code"><code>class ExpQuadHaversine(pm.gp.cov.Stationary):Â </code>&#13;
<code>Â Â Â Â def __init__(self, input_dims, ls, ls_inv=None, r=6371, active_dims=None):Â </code>&#13;
<code>Â Â Â Â Â Â Â Â super().__init__(input_dims, ls=ls, ls_inv=ls_inv, active_dims=active_dims)Â </code>&#13;
<code>Â Â Â Â Â Â Â Â self.r = r # earth radius in kmÂ </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â def haversine_distance(self, X):Â </code>&#13;
<code>Â Â Â Â Â Â Â Â lat = np.radians(X[:, 0])Â </code>&#13;
<code>Â Â Â Â Â Â Â Â lon = np.radians(X[:, 1])Â </code>&#13;
<code>Â Â Â Â Â Â Â Â latd = lat[:,None] - latÂ </code>&#13;
<code>Â Â Â Â Â Â Â Â lond = lon[:,None] - lonÂ </code>&#13;
<code>Â Â Â Â Â Â Â Â d = pt.cos(lat[:,None]) * pt.cos(lat)Â </code>&#13;
<code>Â Â Â Â Â Â Â Â a = pt.sin(latd / 2)** 2 + d * pt.sin(lond / 2)** 2Â </code>&#13;
<code>Â Â Â Â Â Â Â Â c = 2 * pt.arctan2(pt.sqrt(a), pt.sqrt(1 - a))Â </code>&#13;
<code>Â Â Â Â Â Â Â Â return self.r * cÂ </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â def full(self, X, _):Â </code>&#13;
<code>Â Â Â Â Â Â Â Â return pt.exp(-0.5 * self.haversine_distance(X)**2)</code></pre>&#13;
<p>Now that we have defined the class <code>ExpQuadHaversine </code>we can use it to define the covariance matrix as we did with the previous models with the built-in kernels. For this model, we are going to introduce another change. We are going to define a parameter <em>Î·</em>. The role of this parameter is to scale the GP in the y-axis direction. It is pretty common to define GPs with both <em>â„“</em> and <em>Î·</em>.</p>&#13;
<p><span id="x1-170021r15"/> <span id="x1-170022"/><strong>CodeÂ 8.15</strong></p>&#13;
<pre id="listing-116" class="source-code"><code>with pm.Model() as model_islands:Â </code>&#13;
<code>Â Â Â Â Î·Â = pm.Exponential('Î·', 2)Â </code>&#13;
<code>Â Â Â Â â„“Â = pm.InverseGamma('â„“', *get_ig_params(islands_dist))Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â cov = Î·Â * ExpQuadHaversine(2, ls=â„“)Â </code>&#13;
<code>Â Â Â Â gp = pm.gp.Latent(cov_func=cov)Â </code>&#13;
<code>Â Â Â Â f = gp.prior('f', X=X)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â <em>Î±</em>Â = pm.Normal('<em>Î±</em>', 0, 5)Â </code>&#13;
<code>Â Â Â Â <em>Î²</em>Â = pm.Normal('<em>Î²</em>', 0, 1)Â </code>&#13;
<code>Â Â Â Â Î¼Â = pm.math.exp(<em>Î±</em> + <em>Î²</em>Â * log_pop + f)Â </code>&#13;
<code>Â Â Â Â _ = pm.Poisson('tt_pred', Î¼, observed=total_tools)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â idata_islands = pm.sample()</code></pre>&#13;
<p>To understand the posterior distribution of covariance functions in terms of distances, we can plot a few samples from the posterior distribution as in <em>Figure <a href="#x1-170037r14">8.14</a></em>. The black curve represents the posterior median covariance at each distance and the gray curves sample functions from the joint posterior distribution of <em>â„“</em> and <em>Î·</em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file235.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-170037r14"/><strong>FigureÂ 8.14</strong>: Posterior distribution of the spatial covariance</p>&#13;
<p>The thick black line in <em>Figure <a href="#x1-170037r14">8.14</a></em> is the posterior median of the covariance between pairs of societies as a function of distance. We use the median because the distributions for <em>â„“</em> and <em>Î·</em> are very skewed. We can see that the covariance is, on average, not that high and also drops to almost 0 at about 2,000 kilometers. The thin lines represent the uncertainty, and we can see that there is a lot of uncertainty.</p>&#13;
<p>Now letâ€™s take a look at how strongly correlated the island societies are according to the model and data. To do this, we have to turn the covariance matrix into a correlation matrix. See the accompanying code for details. <em>Figure <a href="#x1-170038r15">8.15</a></em> shows a heatmap of the mean correlation matrix.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file236.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-170038r15"/><strong>FigureÂ 8.15</strong>: Posterior mean correlation matrix</p>&#13;
<p>Two observations that stand out from the rest is, first, that Hawaii is very lonely. This makes sense, as Hawaii is very far away from the rest of the island societies. Also, we can see that Malekula (Ml), Tikopia (Ti), and Santa Cruz (SC) are highly correlated with one another. This also makes sense, as these societies are very close together, and they also have a similar number of tools.</p>&#13;
<p>The left panel of <em>Figure <a href="#x1-170039r16">8.16</a></em> is essentially a map. The island societies are represented in their relative positions. The lines are the posterior median correlations among societies. The opacity of the lines is proportional to the value of the correlations.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file237.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-170039r16"/><strong>FigureÂ 8.16</strong>: Posterior distribution of the spatial covariance</p>&#13;
<p>On the right panel of <em>Figure <a href="#x1-170039r16">8.16</a></em> , we have again the posterior median correlations, but this time plotted in terms of the log population versus the total number of tools. The dashed lines represent the median number of tools and the HDI of 94% as a function of log population. In both panels of <em>Figure <a href="#x1-170039r16">8.16</a></em>, the size of the dots is proportional to the population of each island society. Notice how the correlations among Malekula, Tikopia, and Santa Cruz describe the fact that they have a rather low number of tools close to the median or lower than the expected number of tools for their populations. Something similar is happening with Trobriand and Manus; they are geographically close and have fewer tools than expected for their population sizes. Tonga has way more tools than expected for its population and a relatively high correlation with Fiji. In a way, the model is telling us that Tonga has a positive effect on Lua Fiji, increasing the total number of tools and counteracting the effect of it on its close neighbors, Malekula, Tikopia, and Santa Cruz. <span id="x1-170040r409"/></p>&#13;
</section>&#13;
<section id="hilbert-space-gps" class="level3 sectionHead" data-number="1.12.10">&#13;
<h2 class="sectionHead" data-number="1.12.10">8.10 <span id="x1-17100010"/>Hilbert space GPs</h2>&#13;
<p><span id="dx1-171001"/></p>&#13;
<p>Gaussian processes can be slow. The main reason is that their computation requires us to invert a matrix, whose size grows with the number of observations. This operation is computationally costly and does not scale very nicely. For that reason, a large portion of the research around GPs has been to find approximations to compute them faster and allow scaling them to large data.</p>&#13;
<p>We are going to discuss only one of those approximations, namely the <strong>Hilbert Space Gaussian Process</strong> (<strong>HSGP</strong>), without going into the details of how this approximation is achieved. Conceptually, we can think of it as a basis function expansion similar, in spirit, to how splines are constructed (see <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em>). The consequence of this approximation is that it turns the matrix inversion into just matrix multiplication, a much faster operation.</p>&#13;
<div id="tcolobox-19" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>But When Will It Work?</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>We can only use HSGPs for low dimensions (1 to maybe 3 or 4), and only for some kernels like the exponential quadratic or Matern. The reason is that for the HSGP approximation to work, the kernel has to be written in a special form known as power spectral density, and not all kernels can be written in this form.</p>&#13;
</div>&#13;
</div>&#13;
<p>Using the HSGP approximation in PyMC is straightforward, as we will demonstrate with the bikes dataset. We want to model the number of rented bikes as a function of the time of the day in hours. The following code block shows the PyMC implementation of such a model.</p>&#13;
<p><span id="x1-171002r16"/> <span id="x1-171003"/><strong>CodeÂ 8.16</strong></p>&#13;
<pre id="listing-117" class="source-code"><code>with pm.Model() as model_hsgp:Â </code>&#13;
<code>Â Â Â Â â„“Â = pm.InverseGamma('â„“', *get_ig_params(X))Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â cov = pm.gp.cov.ExpQuad(1, ls=â„“)Â </code>&#13;
<code>Â Â Â Â gp = pm.gp.HSGP(m=[10], c=1.5, cov_func=cov)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â f = gp.prior('f', X=X)Â </code>&#13;
<code>Â Â Â Â <em>Î±</em>Â = pm.HalfNormal('<em>Î±</em>', 1)Â </code>&#13;
<code>Â Â Â Â _ = pm.NegativeBinomial("obs", np.exp(f), <em>Î±</em>, observed=y)Â </code>&#13;
<code>Â </code>&#13;
<code>Â Â Â Â idata_hsgp = pm.sample()</code></pre>&#13;
<p>The main difference from previous GP models is the use of the <code>pm.gp.HSGP(.)</code> class instead of the <code>pm.gp.Latent(.) </code>class, which we should have used for non-Gaussian likelihoods and standard GPs. The class <code>pm.gp.HSGP(.) </code>has two parameters:</p>&#13;
<ul>&#13;
<li><p><code>m </code>is the number of basic functions we use to approximate the GP. The larger the value of <code>m</code>, the better the approximation will be and the more costly the computation.</p></li>&#13;
<li><p><code>c </code>is a boundary factor. For a fixed and sufficiently large value of <code>m</code>, <code>c </code>affects the approximation of the mean function mainly near the boundaries. It should not be smaller than 1.2 (PyMC will give you a warning if you use a value lower than this), and usually 1.5 is a good choice. Changing this parameter does not affect the speed of the computations.</p></li>&#13;
</ul>&#13;
<p>We set <code>m=10 </code>partially because we are fans of the decimal system and partially based on the recommendations in the paper <em>Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming</em> written by <a href="Bibliography.xhtml#Xriutortmayol_2022">Riutort-Mayol etÂ al.</a>Â [<a href="Bibliography.xhtml#Xriutortmayol_2022">2022</a>]. In practice, the results are robust to the exact values of <code>m </code>and <code>c</code>, as long as they are within a certain range based on what your prior for the length scale is. For details on how HSGP works and some advice on how to use it in practice, you can read <a href="Bibliography.xhtml#Xriutortmayol_2022">Riutort-Mayol etÂ al.</a>Â [<a href="Bibliography.xhtml#Xriutortmayol_2022">2022</a>].</p>&#13;
<p>Now letâ€™s see the results. <em>Figure <a href="#x1-171015r17">8.17</a></em> shows the mean posterior GP in black and 100 samples (realizations) from the GP posterior (gray lines). You can compare these results to the ones obtained using splines (see <em>Figure <a href="CH06.xhtml#x1-124013r8">6.8</a></em>).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file238.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-171015r17"/><strong>FigureÂ 8.17</strong>: Posterior mean for the HSGP model for rented bikes as a function of the time of the day</p>&#13;
<p>The HSGP approximation is also implemented in Bambi. Letâ€™s see how we can use it. <span id="x1-171016r404"/></p>&#13;
<section id="hsgp-with-bambi" class="level4 subsectionHead" data-number="1.12.10.1">&#13;
<h3 class="subsectionHead" data-number="1.12.10.1">8.10.1 <span id="x1-1720001"/>HSGP with Bambi</h3>&#13;
<p><span id="dx1-172001"/> <span id="dx1-172002"/></p>&#13;
<p>To fit the previous model with Bambi, we need to write the following:</p>&#13;
<p><span id="x1-172003r17"/> <span id="x1-172004"/><strong>CodeÂ 8.17</strong></p>&#13;
<pre id="listing-118" class="source-code"><code>bmb.Model("rented âˆ¼Â 0 + hsgp(hour, m=10, c=1.5)", bikes,Â </code>&#13;
<code>Â Â Â Â Â Â Â Â Â Â family="negativebinomial")</code></pre>&#13;
<p>This will work, but instead, we will provide priors to Bambi, as we did with PyMC. This will result in a much faster sampling and more reliable samples.</p>&#13;
<p>As we saw in <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em>, to define priors in Bambi, we just need to pass a dictionary to the <code>priors </code>argument of <code>bmb.Model</code>. But we must be aware that HSGP terms do not receive priors. Instead, we need to define priors for <em>â„“</em> (called <code>ell </code>in Bambi) and <em>Î·</em> (called <code>sigma </code>in Bambi) and pass those priors to the HSGP terms. One more thing: as in the previous model, we did not use <em>Î·</em> but since Bambi is expecting it, we use a dirty trick to define a prior that is essentially 1.</p>&#13;
<p><span id="x1-172007r18"/> <span id="x1-172008"/><strong>CodeÂ 8.18</strong></p>&#13;
<pre id="listing-119" class="source-code"><code>prior_gp = {Â </code>&#13;
<code>Â Â Â Â "sigma": bmb.Prior("Gamma", mu=1, sigma=0.01),Â </code>&#13;
<code>Â Â Â Â "ell": bmb.Prior("InverseGamma", **get_ig_params(X))Â </code>&#13;
<code>}Â </code>&#13;
<code>priors = {Â </code>&#13;
<code>Â Â Â Â "hsgp(hour, m=10, c=1.5)": prior_gp,Â </code>&#13;
<code>Â Â Â Â "alpha": bmb.Prior("HalfNormal", sigma=1)Â </code>&#13;
<code>}Â </code>&#13;
<code>Â </code>&#13;
<code>model_hsb = bmb.Model("rented âˆ¼Â 0 + hsgp(hour, m=10, c=1.5)", bikes,Â </code>&#13;
<code>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â family="negativebinomial",Â </code>&#13;
<code>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â priors=priors)Â </code>&#13;
<code>Â </code>&#13;
<code>idata_hsb = model_hsb.fit()</code></pre>&#13;
<p>I invite you to check that the parameters computed by Bambi are very similar to those we got with PyMC. <em>Figure <a href="#x1-172023r18">8.18</a></em> shows the mean posterior GP in black and a band for the HDI of 94%. The figure was generated with <code>bmb.interpret.plot_predictions</code>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file239.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-172023r18"/><strong>FigureÂ 8.18</strong>: Posterior mean for the HSGP model for rented bikes as a function of the time of the day, using Bambi</p>&#13;
<p>In this section, we have explored the concept of HSGP as a powerful approximation to scale Gaussian processes to large datasets. By combining the flexibility of PyMC and Bambi with the scalability offered by HSGPs, researchers and practitioners can more effectively tackle complex modeling tasks, paving the way for the application of Gaussian processes on increasingly large and intricate datasets. <span id="x1-172024r414"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="summary-7" class="level3 sectionHead" data-number="1.12.11">&#13;
<h2 class="sectionHead" data-number="1.12.11">8.11 <span id="x1-17300011"/>Summary</h2>&#13;
<p>A Gaussian process is a generalization of the multivariate Gaussian distribution to infinitely many dimensions and is fully specified by a mean function and a covariance function. Since we can conceptually think of functions as infinitely long vectors, we can use Gaussian processes as priors over functions. In practice, we do not work with infinite objects but with multivariate Gaussian distributions with as many dimensions as data points. To define their corresponding covariance function, we used properly parameterized kernels; and by learning about those hyperparameters, we ended up learning about arbitrary complex functions.</p>&#13;
<p>In this chapter, we have given a short introduction to GPs. We have covered regression, semi-parametric models (the islands example), combining two or more kernels to better describe the unknown function, and how a GP can be used for classification tasks. There are many other topics we could have discussed. Nevertheless, I hope this introduction to GPs has motivated you sufficiently to keep using, reading, and learning about Gaussian processes and Bayesian non-parametric models. <span id="x1-173001r422"/></p>&#13;
</section>&#13;
<section id="exercises-7" class="level3 sectionHead" data-number="1.12.12">&#13;
<h2 class="sectionHead" data-number="1.12.12">8.12 <span id="x1-17400012"/>Exercises</h2>&#13;
<ol>&#13;
<li><div id="x1-174002x1">&#13;
<p>For the example in the <em>Covariance functions and kernels</em> section, make sure you understand the relationship between the input data and the generated covariance matrix. Try using other input such as <code>data = np.random.normal(size=4)</code>.</p>&#13;
</div></li>&#13;
<li><div id="x1-174004x2">&#13;
<p>Rerun the code generating <em>Figure <a href="#x1-160010r3">8.3</a></em> and increase the number of samples obtained from the GP prior to around 200. In the original figure, the number of samples is 2. What is the range of the generated values?</p>&#13;
</div></li>&#13;
<li><div id="x1-174006x3">&#13;
<p>For the generated plot in the previous exercise, compute the standard deviation for the values at each point. Do this in the following form:</p>&#13;
<ul>&#13;
<li><p>Visually, just observing the plots</p></li>&#13;
<li><p>Directly from the values generated from <code>pz.MVNormal(.).rvs</code></p></li>&#13;
<li><p>By inspecting the covariance matrix (if you have doubts go back to exercise 1)</p></li>&#13;
</ul>&#13;
<p>Did the values you get from these three methods match?</p>&#13;
</div></li>&#13;
<li><div id="x1-174008x4">&#13;
<p>Use test points <code>np.linspace(np.floor(x.min()), 20, 100)[:,None]</code> and re-run <code>model_reg</code>. Plot the results. What did you observe? How is this related to the specification of the GP prior?</p>&#13;
</div></li>&#13;
<li><div id="x1-174010x5">&#13;
<p>Repeat exercise 1, but this time use a linear kernel (see the accompanying code for a linear kernel).</p>&#13;
</div></li>&#13;
<li><div id="x1-174012x6">&#13;
<p>Check out <a href="https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html" class="url">https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html</a> in PyMCâ€™s documentation.</p>&#13;
</div></li>&#13;
<li><div id="x1-174014x7">&#13;
<p>Run a logistic regression model for the <code>space_flu </code>data. What do you see? Can you explain the result?</p>&#13;
</div></li>&#13;
<li><div id="x1-174016x8">&#13;
<p>Change the logistic regression model in order to fit the data. Tip: Use an order 2 polynomial.</p>&#13;
</div></li>&#13;
<li><div id="x1-174018x9">&#13;
<p>Compare the model for the coal mining disaster with the one from the PyMC documentation ( <a href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters" class="url">https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#case-study-2-coal-mining-disasters</a>). Describe the differences between both models in terms of model specification and results.</p>&#13;
</div></li>&#13;
</ol>&#13;
</section>&#13;
<section id="join-our-community-discord-space-8" class="level3 likesectionHead" data-number="1.12.13">&#13;
<h2 class="likesectionHead" data-number="1.12.13"><span id="x1-17500012"/>Join our community Discord space</h2>&#13;
<p>Join our Discord community to meet like-minded people and learn alongside more than 5000 members at: <a href="https://packt.link/bayesian">https://packt.link/bayesian</a></p>&#13;
<p><img src="../media/file1.png" alt="PIC"/></p>&#13;
<p><span id="x1-175001r368"/></p>&#13;
</section>&#13;
</section>&#13;
</body></html>
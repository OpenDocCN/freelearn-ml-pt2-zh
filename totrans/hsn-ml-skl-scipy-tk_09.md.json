["```py\n{ 0: 'T-shirt/top ', 1: 'Trouser  ', 2: 'Pullover  ', 3: 'Dress  ', 4: 'Coat  ', 5: 'Sandal  ', 6: 'Shirt  ', 7: 'Sneaker  ', 8: 'Bag  ', 9: 'Ankle boot' }\n```", "```py\nfrom sklearn.datasets import fetch_openml\nfashion_mnist = fetch_openml(data_id=40996) \n```", "```py\nlabels_s = '0 T-shirt/top \\n1 Trouser \\n2 Pullover \\n3 Dress \\n4 Coat \\n5 Sandal \\n6 Shirt \\n7 Sneaker \\n8 Bag \\n9 Ankle boot'\n\nfashion_label_translation = {\n    int(k): v for k, v in [\n        item.split(maxsplit=1) for item in labels_s.split('\\n')\n    ]\n}\n\ndef translate_label(y, translation=fashion_label_translation):\n    return pd.Series(y).apply(lambda y: translation[int(y)]).values\n```", "```py\ndef display_fashion(img, target, ax):\n\n    if len(img.shape):\n        w = int(np.sqrt(img.shape[0]))\n        img = img.reshape((w, w))\n\n    ax.imshow(img, cmap='Greys')\n    ax.set_title(f'{target}')\n    ax.grid(False)\n\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nfashion_mnist_sample = {}\n\nfashion_mnist_sample['data'], _, fashion_mnist_sample['target'], _ = train_test_split(\n    fashion_mnist['data'], fashion_mnist['target'], train_size=10000\n)\n\nx, y = fashion_mnist_sample['data'], fashion_mnist_sample['target']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n```", "```py\ntranslation = fashion_label_translation\ny_train_translated = translate_label(y_train, translation=translation)\ny_test_translated = translate_label(y_test, translation=translation)\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_translated)\ny_test_encoded = le.transform(y_test_translated)\n```", "```py\nimport random \n\nfig, axs = plt.subplots(1, 10, figsize=(16, 12))\n\nfor i in range(10):\n    rand = random.choice(range(x_train.shape[0]))\n    display_fashion(x_train[rand], y_train_translated[rand], axs[i])\n\nfig.show()\n```", "```py\nfrom sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=500)\nclf.fit(x_train, y_train_encoded)\ny_test_pred = clf.predict(x_test)\n```", "```py\npd.Series(clf.loss_curve_).plot(\n    title=f'Loss Curve; stopped after {clf.n_iter_} epochs'\n)\n```", "```py\nfrom sklearn.neural_network import MLPClassifier\n\nlearning_rate_init_options = [1, 0.1, 0.01, 0.001, 0.0001]\n\nfig, axs = plt.subplots(1, len(learning_rate_init_options), figsize=(15, 5), sharex=True, sharey=True)\n\nfor i, learning_rate_init in enumerate(learning_rate_init_options):\n\n    print(f'{learning_rate_init} ', end='')\n\n    clf = MLPClassifier(\n        hidden_layer_sizes=(500, ), \n        learning_rate='constant',\n        learning_rate_init=learning_rate_init,\n        validation_fraction=0.2,\n        early_stopping=True, \n        n_iter_no_change=120,\n        max_iter=120, \n        solver='sgd',\n        batch_size=25,\n        verbose=0,\n    )\n\n    clf.fit(x_train[:1000,:], y_train_encoded[:1000])\n\n    pd.Series(clf.validation_scores_).plot(\n        title=f'learning_rate={learning_rate_init}', \n        kind='line', \n        color='k',\n        ax=axs[i]\n    )\n\nfig.show()\n```", "```py\nfrom sklearn.neural_network import MLPClassifier\n\nbatch_sizes = [1, 10, 100, 1500]\n\nfig, axs = plt.subplots(1, len(batch_sizes), figsize=(15, 5), sharex=True, sharey=True)\n\nfor i, batch_size in enumerate(batch_sizes):\n\n    print(f'{batch_size} ', end='')\n\n    clf = MLPClassifier(\n        hidden_layer_sizes=(500, ), \n        learning_rate='constant',\n        learning_rate_init=0.001, \n        momentum=0,\n        max_iter=250, \n        early_stopping=True,\n        n_iter_no_change=250,\n        solver='sgd',\n        batch_size=batch_size,\n        verbose=0,\n    )\n\n    clf.fit(x_train[:1500,:], y_train_encoded[:1500])\n\n    pd.Series(clf.validation_scores_).plot( \n        title=f'batch_size={batch_size}',\n        color='k',\n        kind='line', \n        ax=axs[i]\n    )\n\nfig.show()\n```", "```py\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes = [1, 0.75, 0.5, 0.25, 0.1, 0.05]\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n    MLPClassifier(\n        hidden_layer_sizes=(100, 100), \n        solver='adam',\n        early_stopping=False\n    ), \n    x_train, y_train_encoded,\n    train_sizes=train_sizes,\n    scoring=\"precision_macro\",\n    cv=3,\n    verbose=2,\n    n_jobs=-1\n)\n```", "```py\ndf_learning_curve = pd.DataFrame(\n    {\n        'train_sizes': train_sizes,\n        'train_scores': train_scores.mean(axis=1),\n        'test_scores': test_scores.mean(axis=1)\n    }\n).set_index('train_sizes')\n\ndf_learning_curve['train_scores'].plot(\n    title='Learning Curves', ls=':',\n)\n\ndf_learning_curve['test_scores'].plot(\n    title='Learning Curves', ls='-',\n)\n```", "```py\nfrom sklearn.model_selection import validation_curve\n\nmax_iter_range = [5, 10, 25, 50, 75, 100, 150]\n\ntrain_scores, test_scores = validation_curve(\n    MLPClassifier(\n        hidden_layer_sizes=(100, 100), \n        solver='adam',\n        early_stopping=False\n    ), \n    x_train, y_train_encoded,\n    param_name=\"max_iter\", param_range=max_iter_range,\n    scoring=\"precision_macro\",\n    cv=3,\n    verbose=2,\n    n_jobs=-1\n)\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'hidden_layer_sizes': [(50,), (50, 50), (100, 50), (100, 100), (500, 100), (500, 100, 100)],\n    'activation': ['logistic', 'tanh', 'relu'],\n    'learning_rate_init': [0.01, 0.001],\n    'solver': ['sgd', 'adam'],\n}\n\ngs = GridSearchCV(\n    estimator=MLPClassifier(\n        max_iter=50,\n        batch_size=50,\n        early_stopping=True,\n    ), \n    param_grid=param_grid,\n    cv=4,\n    verbose=2,\n    n_jobs=-1\n)\n\ngs.fit(x_train[:2500,:], y_train_encoded[:2500])\n```", "```py\ndef relu(X):\n    return np.clip(X, 0, np.finfo(X.dtype).max)\n\ndefinplace_relu_derivative(Z, delta):\n    delta[Z==0] =0\n```", "```py\nleaky_relu_slope = 0.01\n\ndef leaky_relu(X):\n    X_min = leaky_relu_slope * np.array(X)\n    return np.clip(X, X_min, np.finfo(X.dtype).max)\n\ndef inplace_leaky_relu_derivative(Z, delta):\n    delta[Z < 0] = leaky_relu_slope * delta[Z < 0]\n```", "```py\nfrom sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES\n\nACTIVATIONS['leaky_relu'] = leaky_relu\nDERIVATIVES['leaky_relu'] = inplace_leaky_relu_derivative\n```", "```py\nclf = MLPClassifier(activation='leaky_relu')\n```", "```py\nx_example = array(\n    [[0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0],\n     [0, 0, 1, 1, 0],\n     [0, 0, 1, 1, 0],\n     [0, 0, 0, 0, 0]]\n)\n```", "```py\nfrom scipy import ndimage\n\nkernel = [[1,1,1],[1,1,1],[1,1,1]] \nx_example_convolve = ndimage.convolve(x_example, kernel, mode='constant', cval=0)\nx_example_convolve = x_example_convolve / 9 \n```", "```py\nfrom scipy import ndimage\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef convolve(x, kernel=[[1,1,1],[1,1,1],[1,1,1]]):\n    w = int(np.sqrt(x.shape[1]))\n    x = ndimage.convolve(\n        x.reshape((x.shape[0], w, w)), [kernel], \n        mode='constant', cval=0.0\n    ) \n    x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]) \n    return MinMaxScaler().fit_transform(x)\n```", "```py\nsharpen_kernel = [[0,-1,0], [-1,5,-1], [0,-1,0]]\nx_train_conv = convolve(x_train, sharpen_kernel)\nx_test_conv = convolve(x_test, sharpen_kernel)\n```", "```py\npipinstallscikit-image\n```", "```py\n\nfrom skimage.measure import block_reduce\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef maxpool(x, size=(2,2)):\n    w = int(np.sqrt(x.shape[1]))\n    x = np.array([block_reduce(img.reshape((w, w)), block_size=(size[0], size[1]), func=np.max) for img in x])\n    x = x.reshape(x.shape[0], x.shape[1]*x.shape[2]) \n    return MinMaxScaler().fit_transform(x)\n```", "```py\nx_train_maxpool = maxpool(x_train_conv, size=(5,5))\nx_test_maxpool = maxpool(x_test_conv, size=(5,5))\n```", "```py\nclass ConvolutionTransformer:\n\n    def __init__(self, kernel=[], max_pool=False, max_pool_size=(2,2)):\n        self.kernel = kernel\n        self.max_pool = max_pool\n        self.max_pool_size = max_pool_size\n\n    def fit(self, x):\n        return x\n\n    def transform(self, x, y=None):\n        x = convolve(x, self.kernel)\n        if self.max_pool:\n            x = maxpool(x, self.max_pool_size)\n        return x\n\n    def fit_transform(self, x, y=None):\n        x = self.fit(x)\n        return self.transform(x)\n```", "```py\nkernels = [\n    ('Sharpen', [[0,-1,0], [-1,5,-1], [0,-1,0]]),\n    ('V-Edge', [[-1,0,1], [-2,0,2], [-1,0,1]]),\n    ('H-Edge', [[-1,-2,-1], [0,0,0], [1,2,1]]),\n]\n\nfrom sklearn.pipeline import FeatureUnion\n\nfunion = FeatureUnion(\n    [\n        (kernel[0], ConvolutionTransformer(kernel=kernel[1], max_pool=True, max_pool_size=(4,4)))\n        for kernel in kernels\n    ]\n)\n\nx_train_convs = funion.fit_transform(x_train)\nx_test_convs = funion.fit_transform(x_test)\n```", "```py\nfrom sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(\n    hidden_layer_sizes=(500, 300),\n    activation='relu',\n    learning_rate_init=0.01,\n    solver='adam',\n    max_iter=80,\n    batch_size=50,\n    early_stopping=True,\n)\n\nmlp.fit(x_train_convs, y_train)\ny_test_predict = mlp.predict(x_test_convs)\n```"]
<html><head></head><body>
		<div>
			<div id="_idContainer082" class="Content">
			</div>
		</div>
		<div id="_idContainer083" class="Content">
			<h1 id="_idParaDest-73"><a id="_idTextAnchor074"/>4. Dimensionality Reduction Techniques and PCA</h1>
		</div>
		<div id="_idContainer122" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will apply dimension reduction techniques and describe the concepts behind principal components and dimensionality reduction. We will apply <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) when solving problems using scikit-learn. We will also compare manual PCA versus scikit-learn. By the end of this chapter, you will be able to reduce the size of a dataset by extracting only the most important components of variance within the data.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor075"/>Introduction</h1>
			<p>In the previous chapter, we discussed clustering algorithms and how they can be helpful to find underlying meaning in large volumes of data. This chapter investigates the use of different feature sets (or spaces) in our unsupervised learning algorithms, and we will start with a discussion regarding dimensionality reduction, specifically, <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>). We will then extend our understanding of the benefits of the different feature spaces through an exploration of two independently powerful machine learning architectures in neural network-based autoencoders. Neural networks certainly have a well-deserved reputation for being powerful models in supervised learning problems. Furthermore, through the use of an autoencoder stage, neural networks have been shown to be sufficiently flexible for their application to unsupervised learning problems. Finally, we will build on our neural network implementation and dimensionality reduction as we cover t-distributed nearest neighbors in <em class="italic">Chapter 6</em>, <em class="italic">t-Distributed Stochastic Neighbor Embedding</em>. These techniques will prove helpful when dealing with high-dimensional data, such as image processing or datasets with many features. One strong business benefit of some types of dimension reduction is that it helps to remove features that do not have much impact on final outputs. This creates opportunities to make your algorithms more efficient without any loss in performance.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/>What Is Dimensionality Reduction?</h1>
			<p>Dimensionality reduction is an important tool in any data scientist's toolkit, and due to its wide variety of use cases, is essentially assumed knowledge within the field. So, before we can consider reducing the dimensionality and why we would want to reduce it, we must first have a good understanding of what dimensionality is. To put it simply, dimensionality is the number of dimensions, features, or variables associated with a sample of data. Often, this can be thought of as a number of columns in a spreadsheet, where each sample is on a new row, and each column describes an attribute of the sample. The following table is an example:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B15923_04_01.jpg" alt="Figure 4.1: Two samples of data with three different features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1: Two samples of data with three different features</p>
			<p>In the preceding table, we have two samples of data, each with three independent features or dimensions. Depending on the problem being solved, or the origin of this dataset, we may want to reduce the number of dimensions per sample without losing the provided information. This is where dimensionality reduction can be helpful. But how exactly can dimensionality reduction help us to solve problems? We will cover the applications in more detail in the following section. However, let's say that we had a very large dataset of time series data, such as echocardiogram or ECG (also known as an EKG in some countries) signals, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B15923_04_02.jpg" alt="Figure 4.2: Electrocardiogram (ECG or EKG)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2: Electrocardiogram (ECG or EKG)</p>
			<p>These signals were captured from your company's new model of watch, and we need to look for signs of a heart attack or stroke. After looking through the dataset, we can make a few observations:</p>
			<ul>
				<li>Most of the individual heartbeats are very similar.</li>
				<li>There is some noise in the data from the recording system or from the patient moving during the recording.</li>
				<li>Despite the noise, the heartbeat signals are still visible.</li>
				<li>There is a lot of data – too much to be able to process using the hardware available on the watch.</li>
			</ul>
			<p>It is in such a situation that dimensionality reduction really shines. By using dimensionality reduction, we are able to remove much of the noise from the signal, which, in turn, will assist with the performance of the algorithms that are applied to the data as well as reduce the size of the dataset to allow for reduced hardware requirements. The techniques that we are going to discuss in this chapter, in particular, PCA and autoencoders, have been well applied in research and industry to effectively process, cluster, and classify such datasets.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor077"/>Applications of Dimensionality Reduction</h2>
			<p>Before we start a detailed investigation of dimensionality reduction and PCA, we will discuss some of the common applications for these techniques:</p>
			<ul>
				<li><strong class="bold">Preprocessing/feature engineering</strong>: One of the most common applications is in the preprocessing or feature engineering stages of developing a machine learning solution. The quality of the information provided during the algorithm development, as well as the correlation between the input data and the desired result, is critical in order for a high-performing solution to be designed. In this situation, PCA can provide assistance, as we are able to isolate the most important components of information from the data and provide this to the model so that only the most relevant information is being provided. This can also have a secondary benefit in that we have reduced the number of features being provided to the model, so there can be a corresponding reduction in the number of calculations to be completed. This can reduce the overall training time for the system. An example use case of this feature engineering would be predicting whether a transaction is at risk of credit card theft. In this scenario, you may be presented with millions of transactions that each have tens or hundreds of features. This would be resource-intensive or even impossible to run a predictive algorithm on in real time; however, by using feature preprocessing, we can distill the many features down to just the top 3-4 most important ones, thereby reducing runtime. </li>
				<li><strong class="bold">Noise reduction</strong>: Dimensionality reduction can also be used as an effective noise reduction/filtering technique. It is expected that the noise within a signal or dataset does not comprise a large component of the variation within the data. Thus, we can remove some of the noise from the signal by removing the smaller components of variation and then restoring the data back to the original dataspace. In the following example, the image on the left has been filtered to the first 20 most significant sources of data, which gives us the image on the right. We can see that the quality of the image has been reduced, but the critical information is still there:<div id="_idContainer086" class="IMG---Figure"><img src="image/B15923_04_03.jpg" alt="Figure 4.3: An image filtered with dimensionality reduction&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 4.3: An image filtered with dimensionality reduction</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This photograph was taken by Arthur Brognoli from Pexels and is available for free use under <a href="https://www.pexels.com/photo-license/">https://www.pexels.com/photo-license/</a>. In this case, we have the original image on the left and the filtered image on the right.</p>
			<ul>
				<li><strong class="bold">Generating plausible artificial datasets</strong>: As PCA divides the dataset into the components of information (or variation), we can investigate the effects of each component or generate new dataset samples by adjusting the ratios between the eigenvalues. We will cover more on eigenvalues later on in this chapter. We can scale these components, which, in effect, increases or decreases the importance of that specific component. This is also referred to as <strong class="bold">statistical shape modeling</strong>, as one common method is to use it to create plausible variants of shapes. It is also used to detect facial landmarks in images in the process of <strong class="bold">active shape modeling</strong>.</li>
				<li><strong class="bold">Financial modeling/risk analysis</strong>: Dimensionality reduction provides a useful toolkit for the finance industry, since being able to consolidate a large number of individual market metrics or signals into a smaller number of components allows for faster, and more efficient, computations. Similarly, the components can be used to highlight those higher-risk products/companies.</li>
			</ul>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor078"/>The Curse of Dimensionality</h2>
			<p>Before we can understand the benefits of using dimensionality reduction techniques, we must first understand why the dimensionality of feature sets needs to be reduced at all. The <strong class="bold">curse of dimensionality</strong> is a phrase commonly used to describe issues that arise when working with data that has a high number of dimensions in the feature space; for example, the number of attributes that are collected for each sample. Consider a dataset of point locations within a game of <em class="italic">Pac-Man</em>. Your character, Pac-Man, occupies a position within the virtual world defined by two dimensions or coordinates (<em class="italic">x</em>, <em class="italic">y</em>). Let's say that we are creating a new computer enemy: an AI-driven ghost to play against, and that it requires some information regarding our character to make its own game logic decisions. For the bot to be effective, we require the player's position (<em class="italic">x</em>, <em class="italic">y</em>) and their velocity in each of the directions (<em class="italic">vx</em>, <em class="italic">vy</em>) in addition to the players last five (<em class="italic">x</em>, <em class="italic">y</em>) positions, the number of remaining hearts, and the number of remaining power pellets in the maze (power pellets temporarily allow Pac-Man to eat ghosts). Now, for each moment in time, our bot requires 16 individual features (or dimensions) to make its decisions. These 16 features correspond to 5 previous positions times the 2 <em class="italic">x</em> and <em class="italic">y</em> coordinates + the 2 <em class="italic">x</em> and <em class="italic">y</em> coordinates of the player's current position + the 2 <em class="italic">x</em> and <em class="italic">y</em> coordinates of player's velocity + 1 feature for the number of hearts + 1 feature for the power pellets = 16. This is clearly a lot more than just the two dimensions as provided by the position:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B15923_04_04.jpg" alt="Figure 4.4: Dimensions in a PacMan game&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4: Dimensions in a PacMan game</p>
			<p>To explain the concept of dimensionality reduction, we will consider a fictional dataset (see <em class="italic">Figure 4.5</em>) of <em class="italic">x</em> and <em class="italic">y</em> coordinates as features, giving two dimensions in the feature space. It should be noted that this example is by no means a mathematical proof but is rather intended to provide a means of visualizing the effect of increased dimensionality. In this dataset, we have six individual samples (or points), and we can visualize the currently occupied volume within the feature space of approximately <em class="italic">(3 – 1) x (4 – 2) = 2 x 2 = 4</em> squared units:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B15923_04_05.jpg" alt="Figure 4.5: Data in a 2D feature space&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5: Data in a 2D feature space</p>
			<p>Suppose the dataset comprises the same number of points, but with an additional feature (the <em class="italic">z</em> coordinate) to each sample. The occupied data volume is now approximately <em class="italic">2 x 2 x 2 = 8</em> cubed units. So, we now have the same number of samples, but the space enclosing the dataset is now larger. As such, the data takes up less relative volume in the available space and is now sparser. This is the curse of dimensionality; as we increase the number of available features, we increase the sparsity of the data, and, in turn, make statistically valid correlations more difficult. Looking back to our example of creating a video game bot to play against a human player, we have 16 features that are a mix of different feature types: positions, velocity, power-ups, and hearts. Depending on the range of possible values for each of these features and the variance to the dataset provided by each feature, the data could be extremely sparse. Even within the constrained world of Pac-Man, the potential variance of each of the features could be quite large, some much larger than others.</p>
			<p>So, without dealing with the sparsity of the dataset, we have more information with the additional feature(s), but may not be able to improve the performance of our machine learning model, as the statistical correlations are more difficult. What we would like to do is to keep the useful information provided by the extra features but minimize the negative effect of sparsity. This is exactly what dimensionality reduction techniques are designed to do and these can be extremely powerful in increasing the performance of your machine learning model.</p>
			<p>Throughout this chapter, we will discuss a number of different dimensionality reduction techniques and will cover one of the most important and useful methods, PCA, in greater detail with an example.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor079"/>Overview of Dimensionality Reduction Techniques</h1>
			<p>The goal of any dimensionality reduction technique is to manage the sparsity of the dataset while keeping any useful information that is provided. In our case of classification, dimensionality reduction is typically used as an important preprocessing step used before the actual classification. Most dimensionality reduction techniques aim to complete this task using a process of <strong class="bold">feature projection</strong>, which adjusts the data from the higher-dimensional space into a space with fewer dimensions to remove the sparsity from the data. Again, as a means of visualizing the projection process, consider a sphere in a 3D space. We can project the sphere into a lower 2D space into a circle with some information loss (the value for the <em class="italic">z</em> coordinate), but retaining much of the information that describes its original shape. We still know the origin, radius, and manifold (outline) of the shape, and it is still very clear that it is a circle. So, depending on the problem that we are trying to solve, we may have reduced the dimensionality while retaining the important information:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B15923_04_06.jpg" alt="Figure 4.6: A projection of a 3D sphere into a 2D space&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6: A projection of a 3D sphere into a 2D space</p>
			<p>The secondary benefit that can be obtained by preprocessing the dataset with a dimensionality reduction stage is the improved computational performance that can be achieved. As the data has been projected into a lower-dimensional space, it will contain fewer, but potentially more powerful, features. The fact that there are fewer features means that, during later classification or regression stages, the size of the dataset being processed is significantly smaller. This will potentially reduce the required system resources and processing time for classification/regression, and, in some cases, the dimensionality reduction technique can also be used directly to complete the analysis.</p>
			<p>This analogy also introduces one of the important considerations of dimensionality reduction. We are always trying to balance the information loss resulting from the projection into lower dimensional space by reducing the sparsity of the data. Depending on the nature of the problem and the dataset being used, the correct balance could present itself and be relatively straightforward. In some applications, this decision may rely on the outcome of additional validation methods, such as cross-validation (particularly in supervised learning problems) or the assessment of experts in your problem domain. In this scenario, cross-validation refers to the practice of partitioning a rolling section of the data to test on, with the inverse serving as the train set until all parts of the dataset are used. This approach allows for the reduction of bias within a machine learning problem. </p>
			<p>One way we like to think about this trade-off in dimensionality reduction is to consider compressing a file or image on a computer for transfer. Dimensionality reduction techniques, such as PCA, are essentially methods of compressing information into a smaller size for transfer, and, in many compression methods, some losses occur as a result of the compression process. Sometimes, these losses are acceptable; if we are transferring a 50 MB image and need to shrink it to 5 MB for transfer, we can expect to still be able to see the main subject of the image, but perhaps some smaller background features will become too blurry to see. We would also not expect to be able to restore the original image to a pixel-perfect representation from the compressed version, but we could expect to restore it with some additional artifacts, such as blurring.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/>Dimensionality Reduction</h2>
			<p>Dimensionality reduction techniques have many uses in machine learning, as the ability to extract the useful information of a dataset can provide performance boosts in many machine learning problems. They can be particularly useful in unsupervised as opposed to supervised learning methods because the dataset does not contain any ground truth labels or targets to achieve. In unsupervised learning, the training environment is being used to organize the data in a way that is appropriate for the problem being solved (for example, classification via clustering), which is typically based on the most important information in the dataset. Dimensionality reduction provides an effective means of extracting important information, and, as there are a number of different methods that we could use, it is beneficial to review some of the available options:</p>
			<ul>
				<li><strong class="bold">Linear Discriminant Analysis</strong> (<strong class="bold">LDA</strong>): This is a particularly handy technique that can be used for both classification as well as dimensionality reduction. LDA will be covered in more detail in <em class="italic">Chapter 7</em>, <em class="italic">Topic Modeling</em>.</li>
				<li><strong class="bold">Non-negative matrix factorization</strong> (<strong class="bold">NMF</strong>): Like many of the dimensionality reduction techniques, this relies on the properties of linear algebra to reduce the number of features in the dataset. NMF will also be covered in more detail in <em class="italic">Chapter 7</em>, <em class="italic">Topic Modeling</em>.</li>
				<li><strong class="bold">Singular Value Decomposition</strong> (<strong class="bold">SVD</strong>): This is somewhat related to PCA (which is covered in more detail in this chapter) and is also a matrix decomposition process not too dissimilar to NMF. </li>
				<li><strong class="bold">Independent Component Analysis</strong> (<strong class="bold">ICA</strong>): This also shares some similarities to SVD and PCA, but relaxing the assumption of the data being a Gaussian distribution allows for non-Gaussian data to be separated.</li>
			</ul>
			<p>Each of the methods described so far all use linear transformation to reduce the sparsity of the data in their original implementation. Some of these methods also have variants that use non-linear kernel functions in the separation process, providing the ability to reduce the sparsity in a non-linear fashion. Depending on the dataset being used, a non-linear kernel may be more effective at extracting the most useful information from the signal.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor081"/>Principal Component Analysis</h1>
			<p>As described previously, PCA is a commonly used and very effective dimensionality reduction technique, which often forms a preprocessing stage for a number of machine learning models and techniques. For this reason, we will dedicate this section of the book to looking at PCA in more detail than any of the other methods. PCA reduces the sparsity in the dataset by separating the data into a series of components where each component represents a source of information within the data. As its name suggests, the first component produced in PCA, the <strong class="bold">principal component</strong>, comprises the majority of information or variance within the data. The principal component can often be thought of as contributing the most amount of interesting information in addition to the mean. With each subsequent component, less information, but more subtlety, is contributed to the compressed data. If we consider all of these components together, there will be no benefit of using PCA, as the original dataset will be returned. To clarify this process and the information returned by PCA, we will use a worked example, completing the PCA calculations by hand. But first, we must review some foundational statistical concepts, which are required to execute the PCA calculations.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor082"/>Mean</h2>
			<p>The mean, or the average value, is simply the addition of all values divided by the number of values in the set.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor083"/>Standard Deviation</h2>
			<p>Often referred to as the spread of the data and related to the variance, the standard deviation is a measure of how much of the data lies within proximity to the mean. In a normally distributed dataset, approximately 68% of the dataset lies within one standard deviation of the mean, (that is, between (mean - 1*std) to (mean + 1*std), you can find 68% of the data if it is normally distributed.)</p>
			<p>The relationship between the variance and standard deviation is quite a simple one – the variance is the standard deviation squared.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor084"/>Covariance</h2>
			<p>Where standard deviation or variance is the spread of the data calculated on a single dimension, the covariance is the variance of one dimension (or feature) against another. When the covariance of a dimension is computed against itself, the result is the same as simply calculating the variance for the dimension.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor085"/>Covariance Matrix</h2>
			<p>A covariance matrix is a matrix representation of the possible covariance values that can be computed for a dataset. Other than being particularly useful in data exploration, covariance matrices are also required to execute the PCA of a dataset. To determine the variance of one feature with respect to another, we simply look up the corresponding value in the covariance matrix. In the following diagram, we can see that, in column 1, row 2, the value is the variance of feature or dataset <em class="italic">Y</em> with respect to <em class="italic">X</em> (<em class="italic">cov(Y, X))</em>. We can also see that there is a diagonal column of covariance values computed against the same feature or dataset; for example, <em class="italic">cov(X, X)</em>. In this situation, the value is simply the variance of <em class="italic">X</em>:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B15923_04_07.jpg" alt="Figure 4.7: The covariance matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7: The covariance matrix</p>
			<p>Typically, the exact values of each of the covariances are not as interesting as looking at the magnitude and relative size of each of the covariances within the matrix. A large value of the covariance of one feature against another would suggest that one feature changes significantly with respect to the other, while a value close to zero would signify very little change. The other interesting aspect of the covariance to look for is the sign associated with the covariance; a positive value indicates that as one feature increases or decreases, then so does the other, while a negative covariance indicates that the two features diverge from one another, with one increasing as the other decreases or vice versa.</p>
			<p>Thankfully, <strong class="source-inline">numpy</strong> and <strong class="source-inline">scipy</strong> provide functions to efficiently perform these calculations for you. In the next exercise, we will compute these values in Python.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor086"/>Exercise 4.01: Computing Mean, Standard Deviation, and Variance Using the pandas Library</h2>
			<p>In this exercise, we will briefly review how to compute some of the foundational statistical concepts using both the <strong class="source-inline">numpy</strong> and <strong class="source-inline">pandas</strong> Python packages. In this exercise, we will use a dataset of the measurements of seeds from different varieties of wheat, created using X-ray imaging. The dataset, which can be found in the accompanying source code, comprises seven individual measurements (<strong class="source-inline">area A</strong>, <strong class="source-inline">perimeter P</strong>, <strong class="source-inline">compactness C</strong>, <strong class="source-inline">length of kernel LK</strong>, <strong class="source-inline">width of kernel WK</strong>, <strong class="source-inline">asymmetry coefficient</strong> <strong class="source-inline">A_Coef</strong>, and <strong class="source-inline">length of kernel groove LKG</strong>) of three different wheat varieties: Kama, Rosa, and Canadian.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced  from <a href="https://archive.ics.uci.edu/ml/datasets/seed">https://archive.ics.uci.edu/ml/datasets/seed</a> (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science) Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset can also be downloaded from <a href="https://packt.live/2RjpDxk">https://packt.live/2RjpDxk</a>.</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li>Import the <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong> packages for use:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Load the dataset and preview the first five lines of data:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer091" class="IMG---Figure"><img src="image/B15923_04_08.jpg" alt="Figure 4.8: The head of the data&#13;&#10;"/></div><p class="figure-caption">Figure 4.8: The head of the data</p></li>
				<li>We only require the area, <strong class="source-inline">A</strong>, and the length of the kernel <strong class="source-inline">LK</strong> features, so remove the other columns:<p class="source-code">df = df[['A', 'LK']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer092" class="IMG---Figure"><img src="image/B15923_04_09.jpg" alt="Figure 4.9: The head after cleaning the data&#13;&#10;"/></div><p class="figure-caption">Figure 4.9: The head after cleaning the data</p></li>
				<li>Visualize the dataset by plotting the <strong class="source-inline">A</strong> versus <strong class="source-inline">LK</strong> values:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.scatter(df['A'], df['LK'])</p><p class="source-code">plt.xlabel('Area of Kernel')</p><p class="source-code">plt.ylabel('Length of Kernel')</p><p class="source-code">plt.title('Kernel Area versus Length')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer093" class="IMG---Figure"><img src="image/B15923_04_10.jpg" alt="Figure 4.10: Plot of the data&#13;&#10;"/></div><p class="figure-caption">Figure 4.10: Plot of the data</p></li>
				<li>Compute the mean value using the <strong class="source-inline">pandas</strong> method:<p class="source-code">df.mean()</p><p>The output is as follows:</p><p class="source-code">A     14.847524</p><p class="source-code">LK     5.628533</p><p class="source-code">dtype: float64</p></li>
				<li>Compute the mean value using the <strong class="source-inline">numpy</strong> method:<p class="source-code">np.mean(df.values, axis=0)</p><p>The output is as follows:</p><p class="source-code">array([14.84752381,  5.62853333])</p></li>
				<li>Compute the standard deviation value using the <strong class="source-inline">pandas</strong> method:<p class="source-code">df.std()</p><p>The output is as follows:</p><p class="source-code">A     2.909699</p><p class="source-code">LK    0.443063</p><p class="source-code">dtype: float64</p></li>
				<li>Compute the standard deviation value using the <strong class="source-inline">numpy</strong> method:<p class="source-code">np.std(df.values, axis=0)</p><p>The output is as follows:</p><p class="source-code">array([2.90276331, 0.44200731])</p></li>
				<li>Compute the variance values using the <strong class="source-inline">pandas</strong> method:<p class="source-code">df.var()</p><p>The output is as follows:</p><p class="source-code">A     8.466351</p><p class="source-code">LK    0.196305</p><p class="source-code">dtype: float64</p></li>
				<li>Compute the variance values using the <strong class="source-inline">numpy</strong> method:<p class="source-code">np.var(df.values, axis=0)</p><p>The output is as follows:</p><p class="source-code">array([8.42603482, 0.19537046])</p></li>
				<li>Compute the covariance matrix using the <strong class="source-inline">pandas</strong> method:<p class="source-code">df.cov()</p><p>The output is as follows:</p><div id="_idContainer094" class="IMG---Figure"><img src="image/B15923_04_11.jpg" alt="Figure 4.11: Covariance matrix using the pandas method&#13;&#10;"/></div><p class="figure-caption">Figure 4.11: Covariance matrix using the pandas method</p></li>
				<li>Compute the covariance matrix using the <strong class="source-inline">numpy</strong> method:<p class="source-code">np.cov(df.values.T)</p><p>The output is as follows:</p><p class="source-code">array([[8.46635078, 1.22470367],</p><p class="source-code">       [1.22470367, 0.19630525]])</p></li>
			</ol>
			<p>Now that we know how to compute the foundational statistic values, we will turn our attention to the remaining components of PCA.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2BHiLFz">https://packt.live/2BHiLFz</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2O80UtW">https://packt.live/2O80UtW</a>.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/>Eigenvalues and Eigenvectors</h2>
			<p>The mathematical concept of eigenvalues and eigenvectors is a very important one in the fields of physics and engineering, and they also form the final steps in computing the principal components of a dataset. The exact mathematical definition of eigenvalues and eigenvectors is outside the scope of this book, as it is quite involved and requires a reasonable understanding of linear algebra. Any square matrix <em class="italic">A</em> of dimensions <em class="italic">n x n</em> has a vector, <em class="italic">x</em>, of shape <em class="italic">n x 1</em> in such a way that it satisfies the following relation: </p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B15923_04_12.jpg" alt="Figure 4.12: Equation representing PCA&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12: Equation representing PCA</p>
			<p>Here, the term <img src="image/B15923_04_Formula_01.png" alt="C:\Users\user\Downloads\B15923_04_Formula_01.PNG"/> is a numerical value and denotes the eigenvalue, whereas <em class="italic">x</em> denotes the corresponding eigenvector. <em class="italic">N</em> denotes the order of the matrix, <em class="italic">A</em>. There will be exactly <em class="italic">n</em> eigenvalue and eigenvectors for matrix <em class="italic">A</em>. Without diving into the mathematical details of PCA, let's take a look at another way of representing the preceding equation as follows:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B15923_04_13.jpg" alt="Figure 4.13: Alternative equation representing PCA&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13: Alternative equation representing PCA</p>
			<p>Putting this simply into the context of PCA, we can derive the following:</p>
			<ul>
				<li><strong class="bold">Covariance Matrix</strong> (<em class="italic">A</em>): As discussed in the preceding section, matrix <em class="italic">A</em> should be a square matrix before it can undergo eigenvalue decomposition. Since, in the case of our dataset, it has rows greater than the number of columns (let the shape of dataset be <em class="italic">m x n</em> where <em class="italic">m</em> is the number of rows and <em class="italic">n</em> is the number of columns). Therefore, we cannot perform eigenvalue decomposition directly. To perform eigenvalue decomposition on a rectangular matrix, it is first converted to a square matrix by computing its covariance matrix. A covariance matrix has a shape of <em class="italic">n x n</em>, that is, it is a square matrix of order '<em class="italic">n</em>'. </li>
				<li><strong class="bold">Eigenvectors</strong> (<em class="italic">U</em>) are the components contributing information to the dataset as described in the first paragraph of this section on principal components called eigenvectors. Each eigenvector describes some amount of variability within the dataset. This variability is indicated by the corresponding eigenvalue. The larger the eigenvalue, the greater its contribution. An eigenvectors matrix has a shape of <em class="italic">n x n</em>.</li>
				<li><strong class="bold">Eigenvalues</strong> (<img src="image/B15923_04_Formula_02.png" alt="C:\Users\user\Downloads\B15923_04_Formula_02.PNG"/>) are the individual values that describe how much contribution each eigenvector provides to the dataset. As described previously, the single eigenvector that describes the largest contribution is referred to as the principal component, and, as such, will have the largest eigenvalue. Accordingly, the eigenvector with the smallest eigenvalue contributes the least amount of variance or information to the data. Eigenvalues are a diagonal matrix, which has the diagonal elements representing eigenvalues.</li>
			</ul>
			<p>Please note that even the SVD of a covariance matrix of data produces eigenvalue decomposition, which we will see in <em class="italic">Exercise 4.04</em>, <em class="italic">scikit-learn PCA</em>. However, SVD uses a different process for the decomposition of the matrix. Remember that eigenvalue decomposition can be done for a square matrix only, whereas SVD can be done for a rectangular matrix as well.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="bold">Square matrix</strong>: A square matrix has the same number of rows and columns. The number of rows in a square matrix is called the order of the matrix. A matrix that has an unequal number of rows and columns is known as a rectangular matrix.</p>
			<p class="callout"><strong class="bold">Diagonal matrix</strong>: A diagonal matrix has all non-diagonal elements as zero.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/>Exercise 4.02: Computing Eigenvalues and Eigenvectors</h2>
			<p>As discussed previously, deriving and computing the eigenvalues and eigenvectors manually is a little involved and is not within the scope of this book. Thankfully, <strong class="source-inline">numpy</strong> provides all the functionality for us to compute these values. Again, we will use the Seeds dataset for this example:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>. (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.) Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset can also be downloaded from <a href="https://packt.live/34gOQ0B">https://packt.live/34gOQ0B</a>.</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">pandas</strong> and <strong class="source-inline">numpy</strong> packages:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p></li>
				<li>Load the dataset:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer099" class="IMG---Figure"><img src="image/B15923_04_14.jpg" alt="Figure 4.14: The first five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 4.14: The first five rows of the dataset</p></li>
				<li>Again, we only require the <strong class="source-inline">A</strong> and <strong class="source-inline">LK</strong> features, so remove the other columns:<p class="source-code">df = df[['A', 'LK']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer100" class="IMG---Figure"><img src="image/B15923_04_15.jpg" alt="Figure 4.15: The area and length of the kernel features&#13;&#10;"/></div><p class="figure-caption">Figure 4.15: The area and length of the kernel features</p></li>
				<li>From the linear algebra module of <strong class="source-inline">numpy</strong>, use the <strong class="source-inline">eig</strong> function to compute the <strong class="source-inline">eigenvalues</strong> and <strong class="source-inline">eigenvectors</strong> characteristic vectors. Note the use of the covariance matrix of data here:<p class="source-code">eigenvalues, eigenvectors = np.linalg.eig(np.cov(df.T))</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">numpy</strong> function, <strong class="source-inline">cov</strong>, can be used to calculate the covariance matrix of data. It produces a square matrix of order equal to the number of features of data.</p></li>
				<li>Look at the eigenvalues; we can see that the first value is the largest, so the first eigenvector contributes the most information:<p class="source-code">eigenvalues</p><p>The output is as follows:</p><p class="source-code">array([8.64390408, 0.01875194])</p></li>
				<li>It is handy to look at eigenvalues as a percentage of the total variance within the dataset. We will use a cumulative sum function to do this:<p class="source-code">eigenvalues = np.cumsum(eigenvalues)</p><p class="source-code">eigenvalues</p><p>The output is as follows:</p><p class="source-code">array([8.64390408, 8.66265602])</p></li>
				<li>Divide by the last or maximum value to convert eigenvalues into a percentage:<p class="source-code">eigenvalues /= eigenvalues.max()</p><p class="source-code">eigenvalues</p><p>The output is as follows:</p><p class="source-code">array([0.99783531, 1.])</p><p>We can see here that the first (or principal) component comprises 99% of the variation within the data, and, therefore, most of the information.</p></li>
				<li>Now, let's take a look at <strong class="source-inline">eigenvectors</strong>:<p class="source-code">eigenvectors</p><p>A section of the output is as follows:</p><p class="source-code">array([[ 0.98965371, -0.14347657],</p><p class="source-code">       [ 0.14347657,  0.98965371]])</p></li>
				<li>Confirm that the shape of the eigenvector matrix is in (<strong class="source-inline">n x n</strong>) format; that is, <strong class="source-inline">2</strong> x <strong class="source-inline">2</strong>:<p class="source-code">eigenvectors.shape</p><p>The output is as follows:</p><p class="source-code">(2, 2)</p></li>
				<li>So, from the eigenvalues, we saw that the principal component was the first eigenvector. Look at the values for the first eigenvector:<p class="source-code">P = eigenvectors[0]</p><p class="source-code">P</p><p>The output is as follows:</p><p class="source-code">array([0.98965371, -0.14347657])</p></li>
			</ol>
			<p>We have decomposed the dataset down into the principal components, and, using the eigenvectors, we can further reduce the dimensionality of the available data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3e5x3N3">https://packt.live/3e5x3N3</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3f5Skrk">https://packt.live/3f5Skrk</a>.</p>
			<p>In later examples, we will consider PCA and apply this technique to an example dataset.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/>The Process of PCA</h2>
			<p>Now, we have all of the pieces ready to complete PCA in order to reduce the number of dimensions in a dataset.</p>
			<p>The overall algorithm for completing PCA is as follows:</p>
			<ol>
				<li value="1">Import the required Python packages (<strong class="source-inline">numpy</strong> and <strong class="source-inline">pandas</strong>).</li>
				<li>Load the entire dataset.</li>
				<li>From the available data, select the features that you wish to use in dimensionality reduction.<p class="callout-heading">Note</p><p class="callout">If there is a significant difference in the scale between the features of the dataset; for example, one feature ranges in values between 0 and 1, and another between 100 and 1,000, you may need to normalize one of the features, as such differences in magnitude can eliminate the effect of the smaller features. In such a situation, you may need to divide the larger feature by its maximum value. </p><p class="callout">As an example, take a look at this:</p><p class="callout"><strong class="source-inline">x1 = [0.1, 0.23, 0.54, 0.76, 0.78]</strong></p><p class="callout"><strong class="source-inline">x2 = [121, 125, 167, 104, 192]</strong></p><p class="callout"><strong class="source-inline"># Normalise x2 to be between 0 and 1</strong></p><p class="callout"><strong class="source-inline">x2 = (x2-np.min(x2)) / (np.max(x2)-np.min(x2))</strong></p></li>
				<li>Compute the <strong class="source-inline">covariance</strong> matrix of the selected (and possibly normalized) data.</li>
				<li>Compute the eigenvalues and eigenvectors of the <strong class="source-inline">covariance</strong> matrix.</li>
				<li>Sort the eigenvalues (and corresponding eigenvectors) from the highest to the lowest.</li>
				<li>Compute the eigenvalues as a percentage of the total variance within the dataset.</li>
				<li>Select the number of eigenvalues and corresponding eigenvectors. They will be required to comprise a predetermined value of a minimum composition variance.<p class="callout-heading">Note</p><p class="callout">At this stage, the sorted eigenvalues represent a percentage of the total variance within the dataset. As such, we can use these values to select the number of eigenvectors required, either for the problem being solved or to sufficiently reduce the size of the dataset being applied to the model. For example, say that we required at least 90% of the variance to be accounted for within the output of PCA. We would then select the number of eigenvalues (and corresponding eigenvectors) that comprise at least 90% of the variance.</p></li>
				<li>Multiply the dataset by the selected eigenvectors and you have completed a PCA, thereby reducing the number of features representing the data.</li>
				<li>Plot the result.</li>
			</ol>
			<p>Before moving on to the next exercise, note that <strong class="bold">transpose</strong> is a term from linear algebra that means to swap the rows with the columns and vice versa. Let's say we </p>
			<p>have a matrix of <strong class="source-inline">X=[1, 2, 3]</strong>, then, the transpose of <em class="italic">X</em> would be <img src="image/B15923_04_Formula_03.png" alt="C:\Users\user\Downloads\B15923_04_Formula_03.PNG"/>.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor090"/>Exercise 4.03: Manually Executing PCA</h2>
			<p>For this exercise, we will be completing PCA manually, again using the Seeds dataset. For this example, we want to sufficiently reduce the number of dimensions within the dataset to comprise at least 75% of the available variance:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>. (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.) Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset can also be downloaded from <a href="https://packt.live/2Xe7cxO">https://packt.live/2Xe7cxO</a>.</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">pandas</strong> and <strong class="source-inline">numpy</strong> packages:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Load the dataset:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer102" class="IMG---Figure"><img src="image/B15923_04_16.jpg" alt="Figure 4.16: The first five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 4.16: The first five rows of the dataset</p></li>
				<li>Again, we only require the <strong class="source-inline">A</strong> and <strong class="source-inline">LK</strong> features, so remove the other columns. In this example, we are not normalizing the selected dataset:<p class="source-code">df = df[['A', 'LK']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer103" class="IMG---Figure"><img src="image/B15923_04_17.jpg" alt="Figure 4.17: The area and length of the kernel features&#13;&#10;"/></div><p class="figure-caption">Figure 4.17: The area and length of the kernel features</p></li>
				<li>Compute the <strong class="source-inline">covariance</strong> matrix for the selected data. Note that we need to take the transpose of the <strong class="source-inline">covariance</strong> matrix to ensure that it is based on the number of features (2) and not samples (150):<p class="source-code">data = np.cov(df.values.T)</p><p class="source-code">"""</p><p class="source-code">The transpose is required to ensure the covariance matrix is </p><p class="source-code">based on features, not samples data</p><p class="source-code">"""</p><p class="source-code">data</p><p>The output is as follows:</p><p class="source-code">array([[8.46635078, 1.22470367],</p><p class="source-code">       [1.22470367, 0.19630525]])</p></li>
				<li>Compute the eigenvectors and eigenvalues for the covariance matrix, Again, use the <strong class="source-inline">full_matrices</strong> function argument:<p class="source-code">eigenvectors, eigenvalues, _ = np.linalg\</p><p class="source-code">                               .svd(data, full_matrices=False)</p></li>
				<li>Eigenvalues are returned, sorted from the highest value to the lowest:<p class="source-code">eigenvalues</p><p>The output is as follows:</p><p class="source-code">array([8.64390408, 0.01875194])</p></li>
				<li>Eigenvectors are returned as a matrix: <p class="source-code">eigenvectors</p><p>The output is as follows:</p><p class="source-code">array([[-0.98965371, -0.14347657],</p><p class="source-code">       [-0.14347657,  0.98965371]])</p></li>
				<li>Compute the eigenvalues as a percentage of the variance within the dataset:<p class="source-code">eigenvalues = np.cumsum(eigenvalues)</p><p class="source-code">eigenvalues /= eigenvalues.max()</p><p class="source-code">eigenvalues</p><p>The output is as follows:</p><p class="source-code">array([0. 99783531, 1.        ])</p></li>
				<li>As per the introduction to the exercise, we need to describe the data with at least 75% of the available variance. As per <em class="italic">Step 7</em>, the principal component comprises 99% of the available variance. As such, we require only the principal component from the dataset. What are the principal components? Let's take a look:<p class="source-code">P = eigenvectors[0]</p><p class="source-code">P</p><p>The output is as follows:</p><p class="source-code">array([-0.98965371, -0.14347657])</p><p>Now, we can apply the dimensionality reduction process. Execute a matrix multiplication of the principal component with the transpose of the dataset.</p><p class="callout-heading">Note</p><p class="callout">The dimensionality reduction process is a matrix multiplication of the selected eigenvectors and the data to be transformed.</p></li>
				<li>Without taking the transpose of the <strong class="source-inline">df.values</strong> matrix, multiplication could not occur:<p class="source-code">x_t_p = P.dot(df.values.T)</p><p class="source-code">x_t_p</p><p>A section of the output is as follows:</p><div id="_idContainer104" class="IMG---Figure"><img src="image/B15923_04_18.jpg" alt="Figure 4.18: The result of matrix multiplication&#13;&#10;"/></div><p class="figure-caption">Figure 4.18: The result of matrix multiplication</p><p class="callout-heading">Note</p><p class="callout">The transpose of the dataset is required to execute matrix multiplication, as the <strong class="bold">inner dimensions of the matrix must be the same</strong> for matrix multiplication to occur. For <strong class="bold">A</strong> ("A dot B") to be valid, <strong class="bold">A</strong> must have the shape of <em class="italic">m x n</em>, and <strong class="bold">B</strong> must have the shape of <em class="italic">n x p</em>. In this example, the inner dimensions of <strong class="bold">A</strong> and <strong class="bold">B</strong> are both <em class="italic">n</em>. The resulting matrix would have dimensions of <em class="italic">m x p</em>.</p><p>In the following example, the output of the PCA is a single-column, 210-sample dataset. As such, we have just reduced the size of the initial dataset by half, comprising approximately 99% of the variance within the data.</p></li>
				<li>Plot the values of the principal component:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.plot(x_t_p)</p><p class="source-code">plt.title('Principal Component of Selected Seeds Dataset')</p><p class="source-code">plt.xlabel('Sample')</p><p class="source-code">plt.ylabel('Component Value')</p><p class="source-code">plt.show() </p><p>The output is as follows, and shows the new component values of the 210-sample dataset, as seen printed in the preceding step:</p><p> </p><div id="_idContainer105" class="IMG---Figure"><img src="image/B15923_04_19.jpg" alt="Figure 4.19: The Seeds dataset transformed using a manual PCA&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.19: The Seeds dataset transformed using a manual PCA</p>
			<p>In this exercise, we simply computed the covariance matrix of the dataset without applying any transformations to the dataset beforehand. If the two features have roughly the same mean and standard deviation, this is perfectly fine. However, if one feature is much larger in value (and has a somewhat different mean) than the other, then this feature may dominate the other when decomposing into components. This could have the effect of removing the information provided by the smaller feature altogether. One simple normalization technique before computing the covariance matrix would be to subtract the respective means from the features, thus centering the dataset around zero. We will demonstrate this in <em class="italic">Exercise 4.05</em>, <em class="italic">Visualizing Variance Reduction with Manual PCA</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fa8X57">https://packt.live/3fa8X57</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3iOvg2P">https://packt.live/3iOvg2P</a>.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor091"/>Exercise 4.04: scikit-learn PCA</h2>
			<p>Typically, we will not complete PCA manually, especially when scikit-learn provides an optimized API with convenient methods that allow us to easily transform the data to and from the reduced-dimensional space. In this exercise, we will look at using a scikit-learn PCA on the Seeds dataset in more detail:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>. (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.) Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset also can be downloaded from <a href="https://packt.live/2Ri6VGk">https://packt.live/2Ri6VGk</a>.</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">PCA</strong> modules from the <strong class="source-inline">sklearn</strong> packages:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p></li>
				<li>Load the dataset:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer106" class="IMG---Figure"><img src="image/B15923_04_20.jpg" alt="Figure 4.20: The first five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 4.20: The first five rows of the dataset</p></li>
				<li>Again, we only require the <strong class="source-inline">A</strong> and <strong class="source-inline">LK</strong> features, so remove the other columns. In this example, we are not normalizing the selected dataset:<p class="source-code">df = df[['A', 'LK']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer107" class="IMG---Figure"><img src="image/B15923_04_21.jpg" alt="Figure 4.21: The area and length of the kernel features&#13;&#10;"/></div><p class="figure-caption">Figure 4.21: The area and length of the kernel features</p></li>
				<li>Fit the data to a scikit-learn PCA model of the covariance data. Using the default values, as we have here, produces the maximum number of eigenvalues and eigenvectors possible for the dataset:<p class="source-code">model = PCA()</p><p class="source-code">model.fit(df.values)</p><p>The output is as follows:</p><p class="source-code">PCA(copy=True, iterated_power='auto', n_components=None, </p><p class="source-code">    random_state=None, </p><p class="source-code">    svd_solver='auto', tol=0.0, whiten=False)</p><p>Here, <strong class="source-inline">copy</strong> indicates that the data fit within the model is copied before any calculations are applied. If <strong class="source-inline">copy</strong> was set to <strong class="source-inline">False</strong>, data passed to PCA is overwritten. <strong class="source-inline">iterated_power</strong> shows that the <strong class="source-inline">A</strong> and <strong class="source-inline">LK</strong> features are the number of principal components to keep. The default value is <strong class="source-inline">None</strong>, which selects the number of components as one less than the minimum of either the number of samples or the number of features. <strong class="source-inline">random_state</strong> allows the user to specify a seed for the random number generator used by the SVD solver. <strong class="source-inline">svd_solver</strong> specifies the SVD solver to be used during PCA. <strong class="source-inline">tol</strong> is the tolerance value used by the SVD solver. With <strong class="source-inline">whiten</strong>, the component vectors are multiplied by the square root of the number of samples. This will remove some information but can improve the performance of some downstream estimators.</p></li>
				<li>The percentage of variance described by the components (eigenvalues) is contained within the <strong class="source-inline">explained_variance_ratio_</strong> property. Display the values for <strong class="source-inline">explained_variance_ratio_</strong>:<p class="source-code">model.explained_variance_ratio_</p><p>The output is as follows:</p><p class="source-code">array([0.99783531, 0.00216469])</p></li>
				<li>Display the eigenvectors via the <strong class="source-inline">components_</strong> property:<p class="source-code">model.components_</p><p>The output is as follows:</p><p class="source-code">array([[0.98965371, 0.14347657]])</p></li>
				<li>In this exercise, we will again only use the primary component, so we will create a new <strong class="source-inline">PCA</strong> model, this time specifying the number of components (eigenvectors/eigenvalues) to be <strong class="source-inline">1</strong>:<p class="source-code">model = PCA(n_components=1)</p></li>
				<li>Use the <strong class="source-inline">fit</strong> method to fit the <strong class="source-inline">covariance</strong> matrix to the <strong class="source-inline">PCA</strong> model and generate the corresponding eigenvalues/eigenvectors:<p class="source-code">model.fit(df.values)</p><p>The output is as follows:</p><p class="source-code">PCA(copy=True, iterated_power='auto', n_components=1, </p><p class="source-code">    random_state=None,</p><p class="source-code">    svd_solver='auto', tol=0.0, whiten=False)</p><p>The model is fitted using a number of default parameters, as listed in the preceding output. <strong class="source-inline">copy = True</strong> is the data provided to the <strong class="source-inline">fit</strong> method, which is copied before PCA is applied. <strong class="source-inline">iterated_power='auto'</strong> is used to define the number of iterations by the internal SVD solver. <strong class="source-inline">n_components=1</strong> specifies that the PCA model is to return only the principal component. <strong class="source-inline">random_state=None</strong> specifies the random number generator to be used by the internal SVD solver if required. <strong class="source-inline">svd_solver='auto'</strong> is the type of SVD solver used. <strong class="source-inline">tol=0.0</strong> is the tolerance value for the SVD solver. <strong class="source-inline">whiten=False</strong> specifies that the eigenvectors are not to be modified. If set to <strong class="source-inline">True</strong>, whitening modifies the components further by multiplying by the square root of the number of samples and dividing by the singular values. This can help to improve the performance of later algorithm steps.</p><p>Typically, you will not need to worry about adjusting any of these parameters, other than the number of components (<strong class="source-inline">n_components</strong>), which you can pass while declaring the PCA object as <strong class="source-inline">model = PCA(n_components=1)</strong>.</p></li>
				<li>Display the eigenvectors using the <strong class="source-inline">components_</strong> property:<p class="source-code">model.components_</p><p>The output is as follows:</p><p class="source-code">array([[0.98965371, 0.14347657]])</p></li>
				<li>Transform the Seeds dataset into the lower space by using the <strong class="source-inline">fit_transform</strong> method of the model on the dataset. Assign the transformed values to the <strong class="source-inline">data_t</strong> variable:<p class="source-code">data_t = model.fit_transform(df.values)</p></li>
				<li>Plot the transformed values to visualize the result:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.plot(data_t)</p><p class="source-code">plt.xlabel('Sample') </p><p class="source-code">plt.ylabel('Transformed Data')</p><p class="source-code">plt.title('The dataset transformed by the principal component')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer108" class="IMG---Figure"><img src="image/B15923_04_22.jpg" alt="Figure 4.22: The seeds dataset transformed using the scikit-learn PCA&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.22: The seeds dataset transformed using the scikit-learn PCA</p>
			<p>You have just reduced the dimensionality of the Seeds dataset using manual PCA, along with the scikit-learn API. But before we celebrate too early, compare <em class="italic">Figure 4.19</em> and <em class="italic">Figure 4.22</em>; these plots should be identical, shouldn't they? We used two separate methods to complete a PCA on the same dataset and selected the principal component for both. In the next activity, we will investigate why there are differences between the two.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZQV85c">https://packt.live/2ZQV85c</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2VSG99R">https://packt.live/2VSG99R</a>.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor092"/>Activity 4.01: Manual PCA versus scikit-learn</h2>
			<p>Suppose that you have been asked to port some legacy code from an older application executing PCA manually to a newer application that uses scikit-learn. During the porting process, you observe some differences between the output of the manual PCA and that of your port. Why is there a difference between the output of our manual PCA and scikit-learn? Compare the results of the two approaches on the Seeds dataset. What are the differences between them?</p>
			<p>The aim of this activity is to truly dive into understanding how PCA works by doing it from scratch, and then comparing your implementation against the one included in scikit-learn to see whether there are any major differences:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>. (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.) Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset can also be downloaded from <a href="https://packt.live/2JIH1qT">https://packt.live/2JIH1qT</a>.</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong> plotting libraries and the scikit-learn <strong class="source-inline">PCA</strong> model.</li>
				<li>Load the dataset and select only the kernel features as per the previous exercises. Display the first five rows of the data.</li>
				<li>Compute the <strong class="source-inline">covariance</strong> matrix for the data.</li>
				<li>Transform the data using the scikit-learn API and only the first principal component. Store the transformed data in the <strong class="source-inline">sklearn_pca</strong> variable.</li>
				<li>Transform the data using the manual PCA and only the first principal component. Store the transformed data in the <strong class="source-inline">manual_pca</strong> variable.</li>
				<li>Plot the <strong class="source-inline">sklearn_pca</strong> and <strong class="source-inline">manual_pca</strong> values on the same plot to visualize the difference.</li>
				<li>Notice that the two plots look almost identical, but with some key differences. What are these differences?</li>
				<li>See whether you can modify the output of the manual PCA process to bring it in line with the scikit-learn version.<p class="callout-heading">Note</p><p class="callout">Hint: The scikit-learn API subtracts the mean of the data prior to the transform.</p></li>
			</ol>
			<p><strong class="bold">Expected output</strong>: By the end of this activity, you will have transformed the dataset using both the manual and scikit-learn PCA methods. You will have produced a plot demonstrating that the two reduced datasets are, in fact, identical, and you should have an understanding of why they initially looked quite different. The final plot should look similar to the following:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B15923_04_23.jpg" alt="Figure 4.23: The expected final plot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.23: The expected final plot</p>
			<p>This plot will demonstrate that the dimensionality reduction completed by the two methods are, in fact, the same.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 437.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor093"/>Restoring the Compressed Dataset</h2>
			<p>Now that we have covered a few different examples of transforming a dataset into a lower-dimensional space, we should consider what practical effect this transformation has had on the data. Using PCA as a preprocessing step to condense the number of features in the data will result in some of the variance being discarded. The following exercise will walk us through this process so that we can see how much information has been discarded by the transformation.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor094"/>Exercise 4.05: Visualizing Variance Reduction with Manual PCA</h2>
			<p>One of the most important aspects of dimensionality reduction is understanding how much information has been removed from the dataset as a result of the dimensionality reduction process. Removing too much information will add additional challenges to later processing, while not removing enough defeats the purpose of PCA or other techniques. In this exercise, we will visualize the amount of information that has been removed from the Seeds dataset as a result of PCA:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>. (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.) Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset also can be downloaded from <a href="https://packt.live/2RhnDFS">https://packt.live/2RhnDFS</a>.</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong> plotting libraries:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Read in the <strong class="source-inline">wheat kernel</strong> features from the Seeds dataset:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')[['A', 'LK']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer110" class="IMG---Figure"><img src="image/B15923_04_24.jpg" alt="Figure 4.24: Kernel features&#13;&#10;"/></div><p class="figure-caption">Figure 4.24: Kernel features</p></li>
				<li>Center the dataset around zero by subtracting the respective means.<p class="callout-heading">Note</p><p class="callout">As discussed at the end of <em class="italic">Exercise 4.03</em>, <em class="italic">Manually Executing PCA</em>, here, we are centering the data before computing the covariance matrix.</p><p>The code appears as follows:</p><p class="source-code">means = np.mean(df.values, axis=0)</p><p class="source-code">means</p><p>The output is as follows:</p><p class="source-code">array([14.84752381,  5.62853333])</p></li>
				<li>To calculate the data and print the results, use the following code:<p class="source-code">data = df.values - means</p><p class="source-code">data</p><p>A section of the output is as follows:</p><div id="_idContainer111" class="IMG---Figure"><img src="image/B15923_04_25.jpg" alt="Figure 4.25: Section of the output&#13;&#10;"/></div><p class="figure-caption">Figure 4.25: Section of the output</p></li>
				<li>Use manual PCA to transform the data on the basis of the first principal component:<p class="source-code">eigenvectors, eigenvalues, _ = np.linalg.svd(np.cov(data.T), \</p><p class="source-code">                               full_matrices=False)</p><p class="source-code">P = eigenvectors[0]</p><p class="source-code">P</p><p>The output is as follows:</p><p class="source-code">array([-0.98965371, -0.14347657])</p></li>
				<li>Transform the data into the lower-dimensional space by doing a dot product of the preceding <strong class="source-inline">P</strong> with a transposed version of the data matrix:<p class="source-code">data_transformed = P.dot(data.T)</p></li>
				<li>Reshape the principal components for later use:<p class="source-code">P = P.reshape((-1, 1))</p></li>
				<li>To compute the inverse transform of the reduced dataset, we need to restore the selected eigenvectors to the higher-dimensional space. To do this, we will invert the matrix. Matrix inversion is another linear algebra technique that we will only cover very briefly. A square matrix, <em class="italic">A</em>, is said to be invertible if there is another square matrix, <em class="italic">B</em>, and if <em class="italic">AB=BA=I</em>, where <em class="italic">I</em> is a special matrix known as an identity matrix, consisting of values of <strong class="source-inline">1</strong> only through the center diagonal:<p class="source-code">P_transformed = np.linalg.pinv(P)</p><p class="source-code">P_transformed</p><p>The output is as follows:</p><p class="source-code">array([[-0.98965371, -0.14347657]])</p></li>
				<li>Prepare the transformed data for use in the matrix multiplication:<p class="source-code">data_transformed = data_transformed.reshape((-1, 1))</p></li>
				<li>Compute the inverse transform of the reduced data and plot the result to visualize the effect of removing the variance from the data:<p class="source-code">data_restored = data_transformed.dot(P_transformed)</p><p class="source-code">data_restored</p><p>A section of the output is as follows:</p><div id="_idContainer112" class="IMG---Figure"><img src="image/B15923_04_26.jpg" alt="Figure 4.26: The inverse transform of the reduced data&#13;&#10;"/></div><p class="figure-caption">Figure 4.26: The inverse transform of the reduced data</p></li>
				<li>Add the <strong class="source-inline">means</strong> array back to the transformed data:<p class="source-code">data_restored += means</p></li>
				<li>Visualize the result by plotting the original and the transformed datasets:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.plot(data_restored[:,0], data_restored[:,1], \</p><p class="source-code">         linestyle=':', label='PCA restoration')</p><p class="source-code">plt.scatter(df['A'], df['LK'], marker='*', label='Original')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.xlabel('Area of Kernel')</p><p class="source-code">plt.ylabel('Length of Kernel')</p><p class="source-code">plt.title('Inverse transform after removing variance')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer113" class="IMG---Figure"><img src="image/B15923_04_27.jpg" alt="Figure 4.27: The inverse transform after removing variance&#13;&#10;"/></div><p class="figure-caption">Figure 4.27: The inverse transform after removing variance</p></li>
				<li>There are only two components of variation in this dataset. If we do not remove any of the components, what will be the result of the inverse transform? Again, transform the data into the lower-dimensional space, but this time, use all of the eigenvectors:<p class="source-code">P = eigenvectors</p><p class="source-code">data_transformed = P.dot(data.T)</p></li>
				<li>Transpose <strong class="source-inline">data_transformed</strong> to put it in the correct shape for matrix multiplication:<p class="source-code">data_transformed = data_transformed.T</p></li>
				<li>Now, restore the data back to the higher-dimensional space: <p class="source-code">data_restored = data_transformed.dot(P)</p><p class="source-code">data_restored</p><p>A section of the output is as follows:</p><div id="_idContainer114" class="IMG---Figure"><img src="image/B15923_04_28.jpg" alt="Figure 4.28: The restored data&#13;&#10;"/></div><p class="figure-caption">Figure 4.28: The restored data</p></li>
				<li>Add the means back to the restored data:<p class="source-code">data_restored += means</p></li>
				<li>Visualize the restored data in the context of the original dataset:<p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.scatter(data_restored[:,0], data_restored[:,1], \</p><p class="source-code">            marker='d', label='PCA restoration', c='k')</p><p class="source-code">plt.scatter(df['A'], df['LK'], marker='o', \</p><p class="source-code">            label='Original', c='#1f77b4')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.xlabel('Area of Kernel')</p><p class="source-code">plt.ylabel('Length of Kernel')</p><p class="source-code">plt.title('Inverse transform after removing variance')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer115" class="IMG---Figure"><img src="image/B15923_04_29.jpg" alt="Figure 4.29: The inverse transform after removing the variance&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.29: The inverse transform after removing the variance</p>
			<p>If we compare the two plots produced in this exercise, we can see that the PCA went down, and the restored dataset is essentially a negative linear trend line between the two feature sets. We can compare this to the dataset restored from all of the available components, where we have recreated the original dataset as a whole.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38EztBu">https://packt.live/38EztBu</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3f8LDVC">https://packt.live/3f8LDVC</a>.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor095"/>Exercise 4.06: Visualizing Variance Reduction with scikit-learn</h2>
			<p>In this exercise, we will again visualize the effect of reducing the dimensionality of the dataset; however, this time, we will be using the scikit-learn API. This is this method that you will commonly use in practical applications due to the power and simplicity of the scikit-learn model:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>. (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.) Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset can also be downloaded from <a href="https://packt.live/3bVlJm4">https://packt.live/3bVlJm4</a>.</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong> plotting libraries and the <strong class="source-inline">PCA</strong> model from scikit-learn:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.decomposition import PCA</p></li>
				<li>Read in the <strong class="source-inline">Wheat Kernel</strong> features from the Seeds dataset:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')[['A', 'LK']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer116" class="IMG---Figure"><img src="image/B15923_04_30.jpg" alt="Figure 4.30: The Wheat Kernel features from the Seeds dataset&#13;&#10;"/></div><p class="figure-caption">Figure 4.30: The Wheat Kernel features from the Seeds dataset</p></li>
				<li>Use the scikit-learn API to transform the data on the basis of the first principal component:<p class="source-code">model = PCA(n_components=1)</p><p class="source-code">data_p = model.fit_transform(df.values)</p></li>
				<li>Compute the inverse transform of the reduced data and plot the result to visualize the effect of removing the variance from the data:<p class="source-code">data = model.inverse_transform(data_p)</p><p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.plot(data[:,0], data[:,1], linestyle=':', \</p><p class="source-code">         label='PCA restoration')</p><p class="source-code">plt.scatter(df['A'], df['LK'], marker='*', label='Original')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.xlabel('Area of Kernel')</p><p class="source-code">plt.ylabel('Length of Kernel')</p><p class="source-code">plt.title('Inverse transform after removing variance')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer117" class="IMG---Figure"><img src="image/B15923_04_31.jpg" alt="Figure 4.31: The inverse transform after removing the variance&#13;&#10;"/></div><p class="figure-caption">Figure 4.31: The inverse transform after removing the variance</p></li>
				<li>There are only two components of variation in this dataset. If we do not remove any of the components, what will the result of the inverse transform be? Let's find out by computing the inverse transform and seeing how the results change without removing any components:<p class="source-code">model = PCA()</p><p class="source-code">data_p = model.fit_transform(df.values)</p><p class="source-code">data = model.inverse_transform(data_p)</p><p class="source-code">plt.figure(figsize=(10, 7))</p><p class="source-code">plt.scatter(data[:,0], data[:,1], marker='d', \</p><p class="source-code">            label='PCA restoration', c='k')</p><p class="source-code">plt.scatter(df['A'], df['LK'], marker='o', \</p><p class="source-code">            label='Original', c='#1f77b4')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.xlabel('Area of Kernel')</p><p class="source-code">plt.ylabel('Length of Kernel')</p><p class="source-code">plt.title('Inverse transform after removing variance')</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer118" class="IMG---Figure"><img src="image/B15923_04_29.jpg" alt="Figure 4.32: The inverse transform after removing the variance&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.32: The inverse transform after removing the variance</p>
			<p>As we can see here, if we don't remove any of the components in the PCA, it will recreate the original data when performing inverse transform. We have demonstrated the effect of removing information from the dataset and the ability to recreate the original data using all of the available eigenvectors.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2O362zv">https://packt.live/2O362zv</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fdWYDU">https://packt.live/3fdWYDU</a>.</p>
			<p>The previous exercises specified the reduction in dimensionality using PCA to two dimensions, partly to allow the results to be visualized easily. We can, however, use PCA to reduce the dimensions to any value less than that of the original set. The following example demonstrates how PCA can be used to reduce a dataset to three dimensions, thereby allowing visualizations.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor096"/>Exercise 4.07: Plotting 3D Plots in Matplotlib</h2>
			<p>Creating 3D scatter plots in <strong class="source-inline">matplotlib</strong> is unfortunately not as simple as providing a series of (<em class="italic">x</em>, <em class="italic">y</em>, <em class="italic">z</em>) coordinates to a scatter plot. In this exercise, we will work through a simple 3D plotting example using the Seeds dataset:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>. (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.)</p>
			<p class="callout">Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset can also be downloaded from <a href="https://packt.live/3c2tAhT">https://packt.live/3c2tAhT</a>.</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong> and <strong class="source-inline">matplotlib</strong>. To enable 3D plotting, you will also need to import <strong class="source-inline">Axes3D</strong>:<p class="source-code">from mpl_toolkits.mplot3d import Axes3D</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Read in the dataset and select the <strong class="source-inline">A</strong>, <strong class="source-inline">LK</strong>, and <strong class="source-inline">C</strong> columns:<p class="source-code">df = pd.read_csv('../Seed_Data.csv')[['A', 'LK', 'C']]</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer119" class="IMG---Figure"><img src="image/B15923_04_33.jpg" alt="Figure 4.33: The first five rows of the data&#13;&#10;"/></div><p class="figure-caption">Figure 4.33: The first five rows of the data</p></li>
				<li>Plot the data in three dimensions and use the <strong class="source-inline">projection='3d'</strong> argument with the <strong class="source-inline">add_subplot</strong> method to create the 3D plot:<p class="source-code">fig = plt.figure(figsize=(10, 7))</p><p class="source-code"># Where Axes3D is required</p><p class="source-code">ax = fig.add_subplot(111, projection='3d')</p><p class="source-code">ax.scatter(df['A'], df['LK'], df['C'])</p><p class="source-code">ax.set_xlabel('Area of Kernel')</p><p class="source-code">ax.set_ylabel('Length of Kernel')</p><p class="source-code">ax.set_zlabel('Compactness of Kernel')</p><p class="source-code">ax.set_title('Expanded Seeds Dataset')</p><p class="source-code">plt.show()</p><p>The plot will appear as follows:</p><div id="_idContainer120" class="IMG---Figure"><img src="image/B15923_04_34.jpg" alt="Figure 4.34: The expanded Seeds dataset&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.34: The expanded Seeds dataset</p>
			<p class="callout-heading">Note</p>
			<p class="callout">While the <strong class="source-inline">Axes3D</strong> library was imported but not directly used, it is required for configuring the plot window in three dimensions. If the import of <strong class="source-inline">Axes3D</strong> was omitted, the <strong class="source-inline">projection='3d'</strong> argument would return an <strong class="source-inline">AttributeError</strong> exception.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gPM1J9">https://packt.live/3gPM1J9</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2AFVXFr">https://packt.live/2AFVXFr</a>.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor097"/>Activity 4.02: PCA Using the Expanded Seeds Dataset</h2>
			<p>In this activity, we are going to use the complete Seeds dataset to look at the effect of selecting a differing number of components in the PCA decomposition. This activity aims to simulate the process that is typically completed in a real-world problem as we try to determine the optimum number of components to select, attempting to balance the extent of dimensionality reduction and information loss. Therefore, we will be using the scikit-learn PCA model:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a>. (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.) Citation: Contributors gratefully acknowledge the support of their work by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset also can be downloaded from <a href="https://packt.live/3aPY0nj">https://packt.live/3aPY0nj</a>.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong> and <strong class="source-inline">matplotlib</strong>. To enable 3D plotting, you will also need to import <strong class="source-inline">Axes3D</strong>.</li>
				<li>Read in the dataset and select the <strong class="source-inline">Area of Kernel</strong>, <strong class="source-inline">Length of Kernel</strong>, and <strong class="source-inline">Compactness of Kernel</strong> columns.</li>
				<li>Plot the data in three dimensions.</li>
				<li>Create a <strong class="source-inline">PCA</strong> model without specifying the number of components.</li>
				<li>Fit the model to the dataset.</li>
				<li>Display the eigenvalues or <strong class="source-inline">explained_variance_ratio_</strong>.</li>
				<li>We want to reduce the dimensionality of the dataset but still keep at least 90% of the variance. What is the minimum number of components required to keep 90% of the variance?</li>
				<li>Create a new <strong class="source-inline">PCA</strong> model, this time specifying the number of components required to keep at least 90% of the variance.</li>
				<li>Transform the data using the new model.</li>
				<li>Plot the transformed data.</li>
				<li>Restore the transformed data to the original dataspace.</li>
				<li>Plot the restored data in three dimensions in one subplot and the original data in a second subplot to visualize the effect of removing some of the variance:<p class="source-code">fig = plt.figure(figsize=(10, 14))</p><p class="source-code"># Original Data</p><p class="source-code">ax = fig.add_subplot(211, projection='3d')</p><p class="source-code"># Transformed Data</p><p class="source-code">ax = fig.add_subplot(212, projection='3d')</p></li>
			</ol>
			<p><strong class="bold">Expected output</strong>: The final plot will appear as follows:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B15923_04_35.jpg" alt="Figure 4.35: Expected plots&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.35: Expected plots</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 443.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor098"/>Summary</h1>
			<p>In this chapter, we covered the process of dimensionality reduction and PCA. We completed a number of exercises and developed the skills to reduce the size of a dataset by extracting only the most important components of variance within the data, using both a manual PCA process and the model provided by scikit-learn. During this chapter, we also returned the reduced datasets back to the original dataspace and observed the effect of removing the variance on the original data. Finally, we discussed a number of potential applications for PCA and other dimensionality reduction processes. In our next chapter, we will introduce neural network-based autoencoders and use the Keras package to implement them.</p>
		</div>
	</body></html>
- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting Online Ad Click-Through with Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we predicted ad click-through using tree algorithms.
    In this chapter, we will continue our journey of tackling the billion-dollar problem.
    We will focus on learning a very (probably the most) scalable classification model
    – logistic regression. We will explore what the logistic function is, how to train
    a logistic regression model, adding regularization to the model, and variants
    of logistic regression that are applicable to very large datasets. Besides its
    application in classification, we will also discuss how logistic regression and
    random forest models are used to pick significant features. You won’t get bored
    as there will be lots of implementations from scratch with scikit-learn and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting categorical features to numerical – one-hot encoding and original
    encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a logistic regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training on large datasets with online learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling multiclass classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing logistic regression using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting categorical features to numerical – one-hot encoding and ordinal
    encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Predicting Online Ad Click-Through with Tree-Based Algorithms*,
    I mentioned how **one-hot encoding** transforms categorical features to numerical
    features in order to use them in the tree algorithms in scikit-learn and TensorFlow.
    If we transform categorical features into numerical ones using one-hot encoding,
    we don’t limit our choice of algorithms to the tree-based ones that can work with
    categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest solution we can think of in terms of transforming a categorical
    feature with *k* possible values is to map it to a numerical feature with values
    from 1 to *k*. For example, `[Tech, Fashion, Fashion, Sports, Tech, Tech, Sports]`
    becomes `[1, 2, 2, 3, 1, 1, 3]`. However, this will impose an ordinal characteristic,
    such as `Sports` being greater than `Tech`, and a distance property, such as `Sports`
    being closer to `Fashion`than to `Tech`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, one-hot encoding converts the categorical feature to *k* binary features.
    Each binary feature indicates the presence or absence of a corresponding possible
    value. Hence, the preceding example becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, number, display  Description automatically
    generated](img/B21047_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Transforming user interest into numerical features with one-hot
    encoding'
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we used `OneHotEncoder` from scikit-learn to convert a matrix of
    strings into a binary matrix, but here, let’s take a look at another module, `DictVectorizer`,
    which also provides an efficient conversion. It transforms dictionary objects
    (categorical feature: value) into one-hot encoded vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, take a look at the following code, which performs one-hot encoding
    on a list of dictionaries containing categorical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see the mapping by executing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When it comes to new data, we can transform it with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inversely transform the encoded features back to the original features
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'One important thing to note is that if a new (not seen in training data) category
    is encountered in new data, it should be ignored (otherwise, the encoder will
    complain about the unseen categorical value). `DictVectorizer` handles this implicitly
    (while `OneHotEncoder` needs to specify the `ignore` parameter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, we prefer transforming a categorical feature with *k* possible values
    into a numerical feature with values ranging from *1* to *k*. This is **ordinal
    encoding** and we conduct it in order to employ ordinal or ranking knowledge in
    our learning; for example, large, medium, and small become 3, 2, and 1, respectively;
    good and bad become 1 and 0, while one-hot encoding fails to preserve such useful
    information. We can realize ordinal encoding easily through the use of `pandas`,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We convert the string feature into ordinal values based on the mapping we define.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Handling high dimensionality resulting from one-hot encoding can be challenging.
    It may increase computational complexity or lead to overfitting. Here are some
    strategies to handle high dimensionality when using one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection**: This can reduce the number of one-hot encoded features
    while retaining the most informative ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: It transforms the high-dimensional feature space
    into a lower-dimensional representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature aggregation**: Instead of one-hot encoding every category individually,
    consider aggregating categories that share similar characteristics. For example,
    group rare categories into an “Other” category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve covered transforming categorical features into numerical ones. Next, we
    will talk about logistic regression, a classifier that only takes in numerical
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying data with logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we trained tree-based models only based on the first 300,000
    samples out of 40 million. We did so simply because training a tree on a large
    dataset is extremely computationally expensive and time consuming. Since we are
    not limited to algorithms directly taking in categorical features thanks to one-hot
    encoding, we should turn to a new algorithm with high scalability for large datasets.
    As mentioned, logistic regression is one of the most, or perhaps the most, scalable
    classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the logistic function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with an introduction to the **logistic function** (which is more
    commonly referred to as the **sigmoid function**) as the algorithm’s core before
    we dive into the algorithm itself. It basically maps an input to an output of
    a value between *0* and *1*, and is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We define the logistic function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we visualize what it looks like with input variables from -`8` to `8`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, line, screenshot, plot  Description automatically
    generated](img/B21047_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: The logistic function'
  prefs: []
  type: TYPE_NORMAL
- en: In the S-shaped curve, all inputs are transformed into the range from 0 to 1\.
    For positive inputs, a greater value results in an output closer to 1; for negative
    inputs, a smaller value generates an output closer to 0; when the input is 0,
    the output is the midpoint, 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Jumping from the logistic function to logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you have some knowledge of the logistic function, it is easy to map
    it to the algorithm that stems from it. In logistic regression, the function input
    *z* becomes the weighted sum of features. Given a data sample *x* with *n* features,
    *x*[1]*, x*[2]*, …, x*[n] (*x* represents a feature vector and *x* = (*x*[1]*,
    x*[2]*, …, x*[n]*)*), and **weights** (also called **coefficients**) of the model
    *w* (*w* represents a vector (*w*[1]*, w*[2]*, …, w*[n])), *z* is expressed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *T* is the transpose operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Occasionally, the model comes with an **intercept** (also called **bias**),
    *w*[0], which accounts for the inherent bias or baseline probability. In this
    instance, the preceding linear relationship becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As for the output *y*(*z*)in the range of 0 to 1, in the algorithm, it becomes
    the probability of the target being *1* or the positive class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, logistic regression is a probabilistic classifier, similar to the Naïve
    Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'A logistic regression model, or, more specifically, its weight vector *w*,
    is learned from the training data, with the goal of predicting a positive sample
    as close to *1* as possible and predicting a negative sample as close to 0 as
    possible. In mathematical language, the weights are trained to minimize the cost
    defined as the **Mean Squared Error** (**MSE**), which measures the average of
    squares of the difference between the truth and the prediction. Given *m* training
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *y*^((i)) is either `1` (positive class) or `0` (negative class), and
    the cost function *J*(*w*) regarding the weights to be optimized is expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_006.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the preceding cost function is **non-convex**, which means that, when
    searching for the optimal *w*, many local (suboptimal) optimums are found and
    the function does not converge to a global optimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of the **convex** and **non-convex** functions are plotted respectively
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing line, text, diagram, plot  Description automatically
    generated](img/B21047_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Examples of convex and non-convex functions'
  prefs: []
  type: TYPE_NORMAL
- en: In the convex example, there is only one global optimum, while there are two
    optimums in the non-convex example.
  prefs: []
  type: TYPE_NORMAL
- en: For more about convex and non-convex functions, check out [https://web.stanford.edu/class/ee364a/lectures/functions.pdf](https://web.stanford.edu/class/ee364a/lectures/functions.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this, in practice, we use the cost function that results in a convex
    optimization problem, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can take a closer look at the cost of a single training sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_008.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the ground truth *y*^((i)) *= 1*, if the model predicts correctly with
    full confidence (the positive class with 100% probability), the sample cost *j*
    is *0*; the cost *j* increases when the predicted probability ![](img/B21047_04_010.png)
    decreases. If the model incorrectly predicts that there is no chance of the positive
    class, the cost is infinitely high. We can visualize it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following graph for the end result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, display, line, plot  Description automatically
    generated](img/B21047_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Cost function of logistic regression when y=1'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the contrary, when the ground truth *y*^((i)) *= 0*, if the model predicts
    correctly with full confidence (the positive class with *0* probability, or the
    negative class with 100% probability), the sample cost *j* is *0*; the cost j
    increases when the predicted probability ![](img/B21047_04_010.png) increases.
    When it incorrectly predicts that there is no chance of the negative class, the
    cost becomes infinitely high. We can visualize it using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph is the resultant output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, line, rectangle, plot  Description automatically
    generated](img/B21047_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Cost function of logistic regression when y=0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing this alternative cost function is actually equivalent to minimizing
    the MSE-based cost function. The advantages of choosing it over the MSE version
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It is convex, so the optimal model weights can be found
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A summation of the logarithms of prediction, which are as follows, simplifies
    the calculation of its derivative with respect to the weights, which we will talk
    about later:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Due to the logarithmic function, the cost function, which is as follows, is
    also called **logarithmic loss**, or simply **log loss**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_014.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that the cost function is ready, how can we train the logistic regression
    model to minimize the cost function? Let’s see this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training a logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, the question is as follows: how can we obtain the optimal *w* such that
    *J*(*w*) is minimized? We can do so using gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: Training a logistic regression model using gradient descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Gradient descent** (also called **steepest descent**) is a procedure for
    minimizing a loss function by first-order iterative optimization. In each iteration,
    the model parameters move a small step that is proportional to the negative derivative
    of the objective function at the current point. This means the to-be-optimal point
    iteratively moves downhill toward the minimal value of the objective function.
    The proportion we just mentioned is called the **learning rate**, or **step size**.
    It can be summarized in a mathematical equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_015.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the left *w* is the weight vector after a learning step, and the right
    *w* is the one before moving, ![](img/B21047_04_016.png) is the learning rate,
    and ![](img/B21047_04_017.png) is the first-order derivative, the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train a logistic regression model using gradient descent, let’s start with
    the derivative of the cost function *J*(*w*) with respect to *w*. It might require
    some knowledge of calculus but don’t worry, we will walk through it step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first calculate the derivative of ![](img/B21047_04_018.png) with respect
    to *w*. We herein take the *j-th* weight, *w*[j], as an example (note *z=w*^T*x*,
    and we omit the ^((i)) for simplicity):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_019.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_020.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we calculate the derivative of the sample cost *J*(*w*) as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_022.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_023.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_024.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we calculate the entire cost over *m* samples as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then generalize it to ![](img/B21047_04_027.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Combined with the preceding derivations, the weights can be updated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_029.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w* gets updated in each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a substantial number of iterations, the learned parameter *w* is then
    used to classify a new sample *x*’ by means of the following equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_030.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_031.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision threshold is `0.5` by default, but it definitely can be other values.
    In cases where a false negative is supposed to be avoided, for example, when predicting
    fire occurrence (the positive class) for alerts, the decision threshold can be
    lower than `0.5`, such as `0.3`, depending on how paranoid we are and how proactively
    we want to prevent the positive event from happening. On the other hand, when
    the false positive class is the one that should be evaded, for instance, when
    predicting the product success (the positive class) rate for quality assurance,
    the decision threshold can be greater than `0.5`, such as `0.7`, or lower than
    `0.5`, depending on how high a standard you set.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a thorough understanding of the gradient descent-based training and predicting
    process, we will now implement the logistic regression algorithm from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by defining the function that computes the prediction ![](img/B21047_04_032.png)
    with the current weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With this, we are able to continue with the function updating the weights,
    which is as follows, by one step in a gradient descent manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_04_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the function calculating the cost *J*(*w*) is implemented as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we connect all these functions to the model training function by executing
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Updating the `weights` vector in each iteration
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Printing out the current cost for every `100` (this can be another value) iterations
    to ensure `cost` is decreasing and that things are on the right track
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They are implemented in the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we predict the results of new inputs using the trained model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implementing logistic regression is very simple, as you just saw. Let’s now
    examine it using a toy example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We train a logistic regression model for `1000` iterations, at a learning rate
    of `0.1` based on intercept-included weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The decreasing cost means that the model is being optimized over time. We can
    check the model’s performance on new samples as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize this, execute the following code using `0.5` as the classification
    decision threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Blue-filled crosses are testing samples predicted from class 0, while black-filled
    dots are those predicted from class 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Training and testing sets of the toy example'
  prefs: []
  type: TYPE_NORMAL
- en: The model we trained correctly predicts classes of new samples (filled crosses
    and filled dots).
  prefs: []
  type: TYPE_NORMAL
- en: Predicting ad click-through with logistic regression using gradient descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now deploy the algorithm we just developed in our click-through prediction
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with only 10,000 training samples (you will soon see why we don’t
    start with 270,000, as we did in the previous chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We train a logistic regression model over `10000` iterations, at a learning
    rate of `0.01` with bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes 184 seconds to optimize the model. The trained model performs on the
    testing set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s use 100,000 training samples (`n_train = 100000`) and repeat the
    same process. It will take more than an hour – 22 times longer to fit data of
    10 times the size. As I mentioned at the beginning of the chapter, the logistic
    regression classifier can be good at training on large datasets. But our testing
    results seem to contradict this. How could we handle even larger training datasets
    efficiently, not just 100,000 samples, but millions? Let’s look at a more efficient
    way to train a logistic regression model in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training a logistic regression model using stochastic gradient descent (SGD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In gradient descent-based logistic regression models, **all** training samples
    are used to update the weights in every single iteration. Hence, if the number
    of training samples is large, the whole training process will become very time
    consuming and computationally expensive, as you just witnessed in our last example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, a small tweak will make logistic regression suitable for large-sized
    datasets. For each weight update, **only one** training sample is consumed, instead
    of the **complete** training set. The model moves a step based on the error calculated
    by a single training sample. Once all samples are used, one iteration finishes.
    This advanced version of gradient descent is called **SGD**. Expressed in a formula,
    for each iteration, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_034.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_035.png)'
  prefs: []
  type: TYPE_IMG
- en: SGD generally converges much faster than gradient descent where a large number
    of iterations is usually needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement SGD-based logistic regression, we just need to slightly modify
    the `update_weights_gd` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `train_logistic_regression` function, SGD is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s see how powerful SGD is. We will work with 100,000 training samples
    and choose `10` as the number of iterations, `0.01` as the learning rate, and
    print out the current costs for every other iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The training process finishes in just 25 seconds!
  prefs: []
  type: TYPE_NORMAL
- en: 'After successfully implementing the SGD-based logistic regression algorithm
    from scratch, we implement it using the `SGDClassifier` module of scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, `'``log_loss'` for the `loss` parameter indicates that the cost function
    is log loss, `penalty` is the regularization term to reduce overfitting, which
    we will discuss further in the next section, `max_iter` is the number of iterations,
    and the remaining two parameters mean the learning rate is `0.01` and unchanged
    during the course of training. It should be noted that the default `learning_rate`
    is `'optimal'`, where the learning rate slightly decreases as more and more updates
    are made. This can be beneficial for finding the optimal solution on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, train the model and test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Quick and easy!
  prefs: []
  type: TYPE_NORMAL
- en: Training a logistic regression model with regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As I briefly mentioned in the previous section, the `penalty` parameter in
    the logistic regression `SGDClassifier` is related to model **regularization**.
    There are two basic forms of regularization, **L1** (also called **Lasso**) and
    **L2** (also called **Ridge**). In either way, the regularization is an additional
    term on top of the original cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_036.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B21047_04_037.png) is the constant that multiplies the regularization
    term, and *q* is either *1* or *2* representing L1 or L2 regularization where
    the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_038.png)'
  prefs: []
  type: TYPE_IMG
- en: Training a logistic regression model is the process of reducing the cost as
    a function of weights *w*. If it gets to a point where some weights, such as *w*[i],
    *w*[j], and *w*[k] are considerably large, the whole cost will be determined by
    these large weights. In this case, the learned model may just memorize the training
    set and fail to generalize to unseen data. The regularization term is introduced
    in order to penalize large weights, as the weights now become part of the cost
    to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization as a result eliminates overfitting. Finally, parameter α provides
    a trade-off between log loss and generalization. If *α* is too small, it is not
    able to compress large weights and the model may suffer from high variance or
    overfitting; on the other hand, if *α* is too large, the model may become over-generalized
    and perform poorly in terms of fitting the dataset, which is the syndrome of underfitting.
    *α* is an important parameter to tune in order to obtain the best logistic regression
    model with regularization.
  prefs: []
  type: TYPE_NORMAL
- en: As for choosing between the L1 and L2 forms, the rule of thumb is based on whether
    **feature selection** is expected. In **Machine** **Learning** (**ML**) classification,
    feature selection is the process of picking a subset of significant features for
    use in better model construction. In practice, not every feature in a dataset
    carries information that is useful for discriminating samples; some features are
    either redundant or irrelevant and hence can be discarded with little loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a logistic regression classifier, feature selection can only be achieved
    with L1 regularization. To understand this, let’s consider two weight vectors,
    *w*[1]= (*1, 0*) and *w*[2]= (*0.5, 0.5*); supposing they produce the same amount
    of log loss, the L1 and L2 regularization terms of each weight vector are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_039.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_04_040.png)'
  prefs: []
  type: TYPE_IMG
- en: The L1 term of both vectors is equivalent, while the L2 term of *w*[2] is less
    than that of *w*[1]. This indicates that L2 regularization penalizes weights composed
    of significantly large and small weights more than L1 regularization does. In
    other words, L2 regularization favors relatively small values for all weights,
    and avoids significantly large and small values for any weight, while L1 regularization
    allows some weights with a significantly small value and some with a significantly
    large value. Only with L1 regularization can some weights be compressed to close
    to or exactly *0*, which enables feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, the regularization type can be specified by the `penalty` parameter
    with the `none` (without regularization), `"l1"`, `"l2"`, and `"elasticnet"` (a
    mixture of L1 and L2) options, and the multiplier α can be specified by the alpha
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection using L1 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We herein examine L1 regularization for feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize an SGD logistic regression model with L1 regularization, and train
    the model based on 10,000 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'With the trained model, we obtain the absolute values of its coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The bottom `10` coefficients and their values are printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see what these 10 features are using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: They are `1001` from the `0` column (that is the `C1` column) in `X_train`,
    `84c2f017` from the `8` column (that is the `device_model` column), and so on
    and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the top 10 coefficients and their values can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: They are `28905ebd` from the `4` column (that is `site_category`) in `X_train`,
    `7687a86e` from the `3` column (that is `site_domain`), and so on and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: You have seen how feature selection works with L1-regularized logistic regression
    in this section, where weights of unimportant features are compressed to close
    to, or exactly, 0\. Besides L1-regularized logistic regression, random forest
    is another frequently used feature selection technique. Let’s see more in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection using random forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To recap, random forest is bagging over a set of individual decision trees.
    Each tree considers a random subset of the features when searching for the best
    splitting point at each node. In a decision tree, only those significant features
    (along with their splitting values) are used to constitute tree nodes. Consider
    the forest as a whole: the more frequently a feature is used in a tree node, the
    more important it is.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we can rank the importance of features based on their occurrences
    in nodes among all trees, and select the top most important ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The trained `RandomForestClassifier` module in scikit-learn comes with an attribute,
    `feature_importances_`, indicating the feature importance, which is calculated
    as the proportion of occurrences in tree nodes. Again, we will examine feature
    selection with random forest on the dataset with 100,000 ad click samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After fitting the random forest model, we obtain the feature importance scores
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the bottom 10 feature scores and the corresponding 10 least
    important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, take a look at the top 10 feature scores and the corresponding 10 most
    important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we covered how random forest is used for feature selection.
    We ranked the features of the ad click data using a random forest. Can you use
    the top 10 or 20 features to build another logistic regression model for ad click
    prediction?
  prefs: []
  type: TYPE_NORMAL
- en: Training on large datasets with online learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have trained our model on no more than 300,000 samples. If we go
    beyond this figure, memory might be overloaded since it holds too much data, and
    the program will crash. In this section, we will explore how to train on a large-scale
    dataset with **online learning**.
  prefs: []
  type: TYPE_NORMAL
- en: SGD evolves from gradient descent by sequentially updating the model with individual
    training samples one at a time, instead of the complete training set at once.
    We can scale up SGD further with online learning techniques. In online learning,
    new data for training is available in sequential order or in real time, as opposed
    to all at once in an offline learning environment. A relatively small chunk of
    data is loaded and preprocessed for training at a time, which releases the memory
    used to hold the entire large dataset. Besides better computational feasibility,
    online learning is also used because of its adaptability to cases where new data
    is generated in real time and is needed for modernizing the model. For instance,
    stock price prediction models are updated in an online learning manner with timely
    market data; click-through prediction models need to include the most recent data
    reflecting users’ latest behaviors and tastes; spam email detectors have to be
    reactive to ever-changing spammers by considering new features that are dynamically
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The existing model trained by previous datasets can now be updated based on
    the most recently available dataset only, instead of rebuilding it from scratch
    based on previous and recent datasets together, as is the case in offline learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a logistic process  Description automatically generated with
    low confidence](img/B21047_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Online versus offline learning'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, online learning allows the model to continue training
    with new arriving data. However, in offline learning, we have to retrain the whole
    model with the new arriving data along with the old data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SGDClassifier` module in scikit-learn implements online learning with
    the `partial_fit` method (while the `fit` method is applied in offline learning,
    as you have seen). We will train the model with 1,000,000 samples, where we feed
    in 100,000 samples at one time to simulate an online learning environment. Also,
    we will test the trained model on another 100,000 samples as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the encoder on the whole training set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize an SGD logistic regression model where we set the number of iterations
    to `1` in order to partially fit the model and enable online learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Loop over every `100000` samples and partially fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we use the `partial_fit` method for online learning. Also, we specify
    the `classes` parameter, which is required in online learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the trained model on the testing set, the next 100,000 samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: With online learning, training based on a total of 1 million samples only takes
    87 seconds and yields better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We have been using logistic regression for binary classification so far. Can
    we use it for multiclass cases? Yes. However, we do need to make some small tweaks.
    Let’s look at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Handling multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One last thing worth noting is how logistic regression algorithms deal with
    multiclass classification. Although we interact with the scikit-learn classifiers
    in multiclass cases the same way as in binary cases, it is useful to understand
    how logistic regression works in multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression for more than two classes is also called **multinomial
    logistic regression**, better known latterly as **softmax regression**. As you
    have seen in the binary case, the model is represented by one weight vector *w*,
    and the probability of the target being *1* or the positive class is written as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the *K* class case, the model is represented by *K* weight vectors, *w*[1],
    *w*[2], ..., *w*[K], and the probability of the target being class *k* is written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'See the following term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_043.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding term normalizes the following probabilities (*k* from *1* to
    *K*) so that they total *1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The cost function in the binary case is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_045.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the cost function in the multiclass case becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_046.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the ![](img/B21047_04_047.png) function is *1* only if ![](img/B21047_04_048.png)
    is true, otherwise it’s 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the cost function defined, we obtain the ![](img/B21047_04_049.png) step
    for the *j* weight vector in the same way as we derived the step ![](img/B21047_04_050.png)
    in the binary case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a similar manner, all *K* weight vectors are updated in each iteration.
    After sufficient iterations, the learned weight vectors, *w*[1], *w*[2], ...,
    *w*[K], are then used to classify a new sample *x*’ by means of the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_04_052.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To have a better sense, let’s experiment with it with a classic dataset, the
    handwritten digits for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'As the image data is stored in 8*8 matrices, we need to flatten them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We then split the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We then combine grid search and cross-validation to find the optimal multiclass
    logistic regression model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We first define the hyperparameter grid we want to tune for the model. After
    initializing the classifier with some fixed parameters, we set up grid search
    cross-validation. We train on the training set and find the best set of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict using the optimal model, we apply the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: It doesn’t look much different from the previous example, since `SGDClassifier`
    handles multiclass internally. Feel free to compute the confusion matrix as an
    exercise. It will be interesting to see how the model performs on individual classes.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will be a bonus section where we will implement logistic regression
    with TensorFlow and use click prediction as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing logistic regression using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll employ TensorFlow to implement logistic regression, utilizing click prediction
    as our illustrative example again. We use 90% of the first 100,000 samples for
    training and the remaining 10% for testing, and assume that `X_train_enc`, `Y_train`,
    `X_test_enc`, and `Y_test` contain the correct data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import TensorFlow, transform `X_train_enc` and `X_test_enc` into
    a NumPy array, and cast `X_train_enc`, `Y_train`, `X_test_enc`, and `Y_test` to
    `float32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In TensorFlow, it’s common to work with data in the form of NumPy arrays. Additionally,
    TensorFlow operates with float32 by default for computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `tf.data` module to shuffle and batch data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For each weight update, only **one batch** of samples is consumed, instead of
    the one sample or the complete training set. The model moves a step based on the
    error calculated by a batch of samples. The batch size is 1,000 in this example.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.data` provides a set of tools and utilities for efficiently loading and
    preprocessing data for ML modeling. It is designed to handle large datasets and
    enables efficient data pipeline construction for training and evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define the weights and bias of the logistic regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then create a gradient descent optimizer that searches for the best coefficients
    by minimizing the loss. We use Adam (Adam: *A method for stochastic optimization*,
    Kingma, D. P., & Ba, J. (2014)) as our optimizer, which is an advanced gradient
    descent with a learning rate (starting with `0.001`) that is adaptive to gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the optimization process where we compute the current prediction
    and cost and update the model coefficients following the computed gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `tf.GradientTape` allows us to track TensorFlow computations and calculate
    gradients with respect to the given variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We run the training for 5,000 steps (one step is with one batch of random samples):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For every 500 steps, we compute and print out the current cost to check the
    training performance. As you can see, the training loss is decreasing overall.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the model is trained, we use it to make predictions on the testing set
    and report the AUC metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are able to achieve an AUC of `0.736` with the TensorFlow-based logistic
    regression model. You can also tweak the learning rate, the number of training
    steps, and other hyperparameters to obtain a better performance. This will be
    a fun exercise at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of the batch size in SGD can significantly impact the training process
    and the performance of the model. Here are some best practices for choosing the
    batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consider computational resources**: Larger batch sizes require more memory
    and computational resources, while smaller batch sizes may lead to slower convergence.
    Choose a batch size that fits within the memory constraints of your hardware while
    maximizing computational efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Empirical testing**: Experiment with different batch sizes and evaluate model
    performance on a validation dataset. Choose the batch size that yields the best
    trade-off between convergence speed and model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size versus learning rate**: The choice of batch size can interact
    with the learning rate. Larger batch sizes may require higher learning rates to
    prevent slow convergence, while smaller batch sizes may benefit from smaller learning
    rates to avoid instability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consider the nature of the data**: The nature of the data can also influence
    the choice of batch size. For example, in tasks where the samples are highly correlated
    or exhibit temporal dependencies (e.g., time series data), smaller batch sizes
    may be more effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might be curious about how we can efficiently train the model on the entire
    dataset of 40 million samples. You will utilize tools such as **Spark** ([https://spark.apache.org/](https://spark.apache.org/))
    and the `PySpark` module to scale up our solution.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we continued working on the online advertising click-through
    prediction project. This time, we overcame the categorical feature challenge by
    means of the one-hot encoding technique. We then resorted to a new classification
    algorithm, logistic regression, for its high scalability to large datasets. The
    in-depth discussion of the logistic regression algorithm started with the introduction
    of the logistic function, which led to the mechanics of the algorithm itself.
    This was followed by how to train a logistic regression model using gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing a logistic regression classifier by hand and testing it on
    our click-through dataset, you learned how to train the logistic regression model
    in a more advanced manner, using SGD, and we adjusted our algorithm accordingly.
    We also practiced how to use the SGD-based logistic regression classifier from
    scikit-learn and applied it to our project.
  prefs: []
  type: TYPE_NORMAL
- en: We then continued to tackle the problems we might face in using logistic regression,
    including L1 and L2 regularization for eliminating overfitting, online learning
    techniques for training on large-scale datasets, and handling multi-class scenarios.
    You also learned how to implement logistic regression with TensorFlow. Finally,
    the chapter ended with applying the random forest model to feature selection,
    as an alternative to L1-regularized logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back on our learning journey, we have been working on classification
    problems since *Chapter 2*, *Building a Movie Recommendation Engine with Naïve
    Bayes*. We have now covered all the powerful and popular classification models
    in ML. We will move on to solving regression problems in the next chapter; regression
    is the sibling of classification in supervised learning. You will learn about
    regression models, including linear regression, and decision trees for regression.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the logistic regression-based click-through prediction project, can you also
    tweak hyperparameters such as `penalty`, `eta0`, and `alpha` in the `SGDClassifier`
    model? What is the highest testing AUC you are able to achieve?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you try to use more training samples, for instance, 10 million samples,
    in the online learning solution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the TensorFlow-based solution, can you tweak the learning rate, the number
    of training steps, and other hyperparameters to obtain better performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1878468721786989681.png)'
  prefs: []
  type: TYPE_IMG

- en: '*Chapter 3*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neighborhood Approaches and DBSCAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand how neighborhood approaches to clustering work from beginning to
    end
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the DBSCAN algorithm from scratch by using packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the best suited algorithm from k-means, hierarchical clustering, and
    DBSCAN to solve your problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will have a look at DBSCAN clustering approach that will
    serve us best in the highly complex data.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have covered two popular ways of approaching the clustering problem:
    k-means and hierarchical clustering. Both clustering techniques have pros and
    cons associated with how they are carried out. Once again, let''s revisit where
    we have been in the first two chapters so we can gain further context to where
    we will be going in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: In the challenge space of unsupervised learning, you will be presented with
    a collection of feature data, but no complementary labels telling you what these
    feature variables necessarily mean. While you may not get a discrete view into
    what the target labels are, you can get some semblance of structure out of the
    data by clustering similar groups together and seeing what is similar within groups.
    The first approach we covered to achieve this goal of clustering similar data
    points is k-means.
  prefs: []
  type: TYPE_NORMAL
- en: k-means works best for simpler data challenges where speed is paramount. By
    simply looking at the closest data points, there is not a lot of computational
    overhead, however, there is also a greater degree of challenge when it comes to
    higher-dimensional datasets. k-means is also not ideal if you are unaware of the
    potential number of clusters you would be looking for. An example we have worked
    with in *Chapter 2*, *Hierarchical Clustering*, was looking at chemical profiles
    to determine which wines belonged together in a disorganized shipment. This exercise
    only worked well because you knew that three wine types were ordered; however,
    k-means would have been less successful if you had no intuition on what the original
    order was made up of.
  prefs: []
  type: TYPE_NORMAL
- en: The second clustering approach we explored was hierarchical clustering. This
    method can work in multiple ways – either agglomerative or divisive. Agglomerative
    clustering works with a bottom-up approach, treating each data point as its own
    cluster and recursively grouping them together with linkage criteria. Divisive
    clustering works in the opposite direction by treating all data points as one
    large class and recursively breaking them down into smaller clusters. This approach
    has the benefit of fully understanding the entire data distribution, as it calculates
    splitting potential, however, it is typically not done in practice due to its
    greater complexity. Hierarchical clustering is a strong contender for your clustering
    needs when it comes to not knowing anything about the data. Using a dendrogram,
    you can visualize all the splits in your data and consider what number of clusters
    makes sense after the fact. This can be really helpful in your specific use case;
    however, it also comes at a higher computational cost that is seen in k-means.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover a clustering approach that will serve us best
    in the highly complex data: **DBSCAN** (Density-Based Spatial Clustering of Applications
    with Noise). Canonically, this method has always been seen as a high performer
    in datasets that have a lot of densely interspersed data. Let''s walk through
    why it does so well in these use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Clusters as Neighborhoods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the first two chapters, we explored the concept of likeness being described
    as a function of Euclidean distance – data points that are closer to any one point
    can be seen as similar, while those that are further away in Euclidean space can
    be seen as dissimilar. This notion is seen once again in the DBSCAN algorithm.
    As alluded to by the lengthy name, the DBSCAN approach expands upon basic distance
    metric evaluation by also incorporating the notion of density. If there are clumps
    of data points that all exist in the same area as each other, they can be seen
    as members of the same cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Neighbors have a direct connection to clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: Neighbors have a direct connection to clusters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding example, we can see four neighborhoods.
  prefs: []
  type: TYPE_NORMAL
- en: The density-based approach has a number of benefits when compared to the past
    approaches we've covered that focus exclusively on distance. If you were just
    focusing on distance as a clustering threshold, then you may find your clustering
    makes little sense if faced with a sparse feature space with outliers. Both k-means
    and hierarchical clustering will automatically group together all data points
    in the space until no points are left.
  prefs: []
  type: TYPE_NORMAL
- en: 'While hierarchical clustering does provide a path around this issue somewhat,
    since you can dictate where clusters are formed using a dendrogram post-clustering
    run, k-means is the most susceptible to failing, as it is the simplest approach
    to clustering. These pitfalls are less evident when we begin evaluating neighborhood
    approaches to clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Example dendrogram'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.2: Example dendrogram'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By incorporating the notion of neighbor density in DBSCAN, we can leave outliers
    out of clusters if we choose to, based on the hyperparameters we choose at run
    time. Only the data points that have close neighbors will be seen as members within
    the same cluster and those that are farther away can be left as unclustered outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to DBSCAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous section, the strength of DBSCAN becomes apparent
    when we analyze the benefits of taking a density-based approach to clustering.
    DBSCAN evaluates density as a combination of neighborhood radius and minimum points
    found in a neighborhood deemed a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This concept can be driven home if we re-consider the scenario where you are
    tasked with organizing an unlabeled shipment of wine for your store. In the past
    example, it was made clear that we can find similar wines based off their features,
    such as scientific chemical traits. Knowing this information, we can more easily
    group together similar wines and efficiently have our products organized for sale
    in no time. Hopefully, that is clear by now – but what may not have been clear
    is the fact that products that you order to stock your store often reflect real-world
    purchase patterns. To promote variety in your inventory, but still have enough
    stock of the most popular wines, there is a highly uneven distribution of product
    types that you have available. Most people love the classic wines, such as white
    and red, however, you may still carry more exotic wines for your customers who
    love expensive varieties. This makes clustering more difficult, since there are
    uneven class distributions (you don't order 10 of every wine available, for example).
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN differs from k-means and hierarchical clustering because you can build
    this intuition into how we evaluate the clusters of customers we are interested
    in forming. It can cut through the noise in an easier fashion and only point out
    customers who have the highest potential for remarketing in a campaign.
  prefs: []
  type: TYPE_NORMAL
- en: By clustering through the concept of a neighborhood, we can separate out the
    one-off customers that can be seen as random noise, relative to the more valuable
    customers that come back to our store time and time again. This approach, of book,
    calls into question how we establish the best numbers when it comes to neighborhood
    radius and minimum points per neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: As a high-level heuristic, we want to have our neighborhood radius small, but
    not too small. At one end of the extreme, you can have the neighborhood radius
    be quite high – this can max out at treating all points in the feature space as
    one massive cluster. On the opposite end of the extreme, you can have a very small
    neighborhood radius. Too small neighborhood radii can result in no points being
    clustered together and having a large collection of single member clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Similar logic applies when it comes to the minimum number of points that can
    make up a cluster. Minimum points can be seen as a secondary threshold that tunes
    the neighborhood radius a bit depending on what data you have available in your
    space. If all of the data in your feature space is extremely sparse, minimum points
    become extremely valuable, in tandem with neighborhood radius, to make sure you
    don't just have a large number of uncorrelated data points. When you have very
    dense data, the minimum points threshold becomes less of a driving factor as opposed
    to neighborhood radius.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from these two hyperparameter rules, the best options are, as
    usual, dependent on what your dataset looks like. Oftentimes, you will want to
    find the perfect "goldilocks" zone of not being too small in your hyperparameters,
    but also not too large.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN In-Depth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To see how DBSCAN works, we can trace the path of a simple toy program as it
    merges together to form a variety of clusters and noise-labeled data points:'
  prefs: []
  type: TYPE_NORMAL
- en: Given *n* unvisited sample data points, move through each point in a loop and
    mark as visited.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From each point, look at the distance to every other point in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For all points that fall within the neighborhood radius hyperparameter, connect
    them as neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check to see whether the number of neighbors is at least as many as the minimum
    points required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the minimum point threshold is reached, group together as a cluster. If not,
    mark the point as noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until all data points are categorized in clusters or as noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DBSCAN is fairly straightforward in some senses – while there are the new concepts
    of density through neighborhood radius and minimum points, at its core, it is
    still just evaluating using a distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: Walkthrough of the DBSCAN Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is a simple example walking through the preceding steps in slightly more
    detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given four sample data points, view each point as its own cluster [ (1,7) ],
    [ (-8,6) ], [ (-9,4) ] , [ (4, -2) ]:![Figure 3.3: Plot of sample data points'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12626_03_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.3: Plot of sample data points'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Calculate pairwise the Euclidean distance between each of the points:![Figure
    3.4: Point distances'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12626_03_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.4: Point distances'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: From each point, expand out a neighborhood size and form clusters. For the purpose
    of this example, let's imagine we passed through a neighborhood radius of three.
    This means that any two points will be neighbors if the distance between them
    is less than three. Points (-8,6) and (-9,4) are now candidates for clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Points that have no neighbors are marked as noise and remain unclustered. Points
    (1,7) and (4,-2) fall out of our frame of interest as being useless in terms of
    clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Points that have neighbors are then evaluated to see whether they pass the minimum
    points threshold. In this example, if we had passed through a minimum points threshold
    of two, then points (-8,6) and (-9,4) can formally be grouped together as a cluster.
    If we had a minimum points threshold of three, then all four data points in this
    set would be considered superfluous noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this process on remaining un-visited data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of this process, you will have your entire dataset established as
    either within clusters or as unrelated noise. Hopefully, as you can tell by walking
    through the toy example, DBSCAN performance is highly dependent on the threshold
    hyperparameters you choose a priori. This means that you may have to run DBSCAN
    a couple of times with different hyperparameter options to get an understanding
    of how they influence overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: One great thing to notice about DBSCAN is that it does away with concepts of
    centroids that we saw in both k-means and a centroid-focused implementation of
    hierarchical clustering. This feature allows DBSCAN to work better for complex
    datasets, since most data in the wild is not shaped like clean blobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9: Evaluating the Impact of Neighborhood Radius Size'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this exercise, we will work in reverse of what we have typically seen in
    previous examples, by first seeing the packaged implementation of DBSCAN in scikit-learn,
    and then implementing it on our own. This is done on purpose to fully explore
    how different neighborhood radius sizes drastically impact DBSCAN performance.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this exercise, you will become familiar with how tuning neighborhood
    radius size can change how well DBSCAN performs. It is important to understand
    these facets of DBSCAN, as they can save you time in the future by troubleshooting
    your clustering algorithms efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate some dummy data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.5: Visualized Toy Data Example'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.5: Visualized Toy Data Example'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After plotting the dummy data for this toy problem, you will see that the dataset
    has two features and approximately seven to eight clusters. To implement DBSCAN
    using scikit-learn, you will need to instantiate a new scikit-learn class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our example DBSCAN instance is stored in the `db` variable, and our hyperparameters
    are passed through on creation. For the sake of this example, you can see that
    the neighborhood radius (`eps`) is set to 0.5, while the minimum number of points
    is set to 10\. To keep in line with our past chapters, we will once again be using
    Euclidean distance as our distance metric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s set up a loop that allows us to explore potential neighborhood radius
    size options interactively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following two plots:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure: 3.6: Resulting plots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure: 3.6: Resulting plots'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the plots, setting our neighborhood size too small will
    cause everything to be seen as random noise (purple points). Bumping our neighborhood
    size up a little bit allows us to form clusters that make more sense. Try recreating
    the preceding plots and experiment with varying `eps` sizes.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Attributes – Neighborhood Radius
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 9*, *Evaluating the Impact of Neighborhood Radius Size,* you saw
    how impactful setting the proper neighborhood radius is on the performance of
    your DBSCAN implementation. If your neighborhood is too small, then you will run
    into issues where all the data is left unclustered. If you set your neighborhood
    too large, then all of the data will similarly be grouped together into one cluster
    and not provide any value. If you explored the preceding exercise further with
    your own `eps` sizes, you may have noticed that it is very difficult to land on
    great clustering using only the neighborhood size. This is where a minimum points
    threshold comes in handy. We will visit that topic later.
  prefs: []
  type: TYPE_NORMAL
- en: 'To go deeper into the neighborhood concept of DBSCAN, let''s take a deeper
    look at the `eps` hyperparameter you pass at instantiation time. `eps` stands
    for epsilon and is the distance that your algorithm will look within when searching
    for neighbors. This epsilon value is converted to a radius that sweeps around
    any given data point in a circular manner to serve as a neighborhood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Visualization of neighborhood radius where red circle is the
    neighborhood'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: Visualization of neighborhood radius where red circle is the neighborhood'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this instance, there will be four neighbors of the center point.
  prefs: []
  type: TYPE_NORMAL
- en: 'One key aspect to notice here is that the shape formed by your neighborhood
    search is a circle in two dimensions, and a sphere in three dimensions. This may
    impact the performance of your model simply based on how the data is structured.
    Once again, blobs may seem like an intuitive structure to find – this may not
    always be the case. Fortunately, DBSCAN is well equipped to handle this dilemma
    of clusters that you may be interested in, yet that do not fit the explicit blob
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Impact of varying neighborhood radius size'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.8: Impact of varying neighborhood radius size'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the left, the data point will be classified as random noise. On the right,
    the data point has multiple neighbors and could be its own cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4: Implement DBSCAN from Scratch'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using a generated two-dimensional dataset during an interview, you are asked
    to create the DBSCAN algorithm from scratch. To do this, you will need to code
    the intuition behind neighborhood searches and have a recursive call that adds
    neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Given what you've learned about DBSCAN and distance metrics from prior chapters,
    build an implementation of DBSCAN from scratch in Python. You are free to use
    NumPy and SciPy to evaluate distances here.
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will help you complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a random cluster dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create functions from scratch that allow you to call DBSCAN on a dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your created DBSCAN implementation to find clusters in the generated dataset.
    Feel free to use hyperparameters as you see fit, tuning them based on their performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the clustering performance of your DBSCAN implementation from scratch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The desired outcome of this exercise is for you to understand how DBSCAN works
    from the ground up before you use the fully packaged implementation in scikit-learn.
    Taking this approach to any machine learning algorithm from scratch is important,
    as it helps you "earn" the ability to use easier implementations, while still
    being able to discuss DBSCAN in depth in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Expected outcome'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.9: Expected outcome'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Solution for this activity can be found on page 316.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Attributes – Minimum Points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other core component to a successful implementation of DBSCAN beyond the
    neighborhood radius is the minimum number of points required to justify membership
    within a cluster. As mentioned earlier, it is more obvious that this lower bound
    benefits your algorithm when it comes to sparser datasets. That's not to say it
    is a useless parameter when you have very dense data, however – while having single
    data points randomly interspersed through your feature space can be easily bucketed
    as noise, it becomes more of a grey area when we have random patches of two to
    three, for example. Should these data points be their own cluster, or should they
    also be categorized as noise? Minimum points thresholding helps solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the scikit-learn implementation of DBSCAN, this hyperparameter is seen in
    the `min_samples` field passed on DBSCAN instance creation. This field is very
    valuable to play with in tandem with the neighborhood radius size hyperparameter
    to fully round out your density-based clustering approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10: Minimum points threshold deciding whether a group of data points
    is noise or a cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.10: Minimum points threshold deciding whether a group of data points
    is noise or a cluster'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the right, if minimum points threshold is 10 points, it will classify data
    in this neighborhood as noise.
  prefs: []
  type: TYPE_NORMAL
- en: In real-world scenarios, you can see minimum points being highly impactful when
    you have truly large amounts of data. Going back to the wine-clustering example,
    if your store was actually a large wine warehouse, you could have thousands of
    individual wines with only one or two bottles that can easily be viewed as their
    own cluster. This may be helpful depending on your use case; however, it is important
    to keep in mind the subjective magnitudes that come with your data. If you have
    millions of data points, then random noise can easily be seen as hundreds or even
    thousands of random one-off sales. However, if your data is on the scale of hundreds
    or thousands, single data points can be seen as random noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Evaluating the Impact of Minimum Points Threshold'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to our *Exercise 9, Evaluating the Impact of Neighborhood Radius Size*,
    where we explored the value of setting a proper neighborhood radius size, we will
    repeat the exercise, but instead will change the minimum points threshold on a
    variety of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Using our current implementation of DBSCAN, we can easily tune the minimum points
    threshold. Tune this hyperparameter and see how it performs on generated data.
  prefs: []
  type: TYPE_NORMAL
- en: By tuning the minimum points threshold for DBSCAN, you will understand how it
    can affect the quality of your clustering predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, let''s start with randomly generated data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a random cluster dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.11: Plot of generated data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.11: Plot of generated data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the same plotted data as before, let''s grab one of the better performing
    neighborhood radius sizes from *Exercise 1*, *Evaluating the Impact of Neighborhood
    Radius Size* – `eps` = 0.7:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '`eps` is a tunable hyperparameter. However, earlier in the same line, we establish
    that 0.7 comes from previous experimentation, leading us to `eps = 0.7` as the
    optimal value.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After instantiating the DBSCAN clustering algorithm, let''s treat the `min_samples`
    hyperparameter as the variable we wish to tune. We can cycle through a loop to
    find which minimum number of points works best for our use case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at the first plot generated, we can see where we ended if you followed
    *Exercise 1*, *Evaluating the Impact of Neighborhood Radius Size* exactly, using
    10 minimum points to mark the threshold for cluster membership:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12: Plot of Toy problem with a minimum of 10 points'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.12: Plot of Toy problem with a minimum of 10 points'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The remaining two hyperparameter options can be seen to greatly impact the
    performance of your DBSCAN clustering algorithm, and show how a shift in one number
    can greatly influence performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Plots of the Toy problem'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Plots of the Toy problem'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, simply changing the number of minimum points from 19 to 20 adds
    an additional (incorrect!) cluster to our feature space. Given what you've learned
    about minimum points through this exercise, you can now tweak both epsilon and
    minimum points thresholding in your scikit-learn implementation to achieve the
    optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our original generation of the data, we created eight clusters. This points
    out that small changes in minimum points can add entire new clusters that we know
    shouldn't be there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5: Comparing DBSCAN with k-means and Hierarchical Clustering'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are managing store inventory and have received a large shipment of wine,
    but the brand labels fell off the bottles during transit. Fortunately, your supplier
    provided you with the chemical readings for each bottle along with their respective
    serial numbers. Unfortunately, you aren't able to open each bottle of wine and
    taste test the difference – you must find a way to group the unlabeled bottles
    back together according to their chemical readings! You know from the order list
    that you ordered three different types of wine and are given only two wine attributes
    to group the wine types back together.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 2*, *Hierarchical Clustering* we were able to see how k-means and
    hierarchical clustering performed on the wine dataset. In our best case scenario,
    we were able to achieve a silhouette score of 0.59\. Using scikit-learn's implementation
    of DBSCAN, let's see if we can get even better clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will help you complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the wine dataset and check what the data looks like
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate clusters using k-means, agglomerative clustering, and DBSCAN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate a few different options for DSBSCAN hyperparameters and their effect
    on the silhouette score
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the final clusters based on the highest silhouette score
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize clusters generated using each of the three methods
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: We have downloaded this dataset from [https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine).
    You can access it at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson03/Activity05](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson03/Activity05).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By completing this activity, you will be recreating a full workflow of a clustering
    problem. You have already made yourself familiar with the data in *Chapter 2*,
    *Hierarchical Clustering*, and, by the end of this activity, you will have performed
    model selection to find the best model and hyperparameters for your dataset. You
    will have silhouette scores of the wine dataset for each type of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 319.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Versus k-means and Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you've reached an understanding of how DBSCAN is implemented and how
    many different hyperparameters you can tweak to drive performance, let's survey
    how it compares to the clustering methods we covered in *Chapter 1*, *Introduction
    to Clustering* and *Chapter 2*, *Hierarchical Clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed in *Activity 5*, *Comparing DBSCAN with k-means and Hierarchical
    Clustering,* that DBSCAN can be a bit finnicky when it comes to finding the optimal
    clusters via silhouette score. This is a downside of the neighborhood approach
    – k-means and hierarchical clustering really excel when you have some idea regarding
    the number of clusters in your data. In most cases, this number is low enough
    that you can iteratively try a few different numbers and see how it performs.
    DBSCAN, instead, takes a more bottom-up approach by working with your hyperparameters
    and finding the clusters it views as important. In practice, it is helpful to
    consider DBSCAN when the first two options fail, simply because of the amount
    of tweaking needed to get it to work properly. That being said, when your DBSCAN
    implementation is working correctly, it will often immensely outperform k-means
    and hierarchical clustering. (In practice, this often happens with highly intertwined,
    yet still discrete data, such as a feature space containing two half-moons).
  prefs: []
  type: TYPE_NORMAL
- en: Compared to k-means and hierarchical clustering, DBSCAN can be seen as being
    potentially more efficient, since it only has to look at each data point once.
    Instead of multiple iterations of finding new centroids and evaluating where their
    nearest neighbors are, once a point has been assigned to a cluster in DBSCAN,
    it does not change cluster membership. The other key difference that DBSCAN and
    hierarchical clustering both share, compared to k-means, is not needing to explicitly
    pass a number of clusters expected at the time of creation. This can be extremely
    helpful when you have no external guidance on how to break your dataset down.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DBSCAN takes an interesting approach to clustering compared to k-means and hierarchical
    clustering. While hierarchical clustering can, in some aspects, be seen as an
    extension of the nearest neighbors approach seen in k-means, DBSCAN approaches
    the problem of finding neighbors by applying a notion of density. This can prove
    extremely beneficial when it comes to highly complex data that is intertwined
    in a complex fashion. While DBSCAN is very powerful, it is not infallible and
    can be seen as potentially overkill, depending on what your original data looks
    like.
  prefs: []
  type: TYPE_NORMAL
- en: Combined with k-means and hierarchical clustering, however, DBSCAN completes
    a strong toolbox when it comes to the unsupervised learning task of clustering
    your data. When faced with any problem in this space, it is worthwhile to compare
    the performance of each method and see which performs best.
  prefs: []
  type: TYPE_NORMAL
- en: 'With clustering explored, we will now move onto another key piece of rounding
    out your skills in unsupervised learning: dimensionality reduction. Through smart
    reduction of dimensions, we can make clustering easier to understand and communicate
    to stakeholders. Dimensionality reduction is also key to creating all types of
    machine learning models in the most efficient manner possible.'
  prefs: []
  type: TYPE_NORMAL

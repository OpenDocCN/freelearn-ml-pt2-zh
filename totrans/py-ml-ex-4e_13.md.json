["```py\n    >>> import torch\n    >>> sentence = torch.tensor(\n            [0, # python\n             8, # machine    \n             1, # learning\n             6, # by\n             2] # example\n        ) \n    ```", "```py\n    >>> torch.manual_seed(0)\n    >>> embed = torch.nn.Embedding(10, 16)\n    >>> sentence_embed = embed(sentence).detach()\n    >>> sentence_embed\n    tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, \n             -2.1152,\n              0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, \n             -0.2473],\n            [-0.8834, -0.4189, -0.8048,  0.5656,  0.6104,  0.4669,  1.9507, \n             -1.0631,\n             -0.0773,  0.1164, -0.5940, -1.2439, -0.1021, -1.0335, -0.3126,  \n              0.2458],\n            [-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  \n              1.8530,\n              0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, \n             -0.8437],\n            [ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965, -0.2282,  \n              0.2800,\n              0.2469,  0.0769,  0.3380,  0.4544,  0.4569, -0.8654,  0.7813, \n             -0.9268],\n             [-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408, \n               0.4412,\n              -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, \n              -1.5867]]) \n    ```", "```py\n    >>> d = sentence_embed.shape[1]\n    >>> w_key = torch.rand(d, d)\n    >>> w_query = torch.rand(d, d)\n    >>> w_value = torch.rand(d, d) \n    ```", "```py\n    >>> token1_embed = sentence_embed[0]\n    >>> key_1 = w_key.matmul(token1_embed)\n    >>> query_1 = w_query.matmul(token1_embed)\n    >>> value_1 = w_value.matmul(token1_embed) \n    ```", "```py\n>>> key_1\ntensor([-1.1371, -0.5677, -0.9324, -0.3195, -2.8886, -1.2679, -1.1153,  \n         0.2904, 0.3825,  0.3179, -0.4977, -3.8230,  0.3699, -0.3932, \n        -1.8788, -3.3556]) \n```", "```py\n    >>> keys = sentence_embed.matmul(w_key.T)\n    >>> keys[0]\n    tensor([-1.1371, -0.5677, -0.9324, -0.3195, -2.8886, -1.2679, -1.1153,  \n             0.2904, 0.3825,  0.3179, -0.4977, -3.8230,  0.3699, -0.3932, \n            -1.8788, -3.3556]) \n    ```", "```py\n>>> values = sentence_embed.matmul(w_value.T) \n```", "```py\n    >>> import torch.nn.functional as F\n    >>> a1 = F.softmax(query_1.matmul(keys.T) / d ** 0.5, dim=0)\n    >>> a1\n    tensor([3.2481e-01, 4.2515e-01, 6.8915e-06, 2.5002e-01, 1.5529e-05]) \n    ```", "```py\n    >>> z1 = a1.matmul(values)\n    >>> z1\n    tensor([-0.7136, -1.1795, -0.5726, -0.4959, -0.6838, -1.6460, -0.3782, -1.0066,\n            -0.4798, -0.8996, -1.2138, -0.3955, -1.3302, -0.3832, -0.8446, -0.8470]) \n    ```", "```py\n    >>> from torchtext.datasets import IMDB\n    >>> train_dataset = list(IMDB(split='train'))\n    >>> test_dataset = list(IMDB(split='test'))\n    >>> print(len(train_dataset), len(test_dataset))\n    25000 25000 \n    ```", "```py\n    >>> train_texts = [train_sample[1] for train_sample in train_dataset]\n    >>> train_labels = [train_sample[0] for train_sample in train_dataset]\n    >>> test_texts = [test_sample[1] for test_sample in test_dataset]\n    >>> test_labels = [test_sample[0] for test_sample in test_dataset] \n    ```", "```py\n    pip install transformers==4.32.1 \n    ```", "```py\nconda install -c huggingface transformers=4.32.1 \n```", "```py\n    >>> import transformers\n    >>> from transformers import DistilBertTokenizerFast\n    >>> tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') \n    ```", "```py\n>>> tokenizer = DistilBertTokenizerFast.from_pretrained(\n                                             'distilbert-base-uncased',\n                                             local_files_only=True) \n```", "```py\n    >>> train_encodings = tokenizer(train_texts, truncation=True,\n                                    padding=True)\n    >>> test_encodings = tokenizer(test_texts, truncation=True, padding=True) \n    ```", "```py\n>>> train_encodings[0]\nEncoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]) \n```", "```py\n    >>> import torch\n    >>> class IMDbDataset(torch.utils.data.Dataset):\n        def __init__(self, encodings, labels):\n            self.encodings = encodings\n            self.labels = labels\n        def __getitem__(self, idx):\n            item = {key: torch.tensor(val[idx])\n                for key, val in self.encodings.items()}\n            item['labels'] = torch.tensor([0., 1.] \n                                   if self.labels[idx] == 2\n                                   else [1., 0.])\n            return item\n        def __len__(self):\n            return len(self.labels) \n    ```", "```py\n    >>> train_encoded_dataset = IMDbDataset(train_encodings, train_labels)\n    >>> test_encoded_dataset = IMDbDataset(test_encodings, test_labels) \n    ```", "```py\n    >>> batch_size = 32\n    >>> train_dl = torch.utils.data.DataLoader(train_encoded_dataset,\n                                               batch_size=batch_size,\n                                               shuffle=True)       \n    >>> test_dl = torch.utils.data.DataLoader(test_encoded_dataset,\n                                              batch_size=batch_size,\n                                              shuffle=False) \n    ```", "```py\n    >>> from transformers import DistilBertForSequenceClassification\n    >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    >>> model = DistilBertForSequenceClassification.from_pretrained(\n                                                   'distilbert-base-uncased',\n                                                  local_files_only=True)\n    >>> model.to(device) \n    ```", "```py\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=5e-5) \n    ```", "```py\n    >>> def train(model, dataloader, optimizer):\n        model.train()\n        total_loss = 0\n        for batch in dataloader:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, \n                            labels=labels)\n            loss = outputs['loss']\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()*len(batch)\n        return total_loss/len(dataloader.dataset) \n    ```", "```py\n    >>> def evaluate(model, dataloader):\n        model.eval()\n        total_acc = 0\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                outputs = model(input_ids, attention_mask=attention_mask)\n                logits = outputs['logits']\n                pred = torch.argmax(logits, 1)\n                 total_acc += (pred == torch.argmax(labels, 1)).float().sum().item()\n        return  total_acc/len(dataloader.dataset) \n    ```", "```py\n    >>> torch.manual_seed(0)\n    >>> num_epochs = 1\n    >>> for epoch in range(num_epochs):\n    >>>     train_loss = train(model, train_dl, optimizer)\n    >>>     train_acc = evaluate(model, train_dl)\n    >>>     print(f'Epoch {epoch+1} - loss: {train_loss:.4f} -\n                    accuracy: {train_acc:.4f}')\n    Epoch 1 - loss: 0.0242 - accuracy: 0.9642 \n    ```", "```py\n    >>> test_acc = evaluate(model, test_dl)\n    >>> print(f'Accuracy on test set: {100 * test_acc:.2f} %')\n    Accuracy on test set: 92.75 % \n    ```", "```py\n    >>> model = DistilBertForSequenceClassification.from_pretrained(\n                                                    'distilbert-base-uncased',\n                                                    local_files_only=True)\n    >>> model.to(device)\n    >>> optim = torch.optim.Adam(model.parameters(), lr=5e-5) \n    ```", "```py\n    conda install -c conda-forge accelerate \n    ```", "```py\npip install accelerate \n```", "```py\n    >>> from transformers import Trainer, TrainingArguments\n    >>> training_args = TrainingArguments(\n            output_dir='./results',\n            num_train_epochs=1,   \n            per_device_train_batch_size=32,\n            logging_dir='./logs',\n            logging_steps=50,\n        )\n    >>> trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_encoded_dataset,\n        optimizers=(optim, None)\n    ) \n    ```", "```py\n    >>> from datasets import load_metric\n    >>> import numpy as np\n    >>> metric = load_metric(\"accuracy\")\n    >>> def compute_metrics(eval_pred):\n            logits, labels = eval_pred\n            pred = np.argmax(logits, axis=-1)\n            return metric.compute(predictions=pred, \n                                  references=np.argmax(labels, 1))\n    >>> trainer = Trainer(\n            model=model,\n            compute_metrics=compute_metrics,\n            args=training_args,\n            train_dataset=train_encoded_dataset,\n            eval_dataset=test_encoded_dataset,\n            optimizers=(optim, None)\n        ) \n    ```", "```py\n    >>> trainer.train()\n    Step     Training Loss\n    50       0.452500\n    100      0.321200\n    150      0.325800\n    200      0.258700\n    250      0.244300\n    300      0.239700\n    350      0.256700\n    400      0.234100\n    450      0.214500\n    500      0.240600\n    550      0.209900\n    600      0.228900\n    650      0.187800\n    700      0.194800\n    750      0.189500\n    TrainOutput(global_step=782, training_loss=0.25071206544061453, metrics={'train_runtime': 374.6696, 'train_samples_per_second': 66.725, 'train_steps_per_second': 2.087, 'total_flos': 3311684966400000.0, 'train_loss': 0.25071206544061453, 'epoch': 1.0}) \n    ```", "```py\n    >>> print(trainer.evaluate())\n    {'eval_loss': 0.18415148556232452, 'eval_accuracy': 0.929, 'eval_runtime': 122.457, 'eval_samples_per_second': 204.153, 'eval_steps_per_second': 25.519, 'epoch': 1.0} \n    ```", "```py\n    >>> from transformers import pipeline, set_seed\n    >>> generator = pipeline('text-generation', model='gpt2')\n    >>> set_seed(0)\n    >>> generator(\"I love machine learning\",\n                  max_length=20,\n                  num_return_sequences=3)\n    [{'generated_text': 'I love machine learning, so you should use machine learning as your tool for data production.\\n\\n'},\n    {'generated_text': 'I love machine learning. I love learning and I love algorithms. I love learning to control systems.'},\n    {'generated_text': 'I love machine learning, but it would be pretty difficult for it to keep up with the demands and'}] \n    ```", "```py\nI love machine learning,” and it produces three alternative sequences with a maximum length of 20 tokens each.\n```", "```py\n    >>> from transformers import TextDataset, GPT2Tokenizer\n    >>> tokenizer = GPT2Tokenizer.from_pretrained('gpt2', local_files_only=True) \n    ```", "```py\n    >>> text_dataset = TextDataset(tokenizer=tokenizer,\n                                   file_path='warpeace_input.txt',\n                                   block_size=128) \n    ```", "```py\n>>> len(text_dataset)\n>>> 6176 \n```", "```py\n    >>> from transformers import DataCollatorForLanguageModeling\n    >>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n                                                        mlm=False) \n    ```", "```py\n    >>> import torch\n    >>> from transformers import GPT2LMHeadModel\n    >>> model = GPT2LMHeadModel.from_pretrained('gpt2')\n    >>> model.to(device)\n    >>> optim = torch.optim.Adam(model.parameters(), lr=5e-5) \n    ```", "```py\n    >>> from transformers import Trainer, TrainingArguments\n    >>> training_args = TrainingArguments(\n            output_dir='./gpt_results',\n            num_train_epochs=20,   \n            per_device_train_batch_size=16,\n            logging_dir='./gpt_logs',\n            save_total_limit=1,\n            logging_steps=500,\n        )\n    >>> trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        optimizers=(optim, None)\n    ) \n    ```", "```py\n    >>> trainer.train()\n    Step    Training Loss\n    500        3.414100\n    1000       3.149500\n    1500       3.007500\n    2000       2.882600\n    2500       2.779100\n    3000       2.699200\n    3500       2.621700\n    4000       2.548800\n    4500       2.495400\n    5000       2.447600\n    5500       2.401400\n    6000       2.367600\n    6500       2.335500\n    7000       2.315100\n    7500       2.300400\n    TrainOutput(global_step=7720, training_loss=2.640813370936893, metrics={'train_runtime': 1408.7655, 'train_samples_per_second': 87.68, 'train_steps_per_second': 5.48, 'total_flos': 8068697948160000.0, 'train_loss': 2.640813370936893, 'epoch': 20.0}) \n    ```", "```py\n    >>> def generate_text(prompt_text, model, tokenizer, max_length):\n            input_ids = tokenizer.encode(prompt_text, \n                                         return_tensors=\"pt\").to(device)\n\n            # Generate response\n            output_sequences = model.generate(\n                input_ids=input_ids,\n                max_length=max_length,\n                num_return_sequences=1,\n                no_repeat_ngram_size=2,\n                top_p=0.9,\n            )\n            # Decode the generated responses\n            responses = []\n            for response_id in output_sequences:\n                response = tokenizer.decode(response_id,\n                                            skip_special_okens=True)\n                responses.append(response)\n            return responses \n    ```", "```py\n    >>> prompt_text = \"the emperor\"\n    >>> responses = generate_text(prompt_text, model, tokenizer, 100)\n    >>> for response in responses:\n            print(response)\n    the emperor's, and the Emperor Francis, who was in attendance on him, was present.\n    The Emperor was present because he had received the news that the French troops were advancing on Moscow, that Kutuzov had been wounded, the Emperor's wife had died, a letter from Prince Andrew had come from Prince Vasili, Prince Bolkonski had seen at the palace, news of the death of the Emperor, but the most important news was that … \n    ```"]
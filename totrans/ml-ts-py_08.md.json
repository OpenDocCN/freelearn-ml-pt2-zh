["```py\npip install -U pandas-datareader plotly \n```", "```py\npip install prophet \n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom pandas_datareader.data import DataReader\nfrom datetime import datetime\nyahoo_data = DataReader('JPM',  'yahoo', datetime(2001,6,1), datetime(2021,6,1))\nyahoo_df = yahoo_data['Adj Close'].to_frame().reset_index('Date') \n```", "```py\nyahoo_df.dtypes \n```", "```py\nDate         datetime64[ns]\nAdj Close           float64\ndtype: object \n```", "```py\nfrom prophet import Prophet\nforecaster = Prophet()\nforecaster.fit(\n  yahoo_df.rename(columns={\"**Date**\": \"**ds**\", \"**Adj Close**\": \"**y**\"})\n) \n```", "```py\nfuture = forecaster.make_future_dataframe(periods=90) \n```", "```py\nforecast = forecaster.predict(future) \n```", "```py\nforecaster.plot(forecast, figsize=(12, 6)); \n```", "```py\npip install statsmodels \n```", "```py\nfrom statsmodels.tsa.regime_switching.tests.test_markov_autoregression import statsmodels.api as sm\nimport seaborn as sn\nimport pandas as pd\ndta = pd.read_stata('https://www.stata-press.com/data/r14/rgnp.dta').iloc[1:]\ndta.index = pd.DatetimeIndex(dta.date, freq='QS')\ndta_hamilton = dta.rgnp \n```", "```py\ndta_hamilton.plot(title='Growth rate of RGNP') \n```", "```py\nimport statsmodels.api as sm\nmod_hamilton = sm.tsa.MarkovAutoregression(dta_hamilton, k_regimes=2, order=4, switching_ar=False)\nres_hamilton = mod_hamilton.fit() \n```", "```py\nprint(res_hamilton.summary()) \n```", "```py\nres_hamilton.expected_durations \n```", "```py\nfrom pandas_datareader.data import DataReader\nfrom datetime import datetime\nusrec = DataReader('USREC', 'fred', start=datetime(1947, 1, 1), end=datetime(2013, 4, 1)) \n```", "```py\nimport matplotlib.pyplot as plt\n_, ax = plt.subplots(1) ax.plot(res_hamilton.filtered_marginal_probabilities[0]) ax.fill_between(\n  usrec.index, 0, 1, where=usrec['USREC'].values,\n  color='gray', alpha=0.3\n)\nax.set(\n  xlim=(dta_hamilton.index[4], dta_hamilton.index[-1]),\n  ylim=(0, 1),\n  title='Filtered probability of recession'\n) \n```", "```py\npip install pyFTS SimpSOM \n```", "```py\nfrom pyFTS.data import NASDAQ, SP500\ndatasets = {\n  \"SP500\": SP500.get_data()[11500:16000],\n  \"NASDAQ\": NASDAQ.get_data()\n} \n```", "```py\ntrain_split = 2000 \n```", "```py\nfrom pyFTS.common import Transformations\ntdiff = Transformations.Differential(1) \n```", "```py\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(nrows=2, ncols=2)\nfor count, (dataset_name, dataset) in enumerate(datasets.items()):\n  dataset_diff = tdiff.apply(dataset)\n  ax[0][count].plot(dataset)\n  ax[1][count].plot(dataset_diff)\n  ax[0][count].set_title(dataset_name) \n```", "```py\nfrom pyFTS.models import song\nfrom pyFTS.partitioners import Grid\nmodels = {}\nfor count, (dataset_name, dataset) in enumerate(datasets.items()):\n  partitioner_diff = Grid.GridPartitioner(data=dataset, npart=15, transformation=tdiff)\n  model = song.ConventionalFTS(partitioner=partitioner_diff)\n  model.name = dataset_name\n  model.append_transformation(tdiff)\n  model.fit(\n    dataset[:train_split], \n    order=1\n  )\n  models[dataset_name] = model \n```", "```py\n_, ax = plt.subplots(nrows=2, ncols=1, figsize=[12, 6])\nfor count, (dataset_name, dataset) in enumerate(datasets.items()):\n    ax[count].plot(dataset[train_split:train_split+200])\n    model = models[dataset_name]\n    forecasts = model.predict(dataset[train_split:train_split+200], steps_ahead=1)\n    ax[count].plot(forecasts)\n    ax[count].set_title(dataset_name)\n\nplt.tight_layout() \n```", "```py\nfrom pyFTS.benchmarks import Measures\nrows = []\nfor count, (dataset_name, dataset) in enumerate(datasets.items()):\n    row = [dataset_name]\n    test = dataset[train_split:train_split+200]\n    model = models[dataset_name]\n    row.extend(Measures.get_point_statistics(test, model))\n    rows.append(row)\n\npd.DataFrame(\n  rows,columns=[\"Dataset\", \"RMSE\", \"MAPE\", \"Theil's U\"]\n).set_index(\"Dataset\") \n```", "```py\npip install tfcausalimpact \n```", "```py\nimport pandas as pd\nfrom causalimpact import CausalImpact\ndata = pd.read_csv(**\"https://raw.githubusercontent.com/WillianFuks/tfcausalimpact/master/tests/fixtures/volks_data.csv\"**, header=0, sep=**'** **'**, index_col=**'Date'**, parse_dates=True) \n```", "```py\ndata.plot() \n```", "```py\npre_period = [str(np.min(data.index.values)), \"2015-09-13\"]\npost_period = [\"2015-09-20\", str(np.max(data.index.values))]\nci = CausalImpact(data.iloc[:, 0], pre_period, post_period, model_args={'nseasons': 52, 'fit_method': 'vi'}) \n```", "```py\nprint(ci.summary()) \n```", "```py\nPosterior Inference {Causal Impact}\n                          Average              Cumulative\nActual                    126.91               10026.07\nPrediction (s.d.)         171.28 (17.33)       13531.49 (1369.17)\n95% CI                    [136.07, 204.01]     [10749.78, 16116.83]\nAbsolute effect (s.d.)    -44.37 (17.33)       -3505.42 (1369.17)\n95% CI                    [-77.1, -9.16]       [-6090.76, -723.71]\nRelative effect (s.d.)    -25.91% (10.12%)     -25.91% (10.12%)\n95% CI                    [-45.01%, -5.35%]    [-45.01%, -5.35%]\nPosterior tail-area probability p: 0.01\nPosterior probability of a causal effect: 99.2% \n```", "```py\nDuring the post-intervention period, the response variable had\nan average value of approx. 126.91\\. By contrast, in the absence of an intervention, we would have expected an average response of 171.28\\. The 95% interval of this counterfactual prediction is [136.07, 204.01].\nSubtracting this prediction from the observed response yields\nan estimate of the causal effect the intervention had on the\nresponse variable. This effect is -44.37 with a 95% interval of [-77.1, -9.16]. For a discussion of the significance of this effect, see below. \n```", "```py\nci.plot(panels=[\"original\"] \n```"]
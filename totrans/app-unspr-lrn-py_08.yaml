- en: '*Chapter 8*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Market Basket Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Work with transaction-level data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use market basket analysis in the appropriate context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the Apriori algorithm and build association rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform basic visualizations on association rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the key metrics of market basket analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will explore a foundational and reliable algorithm for analyzing
    transaction data.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we are going to change direction entirely. The previous chapter,
    which explored topic models, focused on natural language processing, text data,
    and applying relatively recently developed algorithms. Most data science practitioners
    would agree that natural language processing, including topic models, is toward
    the cutting edge of data science and is an active research area. We now understand
    that topic models can, and should, be leveraged wherever text data could potentially
    drive insights or growth, including in social media analysis, recommendation engines,
    and news filtering.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter takes us into the retail space to explore a foundational and reliable
    algorithm for analyzing transaction data. While this algorithm might not be on
    the cutting edge or in the catalog of the most popular machine learning algorithms,
    it is ubiquitous and undeniably impactful in the retail space. The insights it
    drives are easily interpretable, immediately actionable, and instructive for determining
    analytical next steps. If you work in the retail space or with transaction data,
    you would be well-served to dive deep into market basket analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Market Basket Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine you work for a retailer that sells dozens of products and your boss
    comes to you and asks the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What products are purchased together most frequently?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should the products be organized and positioned in the store?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we identify the best products to discount via coupons?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might reasonably respond with complete bewilderment, as those questions
    are very diverse and do not immediately seem answerable using a single algorithm
    and dataset. However, the answer to all those questions and many more is **market
    basket analysis**. The general idea behind market basket analysis is to identify
    and quantify which items, or groups of items, are purchased together frequently
    enough to drive insight into customer behavior and product relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the analytics, it is worth defining the term market basket.
    A market basket is a permanent set of products in an economic system. In this
    case, permanent does not necessarily mean permanent in the traditional sense.
    It means that until such time as the product is taken out of the catalog, it will
    consistently be available for purchase. The product referenced in the preceding
    definition is any good, service, or element of a group, including a bicycle, having
    your house painted, or a website. Lastly, an economic system could be a company,
    a collection of activities, or a country. The easiest example of a market basket
    is a grocery store, which is a system made up of a collection of food and drink
    items.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: An example market basket where the economic system is the butcher
    shop and the permanent set of items is all the meat products offered by the butcher'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.1: An example market basket where the economic system is the butcher
    shop and the permanent set of items is all the meat products offered by the butcher'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even without using any models or analyses, certain product relationships are
    obvious. Let's take the relationship between meat and vegetables. Typically, market
    basket analysis models return relationships more specific than meat and vegetables,
    but, for argument's sake, we will generalize to meat and vegetables. Okay, there
    is a relationship between meat and vegetables. So what? Well, we know these are
    staple items that are frequently purchased together. We can leverage this information
    by putting the vegetables and meats on opposite sides of the store, which you
    will notice is often the positioning of those two items, forcing customers to
    walk the full distance of the store, and thereby increasing the likelihood that
    they will buy additional items that they might not have bought if they did not
    have to traverse the whole store.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the things retail companies struggle with is how to discount items effectively.
    Let''s consider another obvious relationship: peanut butter and jelly. In the
    United States, peanut butter and jelly sandwiches are incredibly popular, especially
    among children. When peanut butter is in a shopping basket, the chance jelly is
    also there can be assumed to be quite high. Since we know peanut butter and jelly
    are purchased together, it does not make sense to discount them both. If we want
    customers to buy both items, we can just discount one of the items, knowing that
    if we can get the customers to buy the discounted item, they will probably buy
    the other item too, even if it is full price.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: A visualization of market basket analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.2: A visualization of market basket analysis'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just like the topic models in the previous chapter, market basket analysis is
    all about identifying frequently occurring groups. Here, we are looking for frequently
    occurring groups of products, whereas in topic models, we were looking for frequently
    occurring groups of words. Thus, as it was to topic models, the word clustering
    could be applied to market basket analysis. The major differences are that the
    clusters in market basket analysis are micro, only a few products per cluster,
    and the order of the items in the cluster matters when it comes to computing probabilistic
    metrics. We will dive much deeper into these metrics and how they are calculated
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What has clearly been implied by the previous two examples is that, in market
    basket analysis, retailers can discover the relationships – obvious and surprising
    – between the products that customers buy. Once uncovered, the relationships can
    be used to inform and improve the decision-making process. A great aspect of market
    basket analysis is that while this analysis was developed in relation to, discussed
    in terms of, and mostly applied to the retail world, it can be applied to many
    diverse types of businesses.
  prefs: []
  type: TYPE_NORMAL
- en: The only requirement for performing this type of analysis is that the data is
    a list of collections of items. In the retail case, this would be a list of transactions
    where each transaction is a group of purchased products. One example of an alternative
    application is analyzing website traffic. With website traffic, we consider the
    products to be websites, so each element of the list is the collection of websites
    visited by an individual over a specified time period. Needless to say, the applications
    of market basket analysis extend well beyond the principal retail application.
  prefs: []
  type: TYPE_NORMAL
- en: Use Cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are three principal use cases in the traditional retail application:
    pricing enhancement, coupon and discount recommendation, and store layout. As
    was briefly mentioned previously, by using the product associations uncovered
    by the model, retailers can strategically place products in their stores to get
    customers to buy more items and thus spend more money. If any relationship between
    two or more products is sufficiently strong, meaning the product grouping occurs
    often in the dataset and the individual products in the grouping appear separate
    from the group infrequently, then the products could be placed far away from one
    another in the store without significantly jeopardizing the odds of the customer
    purchasing both products. By forcing the customer to traverse the whole store
    to get both products, the retailer increases the chances that the customer will
    notice and purchase additional products. Likewise, retailers can increase the
    chances of customers purchasing two weakly related or non-staple products by placing
    the two items next to each other. Obviously, there are a lot of factors that drive
    store layout, but market basket analysis is definitely one of those factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: How product associations can help inform efficient and lucrative
    store layouts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.3: How product associations can help inform efficient and lucrative
    store layouts'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pricing enhancement and coupon and discount recommendation are two sides of
    the same coin. They can simply be interpreted as where to raise and where to lower
    prices. Consider the case of two strongly related items. These two items are most
    likely going to be purchased in the same transaction, so one way to increase the
    profitability of that transaction would be to increase the price of one of the
    items. If the association between the two items is sufficiently strong, the price
    increase can be made with little to no risk of the customer not purchasing both
    items. In a similar way, retailers can encourage customers to purchase an item
    weakly associated with another through discounting or couponing.
  prefs: []
  type: TYPE_NORMAL
- en: For example, retailers could compare the purchase history of individual customers
    with the results of market basket analysis done on all transactions and find where
    some of the items certain customers are purchasing are weakly associated to items
    those customers are not currently purchasing. Using this comparison, retailers
    could offer discounts to the customers for the as-yet-unpurchased items the model
    suggested were related to the items previously purchased by those customers. If
    you have ever had coupons print out with your receipt at the end of a transaction,
    the chances are high that those items were found to be related to the items involved
    in your just-completed transaction.
  prefs: []
  type: TYPE_NORMAL
- en: A non-traditional, but viable, use of market basket analysis would be to enhance
    online advertising and search engine optimization. Imagine we had access to lists
    of websites visited by individuals. Using market basket analysis, we could find
    relationships between websites and use those relationships to both strategically
    order and group the websites resulting from a search engine query. In many ways,
    this is similar to the store layout use case.
  prefs: []
  type: TYPE_NORMAL
- en: With a general sense of what market basket analysis is all about and a clear
    understanding of its use cases, let's dig into the data used in these models.
  prefs: []
  type: TYPE_NORMAL
- en: Important Probabilistic Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Market basket analysis is built upon the computation of several probabilistic
    metrics. The five major metrics covered here are support, confidence, lift, leverage,
    and conviction. Before digging into transaction data and the specific market basket
    analysis models, including the **Apriori algorithm** and **association rules**,
    we should spend some time defining and exploring these metrics using a small,
    made-up set of transactions. We start by making up some data to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 39: Creating Sample Transaction Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since this is the first exercise of the chapter, let's set the environment.
    This chapter will use the same environment requirements that were used in *Chapter
    7*, *Topic Modeling*. If any of the packages do not load, as happened in the previous
    chapter, use `pip` to install them via the command line. One of the libraries
    we will use is `mlxtend`, which may be unfamiliar to you. It is a machine learning
    extensions library that contains useful supplemental tools, including ensembling,
    stacking, and, of book, market basket analysis models. This exercise does not
    have any real output. We will simply create a sample transaction dataset for use
    in subsequent exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Open a Jupyter notebook with Python 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the following libraries: `matplotlib.pyplot`, which is used to plot
    the results of the models, `mlxtend.frequent_patterns`, which is used to run the
    models, `mlxtend.preprocessing`, which is used to encode and prep the data for
    the models, `numpy`, which is used to work with arrays, and `pandas`, which is
    used to work with DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create 10 fake transactions featuring grocery store items. The data will take
    the form of a list of lists, a data structure that will be relevant later when
    discussing formatting transaction data for the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This simple dataset will make explaining and interpreting the probabilistic
    metrics much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Support** is simply the probability that the item set appears in the data,
    which can be calculated by counting the number of transactions in which the item
    set appears and dividing that count by the total number of transactions. Note
    that an item set can be a single item or a group of items. Support is an important
    metric, despite being very simple, as it is one of the primary metrics used to
    determine the believability and strength of association between groups of items.
    For example, it is possible to have two items that only occur with each other,
    suggesting that their association is very strong, but in a dataset containing
    100 transactions, only appearing twice is not very impressive. Because the item
    set appears in only 2% of the transactions, and 2% is small in terms of the raw
    number of appearances, the association cannot be considered significant and, thus,
    is probably unusable in decision making.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that since support is a probability, it will fall in the range [0,1]. The
    formula takes the following form if the item set is two items, X and Y, and N
    is the total number of transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Formula for support'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.4: Formula for support'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's return momentarily to the made-up data from *Exercise 39*, *Creating Sample
    Transaction Data* and define an item set as being milk and bread. We can easily
    look through the 10 transactions and count the number of transactions in which
    this milk and bread item set occurs – that would be 4 times. Given that there
    are 10 transactions, the support of milk and bread is 4 divided by 10, or 0.4\.
    Whether this is large enough support depends on the dataset itself, which we will
    get into in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **confidence** metric can be thought of in terms of conditional probability,
    as it is basically the probability that product B is purchased given the purchase
    of product A. Confidence is typically notated as A ![](img/C12626_Formula_08_01.png)
    B, and expressed as the proportion of transactions containing A that also contain
    B. Hence, confidence is found by filtering the full set of transactions down to
    those containing A, and then computing the proportion of those transactions that
    contain B. Like support, confidence is a probability, so its range is [0,1]. Using
    the same variable definitions from the support section, the following is the formula
    for confidence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Formula for confidence'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.5: Formula for confidence'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To demonstrate confidence, we will use the items beer and wine. Specifically,
    let''s compute the confidence of Beer ![](img/C12626_Formula_08_02.png) Wine.
    To start, we need to identify the transactions that contain beer. There are 3
    of them, and they are transactions 2, 6, and 7\. Now, of those transactions, how
    many contain wine? The answer is all of them. Thus, the confidence of Beer ![](img/C12626_Formula_08_03.png)
    Wine is 1\. Every time a customer bought beer, they also bought wine. It might
    be obvious, but for identifying actionable associations, higher confidence values
    are better:'
  prefs: []
  type: TYPE_NORMAL
- en: Lift and Leverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will discuss the next two metrics, lift and leverage, simultaneously, since
    despite being calculated differently, both seek to answer the same question. Like
    confidence, **lift** and **leverage** are notated as A ![](img/C12626_Formula_08_04.png)
    B. The question to which we seek an answer is, can one item, say A, be used to
    determine anything about another item, say B? Stated another way, if product A
    is bought by an individual, can we say anything about whether they will or will
    not purchase product B with some level of confidence? These questions are answered
    by comparing the support of A and B under the standard case when A and B are not
    assumed to be independent with the case where the two products are assumed to
    be independent. Lift calculates the ratio of these two cases, so its range is
    [0, Infinity]. When lift equals one, the two products are independent and, hence,
    no conclusions can be made about product B when product A is purchased:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6: Formula for lift'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.6: Formula for lift'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Leverage calculates the difference between the two cases, so its range is [-1,
    1]. Leverage equaling zero can be interpreted the same way as lift equaling one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7: Formula for leverage'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.7: Formula for leverage'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The values of the metrics measure the strength and direction of the relationship
    between the items. If the lift value is 0.1, we could say the relationship between
    the two items is strong in the negative direction. That is, it could be said that
    when one product is purchased, the chance the second product is purchased is diminished.
    The positive and negative associations are separated by the points of independence,
    which, as stated earlier, are 1 for lift and 0 for leverage, and the further away
    the value gets from these points, the stronger the association.
  prefs: []
  type: TYPE_NORMAL
- en: Conviction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last metric to be discussed is conviction, which is a bit less intuitive
    than the other metrics. Conviction is the ratio of the expected frequency that
    X occurs without Y, given that X and Y are independent to the frequency of incorrect
    predictions. The frequency of incorrect predictions is defined as 1 minus the
    confidence of X ![](img/C12626_Formula_08_05.png) Y. Remember that confidence
    can be defined as ![](img/C12626_Formula_08_06.png), which means .![](img/C12626_Formula_08_07.png).
    The numerator could also be thought of as ![](img/C12626_Formula_08_08.png). The
    only difference between the two is that the numerator has the assumption of independence
    between X and Y, while the denominator does not. A value greater than 1 is ideal
    because that means the association between products or item sets X and Y is incorrect
    more often if the association between X and Y is random chance (in other words,
    X and Y are independent). To reiterate, this stipulates that the association between
    X and Y is meaningful. A value of 1 applies independence, and a value of less
    than 1 signifies that the random chance X and Y relationship is correct more often
    than the X and Y relationship that has been defined as X ![](img/C12626_Formula_08_09.png)
    Y. Under this situation, the relationship might go the other way (in other words,
    Y ![](img/C12626_Formula_08_10.png) X). Conviction has the range [0, Inf] and
    the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8: Formula for conviction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.8: Formula for conviction'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's again return to the products beer and wine, but for this explanation,
    we will consider the opposite association of Wine ![](img/C12626_Formula_08_11.png)
    Beer. Support(Y) or, in this case, Support(Beer) is 3/10 and Confidence X ![](img/C12626_Formula_08_12.png)
    Y, or, in this case, Confidence(Wine ![](img/C12626_Formula_08_13.png) Beer),
    is 3/4\. Thus, the Conviction(Wine ![](img/C12626_Formula_08_14.png) Beer) is
    (1-3/10) / (1-3/4) = (7/10) * (4/1). We can conclude by saying that Wine ![](img/C12626_Formula_08_15.png)
    Beer would be incorrect 2.8 times as often if wine and beer were independent.
    Thus, the previously articulated association between wine and beer is legitimate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 40: Computing Metrics'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we use the fake data in *Exercise 39*, *Creating Sample Transaction
    Data* to compute the five previously described metrics, which we will use again
    in the covering of the Apriori algorithm and association rules. The association
    on which these metrics will be evaluated is Milk ![](img/C12626_Formula_08_16.png)
    Bread.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All exercises in this chapter need to be performed in the same Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define and print the frequencies that are the basis of all five metrics, which
    would be Frequency(Milk), Frequency(Bread), and Frequency(Milk, Bread). Also,
    define N as the total number of transactions in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9: Screenshot of the frequencies'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.9: Screenshot of the frequencies'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Calculate and print Support(Milk ![](img/C12626_Formula_08_17.png) Bread):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The support of `x` to `y` is `0.4`. From experience, if we were working with
    a full transaction dataset, this support value would be considered very large
    in many cases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate and print Confidence(Milk ![](img/C12626_Formula_08_17.png) Bread):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The confidence of `x` to `y` is `0.5714`. This means that the probability of
    Y being purchased given that `x` was purchased is just slightly higher than 50%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate and print Lift(Milk ![](img/C12626_Formula_08_19.png) Bread):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The lift of `x` to `y` is `1.1429`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate and print Leverage(Milk ![](img/C12626_Formula_08_19.png) Bread):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The leverage of `x` to `y` is `0.05`. Both lift and leverage can be used to
    say that the association `x` to `y` is positive (in other words, `x` implies `y`),
    but weak. That is, the values are close to 1 and 0, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate and print Conviction(Milk ![](img/C12626_Formula_08_19.png) Bread):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The conviction value of `1.1667` can be interpreted by saying the Milk ![](img/C12626_Formula_08_19.png)
    Bread association would be incorrect `1.1667` times as often if milk and bread
    were independent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before diving into the Apriori algorithm and association rule learning on actual
    data, we will explore transaction data and get some retail data loaded and prepped
    for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of Transaction Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data used in market basket analysis is transaction data or any type of
    data that resembles transaction data. In its most basic form, transaction data
    has some sort of transaction identifier, such as an invoice or transaction number,
    and a list of products associated with said identifier. It just so happens that
    these two base elements are all that is needed to perform market basket analysis.
    However, transaction data rarely – it is probably even safe to say never – comes
    in this basic form. Transaction data typically includes pricing information, dates
    and times, and customer identifiers, among many other things:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10: Each available product is going to map back to multiple invoice
    numbers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.10: Each available product is going to map back to multiple invoice
    numbers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Due to the complexity of transaction data, data cleaning is crucial. The goal
    of data cleaning in the context of market basket analysis is to filter out all
    the unnecessary information, which includes removing variables in the data that
    are not relevant, and filtering out problematic transactions. The techniques used
    to complete these two cleaning steps vary, depending on the particular transaction
    data file. In an attempt to not get bogged down in data cleaning, the exercises
    from here on out will use a subset of an online retail dataset from the UCI Machine
    Learning Repository, and the activities will use the whole dataset. This both
    limits the data cleaning discussion, but also gives us an opportunity to discuss
    how the results change when the size of the dataset changes. This is important
    because if you work for a retailer and run market basket analysis, it will be
    important to understand and be able to clearly articulate the fact that, as more
    data is received, product relationships can, and most likely will, shift. Before
    discussing the specific cleaning process required for this dataset, let's load
    the online retail dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 41: Loading Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will load and view an example online retail dataset. This
    dataset is originally from the UCI Machine Learning Repository and can be found
    at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45).
    Once you have downloaded the dataset, save it and note the path. Now, let's proceed
    with the exercise. The output of this exercise is the transaction data that will
    be used in future modeling exercises and some exploratory figures to help us better
    understand the data with which we are working.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This dataset is taken from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Exercise39-Exercise45).
    Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining, Journal
    of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208,
    2012.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `read_excel` function from `pandas`, load the data. Note that the
    first row of the Excel file contains the column names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The path to `Online Retail.xlsx` should be changed as per the location of the
    file on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print out the first 10 rows of the DataFrame. Notice that the data contains
    some columns that will not be relevant to market basket analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.11: The raw online retail data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.11: The raw online retail data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print out the data type for each column in the DataFrame. This information
    will come in handy when trying to perform specific cleaning tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.12: Data type for each column in the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.12: Data type for each column in the dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Get the dimensions of the DataFrame, as well as the number of unique invoice
    numbers and customer identifications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this exercise, we have loaded the data and performed some exploratory work.
  prefs: []
  type: TYPE_NORMAL
- en: Data Cleaning and Formatting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the dataset now loaded, let's delve into the specific data cleaning processes
    to be performed. Since we are going to filter the data down to just the invoice
    numbers and items, we focus the data cleaning on these two columns of the dataset.
    Remember that market basket analysis looks to identify associations between the
    items purchased by all customers over time. As such, the main focus of the data
    cleaning involves removing transactions with non-positive numbers of items. This
    could happen when the transaction involves voiding another transaction, when items
    are returned, or when the transaction is some administrative task. These types
    of transactions will be filtered out in two ways. The first is that canceled transactions
    have invoice numbers that are prefaced with "C," so we will identify those specific
    invoice numbers and remove them from the data. The other approach is to remove
    all transactions with either zero or negative numbers of items. After performing
    these two steps, the data will be subset down to just the invoice number and item
    description columns, and any row of the now two-column dataset with at least one
    missing value is removed.
  prefs: []
  type: TYPE_NORMAL
- en: The next stage of the data cleaning exercise involves putting the data in the
    appropriate format for modeling. In this and subsequent exercises, we will use
    a subset of the full data. The subset will be done by taking the first 5,000 unique
    invoice numbers. Once we have cut the data down to the first 5,000 unique invoice
    numbers, we change the data structure to that needed to run the models. Note that
    the data is currently in long format, where each item is on its own row. The desired
    format is a list of lists, like the made-up data from earlier in the chapter.
    Each subset list represents a unique invoice number, so in this case, the outer
    list should contain 5,000 sub-lists. The elements of the sub-lists are all the
    items belonging to the invoice number that that sub-list represents. With the
    cleaning process described, let's proceed to the exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 42: Data Cleaning and Formatting'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform the cleaning steps described previously. As
    we work through the process, the evolution of the data will be monitored by printing
    out the current state of the data and computing some basic summary metrics. Be
    sure to perform data cleaning in the same notebook in which the data is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an indicator column stipulating whether the invoice number begins with
    "`C`":'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Filter out all transactions having either zero or a negative number of items,
    remove all invoice numbers starting with "C" using the column created in step
    one, subset the DataFrame down to `InvoiceNo` and `Description`, and lastly, drop
    all rows with at least one missing value. Rename the DataFrame `online1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the first 10 rows of the filtered DataFrame, `online1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 8.13: The cleaned online retail dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.13: The cleaned online retail dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print out the dimensions of the cleaned DataFrame and the number of unique
    invoice numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we have already removed approximately 10,000 rows and 5,800 invoice
    numbers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the invoice numbers from the DataFrame as a list. Remove duplicate
    elements to create a list of unique invoice numbers. Confirm that the process
    was successful by printing the length of the list of unique invoice numbers. Compare
    with the output of *Step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take the list from step five and cut it to only include the first 5,000 elements.
    Print out the length of the new list to confirm that it is, in fact, the expected
    length of 5,000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Filter the `online1` DataFrame down by only keeping the invoice numbers in
    the list from the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the first 10 rows of `online1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.14: The cleaned dataset with only 5,000 unique invoice numbers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.14: The cleaned dataset with only 5,000 unique invoice numbers'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print out the dimensions of the DataFrame and the number of unique invoice
    numbers to confirm that the filtering and cleaning process was successful:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the data in `online1` into the aforementioned list of lists called
    `invoice_item_list`. The process for doing this is to iterate over the unique
    invoice numbers and, at each iteration, extract the item descriptions as a list
    and append that list to the larger `invoice_item_list` list. Print out elements
    one through four of the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.15: Four elements of the list of lists, where each sub-list contains
    all the items belonging to an individual invoice'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.15: Four elements of the list of lists, where each sub-list contains
    all the items belonging to an individual invoice'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This step can take some minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Data Encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While cleaning the data is crucial, the most important part of the data preparation
    process is molding the data into the correct form. Before running the models,
    the data, currently in the list of lists form, needs to be encoded and recast
    as a DataFrame. To do this, we will leverage `TransactionEncoder` from the `preprocessing`
    module of `mlxtend`. Outputted from the encoder is a multidimensional array, where
    each row is the length of the total number of unique items in the transaction
    dataset and the elements are Boolean variables, indicating whether that particular
    item is linked to the invoice number that row represents. With the data encoded,
    we can recast it as a DataFrame where the rows are the invoice numbers and the
    columns are the unique items in the transaction dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, the data encoding will be done using `mlxtend`, but
    it is very easy to encode the data without using any package. The first step is
    to unlist the list of lists and return one list with every value from the original
    list of lists. Next, the duplicate products are filtered out and, if preferred,
    the data is sorted in alphabetical order. Before doing the actual encoding, we
    initialize the final DataFrame by having all elements equal to false, a number
    of rows equal to the number of invoice numbers in the dataset, and column names
    equal to the non-duplicated list of product names.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we have 5,000 transactions and over 3,100 unique products. Thus,
    the DataFrame has over 15,000,000 elements. The actual encoding is done by looping
    over each transaction and each item in each transaction. Change the row *i* and
    column *j* cell values in the initialized dataset from false to true if the ![](img/C12626_Formula_08_23.png)
    transaction contains the ![](img/C12626_Formula_08_24.png) product. This double
    loop is not fast as we need to iterate over 15,000,000 cells. There are ways to
    improve performance, including some that have been implemented in `mlxtend`, but
    to better understand the process, it is helpful to work through the double loop
    methodology. The following is an example function to do the encoding from scratch
    without the assistance of any package other than `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise 43: Data Encoding'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we continue the data preparation process by taking the list
    of lists generated in the previous exercise and encoding the data in the specific
    way required to run the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize and fit the transaction encoder. Print out an example of the resulting
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.16: The multi-dimensional array containing the Boolean variables
    indicating product presence in each transaction'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.16: The multi-dimensional array containing the Boolean variables indicating
    product presence in each transaction'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Recast the encoded array as a DataFrame named `online_encoder_df`. Print out
    a predefined subset of the DataFrame that features both true and false values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.17: A small section of the encoded data recast as a DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.17: A small section of the encoded data recast as a DataFrame'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print out the dimensions of the encoded DataFrame. It should have 5,000 rows
    because the data used to generate it was previously filtered down to 5,000 unique
    invoice numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The data is now prepped for modeling. In the next section, we will explore the
    Apriori algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 18: Loading and Preparing Full Online Retail Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we are charged with loading and preparing a large transaction
    dataset for modeling. The final output will be an appropriately encoded dataset
    that has one row for each unique transaction in the dataset, and one column for
    each unique item in the dataset. If an item appears in an individual transaction,
    that element of the DataFrame will be marked true.
  prefs: []
  type: TYPE_NORMAL
- en: This activity will largely repeat the last few exercises, but will use the complete
    online retail dataset file. No new downloads need to be executed, but you will
    need the path to the file downloaded previously. Perform this activity in a separate
    Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the online retail dataset file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This dataset is taken from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail).
    It can be downloaded from https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20\.
    Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining, Journal
    of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208,
    2012.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clean and prep the data for modeling, including turning the cleaned data into
    a list of lists.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Encode the data and recast it as a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 366.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18: A subset of the cleaned, encoded, and recast DataFrame built
    from the complete online retail dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.18: A subset of the cleaned, encoded, and recast DataFrame built from
    the complete online retail dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Apriori Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Apriori** algorithm is a data mining methodology for identifying and quantifying
    frequent item sets in transaction data, and is the foundational component of association
    rule learning. Extending the results of the Apriori algorithm to association rule
    learning will be discussed in the next section. The minimum value to qualify as
    frequent in the Apriori algorithm is an input into the model and, as such, is
    adjustable. Frequency is quantified here as support, so the value inputted into
    the model is the minimum support acceptable for the analysis being done. The model
    then identifies all item sets whose support is greater than, or equal to, the
    minimum support provided to the model. Note that the minimum support parameter
    is not a parameter that can be optimized via a grid search because there is no
    evaluation metric for the Apriori algorithm. Instead, the minimum support parameter
    is set based on the data, the use case, and domain expertise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea behind the Apriori algorithm is the Apriori principle: any subset
    of a frequent item set must itself be frequent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another aspect worth mentioning is the corollary: no superset of an infrequent
    item set can be frequent.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take some examples. If the item set {hammer, saw, and nail} is frequent,
    then, according to the Apriori principle and what is hopefully obvious, any less
    complex item set, say {hammer, saw}, is also frequent. On the contrary, if that
    same item set, {hammer, saw, nail}, is infrequent, then adding complexity, such
    as incorporating wood in the item set {hammer, saw, nail, wood}, is not going
    to result in the item set becoming frequent.
  prefs: []
  type: TYPE_NORMAL
- en: 'It might seem straightforward to calculate the support value for every item
    set in a transactional database and only return those item sets whose support
    is greater than or equal to the prespecified minimum support threshold, but it
    is not because of the number of computations that need to happen. For example,
    take an item set with 10 unique items. This would result in 1,023 individual item
    sets for which support would need to be calculated. Now, try to extrapolate out
    to our working dataset that has 3,135 unique items. That is going to be an enormous
    number of item sets for which we need to compute a support value. Computational
    efficiency is a major issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19: A mapping of how item sets are built and how the Apriori principle
    can greatly decrease the computational requirements (all the grayed-out nodes
    are infrequent)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.19: A mapping of how item sets are built and how the Apriori principle
    can greatly decrease the computational requirements (all the grayed-out nodes
    are infrequent)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In order to address the computational demands, the Apriori algorithm is defined
    as a bottom-up model that has two steps. These steps involve generating candidate
    item sets by adding items to already existing frequent item sets and testing these
    candidate item sets against the dataset to determine whether these candidate datasets
    are also frequent. No support value is computed for item sets that contain infrequent
    item sets. This process repeats until no further candidate item sets exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20: Assuming a minimum support threshold of 0.4, the diagram shows
    the general Apriori algorithm structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.20: Assuming a minimum support threshold of 0.4, the diagram shows
    the general Apriori algorithm structure'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The preceding structure includes establishing an item set, computing support
    values, filtering out infrequent item sets, creating new item sets, and repeating
    the process.
  prefs: []
  type: TYPE_NORMAL
- en: There is a clear tree-like structure that serves as the path for identifying
    candidate item sets. The specific search technique used, which was built for traversing
    tree-like data structures, is called a breadth-first search, which means that
    each step of the search process focuses on completely searching one level of the
    tree before moving on instead of searching branch by branch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-level steps of the algorithm are to:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the set of frequent items. To start, this is typically the set of individual
    items.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Derive candidate item sets by combining frequent item sets together. Move up
    in size one item at a time. That is, go from item sets with one item to two, two
    to three, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the support value for each candidate item set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new frequent item set made up of the candidate item sets whose support
    value exceeded the specified threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 1* to *4* until there are no more frequent item sets; that is,
    until we have worked through all the combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudo code for the Apriori algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Despite the Apriori principle, this algorithm can still face significant computational
    challenges depending on the size of the transaction dataset. There are several
    strategies currently accepted to further reduce the computational demands.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Fixes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transaction reduction is an easy way to reduce the computational load. Note
    that after each candidate set of item sets is generated, the entirety of the transaction
    data needs to be scanned in order to count the number of appearances of each candidate
    item set. If we could shrink the size of the transaction dataset, the size of
    the dataset scans would decrease dramatically. The shrinking of the transaction
    dataset is done by realizing that any transaction containing no frequent item
    sets in the *ith* iteration is not going to contain any frequent item sets in
    subsequent iterations. Therefore, once each transaction contains no frequent item
    sets, it can be removed from the transaction dataset used for future scans.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling the transaction dataset and testing each candidate item set against
    it is another approach to reducing the computational requirements associated with
    scanning the transaction dataset to calculate the support of each item set. When
    this approach is implemented it is important to lower the minimum support requirement
    to guarantee that no item sets that should be present in the final data are left
    out. Given that the sampled transaction dataset will naturally cause the support
    values to be smaller, leaving the minimum support at its original value will incorrectly
    remove what should be frequent item sets from the output of the model.
  prefs: []
  type: TYPE_NORMAL
- en: A similar approach is partitioning. In this case, the dataset is partitioned
    into several individual datasets on which the evaluation of each candidate item
    set is executed. Item sets are deemed frequent in the full transaction dataset
    if frequent in one of the partitions. Each partition is scanned consecutively
    until frequency for an item set is established.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of whether or not one of these techniques is employed, the computational
    requirements are always going to be fairly substantial when it comes to the Apriori
    algorithm. As should now be clear, the essence of the algorithm, the computation
    of support, is not as complex as other models discussed in this text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 44: Executing the Apriori algorithm'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The execution of the Apriori algorithm is made easy with `mlxtend`. As a result,
    this exercise will focus on how to manipulate the outputted dataset and to interpret
    the results. You will recall that the cleaned and encoded transaction data was
    defined as `online_encoder_df`. Perform this exercise in the same notebook that
    all previous exercises were run as we will continue using the environment, data,
    and results already established in that notebook. (So, you should be using the
    notebook that contains the reduced dataset of 5,000 entries, not the full dataset
    as used in the activity.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the Apriori algorithm using `mlxtend` without changing any of the default
    parameter values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is an empty DataFrame. The default minimum support value is set to
    0.5, so since an empty DataFrame was returned, we know that all item sets have
    a support of less than 0.5\. Depending on the number of transactions and the diversity
    of available items, having no item set with a plus 0.5 support is not unusual.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Rerun the Apriori algorithm, but with the minimum support set to 0.01\. This
    minimum support value is the same as saying that when analyzing 5,000 transactions,
    we need an item set to appear 50 times to be considered frequent. As mentioned
    previously, the minimum support can be set to any value in the range [0,1]. There
    is no best minimum support value; the setting of this value is entirely subjective.
    Many businesses have their own specific thresholds for significance, but there
    is no industry standard or method for optimizing this value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.21: Basic output of the Apriori algorithm run using mlxtend'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.21: Basic output of the Apriori algorithm run using mlxtend'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that the item sets are designated numerically in the output, which makes
    the results hard to interpret.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Rerun the Apriori algorithm with the same minimum support as in *Step 2*, but
    this time set `use_colnames` to True. This will replace the numerical designations
    with the actual item names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.22: The output of the Apriori algorithm with the actual item names
    instead of numerical designations'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.22: The output of the Apriori algorithm with the actual item names
    instead of numerical designations'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This DataFrame contains every item set whose support value is greater than the
    specified minimum support value. That is, these item sets occur with sufficient
    frequency to potentially be meaningful and therefore actionable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add an additional column to the output of *Step 3* that contains the size of
    the item set, which will help with filtering and further analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_08_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.23: The Apriori algorithm output plus an additional column containing
    the lengths of the item sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Find the support of the item set containing ''`10 COLOUR SPACEBOY PEN`'':'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.24: The output DataFrame filtered down to a single item set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.24: The output DataFrame filtered down to a single item set'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This single row DataFrame gives us the support value for this specific item
    set that contains one item. The support value says that this specific item set
    appears in 1.5% of the transactions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Return all item sets of length 2 whose support is in the range [0.02, 0.021]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.25: The Apriori algorithm output DataFrame filtered by length and
    support'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.25: The Apriori algorithm output DataFrame filtered by length and
    support'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This DataFrame contains all the item sets (pairs of items bought together) whose
    support value is in the range specified at the start of the step. Each of these
    item sets appears in between 2.0% and 2.1% of transactions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that when filtering on `support`, it is wise to specify a range instead
    of a specific value since it is quite possible to pick a value for which there
    are no item sets. The preceding output has 18 item sets. Keep note of that and
    the particular items in the item sets because we will be running this same filter
    when we scale up to the full data and we will want to execute a comparison.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the support values. Note that this plot will have no support values less
    than 0.01 because that was the value used as the minimum support:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.26: Distribution of the support values returned by the Apriori algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.26: Distribution of the support values returned by the Apriori algorithm'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The maximum support value is approximately 0.14, which is approximately 700
    transactions. What might appear to be a small value may not be given the number
    of products available. Larger numbers of products tend to result in lower support
    values because the variability of item combinations increases.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you can think of more ways in which this data could be used and with
    a view to supporting retail businesses. We will generate even more useful information
    in the next section by using the Apriori algorithm results to generate association
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 19: Apriori on the Complete Online Retail Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you work for an online retailer. You are given all the transaction data
    from the last month and told to find all the item sets appearing in at least 1%
    of the transactions. Once the qualifying item sets are identified, you are subsequently
    told to identify the distribution of the support values. The distribution of support
    values will tell all interested parties whether groups of items exist that are
    purchased together with high probability as well as the mean of the support values.
    Let's collect all the information for the company leadership and strategists.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, you will run the Apriori algorithm on the full online retail
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This dataset is taken from [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson08/Activity18-Activity20).
    Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining, Journal
    of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208,
    2012.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you complete this activity in the same notebook as the previous
    activity (in other words, the notebook that uses the full dataset, not the notebook
    that uses the subset of 5,000 items that you're using for the exercises).
  prefs: []
  type: TYPE_NORMAL
- en: This will also provide you with an opportunity to compare the results with those
    generated using only 5,000 transactions. This is an interesting activity, as it
    provides some insight into the ways in which the data may change as more data
    is collected, as well as some insight into how support values change when the
    partitioning technique is employed. Note that what was done in the exercises is
    not a perfect representation of the partitioning technique because 5,000 was an
    arbitrary number of transactions to sample.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All the activities in this chapter need to be performed in the same notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the Apriori algorithm on the full data with reasonable parameter settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter the results down to the item set containing `10 COLOUR SPACEBOY PEN`.
    Compare the support value to that of *Exercise 44*, *Executing the Apriori algorithm*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add another column containing the item set length. Then, filter down to those
    item sets whose length is two and whose support is in the range [0.02, 0.021].
    Compare this to the result from *Exercise 44*, *Executing the Apriori algorithm*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the `support` values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 367.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this activity will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.27: Distribution of support values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.27: Distribution of support values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Association Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Association rule learning is a machine learning model that seeks to unearth
    the hidden patterns (in other words, relationships) in transaction data that describe
    the shopping habits of the customers of any retailer. The definition of an association
    rule was hinted at when the common probabilistic metrics were defined and explained
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the imaginary frequent item set {Milk, Bread}. Two association rules
    can be formed from that item set: Milk ![](img/C12626_Formula_08_19.png) Bread
    and Bread ![](img/C12626_Formula_08_19.png) Milk. For simplicity, the first item
    set in the association rule is referred to as the antecedent, while the second
    item set in the association rule is referred to as the consequent. Once the association
    rules have been identified, all the previously discussed metrics can be computed
    to evaluate the validity of the association rules determining whether or not the
    rules can be leveraged in the decision-making process.'
  prefs: []
  type: TYPE_NORMAL
- en: The establishment of an association rule is based on support and confidence.
    Support, as we discussed in the last section, identifies which item sets are frequent,
    while confidence measures the frequency of truthfulness for a particular rule.
    Confidence is typically referred to as the measure of interestingness, as it is
    the metric that determines whether an association should be formed. Thus, the
    establishment of an association rule is a two-step process. Identify frequent
    datasets and then evaluate the confidence of a candidate association rule and,
    if that confidence value exceeds some arbitrary threshold, the result is an association
    rule.
  prefs: []
  type: TYPE_NORMAL
- en: A major issue of association rule learning is the discovery of spurious associations,
    which are highly likely given the huge numbers of potential rules. Spurious associations
    are defined as associations that occur with surprising regularity in the data
    given that the association occurs entirely by chance. To clearly articulate the
    idea, assume we are in a situation where we have 100 candidate rules. If we run
    a statistical test for independence at the 0.05 significance level, we are still
    faced with a 5% chance that an association is found when no association exists.
    Let's further assume that all 100 candidate rules are not valid associations.
    Given the 5% chance, we should still expect to find 5 valid association rules.
    Now scale the imaginary candidate rule list up to millions or billions, so that
    that 5% amounts to an enormous number of associations. This problem is not unlike
    the issue of statistical significance and error faced by virtually every model.
    It is worth calling out that some techniques exist to combat the spurious association
    issue, but they are neither consistently incorporated in the frequently used association
    rule libraries nor in the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now apply our working knowledge of association rule learning to the online
    retail dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 45: Deriving Association Rules'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will derive association rules for the online retail dataset
    and explore the associated metrics. Ensure that you complete this exercise in
    the same notebook as the previous exercises (in other words, the notebook that
    uses the 5,000-item subset, not the full dataset from the activities).
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `mlxtend` library to derive association rules for the online retail
    dataset. Use confidence as the measure of interestingness, set the minimum threshold
    to 0.6, and return all the metrics, not just support. Count the number of returned
    association rules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.28: The first 7 rows of the association rules generated using only
    5,000 transactions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.28: The first 7 rows of the association rules generated using only
    5,000 transactions'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the number of associations as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 5,070 association rules were found.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The number of association rules may differ.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Try running another version of the model. Choose any minimum threshold and
    any measure of interestingness. Count and explore the returned rules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.29: The first 7 rows of the association rules'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.29: The first 7 rows of the association rules'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the number of associations as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The number of association rules found using the metric lift and the minimum
    threshold value of 50 is 26, which is significantly lower than in *Step 2*. We
    will see in the following that 50 is quite a high threshold value, so it is not
    surprising that we returned fewer association rules.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot confidence against support and identify specific trends in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.30: A plot of confidence against support'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.30: A plot of confidence against support'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that there are not any association rules with both extremely high confidence
    and extremely high support. This should hopefully make sense. If an item set has
    high support, the items are likely to appear with many other items, making the
    chances of high confidence very low.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Look at the distribution of confidence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.31: The distribution of confidence values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.31: The distribution of confidence values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, look at the distribution of lift:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.32: The distribution of lift values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.32: The distribution of lift values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As mentioned previously, this plot shows that 50 is a high threshold value in
    that there are not many points above that value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, look at the distribution of leverage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.33: The distribution of leverage values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_08_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.33: The distribution of leverage values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, look at the distribution of conviction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.34: The distribution of conviction values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_08_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.34: The distribution of conviction values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What is interesting about the four distributions is that spikes of varying sizes
    appear at the upper ends of the plots, implying that there are a few very strong
    association rules. The distribution of confidence tails off as the confidence
    values get larger, but, at the very end, around the highest values, the distribution
    jumps up a little. The lift distribution has the most obvious spike. The conviction
    distribution plot shows a small spike, perhaps more accurately described as a
    bump, around 50\. Lastly, the leverage distribution does not really show any spike
    in the higher values, but it does feature a long tail with some very high leverage
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Take some time to explore the association rules found by the model. Do the product
    pairings make sense to you? What happened to the number of association rules when
    you changed the model parameter values? Do you appreciate the impact that these
    rules would have when attempting to improve any retail business?
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 20: Finding the Association Rules on the Complete Online Retail Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's pick up the scenario set out in *Activity 19* *Apriori on the Complete
    Online Retail Dataset*. The company leadership comes back to you and says it is
    great that we know how frequently each item set occurs in the dataset, but which
    item sets can we act upon? Which item sets can we use to change the store layout
    or adjust pricing? To find these answers, we derive the full association rules.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, let's derive association rules from the complete online retail
    transaction dataset. Ensure that you complete this activity in the notebook that
    uses the full dataset (in other words, the notebook with the complete retail dataset,
    not the notebook from the exercises that uses the 5,000-item subset).
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will help us to perform the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Fit the association rule model on the full dataset. Use metric confidence and
    a minimum threshold of 0.6.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count the number of association rules. Is the number different to that found
    in *step 1* of Exercise 45, *Deriving Association Rules*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot confidence against support.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the distributions of confidence, lift, leverage, and conviction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 370.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By the end of this activity, you will have a plot of lift, leverage, and conviction.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Market basket analysis is used to analyze and extract insights from transaction
    or transaction-like data that can be used to help drive growth in many industries,
    most famously the retail industry. These decisions can include how to layout the
    retail space, what products to discount, and how to price products. One of the
    central pillars of market basket analysis is the establishment of association
    rules. Association rule learning is a machine learning approach to uncovering
    the associations between the products individuals purchase that are strong enough
    to be leveraged in business decisions. Association rule learning relies on the
    Apriori algorithm to find frequent item sets in a computationally efficient way.
    These models are atypical of machine learning models because no prediction is
    being done, the results cannot really be evaluated using any one metric, and the
    parameter values are selected not by grid search, but by domain requirements specific
    to the question of interest. That being said, the goal of pattern extraction that
    is at the heart of all machine learning models is most definitely present here.
    At the conclusion of this chapter, you should feel comfortable evaluating and
    interpreting the probabilistic metrics, be able to run and adjust the Apriori
    algorithm and association rule learning model using `mlxtend`, and know how these
    models are applied in business. Know that there is a decent chance the positioning
    and pricing of items in your neighborhood grocery store were chosen based on the
    past actions made by you and many other customers in that store!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we explore hotspot analysis using kernel density estimation,
    arguably one of the most frequently used algorithms in all of statistics and machine
    learning.
  prefs: []
  type: TYPE_NORMAL

- en: '*Chapter 4*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement logistic regression and explain how it can be used to classify data
    into specific groups or classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the K-nearest neighbors clustering algorithm for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use decision trees for data classification, including the ID3 algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the concept of entropy within data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how decision trees such as ID3 aim to reduce entropy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use decision trees for data classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter introduces classification problems, classification using linear
    and logistic regression, K-nearest neighbors classification, and decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapter, we began our supervised machine learning journey using
    regression techniques, predicting the continuous variable output given a set of
    input data. We will now turn to the other sub-type of machine learning problems
    that we previously described: classification problems. Recall that classification
    tasks aim to predict, given a set of input data, which one of a specified number
    of groups of classes data belongs to.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will extend the concepts learned in *Chapter 3*, *Regression
    Analysis*, and will apply them to a dataset labeled with classes, rather than
    continuous values, as output.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression as a Classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We covered linear regression in the context of predicting continuous variable
    output in the previous chapter, but it can also be used to predict the class that
    a set of data is a member of. Linear regression classifiers are not as powerful
    as other types of classifiers that we will cover in this chapter, but they are
    particularly useful in understanding the process of classification. Let's say
    we had a fictional dataset containing two separate groups, Xs and Os, as shown
    in *Figure 4.1*. We could construct a linear classifier by first using linear
    regression to fit the equation of a straight line to the dataset. For any value
    that lies above the line, the *X* class would be predicted, and for any value
    beneath the line, the *O* class would be predicted. Any dataset that can be separated
    by a straight line is known as linearly separable, which forms an important subset
    of data types in machine learning problems. While this may not be particularly
    helpful in the context of a linear regression-based classifier, it often is in
    the case of other classifiers, such as **support vector machines** (**SVM**),
    decision trees, and linear neural network-based classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Linear regression as a classifier](img/C12622_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Linear regression as a classifier'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 36: Linear Regression as a Classifier'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This exercise contains a contrived example of using linear regression as a classifier.
    In this exercise, we will use a completely fictional dataset, and test how linear
    regression fares as a classifier. The dataset is composed of manually selected
    *x* and *y* values for a scatterplot that are approximately divided into two groups.
    The dataset has been specifically designed for this exercise, to demonstrate how
    linear regression can be used as a classifier, and this is available in the accompanying
    code files for this book, as well as on GitHub, at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `linear_classifier.csv` dataset into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2: First five rows](img/C12622_04_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.2: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking through the dataset, each row contains a set of *x, y* coordinates,
    as well as the label corresponding to which class the data belongs to, either
    a cross (**x**) or a circle (**o**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Produce a scatterplot of the data with the marker for each point as the corresponding
    class label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following scatterplot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 Scatterplot of a linear classifier](img/C12622_04_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 4.3 Scatterplot of a linear classifier
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using the scikit-learn `LinearRegression` API from the previous chapter, fit
    a linear model to the *x*, *y* coordinates of the dataset and print out the linear
    equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4: Output of model fitting](img/C12622_04_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.4: Output of model fitting'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the fitted trendline over the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.5: Scatterplot with trendline](img/C12622_04_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.5: Scatterplot with trendline'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the fitted trendline, the classifier can then be applied. For each row
    in the dataset, determine whether the *x, y* point lies above or below the linear
    model (or trendline). If the point lies below the trendline, the model predicts
    the **o** class, if above the line, the **x** class is predicted. Include these
    values as a column of predicted labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.6: First five rows](img/C12622_04_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.6: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the points with the corresponding ground truth labels. For those points
    where the labels were correctly predicted, plot the corresponding class. For those
    incorrect predictions, plot a diamond:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.7: Scatterplot showing incorrect predictions](img/C12622_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Scatterplot showing incorrect predictions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that, in this plot, the linear classifier made two incorrect predictions
    in this completely fictional dataset, one at *x = 1*, another at *x = 3*.
  prefs: []
  type: TYPE_NORMAL
- en: But what if our dataset is not linearly separable and we cannot classify the
    data using a straight line model, which is very frequently the case. In this scenario,
    we turn to other classification methods, many of which use different models, but
    the process logically flows from our simplified linear classifier model.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **logistic** or **logit** model is one such non-linear model that has been
    effectively used for classification tasks in a number of different domains. In
    this section, we will use it to classify images of hand-written digits. In understanding
    the logistic model, we also take an important step in understanding the operation
    of a particularly powerful machine learning model, **artificial neural networks**.
    So, what exactly is the logistic model? Like the linear model, which is composed
    of a linear or straight-line function, the logistic model is composed of the standard
    logistic function, which, in mathematical terms, looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Logistic function](img/C12622_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Logistic function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practical terms, when trained, this function returns the probability of the
    input information belonging to a particular class or group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we would like to predict whether a single entry of data belongs to one
    of two groups. As in the previous example, in linear regression, this would equate
    to *y* being either zero or one, and *x* can take a value between ![](img/C12622_Formula_04_02.png)
    and ![](img/C12622_Formula_04_03.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: Equation for y](img/C12622_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Equation for y'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A range of zero to one and ![](img/C12622_Formula_04_02.png) to ![](img/C12622_Formula_04_03.png)
    are significantly different; to improve this, we will calculate the odds ratio,
    which will then vary from greater than zero to less than ![](img/C12622_Formula_04_03.png),
    which is a step in the right direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10: Odds ratio](img/C12622_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Odds ratio'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can use the mathematical relationships of the natural log to reduce this
    even further. As the odds ratio approaches zero, ![](img/C12622_Formula_04_04.png)ss
    approaches ![](img/C12622_Formula_04_02.png); similarly, as the odds ratio approaches
    one, ![](img/C12622_Formula_04_04.png) approaches ![](img/C12622_Formula_04_03.png).
    This is exactly what we want; that is, for the two classification options to be
    as far apart as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: Natural log of classified points](img/C12622_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Natural log of classified points'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With a little bit of equation re-arranging we get the logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: Logistic function](img/C12622_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Logistic function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice the exponents of *e*, that is, ![](img/C12622_Formula_04_05.png), and
    that this relationship is a linear function with two training parameters or *weights*,
    ![](img/C12622_Formula_04_06.png) and ![](img/C12622_Formula_04_02.png), as well
    as the input feature, *x*. If we were to plot the logistic function over the range
    *(-6, 6)*, we would get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13: Logistic function curve](img/C12622_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Logistic function curve'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Examining *Figure 4.13*, we can see some features that are important for a classification
    task. The first thing to note is that, if we look at the probability values on
    the *y* axis at the extremes of the function, the values are almost at zero when
    *x = -6* and at one when *x = 6*. While it looks like the values are in fact zero
    and one, this is not exactly the case. The logistic function approaches zero and
    one at these extremes and will only equal zero and one when *x* is at a positive
    or negative infinity. In practical terms, what this means is that the logistic
    function will never return a probability of one or greater or less than or equal
    to zero, which is perfect for a classification task. We can never have a probability
    of greater than one, as, by definition, a probability of one is a certainty of
    an event occurring. Likewise, we cannot have a probability of less than zero,
    as, by definition, a probability of zero is a certainty of the event not occurring.
    The fact that the logistic function approaches but never equals one or zero means
    that there is always some uncertainty in the outcome or the classification.
  prefs: []
  type: TYPE_NORMAL
- en: The final feature to notice about the logistic function is that at *x = 0*,
    the probability is 0.5, which, if we were to get this result, would indicate that
    the model is equally uncertain about the outcome of the corresponding class; that
    is, it really has no idea. Typically, this is the default position at the start
    of training, and, as the model is exposed to training data, it becomes more confident
    in its decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is very important to correctly understand and interpret the probability information
    provided by classification models such as linear regression. Consider this probability
    score as the chance of the input information belonging to a particular class given
    the variability in the information provided by the training data. One common mistake
    is to use this probability score as an objective measure of whether the model
    can be trusted about its prediction; unfortunately, this isn't necessarily the
    case. *A model can provide a probability of 99.99% that some data belongs to a
    particular class and still be 99.99% wrong*.
  prefs: []
  type: TYPE_NORMAL
- en: What we do use the probability value for is selecting the predicted class by
    the classifier. Say we had a model that was to predict whether some set of data
    belonged to class A or class B. If the logistic model returned a probability of
    0.7 for class A, then we would return class A as the predicted class for the model.
    If the probability was only 0.2, the predicted class for the model would be class
    B.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 37: Logistic Regression as a Classifier – Two-Class Classifier'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this exercise, we will be using a sample of the famous MNIST dataset (available
    at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/) or on
    GitHub at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python)),
    which is a sequence of images of handwritten postcode digits, zero through nine,
    with corresponding labels. The MNIST dataset is comprised of 60,000 training samples
    and 10,000 test samples, where each sample is a grayscale image with a size of
    28 x 28 pixels. In this exercise, we will use logistic regression to build a classifier.
    The first classifier we will build is a two-class classifier, where we will determine
    whether the image is a handwritten zero or a one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we will need to import a few dependencies. Execute the following
    import statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will also need to download the MNIST datasets. You will only need to do
    this once, so after this step, feel free to comment out or remove these cells.
    Download the image data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the corresponding labels for the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once all the files have been successfully downloaded, check out the files in
    the local directory using the following command for Windows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14: Files in directory](img/C12622_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.14: Files in directory'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: For Linux and macOS, check out the files in the local directory using the `!ls
    *.gz` command.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the downloaded data. Don''t worry too much about the exact details of
    reading the data, as these are specific to the MNIST dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As always, having a thorough understanding of the data is key, so create an
    image plot of the first 10 images in the training sample. Notice the grayscale
    images and that the corresponding labels are the digits zero through nine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.15: Training images](img/C12622_04_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.15: Training images'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As the initial classifier is aiming to classify either images of zeros or images
    of ones, we must first select these samples from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Visualize one sample from the zero selection and another from the handwritten
    one digits to ensure we have correctly allocated the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s the code for zero:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.16: First handwritten image](img/C12622_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.16: First handwritten image'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here''s the code for one:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.17: Second handwritten image](img/C12622_04_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.17: Second handwritten image'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We are almost at the stage where we can start building the model, however,
    as each sample is an image and has data in a matrix format, we must first re-arrange
    each of the images. The model needs the images to be provided in vector form,
    that is, all the information for each image is stored in one row. Do that as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can build and fit the logistic regression model with the selected images
    and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.18: Logistic Regression Model](img/C12622_04_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.18: Logistic Regression Model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note how the scikit-learn API calls for logistic regression are consistent with
    that of linear regression. There is an additional argument, `solver`, which specifies
    the type of optimization process to be used. We have provided this argument here
    with the default value to suppress a future warning in this version of scikit-learn
    that requires `solver` to be specified. The specifics of the `solver` argument
    are out of scope for this chapter and has only been included to suppress the warning
    message.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the performance of this model against the corresponding training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.19: Model score](img/C12622_04_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.19: Model score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example, the model was able to predict the training labels with 100%
    accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the first two predicted labels for the training data using the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20: The first two labels the model predicted](img/C12622_04_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.20: The first two labels the model predicted'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'How is the logistic regression model making the classification decisions? Look
    at some of the probabilities produced by the model for the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.21: Array of probabilities](img/C12622_04_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.21: Array of probabilities'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that, for each prediction made, there are two probability values.
    The first corresponding to the probability of the class being zero, the second
    the probability of the class being one, both of which add up to one. We can see
    that, in the first example, the prediction probability is 0.9999999 for class
    zero and thus the prediction is class zero. Similarly, the inverse is true for
    the second example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the performance of the model against the test set to check its performance
    against data that it has not seen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.22: Model score](img/C12622_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Model score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Refer to *Chapter 6*, *Model Evaluation*, for better methods of objectively
    measuring the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: We can see here that logistic regression is a powerful classifier that is able
    to distinguish between hand-written samples of zero and one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 38: Logistic Regression – Multiclass Classifier'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous exercise, we examined using logistic regression to classify
    between one of two groups. Logistic regression, however, can also be used to classify
    a set of input information to *k* different groups and it is this multiclass classifier
    we will be investigating in this exercise. The process for loading the MNIST training
    and test data is identical to the previous exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the training/test images and the corresponding labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Given that the training data is so large, we will select a subset of the overall
    data to reduce the training time as well as the system resources required for
    the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, in this example, we are using data from all 10 classes, not just
    classes zero and one, so we are making this example a multiclass classification
    problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Again, reshape the input data in vector form for later use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.23: Reshaping the data](img/C12622_04_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.23: Reshaping the data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The next cell is intentionally commented out. Leave this code commented out
    for the moment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Construct the logistic model. There are a few extra arguments, as follows:
    the `lbfgs` value for `solver` is geared up for multiclass problems, with additional
    `max_iter` iterations required for converging on a solution. The `multi_class`
    argument is set to `multinomial` to calculate the loss over the entire probability
    distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.24: Logistic regression model](img/C12622_04_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.24: Logistic regression model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Refer to the documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    for more information on the arguments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Determine the accuracy score against the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.25: Model score](img/C12622_04_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.25: Model score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Determine the first two predictions for the training set and plot the images
    with the corresponding predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.26: Model score predicted values](img/C12622_04_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.26: Model score predicted values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Show the images for the first two samples of the training set to see whether
    we are correct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.27: Images plotted using prediction](img/C12622_04_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.27: Images plotted using prediction'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Again, print out the probability scores provided by the model for the first
    sample of the training set. Confirm that there are 10 different values for each
    of the 10 classes in the set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.28: Array of predicted values](img/C12622_04_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.28: Array of predicted values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that, in the probability array of the first sample, the fifth (index
    four) sample is the highest probability, thus indicating a prediction of four.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the accuracy of the model against the test set. This will provide a
    reasonable estimate of the model''s *in the wild* performance, as it has never
    seen the data in the test set. It is expected that the accuracy rate of the test
    set will be slightly lower than the training set, given that the model has not
    been exposed to this data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.29: Model score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12622_04_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.29: Model score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: When checked against the test set, the model produced accuracy of 87.8%. When
    applying a test set, a performance drop is expected, as this is the very first
    time the model has seen these samples; while, during training, the training set
    was repeatedly shown to the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the cell with the commented-out code, as shown in *step four*. Uncomment
    the code in this cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This cell simply scales all the image values to between zero and one. Grayscale
    images are comprised of pixels with values between and including 0 – 255, where
    0 is black and 255 is white.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click **Restart & Run-All** to rerun the entire notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Find the training set error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following score:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.30: Training set model score](img/C12622_04_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.30: Training set model score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Find the test set error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following score:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.31: Test set model score](img/C12622_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.31: Test set model score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What effect did normalizing the images have on the overall performance of the
    system? The training error is worse! We went from 100% accuracy in the training
    set to 98.6%. Yes, there was a reduction in the performance of the training set,
    but an increase in the test set from 87.8% accuracy to 90.02%. The test set performance
    is of more interest, as the model has not seen this data before, and so it is
    a better representation of the performance than we could expect once the model
    is in the field. So, why do we get a better result? Again, review *Figure 4.13*,
    and notice the shape of the curve as it approaches and . The curve saturates or
    flattens at almost zero and almost one. So, if we use an image (or *x* values)
    of between 0 and 255, the class probability defined by the logistic function is
    well within this flat region of the curve. Predictions within this region are
    highly unlikely to change very much, as they will need to have very large changes
    in *x* values for any meaningful change in *y*. Scaling the images to be between
    zero and one initially puts the predictions closer to *p(x) = 0.5*, and so, changes
    in *x* can have a bigger impact on the value for *y*. This allows for more sensitive
    predictions and results in getting a couple of predictions in the training set
    wrong, but more in the test set right. It is recommended, for your logistic regression
    models, that you scale the input values to be between either zero and one or negative-one
    and one prior to training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function will scale values of a NumPy array between negative-one
    and one with a mean of approximately zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Activity 11: Linear Regression Classifier – Two-Class Classifier'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will build a two-class linear regression-based classifier
    using the MNIST dataset to classify between two digits: zero and one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load the MNIST data into memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize a sample of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a linear classifier model to classify the digits zero and one. The
    model we are going to create is to determine whether the samples are either the
    digits zero or one. To do this, we first need to select only those samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the selected information with images of one sample of zero and one
    sample of one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to provide the image information to the model, we must first flatten
    the data out so that each image is 1 x 784 pixels in shape.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's construct the model; use the `LinearRegression` API and call the `fit`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the R2 score against the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the label predictions for each of the training samples, using a threshold
    of 0.5\. Values greater than 0.5 classify as one; values less than or equal to
    0.5 classify as zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the classification accuracy of the predicted training values versus
    the ground truth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the performance against the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 352.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 12: Iris Classification Using Logistic Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will be using the well-known Iris Species dataset (available
    at [https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)
    or on GitHub at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python))
    created in 1936 by botanist, Ronald Fisher. The dataset contains sepal and petal
    length and width measurements for three different iris flower species: iris setosa,
    iris versicolor, and iris virginica. In this activity, we will use the measurements
    provided in the dataset to classify the different flower species.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages. For this activity, we will require the pandas
    package for loading the data, the Matplotlib package for plotting, and scikit-learn
    for creating the logistic regression model. Import all the required packages and
    relevant modules for these tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load the iris dataset using pandas and examine the first five rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is feature engineering. We need to select the most appropriate
    features that will provide the most powerful classification model. Plot a number
    of different features versus the allocated species classifications, for example,
    sepal length versus petal length and species. Visually inspect the plots and look
    for any patterns that could indicate separation between each of the species.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the features by writing the column names in the following list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Before we can construct the model, we must first convert the `species` values
    into labels that can be used within the model. Replace the `Iris-setosa` species
    string with the value `0`, the `Iris-versicolor` species string with the value
    `1`, and the `Iris-virginica` species string with the value `2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the model using the `selected_features` and the assigned `species` labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the accuracy of the model against the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct another model using your second choice `selected_features` and compare
    the performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct another model using all available information and compare the performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 357.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Classification Using K-Nearest Neighbors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we are comfortable with creating multiclass classifiers using logistic
    regression and are getting reasonable performance with these models, we will turn
    our attention to another type of classifier: the **K-nearest neighbors** (**K-NN**)
    clustering method of classification. This is a handy method, as it can be used
    in both supervised classification problems as well as in unsupervised problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.32: Visual representation of K-NN](img/C12622_04_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.32: Visual representation of K-NN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The solid circle approximately in the center is the test point requiring classification,
    while the inner circle shows the classification process where *K=3* and the outer
    circle where *K=5*.
  prefs: []
  type: TYPE_NORMAL
- en: K-NN is one of the simplest "learning" algorithms available for data classification.
    The use of learning in quotation marks is explicit, as K-NN doesn't really learn
    from the data and encode these learnings in parameters or weights like other methods,
    such as logistic regression. K-NN uses instance-based or lazy learning in that
    it simply stores or memorizes all the training samples and the corresponding classes.
    It derives its name, K-nearest neighbors, from the fact that, when a test sample
    is provided to the algorithm for class prediction, it uses a majority vote of
    the *K*-nearest points to determine the corresponding class. If we look at *Figure
    4.35*, the solid circle is the test point to be classified. If we use *K=3*, the
    nearest three points lie within the inner dotted circle, and, in this case, the
    classification would be a hollow circle. If, however we were to take *K=5*, the
    nearest five points lie within the outer dotted circle and the classification
    would be a cross (three crosses to two hollow circles).
  prefs: []
  type: TYPE_NORMAL
- en: 'This figure highlights a few characteristics of K-NN classification that should
    be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The selection of *K* is quite important. In this simple example, switching
    *K* from three to five flipped the class prediction due to the proximity of both
    classes. As the final classification is taken by a majority vote, it is often
    useful to use odd numbers of *K* to ensure that there is a winner in the voting
    process. If an even value of *K* is selected, and a tie in the vote occurs, then
    there are a number of different methods available for breaking the tie, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing *K* by one until the tie is broken
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Selecting the class on the basis of the smallest Euclidean distance to the nearest
    point
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Applying a weighting function to bias the test point toward those neighbors
    which are closer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: K-NN models have the ability to form extremely complex non-linear boundaries,
    which can be advantageous in classifying images or datasets with interesting boundaries.
    Considering that, in *Figure 4.35*, the test point changes from a hollow circle
    classification to a cross with an increase in *K*, we can see here that a complex
    boundary could be formed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-NN models can be highly sensitive to local features in the data, given that
    the classification process is only really dependent on the nearby points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As K-NN models memorize all the training information to make predictions, they
    can struggle with generalizing to new, unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is another variant of K-NN, which, rather than specifying the number of
    nearest neighbors, specifies the size of the radius around the test point at which
    to look. This method, known as the **radius neighbors classification**, will not
    be considered in this chapter, but in understanding K-NN, you will also develop
    an understanding of radius neighbors classification and how to use the model through
    scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our explanation of K-NN classification and the next exercise examines modeling
    data with two features or two dimensions as it enables simple visualization and
    a greater understanding of the K-NN modeling process. K-NN classification, like
    linear and logistic regression, is not limited to use in only two dimensions;
    it can be applied in *N* dimensional datasets as well. This will be examined in
    further detail in *Activity 13: K-NN Multiclass Classifier*, wherein we''ll classify
    MNIST using K-NN. Remember, just because there are too many dimensions to plot,
    it doesn''t mean it cannot be classified with *N* dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 39: K-NN Classification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To allow visualization of the K-NN process, we will turn our attention in this
    exercise to a different dataset, the well-known Iris dataset. This dataset is
    provided as part of the accompanying code files for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we need to import pandas, Matplotlib, and the `KNeighborsClassifier`
    of scikit-learn. We will use the shorthand notation `KNN` for quick access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Iris dataset and examine the first five rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.33: First five rows](img/C12622_04_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.33: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At this stage, we need to choose the most appropriate features from the dataset
    for use with the classifier. We could simply select all four (sepal and petal,
    length and width), however, as this exercise is designed to allow visualization
    of the K-NN process, we will only select sepal length and petal width. Construct
    a scatterplot for sepal length versus petal width for each of the classes in the
    dataset with the corresponding species:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.34: Scatterplot of iris data](img/C12622_04_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.34: Scatterplot of iris data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Looking at this graph, we can see that the species are reasonably well separated
    by the petal width, with the greatest similarity between the Iris versicolor and
    the Iris virginica species. There are a couple of Iris virginica species points
    that lie within the Iris versicolor cluster. As a test point for later use, select
    one of these points at the boundary—sample 134:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.35: Boundary sample](img/C12622_04_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.35: Boundary sample'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the data again, highlighting the location of the test sample/point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.36: Scatterplot with the test sample](img/C12622_04_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.36: Scatterplot with the test sample'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Construct a K-NN classifier model with *K = 3* and fit it to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.37: K Neighbor classifier](img/C12622_04_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.37: K Neighbor classifier'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Check the performance of the model against the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will show the performance score:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12622_04_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.38: Model score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As the accuracy is over 97% on the test set, the next step will be to check
    the test sample.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Predict the species of the test sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.39: Predicted test sample](img/C12622_04_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.39: Predicted test sample'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Verify it with the actual species of the sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.40: Species of the sample](img/C12622_04_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.40: Species of the sample'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This prediction is clearly incorrect but is not surprising given the location
    of the test point on the boundary. What would be helpful would be to know where
    the boundary for the model is drawn. We will draw this boundary in the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 40: Visualizing K-NN Boundaries'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To visualize the decision boundaries produced by the K-NN classifier, we need
    to sweep over the prediction space, that is, the minimum and maximum values for
    petal width and sepal length, and determine the classifications made by the model
    at those points. Once we have this sweep, we can then plot the classification
    decisions made by the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the relevant packages. We will also need NumPy for this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Iris dataset into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.41: First five rows](img/C12622_04_41.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.41: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While we could use the species strings to create the model in the previous
    exercise, in plotting the decision boundaries, it would be more useful to map
    the species to separate integer values. To do this, create a list of the labels
    for later reference and iterate through this list, replacing the existing label
    with the corresponding index in the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.42: First five rows](img/C12622_04_42.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.42: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice the use of the `enumerate` function in the `for` loop definition. When
    iterating through the `for` loop, the `enumerate` function provides the index
    of the value in the list as well as the value itself through each iteration. We
    assign the index of the value to the `idx` variable and the value to `label`.
    Using `enumerate` in this way provides an easy way to replace the species strings
    with a unique integer label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Construct a K-NN classification model, again using three nearest neighbors
    and fit to the sepal length and petal width with the newly labeled species data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.43: K Neighbors classifier](img/C12622_04_43.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.43: K Neighbors classifier'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To visualize our decision boundaries, we need to create a mesh or range of
    predictions across the information space, that is, all values of sepal length
    and petal width. Starting with 1mm less than the minimum for both the petal width
    and sepal length, and finishing at 1mm more than the maximum for petal width and
    sepal length, use the `arange` function of NumPy to create a range of values between
    these limits in increments of 0.1 (spacing):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the NumPy `meshgrid` function to combine the two ranges into a grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check out `xx`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.44: Array of meshgrid xx values](img/C12622_04_44.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.44: Array of meshgrid xx values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Check out `yy`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.45: Array of meshgrid yy values](img/C12622_04_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.45: Array of meshgrid yy values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Concatenate the mesh into a single NumPy array using `np.c_`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.46: Array of predicted values](img/C12622_04_46.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.46: Array of predicted values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: While this function call looks a little mysterious, it simply concatenates the
    two separate arrays together (refer to [https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html))
    and is shorthand for concatenate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Produce the class predictions for the mesh:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.47: Array of predicted y values](img/C12622_04_47.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.47: Array of predicted y values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To consistently visualize the boundaries, we will need two sets of consistent
    colors; a lighter set of colors for the decision boundaries and a darker set of
    colors for the points of the training set themselves. Create two `ListedColormaps`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To highlight the decision boundaries, first plot the training data according
    to the iris species, using the `cmap_bold` color scheme and different markers
    for each of the different species:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12622_04_48.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.48: Scatterplot with highlighted decision boundaries'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using the prediction mesh made previously, plot the decision boundaries in
    addition to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.49: The decision boundaries](img/C12622_04_49.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.49: The decision boundaries'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 4.49* has been modified for grayscale print and has additional labels,
    indicating the prediction boundaries of the classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 13: K-NN Multiclass Classifier'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will use the K-NN model to classify the MNIST dataset into
    10 different digit-based classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load the MNIST data into memory. First the training images, then the training
    labels, then the test images, and finally, the test labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize a sample of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a K-NN classifier, with three nearest neighbors to classify the MNIST
    dataset. Again, to save processing power, randomly sample 5,000 images for use
    in training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to provide the image information to the model, we must first flatten
    the data out so that each image is 1 x 784 pixels in shape.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the three-neighbor KNN model and fit the data to the model. Note that,
    in this activity, we are providing 784 features or dimensions to the model, not
    simply 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the score against the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the first two predictions for the model against the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the performance against the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 360.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Classification Using Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final classification method that we will be examining in this chapter is
    **decision trees**, which have found particular use in applications such as natural
    language processing. There are a number of different machine learning algorithms
    that fall within the overall umbrella of decision trees, such as ID3, CART, and
    the powerful random forest classifiers (covered in *Chapter 5*, *Ensemble Modeling*).
    In this chapter, we will investigate the use of the ID3 method in classifying
    categorical data, and we will use the scikit-learn CART implementation as another
    means of classifying the Iris dataset. So, what exactly are decision trees?
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, decision trees are a learning algorithm that apply a
    sequential series of decisions based on input information to make the final classification.
    Recalling your childhood biology class, you may have used a process similar to
    decision trees in the classification of different types of animals via dichotomous
    keys. Just like the example dichotomous key shown in *Figure 4.50*, decision trees
    aim to classify information following the result of a number of decision or question
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.50: Animal classification using dichotomous key](img/C12622_04_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.50: Animal classification using dichotomous key'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Depending upon the decision tree algorithm being used, the implementation of
    the decision steps may vary slightly, but we will be considering the implementation
    of the ID3 algorithm specifically. The **Iterative Dichotomiser 3** (**ID3**)
    algorithm aims to classify the data on the basis of each decision providing the
    largest information gain. To further understand this design, we also need to understand
    two additional concepts: entropy and information gain.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ID3 algorithm was first proposed by the Australian researcher Ross Quinlan
    in 1985 ([https://doi.org/10.1007/BF00116251](https://doi.org/10.1007/BF00116251)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy**: In the context of information theory, entropy is the average rate
    at which information is provided by a random source of data. Mathematically speaking,
    this entropy is defined as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.51: Entropy equation](img/C12622_04_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.51: Entropy equation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this scenario, when the random source of data produces a low probability
    value, the event carries more information, as it is unexpected compared to when
    a high-probability event occurs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Information gain**: Somewhat related to entropy is the amount of information
    gained about a random variable by observing another random variable. Given a dataset,
    *S*, and an attribute to observe, *a*, the information gain is defined mathematically
    as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.52: Information gain equation](img/C12622_04_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.52: Information gain equation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The information gain of dataset *S*, for attribute *a*, is equal to the entropy
    of *S* minus the entropy of *S* conditional on attribute *a*, or the entropy of
    dataset *S* minus the proportion of elements in *t* to the elements in *S*, times
    the entropy of *t*, where *t* is one of the categories in attribute *a*.
  prefs: []
  type: TYPE_NORMAL
- en: If at first you find the mathematics here a little daunting, don't worry, for
    it is far simpler than it seems. To clarify the ID3 process, we will walk through
    the process using the same dataset as was provided by Quinlan in the original
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 41: ID3 Classification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the original paper, Quinlan provided a small dataset of 10 weather observation
    samples labeled with either **P** to indicate that the weather was suitable for,
    say, a Saturday morning game of cricket, or baseball for our North American friends.
    If the weather was not suitable for a game, label **N** was provided. The example
    dataset described in the paper will be created in the exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Jupyter notebook, create a pandas DataFrame of the following training
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.53: Pandas DataFrame](img/C12622_04_53.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.53: Pandas DataFrame'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the original paper, the ID3 algorithm starts by taking a small sample of
    the training set at random and fitting the tree to this window. This can be a
    useful method for large datasets, but given that ours is quite small, we will
    simply start with the entire training set. The first step is to calculate the
    entropy for the `P` and `N`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.54: Entropy decision](img/C12622_04_54.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.54: Entropy decision'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We will need to repeat this calculation, so wrap it into a function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to calculate which attribute provides the highest information
    gain out of `groupby` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.56: Probabilities entropies and gains](img/C12622_04_56.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.56: Probabilities entropies and gains'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The final gain equation for **Outlook** can be re-written as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.57: Equation of information gain](img/C12622_04_57.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.57: Equation of information gain'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We need to repeat this process quite a few times, so wrap it in a function
    for ease of use later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat this process for each of the other columns to compute the corresponding
    information gain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.58: Gains](img/C12622_04_58.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.58: Gains'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This information provides the first decision of the tree. We want to split
    on the maximum information gain, thus we split on **Outlook**. Look at the data
    splitting on **Outlook**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.59: Information gain](img/C12622_04_59.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.59: Information gain'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice that all the overcast records have a decision of **P**. This provides
    our first terminating leaf of the decision tree. If it is overcast, we are going
    to play, while if it is rainy or sunny, there is a chance we will not play. The
    decision tree so far can be represented as in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.60: Decision tree](img/C12622_04_60.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.60: Decision tree'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: This figure was created manually for reference and is not contained in or obtained
    from the accompanying source code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now repeat this process, splitting by information gain until all the data
    is allocated and all branches of the tree terminate. First, remove the overcast
    samples, as they no longer provide any additional information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.61: Data after removing the overcast samples](img/C12622_04_61.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.61: Data after removing the overcast samples'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we will turn our attention to the sunny samples and will rerun the gain
    calculations to determine the best way to split the sunny information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Recompute the entropy for the sunny samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.62: Entropy decision](img/C12622_04_62.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.62: Entropy decision'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Run the gain calculations for the sunny samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.63: Gains](img/C12622_04_63.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.63: Gains'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Again, we select the largest gain, which is **Humidity**. Group the data by
    **Humidity**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.64: After grouping data according to humidity](img/C12622_04_64.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.64: After grouping data according to humidity'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can see here that we have two terminating leaves in that when the **Humidity**
    is high there is a decision not to play, and, vice versa, when the **Humidity**
    is normal, there is the decision to play. So, updating our representation of the
    decision tree, we have:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.65: Decision tree with two values](img/C12622_04_65.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.65: Decision tree with two values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, the last set of data that requires classification is the rainy outlook
    data. Extract only the **rain** data and rerun the entropy calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.66: Entropy decision](img/C12622_04_66.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.66: Entropy decision'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Repeat the gain calculation with the **rain** subset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.67: Gains](img/C12622_04_67.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.67: Gains'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Again, splitting on the attribute with the largest gain value requires splitting
    on the **Windy** values. So, group the remaining information by **Windy**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.68: Data grouped according to Windy](img/C12622_04_68.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.68: Data grouped according to Windy'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, we have all the terminating leaves required to complete the tree,
    as splitting on **Windy** provides two sets, all of which indicate either play
    (**P**) or no-play (**N**) values. Our complete decision tree is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.69: Final decision tree](img/C12622_04_69.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.69: Final decision tree'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can see here that decision trees, very much like K-NN models, consume the
    entire training set to construct the model. So, how do we make predictions with
    unseen information? Simply follow the tree. Look at the decision being made at
    each node and apply the data from the unseen sample. The prediction will then
    end up being the label specified at the terminating leaf. Let''s say we had a
    weather forecast for the upcoming Saturday and we wanted to predict whether we
    were going to play or not. The weather forecast is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.70: Weather forecast for upcoming Saturday](img/C12622_04_70.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.70: Weather forecast for upcoming Saturday'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The decision tree for this would be as follows (the dashed circles indicate
    selected leaves in the tree):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.71: Making a new prediction using a decision tree](img/C12622_04_71.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.71: Making a new prediction using a decision tree'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, hopefully, you have a reasonable understanding of the underlying concept
    of decision trees and the process of making sequential decisions. We have covered
    one of the first decision tree methodologies in this exercise, but there are certainly
    more available, and many of the more modern methods, such as random forests, do
    not suffer as much from overfitting (see *Chapter 5*, *Ensemble Modeling*, for
    more information) as ID3\. With the principles of decision trees in our toolkit,
    we will now look at applying a more complicated model using the functionality
    provided in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn decision tree methods implement the **CART** (**Classification
    and Regression Tree**) method, which provides the ability to use decision trees
    in both classification and regression problems. CART differs from ID3 in that
    the decisions are made by comparing the values of features against a calculated
    value, for example, for the Iris dataset, *is the petal width less than x mm*?
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 42: Iris Classification Using a CART Decision Tree'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will classify the Iris data using scikit-learn''s decision
    tree classifier, which can be used in both classification and regression problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Iris dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.72: First five rows](img/C12622_04_72.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.72: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Take a random sample of 10 rows to use for testing. Decision trees can overfit
    the training data, so this will provide an independent measure of the accuracy
    of the tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data and check the corresponding accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.73: Output of the model score](img/C12622_04_73.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.73: Output of the model score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Our model achieves 100% accuracy on the training set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the performance against the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.74: Output of the model score using df_test](img/C12622_04_74.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.74: Output of the model score using df_test'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One of the great things about decision trees is that we can visually represent
    the model and see exactly what is going on. Install the required dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the graphing package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.75: Decisions of the CART decision tree](img/C12622_04_75.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.75: Decisions of the CART decision tree'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This figure illustrates the decisions of the CART decision tree in the scikit-learn
    model. The first line of the node is the decision that is made at each step. The
    first node *X[2] <= 2.45* indicates that the training data is split on column
    two (petal length) on the basis of being less than or equal to 2.45\. Those samples
    with a petal length of less than 2.45 (of which there are 46) are all of the iris
    setosa class, and, as such, all of a `gini` (a metric similar to information gain)
    of zero. If the petal length is greater than 2.45, the next decision is whether
    the petal width (column three) is less than or equal to 1.75mm. This decision/branching
    process continues until the tree has been exhausted and all terminating leaves
    have been constructed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We covered a number of powerful and extremely useful classification models in
    this chapter, starting with the use of linear regression as a classifier, then
    we observed a significant performance increase through the use of the logistic
    regression classifier. We then moved on to **memorizing** models, such as K-NN,
    which, while simple to fit, was able to form complex non-linear boundaries in
    the classification process, even with images as input information into the model.
    We then finished our introduction to classification problems, looking at decision
    trees and the ID3 algorithm. We saw how decision trees, like K-NN models, memorize
    the training data using rules and decision gates to make predictions with quite
    a high degree of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be extending what we have learned in this chapter.
    It will cover ensemble techniques, including boosting and the very effective random
    forest method.
  prefs: []
  type: TYPE_NORMAL

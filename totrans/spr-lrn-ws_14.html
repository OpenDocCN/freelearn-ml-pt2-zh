<html><head></head><body>
		<br/>
		<p class="style0">6. Ensemble Modeling</p>
		<div style="page-break-before: always;"/>
	

		<br/>
		<h4 class="style0">Activity 6.01: Stacking with Standalone and Ensemble Algorithms</h4>
		<br/>
		<p class="style0">Solution</p>
		<br/>
		<p class="style0">Import the relevant libraries:</p>
		<br/>
		<p class="style0">import pandas as pd</p>
		<br/>
		<p class="style0">import numpy as np</p>
		<br/>
		<p class="style0">import seaborn as sns</p>
		<br/>
		<p class="style0">%matplotlib inline</p>
		<br/>
		<p class="style0">import matplotlib.pyplot as plt</p>
		<br/>
		<p class="style0">from sklearn.model_selection import train_test_split</p>
		<br/>
		<p class="style0">from sklearn.metrics import mean_absolute_error</p>
		<br/>
		<p class="style0">from sklearn.model_selection import KFold</p>
		<br/>
		<p class="style0">from sklearn.linear_model import LinearRegression</p>
		<br/>
		<p class="style0">from sklearn.tree import DecisionTreeRegressor</p>
		<br/>
		<p class="style0">from sklearn.neighbors import KNeighborsRegressor</p>
		<br/>
		<p class="style0">from sklearn.ensemble import GradientBoostingRegressor, \</p>
		<br/>
		<p class="style0">RandomForestRegressor</p>
		<br/>
		<p class="style0">Read the data:</p>
		<br/>
		<p class="style0">data = pd.read_csv('boston_house_prices.csv')</p>
		<br/>
		<p class="style0">data.head()</p>
		<br/>
		<h4 class="style2">Note</h4>
		<br/>
		<p class="style2">The preceding code snippet assumes that the dataset is presented in the same folder as that of the exercise notebook. However, if your dataset is present in the Datasets folder, you need to use the following code: data = pd.read_csv('../Datasets/boston_house_prices.csv')</p>
		<br/>
		<p class="style0">You will get the following output:</p>
		<br style="line-height: 1,4"/>
		<div>
			<img src="../Images/image-J3QAVKGH.jpg" alt="Figure 6.15: Top rows of the Boston housing dataset&#13;&#10;" height="100%"/>
		</div>
		<br/>
		<p class="style0" style="text-align: center">Figure 6.15: Top rows of the Boston housing dataset</p>
		<br/>
		<p class="style0">Preprocess the dataset to remove null values to prepare the data for modeling:</p>
		<br/>
		<p class="style0"># check how many columns have less than 10 % null data</p>
		<br/>
		<p class="style0">perc_missing = data.isnull().mean()*100</p>
		<br/>
		<p class="style0">cols = perc_missing[perc_missing &lt; 10].index.tolist()</p>
		<br/>
		<p class="style0">cols</p>
		<br/>
		<p class="style0">You will get the following output:</p>
		<br style="line-height: 1,4"/>
		<div>
			<img src="../Images/image-ZQI9Z8VR.jpg" alt="Figure 6.16: Number of columns&#13;&#10;" height="100%"/>
		</div>
		<br/>
		<p class="style0" style="text-align: center">Figure 6.16: Number of columns</p>
		<br/>
		<p class="style0">And then fill in the missing values, if any:</p>
		<br/>
		<p class="style0">data_final = data.fillna(-1)</p>
		<br/>
		<p class="style0">Divide the dataset into train and validation DataFrames:</p>
		<br/>
		<p class="style0">train, val = train, val = train_test_split(data_final, \</p>
		<br/>
		<p class="style0">                                           test_size=0.2, \</p>
		<br/>
		<p class="style0">                                           random_state=11)</p>
		<br/>
		<p class="style0">x_train = train.drop(columns=['PRICE'])</p>
		<br/>
		<p class="style0">y_train = train['PRICE'].values</p>
		<br/>
		<p class="style0">x_val = val.drop(columns=['PRICE'])</p>
		<br/>
		<p class="style0">y_val = val['PRICE'].values</p>
		<br/>
		<p class="style0">Initialize dictionaries in which to store the train and validation MAE values:</p>
		<br/>
		<p class="style0">train_mae_values, val_mae_values = {}, {}</p>
		<br/>
		<p class="style0">Train a decision tree (dt) model with the following hyperparameters and save the scores:</p>
		<br/>
		<p class="style0">dt_params = {'criterion': 'mae', 'min_samples_leaf': 15, \</p>
		<br/>
		<p class="style0">             'random_state': 11}</p>
		<br/>
		<p class="style0">dt = DecisionTreeRegressor(**dt_params)</p>
		<br/>
		<p class="style0">dt.fit(x_train, y_train)</p>
		<br/>
		<p class="style0">dt_preds_train = dt.predict(x_train)</p>
		<br/>
		<p class="style0">dt_preds_val = dt.predict(x_val)</p>
		<br/>
		<p class="style0">train_mae_values['dt'] = mean_absolute_error(y_true=y_train, \</p>
		<br/>
		<p class="style0">                                             y_pred=dt_preds_train)</p>
		<br/>
		<p class="style0">val_mae_values['dt'] = mean_absolute_error(y_true=y_val, \</p>
		<br/>
		<p class="style0">                                           y_pred=dt_preds_val)</p>
		<br/>
		<p class="style0">Train a k-nearest neighbours (knn) model with the following hyperparameters and save the scores:</p>
		<br/>
		<p class="style0">knn_params = {'n_neighbors': 5}</p>
		<br/>
		<p class="style0">knn = KNeighborsRegressor(**knn_params)</p>
		<br/>
		<p class="style0">knn.fit(x_train, y_train)</p>
		<br/>
		<p class="style0">knn_preds_train = knn.predict(x_train)</p>
		<br/>
		<p class="style0">knn_preds_val = knn.predict(x_val)</p>
		<br/>
		<p class="style0">train_mae_values['knn'] = mean_absolute_error(y_true=y_train, \</p>
		<br/>
		<p class="style0">                                              y_pred=knn_preds_train)</p>
		<br/>
		<p class="style0">val_mae_values['knn'] = mean_absolute_error(y_true=y_val, \</p>
		<br/>
		<p class="style0">                                            y_pred=knn_preds_val)</p>
		<br/>
		<p class="style0">Train a random forest (rf) model with the following hyperparameters and save the scores:</p>
		<br/>
		<p class="style0">rf_params = {'n_estimators': 20, 'criterion': 'mae', \</p>
		<br/>
		<p class="style0">             'max_features': 'sqrt', 'min_samples_leaf': 10, \</p>
		<br/>
		<p class="style0">             'random_state': 11, 'n_jobs': -1}</p>
		<br/>
		<p class="style0">rf = RandomForestRegressor(**rf_params)</p>
		<br/>
		<p class="style0">rf.fit(x_train, y_train)</p>
		<br/>
		<p class="style0">rf_preds_train = rf.predict(x_train)</p>
		<br/>
		<p class="style0">rf_preds_val = rf.predict(x_val)</p>
		<br/>
		<p class="style0">train_mae_values['rf'] = mean_absolute_error(y_true=y_train, \</p>
		<br/>
		<p class="style0">                                             y_pred=rf_preds_train)</p>
		<br/>
		<p class="style0">val_mae_values['rf'] = mean_absolute_error(y_true=y_val, \</p>
		<br/>
		<p class="style0">                                           y_pred=rf_preds_val)</p>
		<br/>
		<p class="style0">Train a gradient boosting regression (gbr) model with the following hyperparameters and save the scores:</p>
		<br/>
		<p class="style0">gbr_params = {'n_estimators': 20, 'criterion': 'mae', \</p>
		<br/>
		<p class="style0">              'max_features': 'sqrt', 'min_samples_leaf': 10, \</p>
		<br/>
		<p class="style0">              'random_state': 11}</p>
		<br/>
		<p class="style0">gbr = GradientBoostingRegressor(**gbr_params)</p>
		<br/>
		<p class="style0">gbr.fit(x_train, y_train)</p>
		<br/>
		<p class="style0">gbr_preds_train = gbr.predict(x_train)</p>
		<br/>
		<p class="style0">gbr_preds_val = gbr.predict(x_val)</p>
		<br/>
		<p class="style0">train_mae_values['gbr'] = mean_absolute_error(y_true=y_train, \</p>
		<br/>
		<p class="style0">                                              y_pred=gbr_preds_train)</p>
		<br/>
		<p class="style0">val_mae_values['gbr'] = mean_absolute_error(y_true=y_val, \</p>
		<br/>
		<p class="style0">                                            y_pred=gbr_preds_val)</p>
		<br/>
		<p class="style0">Prepare the training and validation datasets, with the four meta estimators having the same hyperparameters that were used in the previous steps. First, we build the training set:</p>
		<br/>
		<p class="style0">num_base_predictors = len(train_mae_values) # 4</p>
		<br/>
		<p class="style0">x_train_with_metapreds = np.zeros((x_train.shape[0], \</p>
		<br/>
		<p class="style0">                         x_train.shape[1]+num_base_predictors))</p>
		<br/>
		<p class="style0">x_train_with_metapreds[:, :-num_base_predictors] = x_train</p>
		<br/>
		<p class="style0">x_train_with_metapreds[:, -num_base_predictors:] = -1</p>
		<br/>
		<p class="style0">kf = KFold(n_splits=5, random_state=11)</p>
		<br/>
		<p class="style0">for train_indices, val_indices in kf.split(x_train):</p>
		<br/>
		<p class="style0">    kfold_x_train, kfold_x_val = x_train.iloc[train_indices], \</p>
		<br/>
		<p class="style0">                                 x_train.iloc[val_indices]</p>
		<br/>
		<p class="style0">    kfold_y_train, kfold_y_val = y_train[train_indices], \</p>
		<br/>
		<p class="style0">                                 y_train[val_indices]</p>
		<br/>
		<p class="style0">    predictions = []</p>
		<br/>
		<p class="style0">    dt = DecisionTreeRegressor(**dt_params)</p>
		<br/>
		<p class="style0">    dt.fit(kfold_x_train, kfold_y_train)</p>
		<br/>
		<p class="style0">    predictions.append(dt.predict(kfold_x_val))</p>
		<br/>
		<p class="style0">    knn = KNeighborsRegressor(**knn_params)</p>
		<br/>
		<p class="style0">    knn.fit(kfold_x_train, kfold_y_train)</p>
		<br/>
		<p class="style0">    predictions.append(knn.predict(kfold_x_val))</p>
		<br/>
		<p class="style0">    gbr = GradientBoostingRegressor(**gbr_params)</p>
		<br/>
		<p class="style0">    rf.fit(kfold_x_train, kfold_y_train)</p>
		<br/>
		<p class="style0">    predictions.append(rf.predict(kfold_x_val))</p>
		<br/>
		<p class="style0">    gbr = GradientBoostingRegressor(**gbr_params)</p>
		<br/>
		<p class="style0">    gbr.fit(kfold_x_train, kfold_y_train)</p>
		<br/>
		<p class="style0">    predictions.append(gbr.predict(kfold_x_val))</p>
		<br/>
		<p class="style0">    for i, preds in enumerate(predictions):</p>
		<br/>
		<p class="style0">        x_train_with_metapreds[val_indices, -(i+1)] = preds</p>
		<br/>
		<p class="style0">Prepare the validation set:</p>
		<br/>
		<p class="style0">x_val_with_metapreds = np.zeros((x_val.shape[0], \</p>
		<br/>
		<p class="style0">                                 x_val.shape[1]+num_base_predictors))</p>
		<br/>
		<p class="style0">x_val_with_metapreds[:, :-num_base_predictors] = x_val</p>
		<br/>
		<p class="style0">x_val_with_metapreds[:, -num_base_predictors:] = -1</p>
		<br/>
		<p class="style0">predictions = []</p>
		<br/>
		<p class="style0">dt = DecisionTreeRegressor(**dt_params)</p>
		<br/>
		<p class="style0">dt.fit(x_train, y_train)</p>
		<br/>
		<p class="style0">predictions.append(dt.predict(x_val))</p>
		<br/>
		<p class="style0">knn = KNeighborsRegressor(**knn_params)</p>
		<br/>
		<p class="style0">knn.fit(x_train, y_train)</p>
		<br/>
		<p class="style0">predictions.append(knn.predict(x_val))</p>
		<br/>
		<p class="style0">gbr = GradientBoostingRegressor(**gbr_params)</p>
		<br/>
		<p class="style0">rf.fit(x_train, y_train)</p>
		<br/>
		<p class="style0">predictions.append(rf.predict(x_val))</p>
		<br/>
		<p class="style0">gbr = GradientBoostingRegressor(**gbr_params)</p>
		<br/>
		<p class="style0">gbr.fit(x_train, y_train)</p>
		<br/>
		<p class="style0">predictions.append(gbr.predict(x_val))</p>
		<br/>
		<p class="style0">for i, preds in enumerate(predictions):</p>
		<br/>
		<p class="style0">    x_val_with_metapreds[:, -(i+1)] = preds</p>
		<br/>
		<p class="style0">Train a linear regression (lr) model as the stacked model:</p>
		<br/>
		<p class="style0">lr = LinearRegression(normalize=True)</p>
		<br/>
		<p class="style0">lr.fit(x_train_with_metapreds, y_train)</p>
		<br/>
		<p class="style0">lr_preds_train = lr.predict(x_train_with_metapreds)</p>
		<br/>
		<p class="style0">lr_preds_val = lr.predict(x_val_with_metapreds)</p>
		<br/>
		<p class="style0">train_mae_values['lr'] = mean_absolute_error(y_true=y_train, \</p>
		<br/>
		<p class="style0">                                             y_pred=lr_preds_train)</p>
		<br/>
		<p class="style0">val_mae_values['lr'] = mean_absolute_error(y_true=y_val, \</p>
		<br/>
		<p class="style0">                                           y_pred=lr_preds_val)</p>
		<br/>
		<p class="style0">Visualize the train and validation errors for each individual model and the stacked model:</p>
		<br/>
		<p class="style0">mae_scores = pd.concat([pd.Series(train_mae_values, name='train'), \</p>
		<br/>
		<p class="style0">                        pd.Series(val_mae_values, name='val')], \</p>
		<br/>
		<p class="style0">                        axis=1)</p>
		<br/>
		<p class="style0">mae_scores</p>
		<br/>
		<p class="style0">First, you get the following output:</p>
		<br style="line-height: 1,4"/>
		<div>
			<img src="../Images/image-YV2QPP1E.jpg" alt="Figure 6.17: Values of training and validation errors&#13;&#10;" height="100%"/>
		</div>
		<br/>
		<br/>
		<p class="style0" style="text-align: center">Figure 6.17: Values of training and validation errors</p>
		<br/>
		<p class="style0">Now, plot the MAE scores on a bar plot using the following code:</p>
		<br/>
		<p class="style0">mae_scores.plot(kind='bar', figsize=(10,7))</p>
		<br/>
		<p class="style0">plt.ylabel('MAE')</p>
		<br/>
		<p class="style0">plt.xlabel('Model')</p>
		<br/>
		<p class="style0">plt.show()</p>
		<br/>
		<p class="style0">The final output will be as follows:</p>
		<br style="line-height: 1,4"/>
		<div>
			<img src="../Images/image-2OFCL0GD.jpg" alt="Figure 6.18: Visualization of training and validation errors&#13;&#10;" height="100%"/>
		</div>
		<br/>
		<p class="style0" style="text-align: center">Figure 6.18: Visualization of training and validation errors</p>
		<br/>
		<h4 class="style2">Note</h4>
		<br/>
		<p class="style2">To access the source code for this specific section, please refer to https://packt.live/3fNqtMG.</p>
		<br/>
		<p class="style2">You can also run this example online at https://packt.live/2Yn2VIl. You must execute the entire Notebook in order to get the desired result.</p>
		<div style="page-break-before: always;"/>
	</body></html>
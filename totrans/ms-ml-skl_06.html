<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Clustering with K-Means"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Clustering with K-Means</h1></div></div></div><p>In the previous chapters we discussed supervised learning tasks; we examined algorithms for regression and classification that learned from labeled training data. In this chapter we will discuss an unsupervised learning task called clustering. Clustering is used to find groups of similar observations within a set of unlabeled data. We will discuss the K-Means clustering algorithm, apply it to an image compression problem, and learn to measure its performance. Finally, we will work through a semi-supervised learning problem that combines clustering with classification.</p><p>Recall from <a class="link" href="ch01.html" title="Chapter 1. The Fundamentals of Machine Learning">Chapter 1</a>, <span class="emphasis"><em>The Fundamentals of Machine Learning</em></span>, that the goal of unsupervised learning is to discover hidden structure or patterns in unlabeled training data. <span class="strong"><strong>Clustering</strong></span>, or <span class="strong"><strong>cluster analysis</strong></span>, is the task of grouping observations such that members of the <a class="indexterm" id="id340"/>same group, or cluster, are more similar to each other by a given <a class="indexterm" id="id341"/>metric than they are to the members of the other clusters. As with supervised learning, we will represent an observation as an <span class="emphasis"><em>n</em></span>-dimensional vector. For example, assume that your training data consists of the samples plotted in the following figure:</p><div class="mediaobject"><img alt="Clustering with K-Means" src="graphics/8365OS_06_01.jpg"/></div><p>Clustering might <a class="indexterm" id="id342"/>reveal the following two groups, indicated by squares and circles:</p><div class="mediaobject"><img alt="Clustering with K-Means" src="graphics/8365OS_06_02.jpg"/></div><p>Clustering could also reveal the following four groups:</p><div class="mediaobject"><img alt="Clustering with K-Means" src="graphics/8365OS_06_03.jpg"/></div><p>Clustering is commonly used to explore a dataset. Social networks can be clustered to identify communities and to suggest missing connections between people. In biology, clustering is used to find groups of genes with similar expression patterns. Recommendation systems <a class="indexterm" id="id343"/>sometimes employ clustering to identify products or media that might appeal to a user. In marketing, clustering is used to find segments of similar consumers. In the following sections, we will work through an example of using the K-Means algorithm to cluster a dataset.</p><div class="section" title="Clustering with the K-Means algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Clustering with the K-Means algorithm</h1></div></div></div><p>The K-Means <a class="indexterm" id="id344"/>algorithm is a clustering method that is popular because of its speed and scalability. K-Means is an iterative process of moving the centers of the clusters, or the <span class="strong"><strong>centroids</strong></span>, to the mean position of their constituent <a class="indexterm" id="id345"/>points, and re-assigning instances to their closest clusters. The titular <span class="inlinemediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_18.jpg"/></span> is a hyperparameter that specifies the number of clusters that should be created; K-Means automatically assigns observations to clusters but cannot <a class="indexterm" id="id346"/>determine the appropriate number of clusters. <span class="inlinemediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_18.jpg"/></span> must be a positive integer that is less than the number of instances in the training set. Sometimes, the number of clusters is specified by the clustering problem's context. For example, a company that manufactures shoes might know that it is able to support manufacturing three new models. To understand what groups of customers to target with each model, it surveys customers and creates three clusters from the results. That is, the value of <span class="inlinemediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_18.jpg"/></span> was specified by the problem's context. Other problems may not require a specific number of clusters, and the optimal number of clusters may be ambiguous. We will discuss a heuristic to estimate the optimal number of clusters called the elbow method later in this chapter.</p><p>The parameters of K-Means are the positions of the clusters' centroids and the observations that are assigned to each cluster. Like generalized linear models and decision trees, the optimal values of K-Means' parameters are found by minimizing a cost function. The cost function for K-Means is given by the following equation:</p><div class="mediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_04.jpg"/></div><p>In the preceding equation, <span class="inlinemediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_19.jpg"/></span> is the centroid for the cluster <span class="inlinemediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_20.jpg"/></span>. The cost function sums the distortions of the clusters. Each cluster's distortion is equal to the sum of the squared distances between its centroid and its constituent instances. The distortion is small for compact clusters and large for clusters that contain scattered instances. The parameters that minimize the cost function are learned through an iterative process of assigning observations to clusters and then moving the clusters. First, the clusters' centroids are initialized to random positions. In practice, setting the centroids' positions <a class="indexterm" id="id347"/>equal to the positions of randomly selected observations yields the best results. During each iteration, K-Means assigns <a class="indexterm" id="id348"/>observations to the cluster that they are closest to, and then moves the centroids to their assigned observations' mean location. Let's work through an example by hand using the training data shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Instance</p>
</th><th style="text-align: left" valign="bottom">
<p>X0</p>
</th><th style="text-align: left" valign="bottom">
<p>X1</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td></tr></tbody></table></div><p>There are two <a class="indexterm" id="id349"/>explanatory variables and each instance has two features. The instances are plotted in the following figure:</p><div class="mediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_05.jpg"/></div><p>Assume that K-Means initializes the centroid for the first cluster to the fifth instance and the centroid for the second cluster to the eleventh instance. For each instance, we will calculate its distance <a class="indexterm" id="id350"/>to both centroids, and assign it to the cluster with the closest centroid. The initial assignments are shown in the <span class="strong"><strong>Cluster</strong></span> column of the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Instance</p>
</th><th style="text-align: left" valign="bottom">
<p>X0</p>
</th><th style="text-align: left" valign="bottom">
<p>X1</p>
</th><th style="text-align: left" valign="bottom">
<p>C1 distance</p>
</th><th style="text-align: left" valign="bottom">
<p>C2 distance</p>
</th><th style="text-align: left" valign="bottom">
<p>Last cluster</p>
</th><th style="text-align: left" valign="bottom">
<p>Cluster</p>
</th><th style="text-align: left" valign="bottom">
<p>Changed?</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>3.16228</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>1.41421</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>3.16228</p>
</td><td style="text-align: left" valign="top">
<p>2.82843</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>3.16228</p>
</td><td style="text-align: left" valign="top">
<p>2.82843</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1.41421</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>3.60555</p>
</td><td style="text-align: left" valign="top">
<p>4.12311</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>7.21110</p>
</td><td style="text-align: left" valign="top">
<p>7.07107</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>4.47214</p>
</td><td style="text-align: left" valign="top">
<p>4.24264</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>4.12311</p>
</td><td style="text-align: left" valign="top">
<p>3.60555</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>2.82843</p>
</td><td style="text-align: left" valign="top">
<p>3.16228</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>1.41421</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>1.41421</p>
</td><td style="text-align: left" valign="top">
<p>2.82843</p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>C1 centroid</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>C2 centroid</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr></tbody></table></div><p>The plotted centroids and the initial cluster assignments are shown in the following graph. Instances assigned to the first cluster are marked with <span class="strong"><strong>Xs</strong></span>, and instances assigned to the second <a class="indexterm" id="id351"/>cluster are marked with dots. The markers for the centroids are larger than the markers for the instances.</p><div class="mediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_06.jpg"/></div><p>Now we will move both centroids to the means of their constituent instances, recalculate the distances of the training instances to the centroids, and reassign the instances to the closest centroids:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Instance</p>
</th><th style="text-align: left" valign="bottom">
<p>X0</p>
</th><th style="text-align: left" valign="bottom">
<p>X1</p>
</th><th style="text-align: left" valign="bottom">
<p>C1 distance</p>
</th><th style="text-align: left" valign="bottom">
<p>C2 distance</p>
</th><th style="text-align: left" valign="bottom">
<p>Last Cluster</p>
</th><th style="text-align: left" valign="bottom">
<p>New Cluster</p>
</th><th style="text-align: left" valign="bottom">
<p>Changed?</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>3.492850</p>
</td><td style="text-align: left" valign="top">
<p>2.575394</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>1.341641</p>
</td><td style="text-align: left" valign="top">
<p>2.889107</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>3.255764</p>
</td><td style="text-align: left" valign="top">
<p>3.749830</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>3.492850</p>
</td><td style="text-align: left" valign="top">
<p>1.943067</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>0.447214</p>
</td><td style="text-align: left" valign="top">
<p>1.943067</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>3.687818</p>
</td><td style="text-align: left" valign="top">
<p>3.574285</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>7.443118</p>
</td><td style="text-align: left" valign="top">
<p>6.169378</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>4.753946</p>
</td><td style="text-align: left" valign="top">
<p>3.347250</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>4.242641</p>
</td><td style="text-align: left" valign="top">
<p>4.463000</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>2.720294</p>
</td><td style="text-align: left" valign="top">
<p>4.113194</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>1.843909</p>
</td><td style="text-align: left" valign="top">
<p>0.958315</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>C2</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>3.260775</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>C1</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>C1 centroid</p>
</td><td style="text-align: left" valign="top">
<p>3.8</p>
</td><td style="text-align: left" valign="top">
<p>6.4</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>C2 centroid</p>
</td><td style="text-align: left" valign="top">
<p>4.571429</p>
</td><td style="text-align: left" valign="top">
<p>4.142857</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr></tbody></table></div><p>The new clusters <a class="indexterm" id="id352"/>are plotted in the following graph. Note <a class="indexterm" id="id353"/>that the centroids are diverging and several instances have changed their assignments:</p><div class="mediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_07.jpg"/></div><p>Now, we will move the centroids to the means of their constituents' locations again and reassign the <a class="indexterm" id="id354"/>instances to their nearest centroids. The centroids continue to diverge, as shown in the following figure:</p><div class="mediaobject"><img alt="Clustering with the K-Means algorithm" src="graphics/8365OS_06_08.jpg"/></div><p>None of the instances' centroid assignments will change in the next iteration; K-Means will continue iterating until some stopping criteria is satisfied. Usually, this criterion is either a <a class="indexterm" id="id355"/>threshold for the difference between the values of the cost function for subsequent iterations, or a threshold for the change in the positions of the <a class="indexterm" id="id356"/>centroids between subsequent iterations. If these stopping criteria are small enough, K-Means will converge on an optimum. This optimum will not necessarily be the global optimum.</p><div class="section" title="Local optima"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec28"/>Local optima</h2></div></div></div><p>Recall that K-Means <a class="indexterm" id="id357"/>initially sets the positions of the clusters' centroids to the positions of randomly selected observations. Sometimes, the random initialization is unlucky and the centroids are set to positions that cause K-Means <a class="indexterm" id="id358"/>to converge to a local optimum. For example, assume that K-Means randomly initializes two cluster centroids to the following positions:</p><div class="mediaobject"><img alt="Local optima" src="graphics/8365OS_06_09.jpg"/></div><p>K-Means will eventually converge on a local optimum like that shown in the following figure. These clusters may be informative, but it is more likely that the top and bottom groups of <a class="indexterm" id="id359"/>observations are more informative clusters. To avoid <a class="indexterm" id="id360"/>local optima, K-Means is often repeated dozens or even hundreds of times. In each iteration, it is randomly initialized to different starting cluster positions. The initialization that minimizes the cost function best is selected.</p><div class="mediaobject"><img alt="Local optima" src="graphics/8365OS_06_10.jpg"/></div></div><div class="section" title="The elbow method"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec29"/>The elbow method</h2></div></div></div><p>If <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span> is not <a class="indexterm" id="id361"/>specified by the problem's context, the optimal number of clusters can be estimated using a technique called the <span class="strong"><strong>elbow method</strong></span>. The elbow method plots the value of the cost function produced by different values of <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span>. As <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span> increases, the average distortion will decrease; each cluster will have fewer <a class="indexterm" id="id362"/>constituent instances, and the instances will be closer to their respective centroids. However, the improvements to the average distortion will decline as <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span> increases. The value of <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span> at which the improvement to the distortion declines the most is called the elbow. Let's use the elbow method to choose the number of clusters for a dataset. The following scatter plot visualizes a dataset with two obvious clusters:</p><div class="mediaobject"><img alt="The elbow method" src="graphics/8365OS_06_11.jpg"/></div><p>We will <a class="indexterm" id="id363"/>calculate and plot the mean distortion of the clusters <a class="indexterm" id="id364"/>for each value of <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span> from 1 to 10 with the following code:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; from sklearn.cluster import KMeans 
&gt;&gt;&gt; from scipy.spatial.distance import cdist 
&gt;&gt;&gt; import matplotlib.pyplot as plt 

&gt;&gt;&gt; cluster1 = np.random.uniform(0.5, 1.5, (2, 10))
&gt;&gt;&gt; cluster2 = np.random.uniform(3.5, 4.5, (2, 10))
&gt;&gt;&gt; X = np.hstack((cluster1, cluster2)).T
&gt;&gt;&gt; X = np.vstack((x, y)).T 

&gt;&gt;&gt; K = range(1, 10) 
&gt;&gt;&gt; meandistortions = [] 
&gt;&gt;&gt; for k in K: 
&gt;&gt;&gt;     kmeans = KMeans(n_clusters=k) 
&gt;&gt;&gt;     kmeans.fit(X) 
&gt;&gt;&gt;     meandistortions.append(sum(np.min(cdist(X, kmeans.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0]) 

&gt;&gt;&gt; plt.plot(K, meandistortions, 'bx-') 
&gt;&gt;&gt; plt.xlabel('k') 
&gt;&gt;&gt; plt.ylabel('Average distortion') 
&gt;&gt;&gt; plt.title('Selecting k with the Elbow Method') 
&gt;&gt;&gt; plt.show()</pre></div><div class="mediaobject"><img alt="The elbow method" src="graphics/8365OS_06_12.jpg"/></div><p>The average distortion improves rapidly as we increase <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span> from <span class="strong"><strong>1</strong></span> to <span class="strong"><strong>2</strong></span>. There is little improvement for <a class="indexterm" id="id365"/>values of <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span> greater than 2. Now let's use <a class="indexterm" id="id366"/>the elbow method on the following dataset with three clusters:</p><div class="mediaobject"><img alt="The elbow method" src="graphics/8365OS_06_13.jpg"/></div><p>The following figure shows the elbow plot for the dataset. From this, we can see that the rate of improvement <a class="indexterm" id="id367"/>to the average distortion declines the most <a class="indexterm" id="id368"/>when adding a fourth cluster, that is, the elbow method confirms that <span class="inlinemediaobject"><img alt="The elbow method" src="graphics/8365OS_06_18.jpg"/></span> should be set to three for this dataset.</p><div class="mediaobject"><img alt="The elbow method" src="graphics/8365OS_06_14.jpg"/></div></div></div></div>
<div class="section" title="Evaluating clusters"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec48"/>Evaluating clusters</h1></div></div></div><p>We defined <a class="indexterm" id="id369"/>machine learning as the design and study of systems that learn from experience to improve their performance of a task as measured by a given metric. K-Means is an unsupervised learning algorithm; there are no labels or ground truth to compare with the clusters. However, we can still evaluate the performance of the algorithm using intrinsic measures. We have already discussed measuring the distortions of the clusters. In this section, we will discuss another performance measure for <a class="indexterm" id="id370"/>clustering called the <span class="strong"><strong>silhouette coefficient</strong></span>. The silhouette coefficient is a measure of the compactness and separation of the clusters. It increases as the quality of the clusters increase; it is large for compact clusters that are far from each other and small for large, overlapping clusters. The silhouette coefficient is calculated per instance; for a set of instances, it is calculated as the mean of the individual samples' scores. The silhouette coefficient for an instance is calculated with the following equation:</p><div class="mediaobject"><img alt="Evaluating clusters" src="graphics/8365OS_06_15.jpg"/></div><p>
<span class="emphasis"><em>a</em></span> is the mean distance between the instances in the cluster. <span class="emphasis"><em>b</em></span> is the mean distance between the instance and the instances in the next closest cluster. The following example runs K-Means <a class="indexterm" id="id371"/>four times to create two, three, four, and eight clusters from a toy dataset and calculates the silhouette coefficient for each run:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.cluster import KMeans
&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.subplot(3, 2, 1)
&gt;&gt;&gt; x1 = np.array([1, 2, 3, 1, 5, 6, 5, 5, 6, 7, 8, 9, 7, 9])
&gt;&gt;&gt; x2 = np.array([1, 3, 2, 2, 8, 6, 7, 6, 7, 1, 2, 1, 1, 3])
&gt;&gt;&gt; X = np.array(zip(x1, x2)).reshape(len(x1), 2)
&gt;&gt;&gt; plt.xlim([0, 10])
&gt;&gt;&gt; plt.ylim([0, 10])
&gt;&gt;&gt; plt.title('Instances')
&gt;&gt;&gt; plt.scatter(x1, x2)
&gt;&gt;&gt; colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'b']
&gt;&gt;&gt; markers = ['o', 's', 'D', 'v', '^', 'p', '*', '+']
&gt;&gt;&gt; tests = [2, 3, 4, 5, 8]
&gt;&gt;&gt; subplot_counter = 1
&gt;&gt;&gt; for t in tests:
&gt;&gt;&gt;     subplot_counter += 1
&gt;&gt;&gt;     plt.subplot(3, 2, subplot_counter)
&gt;&gt;&gt;     kmeans_model = KMeans(n_clusters=t).fit(X)
&gt;&gt;&gt;     for i, l in enumerate(kmeans_model.labels_):
&gt;&gt;&gt;         plt.plot(x1[i], x2[i], color=colors[l], marker=markers[l], ls='None')
&gt;&gt;&gt;     plt.xlim([0, 10])
&gt;&gt;&gt;     plt.ylim([0, 10])
&gt;&gt;&gt;     plt.title('K = %s, silhouette coefficient = %.03f' % (
&gt;&gt;&gt;         t, metrics.silhouette_score(X, kmeans_model.labels_, metric='euclidean')))
&gt;&gt;&gt; plt.show()</pre></div><p>This script produces the following figure:</p><div class="mediaobject"><img alt="Evaluating clusters" src="graphics/8365OS_06_16.jpg"/></div><p>The dataset contains three obvious clusters. Accordingly, the silhouette coefficient is greatest when <span class="inlinemediaobject"><img alt="Evaluating clusters" src="graphics/8365OS_06_18.jpg"/></span> is equal to three. Setting <span class="inlinemediaobject"><img alt="Evaluating clusters" src="graphics/8365OS_06_18.jpg"/></span> equal to eight produces clusters of instances that <a class="indexterm" id="id372"/>are as close to each other as they are to the instances in some of the other clusters, and the silhouette coefficient of these clusters is smallest.</p></div>
<div class="section" title="Image quantization"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec49"/>Image quantization</h1></div></div></div><p>In the previous <a class="indexterm" id="id373"/>sections, we used clustering to explore the structure of a dataset. Now let's apply it to a different problem. Image quantization is a lossy compression method that replaces a range of similar colors in an image with a single color. Quantization reduces the size of the image file since fewer bits are required to represent the colors. In the following example, we will use clustering to discover a compressed palette for an image that contains its most important colors. We will then rebuild the image using the compressed palette. This example requires the <code class="literal">mahotas</code> image processing library, which can be installed using <code class="literal">pip install mahotas</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; from sklearn.cluster import KMeans
&gt;&gt;&gt; from sklearn.utils import shuffle
&gt;&gt;&gt; import mahotas as mh</pre></div><p>First we read and flatten the image:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; original_img = np.array(mh.imread('img/tree.jpg'), dtype=np.float64) / 255
&gt;&gt;&gt; original_dimensions = tuple(original_img.shape)
&gt;&gt;&gt; width, height, depth = tuple(original_img.shape)
&gt;&gt;&gt; image_flattened = np.reshape(original_img, (width * height, depth))</pre></div><p>We then use K-Means to create 64 clusters from a sample of 1,000 randomly selected colors. Each of the clusters will be a color in the compressed palette. The code is as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; image_array_sample = shuffle(image_flattened, random_state=0)[:1000]
&gt;&gt;&gt; estimator = KMeans(n_clusters=64, random_state=0)
&gt;&gt;&gt; estimator.fit(image_array_sample)</pre></div><p>Next, we predict the cluster assignment for each of the pixels in the original image:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; cluster_assignments = estimator.predict(image_flattened)</pre></div><p>Finally, we create the compressed image from the compressed palette and cluster assignments:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; compressed_palette = estimator.cluster_centers_
&gt;&gt;&gt; compressed_img = np.zeros((width, height, compressed_palette.shape[1]))
&gt;&gt;&gt; label_idx = 0
&gt;&gt;&gt; for i in range(width):
&gt;&gt;&gt;     for j in range(height):
&gt;&gt;&gt;         compressed_img[i][j] = compressed_palette[cluster_assignments[label_idx]]
&gt;&gt;&gt;         label_idx += 1
&gt;&gt;&gt; plt.subplot(122)
&gt;&gt;&gt; plt.title('Original Image')
&gt;&gt;&gt; plt.imshow(original_img)
&gt;&gt;&gt; plt.axis('off')
&gt;&gt;&gt; plt.subplot(121)
&gt;&gt;&gt; plt.title('Compressed Image')
&gt;&gt;&gt; plt.imshow(compressed_img)
&gt;&gt;&gt; plt.axis('off')
&gt;&gt;&gt; plt.show()</pre></div><p>The original and <a class="indexterm" id="id374"/>compressed versions of the image are show in the following figure:</p><div class="mediaobject"><img alt="Image quantization" src="graphics/8365OS_06_17.jpg"/></div></div>
<div class="section" title="Clustering to learn features"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec50"/>Clustering to learn features</h1></div></div></div><p>In this <a class="indexterm" id="id375"/>example, we will combine clustering with classification in a semi-supervised learning problem. You will learn features by clustering unlabeled data and use the learned features to build a supervised classifier.</p><p>Suppose you own a cat and a dog. Suppose that you have purchased a smartphone, ostensibly to use to communicate with humans, but in practice just to use to photograph your cat and dog. Your photographs are awesome and you are certain that your friends and co-workers would love to review all of them in detail. You'd like to be courteous and respect that some people will only want to see your cat photos, while others will only want to see your dog photos, but separating the photos is laborious. Let's build a semi-supervised learning system that can classify images of cats and dogs.</p><p>Recall from <a class="link" href="ch03.html" title="Chapter 3. Feature Extraction and Preprocessing">Chapter 3</a>, <span class="emphasis"><em>Feature Extraction and Preprocessing</em></span>, that a naïve approach to classifying images is to use the intensities, or brightnesses, of all of the pixels as explanatory variables. This approach produces high-dimensional feature vectors for even small images. Unlike the high-dimensional feature vectors we used to represent documents, these vectors are not sparse. Furthermore, it is obvious that this approach is sensitive to the image's illumination, scale, and orientation. In <a class="link" href="ch03.html" title="Chapter 3. Feature Extraction and Preprocessing">Chapter 3</a>, <span class="emphasis"><em>Feature Extraction and Preprocessing</em></span>, we also discussed SIFT and SURF descriptors, which describe interesting regions of an image in ways that are invariant to scale, rotation, and illumination. In this example, we will cluster the descriptors extracted from all of the images to learn features. We will then represent an image with a vector with one element for each cluster. Each element will encode the number of descriptors extracted from the image that were <a class="indexterm" id="id376"/>assigned to the cluster. This approach is sometimes called the <span class="strong"><strong>bag-of-features</strong></span> representation, as the collection of clusters is analogous to the <a class="indexterm" id="id377"/>bag-of-words representation's vocabulary. We will use 1,000 images of cats and 1,000 images of dogs from the training set for Kaggle's <span class="emphasis"><em>Dogs vs. Cats</em></span> competition. The dataset can be downloaded from <a class="ulink" href="https://www.kaggle.com/c/dogs-vs-cats/data">https://www.kaggle.com/c/dogs-vs-cats/data</a>. We will label cats as the positive class and dogs as the negative class. Note that the images have different dimensions; since our feature vectors do not represent pixels, we do not need to resize the images to have the same dimensions. We will train using the first 60 percent of the images, and test on the remaining 40 percent:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import mahotas as mh
&gt;&gt;&gt; from mahotas.features import surf
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression
&gt;&gt;&gt; from sklearn.metrics import *
&gt;&gt;&gt; from sklearn.cluster import MiniBatchKMeans
&gt;&gt;&gt; import glob</pre></div><p>First, we load the images, convert them to grayscale, and extract the SURF descriptors. SURF descriptors can be extracted more quickly than many similar features, but extracting descriptors from 2,000 images is still computationally expensive. Unlike the previous examples, this script requires several minutes to execute on most computers:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; all_instance_filenames = []
&gt;&gt;&gt; all_instance_targets = []
&gt;&gt;&gt; for f in glob.glob('cats-and-dogs-img/*.jpg'):
&gt;&gt;&gt;     target = 1 if 'cat' in f else 0
&gt;&gt;&gt;     all_instance_filenames.append(f)
&gt;&gt;&gt;     all_instance_targets.append(target)
&gt;&gt;&gt; surf_features = []
&gt;&gt;&gt; counter = 0
&gt;&gt;&gt; for f in all_instance_filenames:
&gt;&gt;&gt;     print 'Reading image:', f
&gt;&gt;&gt;     image = mh.imread(f, as_grey=True)
&gt;&gt;&gt;     surf_features.append(surf.surf(image)[:, 5:])

&gt;&gt;&gt; train_len = int(len(all_instance_filenames) * .60)
&gt;&gt;&gt; X_train_surf_features = np.concatenate(surf_features[:train_len])
&gt;&gt;&gt; X_test_surf_feautres = np.concatenate(surf_features[train_len:])
&gt;&gt;&gt; y_train = all_instance_targets[:train_len]
&gt;&gt;&gt; y_test = all_instance_targets[train_len:]</pre></div><p>We then group the extracted descriptors into 300 clusters in the following code sample. We use <code class="literal">MiniBatchKMeans</code>, a variation of K-Means that uses a random sample of the instances in each iteration. As it computes the distances to the centroids for only a sample of the instances in each iteration, <code class="literal">MiniBatchKMeans</code> converges more quickly but its clusters' <a class="indexterm" id="id378"/>distortions may be greater. In practice, the results are similar, and this compromise is acceptable.:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; n_clusters = 300
&gt;&gt;&gt; print 'Clustering', len(X_train_surf_features), 'features'
&gt;&gt;&gt; estimator = MiniBatchKMeans(n_clusters=n_clusters)
&gt;&gt;&gt; estimator.fit_transform(X_train_surf_features)</pre></div><p>Next, we construct feature vectors for the training and testing data. We find the cluster associated with each of the extracted SURF descriptors, and count them using NumPy's <code class="literal">binCount()</code> function. The following code produces a 300-dimensional feature vector for each instance:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; X_train = []
&gt;&gt;&gt; for instance in surf_features[:train_len]:
&gt;&gt;&gt;     clusters = estimator.predict(instance)
&gt;&gt;&gt;     features = np.bincount(clusters)
&gt;&gt;&gt;     if len(features) &lt; n_clusters:
&gt;&gt;&gt;         features = np.append(features, np.zeros((1, n_clusters-len(features))))
&gt;&gt;&gt;     X_train.append(features)

&gt;&gt;&gt; X_test = []
&gt;&gt;&gt; for instance in surf_features[train_len:]:
&gt;&gt;&gt;     clusters = estimator.predict(instance)
&gt;&gt;&gt;     features = np.bincount(clusters)
&gt;&gt;&gt;     if len(features) &lt; n_clusters:
&gt;&gt;&gt;         features = np.append(features, np.zeros((1, n_clusters-len(features))))
&gt;&gt;&gt;     X_test.append(features)</pre></div><p>Finally, we train <a class="indexterm" id="id379"/>a logistic regression classifier on the feature vectors and targets, and assess its precision, recall, and accuracy:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; clf = LogisticRegression(C=0.001, penalty='l2')
&gt;&gt;&gt; clf.fit_transform(X_train, y_train)
&gt;&gt;&gt; predictions = clf.predict(X_test)
&gt;&gt;&gt; print classification_report(y_test, predictions)
&gt;&gt;&gt; print 'Precision: ', precision_score(y_test, predictions)
&gt;&gt;&gt; print 'Recall: ', recall_score(y_test, predictions)
&gt;&gt;&gt; print 'Accuracy: ', accuracy_score(y_test, predictions)

Reading image: dog.9344.jpg
...
Reading image: dog.8892.jpg
Clustering 756914 features
             precision    recall  f1-score   support

          0       0.71      0.76      0.73       392
          1       0.75      0.70      0.72       408

avg / total       0.73      0.73      0.73       800

Precision:  0.751322751323
Recall:  0.696078431373
Accuracy:  0.7275</pre></div><p>This semi-supervised system has better precision and recall than a logistic regression classifier that uses only the pixel intensities as features. Furthermore, our feature representations have only 300 dimensions; even small 100 x 100 pixel images would have 10,000 dimensions.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec51"/>Summary</h1></div></div></div><p>In this chapter, we discussed our first unsupervised learning task: clustering. Clustering is used to discover structure in unlabeled data. You learned about the K-Means clustering algorithm, which iteratively assigns instances to clusters and refines the positions of the cluster centroids. While K-Means learns from experience without supervision, its performance is still measurable; you learned to use distortion and the silhouette coefficient to evaluate clusters. We applied K-Means to two different problems. First, we used K-Means for image quantization, a compression technique that represents a range of colors with a single color. We also used K-Means to learn features in a semi-supervised image classification problem.</p><p>In the next chapter, we will discuss another unsupervised learning task called dimensionality reduction. Like the semi-supervised feature representations we created to classify images of cats and dogs, dimensionality reduction can be used to reduce the dimensions of a set of explanatory variables while retaining as much information as possible.</p></div></body></html>
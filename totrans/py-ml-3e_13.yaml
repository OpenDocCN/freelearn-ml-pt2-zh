- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallelizing Neural Network Training with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will move on from the mathematical foundations of machine
    learning and deep learning to focus on TensorFlow. TensorFlow is one of the most
    popular deep learning libraries currently available, and it lets us implement
    neural networks (NNs) much more efficiently than any of our previous NumPy implementations.
    In this chapter, we will start using TensorFlow and see how it brings significant
    benefits to training performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will begin the next stage of our journey into machine learning
    and deep learning, and we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How TensorFlow improves training performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with TensorFlow's `Dataset` API (`tf.data`) to build input pipelines
    and efficient model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with TensorFlow to write optimized machine learning code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using TensorFlow high-level APIs to build a multilayer NN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing activation functions for artificial NNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Keras (`tf.keras`), a high-level wrapper around TensorFlow that
    can be used to implement common deep learning architectures conveniently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow and training performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow can speed up our machine learning tasks significantly. To understand
    how it can do this, let's begin by discussing some of the performance challenges
    we typically run into when we run expensive calculations on our hardware. Then,
    we will take a high-level look at what TensorFlow is and what our learning approach
    will be in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Performance challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance of computer processors has, of course, been continuously improving
    in recent years, and that allows us to train more powerful and complex learning
    systems, which means that we can improve the predictive performance of our machine
    learning models. Even the cheapest desktop computer hardware that's available
    right now comes with processing units that have multiple cores.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we saw that many functions in scikit-learn allow us
    to spread those computations over multiple processing units. However, by default,
    Python is limited to execution on one core due to the **global interpreter lock**
    (**GIL**). So, although we, indeed, take advantage of Python's multiprocessing
    library to distribute our computations over multiple cores, we still have to consider
    that the most advanced desktop hardware rarely comes with more than eight or 16
    such cores.
  prefs: []
  type: TYPE_NORMAL
- en: You will recall from *Chapter 12*, *Implementing a Multilayer Artificial Neural
    Network from Scratch*, that we implemented a very simple multilayer perceptron
    (MLP) with only one hidden layer consisting of 100 units. We had to optimize approximately
    80,000 weight parameters ([784*100 + 100] + [100 * 10] + 10 = 79,510) to learn
    a model for a very simple image classification task. The images in MNIST are rather
    small (![](img/B13208_13_001.png)), and we can only imagine the explosion in the
    number of parameters if we wanted to add additional hidden layers or work with
    images that have higher pixel densities. Such a task would quickly become unfeasible
    for a single processing unit. The question then becomes, how can we tackle such
    problems more effectively?
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious solution to this problem is to use graphics processing units (GPUs),
    which are real work horses. You can think of a graphics card as a small computer
    cluster inside your machine. Another advantage is that modern GPUs are relatively
    cheap compared to the state-of-the-art central processing units (CPUs), as you
    can see in the following overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sources for the information in the table are the following websites (Date:
    October 2019):'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ark.intel.com/content/www/us/en/ark/products/189123/intel-core-i9-9960x-x-series-processor-22m-cache-up-to-4-50-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/189123/intel-core-i9-9960x-x-series-processor-22m-cache-up-to-4-50-ghz.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2080-ti/](https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2080-ti/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 65 percent of the price of a modern CPU, we can get a GPU that has 272 times
    more cores and is capable of around 10 times more floating-point calculations
    per second. So, what is holding us back from utilizing GPUs for our machine learning
    tasks? The challenge is that writing code to target GPUs is not as simple as executing
    Python code in our interpreter. There are special packages, such as CUDA and OpenCL,
    that allow us to target the GPU. However, writing code in CUDA or OpenCL is probably
    not the most convenient environment for implementing and running machine learning
    algorithms. The good news is that this is what TensorFlow was developed for!
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow is a scalable and multiplatform programming interface for implementing
    and running machine learning algorithms, including convenience wrappers for deep
    learning. TensorFlow was developed by the researchers and engineers from the Google
    Brain team. While the main development is led by a team of researchers and software
    engineers at Google, its development also involves many contributions from the
    open source community. TensorFlow was initially built for internal use at Google,
    but it was subsequently released in November 2015 under a permissive open source
    license. Many machine learning researchers and practitioners from academia and
    industry have adapted TensorFlow to develop deep learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the performance of training machine learning models, TensorFlow allows
    execution on both CPUs and GPUs. However, its greatest performance capabilities
    can be discovered when using GPUs. TensorFlow supports CUDA-enabled GPUs officially.
    Support for OpenCL-enabled devices is still experimental. However, OpenCL will
    likely be officially supported in the near future. TensorFlow currently supports
    frontend interfaces for a number of programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily for us as Python users, TensorFlow's Python API is currently the most
    complete API, thereby it attracts many machine learning and deep learning practitioners.
    Furthermore, TensorFlow has an official API in C++. In addition, new tools based
    on TensorFlow have been released, TensorFlow.js and TensorFlow Lite, that focus
    on running and deploying machine learning models in a web browser and on mobile
    and Internet of Things (IoT) devices. The APIs in other languages, such as Java,
    Haskell, Node.js, and Go, are not stable yet, but the open source community and
    TensorFlow developers are constantly improving them.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is built around a computation graph composed of a set of nodes. Each
    node represents an operation that may have zero or more input or output. A tensor
    is created as a symbolic handle to refer to the input and output of these operations.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, tensors can be understood as a generalization of scalars, vectors,
    matrices, and so on. More concretely, a scalar can be defined as a rank-0 tensor,
    a vector can be defined as a rank-1 tensor, a matrix can be defined as a rank-2
    tensor, and matrices stacked in a third dimension can be defined as rank-3 tensors.
    But note that in TensorFlow, the values are stored in NumPy arrays, and the tensors
    provide references to these arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the concept of a tensor clearer, consider the following figure, which
    represents tensors of ranks 0 and 1 in the first row, and tensors of ranks 2 and
    3 in the second row:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_02.png)'
  prefs: []
  type: TYPE_IMG
- en: In the original TensorFlow release, TensorFlow computations relied on constructing
    a static, directed graph to represent the data flow. As the use of static computation
    graphs proved to be a major friction point for many users, the TensorFlow library
    recently received a major overhaul with its 2.0 version, which makes building
    and training NN models a lot simpler. While TensorFlow 2.0 still supports static
    computation graphs, it now uses dynamic computation graphs, which allows for more
    flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: How we will learn TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we are going to cover TensorFlow's programming model, in particular,
    creating and manipulating tensors. Then, we will see how to load data and utilize
    TensorFlow `Dataset` objects, which will allow us to iterate through a dataset
    efficiently. In addition, we will discuss the existing, ready-to-use datasets
    in the `tensorflow_datasets` submodule and learn how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: After learning about these basics, the `tf.keras` API will be introduced and
    we will move forward to building machine learning models, learn how to compile
    and train the models, and learn how to save the trained models on disk for future
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: First steps with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will take our first steps in using the low-level TensorFlow
    API. After installing TensorFlow, we will cover how to create tensors in TensorFlow
    and different ways of manipulating them, such as changing their shape, data type,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on how your system is set up, you can typically just use Python''s
    `pip` installer and install TensorFlow from PyPI by executing the following from
    your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will install the latest *stable* version, which is 2.0.0 at the time of
    writing. In order to ensure that the code presented in this chapter can be executed
    as expected, it is recommended that you use TensorFlow 2.0.0, which can be installed
    by specifying the version explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In case you want to use GPUs (recommended), you need a compatible NVIDIA graphics
    card, along with the CUDA Toolkit and the NVIDIA cuDNN library to be installed.
    If your machine satisfies these requirements, you can install TensorFlow with
    GPU support, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For more information about the installation and setup process, please see the
    official recommendations at [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that TensorFlow is still under active development; therefore, every couple
    of months, newer versions are released with significant changes. At the time of
    writing this chapter, the latest TensorFlow version is 2.0\. You can verify your
    TensorFlow version from your terminal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Troubleshooting your installation of TensorFlow**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you experience problems with the installation procedure, read more about
    system- and platform-specific recommendations that are provided at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/).
    Note that all the code in this chapter can be run on your CPU; using a GPU is
    entirely optional but recommended if you want to fully enjoy the benefits of TensorFlow.
    For example, while training some NN models on CPU could take a week, the same
    models could be trained in just a few hours on a modern GPU. If you have a graphics
    card, refer to the installation page to set it up appropriately. In addition,
    you may find this TensorFlow-GPU setup guide helpful, which explains how to install
    the NVIDIA graphics card drivers, CUDA, and cuDNN on Ubuntu (not required but
    recommended requirements for running TensorFlow on a GPU): [https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf).
    Furthermore, as you will see in *Chapter 17*, *Generative Adversarial Networks
    for Synthesizing New Data*, you can also train your models using a GPU for free
    via Google Colab.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating tensors in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s consider a few different ways of creating tensors, and then see
    some of their properties and how to manipulate them. Firstly, we can simply create
    a tensor from a list or a NumPy array using the `tf.convert_to_tensor` function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This resulted in tensors `t_a` and `t_b`, with their properties, `shape=(3,)`
    and `dtype=int32,` adopted from their source. Similar to NumPy arrays, we can
    further see these properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To get access to the values that a tensor refers to, we can simply call the
    `.numpy()` method on a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, creating a tensor of constant values can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Manipulating the data type and shape of a tensor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning ways to manipulate tensors is necessary to make them compatible for
    input to a model or an operation. In this section, you will learn how to manipulate
    tensor data types and shapes via several TensorFlow functions that cast, reshape,
    transpose, and squeeze.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tf.cast()` function can be used to change the data type of a tensor to
    a desired type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you will see in upcoming chapters, certain operations require that the input
    tensors have a certain number of dimensions (that is, rank) associated with a
    certain number of elements (shape). Thus, we might need to change the shape of
    a tensor, add a new dimension, or squeeze an unnecessary dimension. TensorFlow
    provides useful functions (or operations) to achieve this, such as `tf.transpose()`,
    `tf.reshape()`, and `tf.squeeze()`. Let''s take a look at some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transposing a tensor**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Reshaping a tensor (for example, from a 1D vector to a 2D array)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Removing the unnecessary dimensions (dimensions that have size 1, which are
    not needed)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Applying mathematical operations to tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applying mathematical operations, in particular linear algebra operations, is
    necessary for building most machine learning models. In this subsection, we will
    cover some widely used linear algebra operations, such as element-wise product,
    matrix multiplication, and computing the norm of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s instantiate two random tensors, one with uniform distribution
    in the range [–1, 1) and the other with a standard normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that `t1` and `t2` have the same shape. Now, to compute the element-wise
    product of `t1` and `t2`, we can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To compute the mean, sum, and standard deviation along a certain axis (or axes),
    we can use `tf.math.reduce_mean()`, `tf.math.reduce_sum()`, and `tf.math.reduce_std()`.
    For example, the mean of each column in `t1` can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The matrix-matrix product between `t1` and `t2` (that is, ![](img/B13208_13_002.png),
    where the superscript T is for transpose) can be computed by using the `tf.linalg.matmul()`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, computing ![](img/B13208_13_003.png) is performed by transposing
    `t1`, resulting in an array of size ![](img/B13208_13_004.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `tf.norm()` function is useful for computing the ![](img/B13208_13_005.png)
    norm of a tensor. For example, we can calculate the ![](img/B13208_13_006.png)
    norm of `t1` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Split, stack, and concatenate tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will cover TensorFlow operations for splitting a tensor
    into multiple tensors, or the reverse: stacking and concatenating multiple tensors
    into a single one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we have a single tensor and we want to split it into two or more
    tensors. For this, TensorFlow provides a convenient `tf.split()` function, which
    divides an input tensor into a list of equally-sized tensors. We can determine
    the desired number of splits as an integer using the argument `num_or_size_splits`
    to split a tensor along a desired dimension specified by the `axis` argument.
    In this case, the total size of the input tensor along the specified dimension
    must be divisible by the desired number of splits. Alternatively, we can provide
    the desired sizes in a list. Let''s have a look at an example of both these options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Providing the number of splits (must be divisible)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, a tensor of size 6 was divided into a list of three tensors
    each with size 2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Providing the sizes of different splits**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, instead of defining the number of splits, we can also specify
    the sizes of the output tensors directly. Here, we are splitting a tensor of size
    `5` into tensors of sizes `3` and `2`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sometimes, we are working with multiple tensors and need to concatenate or
    stack them to create a single tensor. In this case, TensorFlow functions such
    as `tf.stack()` and `tf.concat()` come in handy. For example, let''s create a
    1D tensor, `A`, containing 1s with size `3` and a 1D tensor, `B`, containing 0s
    with size `2` and concatenate them into a 1D tensor, `C`, of size `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we create 1D tensors `A` and `B`, both with size `3`, then we can stack
    them together to form a 2D tensor, `S`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The TensorFlow API has many operations that you can use for building a model,
    processing your data, and more. However, covering every function is outside the
    scope of this book, where we will focus on the most essential ones. For the full
    list of operations and functions, you can refer to the documentation page of TensorFlow
    at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf).
  prefs: []
  type: TYPE_NORMAL
- en: Building input pipelines using tf.data – the TensorFlow Dataset API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are training a deep NN model, we usually train the model incrementally
    using an iterative optimization algorithm such as stochastic gradient descent,
    as we have seen in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned at the beginning of this chapter, the Keras API is a wrapper around
    TensorFlow for building NN models. The Keras API provides a method, `.fit()`,
    for training the models. In cases where the training dataset is rather small and
    can be loaded as a tensor into the memory, TensorFlow models (that are built with
    the Keras API) can directly use this tensor via their `.fit()` method for training.
    In typical use cases, however, when the dataset is too large to fit into the computer
    memory, we will need to load the data from the main storage device (for example,
    the hard drive or solid-state drive) in chunks, that is, batch by batch (note
    the use of the term "batch" instead of "mini-batch" in this chapter to stay close
    to the TensorFlow terminology). In addition, we may need to construct a data-processing
    pipeline to apply certain transformations and preprocessing steps to our data,
    such as mean centering, scaling, or adding noise to augment the training procedure
    and to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Applying preprocessing functions manually every time can be quite cumbersome.
    Luckily, TensorFlow provides a special class for constructing efficient and convenient
    preprocessing pipelines. In this section, we will see an overview of different
    methods for constructing a TensorFlow `Dataset`, including dataset transformations
    and common preprocessing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a TensorFlow Dataset from existing tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the data already exists in the form of a tensor object, a Python list, or
    a NumPy array, we can easily create a dataset using the `tf.data.Dataset.from_tensor_slices()`
    function. This function returns an object of class `Dataset`, which we can use
    to iterate through the individual elements in the input dataset. As a simple example,
    consider the following code, which creates a dataset from a list of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily iterate through a dataset entry by entry as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to create batches from this dataset, with a desired batch size of
    `3`, we can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create two batches from this dataset, where the first three elements
    go into batch #1, and the remaining elements go into batch #2\. The `.batch()`
    method has an optional argument, `drop_remainder`, which is useful for cases when
    the number of elements in the tensor is not divisible by the desired batch size.
    The default for `drop_remainder` is `False`. We will see more examples illustrating
    the behavior of this method later in the subsection *Shuffle, batch, and repeat*.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining two tensors into a joint dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, we may have the data in two (or possibly more) tensors. For example,
    we could have a tensor for features and a tensor for labels. In such cases, we
    need to build a dataset that combines these tensors together, which will allow
    us to retrieve the elements of these tensors in tuples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we have two tensors, `t_x` and `t_y`. Tensor `t_x` holds our feature
    values, each of size `3`, and `t_y` stores the class labels. For this example,
    we first create these two tensors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we want to create a joint dataset from these two tensors. Note that there
    is a required one-to-one correspondence between the elements of these two tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we first created two separate datasets, namely `ds_x` and `ds_y`. We
    then used the `zip` function to form a joint dataset. Alternatively, we can create
    the joint dataset using `tf.data.Dataset.from_tensor_slices()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: which results in the same output.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a common source of error could be that the element-wise correspondence
    between the original features (*x*) and labels (*y*) might be lost (for example,
    if the two datasets are shuffled separately). However, once they are merged into
    one dataset, it is safe to apply these operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will see how to apply transformations to each individual element of
    a dataset. For this, we will use the previous `ds_joint` dataset and apply feature-scaling
    to scale the values to the range [-1, 1), as currently the values of `t_x` are
    in the range [0, 1) based on a random uniform distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Applying this sort of transformation can be used for a user-defined function.
    For example, if we have a dataset created from the list of image filenames on
    disk, we can define a function to load the images from these filenames and apply
    that function by calling the `.map()` method. You will see an example of applying
    multiple transformations to a dataset later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle, batch, and repeat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As was mentioned in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification,* to train an NN model using stochastic gradient descent optimization,
    it is important to feed training data as randomly shuffled batches. You have already
    seen how to create batches by calling the `.batch()` method of a dataset object.
    Now, in addition to creating batches, you will see how to shuffle and reiterate
    over the datasets. We will continue working with the previous `ds_joint` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a shuffled version from the `ds_joint` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: where the rows are shuffled without losing the one-to-one correspondence between
    the entries in `x` and `y`. The `.shuffle()` method requires an argument called
    `buffer_size`, which determines how many elements in the dataset are grouped together
    before shuffling. The elements in the buffer are randomly retrieved and their
    place in the buffer is given to the next elements in the original (unshuffled)
    dataset. Therefore, if we choose a small `buffer_size`, we may not shuffle the
    dataset perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset is small, choosing a relatively small `buffer_size` may negatively
    affect the predictive performance of the NN as the dataset may not be completely
    randomized. In practice, however, it usually does not have a noticeable effect
    when working with relatively large datasets, which is common in deep learning.
    Alternatively, to ensure complete randomization during each epoch, we can simply
    choose a buffer size that is equal to the number of the training examples, as
    in the preceding code (`buffer_size=len(t_x)`).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will recall that dividing a dataset into batches for model training is
    done by calling the `.batch()` method. Now, let''s create such batches from the
    `ds_joint` dataset and take a look at what a batch looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, when training a model for multiple epochs, we need to shuffle
    and iterate over the dataset by the desired number of epochs. So, let''s repeat
    the batched dataset twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in two copies of each batch. If we change the order of these two
    operations, that is, first batch and then repeat, the results will be different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Notice the difference between the batches. When we first batch and then repeat,
    we get four batches. On the other hand, when repeat is performed first, three
    batches are created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to get a better understanding of how these three operations (batch,
    shuffle, and repeat) behave, let''s experiment with them in different orders.
    First, we will combine the operations in the following order: (1) shuffle, (2)
    batch, and (3) repeat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try a different order: (2) batch, (1) shuffle, and (3) repeat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'While the first code example (shuffle, batch, repeat) appears to have shuffled
    the dataset as expected, we can see that in the second case (batch, shuffle, repeat),
    the elements within a batch were not shuffled at all. We can observe this lack
    of shuffling by taking a closer look at the tensor containing the target values,
    `y`. All batches contain either the pair of values `[y=0, y=1]` or the remaining
    pair of values `[y=2, y=3]`; we do not observe the other possible permutations:
    `[y=2, y=0]`, `[y=1, y=3]`, and so forth. Note that in order to ensure these results
    are not coincidental, you may want to repeat this with a higher number than 3\.
    For example, try it with `.repeat(20)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, can you predict what will happen if we use the shuffle operation after
    repeat, for example, (2) batch, (3) repeat, (1) shuffle? Give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: One common source of error is to call `.batch()` twice in a row on a given dataset.
    By doing this, retrieving items from the resulting dataset will create a batch
    of batches of examples. Basically, each time you call `.batch()` on a dataset,
    it will increase the rank of the retrieved tensors by one.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dataset from files on your local storage disk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will build a dataset from image files stored on disk. There
    is an image folder associated with the online content of this chapter. After downloading
    the folder, you should be able to see six images of cats and dogs in JPEG format.
  prefs: []
  type: TYPE_NORMAL
- en: 'This small dataset will show how building a dataset from stored files generally
    works. To accomplish this, we are going to use two additional modules in TensorFlow:
    `tf.io` to read the image file contents, and `tf.image` to decode the raw contents
    and image resizing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**tf.io and tf.image modules**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tf.io` and `tf.image` modules provide a lot of additional and useful functions,
    which are beyond the scope of the book. You are encouraged to browse through the
    official documentation to learn more about these functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io)
    for `tf.io`'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image)
    for `tf.image`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, let''s take a look at the content of these files. We will
    use the `pathlib` library to generate a list of image files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will visualize these image examples using Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the example images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just from this visualization and the printed image shapes, we can already see
    that the images have different aspect ratios. If you print the aspect ratios (or
    data array shapes) of these images, you will see that some images are 900 pixels
    high and 1200 pixels wide (![](img/B13208_13_008.png)), some are ![](img/B13208_13_009.png),
    and one is ![](img/B13208_13_010.png). Later, we will preprocess these images
    to a consistent size. Another point to consider is that the labels for these images
    are provided within their filenames. So, we extract these labels from the list
    of filenames, assigning label `1` to dogs and label `0` to cats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have two lists: a list of filenames (or paths of each image) and a
    list of their labels. In the previous section, you already learned two ways of
    creating a joint dataset from two tensors. Here, we will use the second approach
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We have called this dataset `ds_files_labels`, since it has filenames and labels.
    Next, we need to apply transformations to this dataset: load the image content
    from its file path, decode the raw content, and resize it to a desired size, for
    example, ![](img/B13208_13_011.png). Previously, we saw how to apply a lambda
    function using the `.map()` method. However, since we need to apply multiple preprocessing
    steps this time, we are going to write a helper function instead and use it when
    calling the `.map()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following visualization of the retrieved example images,
    along with their labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_04.png)'
  prefs: []
  type: TYPE_IMG
- en: The `load_and_preprocess()` function wraps all four steps into a single function,
    including the loading of the raw content, decoding it, and resizing the images.
    The function then returns a dataset that we can iterate over and apply other operations
    that we learned about in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Fetching available datasets from the tensorflow_datasets library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `tensorflow_datasets` library provides a nice collection of freely available
    datasets for training or evaluating deep learning models. The datasets are nicely
    formatted and come with informative descriptions, including the format of features
    and labels and their type and dimensionality, as well as the citation of the original
    paper that introduced the dataset in BibTeX format. Another advantage is that
    these datasets are all prepared and ready to use as `tf.data.Dataset` objects,
    so all the functions we covered in the previous sections can be used directly.
    So, let's see how to use these datasets in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the `tensorflow_datasets` library via `pip` from
    the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s import this module and take a look at the list of available datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code indicates that there are currently 101 datasets available
    (101 datasets at the time of writing this chapter, but this number will likely
    increase)—we printed the first five datasets to the command line. There are two
    ways of fetching a dataset, which we will cover in the following paragraphs by
    fetching two different datasets: CelebA (`celeb_a`) and the MNIST digit dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first approach consists of three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calling the dataset builder function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing the `download_and_prepare()` method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calling the `as_dataset()` method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s work with the first step for the CelebA dataset and print the associated
    description that is provided within the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This provides some useful information to understand the structure of this dataset.
    The features are stored as a dictionary with three keys: `''image''`, `''landmarks''`,
    and `''attributes''`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `'image'` entry refers to the face image of a celebrity; `'landmarks'` refers
    to the dictionary of extracted facial points, such as the position of the eyes,
    nose, and so on; and `'attributes'` is a dictionary of 40 facial attributes for
    the person in the image, like facial expression, makeup, hair properties, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will call the `download_and_prepare()` method. This will download
    the data and store it on disk in a designated folder for all TensorFlow Datasets.
    If you have already done this once, it will simply check whether the data is already
    downloaded so that it does not re-download it if it already exists in the designated
    location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will instantiate the datasets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset is already split into train, test, and validation datasets. In
    order to see what the image examples look like, we can execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the elements of this dataset come in a dictionary. If we want to
    pass this dataset to a supervised deep learning model during training, we have
    to reformat it as a tuple of `(features, label)`. For the label, we will use the
    `''Male''` category from the attributes. We will do this by applying a transformation
    via `map()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s batch the dataset and take a batch of 18 examples from it to
    visualize them with their labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The examples and their labels that are retrieved from `ds_train` are shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_05.png)'
  prefs: []
  type: TYPE_IMG
- en: This was all we needed to do to fetch and use the CelebA image dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will proceed with the second approach for fetching a dataset from
    `tensorflow_datasets`. There is a wrapper function called `load()` that combines
    the three steps for fetching a dataset in one. Let''s see how it can be used to
    fetch the MNIST digit dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the MNIST dataset is split into two partitions. Now, we can
    take the train partition, apply a transformation to convert the elements from
    a dictionary to a tuple, and visualize 10 examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The retrieved example handwritten digits from this dataset are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_06.png)'
  prefs: []
  type: TYPE_IMG
- en: This concludes our coverage of building and manipulating datasets and fetching
    datasets from the `tensorflow_datasets` library. Next, we will see how to build
    NN models in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow style guide**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the official TensorFlow style guide ([https://www.tensorflow.org/community/style_guide](https://www.tensorflow.org/community/style_guide))
    recommends using two-character spacing for code indents. However, this book uses
    four characters for indents as it is more consistent with the official Python
    style guide and also helps with displaying the code syntax highlighting in many
    text editors correctly, as well as the accompanying Jupyter code notebooks at
    [https://github.com/rasbt/python-machine-learning-book-3rd-edition](https://github.com/rasbt/python-machine-learning-book-3rd-edition).
  prefs: []
  type: TYPE_NORMAL
- en: Building an NN model in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, you have learned about the basic utility components
    of TensorFlow for manipulating tensors and organizing data into formats that we
    can iterate over during training. In this section, we will finally implement our
    first predictive model in TensorFlow. As TensorFlow is a bit more flexible but
    also more complex than machine learning libraries such as scikit-learn, we will
    start with a simple linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Keras API (tf.keras)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras is a high-level NN API and was originally developed to run on top of other
    libraries such as TensorFlow and Theano. Keras provides a user-friendly and modular
    programming interface that allows easy prototyping and the building of complex
    models in just a few lines of code. Keras can be installed independently from
    PyPI and then configured to use TensorFlow as its backend engine. Keras is tightly
    integrated into TensorFlow and its modules are accessible through `tf.keras`.
    In TensorFlow 2.0, `tf.keras` has become the primary and recommended approach
    for implementing models. This has the advantage that it supports TensorFlow-specific
    functionalities, such as dataset pipelines using `tf.data`, which you learned
    about in the previous section. In this book, we will use the `tf.keras` module
    to build NN models.
  prefs: []
  type: TYPE_NORMAL
- en: As you will see in the following subsections, the Keras API (`tf.keras`) makes
    building an NN model extremely easy. The most commonly used approach for building
    an NN in TensorFlow is through `tf.keras.Sequential()`, which allows stacking
    layers to form a network. A stack of layers can be given in a Python list to a
    model defined as `tf.keras.Sequential()`. Alternatively, the layers can be added
    one by one using the `.add()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, `tf.keras` allows us to define a model by subclassing `tf.keras.Model`.
    This gives us more control over the forward pass by defining the `call()` method
    for our model class to specify the forward pass explicitly. We will see examples
    of both of these approaches for building an NN model using the `tf.keras` API.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as you will see in the following subsections, models built using the
    `tf.keras` API can be compiled and trained via the `.compile()` and `.fit()` methods.
  prefs: []
  type: TYPE_NORMAL
- en: Building a linear regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will build a simple model to solve a linear regression
    problem. First, let''s create a toy dataset in NumPy and visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, the training examples will be shown in a scatterplot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will standardize the features (mean centering and dividing by the
    standard deviation) and create a TensorFlow `Dataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can define our model for linear regression as ![](img/B13208_13_012.png).
    Here, we are going to use the Keras API. `tf.keras` provides predefined layers
    for building complex NN models, but to start, you will learn how to define a model
    from scratch. Later in this chapter, you will see how to use those predefined
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this regression problem, we will define a new class derived from the `tf.keras.Model`
    class. Subclassing `tf.keras.Model` allows us to use the Keras tools for exploring
    a model, training, and evaluation. In the constructor of our class, we will define
    the parameters of our model, `w` and `b`, which correspond to the weight and the
    bias parameters, respectively. Finally, we will define the `call()` method to
    determine how this model uses the input data to generate its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will instantiate a new model from the `MyModel()` class that we can
    train based on the training data. The TensorFlow Keras API provides a method named
    `.summary()` for models that are instantiated from `tf.keras.Model`, which allows
    us to get a summary of the model components layer by layer and the number of parameters
    in each layer. Since we have sub-classed our model from `tf.keras.Model`, the
    `.summary()` method is also available to us. But, in order to be able to call
    `model.summary()`, we first need to specify the dimensionality of the input (the
    number of features) to this model. We can do this by calling `model.build()` with
    the expected shape of the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Note that we used `None` as a placeholder for the first dimension of the expected
    input tensor via `model.build()`, which allows us to use an arbitrary batch size.
    However, the number of features is fixed (here `1`) as it directly corresponds
    to the number of weight parameters of the model. Building model layers and parameters
    after instantiation by calling the `.build()` method is called *late variable
    creation*. For this simple model, we already created the model parameters in the
    constructor; therefore, specifying the `input_shape` via `build()` has no further
    effect on our parameters, but still it is needed if we want to call `model.summary()`.
  prefs: []
  type: TYPE_NORMAL
- en: After defining the model, we can define the cost function that we want to minimize
    to find the optimal model weights. Here, we will choose the **mean squared error**
    (**MSE**) as our cost function. Furthermore, to learn the weight parameters of
    the model, we will use stochastic gradient descent. In this subsection, we will
    implement this training via the stochastic gradient descent procedure by ourselves,
    but in the next subsection, we will use the Keras methods `compile()` and `fit()`
    to do the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the stochastic gradient descent algorithm, we need to compute
    the gradients. Rather than manually computing the gradients, we will use the TensorFlow
    API `tf.GradientTape`. We will cover `tf.GradientTape` and its different behaviors
    in *Chapter 14*, *Going Deeper – The Mechanics of TensorFlow*. The code is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can set the hyperparameters and train the model for 200 epochs. We
    will create a batched version of the dataset and repeat the dataset with `count=None`,
    which will result in an infinitely repeated dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the trained model and plot it. For test data, we will create
    a NumPy array of values evenly spaced between 0 to 9\. Since we trained our model
    with standardized features, we will also apply the same standardization to the
    test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the scatterplot of the training examples and the
    trained linear regression model, as well as the convergence history of the weight,
    *w*, and the bias unit, *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Model training via the .compile() and .fit() methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, we saw how to train a model by writing a custom function,
    `train()`, and applied stochastic gradient descent optimization. However, writing
    the `train()` function can be a repeatable task across different projects. The
    TensorFlow Keras API provides a convenient `.fit()` method that can be called
    on an instantiated model. To show how this works, let''s create a new model and
    compile it by selecting the optimizer, loss function, and evaluation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can simply call the `fit()` method to train the model. We can pass
    a batched dataset (like `ds_train`, which was created in the previous example).
    However, this time you will see that we can pass the NumPy arrays for `x` and
    `y` directly, without needing to create a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: After the model is trained, visualize the results and make sure that they are
    similar to the results of the previous method.
  prefs: []
  type: TYPE_NORMAL
- en: Building a multilayer perceptron for classifying flowers in the Iris dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, you saw how to build a model from scratch. We trained
    this model using stochastic gradient descent optimization. While we started our
    journey based on the simplest possible example, you can see that defining the
    model from scratch, even for such a simple case, is neither appealing nor a good
    practice. TensorFlow instead provides already defined layers through `tf.keras.layers`
    that can be readily used as the building blocks of an NN model. In this section,
    you will learn how to use these layers to solve a classification task using the
    Iris flower dataset and build a two-layer perceptron using the Keras API. First,
    let''s get the data from `tensorflow_datasets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This prints some information about this dataset (not printed here to save space).
    However, you will notice in the shown information that this dataset comes with
    only one partition, so we have to split the dataset into training and testing
    partitions (and validation for proper machine learning practice) on our own. Let's
    assume that we want to use two-thirds of the dataset for training and keep the
    remaining examples for testing. The `tensorflow_datasets` library provides a convenient
    tool that allows us to determine slices and splits via the `DatasetBuilder` object
    prior to loading a dataset. You can find out more about splits at [https://www.tensorflow.org/datasets/splits](https://www.tensorflow.org/datasets/splits).
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative approach is to load the entire dataset first and then use `.take()`
    and `.skip()` to split the dataset to two partitions. If the dataset is not shuffled
    at first, we can also shuffle the dataset. However, we need to be very careful
    with this because it can lead to mixing the train/test examples, which is not
    acceptable in machine learning. To avoid this, we have to set an argument, `reshuffle_each_iteration=False`,
    in the `.shuffle()` method. The code for splitting the dataset into train/test
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, as you have already seen in the previous sections, we need to apply a
    transformation via the `.map()` method to convert the dictionary to a tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to use the Keras API to build a model efficiently. In particular,
    using the `tf.keras.Sequential` class, we can stack a few Keras layers and build
    an NN. You can see the list of all the Keras layers that are already available
    at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers).
    For this problem, we are going to use the `Dense` layer (`tf.keras.layers.Dense`),
    which is also known as a fully connected (FC) layer or linear layer, and can be
    best represented by ![](img/B13208_13_013.png), where *x* is the input features,
    *w* and *b* are the weight matrix and the bias vector, and *f* is the activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you think of the layers in an NN, each layer receives its inputs from the
    preceding layer; therefore, its dimensionality (rank and shape) is fixed. Typically,
    we need to concern ourselves with the dimensionality of output only when we design
    an NN architecture. (Note: the first layer is the exception, but TensorFlow/Keras
    allows us to decide the input dimensionality of the first layer after defining
    the model through *late variable creation*.) Here, we want to define a model with
    two hidden layers. The first one receives an input of four features and projects
    them to 16 neurons. The second layer receives the output of the previous layer
    (which has size `16`) and projects them to three output neurons, since we have
    three class labels. This can be done using the `Sequential` class and the `Dense`
    layer in Keras as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we determined the input shape for the first layer via `input_shape=(4,)`,
    and therefore, we did not have to call `.build()` anymore in order to use `iris_model.summary()`.
  prefs: []
  type: TYPE_NORMAL
- en: The printed model summary indicates that the first layer (`fc1`) has 80 parameters,
    and the second layer has 51 parameters. You can verify that by ![](img/B13208_13_014.png),
    where ![](img/B13208_13_015.png) is the number of input units, and ![](img/B13208_13_016.png)
    is the number of output units. Recall that for a fully (densely) connected layer,
    the learnable parameters are the weight matrix of size ![](img/B13208_13_017.png)
    and the bias vector of size ![](img/B13208_13_016.png). Furthermore, notice that
    we used the sigmoid activation function for the first layer and softmax activation
    for the last (output) layer. Softmax activation in the last layer is used to support
    multi-class classification, since here we have three class labels (which is why
    we have three neurons at the output layer). We will discuss the different activation
    functions and their applications later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will compile this model to specify the loss function, the optimizer,
    and the metrics for evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can train the model. We will specify the number of epochs to be `100`
    and the batch size to be `2`. In the following code, we will build an infinitely
    repeating dataset, which will be passed to the `fit()` method for training the
    model. In this case, in order for the `fit()` method to be able to keep track
    of the epochs, it needs to know the number of steps for each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the size of our training data (here, `100`) and the batch size (`batch_size`),
    we can determine the number of steps in each epoch, `steps_per_epoch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned variable history keeps the training loss and the training accuracy
    (since they were specified as metrics to `iris_model.compile()`) after each epoch.
    We can use this to visualize the learning curves as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The learning curves (training loss and training accuracy) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating the trained model on the test dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we specified `''accuracy''` as our evaluation metric in `iris_model.compile()`,
    we can now directly evaluate the model on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we have to batch the test dataset as well, to ensure that the input
    to the model has the correct dimension (rank). As we discussed earlier, calling
    `.batch()` will increase the rank of the retrieved tensors by 1\. The input data
    for `.evaluate()` must have one designated dimension for the batch, although here
    (for evaluation), the size of the batch does not matter. Therefore, if we pass
    `ds_batch.batch(50)` to the `.evaluate()` method, the entire test dataset will
    be processed in one batch of size 50, but if we pass `ds_batch.batch(1)`, 50 batches
    of size 1 will be processed.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and reloading the trained model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Trained models can be saved on disk for future use. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The first option is the filename. Calling `iris_model.save()` will save both
    the model architecture and all the learned parameters. However, if you want to
    save only the architecture, you can use the `iris_model.to_json()` method, which
    saves the model configuration in JSON format. Or if you want to save only the
    model weights, you can do that by calling `iris_model.save_weights()`. The `save_format`
    can be specified to be either `'h5'` for the HDF5 format or `'tf'` for TensorFlow
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s reload the saved model. Since we have saved both the model architecture
    and the weights, we can easily rebuild and reload the parameters in just one line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Try to verify the model architecture by calling `iris_model_new.summary()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s evaluate this new model that is reloaded on the test dataset
    to verify that the results are the same as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Choosing activation functions for multilayer neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For simplicity, we have only discussed the sigmoid activation function in the
    context of multilayer feedforward NNs so far; we used it in the hidden layer as
    well as the output layer in the MLP implementation in *Chapter 12*, *Implementing
    a Multilayer Artificial Neural Network from Scratch*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this book, the sigmoidal logistic function, ![](img/B13208_13_033.png),
    is referred to as the *sigmoid* function for brevity, which is common in machine
    learning literature. In the following subsections, you will learn more about alternative
    nonlinear functions that are useful for implementing multilayer NNs.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, we can use any function as an activation function in multilayer
    NNs as long as it is differentiable. We can even use linear activation functions,
    such as in Adaline (*Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*). However, in practice, it would not be very useful to use
    linear activation functions for both hidden and output layers, since we want to
    introduce nonlinearity in a typical artificial NN to be able to tackle complex
    problems. The sum of linear functions yields a linear function after all.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic (sigmoid) activation function that we used in *Chapter 12* probably
    mimics the concept of a neuron in a brain most closely—we can think of it as the
    probability of whether a neuron fires. However, the logistic (sigmoid) activation
    function can be problematic if we have highly negative input, since the output
    of the sigmoid function will be close to zero in this case. If the sigmoid function
    returns output that is close to zero, the NN will learn very slowly, and it will
    be more likely to get trapped in the local minima during training. This is why
    people often prefer a hyperbolic tangent as an activation function in hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Before we discuss what a hyperbolic tangent looks like, let’s briefly recapitulate
    some of the basics of the logistic function and look at a generalization that
    makes it more useful for multilabel classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic function recap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As was mentioned in the introduction to this section, the logistic function
    is, in fact, a special case of a sigmoid function. You will recall from the section
    on logistic regression in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using scikit-learn*, that we can use a logistic function to model the probability
    that sample *x* belongs to the positive class (class `1`) in a binary classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The given net input, *z*, is shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The logistic (sigmoid) function will compute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that ![](img/B13208_13_036.png) is the bias unit (*y*-axis intercept,
    which means ![](img/B13208_13_037.png)). To provide a more concrete example, let’s
    take a model for a two-dimensional data point, *x*, and a model with the following
    weight coefficients assigned to the *w* vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: If we calculate the net input (*z*) and use it to activate a logistic neuron
    with those particular feature values and weight coefficients, we get a value of
    `0.888`, which we can interpret as an 88.8 percent probability that this particular
    sample, *x*, belongs to the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 12*, we used the one-hot encoding technique to represent multiclass
    ground truth labels and designed the output layer consisting of multiple logistic
    activation units. However, as will be demonstrated by the following code example,
    an output layer consisting of multiple logistic activation units does not produce
    meaningful, interpretable probability values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the output, the resulting values cannot be interpreted as
    probabilities for a three-class problem. The reason for this is that they do not
    sum up to 1\. However, this is, in fact, not a big concern if we use our model
    to predict only the class labels and not the class membership probabilities. One
    way to predict the class label from the output units obtained earlier is to use
    the maximum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: In certain contexts, it can be useful to compute meaningful class probabilities
    for multiclass predictions. In the next section, we will take a look at a generalization
    of the logistic function, the `softmax` function, which can help us with this
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating class probabilities in multiclass classification via the softmax
    function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you saw how we can obtain a class label using the `argmax`
    function. Previously, in the section *Building a multilayer perceptron for classifying
    flowers in the Iris dataset*, we determined `activation=’softmax’` in the last
    layer of the MLP model. The `softmax` function is a soft form of the `argmax`
    function; instead of giving a single class index, it provides the probability
    of each class. Therefore, it allows us to compute meaningful class probabilities
    in multiclass settings (multinomial logistic regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'In `softmax`, the probability of a particular sample with net input *z* belonging
    to the *i*th class can be computed with a normalization term in the denominator,
    that is, the sum of the exponentially weighted linear functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To see `softmax` in action, let’s code it up in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the predicted class probabilities now sum up to 1, as we would
    expect. It is also notable that the predicted class label is the same as when
    we applied the `argmax` function to the logistic output.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may help to think of the result of the `softmax` function as a *normalized*
    output that is useful for obtaining meaningful class-membership predictions in
    multiclass settings. Therefore, when we build a multiclass classification model
    in TensorFlow, we can use the `tf.keras.activations.softmax()` function to estimate
    the probabilities of each class membership for an input batch of examples. To
    see how we can use the `softmax` activation function in TensorFlow, in the following
    code, we will convert `Z` to a tensor, with an additional dimension reserved for
    the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Broadening the output spectrum using a hyperbolic tangent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another sigmoidal function that is often used in the hidden layers of artificial
    NNs is the **hyperbolic tangent** (commonly known as **tanh**), which can be interpreted
    as a rescaled version of the logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The advantage of the hyperbolic tangent over the logistic function is that
    it has a broader output spectrum ranging in the open interval (–1, 1), which can
    improve the convergence of the back-propagation algorithm (*Neural Networks for
    Pattern Recognition*, *C. M. Bishop*, *Oxford University Press*, pages: 500-501,
    *1995*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the logistic function returns an output signal ranging in the
    open interval (0, 1). For a simple comparison of the logistic function and the
    hyperbolic tangent, let’s plot the two sigmoidal functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the shapes of the two sigmoidal curves look very similar; however,
    the `tanh` function has double the output space of the `logistic` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we previously implemented the `logistic` and `tanh` functions verbosely
    for the purpose of illustration. In practice, we can use NumPy’s `tanh` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, when building an NN model, we can use `tf.keras.activations.tanh()`
    in TensorFlow to achieve the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the logistic function is available in SciPy’s `special` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can use the `tf.keras.activations.sigmoid()` function in TensorFlow
    to do the same computation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Rectified linear unit activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Rectified linear unit** (**ReLU**) is another activation function that is
    often used in deep NNs. Before we delve into ReLU, we should step back and understand
    the vanishing gradient problem of tanh and logistic activations.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand this problem, let’s assume that we initially have the net input
    ![](img/B13208_13_040.png), which changes to ![](img/B13208_13_041.png). Computing
    the tanh activation, we get ![](img/B13208_13_042.png) and ![](img/B13208_13_043.png),
    which shows no change in the output (due to the asymptotic behavior of the tanh
    function and numerical errors).
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the derivative of activations with respect to the net input
    diminishes as *z* becomes large. As a result, learning weights during the training
    phase becomes very slow because the gradient terms may be very close to zero.
    ReLU activation addresses this issue. Mathematically, ReLU is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ReLU is still a nonlinear function that is good for learning complex functions
    with NNs. Besides this, the derivative of ReLU, with respect to its input, is
    always 1 for positive input values. Therefore, it solves the problem of vanishing
    gradients, making it suitable for deep NNs. In TensorFlow, we can apply the ReLU
    activation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: We will use the ReLU activation function in the next chapter as an activation
    function for multilayer convolutional NNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know more about the different activation functions that are commonly
    used in artificial NNs, let’s conclude this section with an overview of the different
    activation functions that we encountered in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B13208_13_11.png)'
  prefs: []
  type: TYPE_IMG
- en: You can find the list of all activation functions available in the Keras API
    at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use TensorFlow, an open source library for
    numerical computations, with a special focus on deep learning. While TensorFlow
    is more inconvenient to use than NumPy, due to its additional complexity to support
    GPUs, it allows us to define and train large, multilayer NNs very efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you learned about using the TensorFlow Keras API to build complex machine
    learning and NN models and run them efficiently. We explored model building in
    TensorFlow by defining a model from scratch via subclassing the `tf.keras.Model`
    class. Implementing models can be tedious when we have to program at the level
    of matrix-vector multiplications and define every detail of each operation. However,
    the advantage is that this allows us, as developers, to combine such basic operations
    and build more complex models. We then explored `tf.keras.layers`, which makes
    building NN models a lot easier than implementing them from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned about different activation functions and understood their
    behaviors and applications. Specifically, in this chapter, we covered tanh, softmax,
    and ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll continue our journey and dive deeper into TensorFlow,
    where we’ll find ourselves working with TensorFlow function decoration and TensorFlow
    Estimators. Along the way, you’ll learn many new concepts, such as variables and
    feature columns.
  prefs: []
  type: TYPE_NORMAL

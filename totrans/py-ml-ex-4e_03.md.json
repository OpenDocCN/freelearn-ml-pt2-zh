["```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np \n```", "```py\n>>> pos_fraction = np.linspace(0.00, 1.00, 1000) \n```", "```py\n>>> gini = 1 – pos_fraction**2 – (1-pos_fraction)**2 \n```", "```py\n>>> plt.plot(pos_fraction, gini)\n>>> plt.ylim(0, 1)\n>>> plt.xlabel('Positive fraction')\n>>> plt.ylabel('Gini impurity')\n>>> plt.show() \n```", "```py\n>>> def gini_impurity(labels):\n...     # When the set is empty, it is also pure\n...     if len(labels) == 0:\n...         return 0\n...     # Count the occurrences of each label\n...     counts = np.unique(labels, return_counts=True)[1]\n...     fractions = counts / float(len(labels))\n...     return 1 - np.sum(fractions ** 2) \n```", "```py\n>>> print(f'{gini_impurity([1, 1, 0, 1, 0]):.4f}')\n0.4800\n>>> print(f'{gini_impurity([1, 1, 0, 1, 0, 0]):.4f}')\n0.5000\n>>> print(f'{gini_impurity([1, 1, 1, 1]):.4f}')\n0.0000 \n```", "```py\n>>> pos_fraction = np.linspace(0.001, 0.999, 1000)\n>>> ent = - (pos_fraction * np.log2(pos_fraction) +\n...         (1 - pos_fraction) * np.log2(1 - pos_fraction))\n>>> plt.plot(pos_fraction, ent)\n>>> plt.xlabel('Positive fraction')\n>>> plt.ylabel('Entropy')\n>>> plt.ylim(0, 1)\n>>> plt.show() \n```", "```py\n>>> def entropy(labels):\n...     if len(labels) == 0:\n...         return 0\n...     counts = np.unique(labels, return_counts=True)[1]\n...     fractions = counts / float(len(labels))\n...     return - np.sum(fractions * np.log2(fractions)) \n```", "```py\n>>> print(f'{entropy([1, 1, 0, 1, 0]):.4f}')\n0.9710\n>>> print(f'{entropy([1, 1, 0, 1, 0, 0]):.4f}')\n1.0000\n>>> print(f'{entropy([1, 1, 1, 1]):.4f}')\n-0.0000 \n```", "```py\n>>> criterion_function = {'gini': gini_impurity,\n...                       'entropy': entropy}\n>>> def weighted_impurity(groups, criterion='gini'):\n...     \"\"\"\n...     Calculate weighted impurity of children after a split\n...     @param groups: list of children, and a child consists a\n...                    list of class labels\n...     @param criterion: metric to measure the quality of a split,\n...                       'gini' for Gini impurity or 'entropy' for\n...                           information gain\n...     @return: float, weighted impurity\n...     \"\"\"\n...     total = sum(len(group) for group in groups)\n...     weighted_sum = 0.0\n...     for group in groups:\n...         weighted_sum += len(group) / float(total) *\n...                           criterion_function[criterion](group)\n...     return weighted_sum \n```", "```py\n>>> children_1 = [[1, 0, 1], [0, 1]]\n>>> children_2 = [[1, 1], [0, 0, 1]]\n>>> print(f\"Entropy of #1 split: {weighted_impurity(children_1,\n...       'entropy'):.4f}\")\nEntropy of #1 split: 0.9510\n>>> print(f\"Entropy of #2 split: {weighted_impurity(children_2,\n...       'entropy'):.4f}\")\nEntropy of #2 split: 0.5510 \n```", "```py\nGini(interest, tech) = weighted_impurity([[1, 1, 0], [0, 0, 0, 1]])\n                     = 0.405 \n```", "```py\nGini(interest, Fashion) = weighted_impurity([[0, 0], [1, 0, 1, 0, 1]])\n                        = 0.343 \n```", "```py\nGini(interest, Sports) = weighted_impurity([[0, 1], [1, 0, 0, 1, 0]]) \n                       = 0.486\nGini(occupation, professional) = weighted_impurity([[0, 0, 1, 0],\n                                                    [1, 0, 1]]) = 0.405\nGini(occupation, student) = weighted_impurity([[0, 0, 1, 0],\n                                               [1, 0, 1]]) = 0.405\nGini(occupation, retired) = weighted_impurity([[1, 0, 0, 0, 1, 1], [1]])\n                          = 0.429 \n```", "```py\nGini(interest, tech) = weighted_impurity([[0, 1],\n    [1, 1, 0]]) = 0.467\nGini(interest, Sports) = weighted_impurity([[1, 1, 0],\n    [0, 1]]) = 0.467\nGini(occupation, professional) = weighted_impurity([[0, 1, 0],\n    [1, 1]]) = 0.267\nGini(occupation, student) = weighted_impurity([[1, 0, 1],\n    [0, 1]]) = 0.467\nGini(occupation, retired) = weighted_impurity([[1, 0, 1, 1],\n    [0]]) = 0.300 \n```", "```py\n>>> def split_node(X, y, index, value):\n...     x_index = X[:, index]\n...     # if this feature is numerical\n...     if X[0, index].dtype.kind in ['i', 'f']:\n...         mask = x_index >= value\n...     # if this feature is categorical\n...     else:\n...         mask = x_index == value\n...     # split into left and right child\n...     left = [X[~mask, :], y[~mask]]\n...     right = [X[mask, :], y[mask]]\n...     return left, right \n```", "```py\n>>> def get_best_split(X, y, criterion):\n...     best_index, best_value, best_score, children =\n...                                        None, None, 1, None\n...     for index in range(len(X[0])):\n...         for value in np.sort(np.unique(X[:, index])):\n...             groups = split_node(X, y, index, value)\n...             impurity = weighted_impurity(\n...                         [groups[0][1], groups[1][1]], criterion)\n...             if impurity < best_score:\n...                 best_index, best_value, best_score, children =\n...                                index, value, impurity, groups\n...     return {'index': best_index, 'value': best_value,\n...             'children': children} \n```", "```py\n>>> def get_leaf(labels):\n...     # Obtain the leaf as the majority of the labels\n...     return np.bincount(labels).argmax() \n```", "```py\n>>> def split(node, max_depth, min_size, depth, criterion):\n...     left, right = node['children']\n...     del (node['children'])\n...     if left[1].size == 0:\n...         node['right'] = get_leaf(right[1])\n...         return\n...     if right[1].size == 0:\n...         node['left'] = get_leaf(left[1])\n...         return\n...     # Check if the current depth exceeds the maximal depth\n...     if depth >= max_depth:\n...         node['left'], node['right'] =\n...                         get_leaf(left[1]), get_leaf(right[1])\n...         return\n...     # Check if the left child has enough samples\n...     if left[1].size <= min_size:\n...         node['left'] = get_leaf(left[1])\n...     else:\n...         # It has enough samples, we further split it\n...         result = get_best_split(left[0], left[1], criterion)\n...         result_left, result_right = result['children']\n...         if result_left[1].size == 0:\n...             node['left'] = get_leaf(result_right[1])\n...         elif result_right[1].size == 0:\n...             node['left'] = get_leaf(result_left[1])\n...         else:\n...             node['left'] = result\n...             split(node['left'], max_depth, min_size,\n...                                       depth + 1, criterion)\n...     # Check if the right child has enough samples\n...     if right[1].size <= min_size:\n...         node['right'] = get_leaf(right[1])\n...     else:\n...         # It has enough samples, we further split it\n...         result = get_best_split(right[0], right[1], criterion)\n...         result_left, result_right = result['children']\n...         if result_left[1].size == 0:\n...             node['right'] = get_leaf(result_right[1])\n...         elif result_right[1].size == 0:\n...             node['right'] = get_leaf(result_left[1])\n...         else:\n...             node['right'] = result\n...             split(node['right'], max_depth, min_size,\n...                                         depth + 1, criterion) \n```", "```py\n>>> def train_tree(X_train, y_train, max_depth, min_size,\n...                criterion='gini'):\n...     X = np.array(X_train)\n...     y = np.array(y_train)\n...     root = get_best_split(X, y, criterion)\n...     split(root, max_depth, min_size, 1, criterion)\n...     return root \n```", "```py\n>>> X_train = [['tech', 'professional'],\n...            ['fashion', 'student'],\n...            ['fashion', 'professional'],\n...            ['sports', 'student'],\n...            ['tech', 'student'],\n...            ['tech', 'retired'],\n...            ['sports', 'professional']]\n>>> y_train = [1, 0, 0, 0, 1, 0, 1]\n>>> tree = train_tree(X_train, y_train, 2, 2) \n```", "```py\n>>> CONDITION = {'numerical': {'yes': '>=', 'no': '<'},\n...              'categorical': {'yes': 'is', 'no': 'is not'}}\n>>> def visualize_tree(node, depth=0):\n...     if isinstance(node, dict):\n...         if node['value'].dtype.kind in ['i', 'f']:\n...             condition = CONDITION['numerical']\n...         else:\n...             condition = CONDITION['categorical']\n...         print('{}|- X{} {} {}'.format(depth * '  ',\n...             node['index'] + 1, condition['no'], node['value']))\n...         if 'left' in node:\n...             visualize_tree(node['left'], depth + 1)\n...         print('{}|- X{} {} {}'.format(depth * '  ',\n...             node['index'] + 1, condition['yes'], node['value']))\n...         if 'right' in node:\n...             visualize_tree(node['right'], depth + 1)\n...     else:\n...         print(f\"{depth * '  '}[{node}]\")\n>>> visualize_tree(tree)\n|- X1 is not fashion\n |- X2 is not professional\n   [0]\n |- X2 is professional\n   [1]\n|- X1 is fashion\n [0] \n```", "```py\n>>> X_train_n = [[6, 7],\n...             [2, 4],\n...             [7, 2],\n...             [3, 6],\n...             [4, 7],\n...             [5, 2],\n...             [1, 6],\n...             [2, 0],\n...             [6, 3],\n...             [4, 1]]\n>>> y_train_n = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n>>> tree = train_tree(X_train_n, y_train_n, 2, 2)\n>>> visualize_tree(tree)\n|- X2 < 4\n |- X1 < 7\n   [1]\n |- X1 >= 7\n   [0]\n|- X2 >= 4\n |- X1 < 2\n   [1]\n |- X1 >= 2\n   [0] \n```", "```py\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> tree_sk = DecisionTreeClassifier(criterion='gini',\n...                                max_depth=2, min_samples_split=2)\n>>> tree_sk.fit(X_train_n, y_train_n) \n```", "```py\n>>> from sklearn.tree import export_graphviz\n>>> export_graphviz(tree_sk, out_file='tree.dot',\n...                 feature_names=['X1', 'X2'], impurity=False,\n...                 filled=True, class_names=['0', '1']) \n```", "```py\ndot -Tpng tree.dot -o tree.png \n```", "```py\nhead train | sed 's/,,/, ,/g;s/,,/, ,/g' | column -s, -t \n```", "```py\n>>> import pandas as pd\n>>> n_rows = 300000\n>>> df = pd.read_csv(\"train.csv\", nrows=n_rows) \n```", "```py\n>>> print(df.head(5))\nid  click      hour C1 banner_pos   site_id ... C16 C17 C18 C19     C20 C21\n0  1.000009e+18      0 14102100 1005          0 1fbe01fe ... 50 1722 0  35 -1 79\n1  1.000017e+19      0 14102100 1005          0 1fbe01fe ... 50 1722 0  35 100084 79\n2  1.000037e+19      0 14102100 1005          0 1fbe01fe ... 50 1722 0  35 100084 79\n3  1.000064e+19      0 14102100 1005          0 1fbe01fe ... 50 1722 0  35 100084 79\n4  1.000068e+19      0 14102100 1005          1 fe8cc448 ... 50 2161 0  35 -1 157 \n```", "```py\n>>> Y = df['click'].values \n```", "```py\n>>> X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'],\n                axis=1).values\n>>> print(X.shape)\n(300000, 19) \n```", "```py\n>>> n_train = int(n_rows * 0.9)\n>>> X_train = X[:n_train]\n>>> Y_train = Y[:n_train]\n>>> X_test = X[n_train:]\n>>> Y_test = Y[n_train:] \n```", "```py\n>>> from sklearn.preprocessing import OneHotEncoder\n>>> enc = OneHotEncoder(handle_unknown='ignore') \n```", "```py\n>>> X_train_enc = enc.fit_transform(X_train)\n>>> X_train_enc[0]\n<1x8385 sparse matrix of type '<class 'numpy.float64'>'\nwith 19 stored elements in Compressed Sparse Row format>\n>>> print(X_train_enc[0])\n  (0, 2)\t\t1.0\n  (0, 6)\t\t1.0\n  (0, 188)\t\t1.0\n  (0, 2608)\t\t1.0\n  (0, 2679)\t\t1.0\n  (0, 3771)\t\t1.0\n  (0, 3885)\t\t1.0\n  (0, 3929)\t\t1.0\n  (0, 4879)\t\t1.0\n  (0, 7315)\t\t1.0\n  (0, 7319)\t\t1.0\n  (0, 7475)\t\t1.0\n  (0, 7824)\t\t1.0\n  (0, 7828)\t\t1.0\n  (0, 7869)\t\t1.0\n  (0, 7977)\t\t1.0\n  (0, 7982)\t\t1.0\n  (0, 8021)\t\t1.0\n  (0, 8189)\t\t1.0 \n```", "```py\n>>> X_test_enc = enc.transform(X_test) \n```", "```py\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> parameters = {'max_depth': [3, 10, None]} \n```", "```py\n>>> decision_tree = DecisionTreeClassifier(criterion='gini',\n...                                        min_samples_split=30) \n```", "```py\n>>> grid_search = GridSearchCV(decision_tree, parameters,\n...                            n_jobs=-1, cv=3, scoring='roc_auc') \n```", "```py\n>>> grid_search.fit(X_train, y_train)\n>>> print(grid_search.best_params_)\n{'max_depth': 10} \n```", "```py\n>>> decision_tree_best = grid_search.best_estimator_\n>>> pos_prob = decision_tree_best.predict_proba(X_test)[:, 1]\n>>> from sklearn.metrics import roc_auc_score\n>>> print(f'The ROC AUC on testing set is: {roc_auc_score(Y_test,\n...           pos_prob):.3f}')\nThe ROC AUC on testing set is: 0.719 \n```", "```py\n>>> pos_prob = np.zeros(len(Y_test))\n>>> click_index = np.random.choice(len(Y_test),\n...                   int(len(Y_test) * 51211.0/300000),\n...                   replace=False)\n>>> pos_prob[click_index] = 1\n>>> print(f'The ROC AUC on testing set using random selection is: {roc_auc_score(Y_test, pos_prob):.3f}')\nThe ROC AUC on testing set using random selection is: 0.499 \n```", "```py\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> random_forest = RandomForestClassifier(n_estimators=100,\n...                     criterion='gini', min_samples_split=30,\n...                     n_jobs=-1) \n```", "```py\n>>> grid_search = GridSearchCV(random_forest, parameters,\n...                            n_jobs=-1, cv=3, scoring='roc_auc')\n>>> grid_search.fit(X_train, y_train)\n>>> print(grid_search.best_params_)\n{'max_depth': None} \n```", "```py\n>>> random_forest_best = grid_search.best_estimator_\n>>> pos_prob = random_forest_best.predict_proba(X_test)[:, 1]\n>>> print(f'The ROC AUC on testing set using random forest is: {roc_auc_\n...       score(Y_test, pos_prob):.3f}')\nThe ROC AUC on testing set using random forest is: 0.759 \n```", "```py\nconda install -c conda-forge xgboost \n```", "```py\npip install xgboost \n```", "```py\npip install CMake \n```", "```py\n    >>> import xgboost as xgb\n    >>> model = xgb.XGBClassifier(learning_rate=0.1, max_depth=10,\n    ...                           n_estimators=1000) \n    ```", "```py\n    >>> model.fit(X_train_enc, Y_train) \n    ```", "```py\n    >>> pos_prob = model.predict_proba(X_test_enc)[:, 1]\n    >>> print(f'The ROC AUC on testing set using GBT is: {roc_auc_score(Y_test, pos_prob):.3f}')\n    The ROC AUC on testing set using GBT is: 0.771 \n    ```"]
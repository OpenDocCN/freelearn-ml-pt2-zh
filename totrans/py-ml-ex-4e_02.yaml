- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a Movie Recommendation Engine with Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As promised, in this chapter, we will kick off our supervised learning journey
    with machine learning classification, and specifically, binary classification.
    The goal of the chapter is to build a movie recommendation system, which is a
    good starting point for learning classification from a real-life example—movie
    streaming service providers are already doing this, and we can do the same.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn the fundamental concepts of classification,
    including what it does and its various types and applications, with a focus on
    solving a binary classification problem using a simple, yet powerful, algorithm,
    Naïve Bayes. Finally, the chapter will demonstrate how to fine-tune a model, which
    is an important skill that every data science or machine learning practitioner
    should learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will go into detail on the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Naïve Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Naïve Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a movie recommender with Naïve Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating classification performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning models with cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Movie recommendation can be framed as a machine learning classification problem.
    If it is predicted that you’ll like a movie because you’ve liked or watched similar
    movies, for example, then it will be on your recommended list; otherwise, it won’t.
    Let’s get started by learning the important concepts of machine learning classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification** is one of the main instances of supervised learning. Given
    a training set of data containing observations and their associated categorical
    outputs, the goal of classification is to learn a general rule that correctly
    maps the **observations** (also called **features** or **predictive variables**)
    to the target **categories** (also called **labels** or **classes**). Putting
    it another way, a trained classification model will be generated after the model
    learns from the features and targets of training samples, as shown in the first
    half of *Figure 2.1*. When new or unseen data comes in, the trained model will
    be able to determine their desired class memberships. Class information will be
    predicted based on the known input features using the trained classification model,
    as displayed in the second half of *Figure 2.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: The training and prediction stages in classification'
  prefs: []
  type: TYPE_NORMAL
- en: In general, there are three types of classification based on the possibility
    of class output—**binary**, **multiclass**, and **multi-label classification**.
    We will cover them one by one in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Binary classification classifies observations into one of two possible classes.
    Spam email filtering we encounter every day is a typical use case of binary classification,
    which identifies email messages (input observations) as spam or not spam (output
    classes). Customer churn prediction is another frequently mentioned example, where
    a prediction system takes in customer segment data and activity data from **customer
    relationship management** (**CRM**) systems and identifies which customers are
    likely to churn.
  prefs: []
  type: TYPE_NORMAL
- en: Another application in the marketing and advertising industry is click-through
    prediction for online ads—that is, whether or not an ad will be clicked, given
    users’ interest information and browsing history. Last but not least, binary classification
    is also being employed in biomedical science, for example, in early cancer diagnosis,
    classifying patients into high- or low-risk groups based on MRI images.
  prefs: []
  type: TYPE_NORMAL
- en: 'As demonstrated in *Figure 2.2*, binary classification tries to find a way
    to separate data into two classes (denoted by dots and crosses):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Binary classification example'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget that predicting whether a person likes a movie is also a binary
    classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This type of classification is also referred to as **multinomial classification**.
    It allows more than two possible classes, as opposed to only two in binary cases.
    Handwritten digit recognition is a common instance of classification and has a
    long history of research and development since the early 1900s. A classification
    system, for example, can learn to read and understand handwritten ZIP codes (digits
    from 0 to 9 in most countries) by which envelopes are automatically sorted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Handwritten digit recognition has become a *“Hello, World!”* in the journey
    of studying machine learning, and the scanned document dataset constructed by
    the **National Institute of Standards and Technology** (**NIST**), called **Modified
    National Institute of Standards and Technology** (**MNIST**), is a benchmark dataset
    frequently used to test and evaluate multiclass classification models. *Figure
    2.3* shows four samples taken from the MNIST dataset, representing the digits
    “`9`,” “`2`,” “`1`,” and “`3`,” respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Samples from the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, in *Figure 2.4*, the multiclass classification model tries
    to find segregation boundaries to separate data into the following three different
    classes (denoted by dots, crosses, and triangles):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, diagram  Description automatically generated](img/B21047_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Multiclass classification example'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-label classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first two types of classification, target classes are mutually exclusive
    and a sample is assigned *one, and only one*, label. It is the opposite in multi-label
    classification. Increasing research attention has been drawn to multi-label classification
    by the nature of the combination of categories in modern applications. For example,
    a picture that captures a sea and a sunset can simultaneously belong to both conceptual
    scenes, whereas it can only be an image of either a cat or dog in a binary case,
    or one type of fruit among oranges, apples, and bananas in a multiclass case.
    Similarly, adventure films are often combined with other genres, such as fantasy,
    science fiction, horror, and drama.
  prefs: []
  type: TYPE_NORMAL
- en: Another typical application is protein function classification, as a protein
    may have more than one function—storage, antibody, support, transport, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: A typical approach to solving an *n*-label classification problem is to transform
    it into a set of *n* binary classification problems, where each binary classification
    problem is handled by an individual binary classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 2.5* to see the restructuring of a multi-label classification
    problem into a multiple-binary classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a multi-label classifier  Description automatically generated
    with medium confidence](img/B21047_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Transforming three-label classification into three independent
    binary classifications'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the protein function classification example once more, we can transform
    it into several binary classifications, such as: Is it for storage? Is it for
    antibodies? Is it for support?'
  prefs: []
  type: TYPE_NORMAL
- en: To solve problems like these, researchers have developed many powerful classification
    algorithms, among which Naïve Bayes, **Support Vector Machines** (**SVMs**), decision
    trees, logistic regression, and neural networks are often used.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will cover the mechanics of Naïve Bayes and its
    in-depth implementation, along with other important concepts, including classifier
    tuning and classification performance evaluation. Stay tuned for upcoming chapters
    that cover the other classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Naïve Bayes** classifier belongs to the family of probabilistic classifiers.
    It computes the probabilities of each predictive **feature** (also referred to
    as an **attribute** or **signal**) of the data belonging to each class in order
    to make a prediction of the probability distribution over all classes. Of course,
    from the resulting probability distribution, we can conclude the most likely class
    that the data sample is associated with. What Naïve Bayes does specifically, as
    its name indicates, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes**: As in, it maps the probability of observed input features given
    a possible class to the probability of the class given observed pieces of evidence
    based on Bayes’ theorem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naïve**: As in, it simplifies probability computation by assuming that predictive
    features are mutually independent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I will explain Bayes’ theorem with examples in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ theorem by example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is important to understand Bayes’ theorem before diving into the classifier.
    Let *A* and *B* denote any two events. Events could be that *it will rain tomorrow,
    two kings are drawn from a deck of cards, or a person has cancer*. In Bayes’ theorem,
    *P*(*A* | *B*) is the probability that *A* occurs given that *B* is true. It can
    be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *P*(*B* | *A*) is the probability of observing *B* given that *A* occurs,
    while *P*(*A*) and *P*(*B*) are the probability that *A* and *B* occur, respectively.
    Is that too abstract? Let’s consider the following concrete examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: Given two coins, one is unfair, with 90% of flips getting a
    head and 10% getting a tail, while the other one is fair. Randomly pick one coin
    and flip it. What is the probability that this coin is the unfair one, if we get
    a head?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can solve this by first denoting *U* for the event of picking the unfair
    coin, *F* for the fair coin, and *H* for the event of getting a head. So, the
    probability that the unfair coin has been picked when we get a head, *P(U |H)*,
    can be calculated with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we know, *P*(*H* | *U*) is `0.9`. *P*(*U*) is `0.5` because we randomly
    pick a coin out of two. However, deriving the probability of getting a head, *P*(*H*),
    is not that straightforward, as two events can lead to the following, where *U*
    is when the unfair coin is picked, and *F* is when the fair coin is picked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, *P*(*U*|*H*) becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_004.png)'
  prefs: []
  type: TYPE_IMG
- en: So, under Bayes’ theorem, the probability that the unfair coin has been picked
    when we get a head is `0.64`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 2**: Suppose a physician reported the following cancer screening
    test scenario among 10,000 people:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **Cancer** | **No Cancer** | **Total** |'
  prefs: []
  type: TYPE_TB
- en: '| **Test Positive** | 80 | 900 | 980 |'
  prefs: []
  type: TYPE_TB
- en: '| **Test Negative** | 20 | 9000 | 9020 |'
  prefs: []
  type: TYPE_TB
- en: '| **Total** | 100 | 9900 | 10000 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.1: Example of a cancer screening result'
  prefs: []
  type: TYPE_NORMAL
- en: This indicates that 80 out of 100 cancer patients are correctly diagnosed, while
    the other 20 are not; cancer is falsely detected in 900 out of 9,900 healthy people.
  prefs: []
  type: TYPE_NORMAL
- en: If the result of this screening test on a person is positive, what is the probability
    that they actually have cancer? Let’s assign the event of having cancer and positive
    testing results as *C* and *Pos*, respectively. So we have *P*(*Pos* |*C*) = `80/100`
    = `0.8`, *P*(*C*) = `100/10000` = `0.01`, and *P*(*Pos*) = `980/10000` = `0.098`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply Bayes’ theorem to calculate *P*(*C*|*Pos*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Given a positive screening result, the chance that the subject has cancer is
    8.16%, which is significantly higher than the one under the general assumption
    (100/10000=1%) without the subject undergoing the screening.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 3**: Three machines, *A*, *B*, and *C*, in a factory account for
    35%, 20%, and 45% of bulb production. The fraction of defective bulbs produced
    by each machine is 1.5%, 1%, and 2%, respectively. A bulb produced by this factory
    was identified as defective, which is denoted as event *D*. What are the probabilities
    that this bulb was manufactured by machine *A*, *B*, or *C*, respectively?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Again, we can simply follow Bayes’ theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_006.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_007.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_008.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_009.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_010.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_011.png)'
  prefs: []
  type: TYPE_IMG
- en: So, under Bayes’ theorem, the probabilities that this bulb was manufactured
    by machine *A*, *B*, or *C*, are `0.323`, `0.123`, and `0.554` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, either way, we do not even need to calculate *P*(*D*) since we know that
    the following is the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also know the following concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we have the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_014.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_015.png)'
  prefs: []
  type: TYPE_IMG
- en: This shortcut approach gave us the same results as the original method, but
    faster. Now that you understand Bayes’ theorem as the backbone of Naïve Bayes,
    we can easily move forward with the classifier itself.
  prefs: []
  type: TYPE_NORMAL
- en: The mechanics of Naïve Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by discussing the magic behind the algorithm—how Naïve Bayes works.
    Given a data sample, *x*, with *n* features, *x*[1], *x*[2],..., *x*[n] (*x* represents
    a feature vector and *x* = (*x*[1], *x*[2],..., *x*[n])), the goal of Naïve Bayes
    is to determine the probabilities that this sample belongs to each of *K* possible
    classes *y*[1], *y*[2],..., *y*[K], which is *P(y*[K] *|x)* or *P*(*x*[1], *x*[2],...,
    *x*[n]), where *k* = 1, 2, …, *K*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This looks no different from what we have just dealt with: *x* or *x*[1], *x*[2],...,
    *x*[n]. This is a joint event where a sample that has observed feature values
    *x*[1], *x*[2],..., *x*[n]. *y*[K] is the event that the sample belongs to class
    *k*. We can apply Bayes’ theorem right away:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s look at each component in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*y*[k]) portrays how classes are distributed, with no further knowledge
    of observation features. Thus, it is also called **prior** in Bayesian probability
    terminology. Prior can be either predetermined (usually in a uniform manner where
    each class has an equal chance of occurrence) or learned from a set of training
    samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*y*[k]|*x*), in contrast to prior *P*(*y*[k]), is the **posterior**, with
    extra knowledge of observation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*x* |*y*[K]), or *P*(*x*[1]*, x*[2]*,..., x*[n]|*y*[k]), is the joint distribution
    of *n* features, given that the sample belongs to class *y*[k]. This is how likely
    the features with such values co-occur. This is named **likelihood** in Bayesian
    terminology. Obviously, the likelihood will be difficult to compute as the number
    of features increases. In Naïve Bayes, this is solved thanks to the feature independence
    assumption. The joint conditional distribution of *n* features can be expressed
    as the joint product of individual feature conditional distributions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21047_02_017.png)'
  prefs: []
  type: TYPE_IMG
- en: Each conditional distribution can be efficiently learned from a set of training
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*x*), also called **evidence**, solely depends on the overall distribution
    of features, which is not specific to certain classes and is therefore a normalization
    constant. As a result, posterior is proportional to prior and likelihood:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21047_02_018.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.6* summarizes how a Naïve Bayes classification model is trained and
    applied to new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Training and prediction stages in Naïve Bayes classification'
  prefs: []
  type: TYPE_NORMAL
- en: A Naïve Bayes classification model is trained using labeled data, where each
    instance is associated with a class label. During training, the model learns the
    probability distribution of the features given each class. This involves calculating
    the likelihood of observing each feature value given each class. Once trained,
    the model can be applied to new, unlabeled data. To classify a new instance, the
    model calculates the probability of each class given the observed features using
    Bayes’ theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see a Naïve Bayes classifier in action through a simplified example of
    movie recommendation before we jump to the implementations of Naïve Bayes. Given
    four (pseudo) users, whether they like each of three movies, *m*[1]*, m*[2]*,*
    and *m*[3] (indicated as 1 or 0), and whether they like a target movie (denoted
    as event *Y*) or not (denoted as event *N*), as shown in the following table,
    we are asked to predict how likely it is that another user will like that movie:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ID** | **m1** | **m2** | **m3** | **Whether the user likes the target
    movie** |'
  prefs: []
  type: TYPE_TB
- en: '| **Training data** | 1 | 0 | 1 | 1 | **Y** |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 0 | 1 | **N** |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 0 | 0 | **Y** |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1 | 1 | 0 | **Y** |'
  prefs: []
  type: TYPE_TB
- en: '| **Testing case** | 5 | 1 | 1 | 0 | **?** |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.2: Toy data example for a movie recommendation'
  prefs: []
  type: TYPE_NORMAL
- en: Whether users like three movies, *m*[1]*, m*[2]*,* and *m*[3], are features
    (signals) that we can utilize to predict the target class. The training data we
    have are the four samples with both ratings and target information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s first compute the prior, *P*(*Y*) and *P*(*N*). From the training
    set, we can easily get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_019.png)'
  prefs: []
  type: TYPE_IMG
- en: Alternatively, we can also impose an assumption of a uniform prior that *P*(*Y*)
    = 50%, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we will denote the event that a user likes three movies or
    not as *f*[1]*, f*[2]*,* and *f*[3], respectively. To calculate posterior *P*(*Y|
    x*)*,* where *x* = (1, 1, 0), the first step is to compute the likelihoods, *P*(*f*[1]
    *= 1*| *Y*), *P*(*f*[2] *= 1 Y*), and *P*(*f*[3] *= 0*| *Y*), and similarly, *P*(*f*[1]
    *= 1*| *N*), *P*(*f*[2] *= 1*| *N*), and *P*(*f*[3] *= 0*| *N*), based on the
    training set. However, you may notice that since *f*[1] *= 1* was not seen in
    the *N* class, we will get *P*(*f*[1] *= 1*|*N*) *= 0*. Consequently, we will
    have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_020.png)'
  prefs: []
  type: TYPE_IMG
- en: This means we will recklessly predict class = *Y* by any means.
  prefs: []
  type: TYPE_NORMAL
- en: 'To eliminate the zero-multiplication factor, the unknown likelihood, we usually
    assign an initial value of 1 to each feature, that is, we start counting each
    possible value of a feature from one. This technique is also known as **Laplace
    smoothing**. With this amendment, we now have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_021.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_022.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, given class *N*, 0 + 1 means there are zero likes of *m*[1] plus + 1 smoothing;
    1 + 2 means there is one data point (ID = 2) plus 2 (2 possible values) + 1 smoothing.
    Given class *Y*, 1 + 1 means there is one like of *m*[1] (ID = 4) plus + 1 smoothing;
    3 + 2 means there are 3 data points (ID = 1, 3, 4) plus 2 (2 possible values)
    + 1 smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can compute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_023.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_024.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_025.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_02_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can compute the ratio between two posteriors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, remember this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, finally, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_029.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a `92.1%` chance that the new user will like the target movie.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you now have a solid understanding of Naïve Bayes after going through
    the theory and a toy example. Let’s get ready for its implementation in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After calculating the movie preference example by hand, as promised, we are
    going to implement Naïve Bayes from scratch. After that, we will implement it
    using the `scikit-learn` package.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Naïve Bayes from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we develop the model, let’s define the toy dataset we just worked with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For the model, starting with the prior, we first group the data by label and
    record their indices by classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With `label_indices`, we calculate the prior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the computed prior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With `prior` calculated, we continue with `likelihood`, which is the conditional
    probability, `P(feature|class)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the `smoothing` value to 1 here, which can also be 0 for no smoothing,
    or any other positive value, as long as a higher classification performance is
    achieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you ever find any of this confusing, feel free to check *Figure 2.7* to
    refresh your memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B21047_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: A simple example of computing prior and likelihood'
  prefs: []
  type: TYPE_NORMAL
- en: 'With prior and likelihood ready, we can now compute posterior for the testing/new
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s predict the class of our one sample test set using this prediction
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is exactly what we got previously. We have successfully developed Naïve
    Bayes from scratch and we can now move on to the implementation using `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Naïve Bayes with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Coding from scratch and implementing your own solutions is the best way to
    learn about machine learning models. Of course, you can take a shortcut by directly
    using the `BernoulliNB` module ([https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html))
    from the scikit-learn API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s initialize a model with a smoothing factor (specified as `alpha` in `scikit-learn`)
    of `1.0`, and `prior` learned from the training set (specified as `fit_prior=True`
    in `scikit-learn`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the Naïve Bayes classifier with the `fit` method, we use the following
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the predicted probability results with the `predict_proba` method,
    we use the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we do the following to directly acquire the predicted class with the
    `predict` method (0.5 is the default threshold, and if the predicted probability
    of class `Y` is greater than 0.5, class `Y` is assigned; otherwise, `N` is used):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The prediction results using scikit-learn are consistent with what we got using
    our own solution. Now that we’ve implemented the algorithm both from scratch and
    using `scikit-learn`, why don’t we use it to solve the movie recommendation problem?
  prefs: []
  type: TYPE_NORMAL
- en: Building a movie recommender with Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the toy example, it is now time to build a movie recommender (or, more
    specifically, movie preference classifier) using a real dataset. We herein use
    a movie rating dataset ([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)).
    The movie rating data was collected by the GroupLens Research group from the MovieLens
    website ([http://movielens.org](http://movielens.org)).
  prefs: []
  type: TYPE_NORMAL
- en: 'For demonstration purposes, we will use the stable small dataset, MovieLens
    1M Dataset (which can be downloaded from [https://files.grouplens.org/datasets/movielens/ml-1m.zip](https://files.grouplens.org/datasets/movielens/ml-1m.zip)
    or [https://grouplens.org/datasets/movielens/1m/](https://grouplens.org/datasets/movielens/1m/))
    for `ml-1m.zip` (size: 1 MB) file). It has around 1 million ratings, ranging from
    1 to 5 with half-star increments, given by 6,040 users on 3,706 movies (last updated
    September 2018).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unzip the `ml-1m.zip` file and you will see the following four files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`movies.dat`: It contains the movie information in the format of `MovieID::Title::Genres`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ratings.dat`: It contains user movie ratings in the format of `UserID::MovieID::Rating::Timestamp`.
    We will only be using data from this file in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`users.dat`: It contains user information in the format of `UserID::Gender::Age::Occupation::Zip-code`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`README`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s attempt to predict whether a user likes a particular movie based on how
    they rate other movies (again, ratings are from 1 to 5).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we import all the necessary modules and read the `ratings.dat` into
    a `pandas` DataFrame object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s see how many unique users and movies are in this million-row dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will construct a 6,040 (the number of users) by 3,706 (the number
    of movies) matrix where each row contains movie ratings from a user, and each
    column represents a movie, using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Besides the rating matrix `data`, we also record the `movie ID` to column index
    mapping. The column index is from 0 to 3,705 as we have 3,706 movies.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is always recommended to analyze the data distribution in order to identify
    if there is a class imbalance issue in the dataset. We do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, most ratings are unknown; for the known ones, 35% are of rating
    4, followed by 26% of rating 3, 23% of rating 5, and then 11% and 6% of ratings
    2 and 1, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since most ratings are unknown, we take the movie with the most known ratings
    as our target movie for easier prediction validation. We look for rating counts
    for each movie as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the target movie is ID, and we will treat ratings of other movies as features.
    We only use rows with ratings available for the target movie so we can validate
    how good the prediction is. We construct the dataset accordingly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can consider movies with ratings greater than 3 as being liked (being recommended):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As a rule of thumb in solving classification problems, we need to always analyze
    the label distribution and see how balanced (or imbalanced) the dataset is.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dealing with imbalanced datasets in classification problems requires careful
    consideration and appropriate techniques to ensure that the model effectively
    learns from the data and produces reliable predictions. Here are several strategies
    to address class imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Oversampling**: We can increase the number of instances in the minority class
    by generating synthetic samples or duplicating existing ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Undersampling**: We can decrease the number of instances in the majority
    class by randomly removing samples. Note that we can even combine oversampling
    and undersampling for a more balanced dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class weighting**: We can also assign higher weights to minority class samples
    during model training. In this way, we penalize misclassifications of the minority
    class more heavily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, to comprehensively evaluate our classifier’s performance, we can randomly
    split the dataset into two sets, the training and testing sets, which simulate
    learning data and prediction data, respectively. Generally, the proportion of
    the original dataset to include in the testing split can be 20%, 25%, 30%, or
    33.3%.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some guidelines for choosing the testing split:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Small datasets**: If you have a small dataset (e.g., less than a few thousand
    samples), a larger testing split (e.g., 25% to 30%) may be appropriate to ensure
    that you have enough data for training and testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medium to large datasets**: For medium to large datasets (e.g., tens of thousands
    to millions of samples), a smaller testing split (e.g., 20%) may still provide
    enough data for evaluation while allowing more data to be used for training. A
    20% testing split is a common choice in such cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple models**: Less complex models are generally less prone to overfitting,
    so using a smaller test set split may work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex models**: Complex models like deep learning models can be more prone
    to overfitting. Hence, a larger test set split (e.g., 30%) is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use the `train_test_split` function from `scikit-learn` to do the random
    splitting and to preserve the percentage of samples for each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It is a good practice to assign a fixed `random_state` (for example, `42`) during
    experiments and exploration in order to guarantee that the same training and testing
    sets are generated every time the program runs. This allows us to make sure that
    the classifier functions and performs well on a fixed dataset before we incorporate
    randomness and proceed further.
  prefs: []
  type: TYPE_NORMAL
- en: 'We check the training and testing sizes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Another good thing about the `train_test_split` function is that the resulting
    training and testing sets will have the same class ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Naïve Bayes model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we train a Naïve Bayes model on the training set. You may notice that
    the values of the input features are from 0 to 5, as opposed to 0 or 1 in our
    toy example. Hence, we use the `MultinomialNB` module ([https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html))
    from scikit-learn instead of the `BernoulliNB` module, as `MultinomialNB` can
    work with integer features as well as fractional counts. We import the module,
    initialize a model with a smoothing factor of `1.0` and `prior` learned from the
    training set, and train this model against the training set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use the trained model to make predictions on the testing set. We get
    the predicted probabilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: For each testing sample, we output the probability of class 0, followed by the
    probability of class 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get the predicted class for the test set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate the model’s performance with classification accuracy,
    which is the proportion of correct predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The classification accuracy is around 72%, which means that the Naïve Bayes
    classifier we’ve constructed accurately suggests movies to users about three quarters
    of the time. Ideally, we could also utilize movie genre information from the `movies.dat`
    file, and user demographics (gender, age, occupation, and ZIP code) information
    from the `users.dat` file. Obviously, movies in similar genres tend to attract
    similar users, and users of similar demographics likely have similar movie preferences.
    We will leave it as an exercise for you to explore further.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have covered in depth the first machine learning classifier and evaluated
    its performance by prediction accuracy. Are there any other classification metrics?
    Let’s see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating classification performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Beyond accuracy, there are several metrics we can use to gain more insight
    and avoid class imbalance effects. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F1 score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The area under the curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A **confusion matrix** summarizes testing instances by their predicted values
    and true values, presented as a contingency table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B21047_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Contingency table for a confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, we can compute the confusion matrix of our Naïve Bayes
    classifier. We use the `confusion_matrix` function from `scikit-learn` to compute
    it, but it is very easy to code it ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the resulting confusion matrix, there are 47 false positive
    cases (where the model misinterprets a dislike as a like for a movie), and 148
    false negative cases (where it fails to detect a like for a movie). Hence, classification
    accuracy is just the proportion of all true cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_030.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Precision** measures the fraction of positive calls that are correct, which
    are the following, in our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_031.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Recall**, on the other hand, measures the fraction of true positives that
    are correctly identified, which are the following in our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_032.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall is also called the **true positive rate**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **f1 score** comprehensively includes both the precision and the recall
    and equates to their **harmonic mean**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_033.png)'
  prefs: []
  type: TYPE_IMG
- en: We tend to value the **f1** score above precision or recall alone.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compute these three measurements using corresponding functions from `scikit-learn`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the negative (dislike) class can also be viewed as positive,
    depending on the context. For example, assign the `0` class as `pos_label` and
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the precision, recall, and f1 score for each class, instead of exhausting
    all class labels in the three function calls as shown earlier, a quicker way is
    to call the `classification_report` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here, `weighted avg` is the weighted average according to the proportions of
    the class.
  prefs: []
  type: TYPE_NORMAL
- en: The classification report provides a comprehensive view of how the classifier
    performs on each class. It is, as a result, useful in imbalanced classification,
    where we can easily obtain high accuracy by simply classifying every sample as
    the dominant class, while the precision, recall, and f1 score measurements for
    the minority class, however, will be significantly low.
  prefs: []
  type: TYPE_NORMAL
- en: Precision, recall, and the f1 score are also applicable to **multiclass** classification,
    where we can simply treat a class we are interested in as a positive case, and
    any other classes as negative cases.
  prefs: []
  type: TYPE_NORMAL
- en: During the process of tweaking a binary classifier (that is, trying out different
    combinations of hyperparameters, for example, the smoothing factor in our Naïve
    Bayes classifier), it would be perfect if there was a set of parameters in which
    the highest averaged and class individual f1 scores are achieved at the same time.
    It is, however, usually not the case. Sometimes, a model has a higher average
    f1 score than another model, but a significantly low f1 score for a particular
    class; sometimes, two models have the same average f1 scores, but one has a higher
    f1 score for one class and a lower score for another class. In situations such
    as these, how can we judge which model works better? The **Area Under the Curve**
    (**AUC**) of the **Receiver Operating Characteristic** (**ROC**) is a consolidated
    measurement frequently used in binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROC curve is a plot of the true positive rate versus the false positive
    rate at various probability thresholds, ranging from 0 to 1\. For a testing sample,
    if the probability of a positive class is greater than the threshold, then a positive
    class is assigned; otherwise, we use a negative class. To recap, the true positive
    rate is equivalent to recall, and the false positive rate is the fraction of negatives
    that are incorrectly identified as positive. Let’s code and exhibit the ROC curve
    (under thresholds of `0.0`, `0.1`, `0.2`, …, `1.0`) of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let’s calculate the true and false positive rates for all threshold settings
    (remember, there are `516.0` positive testing samples and `1191` negative ones):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can plot the ROC curve with `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to *Figure 2.9* for the resulting ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, line, plot, screenshot  Description automatically
    generated](img/B21047_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: ROC curve'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the graph, the dashed line is the baseline representing random guessing,
    where the true positive rate increases linearly with the false positive rate;
    its AUC is 0.5\. The solid line is the ROC plot of our model, and its AUC is somewhat
    less than 1\. In a perfect case, the true positive samples have a probability
    of 1, so that the ROC starts at the point with 100% true positive and 0% false
    positive. The AUC of such a perfect curve is 1\. To compute the exact AUC of our
    model, we can resort to the `roc_auc_score` function of `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'What AUC value leads to the conclusion that a classifier is good? Unfortunately,
    there is no such “magic” number. We use the following rule of thumb as general
    guidelines: classification models achieving an AUC of `0.7` to `0.8` are considered
    acceptable, `0.8` to `0.9` are great, and anything above `0.9` are superb. Again,
    in our case, we are only using the very sparse movie rating data. Hence, an AUC
    of `0.69` is actually acceptable.'
  prefs: []
  type: TYPE_NORMAL
- en: You have learned several classification metrics, and we will explore how to
    measure them properly and how to fine-tune our models in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning models with cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Limiting the evaluation to a single fixed set may be misleading since it’s highly
    dependent on the specific data points chosen for that set. We can simply avoid
    adopting the classification results from one fixed testing set, which we did in
    experiments previously. Instead, we usually apply the **k-fold cross-validation**
    technique to assess how a model will generally perform in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *k*-fold cross-validation setting, the original data is first randomly
    divided into *k* equal-sized subsets, in which class proportion is often preserved.
    Each of these *k* subsets is then successively retained as the testing set for
    evaluating the model. During each trial, the rest of the *k* -1 subsets (excluding
    the one-fold holdout) form the training set for driving the model. Finally, the
    average performance across all *k* trials is calculated to generate an overall
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Diagram of 3-fold cross-validation'
  prefs: []
  type: TYPE_NORMAL
- en: Statistically, the average performance of *k*-fold cross-validation is a better
    estimate of how a model performs in general. Given different sets of parameters
    pertaining to a machine learning model and/or data preprocessing algorithms, or
    even two or more different models, the goal of model tuning and/or model selection
    is to pick a set of parameters of a classifier so that the best average performance
    is achieved. With these concepts in mind, we can now start to tweak our Naïve
    Bayes classifier, incorporating cross-validation and the AUC of ROC measurements.
  prefs: []
  type: TYPE_NORMAL
- en: In *k*-fold cross-validation, *k* is usually set at 3, 5, or 10\. If the training
    size is small, a large *k* (5 or 10) is recommended to ensure sufficient training
    samples in each fold. If the training size is large, a small value (such as 3
    or 4) works fine since a higher *k* will lead to an even higher computational
    cost of training on a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `split()` method from the `StratifiedKFold` class of `scikit-learn`
    to divide the data into chunks with preserved class distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After initializing a 5-fold generator, we choose to explore the following values
    for the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`alpha`: This represents the smoothing factor, the initial value for each feature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit_prior`: This represents whether to use prior tailored to the training
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start with the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each fold generated by the `split()` method of the `k_fold` object,
    we repeat the process of classifier initialization, training, and prediction with
    one of the aforementioned combinations of parameters, and record the resulting
    AUCs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we present the results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The (`2`, `False`) set enables the best averaged AUC, at `0.65823`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we retrain the model with the best set of hyperparameters (`2`, `False`)
    and compute the AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: An AUC of `0.686` is achieved with the fine-tuned model. In general, tweaking
    model hyperparameters using cross-validation is one of the most effective ways
    to boost learning performance and reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the fundamental concepts of machine learning
    classification, including types of classification, classification performance
    evaluation, cross-validation, and model tuning. You also learned about the simple,
    yet powerful, classifier, Naïve Bayes. We went in depth through the mechanics
    and implementations of Naïve Bayes with a couple of examples, the most important
    one being the movie recommendation project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Binary classification using Naïve Bayes was the main talking point of this
    chapter. In the next chapter, we will solve ad click-through prediction using
    another binary classification algorithm: a **decision tree**.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, we extracted user-movie relationships only from the movie
    rating data where most ratings are unknown. Can you also utilize data from the
    `movies.dat` and `users.dat` files?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Practice makes perfect—another great project to deepen your understanding could
    be heart disease classification. The dataset can be downloaded directly from [https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Don’t forget to fine-tune the model you obtained from Exercise 2 using the techniques
    you learned in this chapter. What is the best AUC it achieves?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To acknowledge the use of the MovieLens dataset in this chapter, I would like
    to cite the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: 'F. Maxwell Harper and Joseph A. Konstan. 2015\. *The MovieLens Datasets: History
    and Context*. ACM **Transactions on Interactive Intelligent Systems** (**TiiS**)
    5, 4, Article 19 (December 2015), 19 pages. DOI: [http://dx.doi.org/10.1145/2827872](http://dx.doi.org/10.1145/2827872).'
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code187846872178698968.png)'
  prefs: []
  type: TYPE_IMG

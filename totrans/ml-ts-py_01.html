<html><head></head><body>
  <div id="_idContainer063">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-31" class="chapterTitle">Time-Series Analysis with Python</h1>
    <p class="normal">Time-Series analysis revolves around getting familiar with a dataset and coming up with ideas and hypotheses. It can be thought of as "storytelling for data scientists" and is a critical step in machine learning, because it can inform and help shape tentative conclusions to test while training a machine learning model. Roughly speaking, the main difference between time-series analysis and machine learning is that time-series analysis does not include formal statistical modeling and inference. </p>
    <p class="normal">While it can be daunting and seem complex, it is a generally very structured process. In this chapter, we will go through the fundamentals in Python for dealing with time-series patterns. In Python, we can do time-series analysis by interactively querying our data using a number of tools that we have at our fingertips. This starts from creating and loading time-series datasets to identifying trend and seasonality. We'll outline both the structure of time-series analysis, and the constituents both in terms of theory and practice in Python by going through examples. </p>
    <p class="normal">The main example will use a dataset of air pollution in London and Delhi. You can find this example as a Jupyter notebook in the book's GitHub repository.</p>
    <p class="normal">We're going to cover the following topics:</p>
    <ul>
      <li class="bullet">What is time-series analysis?</li>
      <li class="bullet">Working with time-series in Python</li>
      <li class="bullet">Understanding the variables</li>
      <li class="bullet">Uncovering relationships between variables</li>
      <li class="bullet">Identifying trend and seasonality</li>
    </ul>
    <p class="normal">We'll start with a characterization and an attempt at a definition of time-series analysis.</p>
    <h1 id="_idParaDest-32" class="title">What is time-series analysis?</h1>
    <p class="normal">The term <strong class="keyword">time-series analysis</strong> (<strong class="keyword">TSA</strong>) refers to the statistical approach to time-series or the analysis of trend <a id="_idIndexMarker068"/>and seasonality. It is often an <em class="italic">ad hoc</em> exploration and analysis that usually involves visualizing distributions, trends, cyclic patterns, and relationships between features, and between features and the target(s). </p>
    <p class="normal">More generally, we <a id="_idIndexMarker069"/>can say TSA is roughly <strong class="keyword">exploratory data analysis</strong> (<strong class="keyword">EDA</strong>) that's specific <a id="_idIndexMarker070"/>to time-series data. This comparison can be misleading however since TSA can include both descriptive and exploratory elements.</p>
    <p class="normal">Let's see quickly the differences between descriptive and exploratory analysis:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Descriptive analysis</strong> summarizes <a id="_idIndexMarker071"/>characteristics of a dataset</li>
      <li class="bullet"><strong class="keyword">Exploratory analysis</strong> analyzes for <a id="_idIndexMarker072"/>patterns, trends, or relationships between variables</li>
    </ul>
    <p class="normal">Therefore, TSA is the initial investigation of a dataset with the goal of discovering patterns, especially trend and seasonality, and obtaining initial insights, testing hypotheses, and extracting meaningful summary statistics.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Definition: Time-Series Analysis (TSA) is the process of extracting a summary and other statistical information from time-series, most <a id="_idIndexMarker073"/>importantly, the analysis of trend and seasonality.</p>
    </div>
    <p class="normal">Since an important part of TSA is gathering statistics and representing your dataset graphically through visualization, we'll do a lot of plots in this chapter. Many statistics and plots described in this chapter are specific to TSA, so even if you are familiar with EDA, you'll find something new.</p>
    <p class="normal">A part of TSA is collecting and reviewing data, examining the distribution of variables (and variable types), and checking for errors, outliers, and missing values. Some errors, variable types, and anomalies can be corrected, therefore EDA is often performed hand in hand with preprocessing and feature engineering, where columns and fields are selected and transformed. The whole process from data loading to machine learning is highly iterative and may involve multiple instances of TSA at different points.</p>
    <p class="normal">Here are a few crucial steps for working with time-series:</p>
    <ul>
      <li class="bullet">Importing the dataset</li>
      <li class="bullet">Data cleaning</li>
      <li class="bullet">Understanding variables</li>
      <li class="bullet">Uncovering relationships between variables</li>
      <li class="bullet">Identifying trend and seasonality</li>
      <li class="bullet">Preprocessing (including feature engineering)</li>
      <li class="bullet">Training a machine learning model</li>
    </ul>
    <p class="normal">Importing the data can be considered prior to TSA, and data cleaning, feature engineering, and training a machine learning model are not strictly part of TSA. </p>
    <p class="normal">Importing the data includes parsing, for example extracting dates. The three steps that are central to TSA are <a id="_idIndexMarker074"/>understanding variables, uncovering relationships between variables, and identifying trend and seasonality. There's a lot more to say about each of them, and in this chapter, we'll talk about them in more detail in their dedicated sections.</p>
    <p class="normal">The steps belonging to TSA and leading to preprocessing (feature engineering) and machine learning are highly iterative, and can be visually appreciated in the following time-series machine learning flywheel:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_01.png" alt="Flywheel%20-%20page%201.png"/></figure>
    <p class="packt_figref">Figure 2.1: The time-series machine learning flywheel</p>
    <p class="normal">This flywheel emphasizes the iterative nature of the work. For example, data cleaning comes often after <a id="_idIndexMarker075"/>loading the data, but will come up again after we've made another discovery about our variables. I've highlighted TSA in dark, while steps that are not strictly part of TSA are grayed out.</p>
    <p class="normal">Let's go through <a id="_idIndexMarker076"/>something practical! We'll start by loading a dataset. Right after importing the data, we'd ask questions like what's the size of the dataset (the number of observations)? How many features or columns do we have? What are the column types?</p>
    <p class="normal">We'll typically look at histograms or distribution plots. For assessing relationships between features and target variables, we'd calculate correlations and visualize them as a correlation heatmap, where the correlation strength between variables is mapped to colors.</p>
    <p class="normal">We'd look for missing values – in a spreadsheet, these would be empty cells – and we'd clean up and correct <a id="_idIndexMarker077"/>these irregularities, where possible.</p>
    <p class="normal">We are going to be analyzing relationships between variables, and in TSA, one of its peculiarities is that we need to investigate the relationship of time with each variable.</p>
    <p class="normal">Generally, a useful way of distinguishing different types of techniques could be between univariate and multivariate analysis, and between graphical and non-graphical techniques. <strong class="keyword">Univariate analysis</strong> means we are looking at a single variable. This means we could be inspecting <a id="_idIndexMarker078"/>values to get the means and the variance, or – for the graphical side – plotting the distribution. We summarize these techniques in the <em class="italic">Understanding the variables</em> section.</p>
    <p class="normal">On the other hand, <strong class="keyword">multivariate analysis</strong> means we are calculating correlations between variables, or – for the <a id="_idIndexMarker079"/>graphical side – drawing a scatter plot, for example. We'll delve into these techniques in the <em class="italic">Uncovering relationships between variables</em> section.</p>
    <p class="normal">Before we continue, let's go through a bit of the basics of time-series with Python. This will cover the basic operations with time-series data as an introduction. After this, we'll go through Python commands with an actual dataset.</p>
    <h1 id="_idParaDest-33" class="title">Working with time-series in Python</h1>
    <p class="normal">Python has a lot of libraries and packages for time-series, such as <code class="Code-In-Text--PACKT-">datetime</code>, <code class="Code-In-Text--PACKT-">time</code>, <code class="Code-In-Text--PACKT-">calendar</code>, <code class="Code-In-Text--PACKT-">dateutil</code>, and <code class="Code-In-Text--PACKT-">pytz</code>, which can be highly confusing for beginners. At the same time, there are many different <a id="_idIndexMarker080"/>data types like <code class="Code-In-Text--PACKT-">date</code>, <code class="Code-In-Text--PACKT-">time</code>, <code class="Code-In-Text--PACKT-">datetime</code>, <code class="Code-In-Text--PACKT-">tzinfo</code>, <code class="Code-In-Text--PACKT-">timedelta</code>, <code class="Code-In-Text--PACKT-">relativedelta</code>, and more. </p>
    <p class="normal">When it comes to using them, the devil is in the details. Just to name one example: many of these types are insensitive to the timezone. You should feel reassured, however, knowing that to get started, familiarity with a small subset of these libraries and data types is enough. </p>
    <h2 id="_idParaDest-34" class="title">Requirements</h2>
    <p class="normal">In this chapter, we'll use several libraries, which we can quickly install from the terminal (or similarly from Anaconda Navigator):</p>
    <pre class="programlisting con"><code class="hljs-con">pip install -U dython scipy numpy pandas seaborn scikit-learn
</code></pre>
    <p class="normal">We'll execute the <a id="_idIndexMarker081"/>commands from the Python (or IPython) terminal, but equally we could execute them from a Jupyter notebook (or a different environment).</p>
    <p class="normal">It's a good start if we at least know datetime and pandas, two very prominent libraries, which we'll cover in the following two sections. We'll create basic objects and do simple manipulations on them.</p>
    <h2 id="_idParaDest-35" class="title">Datetime</h2>
    <p class="normal">The <code class="Code-In-Text--PACKT-">date</code> and <code class="Code-In-Text--PACKT-">datetime</code> data <a id="_idIndexMarker082"/>types are not primitive types in Python the way that numbers (<code class="Code-In-Text--PACKT-">float</code> and <code class="Code-In-Text--PACKT-">int</code>), <code class="Code-In-Text--PACKT-">string</code>, <code class="Code-In-Text--PACKT-">list</code>, <code class="Code-In-Text--PACKT-">dictionary</code>, <code class="Code-In-Text--PACKT-">tuple</code>, or <code class="Code-In-Text--PACKT-">file</code> are. To work with <code class="Code-In-Text--PACKT-">date</code> and <code class="Code-In-Text--PACKT-">datetime</code> objects, we have to import datetime, a library that is part of the Python Standard Library, and the libraries <a id="_idIndexMarker083"/>that come by default with CPython and other main Python distributions.</p>
    <p class="normal">datetime comes with objects such as <code class="Code-In-Text--PACKT-">date</code>, <code class="Code-In-Text--PACKT-">datetime</code>, <code class="Code-In-Text--PACKT-">time</code>, and <code class="Code-In-Text--PACKT-">timedelta</code>, among others. The difference between <code class="Code-In-Text--PACKT-">datetime</code> and <code class="Code-In-Text--PACKT-">date</code> objects is that the <code class="Code-In-Text--PACKT-">datetime</code> object includes time information in addition to a date.</p>
    <p class="normal">To get a date, we can do this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> date
</code></pre>
    <p class="normal">To get today's date:</p>
    <pre class="programlisting code"><code class="hljs-code">today = date.today()
</code></pre>
    <p class="normal">To get some other date:</p>
    <pre class="programlisting code"><code class="hljs-code">other_date = date(<span class="hljs-number">2021</span>, <span class="hljs-number">3</span>, <span class="hljs-number">24</span>)
</code></pre>
    <p class="normal">If we want a <code class="Code-In-Text--PACKT-">datetime</code> object (a timestamp) instead, we can do this as well:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
now = datetime.now()
</code></pre>
    <p class="normal">This will get the current <a id="_idIndexMarker084"/>timestamp. We can create a <code class="Code-In-Text--PACKT-">datetime</code> for a specific <a id="_idIndexMarker085"/>date and time as well:</p>
    <pre class="programlisting code"><code class="hljs-code">some_date = datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">5</span>, <span class="hljs-number">18</span>, <span class="hljs-number">15</span>, <span class="hljs-number">39</span>, <span class="hljs-number">0</span>)
some_date.isoformat()
</code></pre>
    <p class="normal">We can get a string output in isoformat:</p>
    <pre class="programlisting con"><code class="hljs-con">'2021-05-18T15:39:00'
</code></pre>
    <p class="normal">isoformat, short for the ISO 8601 format, is an international standard for representing dates and times.</p>
    <p class="normal">We can also work with time differences using <code class="Code-In-Text--PACKT-">timedelta</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> timedelta 
year = timedelta(days=<span class="hljs-number">365</span>)
</code></pre>
    <p class="normal">These <code class="Code-In-Text--PACKT-">timedelta</code> objects can be added to other objects for calculations. We can do calculations with a <code class="Code-In-Text--PACKT-">timedelta</code> object, for example:</p>
    <pre class="programlisting code"><code class="hljs-code">year * <span class="hljs-number">10</span>
</code></pre>
    <p class="normal">This should give us the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">datetime.timedelta(days=3650) 
</code></pre>
    <p class="normal">The datetime library can parse string inputs to <code class="Code-In-Text--PACKT-">date</code> and <code class="Code-In-Text--PACKT-">datetime</code> types and output these objects as <code class="Code-In-Text--PACKT-">string</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> date
some_date = date.fromisoformat(<span class="hljs-string">'2021-03-24'</span>)
</code></pre>
    <p class="normal">Or:</p>
    <pre class="programlisting code"><code class="hljs-code">some_date = datetime.date(<span class="hljs-number">2021</span>, <span class="hljs-number">3</span>, <span class="hljs-number">24</span>)
</code></pre>
    <p class="normal">We can format the output with string format options, for example like this:</p>
    <pre class="programlisting code"><code class="hljs-code">some_date.strftime(<span class="hljs-string">'%A %d. %B %Y'</span>)
</code></pre>
    <p class="normal">This would give us:</p>
    <pre class="programlisting con"><code class="hljs-con">'Wednesday 24. March 2021'
</code></pre>
    <p class="normal">Similarly, we can read in a <code class="Code-In-Text--PACKT-">date</code> or <code class="Code-In-Text--PACKT-">datetime</code> object from a string, and we can use the same format options:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
dt = datetime.strptime(<span class="hljs-string">'24/03/21 15:48'</span>, <span class="hljs-string">'%d/%m/%y %H:%M'</span>)
</code></pre>
    <p class="normal">You can find a <a id="_idIndexMarker086"/>complete list of formatting options that you can use both for parsing<a id="_idIndexMarker087"/> strings and printing <code class="Code-In-Text--PACKT-">datetime</code> objects here: <a href="https://strftime.org/"><span class="url">https://strftime.org/</span></a>.</p>
    <p class="normal">A few important ones are listed in this table:</p>
    <table id="table001-1" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Format string</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Meaning</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">%Y</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Year as 4 digits</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">%y</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Year as 2 digits</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">%m</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Month as a number</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">%d</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Day</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">%H</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Hour as 2 digits</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">%M</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Minute as 2 digits</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Figure 2.2: Format strings for dates</p>
    <p class="normal">It's useful to remember these strings <a id="_idIndexMarker088"/>with formatting options. For example, the format string for a US date separated by slashes would look like this: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">'%d/%m/%Y'</span>
</code></pre>
    <h2 id="_idParaDest-36" class="title">pandas</h2>
    <p class="normal">We introduced the <a id="_idIndexMarker089"/>pandas library in the previous chapter. pandas is one of the most important libraries in the Python ecosystem for data science, used for data manipulation and analysis. Initially <a id="_idIndexMarker090"/>released in 2008, it has been a major driver of Python's success.</p>
    <p class="normal">pandas comes with significant time-series functionality such as date range generation, frequency conversion, moving window statistics, date shifting, and lagging.</p>
    <p class="normal">Let's go through some of these basics. We can create a time-series as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
pd.date_range(start=<span class="hljs-string">'2021-03-24'</span>, end=<span class="hljs-string">'2021-09-01'</span>)
</code></pre>
    <p class="normal">This gives us a <code class="Code-In-Text--PACKT-">DateTimeIndex</code> like this:</p>
    <pre class="programlisting con"><code class="hljs-con">DatetimeIndex(['2021-03-24', '2021-03-25', '2021-03-26', '2021-03-27',
               '2021-03-28', '2021-03-29', '2021-03-30', '2021-03-31',
               '2021-04-01', '2021-04-02',
               ...
               '2021-08-23', '2021-08-24', '2021-08-25', '2021-08-26',
               '2021-08-27', '2021-08-28', '2021-08-29', '2021-08-30',
               '2021-08-31', '2021-09-01'],
              dtype='datetime64[ns]', length=162, freq='D') 
</code></pre>
    <p class="normal">We can also create a time-series as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">pd.Series(pd.date_range(<span class="hljs-string">"2021"</span>, freq=<span class="hljs-string">"D"</span>, periods=<span class="hljs-number">3</span>))
</code></pre>
    <p class="normal">This would give us a time-series like this:</p>
    <pre class="programlisting con"><code class="hljs-con">0   2021-01-01
1   2021-01-02
2   2021-01-03
dtype: datetime64[ns]
</code></pre>
    <p class="normal">As you can see, this type is called a <code class="Code-In-Text--PACKT-">DatetimeIndex</code>. This means we can use this data type for indexing a dataset.</p>
    <p class="normal">One of the most <a id="_idIndexMarker091"/>important functionalities is parsing to <code class="Code-In-Text--PACKT-">date</code> or <code class="Code-In-Text--PACKT-">datetime</code> objects <a id="_idIndexMarker092"/>from either <code class="Code-In-Text--PACKT-">string</code> or separate columns:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
df = pd.DataFrame({<span class="hljs-string">'year'</span>: [<span class="hljs-number">2021</span>, <span class="hljs-number">2022</span>],
    <span class="hljs-string">'month'</span>: [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>],
    <span class="hljs-string">'day'</span>: [<span class="hljs-number">24</span>, <span class="hljs-number">25</span>]}
)
ts1 = pd.to_datetime(df)
ts2 = pd.to_datetime(<span class="hljs-string">'20210324'</span>, <span class="hljs-built_in">format</span>=<span class="hljs-string">'%Y%m%d'</span>)
</code></pre>
    <p class="normal">We've created two time-series.</p>
    <p class="normal">You can take a rolling window for calculations like this:</p>
    <pre class="programlisting code"><code class="hljs-code">s = pd.Series([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])
s.rolling(<span class="hljs-number">3</span>).<span class="hljs-built_in">sum</span>()
</code></pre>
    <p class="normal">Can you guess the result of this? If not, why don't you put this into your Python interpreter? </p>
    <p class="normal">A time-series would usually be an index with a time object and one or more columns with numeric or other types, such as this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np 
rng = pd.date_range(<span class="hljs-string">'2021-03-24'</span>, <span class="hljs-string">'2021-09-01'</span>, freq=<span class="hljs-string">'D'</span>)
ts = pd.Series(np.random.randn(<span class="hljs-built_in">len</span>(rng)), index=rng)
</code></pre>
    <p class="normal">We can have a look at our time-series:</p>
    <pre class="programlisting con"><code class="hljs-con">2021-03-24   -2.332713
2021-03-25    0.177074
2021-03-26   -2.136295
2021-03-27    2.992240
2021-03-28   -0.457537
                 ...
2021-08-28   -0.705022
2021-08-29    1.089697
2021-08-30    0.384947
2021-08-31    1.003391
2021-09-01   -1.021058
Freq: D, Length: 162, dtype: float64
</code></pre>
    <p class="normal">We can index these time-series <a id="_idIndexMarker093"/>datasets like any other pandas Series or <a id="_idIndexMarker094"/>DataFrame. <code class="Code-In-Text--PACKT-">ts[:2].index</code> would give us:</p>
    <pre class="programlisting code"><code class="hljs-code">DatetimeIndex([<span class="hljs-string">'2021-03-24'</span>, <span class="hljs-string">'2021-03-25'</span>], dtype=<span class="hljs-string">'datetime64[ns]'</span>, freq=<span class="hljs-string">'D'</span>)
</code></pre>
    <p class="normal">Interestingly, we can index directly with strings or datetime objects. For example, <code class="Code-In-Text--PACKT-">ts['2021-03-28':'2021-03-30']</code> gives us:</p>
    <pre class="programlisting con"><code class="hljs-con">2021-03-28   -0.457537
2021-03-29   -1.089423
2021-03-30   -0.708091
Freq: D, dtype: float64
</code></pre>
    <p class="normal">You can shift or lag the values in a time-series back and forward in time using the <code class="Code-In-Text--PACKT-">shift</code> method. This changes the alignment of the data:</p>
    <pre class="programlisting code"><code class="hljs-code">ts.shift(<span class="hljs-number">1</span>)[:<span class="hljs-number">5</span>]
</code></pre>
    <p class="normal">We can also change the <a id="_idIndexMarker095"/>resolution of time-series objects, for example like this:</p>
    <pre class="programlisting code"><code class="hljs-code">ts.asfreq(<span class="hljs-string">'M'</span>)
</code></pre>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Please note the difference between <code class="Code-In-Text--PACKT-">datetime</code> and <code class="Code-In-Text--PACKT-">pd.DateTimeIndex</code>. Even though they encode the same kind of information, they are different types and they might not always play well with each other. Therefore, I'd recommend to always explicitly convert types when doing comparisons.</p>
    </div>
    <p class="normal">In the next section, let's go through a basic example of importing a time-series dataset, getting summary statistics, and plotting some variables.</p>
    <h1 id="_idParaDest-37" class="title">Understanding the variables</h1>
    <p class="normal">We're going to load up a time-series dataset of air pollution, then we are going to do some very basic <a id="_idIndexMarker096"/>inspection of variables.</p>
    <p class="normal">This step is performed on each variable on its own (univariate analysis) and can include summary statistics for each of the variables, histograms, finding missing values or outliers, and testing stationarity.</p>
    <p class="normal">The most important descriptors of continuous variables are the mean and the standard deviation. As a reminder, here are the formulas for the mean and the standard deviation. We are going to build on these formulas later with more complex formulas. The <strong class="keyword">mean</strong> usually refers to the arithmetic mean, which is the most commonly used average <a id="_idIndexMarker097"/>and is defined as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_001.png" alt="" style="height: 3.5em;"/></figure>
    <p class="normal">The <strong class="keyword">standard deviation</strong> is the square root of the average squared difference to this mean <a id="_idIndexMarker098"/>value:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_002.png" alt="" style="height: 4em;"/></figure>
    <p class="normal">The <strong class="keyword">standard error</strong> (<strong class="keyword">SE</strong>) is an approximation of the standard deviation of sampled data. It measures the dispersion of sample means <a id="_idIndexMarker099"/>around the population mean, but normalized by the root of the sample size. The more data points involved in the calculation, the smaller the standard error tends to be. The SE is equal to the standard deviation <a id="_idIndexMarker100"/>divided by the square root of the sample size:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_003.png" alt="" style="height: 2.5em;"/></figure>
    <p class="normal">An important application of the SE is the estimation of confidence intervals of the mean. A <strong class="keyword">confidence interval</strong> gives a range of values for a parameter. For example, the 95<sup class="Superscript--PACKT-">th </sup>percentile upper <a id="_idIndexMarker101"/>confidence limit, <img src="../Images/B17577_02_004.png" alt="" style="height: 0.9em;"/>, is defined as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_005.png" alt="" style="height: 1.9em;"/></figure>
    <p class="normal">Similarly, replacing the plus with a minus, the lower confidence interval is defined as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_006.png" alt="" style="height: 1.9em;"/></figure>
    <p class="normal">The <strong class="keyword">median</strong> is another average, particularly useful when the data can't be described accurately by the mean <a id="_idIndexMarker102"/>and standard deviations. This is the case when there's a long tail, several peaks, or a skew in one or the other direction. The median is defined as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_007.png" alt="" style="height: 2em;"/></figure>
    <p class="normal">This assumes that <em class="italic">X</em> is ordered by value in ascending or descending direction. Then, the value that lies in the middle, just at <img src="../Images/B17577_02_008.png" alt="" style="height: 1em;"/>, is the median. The median is the 50<sup class="Superscript--PACKT-">th</sup> <strong class="keyword">percentile</strong>, which means that it is higher than exactly half or 50% of the points in <em class="italic">X</em>. Other important <a id="_idIndexMarker103"/>percentiles are the 25<sup class="Superscript--PACKT-">th</sup> and the 75<sup class="Superscript--PACKT-">th</sup>, which are also the first <strong class="keyword">quartile</strong> and the <a id="_idIndexMarker104"/>third quartile. The difference between these two is <a id="_idIndexMarker105"/>called the <strong class="keyword">interquartile range</strong>.</p>
    <p class="normal">These are the most common descriptors, but not the only ones even by a long stretch. We won't go into much <a id="_idIndexMarker106"/>more detail here, but we'll see a few more descriptors later.</p>
    <p class="normal">Let's get our hands dirty with some code!</p>
    <p class="normal">We'll import datetime, pandas, matplotlib, and seaborn to use them later. Matplotlib and seaborn are libraries for plotting. Here it goes:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
</code></pre>
    <p class="normal">Then we'll read in a CSV file. The data is from the <strong class="keyword">Our World in Data</strong> (<strong class="keyword">OWID</strong>) website, a collection of statistics <a id="_idIndexMarker107"/>and articles about the state of the world, maintained by Max Roser, research director in economics at the University of Oxford. </p>
    <p class="normal">We can load local files or files on the internet. In this case, we'll load a dataset from GitHub. This is a dataset of air pollutants over time. In pandas you can pass the URL directly into the <code class="Code-In-Text--PACKT-">read_csv()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">pollution = pd.read_csv(
    <span class="hljs-string">'https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Air%20pollution%20by%20city%20-%20Fouquet%20and%20DPCC%20(2011)/Air%20pollution%20by%20city%20-%20Fouquet%20and%20DPCC%20(2011).csv'</span>
)
<span class="hljs-built_in">len</span>(pollution)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">331
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">pollution.columns
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Index(['Entity', 'Year', 'Smoke (Fouquet and DPCC (2011))',
       'Suspended Particulate Matter (SPM) (Fouquet and DPCC (2011))'],
      dtype='object')
</code></pre>
    <p class="normal">If you have problems downloading the file, you can download it manually from the book's GitHub <a id="_idIndexMarker108"/>repository from the <code class="Code-In-Text--PACKT-">chapter2</code> folder.</p>
    <p class="normal">Now we know the size of the dataset (331 rows) and the column names. The column names are a bit long, let's simplify it by renaming them and then carry on:</p>
    <pre class="programlisting code"><code class="hljs-code">pollution = pollution.rename(
    columns={
        <span class="hljs-string">'Suspended Particulate Matter (SPM) (Fouquet and DPCC (2011))'</span>:            <span class="hljs-string">'SPM'</span>,
           <span class="hljs-string">'Smoke (Fouquet and DPCC (2011))'</span> : <span class="hljs-string">'Smoke'</span>,
        <span class="hljs-string">'Entity'</span>: <span class="hljs-string">'City'</span>
    }
)
pollution.dtypes
</code></pre>
    <p class="normal">Here's the output:</p>
    <pre class="programlisting con"><code class="hljs-con">City                                object
Year                                 int64
Smoke                              float64
SPM                                float64
dtype: object
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">pollution.City.unique()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">array(['Delhi', 'London'], dtype=object)
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">pollution.Year.<span class="hljs-built_in">min</span>(), pollution.Year.<span class="hljs-built_in">max</span>()
</code></pre>
    <p class="normal">The minimum and the maximum year are these:</p>
    <pre class="programlisting con"><code class="hljs-con">(1700, 2016)
</code></pre>
    <p class="normal">pandas brings lots of methods to explore and discover your dataset – <code class="Code-In-Text--PACKT-">min()</code>, <code class="Code-In-Text--PACKT-">max()</code>, <code class="Code-In-Text--PACKT-">mean()</code>, <code class="Code-In-Text--PACKT-">count()</code>, and <code class="Code-In-Text--PACKT-">describe()</code> can all come in very handy.</p>
    <p class="normal">City, Smoke, and SPM are much clearer names for the variables. We've learned that our dataset covers two <a id="_idIndexMarker109"/>cities, London and Delhi, and over a time period between 1700 and 2016. </p>
    <p class="normal">We'll convert our Year column from <code class="Code-In-Text--PACKT-">int64</code> to <code class="Code-In-Text--PACKT-">datetime</code>. This will help with plotting:</p>
    <pre class="programlisting code"><code class="hljs-code">pollution[<span class="hljs-string">'Year'</span>] = pollution[<span class="hljs-string">'Year'</span>].apply(
    <span class="hljs-keyword">lambda</span> x: datetime.datetime.strptime(<span class="hljs-built_in">str</span>(x), <span class="hljs-string">'%Y'</span>)
)
pollution.dtypes
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">City             object
Year     datetime64[ns]
Smoke           float64
SPM             float64
dtype: object
</code></pre>
    <p class="normal">Year is now a <code class="Code-In-Text--PACKT-">datetime64[ns]</code> type. It's a <code class="Code-In-Text--PACKT-">datetime</code> of 64 bits. Each value describes a nanosecond, the default unit.</p>
    <p class="normal">Let's check for missing values and get descriptive summary statistics of columns:</p>
    <pre class="programlisting code"><code class="hljs-code">pollution.isnull().mean()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">City                               0.000000
Year                               0.000000
Smoke                              0.090634
SPM                                0.000000
dtype: float64
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">pollution.describe()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">              Smoke    SPM
count    301.000000    331.000000
mean     210.296440    365.970050
std      88.543288     172.512674
min      13.750000     15.000000
25%<span class="bash">      168.571429    288.474026</span>
50%<span class="bash">      208.214286    375.324675</span>
75%<span class="bash">      291.818182    512.609209</span>
max      342.857143    623.376623
</code></pre>
    <p class="normal">The Smoke variable has 9% missing values. For now, we can just focus on the SPM variable, which doesn't have any missing values.</p>
    <p class="normal">The pandas <code class="Code-In-Text--PACKT-">describe()</code> method gives us counts of non-null values, mean and standard deviation, 25th, 50th, and 75th percentiles, and the range as the minimum and maximum.</p>
    <p class="normal">A <strong class="keyword">histogram</strong>, first introduced by Karl Pearson, is a count of values within a series of ranges called bins (or buckets). The variable is first divided into a series of intervals, and then all points that <a id="_idIndexMarker110"/>fall into each interval are counted (bin counts). We can present these counts visually as a barplot.</p>
    <p class="normal">Let's plot a histogram of the SPM variable:</p>
    <pre class="programlisting code"><code class="hljs-code">n, bins, patches = plt.hist(
    x=pollution[<span class="hljs-string">'SPM'</span>], bins=<span class="hljs-string">'auto'</span>,
    alpha=<span class="hljs-number">0.7</span>, rwidth=<span class="hljs-number">0.85</span>
)
plt.grid(axis=<span class="hljs-string">'y'</span>, alpha=<span class="hljs-number">0.75</span>)
plt.xlabel(<span class="hljs-string">'SPM'</span>)
plt.ylabel(<span class="hljs-string">'Frequency'</span>)
</code></pre>
    <p class="normal">This is the plot we get:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_02.png" alt="pollution_hist.png"/></figure>
    <p class="packt_figref">Figure 2.3: Histogram of the SPM variable</p>
    <p class="normal">A histogram can help <a id="_idIndexMarker111"/>if you have continuous measurements and want to understand the distribution of values. Further, a histogram can indicate if there are outliers.</p>
    <p class="normal">This closes the first part of our TSA. We'll come back to our air pollution dataset later.</p>
    <h1 id="_idParaDest-38" class="title">Uncovering relationships between variables</h1>
    <p class="normal">If we are not dealing with a univariate time-series where there's only a single variable, the relationship between the variables needs to be investigated. This includes the direction and rough size of any correlations. This is important to avoid feature leakage and collinearity. </p>
    <p class="normal"><strong class="keyword">Feature leakage</strong> is when a variable unintentionally gives away the target. For example, the variable named <code class="Code-In-Text--PACKT-">amount_paid</code> would <a id="_idIndexMarker112"/>give away the label <code class="Code-In-Text--PACKT-">has_paid.</code> A more complex example would be if we were analyzing data for an online supermarket, and our dataset consisted of customer variables such as age, number of purchases in the past, length of visit, and finally the contents of their cart. What we want to predict, our target, is the result of their buying decision as either abandoned (when they canceled <a id="_idIndexMarker113"/>their purchase) or paid. We could find that a purchase is highly correlated with bags in their cart due to just the simple fact that bags are added at the last step. However, concluding we should offer bags to customers when they land on our <a id="_idIndexMarker114"/>site would probably miss the point, when it's the length of their stay that could be, in fact, the determining variable, and an intervention through a widget or customer service agent might be much more effective.</p>
    <p class="normal"><strong class="keyword">Collinearity</strong> means that independent variables (features) are correlated. The latter case can be problematic <a id="_idIndexMarker115"/>in linear models. Therefore, if we carry out linear regression and find two variables that are highly correlated between themselves, we should remove one of them or use dimensionality reduction techniques such as Principal Component Analysis (PCA).</p>
    <p class="normal">The <strong class="keyword">Pearson correlation</strong> coefficient was developed by Karl Pearson, whom we've discussed in the previous chapter, and named in his honor to distinguish it from other correlation coefficients. The Pearson correlation coefficient <a id="_idIndexMarker116"/>between two variables <em class="italic">X</em> and <em class="italic">Y</em> is defined as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_009.png" alt="" style="height: 3em;"/></figure>
    <p class="normal"><img src="../Images/B17577_02_010.png" alt="" style="height: 1em;"/> is the covariance between the two variables defined as the expected value (the mean) between the differences of each point to the variable mean: </p>
    <figure class="mediaobject"><img src="../Images/B17577_02_011.png" alt="" style="height: 1.8em;"/></figure>
    <p class="normal"><img src="../Images/B17577_02_012.png" alt="" style="height: 1em;"/> is the standard deviation of the variable <em class="italic">X</em>. </p>
    <p class="normal">Expanded, the formula looks like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_013.png" alt="" style="height: 3.5em;"/></figure>
    <p class="normal">There are three types of correlation: positive, negative, and no correlation. Positive correlation means <a id="_idIndexMarker117"/>that as one variable increases the other does as well. In the case of the Pearson correlation coefficient, the increase of one variable to the other should be linear. </p>
    <p class="normal">If we looked at a plot of global life expectancy from 1800 onward, we'd see an increase of years lived with the <a id="_idIndexMarker118"/>time axis. You can see the plot of global life expectancy based on data on OWID:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_03.png" alt="life_expectancy.png"/></figure>
    <p class="packt_figref">Figure 2.4: Life expectancy from 1800 to today</p>
    <p class="normal">We can see how life expectancy has been increasing steadily since the end of the 19<sup class="Superscript--PACKT-">th</sup> century until today.</p>
    <p class="normal">This plot is <a id="_idIndexMarker119"/>called a <strong class="keyword">run chart</strong> or temporal <strong class="keyword">line chart</strong>.</p>
    <p class="normal">In order to calculate <a id="_idIndexMarker120"/>the Pearson correlation, we can use a function from SciPy:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">ignore_nans</span><span class="hljs-function">(</span><span class="hljs-params">a, b</span><span class="hljs-function">):</span>
    index = ~a.isnull() &amp; ~b.isnull()
    <span class="hljs-keyword">return</span> a[index], b[index]
stats.pearsonr(*ignore_nans(pollution[<span class="hljs-string">'Smoke'</span>], pollution[<span class="hljs-string">'SPM'</span>]))
</code></pre>
    <p class="normal">Here's the Pearson correlation and the p-value that indicates significance (the lower, the more significant)</p>
    <pre class="programlisting con"><code class="hljs-con">(0.9454809183096181, 3.313283689287137e-10
</code></pre>
    <p class="normal">We see a very strong positive <a id="_idIndexMarker121"/>correlation of time with life expectancy, 0.94, at very high significance (the second number in the return). You can find more details about the dataset on the OWID website.</p>
    <p class="normal">Conversely, we would see a negative correlation of time with child mortality – as the year increases, child mortality decreases. This plot shows the child mortality per 1,000 children on data taken from OWID:</p>
    <p class="packt_figref"><img src="../Images/B17577_02_04.png" alt="child_mortality.png"/></p>
    <p class="packt_figref">Figure 2.5: Child mortality from 1800 to today in the UK, France, and the USA</p>
    <p class="normal">In this plot, we can see that <a id="_idIndexMarker122"/>in all three countries child mortality has been decreasing since the start of the 19<sup class="Superscript--PACKT-">th</sup> century until today.</p>
    <p class="normal">In the case of the United States, we'll find a negative correlation of -0.95 between child mortality and time.</p>
    <p class="normal">We can also compare the <a id="_idIndexMarker123"/>countries to each other. We can calculate correlations between each feature. In this case, each feature contains the values for the three countries. </p>
    <p class="normal">This gives a <strong class="keyword">correlation matrix</strong> of 3x3, which we can visualize as a heatmap:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_05.png" alt="correlation_heatmap.png"/></figure>
    <p class="packt_figref">Figure 2.6: Correlation heatmap of child mortality between the UK, France, and the USA</p>
    <p class="normal">In this correlation heatmap, we can see that countries are highly correlated (for example, a correlation of 0.78 between France and the United Kingdom). </p>
    <p class="normal">The diagonal of the correlation matrix is always 1.0, and the matrix is symmetrical across the diagonal. Therefore, sometimes we only show the lower triangle below the diagonal (or sometimes the upper triangle). We can see that child mortality in the United Kingdom is more similar <a id="_idIndexMarker124"/>to that of the United States than that of France. </p>
    <p class="normal">Does this mean that the UK went through a similar development as the United States? These statistics and visualizations can often generate questions to answer, or hypotheses that we can test.</p>
    <p class="normal">As mentioned before, the full notebooks for the different datasets are available on GitHub, however, here's the snippet for the heatmap:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> dython
dython.nominal.associations(child_mortality[countries], figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>));
</code></pre>
    <p class="normal">The correlation coefficient struggles with cases where the increases are non-linear or non-continuous, or (because of the squared term) when there are outliers. For example, if we looked at air pollution from the 1700s onward, we'd see a steep increase in air pollutants from coal and – with the introduction of the steam engine – a decrease in pollutants. </p>
    <p class="normal">A <strong class="keyword">scatter plot</strong> can be used for showing and comparing numeric values. It plots values of two variables against each other. Usually, the variables are numerical – otherwise, we'd call this a table. Scatter plots can be crowded in certain areas, and therefore are deceptive if this can't be <a id="_idIndexMarker125"/>appreciated visually. Adding jitter and transparency can help to some degree, however, we can combine a scatter plot with the histograms of the variables we are plotting against each other, so we can see how many points on one or the other variable are being displayed. Scatter plots often have a best-fit line superimposed in order to visualize how one variable is the function of another variable.</p>
    <p class="normal">Here's an example of how to plot a scatter plot with marginal histograms of the two variables in the pollution dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
sns.jointplot(
    x=<span class="hljs-string">"Smoke"</span>, y=<span class="hljs-string">"SPM"</span>,
    edgecolor=<span class="hljs-string">"white"</span>,
    data=pollution
)
plt.xlabel(<span class="hljs-string">"Smoke"</span>)
plt.ylabel(<span class="hljs-string">"SPM"</span>);
</code></pre>
    <p class="normal">Here's the resulting plot:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_06.png" alt="Machine-Learning%20for%20Time-Series%20with%20Python/spm_scatter.png"/></figure>
    <p class="packt_figref">Figure 2.7: Scatter plot with marginal histograms of Smoke against SPM</p>
    <p class="normal">In the scatter plot, we can see that the two variables are extremely similar – the values are all on the diagonal. The correlation between these two variables is perfect, 1.0, which means that they are in fact identical.</p>
    <p class="normal">We've seen the dataset of <strong class="keyword">Suspended Particulate Matter </strong>(<strong class="keyword">SPM</strong>) before. Let's plot SPM over <a id="_idIndexMarker126"/>time:</p>
    <pre class="programlisting code"><code class="hljs-code">pollution = pollution.pivot(<span class="hljs-string">"Year"</span>, <span class="hljs-string">"City"</span>, <span class="hljs-string">"SPM"</span>)
plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
sns.lineplot(data=pollution)
plt.ylabel(<span class="hljs-string">'SPM'</span>);
</code></pre>
    <p class="normal">Here's the plot:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_07.png" alt="Machine-Learning%20for%20Time-Series%20with%20Python/spm_1700_to_today.png"/></figure>
    <p class="packt_figref">Figure 2.8: Suspended particle matter from the 1700s to today</p>
    <p class="normal">We can see in the plot that the air quality (measured as suspended particle matter) in London was getting worse until around 1880 (presumably because of heating materials such as wood and coal), however, has since been improving. </p>
    <p class="normal">We find a correlation coefficient of -0.36 with high significance. The steep decline of pollutants from 1880 onward dominates over the 180 years of slow growth. If we looked separately at the time from 1700 to 1880 and from 1880 to the present, we'd find 0.97 and -0.97 respectively, examples of very strong correlation and very strong anti-correlation.</p>
    <p class="normal">The <strong class="keyword">Spearman rank correlation</strong> can handle outliers and non-linear relationships much better than the Pearson correlation coefficient – although it can't handle non-continuous cases like the one above. The Spearman correlation is the Pearson correlation, only applied <a id="_idIndexMarker127"/>on ranks of variables' values instead of the variables' values directly. The Spearman correlation of the time-series for air pollution is -0.19, and for the two time periods before and after 1880 we get 0.99 and -0.99, respectively. </p>
    <p class="normal">In the case of the Spearman correlation coefficient, the numerical differences are ignored – what counts is the order of the points. In this case, the order of the points within the two <a id="_idIndexMarker128"/>time periods aligns nearly perfectly.</p>
    <p class="normal">In the next section, we'll talk about trend and seasonality.</p>
    <h1 id="_idParaDest-39" class="title">Identifying trend and seasonality</h1>
    <p class="normal">Trend, seasonality, and cyclic variations are the most important characteristics of time-series. A <strong class="keyword">trend</strong> is the presence <a id="_idIndexMarker129"/>of a long-term increase or decrease in the sequence. <strong class="keyword">Seasonality</strong> is a variation <a id="_idIndexMarker130"/>that occurs at specific regular intervals of less than a <a id="_idIndexMarker131"/>year. Seasonality can occur on different time spans <a id="_idIndexMarker132"/>such as daily, weekly, monthly, or yearly. Finally, <strong class="keyword">cyclic variations</strong> are rises and falls that are not of a fixed frequency.</p>
    <p class="normal">An important characteristic of time-series is <strong class="keyword">stationarity</strong>. This refers to a property of time-series not to <a id="_idIndexMarker133"/>change distribution over time, or in other words, that the process that produces the time-series doesn't change with time. Time-Series that don't change over time are called <strong class="keyword">stationary </strong>(or <strong class="keyword">stationary processes</strong>). Many models or measures <a id="_idIndexMarker134"/>assume stationarity and might not work properly if the data is not stationary. Therefore, with these algorithms, the time-series should be decomposed first into <a id="_idIndexMarker135"/>the main signal, and then the seasonal and trend components. In this decomposition, we would subtract the trend and seasonal components from the original time-series.</p>
    <p class="normal">In this section, we'll first go through an example of how to estimate trend and seasonality using curve fitting. Then, we'll look at other tools that can help discover trends, seasonality, and cyclic variations. These include statistics such as autocorrelation and the augmented Dickey–Fuller test, and visualizations such as the autocorrelation plot (also: lag plot) and the periodogram.</p>
    <p class="normal">Let's start with a hopefully clear example of how seasonality and trend can be estimated in just a few lines of Python. For this, we'll come back to the GISS Surface Temperature Analysis dataset released by NASA. We'll load the dataset, and we'll do curve fitting, which comes straight out of the box in NumPy.</p>
    <p class="normal">We'll download the dataset from Datahub (<a href="https://datahub.io/core/global-temp"><span class="url">https://datahub.io/core/global-temp</span></a>) or you can find <a id="_idIndexMarker136"/>it from the book's GitHub repository (in the <code class="Code-In-Text--PACKT-">chapter2</code> folder). </p>
    <p class="normal">Then, we can load it up and pivot it:</p>
    <pre class="programlisting code"><code class="hljs-code">temperatures = pd.read_csv(<span class="hljs-string">'/Users/ben/Downloads/monthly_csv.csv'</span>)
temperatures[<span class="hljs-string">'Date'</span>] = pd.to_datetime(temperatures[<span class="hljs-string">'Date'</span>])
temperatures = temperatures.pivot(<span class="hljs-string">'Date'</span>, <span class="hljs-string">'Source'</span>, <span class="hljs-string">'Mean'</span>)
</code></pre>
    <p class="normal">Now we can <a id="_idIndexMarker137"/>use NumPy's polyfit functionality. It fits a polynomial <a id="_idIndexMarker138"/>of the form:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_014.png" alt="" style="height: 3.5em;"/></figure>
    <p class="normal">In this formula, <em class="italic">k</em> is the degree of the polynomial and <em class="italic">b</em> is the coefficients we are trying to find.</p>
    <p class="normal">It is just a function in NumPy to fit the coefficients. We can use the same function to fit seasonal variation and trend. Since trend can dominate over seasonality, before estimating seasonality, we remove the trend:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> polyfit
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">fit</span><span class="hljs-function">(</span><span class="hljs-params">X, y, degree=</span><span class="hljs-number">3</span><span class="hljs-function">):</span>
    coef = polyfit(X, y, degree)
    trendpoly = np.poly1d(coef)
    <span class="hljs-keyword">return</span> trendpoly(X)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_season</span><span class="hljs-function">(</span><span class="hljs-params">s, yearly_periods=</span><span class="hljs-number">4</span><span class="hljs-params">, degree=</span><span class="hljs-number">3</span><span class="hljs-function">):</span>
    X = [i%(<span class="hljs-number">365</span>/<span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(s))]
    seasonal = fit(X, s.values, degree)
    <span class="hljs-keyword">return</span> pd.Series(data=seasonal, index=s.index)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_trend</span><span class="hljs-function">(</span><span class="hljs-params">s, degree=</span><span class="hljs-number">3</span><span class="hljs-function">):</span>
    X = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(s)))
    trend = fit(X, s.values, degree)
    <span class="hljs-keyword">return</span> pd.Series(data=trend, index=s.index)
</code></pre>
    <p class="normal">Let's plot seasonality and <a id="_idIndexMarker139"/>trend on top of our global temperature increases!</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
temperatures[<span class="hljs-string">'trend'</span>] = get_trend(temperatures[<span class="hljs-string">'GCAG'</span>])
temperatures[<span class="hljs-string">'season'</span>] = get_season(temperatures[<span class="hljs-string">'GCAG'</span>] - temperatures[<span class="hljs-string">'trend'</span>])
sns.lineplot(data=temperatures[[<span class="hljs-string">'GCAG'</span>, <span class="hljs-string">'season'</span>, <span class="hljs-string">'trend'</span>]])
plt.ylabel(<span class="hljs-string">'Temperature change'</span>);
</code></pre>
    <p class="normal">This is the <a id="_idIndexMarker140"/>graph that we get:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_08.png" alt="temperatures_trend_seasonality.png"/></figure>
    <p class="packt_figref">Figure 2.9: Temperature change from the late 19<sup class="Superscript--PACKT-">th</sup> century to today</p>
    <p class="normal">This was to show that you can use plug-in functionality in NumPy for curve fitting in order to find both trend and seasonality. If you want to experiment further, you can play with the degree of the <a id="_idIndexMarker141"/>polynomial or with the seasonality component to see if you can get a better fit, or find another seasonality component. We could have used <a id="_idIndexMarker142"/>functionality from other libraries such as <code class="Code-In-Text--PACKT-">seasonal.seasonal_decompose()</code> in <code class="Code-In-Text--PACKT-">statsmodels</code>, or Facebook's Prophet, which decomposes using Fourier coefficients for the seasonal components.</p>
    <p class="normal">Now that we've seen how to estimate seasonality and trend, let's move on to other statistics and visualizations. Continuing with the pollution dataset, and picking up the EEG dataset we saw in <em class="italic">Chapter 1</em>, we'll show practically in Python how to get these statistics and plots, and how to identify trend and seasonality.</p>
    <p class="normal"><strong class="keyword">Autocorrelation</strong> is the correlation of a signal with a lagged version of itself. The autocorrelation plot <a id="_idIndexMarker143"/>draws the autocorrelation as a function of lag. The autocorrelation plot can help find repeating patterns, and is often used in signal processing. The autocorrelation can help spot a periodic signal. Let's plot the autocorrelation of the pollution data:</p>
    <pre class="programlisting code"><code class="hljs-code">pollution = pollution.pivot(<span class="hljs-string">"Year"</span>, <span class="hljs-string">"City"</span>, <span class="hljs-string">"SPM"</span>)
pd.plotting.autocorrelation_plot(pollution[<span class="hljs-string">'London'</span>])
</code></pre>
    <p class="normal">Here's the <a id="_idIndexMarker144"/>plot that we get:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_09.png" alt="autocorrelation.png"/></figure>
    <p class="packt_figref">Figure 2.10: Autocorrelaton plot of pollution in London</p>
    <p class="normal">We can see high autocorrelations with a lag of only a few years. There is a negative autocorrelation <a id="_idIndexMarker145"/>at around 100 years, after which point the autocorrelation stays around 0.</p>
    <p class="normal">The plot of SPM clearly <a id="_idIndexMarker146"/>shows that air pollution is not a stationary process, since the autocorrelation is not flat. You can also compare the run of pollution that shows there's a <strong class="keyword">trend</strong>, and therefore the mean also changes – another indication that the series is not stationary.</p>
    <p class="normal">We can also test this statistically. A test for stationarity is the augmented Dickey–Fuller test:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> statsmodels.tsa <span class="hljs-keyword">import</span> stattools
stattools.adfuller(pollution[<span class="hljs-string">'London'</span>])
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">(-0.33721640804242853,
 0.9200654843183897,
 13,
 303,
 {'1%': -3.4521175397304784,
  '5%': -2.8711265007266666,
  '10%': -2.571877823851692},
 1684.6992663493872)
</code></pre>
    <p class="normal">The second return <a id="_idIndexMarker147"/>value is the p-value that gives the significance or the probability of obtaining test results at least as extreme as the observation <a id="_idIndexMarker148"/>given the null hypothesis. With p-values below 5% or 0.05 we would typically reject the null hypothesis, and we could assume that our time-series is stationary. In our case, we can't assume that the series is stationary.</p>
    <p class="normal">We saw the graph of <strong class="keyword">electroencephalography</strong> (<strong class="keyword">EEG</strong>) signals in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Time-Series with Python</em>, and we mentioned that EEG signals show <a id="_idIndexMarker149"/>brain waves at several frequency ranges.</p>
    <p class="normal">We can visualize this nicely. Let's go through it step by step in Python. We first need to do a few imports:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> matplotlib.dates <span class="hljs-keyword">import</span> DateFormatter
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_openml
</code></pre>
    <p class="normal">OpenML is a project that provides benchmark datasets and a website for comparison of machine learning algorithms. The scikit-learn library provides an interface to OpenML that allows us to <a id="_idIndexMarker150"/>fetch data from OpenML. The whole measurement <a id="_idIndexMarker151"/>spans 117 seconds. So we need to set this up correctly as an index in pandas:</p>
    <pre class="programlisting code"><code class="hljs-code">eeg = fetch_openml(data_id=<span class="hljs-number">1471</span>, as_frame=<span class="hljs-literal">True</span>)
increment = <span class="hljs-number">117</span> / <span class="hljs-built_in">len</span>(eeg[<span class="hljs-string">'data'</span>])
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
index = np.linspace(
    start=<span class="hljs-number">0</span>,
    stop=increment*<span class="hljs-built_in">len</span>(eeg[<span class="hljs-string">'data'</span>]),
    num=<span class="hljs-built_in">len</span>(eeg[<span class="hljs-string">'data'</span>])
)
ts_index = pd.to_datetime(index, unit=<span class="hljs-string">'s'</span>)
v1 = pd.Series(name=<span class="hljs-string">'V1'</span>, data=eeg[<span class="hljs-string">'data'</span>][<span class="hljs-string">'V1'</span>].values, index=ts_index)
</code></pre>
    <p class="normal">We can slice our dataset directly. Please note that the <code class="Code-In-Text--PACKT-">DatetimeIndex</code> is anchored in 1970, but we can ignore this safely here:</p>
    <pre class="programlisting code"><code class="hljs-code">slicing = (v1.index &gt;= <span class="hljs-string">'1970-01-01 00:00:08'</span>) &amp; (v1.index &lt;=<span class="hljs-string">'1970-01-01 00:01:10.000000000'</span>)
v1[slicing]
</code></pre>
    <p class="normal">Here's the slice:</p>
    <pre class="programlisting con"><code class="hljs-con">1970-01-01 00:00:08.006208692    4289.74
1970-01-01 00:00:08.014019627    4284.10
1970-01-01 00:00:08.021830563    4280.00
1970-01-01 00:00:08.029641498    4289.74
1970-01-01 00:00:08.037452433    4298.46
                                  ...   
1970-01-01 00:01:09.962547567    4289.74
1970-01-01 00:01:09.970358502    4283.08
1970-01-01 00:01:09.978169437    4284.62
1970-01-01 00:01:09.985980373    4289.23
1970-01-01 00:01:09.993791308    4290.77
Name: V1, Length: 7937, dtype: float64
</code></pre>
    <p class="normal">This slicing avoids <a id="_idIndexMarker152"/>an artifact, a strong spike, occurring at around 1:20.</p>
    <p class="normal">The graph we <a id="_idIndexMarker153"/>saw in <em class="italic">Chapter 1</em>, we can plot as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">date_formatter = DateFormatter(<span class="hljs-string">"%S"</span>)
ax = v1[slicing].plot(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
ax.xaxis.set_major_formatter(date_formatter)
plt.ylabel(<span class="hljs-string">'voltage'</span>)
</code></pre>
    <p class="normal">Here's the graph again:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_10.png" alt="Machine-Learning%20for%20Time-Series%20with%20Python/voltage_over_time.png"/></figure>
    <p class="packt_figref">Figure 2.11: Voltage over time in an EEG signal</p>
    <p class="normal">This is the plot of the EEG signal over time. </p>
    <p class="normal">We can also resample the data to look at the series more coarsely, with less resolution, for example like this:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.subplot(<span class="hljs-number">311</span>)
ax1 = v1[slicing].resample(<span class="hljs-string">'1s'</span>).mean().plot(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
ax1.xaxis.set_major_formatter(date_formatter)
plt.subplot(<span class="hljs-number">312</span>)
ax1 = v1[slicing].resample(<span class="hljs-string">'2s'</span>).mean().plot(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
ax1.xaxis.set_major_formatter(date_formatter)
plt.subplot(<span class="hljs-number">313</span>)
ax2 = v1[slicing].resample(<span class="hljs-string">'5s'</span>).mean().plot(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
ax2.xaxis.set_major_formatter(date_formatter) 
plt.xlabel(<span class="hljs-string">'seconds'</span>);
</code></pre>
    <p class="normal">This is the graph with <a id="_idIndexMarker154"/>three subplots we get from resampling to frequencies of 1 second, 2 seconds, and 5 seconds, respectively:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_11.png" alt="eeg_resampled.png"/></figure>
    <p class="packt_figref">Figure 2.12: Resampled EEG signals</p>
    <p class="normal">Each of the resampled <a id="_idIndexMarker155"/>signals in the plot could be more or less useful for analysis depending on the application. For high-frequency analysis, we shouldn't resample at all, while if we are trying to remove as much noise as possible, we should resample to a more coarse time resolution.</p>
    <p class="normal">We can look at cyclic activity on a plot of spectral density. We can do this by applying a Fourier transform. Here, we go <a id="_idIndexMarker156"/>with the Welch method, which averages over time before applying the discrete Fourier transform:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> signal
fs = <span class="hljs-built_in">len</span>(eeg[<span class="hljs-string">'data'</span>]) // <span class="hljs-number">117</span>
f, Pxx_den = signal.welch(
    v1[slicing].values,
    fs,
    nperseg=<span class="hljs-number">2048</span>,
    scaling=<span class="hljs-string">'spectrum'</span>
)
plt.semilogy(f, Pxx_den)
plt.xlabel(<span class="hljs-string">'frequency [Hz]'</span>)
plt.ylabel(<span class="hljs-string">'PSD [V**2/Hz]'</span>)
</code></pre>
    <p class="normal">The spectral density plot, the <strong class="keyword">periodogram</strong>, looks like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_02_12.png" alt="spectral_eeg.png"/></figure>
    <p class="packt_figref">Figure 2.13: Periodogram of the EEG signals</p>
    <p class="normal">The information in <a id="_idIndexMarker157"/>this plot is like the autocorrelation plot that we drew for pollution, however, it gives us <a id="_idIndexMarker158"/>information about how prominent certain frequencies are. In this case we see that low frequencies are particularly powerful. In other words, the signal shows a slow oscillation.</p>
    <p class="normal">This brings the chapter to an end. Let's summarize what we've covered.</p>
    <h1 id="_idParaDest-40" class="title">Summary</h1>
    <p class="normal">In this chapter, we introduced TSA as the process of extracting summary and other statistical information from time-series. We broke this process down into understanding the variables, uncovering relationships between variables, and identifying trend and seasonality.</p>
    <p class="normal">We introduced datetime and pandas, the libraries <em class="italic">sine qua non</em> in TSA, and their functionalities for time-series; for example, resampling. Throughout the chapter, we listed and defined many summary statistics including mean, standard deviation, median, SE, confidence interval, Pearson correlation, and covariance.</p>
    <p class="normal">We also talked about the concepts of seasonality, cyclic variation, and stationarity. We discussed why stationarity is important, and how to test for stationarity.</p>
    <p class="normal">We also showed plotting functionality with Matplotlib and Seaborn, and how to generate different plots such as run charts, temporal line charts, correlation heatmaps, histograms, scatter plots, autocorrelation plots, and periodograms. In the practical example, we used an autocorrelation plot, which shows the correlation between different time steps, and the periodogram, which visualizes the power spectral density.</p>
  </div>
</body></html>
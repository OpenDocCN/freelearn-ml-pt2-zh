["```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nnp.random.seed(123456)\ndata = pd.read_csv('creditcard.csv')\ndata.Time = (data.Time-data.Time.min())/data.Time.std()\ndata.Amount = (data.Amount-data.Amount.mean())/data.Amount.std()\n\n# Train-Test slpit of 70%-30%\nx_train, x_test, y_train, y_test = train_test_split(\ndata.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\n```", "```py\n# --- SECTION 2 ---\n# Base learners evaluation\nbase_classifiers = [('DT', DecisionTreeClassifier(max_depth=3)),\n                    ('NB', GaussianNB()),\n                    ('LR', LogisticRegression())]\n\nfor bc in base_classifiers:\n lr = bc[1]\n lr.fit(x_train, y_train)\n\n predictions = lr.predict(x_test)\n print(bc[0]+' f1', metrics.f1_score(y_test, predictions))\n print(bc[0]+' recall', metrics.recall_score(y_test, predictions)) \n```", "```py\n# --- SECTION 3 ---\n# Filter features according to their correlation to the target\nnp.random.seed(123456)\nthreshold = 0.1\n\ncorrelations = data.corr()['Class'].drop('Class')\nfs = list(correlations[(abs(correlations)>threshold)].index.values)\nfs.append('Class')\ndata = data[fs]\n\nx_train, x_test, y_train, y_test = train_test_split(data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\n\nfor bc in base_classifiers:\n lr = bc[1]\n lr.fit(x_train, y_train)\n\n predictions = lr.predict(x_test)\n print(bc[0]+' f1', metrics.f1_score(y_test, predictions))\n print(bc[0]+' recall', metrics.recall_score(y_test, predictions))\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nnp.random.seed(123456)\ndata = pd.read_csv('creditcard.csv')\ndata.Time = (data.Time-data.Time.min())/data.Time.std()\ndata.Amount = (data.Amount-data.Amount.mean())/data.Amount.std()\n\n# Train-Test slpit of 70%-30%\nx_train, x_test, y_train, y_test = train_test_split(\n data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\n```", "```py\n\n# --- SECTION 2 ---\n# Ensemble evaluation\nbase_classifiers = [('DT', DecisionTreeClassifier(max_depth=5)),\n ('NB', GaussianNB()),\n ('ensemble', LogisticRegression())]\n\nensemble = VotingClassifier(base_classifiers)\nensemble.fit(x_train, y_train)\n\nprint('Voting f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Voting recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n\n# --- SECTION 3 ---\n# Filter features according to their correlation to the target\nnp.random.seed(123456)\nthreshold = 0.1\n\ncorrelations = data.corr()['Class'].drop('Class')\nfs = list(correlations[(abs(correlations)>threshold)].index.values)\nfs.append('Class')\ndata = data[fs]\n\nx_train, x_test, y_train, y_test = train_test_split(\n data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\n\nensemble = VotingClassifier(base_classifiers)\nensemble.fit(x_train, y_train)\n\nprint('Voting f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Voting recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\nimport pandas as pd \n\nfrom stacking_classifier import Stacking\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nnp.random.seed(123456)\ndata = pd.read_csv('creditcard.csv')\ndata.Time = (data.Time-data.Time.min())/data.Time.std()\ndata.Amount = (data.Amount-data.Amount.mean())/data.Amount.std()\n\n# Train-Test slpit of 70%-30%\nx_train, x_test, y_train, y_test = train_test_split(\n data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\n```", "```py\n# --- SECTION 2 ---\n# Ensemble evaluation\nbase_classifiers = [DecisionTreeClassifier(max_depth=5),\n                    GaussianNB(),\n                    LogisticRegression()]\nensemble = Stacking(learner_levels=[base_classifiers, \n                                   [LogisticRegression()]])\n\nensemble.fit(x_train, y_train)\nprint('Stacking f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Stacking recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n\n# --- SECTION 3 ---\n# Filter features according to their correlation to the target\nnp.random.seed(123456)\nthreshold = 0.1\ncorrelations = data.corr()['Class'].drop('Class')\nfs = list(correlations[(abs(correlations) > threshold)].index.values)\nfs.append('Class')\ndata = data[fs]\nx_train, x_test, y_train, y_test = train_test_split(data.drop('Class', axis=1).values, \n                                                    data.Class.values, test_size=0.3)\nensemble = Stacking(learner_levels=[base_classifiers,\n                                   [LogisticRegression()]])\nensemble.fit(x_train, y_train)\nprint('Stacking f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Stacking recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nnp.random.seed(123456)\ndata = pd.read_csv('creditcard.csv')\ndata.Time = (data.Time-data.Time.min())/data.Time.std()\ndata.Amount = (data.Amount-data.Amount.mean())/data.Amount.std()\n# Train-Test slpit of 70%-30%\nx_train, x_test, y_train, y_test = train_test_split(\n                                   data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\n```", "```py\n# --- SECTION 2 ---\n# Ensemble evaluation\nensemble = BaggingClassifier(n_estimators=10,\nbase_estimator=DecisionTreeClassifier(max_depth=5))\nensemble.fit(x_train, y_train)\nprint('Bagging f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Bagging recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n# --- SECTION 3 ---\n# Filter features according to their correlation to the target\nnp.random.seed(123456)\nthreshold = 0.1\ncorrelations = data.corr()['Class'].drop('Class')\nfs = list(correlations[(abs(correlations)>threshold)].index.values)\nfs.append('Class')\ndata = data[fs]\nx_train, x_test, y_train, y_test = train_test_split(\n                                    data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\nensemble = BaggingClassifier(n_estimators=10,\nbase_estimator=DecisionTreeClassifier(max_depth=5))\nensemble.fit(x_train, y_train)\n\nprint('Bagging f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('Bagging recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\n\nnp.random.seed(123456)\ndata = pd.read_csv('creditcard.csv')\ndata.Time = (data.Time-data.Time.min())/data.Time.std()\ndata.Amount = (data.Amount-data.Amount.mean())/data.Amount.std()\n# Train-Test slpit of 70%-30%\nx_train, x_test, y_train, y_test = train_test_split(\n data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\n```", "```py\n# --- SECTION 2 ---\n# Ensemble evaluation\nensemble = AdaBoostClassifier(n_estimators=70, learning_rate=1.0)\nensemble.fit(x_train, y_train)\nprint('AdaBoost f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('AdaBoost recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n```", "```py\n# --- SECTION 3 ---\n# Filter features according to their correlation to the target\nnp.random.seed(123456)\nthreshold = 0.1\ncorrelations = data.corr()['Class'].drop('Class')\nfs = list(correlations[(abs(correlations)>threshold)].index.values)\nfs.append('Class')\ndata = data[fs]\nx_train, x_test, y_train, y_test = train_test_split(\n data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\nensemble = AdaBoostClassifier(n_estimators=70, learning_rate=1.0)\nensemble.fit(x_train, y_train)\nprint('AdaBoost f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('AdaBoost recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\n\nnp.random.seed(123456)\ndata = pd.read_csv('creditcard.csv')\ndata.Time = (data.Time-data.Time.min())/data.Time.std()\ndata.Amount = (data.Amount-data.Amount.mean())/data.Amount.std()\nnp.random.seed(123456)\ndata = pd.read_csv('creditcard.csv')\ndata.Time = (data.Time-data.Time.min())/data.Time.std()\ndata.Amount = (data.Amount-data.Amount.mean())/data.Amount.std()\n# Train-Test slpit of 70%-30%\nx_train, x_test, y_train, y_test = train_test_split(\n data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\n```", "```py\n# --- SECTION 2 ---\n# Ensemble evaluation\nensemble = RandomForestClassifier(n_jobs=4)\nensemble.fit(x_train, y_train)\nprint('RF f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('RF recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n\n# --- SECTION 3 ---\n# Filter features according to their correlation to the target\nnp.random.seed(123456)\nthreshold = 0.1\ncorrelations = data.corr()['Class'].drop('Class')\nfs = list(correlations[(abs(correlations)>threshold)].index.values)\nfs.append('Class')\ndata = data[fs]\nx_train, x_test, y_train, y_test = train_test_split(\n data.drop('Class', axis=1).values, data.Class.values, test_size=0.3)\nensemble = RandomForestClassifier(n_jobs=4)\nensemble.fit(x_train, y_train)\nprint('RF f1', metrics.f1_score(y_test, ensemble.predict(x_test)))\nprint('RF recall', metrics.recall_score(y_test, ensemble.predict(x_test)))\n```"]
<html><head></head><body>
<section id="chapter-9-bayesian-additive-regression-trees" class="level2 chapterHead" data-number="1.13">&#13;
<h1 class="chapterHead" data-number="1.13">Chapter 9<br/>&#13;
<span id="x1-1760009"/>Bayesian Additive Regression Trees</h1>&#13;
<blockquote>&#13;
<p>Individually, we are one drop. Together, we are an ocean. – Ryunosuke Satoro</p>&#13;
</blockquote>&#13;
<p>In the last chapter, we discussed the <strong>Gaussian process</strong> (<strong>GPs</strong>), a non-parametric model for regression. In this chapter, we will learn about another non-parametric model for regression known as Bayesian additive regression trees, or BART to friends. We can consider BART from many different perspectives. It can be an ensemble of decision trees, each with a distinct role and contribution to the overall understanding of the data. These trees, guided by Bayesian priors, work harmoniously to capture the nuances of the data, avoiding the pitfall of individual overfitting. Usually, BART is discussed as a standalone model, and software that implements it is usually limited to one or a few models. In this chapter, we will take a different approach and use PyMC-BART, a Python library that allows the use of BART models within PyMC.</p>&#13;
<p>In this chapter, we will cover the following topics:</p>&#13;
<ul>&#13;
<li><p>Decision trees</p></li>&#13;
<li><p>BART models</p></li>&#13;
<li><p>Flexible regression with BART</p></li>&#13;
<li><p>Partial dependence plots</p></li>&#13;
<li><p>Individual conditional expectation plots</p></li>&#13;
<li><p>Variable selection</p></li>&#13;
</ul>&#13;
<p><span id="x1-176001r423"/></p>&#13;
<section id="decision-trees" class="level3 sectionHead" data-number="1.13.1">&#13;
<h2 class="sectionHead" data-number="1.13.1">9.1 <span id="x1-1770001"/>Decision trees</h2>&#13;
<p><span id="dx1-177001"/></p>&#13;
<p>Before jumping into BART models, let’s take a moment to discuss what decision trees are. A decision tree is like a flowchart that guides you through different questions until you reach a final choice. For instance, suppose you need to decide what type of shoes to wear every morning. To do so, you may ask yourself a series of questions. ”Is it warm?” If yes, you then ask something more specific, like ”Do I have to go to the office?” Eventually, you will stop asking questions and reach an output value like flip-flops, sneakers, boots, moccasins, etc.</p>&#13;
<p>This flowchart can be conveniently encoded in a tree structure, where at the root of the tree we place more general questions, then proceed along the tree to more and more specific ones, and finally arrive at the leaves of the tree with the output of the different types of shoes. Trees are very common data structures in computer science and data analysis.</p>&#13;
<p>More formally, we can say that a tree is a collection of nodes and vertices linking those nodes. The nodes that have questions are called decision nodes, and the <span id="dx1-177002"/>nodes with the output of the trees (like the shoes) are called leaf nodes. When the answers are ”yes” or ”no,” then we have a binary tree, because each node can have at most two children. <em>Figure <a href="#x1-177003r1">9.1</a></em> shows a decision tree. The rounded squares are leaf nodes. The regular squares are decision nodes.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file240.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-177003r1"/><strong>Figure 9.1</strong>: A decision tree to choose footwear.</p>&#13;
<p>We can use decision trees to classify, that is, to return discrete categories, like sneakers, flip-flops, slippers, etc. But we can also use them to perform regression, that is, to return continuous <span id="dx1-177004"/>outcomes like 4.24 or 20.9 (and anything in between). Usually, these trees are called <span id="dx1-177005"/>regression trees. <em>Figure <a href="#x1-177006r2">9.2</a></em> shows a regression tree on the left. We can also see a regression tree as a representation of a piece-wise step-function as shown in the right panel of <em>Figure <a href="#x1-177006r2">9.2</a></em>. This contrasts with cubic splines or GPs, which represent smooth functions (at least to some degree). Trees can be flexible enough to provide good practical approximations of smooth functions.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file241.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-177006r2"/><strong>Figure 9.2</strong>: On the left, a regression tree; on the right is the corresponding piece-wise step function</p>&#13;
<p>Trees can be very flexible; in an extreme case, we could have a tree with as many leaf nodes as observations, and this tree will perfectly fit the data. As we saw in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>, this may not be a great idea unless we add some regularization. In Bayesian terms, we can achieve such regularization <span id="dx1-177007"/>through priors. For instance, we could set a prior that induces shallow trees. In this way, we make it very unlikely that we will end up with a tree with as many nodes as data points. <span id="x1-177008r426"/></p>&#13;
</section>&#13;
<section id="bart-models" class="level3 sectionHead" data-number="1.13.2">&#13;
<h2 class="sectionHead" data-number="1.13.2">9.2 <span id="x1-1780002"/>BART models</h2>&#13;
<p><span id="dx1-178001"/> <span id="dx1-178002"/></p>&#13;
<p>A <strong>Bayesian additive regression trees</strong> (<strong>BART</strong>) model is a sum of <em>m</em> trees that we use to approximate a function [<a href="Bibliography.xhtml#Xchipman2010">Chipman et al.</a>, <a href="Bibliography.xhtml#Xchipman2010">2010</a>]. To complete the model, we need to set priors over trees. The main function of such priors is to prevent overfitting while retaining the flexibility that trees provide. Priors are designed to keep the individual trees relatively shallow and the values at the leaf nodes relatively small.</p>&#13;
<p>PyMC does not support BART models directly but we can use PyMC-BART, a Python module that extends PyMC functionality to support BART models. PyMC-BART offers:</p>&#13;
<ul>&#13;
<li><p>A BART random variable that works very similar to other distributions in PyMC like <code>pm.Normal</code>, <code>pm.Poisson</code>, etc.</p></li>&#13;
<li><p>A sampler called PGBART as trees cannot be sampled with PyMC’s default step methods such as NUTS or Metropolis.</p></li>&#13;
<li><p>The following utility functions to help work with the result of a BART model:</p>&#13;
<ul>&#13;
<li><p><code>pmb.plot_pdp</code>: A function to generate partial dependence plots [<a href="Bibliography.xhtml#Xfriedman2001">Friedman</a>, <a href="Bibliography.xhtml#Xfriedman2001">2001</a>].</p></li>&#13;
<li><p><code>pmb.plot_ice</code>: A function to generate individual conditional expectation plots [<a href="Bibliography.xhtml#XGoldstein2013PeekingIT">Goldstein et al.</a>, <a href="Bibliography.xhtml#XGoldstein2013PeekingIT">2013</a>].</p></li>&#13;
<li><p><code>pmb.plot_variable_importance</code>: A function to estimate the variable importance.</p></li>&#13;
<li><p><code>pmb.plot_convergence</code>: A function that plots the empirical cumulative distribution for the effective sample size and <img src="../media/hat_R.png" style="width:0.65em;"/> values for the BART random variables.</p></li>&#13;
</ul></li>&#13;
</ul>&#13;
<div id="tcolobox-20" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>BARTs Are Priors Over Step Functions</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>We can think of BART as priors over piece-wise constant functions. Furthermore, in the limit of the number of trees <em>m</em> <span class="cmsy-10x-x-109">→ ∞</span>, BART converges to a nowhere-differentiable Gaussian process.</p>&#13;
</div>&#13;
</div>&#13;
<p>In the following sections, we will focus on the applied side of BART, specifically examining how to use PyMC-BART. If you are interested in reading more about the details of how BART models work, the implementation details of PyMC-BART, and how changing the hyperparameters of PyMC-BART affects the results, I recommend reading <a href="Bibliography.xhtml#Xquiroga2022">Quiroga et al.</a> [<a href="Bibliography.xhtml#Xquiroga2022">2022</a>]. <span id="x1-178003r417"/></p>&#13;
<section id="bartian-penguins" class="level4 subsectionHead" data-number="1.13.2.1">&#13;
<h3 class="subsectionHead" data-number="1.13.2.1">9.2.1 <span id="x1-1790001"/>Bartian penguins</h3>&#13;
<p><span id="dx1-179001"/> <span id="dx1-179002"/></p>&#13;
<p>Let’s imagine that, for some reason, we are interested in modeling the body mass of penguins as a function of other body measures. The following code block shows a BART model for such a problem. In this example, <code>X = "flipper_length", "bill_depth", "bill_length"] </code>and <code>Y </code>is the <code>body_mass</code>:</p>&#13;
<p><span id="x1-179003r1"/> <span id="x1-179004"/><strong>Code 9.1</strong></p>&#13;
<pre id="listing-120" class="source-code"><code>with pm.Model() as model_pen: &#13;
 &#13;
    σ = pm.HalfNormal("σ", 1) &#13;
 &#13;
    μ = pmb.BART("μ", X, Y) &#13;
 &#13;
    y = pm.Normal("y", mu=μ, sigma=σ, observed=Y) &#13;
 &#13;
    idata_pen = pm.sample() &#13;
 &#13;
    pm.sample_posterior_predictive(idata_pen, extend_inferencedata=True)</code></pre>&#13;
<p>We can see that using PyMC-BART to define a BART model with PyMC is straightforward. Essentially, we need to define a BART random variable with the arguments <code>X</code>, the covariates, and <code>Y </code>the response variable. Other than that, the rest of the model should look very familiar. As in other regression models, the length of <em>μ</em> will be the same as the observations.</p>&#13;
<p>While theoretically, the trees are only a function of <code>X</code>, PyMC-BART asks for <code>Y </code>to obtain an estimate for the initial value for the variance at the leaf nodes of the trees.</p>&#13;
<p>Once we have fitted a model with a BART variable, the rest of the workflow is as usual. For instance, we can compute a posterior predictive check simply by calling <code>az.plot_ppc(.) </code>and we will get something like <em>Figure <a href="#x1-179011r3">9.3</a></em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file242.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-179011r3"/><strong>Figure 9.3</strong>: Posterior predictive check for <code>model_pen</code></p>&#13;
<p><em>Figure <a href="#x1-179011r3">9.3</a></em>, shows a reasonable fit. Remarkably, we don’t get negative masses even when we use a Normal likelihood. But with PyMC and PyMC-BART, it is super easy to try other likelihoods; just replace the Normal with another distribution like Gamma or a Truncated Normal as you would do in a regular PyMC model and you are good to go. You can then use posterior predictive checks and LOO, as discussed in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>.</p>&#13;
<p>In the next few sections, we are going to discuss how to use and interpret the utility function provided by PyMC-BART (except for <code>pmb.plot_convergence</code>, which is discussed in <em>Chapter <a href="CH10.xhtml#x1-18900010">10</a></em> with other diagnostic methods). <span id="x1-179012r428"/></p>&#13;
</section>&#13;
<section id="partial-dependence-plots" class="level4 subsectionHead" data-number="1.13.2.2">&#13;
<h3 class="subsectionHead" data-number="1.13.2.2">9.2.2 <span id="x1-1800002"/>Partial dependence plots</h3>&#13;
<p><span id="dx1-180001"/> <span id="dx1-180002"/></p>&#13;
<p>A <strong>partial dependence plot</strong> (<strong>PDP</strong>) is a graphical tool widespread in the BART literature, but it is not exclusive to BART. In principle, it can be used with any method or model. It consists of plotting the predicted response as a function of a given covariate <em>X</em><sub><em>i</em></sub>, while averaging over the rest of the covariates <em>X</em><sub><span class="cmsy-8">−</span><em>i</em></sub>. So, essentially, we are plotting how much each covariate contributes to the response variable while keeping all other variables constant. One thing that is particular to BART and other tree-based methods is that the computation of PDPs can be done without refitting the model to synthetic data; instead, it can be efficiently computed from the already fitted trees. This makes BART an attractive choice for model interpretability and understanding the impact of individual features on predictions.</p>&#13;
<p>Once a model like <code>model_pen </code>has been fitted, we can compute a PDP with:</p>&#13;
<p><span id="x1-180003r2"/> <span id="x1-180004"/><strong>Code 9.2</strong></p>&#13;
<pre id="listing-121" class="source-code"><code>pmb.plot_pdp(μ, X, Y)</code></pre>&#13;
<p>Notice we passed the BART random variable, the covariates, and the response variable. The response variable is not really needed, but if passed, and if it is a pandas Series, it will use its name for the y-axis label.</p>&#13;
<p><em>Figure <a href="#x1-180006r4">9.4</a></em> shows one example of a partial dependence plot from <code>model_pen</code>. We can see that <code>flipper_length </code>shows the largest effect, which is approximately linear, while the other two variables show mostly a flat response, indicating their partial contribution is not very large. For a variable with a null contribution to the response, its expected PDP will be a flat, constant line at a value of the average of the response variable.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file243.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-180006r4"/><strong>Figure 9.4</strong>: Partial dependence plot for <code>model_pen</code></p>&#13;
<p>In <em>Figure <a href="#x1-180006r4">9.4</a></em>, we can see that the largest contribution comes from <code>flipper_length</code>, but this does not mean the other two variables are not related to <code>body_mass</code>. We can only say that considering we have <code>flipper_length </code>in the model, the effect of the other two is minimal. <span id="x1-180007r431"/></p>&#13;
</section>&#13;
<section id="individual-conditional-plots" class="level4 subsectionHead" data-number="1.13.2.3">&#13;
<h3 class="subsectionHead" data-number="1.13.2.3">9.2.3 <span id="x1-1810003"/>Individual conditional plots</h3>&#13;
<p><span id="dx1-181001"/> <span id="dx1-181002"/></p>&#13;
<p>When computing partial dependence plots, we assume that variables <em>X</em><sub><em>i</em></sub> and <em>X</em><sub><span class="cmsy-8">−</span><em>i</em></sub> are uncorrelated. In many real-world problems, this is hardly the case, and partial dependence plots can hide relationships in the data. Nevertheless, if the dependence between the subset of chosen variables is not too strong, then partial dependence plots can be useful summaries <a href="Bibliography.xhtml#Xfriedman2001">Friedman</a> [<a href="Bibliography.xhtml#Xfriedman2001">2001</a>].</p>&#13;
<p><strong>Individual Conditional Expectation</strong> (<strong>ICE</strong>) plots are closely related to PDPs. The difference is that instead of plotting the target covariates’ average partial effect on the predicted response, we plot <em>n</em> conditional expectation curves at given fixed values (10 by default). That is, each curve in an ICE plot reflects the partial predicted response as a function of covariate <em>X</em><sub><em>i</em></sub> for a fixed value of <em>X</em><sub><span class="cmsy-8">−</span><em>ij</em></sub>.</p>&#13;
<p>Once a model like <code>model_pen </code>has been fitted, we can compute an ICE plot with the following command:</p>&#13;
<p><span id="x1-181003r3"/> <span id="x1-181004"/><strong>Code 9.3</strong></p>&#13;
<pre id="listing-122" class="source-code"><code>pmb.plot_ice(μ, X, Y)</code></pre>&#13;
<p>The signature is the same as for PDPs. The result is shown in <em>Figure <a href="#x1-181006r5">9.5</a></em>. The gray curves are the conditional expectation curves at different values. If we average them, we should get the PDP curve (in black). If the curves in an ICE plot are mostly parallel to each other, it is because the contributions of the covariates to the response variable are mostly independent. This is the case for <code>flipper_length </code>and <code>bill_length</code>. In this case, the ICE and PDP plots convey the same information. However, if the curve is crossed, it indicates non-independent contributions. In such cases, the PDP would hide the effects. We can see an example of this in the following figure for <code>bill_depth</code>:</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file244.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-181006r5"/><strong>Figure 9.5</strong>: Individual conditional expectation plot for <code>model_pen</code></p>&#13;
<p>By default, ICE plots are centered, meaning that the gray curves are centered around the partial response evaluated at the lowest value on the x-axis. This helps interpret the plots: for instance, it is easier to see whether the lines cross. This also explains why the scale for the y-axis in <em>Figure <a href="#x1-181006r5">9.5</a></em> is different from the scale in <em>Figure <a href="#x1-180006r4">9.4</a></em>. You can change it with the argument <code>centered=False</code>. <span id="x1-181007r434"/></p>&#13;
</section>&#13;
<section id="variable-selection-with-bart" class="level4 subsectionHead" data-number="1.13.2.4">&#13;
<h3 class="subsectionHead" data-number="1.13.2.4">9.2.4 <span id="x1-1820004"/>Variable selection with BART</h3>&#13;
<p><span id="dx1-182001"/></p>&#13;
<p>In <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em>, we already discussed variable selection and explained under which scenarios we may be interested in selecting a subset of variables. PyMC-BART offers a very simple, and almost computational-free, heuristic to estimate variable importance. It keeps track of how many times a covariate is used as a splitting variable. For BART models, the variable importance is computed by averaging over the <em>m</em> trees and over all posterior samples. To further ease interpretation, we can report the values normalized so each value is in the interval [0<em>,</em>1] and the total importance is 1.</p>&#13;
<p>In some implementations of BART, the estimation of the variable importance is very sensitive to the number of trees <em>m</em>. The authors of those implementations recommend that you use a relatively low number of trees for variable selection and a higher number of trees for model fitting/predictions. This is not the case with PyMC-BART, for which the estimates of variable importance are robust to the number of trees.</p>&#13;
<p>Once we have fitted a model like <code>model_pen </code>to perform variable selection with PyMC-BART, we need to do something like:</p>&#13;
<p><span id="x1-182002r4"/> <span id="x1-182003"/><strong>Code 9.4</strong></p>&#13;
<pre id="listing-123" class="source-code"><code>pmb.plot_variable_importance(idata_pen, μ, X)</code></pre>&#13;
<p>Notice we passed the inference data, the BART random variable, and the covariates. The result is shown in <em>Figure <a href="#x1-182005r6">9.6</a></em>:</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file245.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-182005r6"/><strong>Figure 9.6</strong>: Variable importance plot for <code>model_pen</code></p>&#13;
<p>From the top panel, we can see that <code>flipper_length </code>has the largest value of variable importance, followed by <code>bill_depth </code>and <code>fill_length</code>. Notice that this qualitatively agrees with the partial dependence plots and individual conditional expectation plots.</p>&#13;
<p>The simple heuristic of counting how many times a variable enters a tree has some issues. One concerns interpretability, as the lack of a clear threshold separating the <em>important</em> variables from the <em>unimportant</em> ones is problematic. PyMC-BART offers some help. The bottom panel of <em>Figure <a href="#x1-182005r6">9.6</a></em> shows the square of the Pearson correlation coefficient between the predictions generated with the reference model, that is, the model with all covariates, and the predictions generated with the submodels, with fewer covariates. We can use this plot to find the minimal model capable of making predictions that are as close as possible to the reference model. <em>Figure <a href="#x1-182005r6">9.6</a></em> tells us that a model with just <code>flipper_length </code>will have almost the same predictive performance as the model with all three variables. Notice we may add some small gain by adding <code>bill_depth</code>, but it would probably be too small.</p>&#13;
<p>Now, let me briefly explain what <code>pmb.plot_variable_importance </code>is doing under the hood. Primarily, two approximations are taking place:</p>&#13;
<ul>&#13;
<li><p>It does not evaluate all possible combinations of covariates. Instead, it adds one variable at a time, following their relative importance (the top subplot in <em>Figure <a href="#x1-182005r6">9.6</a></em>).</p></li>&#13;
<li><p>It does not refit all models from 1 to n covariates. Instead, it approximates the effect of removing a variable by traversing the trees from the posterior distribution for the reference model and it prunes the branches without the variable of interest. This is similar to the procedure to compute the partial dependence plots, with the difference that for the plots, we excluded all but one variable, while for the variable importance we start by excluding all but the most important one, then all but the two most important ones, and so on until we include all variables.</p></li>&#13;
</ul>&#13;
<p>If this procedure for variable selection sounds familiar to you, it is highly likely that you have been paying attention to this chapter and also <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em>. The procedure is conceptually similar to what Kulprit does. Here, we also make use of the concept of a reference model, and we evaluate a model in terms of its predictive distribution. But the similarities stop there. PyMC-BART does not use the ELPD, instead using the square of the Pearson correlation coefficient, and estimating the submodels by pruning the trees fitted with the reference model, not via a Kullback-Liebler divergence projection.</p>&#13;
<p>Before moving on to another topic, let me just add some words of caution. As we discussed in <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em>, with the output of Kulprit, we should not over-interpret the order of the variables. The sample applies to figures generated with <code>pmb.plot_variable_importance </code>like <em>Figure <a href="#x1-182005r6">9.6</a></em>. If the importance of two variables is very similar, it can easily happen that the order changes if we refit the model with a different random seed or if the data slightly changes, such as after adding or removing a data point. The error bars for the variable importance could help, but it is likely that they underestimate the true variability. Thus, take the order with a pinch of salt, and use it as a guide in the context of your problems. <span id="x1-182006r427"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="distributional-bart-models" class="level3 sectionHead" data-number="1.13.3">&#13;
<h2 class="sectionHead" data-number="1.13.3">9.3 <span id="x1-1830003"/>Distributional BART models</h2>&#13;
<p><span id="dx1-183001"/></p>&#13;
<p>As we saw in <em>Chapter <a href="CH06.xhtml#x1-1200006">6</a></em>, for generalized linear models, we are not restricted to creating linear models for the mean or location parameter; we can also model other parameters, for example, the standard deviation of a Gaussian or even both the mean and standard deviation. The same applies to BART models.</p>&#13;
<p>To exemplify this, let’s model the bike dataset. We will use <code>rented </code>as the response variable and <code>hour</code>, <code>temperature</code>, <code>humidity</code>, and <code>workday </code>as predictor variables. As we did previously, we are going to use a NegativeBinomial distribution as likelihood. This distribution has two parameters <em>μ</em> and <em>alpha</em>. We are going to use a sum of trees for both parameters. The following code block shows the model:</p>&#13;
<p><span id="x1-183002r5"/> <span id="x1-183003"/><strong>Code 9.5</strong></p>&#13;
<pre id="listing-124" class="source-code"><code>with pm.Model() as model_bb: </code>&#13;
<code>    μ = pmb.BART("μ", X, np.log(Y), shape=(2, 348), separate_trees=True) </code>&#13;
<code>    pm.NegativeBinomial('yl', np.exp(μ[0]), np.exp(μ[1]), observed=Y) </code>&#13;
<code>    idata_bb = pm.sample(2000, </code>&#13;
<code>                         random_seed=123, </code>&#13;
<code>                         pgbart={"batch":(0.05, 0.15)})</code></pre>&#13;
<p>Let’s take a moment to be sure we understand this model. First, notice that we passed a <code>shape </code>argument to <code>pmb.BART()</code>. When <code>separate_trees = True</code>, this instructs PyMC-BART to fit two separate sets of sum-of-trees. Then we index <em>μ</em> in order to use the first dimension for the <em>μ</em> parameter of the NegativeBinomial and the second dimension for the <em>α</em> parameter. If, instead, <code>separate_trees = False</code>, then we tell PyMC-BART to fit a single sum-of-trees but each tree will return 2 values at each leaf node, instead of 1. The advantage of this is that the algorithm will run faster and use less memory, as we are only fitting one set of trees. The disadvantage is that we get a less flexible model. In practice, both options can be useful, so which one you should use is another modeling decision.</p>&#13;
<p>Another important aspect of <code>model_bb </code>is that we take the exponential of <em>μ</em>. We do this to ensure that the NegativeBinomial distribution gets only positive values, both for <em>μ</em> and <em>α</em>. This is the same type of transformation we discussed in the context of generalized linear models. What is particular about PyMC-BART is that we applied its inverse to the value of <em>Y</em> we passed to <code>pmb.BART()</code>. In my experience, this helps PyMC-BART to find better solutions. For a model with a Binomial or Categorical likelihood, it is not necessary to apply the inverse of the logistic or softmax, respectively. PyMC-BART handles the Binomial as a particular case and for the Categorical, we have empirically seen good results without the inverse. It is important to remark that the value of <em>Y</em> we passed to <code>pmb.BART() </code>is only used to initialize the sampling of the BART variables. The initialization seems to be robust to the values we pass and passing <em>Y</em> or some transformation of it works well in most cases.</p>&#13;
<p>The third aspect I want you to pay attention to is that we are passing a new argument to <code>pm.sample</code>, namely <code>pgbart</code>. The value for this argument is the dictionary <code>"batch":(0.05, 0.15)</code>. Why are we doing this? Occasionally, to obtain good-quality samples, it becomes necessary to tweak the hyperparameters of the sampler. In previous examples, we opted to omit this aspect to maintain simplicity and focus. However, as we later discuss in more depth in <em>Chapter <a href="CH10.xhtml#x1-18900010">10</a></em>, paying attention to these adjustments can become important. For the particular case of the PGBART sampler, there are two hyperparameters we can change. One is <code>num_particles </code>(defaults to 10), where the larger the number of particles, the more accurate the sampling of BART, but also the more expensive it is. The other is <code>batch</code>; by default, this is a tuple <code>(0.1, 0.1)</code>, meaning that at each step, the sampler fits 10% of the <code>m </code>trees during the tuning phase and the same for the sampling phase. For the <code>model_bb </code>model, we used <code>(0.05, 0.15)</code>, meaning 5% during tuning (2 trees) and 15% (7 trees) during the actual sampling.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file246.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-183010r7"/><strong>Figure 9.7</strong>: Partial dependence plot for <code>model_bb</code></p>&#13;
<p>We can explore the relationship of the covariates to the response for both parameters as in <em>Figure <a href="#x1-183010r7">9.7</a></em>. Notice that variables appear twice: the first column corresponds to parameter <em>μ</em> and the second column to parameter <em>α</em>. We can see that <code>hour </code>has the largest effect on the response variable for both parameters of the NegativeBinomial. <span id="x1-183011r440"/></p>&#13;
</section>&#13;
<section id="constant-and-linear-response" class="level3 sectionHead" data-number="1.13.4">&#13;
<h2 class="sectionHead" data-number="1.13.4">9.4 <span id="x1-1840004"/>Constant and linear response</h2>&#13;
<p><span id="dx1-184001"/></p>&#13;
<p>By default, PyMC-BART will fit trees that return a single value at each leaf node. This is a simple approach that usually works just fine. However, it is important to understand its implications. For instance, this means that predictions for any value outside the range of the observed data used to fit the model will be constants. To see this, go back and check <em>Figure <a href="#x1-177006r2">9.2</a></em>. This tree will return 1.9 for any value below <code>c1</code>. Notice that this will still be the case if we, instead, sum a bunch of trees, because summing a bunch of constant values results in yet another constant value.</p>&#13;
<p>Whether this is a problem or not is up to you and the context in which you apply the BART model. Nevertheless, PyMC-BART offers a <code>response </code>argument that you pass to the BART random variable. Its default value is <code>"constant"</code>. You can change it to <code>"linear"</code>, in which case PyMC-BART will return a linear fit at each leaf node or <code>"mix"</code>, which will propose (during sampling) trees with either constant or linear values.</p>&#13;
<p>To exemplify the difference, let us fit a very simple example: the number of rented bikes versus the temperature. The observed temperature values go from <span class="cmsy-10x-x-109">≈−</span>5 to <span class="cmsy-10x-x-109">≈ </span>35. After fitting this model, we will ask for out-of-sample posterior predictive values in the range [-20, 45]. For that reason, we will set up a model with a mutable variable as introduced in <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>.</p>&#13;
<p><span id="x1-184002r6"/> <span id="x1-184003"/><strong>Code 9.6</strong></p>&#13;
<pre id="listing-125" class="source-code"><code>with pm.Model() as model_tmp1: </code>&#13;
<code>    X_mut1 = pm.MutableData("X_mut1", X) </code>&#13;
<code>    <em>α</em> = pm.HalfNormal('<em>α</em>', 1) </code>&#13;
<code>    μ = pmb.BART("μ", X_mut1, np.log(Y), m=100, response="linear") </code>&#13;
<code>    _ = pm.NegativeBinomial('yl', np.exp(μ), <em>α</em>, observed=Y, shape=μ.shape) </code>&#13;
<code>    idata_tmp1 = pm.sample(random_seed=123)</code></pre>&#13;
<p>Notice that we passed <code>shape=</code><em>μ</em><code>.shape </code>to the likelihood. This is something we need to do to be able to change the shape of <code>X_mut1</code>, which is also a requirement of PyMC, so this is something you should also do for non-BART models like linear regression.</p>&#13;
<p>OK, to continue with the example, in the accompanying code, you will find the code for the <code>model_tmp0 </code>model, which is exactly the same as <strong>model_tmp1</strong>, except that it has the default constant response. The results from both models are shown in <em>Figure <a href="#x1-184010r8">9.8</a></em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file247.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-184010r8"/><strong>Figure 9.8</strong>: Mean predictions with constant and linear responses</p>&#13;
<p>Notice how outside of the range of the data (dashed gray lines), the predictions for the model with constant response are indeed constant. Which one is providing better predictions? I am not sure. I will argue that for predictions on the lower end of temperatures, the linear response is better as it predicts that the number of rented bikes will keep decreasing until eventually reaching 0. But on the higher end of temperatures, a plateau or even a decrease should be more likely than an increase. I mean, I have tried riding my bike at 40 or maybe even 42 degrees, and it is not a super nice experience. What do you think? <span id="x1-184011r443"/></p>&#13;
</section>&#13;
<section id="choosing-the-number-of-trees" class="level3 sectionHead" data-number="1.13.5">&#13;
<h2 class="sectionHead" data-number="1.13.5">9.5 <span id="x1-1850005"/>Choosing the number of trees</h2>&#13;
<p><span id="dx1-185001"/></p>&#13;
<p>The number of trees (<code>m</code>) controls the flexibility of the BART function. As a rule of thumb, the default value of 50 should be enough to get a good approximation. And larger values, like 100 or 200, should provide a more refined answer. Usually, it is hard to overfit by increasing the number of trees, because the larger the number of trees, the smaller the values at the leaf nodes.</p>&#13;
<p>In practice, you may be worried about overshooting <code>m </code>because the computational cost of BART, both in terms of time and memory, will increase. One way to tune <code>m </code>is to perform K-fold cross-validation, as recommended by <a href="Bibliography.xhtml#Xchipman2010">Chipman et al.</a> [<a href="Bibliography.xhtml#Xchipman2010">2010</a>]. Another option is to approximate cross-validation by using LOO as discussed in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>. We have observed that LOO can indeed be of help to provide a reasonable value of <code>m </code>[<a href="Bibliography.xhtml#Xquiroga2022">Quiroga et al.</a>, <a href="Bibliography.xhtml#Xquiroga2022">2022</a>]. <span id="x1-185002r446"/></p>&#13;
</section>&#13;
<section id="summary-8" class="level3 sectionHead" data-number="1.13.6">&#13;
<h2 class="sectionHead" data-number="1.13.6">9.6 <span id="x1-1860006"/>Summary</h2>&#13;
<p>BART is a flexible non-parametric model where a sum of trees is used to approximate an unknown function from the data. Priors are used to regularize inference, mainly by restricting trees’ learning capacity so that no individual tree is able to explain the data, but rather the sum of trees. PyMC-BART is a Python library that extends PyMC to work with BART models.</p>&#13;
<p>We built a few BART models in this chapter, and learned how to perform variable selection and use partial dependence plots and individual conditional plots to interpret the output of BART models. <span id="x1-186001r447"/></p>&#13;
</section>&#13;
<section id="exercises-8" class="level3 sectionHead" data-number="1.13.7">&#13;
<h2 class="sectionHead" data-number="1.13.7">9.7 <span id="x1-1870007"/>Exercises</h2>&#13;
<ol>&#13;
<li><div id="x1-187002x1">&#13;
<p>Explain each of the following:</p>&#13;
<ul>&#13;
<li><p>How is BART different from linear regression and splines?</p></li>&#13;
<li><p>When might you want to use linear regression over BART?</p></li>&#13;
<li><p>When might you want to use Gaussian processes over BART?</p></li>&#13;
</ul>&#13;
</div></li>&#13;
<li><div id="x1-187004x2">&#13;
<p>In your own words, explain why it can be the case that multiple small trees can fit patterns better than one single large tree. What is the difference in the two approaches? What are the trade-offs?</p>&#13;
</div></li>&#13;
<li><div id="x1-187006x3">&#13;
<p>Below, we provide two simple synthetic datasets. Fit a BART model with m=50 to each of them. Plot the data and the mean fitted function. Describe the fit.</p>&#13;
<ul>&#13;
<li><p>x = np.linspace(-1, 1., 200) and y = np.random.normal(2*x, 0.25)</p></li>&#13;
<li><p>x = np.linspace(-1, 1., 200) and y = np.random.normal(x**2, 0.25)</p></li>&#13;
<li><p>Create your own synthetic dataset.</p></li>&#13;
</ul>&#13;
</div></li>&#13;
<li><div id="x1-187008x4">&#13;
<p>Create the following dataset <em>Y</em> = 10sin(<em>πX</em><sub>0</sub><em>X</em><sub>1</sub>)+20(<em>X</em><sub>2</sub> <span class="cmsy-10x-x-109">−</span>0<em>.</em>5)<sup>2</sup> +10<em>X</em><sub>3</sub> +5<em>X</em><sub>4</sub> + <img src="../media/e.png" style="width:0.75em; vertical-align: -0.10em;"/>, where <img src="../media/e.png" style="width:0.75em; vertical-align: -0.10em;"/> <span class="cmsy-10x-x-109">∼<img src="../media/N.PNG" style="width:1.4em; vertical-align: -0.30em;"/></span>(0<em>,</em>1) and <strong><em>X</em></strong><sub>0:9</sub> <span class="cmsy-10x-x-109">∼<img src="../media/U.PNG" style="width:1.4em; vertical-align: -0.30em;"/></span>(0<em>,</em>1). This is called Friedman’s five-dimensional function. Notice that we <span id="dx1-187009"/>actually have 10 dimensions, but the last 5 are pure noise.</p>&#13;
<ul>&#13;
<li><p>Fit a BART model to this data.</p></li>&#13;
<li><p>Compute a PDP and the variable importance (VI).</p></li>&#13;
<li><p>Do the PDP and VI qualitatively agree? How?</p></li>&#13;
</ul>&#13;
</div></li>&#13;
<li><div id="x1-187011x5">&#13;
<p>Use BART with the penguins dataset. Use <code>bill_length</code>, <code>flipper_length</code>, <code>bill_depth</code>, <code>bill_length</code>, and <code>body_mass </code>as covariates and the species <code>Adelie </code>and <code>Chistrap </code>as the response. Try different values of <code>m </code>–, 10, 20, 50, and 100. Use LOO to pick a suitable value.</p>&#13;
</div></li>&#13;
<li><div id="x1-187013x6">&#13;
<p>Check the variable importance for the model in the previous question. Compare the result with one obtained with Kulprit for a generalized linear model with the same covariates and response, built with Bambi.</p>&#13;
</div></li>&#13;
</ol>&#13;
</section>&#13;
<section id="join-our-community-discord-space-9" class="level3 likesectionHead" data-number="1.13.8">&#13;
<h2 class="likesectionHead" data-number="1.13.8"><span id="x1-1880007"/>Join our community Discord space</h2>&#13;
<p>Join our Discord community to meet like-minded people and learn alongside more than 5000 members at: <a href="https://packt.link/bayesian">https://packt.link/bayesian</a></p>&#13;
<p><img src="../media/file1.png" alt="PIC"/></p>&#13;
<p><span id="x1-188001r425"/></p>&#13;
</section>&#13;
</section>&#13;
</body></html>
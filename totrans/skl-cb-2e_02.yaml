- en: Pre-Model Workflow and Pre-Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we will see the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating sample data for toy analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling data to the standard normal distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating binary features through thresholding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with categorical variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputing missing values through various strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A linear model in the presence of outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together with pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Gaussian processes for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SGD for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is data, and what are we doing with it?
  prefs: []
  type: TYPE_NORMAL
- en: A simple answer is that we attempt to place our data as points on paper, graph
    them, think, and look for simple explanations that approximate the data well.
    The simple geometric line of *F=ma* (force being proportional to acceleration)
    explained a lot of noisy data for hundreds of years. I tend to think of data science
    as data compression at times.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, when a machine is given only win-lose outcomes (of winning games
    of checkers, for example) and trained, I think of artificial intelligence. It
    is never taught explicit directions on how to play to win in such a case.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter deals with the pre-processing of data in scikit-learn. Some questions
    you can ask about your dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Are there missing values in your dataset?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there outliers (points far away from the others) in your set?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the variables in the data like? Are they continuous quantities or categories?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do the continuous variable distributions look like? Can any of the variables
    in your dataset be described by normal distributions (bell-shaped curves)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can any continuous variables be turned into categorical variables for simplicity?
    (This tends to be true if the distribution takes on very few particular values
    and not a continuous-like range of values.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the units of the variables involved? Will you mix the variables somehow
    in the machine learning algorithm you chose to use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These questions can have simple or complex answers. Thankfully, you ask them
    many times, even on the same dataset, and after these recipes you will have some
    practice at crafting answers to pre-processing machine learning questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we will see pipelines: a great organizational tool to make sure
    we perform the same operations on both the training and testing sets without errors
    and with relatively little work. We will also see regression examples: **stochastic
    gradient descent** (**SGD**) and Gaussian processes.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating sample data for toy analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If possible, use some of your own data for this book, but in the event you cannot,
    we'll learn how we can use scikit-learn to create toy data. scikit-learn's pseudo,
    theoretically constructed data is very interesting in its own right.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Very similar to getting built-in datasets, fetching new datasets, and creating
    sample datasets, the functions that are used follow the naming convention `make_*`.
    Just to be clear, this data is purely artificial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To save typing, import the `datasets` module as `d`, and `numpy` as `np`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will walk you through the creation of several datasets. In addition
    to the sample datasets, these will be used throughout the book to create data
    with the necessary characteristics for the algorithms on display.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a regression dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, the stalwart—regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By default, this will generate a tuple with a 100 x 100 matrix—100 samples by
    100 features. However, by default, only 10 features are responsible for the target
    data generation. The second member of the tuple is the target variable. It is
    also possible to get more involved in generating data for regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to generate a 1,000 x 10 matrix with five features responsible
    for the target creation, an underlying bias factor of 1.0, and 2 targets, the
    following command will be run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Creating an unbalanced classification dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification datasets are also very simple to create. It's simple to create
    a base classification set, but the basic case is rarely experienced in practice—most
    users don't convert, most transactions aren't fraudulent, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it''s useful to explore classification on unbalanced datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Creating a dataset for clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clusters will also be covered. There are actually several functions to create
    datasets that can be modeled by different cluster algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, blobs are very easy to create and can be modeled by k-means:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0bf9b285-7654-42fc-a2cb-4f14e9d40e44.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's walk you through how scikit-learn produces the regression dataset by taking
    a look at the source code (with some modifications for clarity). Any undefined
    variables are assumed to have the default value of `make_regression`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s actually surprisingly simple to follow. First, a random array is generated
    with the size specified when the function is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the basic dataset, the target dataset is then generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The dot product of `X` and `ground_truth` are taken to get the final target
    values. Bias, if any, is added at this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The dot product is simply a matrix multiplication. So, our final dataset will
    have `n_samples`, which is the number of rows from the dataset, and `n_target,`
    which is the number of target variables.
  prefs: []
  type: TYPE_NORMAL
- en: Due to NumPy's broadcasting, bias can be a scalar value, and this value will
    be added to every sample. Finally, it's a simple matter of adding any noise and
    shuffling the dataset. Voila, we have a dataset that's perfect for testing regression.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling data to the standard normal distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A pre-processing step that is recommended is to scale columns to the standard
    normal. The standard normal is probably the most important distribution in statistics.
    If you've ever been introduced to statistics, you must have almost certainly seen
    z-scores. In truth, that's all this recipe is about—transforming our features
    from their endowed distribution into z-scores.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The act of scaling data is extremely useful. There are a lot of machine learning
    algorithms, which perform differently (and incorrectly) in the event the features
    exist at different scales. For example, SVMs perform poorly if the data isn't
    scaled because they use a distance function in their optimization, which is biased
    if one feature varies from 0 to 10,000 and the other varies from 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `preprocessing` module contains several useful functions for scaling features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the Boston dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Continuing with the Boston dataset, run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'There''s actually a lot to learn from this initially. Firstly, the first feature
    has the smallest mean but varies even more than the third feature. The second
    feature has the largest mean and standard deviation—it takes the widest spread
    of values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The centering and scaling function is extremely simple. It merely subtracts
    the mean and divides by the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pictorially and with pandas, the third feature looks as follows before the
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0d0ec1e7-2c6e-447a-a34c-c6a548aab0db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is what it looks like afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1a8a94aa-e223-4952-94f9-2ab4258b4705.png)'
  prefs: []
  type: TYPE_IMG
- en: The *x* axis label has changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to a function, there is also a centering and scaling class that
    is easy to invoke, and this is particularly useful when used in conjunction with
    pipelines, which are mentioned later. It''s also useful for the centering and
    scaling class to persist across individual scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Scaling features to a mean of zero and a standard deviation of one isn't the
    only useful type of scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-processing also contains a `MinMaxScaler` class, which will scale the data
    within a certain range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s very simple to change the minimum and maximum values of the `MinMaxScaler`
    class from its defaults of `0` and `1`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, another option is normalization. This will scale each sample to
    have a length of one. This is different from the other types of scaling done previously,
    where the features were scaled. Normalization is illustrated in the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If it's not apparent why this is useful, consider the Euclidean distance (a
    measure of similarity) between three of the samples, where one sample has the
    values *(1, 1, 0)*, another has *(3, 3, 0)*, and the final has *(1, -1, 0)*.
  prefs: []
  type: TYPE_NORMAL
- en: The distance between the first and third vector is less than the distance between
    the first and second although the first and third are orthogonal, whereas the
    first and second only differ by a scalar factor of three. Since distances are
    often used as measures of similarity, not normalizing the data first can be misleading.
  prefs: []
  type: TYPE_NORMAL
- en: 'From an alternative perspective, try the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: All the rows are normalized and consist of vectors of length one. In three dimensions,
    all normalized vectors lie on the surface of a sphere centered at the origin.
    The information left is the direction of the vectors because, by definition, by
    normalizing you are dividing the vector by its length. Do always remember, though,
    that when performing this operation you have set an origin at *(0, 0, 0)* and
    you have turned any row of data in the array into a vector relative to this origin.
  prefs: []
  type: TYPE_NORMAL
- en: Creating binary features through thresholding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last recipe, we looked at transforming our data into the standard normal
    distribution. Now, we'll talk about another transformation, one that is quite
    different. Instead of working with the distribution to standardize it, we'll purposely
    throw away data; if we have good reason, this can be a very smart move. Often,
    in what is ostensibly continuous data, there are discontinuities that can be determined
    via binary features.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, note that in the previous chapter, we turned a classification
    problem into a regression problem. With thresholding, we can turn a regression
    problem into a classification problem. This happens in some data science contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating binary features and outcomes is a very useful method, but it should
    be used with caution. Let''s use the Boston dataset to learn how to turn values
    into binary outcomes. First, load the Boston dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to scaling, there are two ways to binarize features in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '`preprocessing.binarize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preprocessing.Binarizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Boston dataset's `target` variable is the median value of houses in thousands.
    This dataset is good for testing regression and other continuous predictors, but
    consider a situation where we want to simply predict whether a house's value is
    more than the overall mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will want to create a threshold value of the mean. If the value
    is greater than the mean, produce a `1`; if it is less, produce a `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This was easy, but let''s check to make sure it worked correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the simplicity of the operation in NumPy, it''s a fair question to ask
    why you would want to use the built-in functionality of scikit-learn. Pipelines,
    covered in the *Putting it all together with pipelines* recipe, will help to explain
    this; in anticipation of this, let''s use the `Binarizer` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's also learn about sparse matrices and the `fit` method.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sparse matrices are special in that zeros aren''t stored; this is done in an
    effort to save space in memory. This creates an issue for the binarizer, so to
    combat it, a special condition for the binarizer for sparse matrices is that the
    threshold cannot be less than zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The fit method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `fit` method exists for the binarizer transformation, but it will not fit
    anything; it will simply return the object. The object, however, will store the
    threshold and be ready for the `transform` method.
  prefs: []
  type: TYPE_NORMAL
- en: Working with categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Categorical variables are a problem. On one hand they provide valuable information;
    on the other hand, it's probably text—either the actual text or integers corresponding
    to the text—such as an index in a lookup table.
  prefs: []
  type: TYPE_NORMAL
- en: So, we clearly need to represent our text as integers for the model's sake,
    but we can't just use the id field or naively represent them. This is because
    we need to avoid a similar problem to the *Creating binary features through thresholding*
    recipe. If we treat data that is continuous, it must be interpreted as continuous.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Boston dataset won't be useful for this section. While it's useful for feature
    binarization, it won't suffice for creating features from categorical variables.
    For this, the iris dataset will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: For this to work, the problem needs to be turned on its head. Imagine a problem
    where the goal is to predict the sepal width; in this case, the species of the
    flower will probably be useful as a feature.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get the data sorted first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Place `X` and `y`, all of the numerical data, side-by-side. Create an encoder
    with scikit-learn to handle the category of the `y` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The encoder creates additional features for each categorical variable, and
    the value returned is a sparse matrix. The result is a sparse matrix by definition;
    each row of the new features has `0` everywhere, except for the column whose value
    is associated with the feature''s category. Therefore, it makes sense to store
    this data in a sparse matrix. The `cat_encoder` is now a standard scikit-learn
    model, which means that it can be used again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous chapter, we turned a classification problem into a regression
    problem. Here, there are three columns:'
  prefs: []
  type: TYPE_NORMAL
- en: The first column is `1` if the flower is a Setosa and `0` otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second column is `1` if the flower is a Versicolor and `0` otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third column is `1` if the flower is a Virginica and `0` otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, we could use any of these three columns to create a regression similar
    to the one in the previous chapter; we will perform a regression to determine
    the degree of setosaness of a flower as a real number. The matching statement
    in classification is whether a flower is a Setosa one or not. This is the problem
    statement if we perform binary classification of the first column.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn has the capacity for this type of multi-output regression. Compare
    it with multiclass classification. Let's try a simple one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the ridge regression regularized linear model. It tends to be very well
    behaved because it is regularized. Instantiate a ridge regressor class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now import a multi-output regressor that takes the ridge regressor instance
    as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'From earlier in this recipe, transform the target variable `y` to a three-part
    target variable, `y_multi`, with `OneHotEncoder()`. If `X` and `y` were part of
    a pipeline, the pipeline would transform the training and testing sets separately,
    and this is preferable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Create training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the multi-output estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the multi-output target on the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `binarize` function from the previous recipe to turn each real number
    into the integers `0` or `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can measure the overall multi-output performance with the `roc_auc_score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, we can do it flower type by flower type, column by column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding multi-output regression, you could be concerned with the dummy
    variable trap: the collinearity of the outputs. Without dropping any output columns,
    you assume that there is a fourth option: that a flower can be of none of the
    three types. To prevent the trap, drop the last column and assume that the flower
    has to be of one of the three types as we do not have any training examples where
    it is not one of the three flower types.'
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to create categorical variables in scikit-learn and Python.
    The `DictVectorizer` class is a good option if you like to limit the dependencies
    of your projects to only scikit-learn and you have a fairly simple encoding scheme.
    However, if you require more sophisticated categorical encoding, patsy is a very
    good option.
  prefs: []
  type: TYPE_NORMAL
- en: DictVectorizer class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another option is to use `DictVectorizer` class. This can be used to directly
    convert strings to features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Imputing missing values through various strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data imputation is critical in practice, and thankfully there are many ways
    to deal with it. In this recipe, we'll look at a few of the strategies. However,
    be aware that there might be other approaches that fit your situation better.
  prefs: []
  type: TYPE_NORMAL
- en: This means scikit-learn comes with the ability to perform fairly common imputations;
    it will simply apply some transformations to the existing data and fill the NAs.
    However, if the dataset is missing data, and there's a known reason for this missing
    data—for example, response times for a server that times out after 100 ms—it might
    be better to take a statistical approach through other packages, such as the Bayesian
    treatment via PyMC, hazards models via Lifelines, or something home-grown.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing to do when learning how to input missing values is to create
    missing values. NumPy''s masking will make this extremely simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: To unravel this a bit, in case NumPy isn't too familiar, it's possible to index
    arrays with other arrays in NumPy. So, to create the random missing data, a random
    Boolean array is created, which is of the same shape as the iris dataset. Then,
    it's possible to make an assignment via the masked array. It's important to note
    that because a random array is used, it is likely that your `masking_array` will
    be different from what's used here.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure this works, use the following command (since we''re using a random
    mask, it might not match directly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A theme prevalent throughout this book (due to the theme throughout scikit-learn)
    is reusable classes that fit and transform datasets that can subsequently be used
    to transform unseen datasets. This is illustrated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the difference in the position `[0, 0]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The imputation works by employing different strategies. The default is mean,
    but in total there are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mean` (default)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`most_frequent` (mode)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'scikit-learn will use the selected strategy to calculate the value for each
    non-missing value in the dataset. It will then simply fill the missing values.
    For example, to redo the iris example with the median strategy, simply reinitialize
    impute with the new strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: If the data is missing values, it might be inherently dirty in other places.
    For instance, in the example in the preceding, *How to do it...* section, `np.nan`
    (the default missing value) was used as the missing value, but missing values
    can be represented in many ways. Consider a situation where missing values are
    `-1`. In addition to the strategy to compute the missing value, it's also possible
    to specify the missing value for the imputer. The default is `nan`, which will
    handle `np.nan` values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see an example of this, modify `iris_X` to have `-1` as the missing value.
    It sounds crazy, but since the iris dataset contains measurements that are always
    possible, many people will fill the missing values with `-1` to signify they''re
    not there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Filling these in is as simple as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pandas also provides a functionality to fill in missing data. It actually might
    be a bit more flexible, but it is less reusable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'To mention its flexibility, `fillna` can be passed any sort of statistic, that
    is, the strategy is more arbitrarily defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: A linear model in the presence of outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, instead of traditional linear regression we will try using the
    Theil-Sen estimator to deal with some outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, create the data corresponding to a line with a slope of `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d1ebad85-c139-418a-94ec-06c1d3ce5791.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Add noise to that data and label it as `y_noisy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f9790eae-a74e-4d0f-93b0-45057e7afff8.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import both `LinearRegression` and `TheilSenRegressor`. Score the estimators
    using the original line as the testing set, `y_truth`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the lines. Note that **o****rdinary least squares** (**OLS**) is way off
    the true line, `y_truth`. Theil-Sen overlaps the real line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/87bf00b0-a0bf-4e0e-8e82-8fcdd3a54a7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot the dataset and the estimated lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/24418ea6-bb8c-4d1d-90bf-c357f9d6bd18.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `TheilSenRegressor` is a robust estimator that performs well in the presence
    of outliers. It uses the measurement of medians, which is more robust to outliers.
    In OLS regression, errors are squared, and thus a squared error can decrease good
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can try several robust estimators in scikit-learn Version 0.19.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the robust linear estimators Theil-Sen, **random sample consensus**
    (**RANSAC**), and the Huber regressor out-perform the other linear regressors
    in the presence of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together with pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've used pipelines and data transformation techniques, we'll walk
    through a more complicated example that combines several of the previous recipes
    into a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll show off some more of pipeline's power. When we used
    it earlier to impute missing values, it was only a quick taste; here, we'll chain
    together multiple pre-processing steps to show how pipelines can remove extra
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s briefly load the iris dataset and seed it with some missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this chapter is to first impute the missing values of `iris_data`,
    and then perform PCA on the corrected dataset. You can imagine (and we'll do it
    later) that this workflow might need to be split between a training dataset and
    a holdout set; pipelines will make this easier, but first we need to take a baby
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create the `imputer` and `pca` classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the classes we need, we can load them into `Pipeline`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This takes a lot more management if we use separate steps. Instead of each step
    requiring a fit transform, this step is performed only once, not to mention that
    we only have to keep track of one object!
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully it was obvious, but each step in a pipeline is passed to a pipeline
    object via a list of tuples, with the first element getting the name and the second
    getting the actual object. Under the hood, these steps are looped through when
    a method such as `fit_transform` is called on the pipeline object.
  prefs: []
  type: TYPE_NORMAL
- en: 'This said, there are quick and dirty ways to create a pipeline, much in the
    same way there was a quick way to perform scaling, though we can use `StandardScaler`
    if we want more power. The `pipeline` function will automatically create the names
    for the pipeline objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the same object that was created in the more verbose method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We just walked through pipelines at a very high level, but it''s unlikely that
    we will want to apply the base transformation. Therefore, the attributes of each
    object in a pipeline can be accessed using a `set_params` method, where the parameter
    follows the `<step_name>__<step_parameter>` convention. For example, let''s change
    the `pca` object to use two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice, `n_components=2` in the preceding output. Just as a test, we can output
    the same transformation we have already done twice, and the output will be an
    N x 2 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Using Gaussian processes for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll use a Gaussian process for regression. In the linear models
    section, we will see how representing prior information on the coefficients was
    possible using Bayesian ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: With a Gaussian process, it's about the variance and not the mean. However,
    with a Gaussian process, we assume the mean is 0, so it's the covariance function
    we'll need to specify.
  prefs: []
  type: TYPE_NORMAL
- en: The basic setup is similar to how a prior can be put on the coefficients in
    a typical regression problem. With a Gaussian process, a prior can be put on the
    functional form of the data, and it's the covariance between the data points that
    is used to model the data, and therefore, must fit the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A big advantage of Gaussian processes is that they can predict probabilistically:
    you can obtain confidence bounds on your predictions. Additionally, the prediction
    can interpolate the observations for the available kernels: predictions from regression
    are smooth and thus a prediction between two points you know about is between
    those two points.'
  prefs: []
  type: TYPE_NORMAL
- en: A disadvantage of Gaussian processes is lack of efficiency in high-dimensional
    spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, let''s use some regression data and walk through how Gaussian processes
    work in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have the data, we''ll create a scikit-learn `GaussianProcessRegressor` object.
    Let''s look at the `gpr` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few parameters that are important and must be set:'
  prefs: []
  type: TYPE_NORMAL
- en: '`alpha`: This is a noise parameter. You can assign a noise value for all observations
    or assign `n` values in the form of a NumPy array where `n` is the length of the
    target observations in the training set you pass to `gpr` for training.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel`: This is a kernel that approximates a function. The default in a previous
    version of scikit-learn was **radial basis functions** (**RBF**), and we will
    construct a flexible kernel from constant kernels and RBF kernels.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize_y`:  You can set it to true if the mean of the target set is not
    zero. If you leave it set to false, it still works fairly well.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_restarts_optimizer`: Set this to 10-20 for practical use. This is the number
    of iterations to optimize the kernel.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import the required kernel functions and set a flexible kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, instantiate and fit the algorithm. Note that `alpha` is set to `5`
    for all values. I came up with that number as being around one-fourth of the target
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the predictions on unseen data as `test_preds`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1a8840b6-2a13-4f42-9d39-ff0de8bb8cf4.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-validation with the noise parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might wonder if that is the best noise parameter, `alpha=5`? To figure this
    out, try some cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, produce a cross-validation score with `alpha=5`. Note the scorer within
    the `cross_val_score` object is `neg_mean_absolute_error`, as the default R-squared
    score is hard to read for this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the scores in `scores_5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Observe that the scores in the last fold do not look the same as the other three.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now produce a report with `alpha=7`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This score looks a little better. Now, try `alpha=7` and `normalize_y` set
    to `True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks even better, as the mean is higher and the standard deviation is
    lower. Let''s select the last model for final training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a5930ca5-55ac-412c-8b48-5282cc355af9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The residuals look a bit more centered. You can also pass a NumPy array for
    `alpha`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to the following graphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/877acdd9-751d-40ab-a2db-8ab3246271bb.png)'
  prefs: []
  type: TYPE_IMG
- en: The array alphas are not compatible with `cross_val_score`, so I cannot select
    this model as the best model by looking at the final graphs and deciding which
    is the best. So, our final model selection is `gpr7n` with `alpha=7` and `normalize_y=True`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Underneath it all, the kernel computes covariances between points in `X`. It
    assumes that similar points in the inputs should lead to similar outputs. Gaussian
    processes are great for confidence predictions and smooth-like outputs. (Later,
    we will see random forests, that do not lead to smooth outputs even though they
    are very predictive.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We may need to understand the uncertainty in our estimates. If we pass the
    `eval_MSE` argument as true, we''ll get `MSE` and the predicted values, so we
    can make the predictions. A tuple of predictions and `MSE` is returned, from a
    mechanics standpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot all of the predictions with error bars as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0116515f-41b8-4bfd-9c78-911c85eac557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Set `n=20` in the preceding code to look at fewer points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/003714ee-b7a8-4bad-a8d8-81da8b3abe4e.png)'
  prefs: []
  type: TYPE_IMG
- en: The uncertainty is very high for some points. As you can see, there is a lot
    of of variance in the estimates for many of the given points. However, the overall
    error is not that bad.
  prefs: []
  type: TYPE_NORMAL
- en: Using SGD for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll get our first taste of stochastic gradient descent. We'll
    use it for regression here.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SGD is often an unsung hero in machine learning. Underneath many algorithms,
    there is SGD doing the work. It's popular due to its simplicity and speed—these
    are both very good things to have when dealing with a lot of data. The other nice
    thing about SGD is that while it's at the core of many machine learning algorithms
    computationally, it does so because it easily describes the process. At the end
    of the day, we apply some transformation on the data, and then we fit our data
    to the model with a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If SGD is good on large datasets, we should probably test it on a fairly large
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: It's probably worth gaining some intuition about the composition and size of
    the object. Thankfully, we're dealing with NumPy arrays, so we can just access
    `nbytes`. The built-in Python way to access the object's size doesn't work for
    NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'This output can be system dependent, so you may not get the same results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'To get some human perspective, we can convert `nbytes` to megabytes. There
    are roughly 1 million bytes in a megabyte:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the number of bytes per data point is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Well, isn't that tidy, and fairly tangential, for what we're trying to accomplish?
    However, it's worth knowing how to get the size of the objects you're dealing
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now that we have the data, we can simply fit a `SGDRegressor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: So, we have another beefy object. The main thing to know now is that our loss
    function is `squared_loss`, which is the same thing that occurs during linear
    regression. It is also worth noting that shuffle will generate a random shuffle
    of the data. This is useful if you want to break a potentially spurious correlation.
    With `X`, scikit-learn will automatically include a column of ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then predict, as we previously have, using scikit-learn''s consistent
    API. You can see we actually got a really good fit. There is barely any variation,
    and the histogram has a nice normal look.:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e0b40007-829c-455a-8bcc-bcb3866d8f80.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clearly, the fake dataset we used wasn't too bad, but you can imagine datasets
    with larger magnitudes. For example, if you worked on Wall Street on any given
    day, there might be 2 billion transactions on any given exchange in a market.
    Now, imagine that you have a week's or a year's data. Running in-core algorithms
    does not work with huge volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: The reason this is normally difficult is that to do SGD, we're required to calculate
    the gradient at every step. The gradient has the standard definition from any
    third calculus course.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gist of the algorithm is that at each step, we calculate a new set of coefficients
    and update this with a learning rate and the outcome of the objective function.
    In pseudocode, this might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`w`: This is the coefficient matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: This shows how big a step to take at each iteration. This
    might be important to tune if you aren''t getting good convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gradient`: This is the matrix of second derivatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cost`: This is the squared error for regression. We''ll see later that this
    cost function can be adapted to work with classification tasks. This flexibility
    is one thing that makes SGD so useful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will not be so bad, except for the fact that the gradient function is expensive.
    As the vector of coefficients gets larger, calculating the gradient becomes very
    expensive. For each update step, we need to calculate a new weight for every point
    in the data, and then update. SGD works slightly differently; instead of the previous
    definition for batch gradient descent, we'll update the parameter with each new
    data point. This data point is picked at random, hence the name stochastic gradient
    descent.
  prefs: []
  type: TYPE_NORMAL
- en: A final note on SGD is that it is a meta-heuristic that gives a lot of power
    to several machine learning algorithms. It is worth checking out some papers on
    meta-heuristics applied to various machine learning algorithms. Cutting-edge solutions
    might be innocently hidden in such papers.
  prefs: []
  type: TYPE_NORMAL

- en: Pre-Model Workflow and Pre-Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we will see the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Creating sample data for toy analysis
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling data to the standard normal distribution
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating binary features through thresholding
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with categorical variables
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputing missing values through various strategies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A linear model in the presence of outliers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together with pipelines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Gaussian processes for regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SGD for regression
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is data, and what are we doing with it?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A simple answer is that we attempt to place our data as points on paper, graph
    them, think, and look for simple explanations that approximate the data well.
    The simple geometric line of *F=ma* (force being proportional to acceleration)
    explained a lot of noisy data for hundreds of years. I tend to think of data science
    as data compression at times.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, when a machine is given only win-lose outcomes (of winning games
    of checkers, for example) and trained, I think of artificial intelligence. It
    is never taught explicit directions on how to play to win in such a case.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter deals with the pre-processing of data in scikit-learn. Some questions
    you can ask about your dataset are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Are there missing values in your dataset?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there outliers (points far away from the others) in your set?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the variables in the data like? Are they continuous quantities or categories?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do the continuous variable distributions look like? Can any of the variables
    in your dataset be described by normal distributions (bell-shaped curves)?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can any continuous variables be turned into categorical variables for simplicity?
    (This tends to be true if the distribution takes on very few particular values
    and not a continuous-like range of values.)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the units of the variables involved? Will you mix the variables somehow
    in the machine learning algorithm you chose to use?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These questions can have simple or complex answers. Thankfully, you ask them
    many times, even on the same dataset, and after these recipes you will have some
    practice at crafting answers to pre-processing machine learning questions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we will see pipelines: a great organizational tool to make sure
    we perform the same operations on both the training and testing sets without errors
    and with relatively little work. We will also see regression examples: **stochastic
    gradient descent** (**SGD**) and Gaussian processes.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Creating sample data for toy analysis
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If possible, use some of your own data for this book, but in the event you cannot,
    we'll learn how we can use scikit-learn to create toy data. scikit-learn's pseudo,
    theoretically constructed data is very interesting in its own right.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Very similar to getting built-in datasets, fetching new datasets, and creating
    sample datasets, the functions that are used follow the naming convention `make_*`.
    Just to be clear, this data is purely artificial:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To save typing, import the `datasets` module as `d`, and `numpy` as `np`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了省略输入，导入`datasets`模块为`d`，`numpy`为`np`：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: This section will walk you through the creation of several datasets. In addition
    to the sample datasets, these will be used throughout the book to create data
    with the necessary characteristics for the algorithms on display.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将带你逐步创建几个数据集。除了示例数据集外，这些数据集将贯穿整本书，用于创建具有算法所需特征的数据。
- en: Creating a regression dataset
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建回归数据集
- en: 'First, the stalwart—regression:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先是可靠的——回归：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By default, this will generate a tuple with a 100 x 100 matrix—100 samples by
    100 features. However, by default, only 10 features are responsible for the target
    data generation. The second member of the tuple is the target variable. It is
    also possible to get more involved in generating data for regression.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这将生成一个包含100 x 100矩阵的元组——100个样本和100个特征。然而，默认情况下，只有10个特征负责目标数据的生成。元组的第二个成员是目标变量。实际上，也可以更深入地参与回归数据的生成。
- en: 'For example, to generate a 1,000 x 10 matrix with five features responsible
    for the target creation, an underlying bias factor of 1.0, and 2 targets, the
    following command will be run:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，为了生成一个1000 x 10的矩阵，其中五个特征负责目标的创建，偏置因子为1.0，并且有两个目标，可以运行以下命令：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Creating an unbalanced classification dataset
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个不平衡的分类数据集
- en: Classification datasets are also very simple to create. It's simple to create
    a base classification set, but the basic case is rarely experienced in practice—most
    users don't convert, most transactions aren't fraudulent, and so on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 分类数据集也非常容易创建。创建一个基本的分类数据集很简单，但基本情况在实践中很少见——大多数用户不会转换，大多数交易不是欺诈性的，等等。
- en: 'Therefore, it''s useful to explore classification on unbalanced datasets:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，探索不平衡数据集上的分类是非常有用的：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Creating a dataset for clustering
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建聚类数据集
- en: Clusters will also be covered. There are actually several functions to create
    datasets that can be modeled by different cluster algorithms.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类也会涉及到。实际上，有多个函数可以创建适用于不同聚类算法的数据集。
- en: 'For example, blobs are very easy to create and can be modeled by k-means:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，blobs非常容易创建，并且可以通过k-means来建模：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will look like the following:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将看起来像这样：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/0bf9b285-7654-42fc-a2cb-4f14e9d40e44.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bf9b285-7654-42fc-a2cb-4f14e9d40e44.png)'
- en: How it works...
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Let's walk you through how scikit-learn produces the regression dataset by taking
    a look at the source code (with some modifications for clarity). Any undefined
    variables are assumed to have the default value of `make_regression`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看源代码（做了一些修改以便清晰）来逐步了解scikit-learn如何生成回归数据集。任何未定义的变量假定其默认值为`make_regression`。
- en: 'It''s actually surprisingly simple to follow. First, a random array is generated
    with the size specified when the function is called:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，跟着做是非常简单的。首先，生成一个随机数组，大小由调用函数时指定：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Given the basic dataset, the target dataset is then generated:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定基本数据集后，接着生成目标数据集：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The dot product of `X` and `ground_truth` are taken to get the final target
    values. Bias, if any, is added at this time:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 计算`X`和`ground_truth`的点积来得到最终的目标值。此时，如果有偏置，也会被加上：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The dot product is simply a matrix multiplication. So, our final dataset will
    have `n_samples`, which is the number of rows from the dataset, and `n_target,`
    which is the number of target variables.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 点积其实就是矩阵乘法。因此，我们的最终数据集将包含`n_samples`，即数据集的行数，和`n_target`，即目标变量的数量。
- en: Due to NumPy's broadcasting, bias can be a scalar value, and this value will
    be added to every sample. Finally, it's a simple matter of adding any noise and
    shuffling the dataset. Voila, we have a dataset that's perfect for testing regression.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于NumPy的广播机制，偏置可以是一个标量值，并且这个值将添加到每个样本中。最后，只需简单地加入噪声并打乱数据集。瞧，我们得到了一个非常适合回归测试的数据集。
- en: Scaling data to the standard normal distribution
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据缩放至标准正态分布
- en: A pre-processing step that is recommended is to scale columns to the standard
    normal. The standard normal is probably the most important distribution in statistics.
    If you've ever been introduced to statistics, you must have almost certainly seen
    z-scores. In truth, that's all this recipe is about—transforming our features
    from their endowed distribution into z-scores.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的预处理步骤是将列缩放至标准正态分布。标准正态分布可能是统计学中最重要的分布。如果你曾接触过统计学，你几乎肯定见过z分数。事实上，这就是这个方法的核心——将特征从其原始分布转换为z分数。
- en: Getting ready
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: The act of scaling data is extremely useful. There are a lot of machine learning
    algorithms, which perform differently (and incorrectly) in the event the features
    exist at different scales. For example, SVMs perform poorly if the data isn't
    scaled because they use a distance function in their optimization, which is biased
    if one feature varies from 0 to 10,000 and the other varies from 0 to 1.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'The `preprocessing` module contains several useful functions for scaling features:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Load the Boston dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How to do it...
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Continuing with the Boston dataset, run the following commands:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There''s actually a lot to learn from this initially. Firstly, the first feature
    has the smallest mean but varies even more than the third feature. The second
    feature has the largest mean and standard deviation—it takes the widest spread
    of values:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The centering and scaling function is extremely simple. It merely subtracts
    the mean and divides by the standard deviation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Pictorially and with pandas, the third feature looks as follows before the
    transformation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/0d0ec1e7-2c6e-447a-a34c-c6a548aab0db.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: 'This is what it looks like afterward:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/1a8a94aa-e223-4952-94f9-2ab4258b4705.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: The *x* axis label has changed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to a function, there is also a centering and scaling class that
    is easy to invoke, and this is particularly useful when used in conjunction with
    pipelines, which are mentioned later. It''s also useful for the centering and
    scaling class to persist across individual scaling:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Scaling features to a mean of zero and a standard deviation of one isn't the
    only useful type of scaling.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-processing also contains a `MinMaxScaler` class, which will scale the data
    within a certain range:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It''s very simple to change the minimum and maximum values of the `MinMaxScaler`
    class from its defaults of `0` and `1`, respectively:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Furthermore, another option is normalization. This will scale each sample to
    have a length of one. This is different from the other types of scaling done previously,
    where the features were scaled. Normalization is illustrated in the following
    command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If it's not apparent why this is useful, consider the Euclidean distance (a
    measure of similarity) between three of the samples, where one sample has the
    values *(1, 1, 0)*, another has *(3, 3, 0)*, and the final has *(1, -1, 0)*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The distance between the first and third vector is less than the distance between
    the first and second although the first and third are orthogonal, whereas the
    first and second only differ by a scalar factor of three. Since distances are
    often used as measures of similarity, not normalizing the data first can be misleading.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'From an alternative perspective, try the following syntax:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: All the rows are normalized and consist of vectors of length one. In three dimensions,
    all normalized vectors lie on the surface of a sphere centered at the origin.
    The information left is the direction of the vectors because, by definition, by
    normalizing you are dividing the vector by its length. Do always remember, though,
    that when performing this operation you have set an origin at *(0, 0, 0)* and
    you have turned any row of data in the array into a vector relative to this origin.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Creating binary features through thresholding
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last recipe, we looked at transforming our data into the standard normal
    distribution. Now, we'll talk about another transformation, one that is quite
    different. Instead of working with the distribution to standardize it, we'll purposely
    throw away data; if we have good reason, this can be a very smart move. Often,
    in what is ostensibly continuous data, there are discontinuities that can be determined
    via binary features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, note that in the previous chapter, we turned a classification
    problem into a regression problem. With thresholding, we can turn a regression
    problem into a classification problem. This happens in some data science contexts.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating binary features and outcomes is a very useful method, but it should
    be used with caution. Let''s use the Boston dataset to learn how to turn values
    into binary outcomes. First, load the Boston dataset:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How to do it...
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to scaling, there are two ways to binarize features in scikit-learn:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '`preprocessing.binarize`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preprocessing.Binarizer`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Boston dataset's `target` variable is the median value of houses in thousands.
    This dataset is good for testing regression and other continuous predictors, but
    consider a situation where we want to simply predict whether a house's value is
    more than the overall mean.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will want to create a threshold value of the mean. If the value
    is greater than the mean, produce a `1`; if it is less, produce a `0`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This was easy, but let''s check to make sure it worked correctly:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Given the simplicity of the operation in NumPy, it''s a fair question to ask
    why you would want to use the built-in functionality of scikit-learn. Pipelines,
    covered in the *Putting it all together with pipelines* recipe, will help to explain
    this; in anticipation of this, let''s use the `Binarizer` class:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There's more...
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's also learn about sparse matrices and the `fit` method.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Sparse matrices
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sparse matrices are special in that zeros aren''t stored; this is done in an
    effort to save space in memory. This creates an issue for the binarizer, so to
    combat it, a special condition for the binarizer for sparse matrices is that the
    threshold cannot be less than zero:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The fit method
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `fit` method exists for the binarizer transformation, but it will not fit
    anything; it will simply return the object. The object, however, will store the
    threshold and be ready for the `transform` method.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Working with categorical variables
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Categorical variables are a problem. On one hand they provide valuable information;
    on the other hand, it's probably text—either the actual text or integers corresponding
    to the text—such as an index in a lookup table.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量是一个问题。一方面，它们提供了有价值的信息；另一方面，它们很可能是文本——无论是实际的文本还是与文本对应的整数——例如查找表中的索引。
- en: So, we clearly need to represent our text as integers for the model's sake,
    but we can't just use the id field or naively represent them. This is because
    we need to avoid a similar problem to the *Creating binary features through thresholding*
    recipe. If we treat data that is continuous, it must be interpreted as continuous.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，显然我们需要将文本表示为整数，以便于模型处理，但不能仅仅使用ID字段或天真地表示它们。这是因为我们需要避免类似于*通过阈值创建二进制特征*食谱中出现的问题。如果我们处理的是连续数据，它必须被解释为连续数据。
- en: Getting ready
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The Boston dataset won't be useful for this section. While it's useful for feature
    binarization, it won't suffice for creating features from categorical variables.
    For this, the iris dataset will suffice.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿数据集对于本节不适用。虽然它对于特征二值化很有用，但不足以从类别变量中创建特征。为此，鸢尾花数据集就足够了。
- en: For this to work, the problem needs to be turned on its head. Imagine a problem
    where the goal is to predict the sepal width; in this case, the species of the
    flower will probably be useful as a feature.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其生效，问题需要彻底转变。设想一个问题，目标是预测花萼宽度；在这种情况下，花卉的物种可能作为一个特征是有用的。
- en: How to do it...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Let''s get the data sorted first:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先整理数据：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Place `X` and `y`, all of the numerical data, side-by-side. Create an encoder
    with scikit-learn to handle the category of the `y` column:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`X`和`y`，所有数值数据，放在一起。使用scikit-learn创建一个编码器来处理`y`列的类别：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The encoder creates additional features for each categorical variable, and
    the value returned is a sparse matrix. The result is a sparse matrix by definition;
    each row of the new features has `0` everywhere, except for the column whose value
    is associated with the feature''s category. Therefore, it makes sense to store
    this data in a sparse matrix. The `cat_encoder` is now a standard scikit-learn
    model, which means that it can be used again:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器为每个类别变量创建附加特征，返回的值是一个稀疏矩阵。根据定义，结果是一个稀疏矩阵；新特征的每一行除了与特征类别关联的列外，其他位置都是`0`。因此，将这些数据存储为稀疏矩阵是合理的。现在，`cat_encoder`是一个标准的scikit-learn模型，这意味着它可以再次使用：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the previous chapter, we turned a classification problem into a regression
    problem. Here, there are three columns:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们将一个分类问题转化为回归问题。在这里，有三列数据：
- en: The first column is `1` if the flower is a Setosa and `0` otherwise
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一列是`1`，如果花是Setosa，则为`1`，否则为`0`。
- en: The second column is `1` if the flower is a Versicolor and `0` otherwise
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二列是`1`，如果花是Versicolor，则为`1`，否则为`0`。
- en: The third column is `1` if the flower is a Virginica and `0` otherwise
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三列是`1`，如果花是Virginica，则为`1`，否则为`0`。
- en: Thus, we could use any of these three columns to create a regression similar
    to the one in the previous chapter; we will perform a regression to determine
    the degree of setosaness of a flower as a real number. The matching statement
    in classification is whether a flower is a Setosa one or not. This is the problem
    statement if we perform binary classification of the first column.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用这三列中的任何一列来创建与上一章类似的回归；我们将执行回归以确定花卉的Setosa程度作为一个实数。如果我们对第一列进行二分类，这就是分类中的问题陈述，判断花卉是否是Setosa。
- en: scikit-learn has the capacity for this type of multi-output regression. Compare
    it with multiclass classification. Let's try a simple one.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn具有执行此类型的多输出回归的能力。与多类分类相比，让我们尝试一个简单的例子。
- en: 'Import the ridge regression regularized linear model. It tends to be very well
    behaved because it is regularized. Instantiate a ridge regressor class:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 导入岭回归正则化线性模型。由于它是正则化的，通常表现得非常稳定。实例化一个岭回归器类：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now import a multi-output regressor that takes the ridge regressor instance
    as an argument:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在导入一个多输出回归器，将岭回归器实例作为参数：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'From earlier in this recipe, transform the target variable `y` to a three-part
    target variable, `y_multi`, with `OneHotEncoder()`. If `X` and `y` were part of
    a pipeline, the pipeline would transform the training and testing sets separately,
    and this is preferable:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从本食谱前面的部分，将目标变量`y`转换为三部分目标变量`y_multi`，并使用`OneHotEncoder()`。如果`X`和`y`是管道的一部分，管道将分别转换训练集和测试集，这是更可取的做法：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create training and testing sets:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Fit the multi-output estimator:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Predict the multi-output target on the testing set:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Use the `binarize` function from the previous recipe to turn each real number
    into the integers `0` or `1`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can measure the overall multi-output performance with the `roc_auc_score`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Or, we can do it flower type by flower type, column by column:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: There's more...
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding multi-output regression, you could be concerned with the dummy
    variable trap: the collinearity of the outputs. Without dropping any output columns,
    you assume that there is a fourth option: that a flower can be of none of the
    three types. To prevent the trap, drop the last column and assume that the flower
    has to be of one of the three types as we do not have any training examples where
    it is not one of the three flower types.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to create categorical variables in scikit-learn and Python.
    The `DictVectorizer` class is a good option if you like to limit the dependencies
    of your projects to only scikit-learn and you have a fairly simple encoding scheme.
    However, if you require more sophisticated categorical encoding, patsy is a very
    good option.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: DictVectorizer class
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another option is to use `DictVectorizer` class. This can be used to directly
    convert strings to features:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Imputing missing values through various strategies
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data imputation is critical in practice, and thankfully there are many ways
    to deal with it. In this recipe, we'll look at a few of the strategies. However,
    be aware that there might be other approaches that fit your situation better.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: This means scikit-learn comes with the ability to perform fairly common imputations;
    it will simply apply some transformations to the existing data and fill the NAs.
    However, if the dataset is missing data, and there's a known reason for this missing
    data—for example, response times for a server that times out after 100 ms—it might
    be better to take a statistical approach through other packages, such as the Bayesian
    treatment via PyMC, hazards models via Lifelines, or something home-grown.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing to do when learning how to input missing values is to create
    missing values. NumPy''s masking will make this extremely simple:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: To unravel this a bit, in case NumPy isn't too familiar, it's possible to index
    arrays with other arrays in NumPy. So, to create the random missing data, a random
    Boolean array is created, which is of the same shape as the iris dataset. Then,
    it's possible to make an assignment via the masked array. It's important to note
    that because a random array is used, it is likely that your `masking_array` will
    be different from what's used here.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure this works, use the following command (since we''re using a random
    mask, it might not match directly):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How to do it...
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A theme prevalent throughout this book (due to the theme throughout scikit-learn)
    is reusable classes that fit and transform datasets that can subsequently be used
    to transform unseen datasets. This is illustrated as follows:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Notice the difference in the position `[0, 0]`:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: How it works...
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The imputation works by employing different strategies. The default is mean,
    but in total there are the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '`mean` (default)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`median`'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`most_frequent` (mode)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'scikit-learn will use the selected strategy to calculate the value for each
    non-missing value in the dataset. It will then simply fill the missing values.
    For example, to redo the iris example with the median strategy, simply reinitialize
    impute with the new strategy:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: If the data is missing values, it might be inherently dirty in other places.
    For instance, in the example in the preceding, *How to do it...* section, `np.nan`
    (the default missing value) was used as the missing value, but missing values
    can be represented in many ways. Consider a situation where missing values are
    `-1`. In addition to the strategy to compute the missing value, it's also possible
    to specify the missing value for the imputer. The default is `nan`, which will
    handle `np.nan` values.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'To see an example of this, modify `iris_X` to have `-1` as the missing value.
    It sounds crazy, but since the iris dataset contains measurements that are always
    possible, many people will fill the missing values with `-1` to signify they''re
    not there:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Filling these in is as simple as the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: There's more...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pandas also provides a functionality to fill in missing data. It actually might
    be a bit more flexible, but it is less reusable:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To mention its flexibility, `fillna` can be passed any sort of statistic, that
    is, the strategy is more arbitrarily defined:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: A linear model in the presence of outliers
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, instead of traditional linear regression we will try using the
    Theil-Sen estimator to deal with some outliers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, create the data corresponding to a line with a slope of `2`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/d1ebad85-c139-418a-94ec-06c1d3ce5791.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: 'Add noise to that data and label it as `y_noisy`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](img/f9790eae-a74e-4d0f-93b0-45057e7afff8.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import both `LinearRegression` and `TheilSenRegressor`. Score the estimators
    using the original line as the testing set, `y_truth`:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Plot the lines. Note that **o****rdinary least squares** (**OLS**) is way off
    the true line, `y_truth`. Theil-Sen overlaps the real line:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![](img/87bf00b0-a0bf-4e0e-8e82-8fcdd3a54a7d.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'Plot the dataset and the estimated lines:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/24418ea6-bb8c-4d1d-90bf-c357f9d6bd18.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: How it works...
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `TheilSenRegressor` is a robust estimator that performs well in the presence
    of outliers. It uses the measurement of medians, which is more robust to outliers.
    In OLS regression, errors are squared, and thus a squared error can decrease good
    results.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'You can try several robust estimators in scikit-learn Version 0.19.0:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As you can see, the robust linear estimators Theil-Sen, **random sample consensus**
    (**RANSAC**), and the Huber regressor out-perform the other linear regressors
    in the presence of outliers.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在存在异常值的情况下，稳健的线性估计器 Theil-Sen、**随机样本一致性**（**RANSAC**）和 Huber 回归器的表现优于其他线性回归器。
- en: Putting it all together with pipelines
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一切整合到管道中
- en: Now that we've used pipelines and data transformation techniques, we'll walk
    through a more complicated example that combines several of the previous recipes
    into a pipeline.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用了管道和数据转换技术，我们将通过一个更复杂的例子，结合之前的几个实例，演示如何将它们组合成一个管道。
- en: Getting ready
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this section, we'll show off some more of pipeline's power. When we used
    it earlier to impute missing values, it was only a quick taste; here, we'll chain
    together multiple pre-processing steps to show how pipelines can remove extra
    work.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示管道的更多强大功能。当我们之前用它来填补缺失值时，只是简单体验了一下；这里，我们将把多个预处理步骤链起来，展示管道如何去除额外的工作。
- en: 'Let''s briefly load the iris dataset and seed it with some missing values:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简单加载鸢尾花数据集，并给它添加一些缺失值：
- en: '[PRE54]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: How to do it...
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现…
- en: The goal of this chapter is to first impute the missing values of `iris_data`,
    and then perform PCA on the corrected dataset. You can imagine (and we'll do it
    later) that this workflow might need to be split between a training dataset and
    a holdout set; pipelines will make this easier, but first we need to take a baby
    step.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是首先填补`iris_data`的缺失值，然后对修正后的数据集执行PCA。你可以想象（我们稍后会做）这个工作流程可能需要在训练数据集和保留集之间拆分；管道将使这更容易，但首先我们需要迈出小小的一步。
- en: 'Let''s load the required libraries:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载所需的库：
- en: '[PRE55]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, create the `imputer` and `pca` classes:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建`imputer`和`pca`类：
- en: '[PRE56]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now that we have the classes we need, we can load them into `Pipeline`:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了需要的类，我们可以将它们加载到`Pipeline`中：
- en: '[PRE57]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: This takes a lot more management if we use separate steps. Instead of each step
    requiring a fit transform, this step is performed only once, not to mention that
    we only have to keep track of one object!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用单独的步骤，这需要更多的管理。与每个步骤都需要进行拟合转换不同，这个步骤只需执行一次，更不用说我们只需要跟踪一个对象！
- en: How it works...
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Hopefully it was obvious, but each step in a pipeline is passed to a pipeline
    object via a list of tuples, with the first element getting the name and the second
    getting the actual object. Under the hood, these steps are looped through when
    a method such as `fit_transform` is called on the pipeline object.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 希望大家已经明白，每个管道中的步骤都是通过元组列表传递给管道对象的，第一个元素是名称，第二个元素是实际的对象。在幕后，当调用像`fit_transform`这样的函数时，这些步骤会在管道对象上循环执行。
- en: 'This said, there are quick and dirty ways to create a pipeline, much in the
    same way there was a quick way to perform scaling, though we can use `StandardScaler`
    if we want more power. The `pipeline` function will automatically create the names
    for the pipeline objects:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，确实有一些快速且简便的方式来创建管道，就像我们之前有一种快速的方式来执行缩放操作一样，尽管我们可以使用`StandardScaler`来获得更强大的功能。`pipeline`函数将自动为管道对象创建名称：
- en: '[PRE58]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This is the same object that was created in the more verbose method:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在更详细的方法中创建的相同对象：
- en: '[PRE59]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: There's more...
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We just walked through pipelines at a very high level, but it''s unlikely that
    we will want to apply the base transformation. Therefore, the attributes of each
    object in a pipeline can be accessed using a `set_params` method, where the parameter
    follows the `<step_name>__<step_parameter>` convention. For example, let''s change
    the `pca` object to use two components:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚以很高的层次走过了管道，但不太可能希望直接应用基本的转换。因此，可以通过`set_params`方法访问管道中每个对象的属性，其中参数遵循`<step_name>__<step_parameter>`的约定。例如，假设我们想把`pca`对象改为使用两个主成分：
- en: '[PRE60]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Notice, `n_components=2` in the preceding output. Just as a test, we can output
    the same transformation we have already done twice, and the output will be an
    N x 2 matrix:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的输出中有`n_components=2`。作为测试，我们可以输出之前已经做过两次的相同变换，输出将是一个 N x 2 的矩阵：
- en: '[PRE61]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Using Gaussian processes for regression
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用高斯过程进行回归
- en: In this recipe, we'll use a Gaussian process for regression. In the linear models
    section, we will see how representing prior information on the coefficients was
    possible using Bayesian ridge regression.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用高斯过程进行回归。在线性模型部分，我们将看到如何通过贝叶斯岭回归表示系数的先验信息。
- en: With a Gaussian process, it's about the variance and not the mean. However,
    with a Gaussian process, we assume the mean is 0, so it's the covariance function
    we'll need to specify.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The basic setup is similar to how a prior can be put on the coefficients in
    a typical regression problem. With a Gaussian process, a prior can be put on the
    functional form of the data, and it's the covariance between the data points that
    is used to model the data, and therefore, must fit the data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'A big advantage of Gaussian processes is that they can predict probabilistically:
    you can obtain confidence bounds on your predictions. Additionally, the prediction
    can interpolate the observations for the available kernels: predictions from regression
    are smooth and thus a prediction between two points you know about is between
    those two points.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: A disadvantage of Gaussian processes is lack of efficiency in high-dimensional
    spaces.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, let''s use some regression data and walk through how Gaussian processes
    work in scikit-learn:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: How to do it…
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have the data, we''ll create a scikit-learn `GaussianProcessRegressor` object.
    Let''s look at the `gpr` object:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'There are a few parameters that are important and must be set:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '`alpha`: This is a noise parameter. You can assign a noise value for all observations
    or assign `n` values in the form of a NumPy array where `n` is the length of the
    target observations in the training set you pass to `gpr` for training.'
  id: totrans-257
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel`: This is a kernel that approximates a function. The default in a previous
    version of scikit-learn was **radial basis functions** (**RBF**), and we will
    construct a flexible kernel from constant kernels and RBF kernels.'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize_y`:  You can set it to true if the mean of the target set is not
    zero. If you leave it set to false, it still works fairly well.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_restarts_optimizer`: Set this to 10-20 for practical use. This is the number
    of iterations to optimize the kernel.'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import the required kernel functions and set a flexible kernel:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, instantiate and fit the algorithm. Note that `alpha` is set to `5`
    for all values. I came up with that number as being around one-fourth of the target
    values:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Store the predictions on unseen data as `test_preds`:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Plot the results:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](img/1a8840b6-2a13-4f42-9d39-ff0de8bb8cf4.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Cross-validation with the noise parameter
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might wonder if that is the best noise parameter, `alpha=5`? To figure this
    out, try some cross-validation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'First, produce a cross-validation score with `alpha=5`. Note the scorer within
    the `cross_val_score` object is `neg_mean_absolute_error`, as the default R-squared
    score is hard to read for this dataset:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Look at the scores in `scores_5`:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Observe that the scores in the last fold do not look the same as the other three.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Now produce a report with `alpha=7`:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This score looks a little better. Now, try `alpha=7` and `normalize_y` set
    to `True`:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This looks even better, as the mean is higher and the standard deviation is
    lower. Let''s select the last model for final training:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Predict it:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Visualize the results:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a5930ca5-55ac-412c-8b48-5282cc355af9.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: 'The residuals look a bit more centered. You can also pass a NumPy array for
    `alpha`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This leads to the following graphs:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/877acdd9-751d-40ab-a2db-8ab3246271bb.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: The array alphas are not compatible with `cross_val_score`, so I cannot select
    this model as the best model by looking at the final graphs and deciding which
    is the best. So, our final model selection is `gpr7n` with `alpha=7` and `normalize_y=True`.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Underneath it all, the kernel computes covariances between points in `X`. It
    assumes that similar points in the inputs should lead to similar outputs. Gaussian
    processes are great for confidence predictions and smooth-like outputs. (Later,
    we will see random forests, that do not lead to smooth outputs even though they
    are very predictive.)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'We may need to understand the uncertainty in our estimates. If we pass the
    `eval_MSE` argument as true, we''ll get `MSE` and the predicted values, so we
    can make the predictions. A tuple of predictions and `MSE` is returned, from a
    mechanics standpoint:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Plot all of the predictions with error bars as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '![](img/0116515f-41b8-4bfd-9c78-911c85eac557.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Set `n=20` in the preceding code to look at fewer points:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/003714ee-b7a8-4bad-a8d8-81da8b3abe4e.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: The uncertainty is very high for some points. As you can see, there is a lot
    of of variance in the estimates for many of the given points. However, the overall
    error is not that bad.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Using SGD for regression
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll get our first taste of stochastic gradient descent. We'll
    use it for regression here.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SGD is often an unsung hero in machine learning. Underneath many algorithms,
    there is SGD doing the work. It's popular due to its simplicity and speed—these
    are both very good things to have when dealing with a lot of data. The other nice
    thing about SGD is that while it's at the core of many machine learning algorithms
    computationally, it does so because it easily describes the process. At the end
    of the day, we apply some transformation on the data, and then we fit our data
    to the model with a loss function.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If SGD is good on large datasets, we should probably test it on a fairly large
    dataset:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: It's probably worth gaining some intuition about the composition and size of
    the object. Thankfully, we're dealing with NumPy arrays, so we can just access
    `nbytes`. The built-in Python way to access the object's size doesn't work for
    NumPy arrays.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'This output can be system dependent, so you may not get the same results:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'To get some human perspective, we can convert `nbytes` to megabytes. There
    are roughly 1 million bytes in a megabyte:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'So, the number of bytes per data point is as follows:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Well, isn't that tidy, and fairly tangential, for what we're trying to accomplish?
    However, it's worth knowing how to get the size of the objects you're dealing
    with.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now that we have the data, we can simply fit a `SGDRegressor`:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: So, we have another beefy object. The main thing to know now is that our loss
    function is `squared_loss`, which is the same thing that occurs during linear
    regression. It is also worth noting that shuffle will generate a random shuffle
    of the data. This is useful if you want to break a potentially spurious correlation.
    With `X`, scikit-learn will automatically include a column of ones.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then predict, as we previously have, using scikit-learn''s consistent
    API. You can see we actually got a really good fit. There is barely any variation,
    and the histogram has a nice normal look.:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![](img/e0b40007-829c-455a-8bcc-bcb3866d8f80.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: How it works…
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clearly, the fake dataset we used wasn't too bad, but you can imagine datasets
    with larger magnitudes. For example, if you worked on Wall Street on any given
    day, there might be 2 billion transactions on any given exchange in a market.
    Now, imagine that you have a week's or a year's data. Running in-core algorithms
    does not work with huge volumes of data.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: The reason this is normally difficult is that to do SGD, we're required to calculate
    the gradient at every step. The gradient has the standard definition from any
    third calculus course.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'The gist of the algorithm is that at each step, we calculate a new set of coefficients
    and update this with a learning rate and the outcome of the objective function.
    In pseudocode, this might look like the following:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The relevant variables are as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '`w`: This is the coefficient matrix.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: This shows how big a step to take at each iteration. This
    might be important to tune if you aren''t getting good convergence.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gradient`: This is the matrix of second derivatives.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cost`: This is the squared error for regression. We''ll see later that this
    cost function can be adapted to work with classification tasks. This flexibility
    is one thing that makes SGD so useful.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will not be so bad, except for the fact that the gradient function is expensive.
    As the vector of coefficients gets larger, calculating the gradient becomes very
    expensive. For each update step, we need to calculate a new weight for every point
    in the data, and then update. SGD works slightly differently; instead of the previous
    definition for batch gradient descent, we'll update the parameter with each new
    data point. This data point is picked at random, hence the name stochastic gradient
    descent.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: A final note on SGD is that it is a meta-heuristic that gives a lot of power
    to several machine learning algorithms. It is worth checking out some papers on
    meta-heuristics applied to various machine learning algorithms. Cutting-edge solutions
    might be innocently hidden in such papers.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL

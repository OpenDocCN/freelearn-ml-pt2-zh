<html><head></head><body>
		<div id="_idContainer057">
			<h1 id="_idParaDest-87"><em class="italic"><a id="_idTextAnchor093"/>Chapter 4</em>: From Gradient Boosting to XGBoost</h1>
			<p>XGBoost is a unique form of gradient boosting with several distinct advantages, which will be explained in <a href="B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117"><em class="italic">Chapter 5</em></a>, <em class="italic">XGBoost Unveiled</em>. In order to understand the advantages of XGBoost over traditional gradient boosting, you must first learn how traditional gradient boosting works. The general structure and hyperparameters of traditional gradient boosting are incorporated by XGBoost. In this chapter, you will discover the power behind gradient boosting, which is at the core of XGBoost.</p>
			<p>In this chapter, you will build gradient boosting models from scratch before comparing gradient boosting models and errors with previous results. In particular, you will focus on the <strong class="bold">learning rate</strong> hyperparameter to build powerful gradient boosting models that include XGBoost. Finally, you will preview a case study on exoplanets highlighting the need for faster algorithms, a critical need in the world of big data that is satisfied by XGBoost.</p>
			<p>In this chapter, we will be covering the following main topics:</p>
			<ul>
				<li><p>From bagging to boosting</p></li>
				<li><p>How gradient boosting works</p></li>
				<li><p>Modifying gradient boosting hyperparameters</p></li>
				<li><p>Approaching big data – gradient boosting versus XGBoost</p></li>
			</ul>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor094"/>Technical requirements</h1>
			<p>The code for this chapter is available at <a href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04">https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04</a>.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor095"/>From bagging to boosting</h1>
			<p>In <a href="B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070"><em class="italic">Chapter 3</em></a>,<em class="italic"> Bagging with Random Forests</em>, you learned why ensemble machine learning algorithms such as <a id="_idIndexMarker224"/>random forests make better predictions by combining many machine learning models into one. Random forests are classified as bagging algorithms because they take the aggregates of bootstrapped samples (decision trees).</p>
			<p>Boosting, by contrast, learns from the mistakes of individual trees. The general idea is to adjust new trees based on the errors of previous trees.</p>
			<p>In boosting, correcting errors for each new tree is a distinct approach from bagging. In a bagging model, new trees pay no attention to previous trees. Also, new trees are built from scratch<a id="_idTextAnchor096"/> using bootstrapping, and the final model aggregates all individual trees. In boosting, however, each new tree is built from the previous tree. The trees do not operate in isolation; instead, they are built on top of one another.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor097"/>Introducing AdaBoost</h2>
			<p><strong class="bold">AdaBoost</strong> is one of the earliest and most popular boosting models. In AdaBoost, each new tree adjusts its <a id="_idIndexMarker225"/>weights based on the errors of the previous trees. More attention is paid to predictions that went wrong by adjusting weights that affect those samples at a higher percentage. By learning from its mistakes, AdaBoost can transform weak learners into strong learners. A weak learner is a machine learning algorithm that barely performs better than chance. By contrast, a stronger learner has learned a considerable amount from data and performs quite well.</p>
			<p>The general idea behind boosting algorithms is to transform weak learners into strong learners. A weak learner is hardly better than random guessing. But there is a purpose behind the weak start. Building on this general idea, boosting works by focusing on iterative error correction, <em class="italic">not</em> by establishing a strong baseline model. If the base model is too strong, the learning process is necessarily limited, thereby undermining the general strategy behind boosting models.</p>
			<p>Weak learners are transformed into strong learners through hundreds of iterations. In this sense, a small edge goes a long way. In fact, boosting has been one of the best general machine learning strategies in terms of producing optimal results for the past couple of decades.</p>
			<p>A detailed study of AdaBoost is beyond the scope of this book. Like many scikit-learn models, it's straightforward to implement AdaBoost in practice. The <strong class="source-inline">AdaBoostRegressor</strong> and <strong class="source-inline">AdaBoostClassifier</strong> algorithms may be downloaded from the <strong class="source-inline">sklearn.ensemble</strong> library and fit to any training set. The most important AdaBoost hyperparameter is <strong class="source-inline">n_estimators</strong>, the number of trees (iterations) required to create a strong learner.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For further information on AdaBoost, check out the official documentation at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html</a> for classifiers and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html</a> for regressors.</p>
			<p>We will now move on to <a id="_idIndexMarker226"/>gradient boosting, a strong alternative to AdaBoost with a slight edge in performance.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor098"/>Distinguishing gradient boosting</h2>
			<p>Gradient boosting uses a different approach than AdaBoost. While gradient boosting also adjusts based on<a id="_idIndexMarker227"/> incorrect predictions, it takes this idea one step further: gradient boosting fits each new tree entirely based on the errors of the previous tree's predictions. That is, for each new tree, gradient boosting looks at the mistakes and then builds a new tree completely around these mistakes. The new tree doesn't care about the predictions that are already correct.</p>
			<p>Building a machine learning algorithm that solely focuses on the errors requires a comprehensive method that sums errors to make accurate final predictions. This method leverages residuals, the difference between the model's predictions and actual values. Here is the general idea:</p>
			<p><em class="italic">Gradient boosting computes the residuals of each tree's predictions and sums all the residuals to score the model.</em></p>
			<p>It's essential to understand <strong class="bold">computing</strong> and <strong class="bold">summing residuals</strong> as this idea is at the core of XGBoost, an advanced version of gradient boosting. When you build your own version of gradient boosting, the process of computing and summing residuals will become clear. In the next section, you will build your own version of a gradient boosting model. First, let's learn in detail how gradient boosting works.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor099"/>How gradient boosting works</h1>
			<p>In this section, we will <a id="_idIndexMarker228"/>look under the hood of gradient boosting and build a gradient boosting model from scratch by training new trees on the errors of the previous trees. The key mathematical idea here is the residual. Next, we will obtain the same results using scikit-learn's gradient boosting algorithm.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor100"/>Residuals</h2>
			<p>The residuals are the <a id="_idIndexMarker229"/>difference between the errors and the predictions <a id="_idIndexMarker230"/>of a given model. In statistics, residuals are commonly analyzed to determine how good a given linear regression model fits the data.</p>
			<p>Consider the following examples:</p>
			<ol>
				<li><p>Bike rentals</p><p>a) <em class="italic">Prediction</em>: 759</p><p>b) <em class="italic">Result</em>: 799</p><p>c) <em class="italic">Residual</em>: 799 - 759 = 40</p></li>
				<li><p>Income</p><p>a) <em class="italic">Prediction</em>: 100,000</p><p>b) <em class="italic">Result</em>: 88,000</p><p>c) <em class="italic">Residual</em>: 88,000 –100,000 = -12,000</p></li>
			</ol>
			<p>As you can see, residuals tell you how far the model's predictions are from reality, and they may be positive or negative.</p>
			<p>Here is a visual example displaying the residuals of a <strong class="bold">linear regression</strong> line:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B15551_04_01.jpg" alt="Figure 4.1 – Residuals of a linear regression line"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Residuals of a linear regression line</p>
			<p>The goal of linear regression is to minimize the square of the residuals. As the graph reveals, a visual of <a id="_idIndexMarker231"/>the residuals indicates how well the line fits the data. In <a id="_idIndexMarker232"/>statistics, linear regression analysis is often performed by graphing the residuals to gain deeper insight into the data.</p>
			<p>In order to build a gradient boosting algorithm from scratch, we will compute the residuals of each tree and fit a new model to the residuals. Let's do this now.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor101"/>Learning how to build gradient boosting models from scratch</h2>
			<p>Building a gradient <a id="_idIndexMarker233"/>boosting model from scratch will provide you with a deeper understanding of how gradient boosting works in code. Before building a model, we need to access data and prepare it for machine learning.</p>
			<h3>Processing the bike rentals dataset</h3>
			<p>We continue with the bike <a id="_idIndexMarker234"/>rentals dataset to compare new models with the previous models:</p>
			<ol>
				<li value="1"><p>We will start by importing <strong class="source-inline">pandas</strong> and <strong class="source-inline">numpy</strong>. We will also add a line to silence any warnings:</p><p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import warnings</p><p class="source-code">warnings.filterwarnings('ignore')</p></li>
				<li><p>Now, load the <strong class="source-inline">bike_rentals_cleaned</strong> dataset and view the first five rows:</p><p class="source-code">df_bikes = pd.read_csv('bike_rentals_cleaned.csv')</p><p class="source-code">df_bikes.head()</p><p>Your output should look like this:</p><div id="_idContainer053" class="IMG---Figure"><img src="image/B15551_04_02.jpg" alt="Figure 4.2 – First five rows of Bike Rental Dataset"/></div><p class="figure-caption">Figure 4.2 – First five rows of Bike Rental Dataset</p></li>
				<li><p>Now, split the data into <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong>. Then, split <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> into training and test sets:</p><p class="source-code">X_bikes = df_bikes.iloc[:,:-1]</p><p class="source-code">y_bikes = df_bikes.iloc[:,-1]</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)</p></li>
			</ol>
			<p>It's time to build a gradient boosting model from scratch!</p>
			<h3>Building a gradient boosting model from scratch</h3>
			<p>Here are the steps for building a <a id="_idIndexMarker235"/>gradient boosting machine learning model from scratch:</p>
			<ol>
				<li value="1"><p>Fit the data to the decision tree: You may use a decision tree stump, which has a <strong class="source-inline">max_depth</strong> value of <strong class="source-inline">1</strong>, or a decision tree with a <strong class="source-inline">max_depth</strong> value of <strong class="source-inline">2</strong> or <strong class="source-inline">3</strong>. The initial <a id="_idIndexMarker236"/>decision tree, called a <strong class="bold">base learner</strong>, should not be fine-tuned for accuracy. We want a model that focuses on learning from errors, not a model that relies heavily on the base learner.</p><p>Initialize a decision tree with <strong class="source-inline">max_depth=2</strong> and fit it on the training set as <strong class="source-inline">tree_1</strong>, since it's the first tree in our ensemble:</p><p class="source-code">from sklearn.tree import DecisionTreeRegressor</p><p class="source-code">tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)</p><p class="source-code">tree_1.fit(X_train, y_train)</p></li>
				<li><p>Make predictions with the training set: Instead of making predictions with the test set, predictions in gradient boosting are initially made with the training set. Why? To compute the residuals, we need to compare the predictions while still in the training phase. The test phase of the model build comes at the end, after all the trees have been constructed. The predictions of the training set for the first round are obtained by adding the <strong class="source-inline">predict</strong> method to <strong class="source-inline">tree_1</strong> with <strong class="source-inline">X_train</strong> as the input:</p><p class="source-code">y_train_pred = tree_1.predict(X_train)</p></li>
				<li><p>Compute the residuals: The residuals are the differences between the predictions and the target column. The predictions of <strong class="source-inline">X_train</strong>, defined here as <strong class="source-inline">y_train_pred</strong>, are subtracted from <strong class="source-inline">y_train</strong>, the target column, to compute the residuals:</p><p class="source-code">y2_train = y_train - y_train_pred</p><p class="callout-heading">Note </p><p class="callout">The residuals are defined as <strong class="source-inline">y2_train</strong> because they are the new target column for the next tree.</p></li>
				<li><p>Fit the new tree on the residuals: Fitting a new tree on the residuals is different than fitting a model on the training set. The primary difference is in the predictions. In the bike rentals dataset, when fitting a new tree on the residuals, we should progressively get smaller numbers.</p><p>Initialize a new tree and fit it on <strong class="source-inline">X_train</strong> and the residuals, <strong class="source-inline">y2_train</strong>:</p><p class="source-code">tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)</p><p class="source-code">tree_2.fit(X_train, y2_train)</p></li>
				<li><p>Repeat steps 2-4: As the <a id="_idIndexMarker237"/>process continues, the residuals should gradually approach <strong class="source-inline">0</strong> from the positive and negative direction. The iterations continue for the number of estimators, <strong class="source-inline">n_estimators</strong>.</p><p>Let's repeat the process for a third tree as follows:</p><p class="source-code">y2_train_pred = tree_2.predict(X_train)</p><p class="source-code">y3_train = y2_train - y2_train_pred</p><p class="source-code">tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)</p><p class="source-code">tree_3.fit(X_train, y3_train)</p><p>This process may continue for dozens, hundreds, or thousands of trees. Under normal circumstances, you would certainly keep going. It will take more than a few trees to transform a weak learner into a strong learner. Since our goal is to understand how gradient boosting works behind the scenes, however, we will move on now that the general idea has been covered.</p></li>
				<li><p>Sum the results: Summing the results requires making predictions for each tree with the test set as follows:</p><p class="source-code">y1_pred = tree_1.predict(X_test)</p><p class="source-code">y2_pred = tree_2.predict(X_test)</p><p class="source-code">y3_pred = tree_3.predict(X_test)</p><p>Since the predictions are positive and negative differences, summing the predictions should result in predictions that are closer to the target column as follows:</p><p class="source-code">y_pred = y1_pred + y2_pred + y3_pred</p></li>
				<li><p>Lastly, let's compute the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) to obtain the results as follows:</p><p class="source-code">from sklearn.metrics import mean_squared_error as MSE</p><p class="source-code">MSE(y_test, y_pred)**0.5</p><p>Here is the expected output:</p><p class="source-code">911.0479538776444</p></li>
			</ol>
			<p>Not bad for a weak <a id="_idIndexMarker238"/>learner that isn't yet strong! Now let's try to obtain the same result using scikit-learn.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor102"/>Building a gradient boosting model in scikit-learn</h2>
			<p>Let's see whether <a id="_idIndexMarker239"/>we can obtain the same result as in the previous section using scikit-learn's <strong class="source-inline">GradientBoostingRegressor</strong>. This <a id="_idIndexMarker240"/>may be done through a few hyperparameter adjustments. The advantage of using <strong class="source-inline">GradientBoostingRegressor</strong> is that it's much faster to build and easier to implement:</p>
			<ol>
				<li value="1"><p>First, import the regressor from the <strong class="source-inline">sklearn.ensemble</strong> library:</p><p class="source-code">from sklearn.ensemble import GradientBoostingRegressor</p></li>
				<li><p>When initializing <strong class="source-inline">GradientBoostingRegressor</strong>, there are several important hyperparameters. To obtain the same results, it's essential to match <strong class="source-inline">max_depth=2</strong> and <strong class="source-inline">random_state=2</strong>. Furthermore, since there are only three trees, we must have <strong class="source-inline">n_estimators=3</strong>. Finally, we must set the <strong class="source-inline">learning_rate=1.0</strong> hyperparameter. We will have much to say about <strong class="source-inline">learning_rate</strong> shortly:</p><p class="source-code">gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=2, learning_rate=1.0)</p></li>
				<li><p>Now that the <a id="_idIndexMarker241"/>model has been initialized, it can be fit on the training data and scored <a id="_idIndexMarker242"/>against the test data:</p><p class="source-code">gbr.fit(X_train, y_train)</p><p class="source-code">y_pred = gbr.predict(X_test)</p><p class="source-code">MSE(y_test, y_pred)**0.5</p><p>The result is as follows:</p><p class="source-code">911.0479538776439</p><p>The result is the same to 11 decimal places!</p><p>Recall that the point of gradient boosting is to build a model with enough trees to transform a weak learner into a strong learner. This is easily done by changing <strong class="source-inline">n_estimators</strong>, the number of iterations, to a much larger number.</p></li>
				<li><p>Let's build and score a gradient boosting regressor with 30 estimators:</p><p class="source-code">gbr = GradientBoostingRegressor(max_depth=2, n_estimators=30, random_state=2, learning_rate=1.0)</p><p class="source-code">gbr.fit(X_train, y_train)</p><p class="source-code">y_pred = gbr.predict(X_test)</p><p class="source-code">MSE(y_test, y_pred)**0.5</p><p>The result is as follows:</p><p class="source-code">857.1072323426944</p><p>The score is an improvement. Now let's look at 300 estimators:</p><p class="source-code">gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0)</p><p class="source-code">gbr.fit(X_train, y_train)</p><p class="source-code">y_pred = gbr.predict(X_test)</p><p class="source-code">MSE(y_test, y_pred)**0.5</p><p>The result is this:</p><p class="source-code">936.3617413678853</p></li>
			</ol>
			<p>This is a surprise! The score has gotten worse! Have we been misled? Is gradient boosting not all that it's cracked up to be?</p>
			<p>Whenever you get a <a id="_idIndexMarker243"/>surprise result, it's worth <a id="_idIndexMarker244"/>double-checking the code. Now, we changed <strong class="source-inline">learning_rate</strong> without saying much about it. So, what happens if we remove <strong class="source-inline">learning_rate=1.0</strong> and use the scikit-learn defaults?</p>
			<p>Let's find out:</p>
			<p class="source-code">gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2)</p>
			<p class="source-code">gbr.fit(X_train, y_train)</p>
			<p class="source-code">y_pred = gbr.predict(X_test)</p>
			<p class="source-code">MSE(y_test, y_pred)**0.5</p>
			<p>The result is this:</p>
			<p class="source-code">653.7456840231495</p>
			<p>Incredible! By using the scikit-learn default for the <strong class="source-inline">learning_rate</strong> hyperparameter, the score has changed from <strong class="source-inline">936</strong> to <strong class="source-inline">654</strong>.</p>
			<p>In the next section, we will learn more about the different gradient boosting hyperparameters with a focus on the <strong class="source-inline">learning_rate</strong> hyperparameter. </p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor103"/>Modifying gradient boosting hyperparameters</h1>
			<p>In this section, we will focus on the <strong class="source-inline">learning_rate</strong>, the most important gradient boosting <a id="_idIndexMarker245"/>hyperparameter, with the possible exception of <strong class="source-inline">n_estimators</strong>, the number of iterations or trees in the model. We will also survey some tree hyperparameters, and <strong class="source-inline">subsample</strong>, which results in <strong class="bold">stochastic gradient boosting</strong>. In addition, we will use <strong class="source-inline">RandomizedSearchCV</strong> and compare results with XGBoost.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor104"/>learning_rate</h2>
			<p>In the last <a id="_idIndexMarker246"/>section, changing the <strong class="source-inline">learning_rate</strong> value of <strong class="source-inline">GradientBoostingRegressor</strong> from <strong class="source-inline">1.0</strong> to scikit-learn's default, which is <strong class="source-inline">0.1</strong>, resulted in enormous gains.</p>
			<p><strong class="source-inline">learning_rate</strong>, also <a id="_idIndexMarker247"/>known as the <em class="italic">shrinkage</em>, shrinks the contribution of <a id="_idIndexMarker248"/>individual trees so that no tree has too much influence when building the model. If an entire ensemble is built from the errors of one base learner, without careful adjustment of hyperparameters, early trees in the model can have too much influence on subsequent development. <strong class="source-inline">learning_rate</strong> limits the influence of individual trees. Generally speaking, as <strong class="source-inline">n_estimators</strong>, the number of trees, goes up, <strong class="source-inline">learning_rate</strong> should go down.</p>
			<p>Determining an optimal <strong class="source-inline">learning_rate</strong> value requires varying <strong class="source-inline">n_estimators</strong>. First, let's hold <strong class="source-inline">n_estimators</strong> constant and see what <strong class="source-inline">learning_rate</strong> does on its own. <strong class="source-inline">learning_rate</strong> ranges from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>. A <strong class="source-inline">learning_rate</strong> value of <strong class="source-inline">1</strong> means that no adjustments are made. The default value of <strong class="source-inline">0.1</strong> means that the tree's influence is weighted at 10%.</p>
			<p>Here is a reasonable range to start with:</p>
			<p><strong class="source-inline">learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]</strong></p>
			<p>Next, we will loop through the values by building and scoring a new <strong class="source-inline">GradientBoostingRegressor</strong> to see how the scores compare:</p>
			<p class="source-code">for value in learning_rate_values:</p>
			<p class="source-code">    gbr = GradientBoostingRegressor(max_depth=2,   n_estimators=300, random_state=2, learning_rate=value)</p>
			<p class="source-code">    gbr.fit(X_train, y_train)</p>
			<p class="source-code">    y_pred = gbr.predict(X_test)</p>
			<p class="source-code">    rmse = MSE(y_test, y_pred)**0.5</p>
			<p class="source-code">    print('Learning Rate:', value, ', Score:', rmse)</p>
			<p>The learning rate values and scores are as follows:</p>
			<p class="source-code">Learning Rate: 0.001 , Score: 1633.0261400367258</p>
			<p class="source-code">Learning Rate: 0.01 , Score: 831.5430182728547</p>
			<p class="source-code">Learning Rate: 0.05 , Score: 685.0192988749717</p>
			<p class="source-code">Learning Rate: 0.1 , Score: 653.7456840231495</p>
			<p class="source-code">Learning Rate: 0.15 , Score: 687.666134269379</p>
			<p class="source-code">Learning Rate: 0.2 , Score: 664.312804425697</p>
			<p class="source-code">Learning Rate: 0.3 , Score: 689.4190385930236</p>
			<p class="source-code">Learning Rate: 0.5 , Score: 693.8856905068778</p>
			<p class="source-code">Learning Rate: 1.0 , Score: 936.3617413678853</p>
			<p>As you can <a id="_idIndexMarker249"/>see from the output, the <a id="_idIndexMarker250"/>default <strong class="source-inline">learning_rate</strong> value of <strong class="source-inline">0.1</strong> gives the best score for 300 trees.</p>
			<p>Now let's vary <strong class="source-inline">n_estimators</strong>. Using the preceding code, we can generate <strong class="source-inline">learning_rate</strong> plots with <strong class="source-inline">n_estimators</strong> of 30, 300, and 3,000 trees, as shown in the following figure:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B15551_04_03.jpg" alt="Figure 4.3 – learning_rate plot for 30 trees"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – learning_rate plot for 30 trees</p>
			<p>As you <a id="_idIndexMarker251"/>can see, with 30 trees, the <strong class="source-inline">learning_rate</strong> value <a id="_idIndexMarker252"/>peaks at around <strong class="source-inline">0.3</strong>.</p>
			<p>Now, let's take a look at the <strong class="source-inline">learning_rate</strong> plot for 3,000 trees:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B15551_04_04.jpg" alt="Fig 4.4 -- learning_rate plot for 3,000 trees"/>
				</div>
			</div>
			<p class="figure-caption">Fig 4.4 -- learning_rate plot for 3,000 trees</p>
			<p>With 3,000 trees, the <strong class="source-inline">learning_rate</strong> value peaks at the second value, which is given as <strong class="source-inline">0.05</strong>.</p>
			<p>These graphs highlight the importance of tuning <strong class="source-inline">learning_rate</strong> and <strong class="source-inline">n_estimators</strong> together.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor105"/>Base learner</h2>
			<p>The initial decision <a id="_idIndexMarker253"/>tree in the gradient boosting regressor is called the <strong class="bold">base learner</strong> because it's at the base of the ensemble. It's the first <a id="_idIndexMarker254"/>learner in the process. The term <em class="italic">learner</em> here is indicative of a <em class="italic">weak learner</em> transforming into a <em class="italic">strong learner</em>.</p>
			<p>Although base learners need not be fine-tuned for accuracy, as covered in <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>,<em class="italic"> Decision Trees in Depth</em>, it's certainly possible to tune base learners for gains in accuracy.</p>
			<p>For instance, we can select a <strong class="source-inline">max_depth</strong> value of <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>, <strong class="source-inline">3</strong>, or <strong class="source-inline">4</strong> and compare results as follows:</p>
			<p class="source-code">depths = [None, 1, 2, 3, 4]</p>
			<p class="source-code">for depth in depths:</p>
			<p class="source-code">    gbr = GradientBoostingRegressor(max_depth=depth, n_estimators=300, random_state=2)</p>
			<p class="source-code">    gbr.fit(X_train, y_train)</p>
			<p class="source-code">    y_pred = gbr.predict(X_test)</p>
			<p class="source-code">    rmse = MSE(y_test, y_pred)**0.5</p>
			<p class="source-code">    print('Max Depth:', depth, ', Score:', rmse) </p>
			<p>The result is as follows:</p>
			<p class="source-code">Max Depth: None , Score: 867.9366621617327</p>
			<p class="source-code">Max Depth: 1 , Score: 707.8261886858736</p>
			<p class="source-code">Max Depth: 2 , Score: 653.7456840231495</p>
			<p class="source-code">Max Depth: 3 , Score: 646.4045923317708</p>
			<p class="source-code">Max Depth: 4 , Score: 663.048387855927</p>
			<p>A <strong class="source-inline">max_depth</strong> value of <strong class="source-inline">3</strong> gives the best results.</p>
			<p>Other base learner hyperparameters, as covered in <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>,<em class="italic"> Decision Trees in Depth</em>, may be tuned in a similar manner.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor106"/>subsample</h2>
			<p><strong class="source-inline">subsample</strong> is a subset of samples. Since samples are the rows, a subset of rows means that all rows may not <a id="_idIndexMarker255"/>be included when building each tree. By changing <strong class="source-inline">subsample</strong> from <strong class="source-inline">1.0</strong> to a smaller decimal, trees only select that <a id="_idIndexMarker256"/>percentage of samples during the build phase. For example, <strong class="source-inline">subsample=0.8</strong> would select 80% of samples for each tree.</p>
			<p>Continuing with <strong class="source-inline">max_depth=3</strong>, we try a range of subsample percentages to improve results:</p>
			<p class="source-code">samples = [1, 0.9, 0.8, 0.7, 0.6, 0.5]</p>
			<p class="source-code">for sample in samples:</p>
			<p class="source-code">    gbr = GradientBoostingRegressor(max_depth=3, n_estimators=300, subsample=sample, random_state=2)</p>
			<p class="source-code">    gbr.fit(X_train, y_train)</p>
			<p class="source-code">    y_pred = gbr.predict(X_test)</p>
			<p class="source-code">    rmse = MSE(y_test, y_pred)**0.5</p>
			<p class="source-code">    print('Subsample:', sample, ', Score:', rmse)</p>
			<p>The result is as follows:</p>
			<p class="source-code">Subsample: 1 , Score: 646.4045923317708</p>
			<p class="source-code">Subsample: 0.9 , Score: 620.1819001443569</p>
			<p class="source-code">Subsample: 0.8 , Score: 617.2355650565677</p>
			<p class="source-code">Subsample: 0.7 , Score: 612.9879156983139</p>
			<p class="source-code">Subsample: 0.6 , Score: 622.6385116402317</p>
			<p class="source-code">Subsample: 0.5 , Score: 626.9974073227554</p>
			<p>A <strong class="source-inline">subsample</strong> value <a id="_idIndexMarker257"/>of <strong class="source-inline">0.7</strong> with 300 trees and <strong class="source-inline">max_depth</strong> of <strong class="source-inline">3</strong> produces the best score yet.</p>
			<p>When <strong class="source-inline">subsample</strong> is not equal to <strong class="source-inline">1.0</strong>, the model is classified as <strong class="bold">stochastic gradient descent</strong>, where <em class="italic">stochastic</em> indicates that some randomness is inherent in the model.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor107"/>RandomizedSearchCV</h2>
			<p>We have a <a id="_idIndexMarker258"/>good working model, but we have not yet performed a grid search, as covered in <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a>,<em class="italic"> Decision Trees in Depth</em>. Our <a id="_idIndexMarker259"/>preliminary analysis indicates that a grid search centered around <strong class="source-inline">max_depth=3</strong>, <strong class="source-inline">subsample=0.7</strong>, <strong class="source-inline">n_estimators=300</strong>, and <strong class="source-inline">learning_rate = 0.1</strong> is a good place to start. We have already shown that as <strong class="source-inline">n_estimators</strong> goes up, <strong class="source-inline">learning_rate</strong> should go down:</p>
			<ol>
				<li value="1"><p>Here is a possible starting point:</p><p class="source-code">params={'subsample':[0.65, 0.7, 0.75],</p><p class="source-code">        'n_estimators':[300, 500, 1000],</p><p class="source-code">         'learning_rate':[0.05, 0.075, 0.1]}</p><p>Since <strong class="source-inline">n_estimators</strong> is going up from the starting value of 300, <strong class="source-inline">learning_rate</strong> is going down from the starting value of <strong class="source-inline">0.1</strong>. Let's keep <strong class="source-inline">max_depth=3</strong> to limit the variance.</p><p>With 27 possible combinations of hyperparameters, we use <strong class="source-inline">RandomizedSearchCV</strong> to try 10 of these combinations in the hopes of finding a good model.</p><p class="callout-heading">Note </p><p class="callout">While 27 combinations are feasible with <strong class="source-inline">GridSearchCV</strong>, at some point you will end up with too many possibilities and <strong class="source-inline">RandomizedSearchCV</strong> will become essential. We use <strong class="source-inline">RandomizedSearchCV</strong> here for practice and to speed up computations.</p></li>
				<li><p>Let's import <strong class="source-inline">RandomizedSearchCV</strong> and initialize a gradient boosting model:</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">gbr = GradientBoostingRegressor(max_depth=3, random_state=2)</p></li>
				<li><p>Next, initialize <strong class="source-inline">RandomizedSearchCV</strong> with <strong class="source-inline">gbr</strong> and <strong class="source-inline">params</strong> as inputs in addition to the number of iterations, the scoring, and the number of folds. Recall that <strong class="source-inline">n_jobs=-1</strong> may speed up computations and <strong class="source-inline">random_state=2</strong> ensures the <a id="_idIndexMarker260"/>consistency of results:</p><p class="source-code">rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, random_state=2)</p></li>
				<li><p>Now fit the model <a id="_idIndexMarker261"/>on the training set and obtain the best parameters and scores:</p><p class="source-code">rand_reg.fit(X_train, y_train)</p><p class="source-code">best_model = rand_reg.best_estimator_</p><p class="source-code">best_params = rand_reg.best_params_</p><p class="source-code">print("Best params:", best_params)</p><p class="source-code">best_score = np.sqrt(-rand_reg.best_score_)</p><p class="source-code">print("Training score: {:.3f}".format(best_score))</p><p class="source-code">y_pred = best_model.predict(X_test)</p><p class="source-code">rmse_test = MSE(y_test, y_pred)**0.5</p><p class="source-code">print('Test set score: {:.3f}'.format(rmse_test))</p><p>The result is as follows:</p><p class="source-code">Best params: {'learning_rate': 0.05, 'n_estimators': 300, 'subsample': 0.65}</p><p class="source-code">Training score: 636.200</p><p class="source-code">Test set score: 625.985</p><p>From here, it's worth experimenting by changing parameters individually or in pairs. Even though the best model currently has <strong class="source-inline">n_estimators=300</strong>, it's certainly possible that raising this hyperparameter will obtain better results with careful adjustment of the <strong class="source-inline">learning_rate</strong> value. <strong class="source-inline">subsample</strong> may be experimented with as well.</p></li>
				<li><p>After a <a id="_idIndexMarker262"/>few rounds of experimentation, we <a id="_idIndexMarker263"/>obtained the following model:</p><p class="source-code">gbr = GradientBoostingRegressor(max_depth=3, n_estimators=1600, subsample=0.75, learning_rate=0.02, random_state=2)</p><p class="source-code">gbr.fit(X_train, y_train)</p><p class="source-code">y_pred = gbr.predict(X_test)</p><p class="source-code">MSE(y_test, y_pred)**0.5 </p><p>The result is the following:</p><p class="source-code">596.9544588974487</p></li>
			</ol>
			<p>With a larger value for <strong class="source-inline">n_estimators</strong> at <strong class="source-inline">1600</strong>, a smaller <strong class="source-inline">learning_rate</strong> value at <strong class="source-inline">0.02</strong>, a <a id="_idIndexMarker264"/>comparable <strong class="source-inline">subsample</strong> value of <strong class="source-inline">0.75</strong>, and the same <strong class="source-inline">max_depth</strong> value of <strong class="source-inline">3</strong>, we obtained the best <strong class="bold">Root Mean Square Error</strong> (<strong class="bold">RMSE</strong>) yet at <strong class="source-inline">597</strong>.</p>
			<p>It may be possible to do better. We encourage you to try!</p>
			<p>Now, let's see how XGBoost differs from gradient boosting using the same hyperparameters covered thus far.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor108"/>XGBoost</h2>
			<p>XGBoost is an <a id="_idIndexMarker265"/>advanced version of gradient boosting with the same general structure, meaning that it transforms weak learners into strong learners by summing the residuals of trees.</p>
			<p>The only difference in hyperparameters from the last section is that XGBoost refers to <strong class="source-inline">learning_rate</strong> as <strong class="source-inline">eta</strong>.</p>
			<p>Let's build an XGBoost regressor with the same hyperparameters to compare the results.</p>
			<p>Import <strong class="source-inline">XGBRegressor</strong> from <strong class="source-inline">xgboost</strong>, and then initialize and score the model as follows:</p>
			<p class="source-code">from xgboost import XGBRegressor</p>
			<p class="source-code">xg_reg = XGBRegressor(max_depth=3, n_estimators=1600, eta=0.02, subsample=0.75, random_state=2)</p>
			<p class="source-code">xg_reg.fit(X_train, y_train)</p>
			<p class="source-code">y_pred = xg_reg.predict(X_test)</p>
			<p class="source-code">MSE(y_test, y_pred)**0.5</p>
			<p>The result is this:</p>
			<p class="source-code">584.339544309016</p>
			<p>The score is better. The reason as to why the score is better will be revealed in the next chapter, <a href="B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117"><em class="italic">Chapter 5</em></a>, <em class="italic">XGBoost Unveiled</em>.</p>
			<p>Accuracy and speed are the two most important concepts when building machine learning models, and we have shown multiple times that XGBoost is very accurate. XGBoost is preferred over gradient boosting in general because it consistently delivers better results, and because it's faster, as demonstrated by the following case study.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor109"/>Approaching big data – gradient boosting versus XGBoost</h1>
			<p>In the <a id="_idIndexMarker266"/>real world, datasets <a id="_idIndexMarker267"/>can be enormous, with trillions of data points. Limiting work to one computer can be disadvantageous <a id="_idIndexMarker268"/>due to the limited resources of one machine. When working with big data, the cloud is often used to take advantage of parallel computers.</p>
			<p>Datasets are big when they push the limits of computation. So far in this book, by limiting datasets to tens of thousands of rows with a hundred or fewer columns, there should have been no significant time delays, unless you ran into errors (happens to everyone).</p>
			<p>In this section, we examine <strong class="bold">exoplanets</strong> over time. The dataset has 5,087 rows and 3,189 columns that record light flux at different times of a star's life cycle. Multiplying columns and rows together results in 1.5 million data points. Using a baseline of 100 trees, we need 150 million data points to build a model.</p>
			<p>In this section, my 2013 MacBook Air had wait times of about 5 minutes. New computers should be faster. I have chosen the exoplanet dataset so that wait times play a significant role without tying up your computer for a very long time.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor110"/>Introducing the exoplanet dataset</h2>
			<p>The <a id="_idIndexMarker269"/>exoplanet dataset is taken from Kaggle and dates from around 2017: <a href="https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data">https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data</a>. The dataset <a id="_idIndexMarker270"/>contains information about the light of stars. Each row is an individual star and the columns reveal different light patterns over time. In addition to light patterns, an exoplanet column is labeled <strong class="source-inline">2</strong> if the star hosts an exoplanet; otherwise, it is labeled <strong class="source-inline">1</strong>.</p>
			<p>The dataset records the light flux <a id="_idIndexMarker271"/>from thousands of stars. <strong class="bold">Light flux</strong>, often referred to as <strong class="bold">luminous flux</strong>, is the perceived <a id="_idIndexMarker272"/>brightness of a star.</p>
			<p class="callout-heading">Note </p>
			<p class="callout">The perceived brightness is different than actual brightness. For instance, an incredibly bright star very far away may have a small luminous flux (looks dim), while a moderately bright star that is very close, like the sun, may have a large luminous flux (looks bright).</p>
			<p>When the light flux of an individual star changes periodically, it is possible that the star is being orbited <a id="_idIndexMarker273"/>by an exoplanet. The assumption is that when an exoplanet orbits in front of a star, it blocks a small fraction of the light, reducing the perceived <a id="_idIndexMarker274"/>brightness by a very slight amount.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Finding exoplanets is rare. The predictive column, on whether a star hosts an exoplanet or not, has very few positive cases, resulting in an imbalanced dataset. Imbalanced datasets require extra precautions. We will cover imbalanced datasets in <a href="B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161"><em class="italic">Chapter 7</em></a>, <em class="italic">Discovering Exoplanets with XGBoost</em>, where we go into further detail with this dataset.</p>
			<p>Next, let's access the exoplanet dataset and prepare it for machine learning.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor111"/>Preprocessing the exoplanet dataset</h2>
			<p>The exoplanet <a id="_idIndexMarker275"/>dataset has been uploaded to our GitHub page at <a href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04">https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04</a>.</p>
			<p>Here are the steps to load and preprocess the exoplanet dataset for machine learning:</p>
			<ol>
				<li value="1"><p>Download <strong class="source-inline">exoplanets.csv</strong> in the same folder as your Jupyter Notebook. Then, open the file and take a look:</p><p class="source-code">df = pd.read_csv('exoplanets.csv')</p><p class="source-code">df.head() </p><p>The DataFrame will look as shown in the following figure:</p><div id="_idContainer056" class="IMG---Figure"><img src="image/B15551_04_05.jpg" alt="Fig 4.5 – Exoplanet DataFrame"/></div><p class="figure-caption">Fig 4.5 – Exoplanet DataFrame</p><p>Not all columns are shown due to space limitations. The flux columns are floats, while the <strong class="source-inline">Label</strong> column is <strong class="source-inline">2</strong> for an exoplanet star and <strong class="source-inline">1</strong> for a non-exoplanet star.</p></li>
				<li><p>Let's' confirm that all <a id="_idIndexMarker276"/>columns are numerical with <strong class="source-inline">df.info()</strong>:</p><p class="source-code">df.info()</p><p>The result is as follows:</p><p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p><p class="source-code">RangeIndex: 5087 entries, 0 to 5086</p><p class="source-code">Columns: 3198 entries, LABEL to FLUX.3197</p><p class="source-code">dtypes: float64(3197), int64(1)</p><p class="source-code">memory usage: 124.1 MB</p><p>As you can see from the output, <strong class="source-inline">3197</strong> columns are floats and <strong class="source-inline">1</strong> column is an <strong class="source-inline">int</strong>, so all columns are numerical.</p></li>
				<li><p>Now, let's confirm the number of null values with the following code:</p><p class="source-code">df.isnull().sum().sum()</p><p>The output is as follows:</p><p class="source-code">0</p><p>The output reveals that there are no null values.</p></li>
				<li><p>Since all columns are numerical with no null values, we may split the data into training and test <a id="_idIndexMarker277"/>sets. Note that the 0th column is the target column, <strong class="source-inline">y</strong>, and all other columns are the predictor columns, <strong class="source-inline">X</strong>:</p><p class="source-code">X = df.iloc[:,1:]</p><p class="source-code">y = df.iloc[:,0]</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</p></li>
			</ol>
			<p>It's time to build a gradient boosting classifier to predict whether stars host exoplanets.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor112"/>Building gradient boosting classifiers</h2>
			<p>Gradient boosting <a id="_idIndexMarker278"/>classifiers work in the same manner as gradient boosting regressors. The difference is primarily in the scoring.</p>
			<p>Let's start by importing <strong class="source-inline">GradientBoostingClassifer</strong> and <strong class="source-inline">XGBClassifier</strong> in addition to <strong class="source-inline">accuracy_score</strong> so that we may compare both models:</p>
			<p class="source-code">from sklearn.ensemble import GradientBoostingClassifier</p>
			<p class="source-code">from xgboost import XGBClassifier</p>
			<p class="source-code">from sklearn.metrics import accuracy_score</p>
			<p>Next, we need a way to compare models using a timer.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor113"/>Timing models</h2>
			<p>Python comes with a <strong class="source-inline">time</strong> library that <a id="_idIndexMarker279"/>can be used to mark time. The general idea is to mark the time before and after a computation. The difference between these times tells us how long the computation took.</p>
			<p>The <strong class="source-inline">time</strong> library is imported as follows:</p>
			<p class="source-code"><strong class="source-inline">import time</strong></p>
			<p>Within the <strong class="source-inline">time</strong> library, the <strong class="source-inline">.time()</strong> method marks time in seconds.</p>
			<p>As an example, see how long it takes to run <strong class="source-inline">df.info()</strong> by assigning start and end times before and after the computation using <strong class="source-inline">time.time()</strong>:</p>
			<p class="source-code">start = time.time()</p>
			<p class="source-code">df.info()</p>
			<p class="source-code">end = time.time()</p>
			<p class="source-code">elapsed = end - start</p>
			<p class="source-code">print('\nRun Time: ' + str(elapsed) + ' seconds.')</p>
			<p>The <a id="_idIndexMarker280"/>output is as follows:</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">RangeIndex: 5087 entries, 0 to 5086</p>
			<p class="source-code">Columns: 3198 entries, LABEL to FLUX.3197</p>
			<p class="source-code">dtypes: float64(3197), int64(1)</p>
			<p class="source-code">memory usage: 124.1 MB</p>
			<p>The runtime is as follows:</p>
			<p class="source-code">Run Time: 0.0525362491607666 seconds.</p>
			<p>Your results will differ from ours, but hopefully it's in the same ballpark.</p>
			<p>Let's now compare <strong class="source-inline">GradientBoostingClassifier</strong> and <strong class="source-inline">XGBoostClassifier</strong> with the exoplanet dataset for its speed using the preceding code to mark time.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Jupyter Notebooks come with magic functions, denoted by the <strong class="source-inline">%</strong> sign before a command. <strong class="source-inline">%timeit</strong> is one such magic function. Instead of computing how long it takes to run the code once, <strong class="source-inline">%timeit</strong> computes how long it takes to run code over multiple runs. See <a href="http://ipython.readthedocs.io/en/stable/interactive/magics.html">ipython.readthedocs.io/en/stable/interactive/magics.html</a> for more information on magic functions.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor114"/>Comparing speed</h2>
			<p>It's time to race <strong class="source-inline">GradientBoostingClassifier</strong> and <strong class="source-inline">XGBoostClassifier</strong> with the <a id="_idIndexMarker281"/>exoplanet dataset. We have set <strong class="source-inline">max_depth=2</strong> and <strong class="source-inline">n_estimators=100</strong> to limit the size of the model. Let's start with <strong class="source-inline">GradientBoostingClassifier</strong>:</p>
			<ol>
				<li value="1"><p>First, we will mark the start time. After building and scoring the model, we will mark the end time. The following code may take around 5 minutes to run depending on the speed of your computer:</p><p class="source-code">start = time.time()</p><p class="source-code">gbr = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=2)</p><p class="source-code">gbr.fit(X_train, y_train)</p><p class="source-code">y_pred = gbr.predict(X_test)</p><p class="source-code">score = accuracy_score(y_pred, y_test)</p><p class="source-code">print('Score: ' + str(score))</p><p class="source-code">end = time.time()</p><p class="source-code">elapsed = end - start</p><p class="source-code">print('\nRun Time: ' + str(elapsed) + ' seconds')</p><p>The result is this:</p><p class="source-code">Score: 0.9874213836477987</p><p class="source-code">Run Time: 317.6318619251251 seconds</p><p><strong class="source-inline">GradientBoostingRegressor</strong> took over 5 minutes to run on my 2013 MacBook Air. Not bad for 150 million data points on an older computer. </p><p class="callout-heading">Note </p><p class="callout">While a score of 98.7% percent is usually outstanding for accuracy, this is not the case with imbalanced datasets, as you will see in <a href="B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161"><em class="italic">Chapter 7</em></a>, <em class="italic">Discovering Exoplanets with XGBoost</em>.</p></li>
				<li><p>Next, we will build an <strong class="source-inline">XGBClassifier</strong> model with the same hyperparameters <a id="_idIndexMarker282"/>and mark the time in the same manner:</p><p class="source-code">start = time.time()</p><p class="source-code">xg_reg = XGBClassifier(n_estimators=100, max_depth=2, random_state=2)</p><p class="source-code">xg_reg.fit(X_train, y_train)</p><p class="source-code">y_pred = xg_reg.predict(X_test)</p><p class="source-code">score = accuracy_score(y_pred, y_test)</p><p class="source-code">print('Score: ' + str(score))</p><p class="source-code">end = time.time()</p><p class="source-code">elapsed = end - start</p><p class="source-code">print('Run Time: ' + str(elapsed) + ' seconds')</p><p>The result is as follows:</p><p class="source-code">Score: 0.9913522012578616</p><p class="source-code">Run Time: 118.90568995475769 seconds</p></li>
			</ol>
			<p>On my 2013 MacBook Air, XGBoost took under 2 minutes, making it more than twice as fast. It's also more accurate by half a percentage point.</p>
			<p>When it comes to big data, an algorithm twice as fast can save weeks or months of computational time and resources. This advantage is huge in the world of big data.</p>
			<p>In the world of boosting, XGBoost is the model of choice due to its unparalleled speed and impressive accuracy.</p>
			<p>As for the exoplanet dataset, it will be revisited in <a href="B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161"><em class="italic">Chapter 7</em></a>, <em class="italic">Discovering Exoplanets with XGBoost</em>, in an important case study that reveals the challenges of working with imbalanced datasets along with a variety of potential solutions to those challenges.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">I recently purchased a 2020 MacBook Pro and updated all software. The difference in time using the same code is staggering:</p>
			<p class="callout">Gradient Boosting Run Time: 197.38 seconds</p>
			<p class="callout">XGBoost Run Time: 8.66 seconds</p>
			<p class="callout">More than a 10-fold difference!</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor115"/>Summary</h1>
			<p>In this chapter, you learned the difference between bagging and boosting. You learned how gradient boosting works by building a gradient boosting regressor from scratch. You implemented a variety of gradient boosting hyperparameters, including <strong class="source-inline">learning_rate</strong>, <strong class="source-inline">n_estimators</strong>, <strong class="source-inline">max_depth</strong>, and <strong class="source-inline">subsample</strong>, which results in stochastic gradient boosting. Finally, you used big data to predict whether stars have exoplanets by comparing the times of <strong class="source-inline">GradientBoostingClassifier</strong> and <strong class="source-inline">XGBoostClassifier</strong>, with <strong class="source-inline">XGBoostClassifier</strong> emerging as twice to over ten times as fast and more accurate.</p>
			<p>The advantage of learning these skills is that you now understand when to apply XGBoost rather than similar machine learning algorithms such as gradient boosting. You can now build stronger XGBoost and gradient boosting models by properly taking advantage of core hyperparameters, including <strong class="source-inline">n_estimators</strong> and <strong class="source-inline">learning_rate</strong>. Furthermore, you have developed the capacity to time all computations instead of relying on intuition.</p>
			<p>Congratulations! You have completed all of the preliminary XGBoost chapters. Until now, the purpose has been to introduce you to machine learning and data analytics within the larger XGBoost narrative. The aim has been to show how the need for XGBoost emerged from ensemble methods, boosting, gradient boosting, and big data.</p>
			<p>The next chapter starts a new leg on our journey with an advanced introduction to XGBoost, where you will learn the mathematical details behind the XGBoost algorithm in addition to hardware modifications that XGBoost makes to improve speed. You'll also be building XGBoost models using the original Python API in a historically relevant case study on the discovery of the Higgs boson. The chapters that follow highlight exciting details, advantages, nuances, and tricks and tips to build swift, efficient, powerful, and industry-ready XGBoost models that you can use for years to come.</p>
		</div>
	</body></html>
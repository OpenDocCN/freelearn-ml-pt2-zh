<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Probabilistic Learning &#x2013; Classification Using Naive Bayes"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Probabilistic Learning – Classification Using Naive Bayes</h1></div></div></div><p>When a meteorologist provides a weather forecast, precipitation is typically described with terms such as "70 percent chance of rain." Such forecasts are known as probability of precipitation reports. Have you ever considered how they are calculated? It is a puzzling question, because in reality, either it will rain or not.</p><p>Weather estimates are based on probabilistic methods or those concerned with describing uncertainty. They use data on past events to extrapolate future events. In the case of weather, the chance of rain describes the proportion of prior days to similar measurable atmospheric conditions in which precipitation occurred. A 70 percent chance of rain implies that in 7 out of the 10 past cases with similar conditions, precipitation occurred somewhere in the area.</p><p>This chapter covers the Naive Bayes algorithm, which uses probabilities in much the same way as a weather forecast. While studying this method, you will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Basic principles of probability</li><li class="listitem" style="list-style-type: disc">The specialized methods and data structures needed to analyze text data with R</li><li class="listitem" style="list-style-type: disc">How to employ Naive Bayes to build an SMS junk message filter</li></ul></div><p>If you've taken a statistics class before, some of the material in this chapter may be a review. Even so, it may be helpful to refresh your knowledge on probability, as these principles are the basis of how Naive Bayes got such a strange name.</p><div class="section" title="Understanding Naive Bayes"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec21"/>Understanding Naive Bayes</h1></div></div></div><p>The basic <a id="id255" class="indexterm"/>statistical ideas necessary to understand the Naive Bayes algorithm have existed for centuries. The technique descended from the work of the 18<sup>th</sup> century mathematician Thomas Bayes, who developed foundational principles to describe the probability of events, and how probabilities should be revised in the light of additional information. These principles formed the foundation for what are now known as <span class="strong"><strong>Bayesian methods</strong></span>.</p><p>We will cover these methods in greater detail later on. But, for now, it suffices to say that a probability is a number between 0 and 1 (that is, between 0 percent and 100 percent), which captures the chance that an event will occur in the light of the available evidence. The lower the probability, the less likely the event is to occur. A probability of 0 indicates that the event will definitely not occur, while a probability of 1 indicates that the event will occur with 100 percent certainty.</p><p>Classifiers based on Bayesian methods utilize training data to calculate an observed probability of each outcome based on the evidence provided by feature values. When the classifier is later applied to unlabeled data, it uses the observed probabilities to predict the most likely class for the new features. It's a simple idea, but it results in a method that often has results on par with more sophisticated algorithms. In fact, Bayesian classifiers have been used for:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Text classification, such as junk e-mail (spam) filtering</li><li class="listitem" style="list-style-type: disc">Intrusion or anomaly detection in computer networks</li><li class="listitem" style="list-style-type: disc">Diagnosing medical conditions given a set of observed symptoms</li></ul></div><p>Typically, Bayesian classifiers are best applied to problems in which the information from numerous attributes should be considered simultaneously in order to estimate the overall probability of an outcome. While many machine learning algorithms ignore features that have weak effects, Bayesian methods utilize all the available evidence to subtly change the predictions. If large number of features have relatively minor effects, taken together, their combined impact could be quite large.</p><div class="section" title="Basic concepts of Bayesian methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec38"/>Basic concepts of Bayesian methods</h2></div></div></div><p>Before jumping into the <a id="id256" class="indexterm"/>Naive Bayes algorithm, it's worth spending some time defining the concepts that are used across Bayesian methods. Summarized in a single sentence, Bayesian probability theory is rooted in the idea that the estimated likelihood of an <span class="strong"><strong>event</strong></span>, or a potential outcome, should be based on the evidence at hand across multiple <span class="strong"><strong>trials</strong></span>, or opportunities for the event to occur.</p><p>The following table illustrates events and trials for several real-world outcomes:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Event</p>
</th><th style="text-align: left" valign="bottom">
<p>Trial</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Heads result</p>
</td><td style="text-align: left" valign="top">
<p>Coin flip</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Rainy weather</p>
</td><td style="text-align: left" valign="top">
<p>A single day</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Message is spam</p>
</td><td style="text-align: left" valign="top">
<p>Incoming e-mail message</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Candidate becomes president</p>
</td><td style="text-align: left" valign="top">
<p>Presidential election</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Win the lottery</p>
</td><td style="text-align: left" valign="top">
<p>Lottery ticket</p>
</td></tr></tbody></table></div><p>Bayesian methods provide <a id="id257" class="indexterm"/>insights into how the probability of these events can be estimated from the observed data. To see how, we'll need to formalize our understanding of probability.</p><div class="section" title="Understanding probability"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec17"/>Understanding probability</h3></div></div></div><p>The probability of <a id="id258" class="indexterm"/>an event is estimated from the observed data by dividing the number of trials in which the event occurred by the total number of trials. For instance, if it <a id="id259" class="indexterm"/>rained 3 out of 10 days with similar conditions as today, the probability of rain today can be estimated as <span class="emphasis"><em>3 / 10 = 0.30</em></span> or 30 percent. Similarly, if 10 out of 50 prior email messages were spam, then the probability of any incoming message being spam can be estimated as <span class="emphasis"><em>10 / 50 = 0.20</em></span> or 20 percent.</p><p>To denote these probabilities, we use notation in the form <span class="emphasis"><em>P(A)</em></span>, which signifies the probability of event <span class="emphasis"><em>A</em></span>. For example, <span class="emphasis"><em>P(rain) = 0.30</em></span> and <span class="emphasis"><em>P(spam) = 0.20</em></span>.</p><p>The probability of all the possible outcomes of a trial must always sum to 1, because a trial always results in some outcome happening. Thus, if the trial has two outcomes that cannot occur simultaneously, such as rainy versus sunny or spam versus ham (nonspam), then knowing the probability of either outcome reveals the probability of the other. For example, given the value <span class="emphasis"><em>P(spam) = 0.20</em></span>, we can calculate <span class="emphasis"><em>P(ham) = 1 – 0.20 = 0.80</em></span>. This concludes that spam and ham are <span class="strong"><strong>mutually exclusive and exhaustive</strong></span> events, which implies that they cannot occur at the same time and are the only possible outcomes.</p><p>Because an event cannot simultaneously happen and not happen, an event is always mutually exclusive and exhaustive with its <span class="strong"><strong>complement</strong></span>, or the event comprising of the outcomes in which the event of interest does not happen. The complement of event <span class="emphasis"><em>A</em></span> is typically denoted <span class="emphasis"><em>A<sup>c</sup></em></span> or <span class="emphasis"><em>A'</em></span>. Additionally, the shorthand notation <span class="emphasis"><em>P(¬A)</em></span> can used to denote the probability of event <span class="emphasis"><em>A</em></span> not occurring, as in <span class="emphasis"><em>P(¬spam) = 0.80</em></span>. This notation is equivalent to <span class="emphasis"><em>P(A<sup>c</sup>)</em></span>.</p><p>To illustrate events and their complements, it is often helpful to imagine a two-dimensional space that is partitioned into probabilities for each event. In the following diagram, the rectangle represents the possible outcomes for an e-mail message. The circle represents the 20 <a id="id260" class="indexterm"/>percent probability that the message is spam. The remaining 80 percent represents the complement <span class="emphasis"><em>P(¬spam)</em></span> or the messages that are not spam:</p><div class="mediaobject"><img src="graphics/B03905_04_01.jpg" alt="Understanding probability"/></div></div><div class="section" title="Understanding joint probability"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec18"/>Understanding joint probability</h3></div></div></div><p>Often, we <a id="id261" class="indexterm"/>are interested in monitoring several <a id="id262" class="indexterm"/>nonmutually exclusive events for the same trial. If certain events occur with the event of interest, we may be able to use them to make predictions. Consider, for instance, a second event based on the outcome that an e-mail message contains the word Viagra. In most cases, this word is likely to appear only in a spam message; its presence in an incoming e-mail is therefore a very strong piece of evidence that the message is spam. The preceding diagram, updated for this second event, might appear as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B03905_04_02.jpg" alt="Understanding joint probability"/></div><p>Notice in the diagram that the Viagra circle does not completely fill the spam circle, nor is it completely contained by the spam circle. This implies that not all spam messages contain the word Viagra and not every e-mail with the word Viagra is spam.</p><p>To zoom in for <a id="id263" class="indexterm"/>a closer look at the overlap between the spam and Viagra circles, we'll employ a visualization known as a <span class="strong"><strong>Venn diagram</strong></span>. First used <a id="id264" class="indexterm"/>in the late 19th century by John Venn, the diagram uses circles to illustrate the overlap between sets of items. In most Venn diagrams, the size of the circles and the degree of the overlap is not meaningful. Instead, it is used as a reminder to allocate probability to all possible combinations of events:</p><div class="mediaobject"><img src="graphics/B03905_04_03.jpg" alt="Understanding joint probability"/></div><p>We know that 20 percent of all messages were spam (the left circle) and 5 percent of all messages contained the word Viagra (the right circle). We would like to quantify the degree of overlap between these two proportions. In other words, we hope to estimate the probability that both <span class="emphasis"><em>P(spam)</em></span> and <span class="emphasis"><em>P(Viagra)</em></span> occur, which can be written as <span class="emphasis"><em>P(spam ∩ Viagra)</em></span>. The upside down 'U' symbol signifies the <span class="strong"><strong>intersection</strong></span> of the two events; the notation <span class="emphasis"><em>A ∩ B</em></span> refers to the event in which both <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> occur.</p><p>Calculating <span class="emphasis"><em>P(spam ∩ Viagra)</em></span> depends on the <span class="strong"><strong>joint probability</strong></span> of the two events or how the probability of one event is related to the probability of the other. If the two events are totally unrelated, they are called <span class="strong"><strong>independent events</strong></span>. This is not to say that independent events cannot occur at the same time; event <a id="id265" class="indexterm"/>independence simply implies that <a id="id266" class="indexterm"/>knowing the outcome of one event does not provide any information about the outcome of the other. For instance, the outcome of a heads result on a coin flip is independent from whether the weather is rainy or sunny on any given day.</p><p>If all events were <a id="id267" class="indexterm"/>independent, it would be impossible to predict one event by <a id="id268" class="indexterm"/>observing another. In other words, <span class="strong"><strong>dependent events</strong></span> are the basis of predictive modeling. Just as the presence of clouds is predictive of a rainy day, the appearance of the word Viagra is predictive of a spam e-mail.</p><div class="mediaobject"><img src="graphics/B03905_04_04.jpg" alt="Understanding joint probability"/></div><p>Calculating the probability of dependent events is a bit more complex than for independent events. If <span class="emphasis"><em>P(spam)</em></span> and <span class="emphasis"><em>P(Viagra)</em></span> were independent, we could easily calculate <span class="emphasis"><em>P(spam ∩ Viagra)</em></span>, the probability of both events happening at the same time. Because 20 percent of all the messages are spam, and 5 percent of all the e-mails contain the word Viagra, we could assume that 1 percent of all messages are spam with the term Viagra. This is because <span class="emphasis"><em>0.05 * 0.20 = 0.01</em></span>. More generally, for independent events <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span>, the probability of both happening can be expressed as <span class="emphasis"><em>P(A ∩ B) = P(A) * P(B)</em></span>.</p><p>This said, we <a id="id269" class="indexterm"/>know that <span class="emphasis"><em>P(spam)</em></span> and <span class="emphasis"><em>P(Viagra)</em></span> are likely to be highly dependent, which means that this calculation is incorrect. To <a id="id270" class="indexterm"/>obtain a reasonable estimate, we need to use a more careful formulation of the relationship between these two events, which is based on advanced Bayesian methods.</p></div><div class="section" title="Computing conditional probability with Bayes' theorem"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec19"/>Computing conditional probability with Bayes' theorem</h3></div></div></div><p>The relationships <a id="id271" class="indexterm"/>between dependent events can be described using <span class="strong"><strong>Bayes' theorem</strong></span>, as shown in the following formula. This <a id="id272" class="indexterm"/>formulation provides a way of thinking about how to revise an estimate of the probability of one event in light of the evidence provided by another event:</p><div class="mediaobject"><img src="graphics/B03905_04_05.jpg" alt="Computing conditional probability with Bayes' theorem"/></div><p>The notation <span class="emphasis"><em>P(A|B)</em></span> is read as the probability of event <span class="emphasis"><em>A</em></span>, given that event <span class="emphasis"><em>B</em></span> occurred. This is known as <span class="strong"><strong>conditional probability</strong></span>, since the probability of <span class="emphasis"><em>A</em></span> is dependent (that is, conditional) on what happened with event <span class="emphasis"><em>B</em></span>. Bayes' theorem tells us that our estimate of <span class="emphasis"><em>P(A|B)</em></span> should be based on <span class="emphasis"><em>P(A ∩ B)</em></span>, a measure of how often <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> are observed to occur together, and <span class="emphasis"><em>P(B)</em></span>, a measure of how often <span class="emphasis"><em>B</em></span> is observed to occur in general.</p><p>Bayes' theorem states that the best estimate of <span class="emphasis"><em>P(A|B)</em></span> is the proportion of trials in which <span class="emphasis"><em>A</em></span> occurred with <span class="emphasis"><em>B</em></span> out of all the trials in which <span class="emphasis"><em>B</em></span> occurred. In plain language, this tells us that if we know event <span class="emphasis"><em>B</em></span> occurred, the probability of event <span class="emphasis"><em>A</em></span> is higher the more often that <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> occur together each time <span class="emphasis"><em>B</em></span> is observed. In a way, this adjusts <span class="emphasis"><em>P(A ∩ B)</em></span> for the probability of <span class="emphasis"><em>B</em></span> occurring; if <span class="emphasis"><em>B</em></span> is extremely rare, <span class="emphasis"><em>P(B)</em></span> and <span class="emphasis"><em>P(A ∩ B)</em></span> will always be small; however, if <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> almost always happen together, <span class="emphasis"><em>P(A|B)</em></span> will be high regardless of the probability of <span class="emphasis"><em>B</em></span>.</p><p>By definition, <span class="emphasis"><em>P(A ∩ B) = P(A|B) * P(B)</em></span>, a fact that can be easily derived by applying a bit of algebra to the previous formula. Rearranging this formula once more with the knowledge that <span class="emphasis"><em>P(A ∩ B) = P(B ∩ A)</em></span> results in the conclusion that <span class="emphasis"><em>P(A ∩ B) = P(B|A) * P(A)</em></span>, which we can then use in the following formulation of Bayes' theorem:</p><div class="mediaobject"><img src="graphics/B03905_04_06.jpg" alt="Computing conditional probability with Bayes' theorem"/></div><p>In fact, this is the traditional way in which Bayes' theorem has been specified, for reasons that will become clear as we apply it to machine learning. First, to better understand how Bayes' theorem works in practice, let's revisit our hypothetical spam filter.</p><p>Without <a id="id273" class="indexterm"/>knowledge of an incoming message's content, the best estimate of its spam status would be <span class="emphasis"><em>P(spam)</em></span>, the probability that any prior message was spam, which we calculated previously to be 20 percent. This estimate is <a id="id274" class="indexterm"/>known as the <span class="strong"><strong>prior probability</strong></span>.</p><p>Suppose that you obtained additional evidence by looking more carefully at the set of previously received messages to examine the frequency that the term Viagra appeared. The probability that the word Viagra was used in previous spam messages, or <span class="emphasis"><em>P(Viagra|spam)</em></span>, is called the <a id="id275" class="indexterm"/>
<span class="strong"><strong>likelihood</strong></span>. The probability that Viagra appeared in any <a id="id276" class="indexterm"/>message at all, or <span class="emphasis"><em>P(Viagra)</em></span>, is known as the <span class="strong"><strong>marginal likelihood</strong></span>.</p><p>By applying Bayes' theorem to this evidence, we can compute a <span class="strong"><strong>posterior probability</strong></span> that measures how likely the <a id="id277" class="indexterm"/>message is to be spam. If the posterior probability is greater than 50 percent, the message is more likely to be spam than ham and it should perhaps be filtered. The following formula shows how Bayes' theorem is applied to the evidence provided by the previous e-mail messages:</p><div class="mediaobject"><img src="graphics/B03905_04_07.jpg" alt="Computing conditional probability with Bayes' theorem"/></div><p>To calculate these components of Bayes' theorem, it helps to construct a <span class="strong"><strong>frequency table</strong></span> (shown on the left in the following diagram) that records the number of times Viagra appeared in spam and ham messages. Just like a two-way cross-tabulation, one dimension of the table indicates levels of the class variable (spam or ham), while the other dimension indicates levels for features (Viagra: yes or no). The cells then indicate the number of instances having the particular combination of class value and feature value. The frequency table can then be used to construct a <span class="strong"><strong>likelihood table</strong></span>, as shown on right in the following diagram. The rows of the likelihood table indicate the conditional probabilities for Viagra (yes/no), given that an e-mail was either spam or ham:</p><div class="mediaobject"><img src="graphics/B03905_04_08.jpg" alt="Computing conditional probability with Bayes' theorem"/></div><p>The likelihood table reveals that <span class="emphasis"><em>P(Viagra=Yes|spam) = 4/20 = 0.20</em></span>, indicating that the probability is 20 percent that a message contains the term Viagra, given that the message is spam. Additionally, since <span class="emphasis"><em>P(A ∩ B) = P(B|A) * P(A)</em></span>, we can calculate <span class="emphasis"><em>P(spam ∩ Viagra)</em></span> as <span class="emphasis"><em>P(Viagra|spam) * P(spam) = (4/20) * (20/100) = 0.04</em></span>. The same result can be found in the frequency table, which notes that 4 out of the 100 messages were spam with the term Viagra. Either way, this is four times greater than the previous estimate of 0.01 we calculated as <span class="emphasis"><em>P(A ∩ B) = P(A) * P(B)</em></span> under the false assumption of independence. This, of course, illustrates the importance of Bayes' theorem while calculating joint probability.</p><p>To compute the posterior probability, <span class="emphasis"><em>P(spam|Viagra)</em></span>, we simply take <span class="emphasis"><em>P(Viagra|spam) * P(spam) / P(Viagra)</em></span> or <span class="emphasis"><em>(4/20) * (20/100) / (5/100) = 0.80</em></span>. Therefore, the probability is 80 percent that a <a id="id278" class="indexterm"/>message is spam, given that it contains the word Viagra. In light of this result, any message containing this term should probably be filtered.</p><p>This is very much how commercial spam filters work, although they consider a much larger number of words simultaneously while computing the frequency and likelihood tables. In the next section, we'll see how this concept is put to use when additional features are involved.</p></div></div><div class="section" title="The Naive Bayes algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec39"/>The Naive Bayes algorithm</h2></div></div></div><p>The <span class="strong"><strong>Naive </strong></span><a id="id279" class="indexterm"/>
<span class="strong"><strong>Bayes</strong></span> algorithm describes a simple method to apply Bayes' theorem to classification problems. Although it is not the only machine learning method that utilizes Bayesian methods, it is the most common one. This is particularly true for text classification, where it has become the de facto standard. The strengths and weaknesses of this algorithm are as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strengths</p>
</th><th style="text-align: left" valign="bottom">
<p>Weaknesses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Simple, fast, and very effective</li><li class="listitem" style="list-style-type: disc">Does well with noisy and missing data</li><li class="listitem" style="list-style-type: disc">Requires relatively few examples for training, but also works well with very large numbers of examples</li><li class="listitem" style="list-style-type: disc">Easy to obtain the estimated probability for a prediction</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Relies on an often-faulty assumption of equally important and independent features</li><li class="listitem" style="list-style-type: disc">Not ideal for datasets with many numeric features</li><li class="listitem" style="list-style-type: disc">Estimated probabilities are less reliable than the predicted classes</li></ul></div>
</td></tr></tbody></table></div><p>The Naive Bayes algorithm is named as such because it makes some "naive" assumptions about the data. In particular, Naive Bayes assumes that all of the features in the dataset are equally important and independent. These assumptions are rarely true in most real-world applications.</p><p>For example, if you <a id="id280" class="indexterm"/>were attempting to identify spam by monitoring e-mail messages, it is almost certainly true that some features will be more important than others. For example, the e-mail sender may be a more important indicator of spam than the message text. Additionally, the words in the message body are not independent from one another, since the appearance of some words is a very good indication that other words are also likely to appear. A message with the word Viagra will probably also contain the words prescription or drugs.</p><p>However, in most cases when these assumptions are violated, Naive Bayes still performs fairly well. This is true even in extreme circumstances where strong dependencies are found among the features. Due to the algorithm's versatility and accuracy across many types of conditions, Naive Bayes is often a strong first candidate for classification learning tasks.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>The exact reason why Naive Bayes works well in spite of its faulty assumptions has been the subject of much speculation. One explanation is that it is not important to obtain a precise estimate of probability, so long as the predictions are accurate. For instance, if a spam filter correctly identifies spam, does it matter whether it was 51 percent or 99 percent confident in its prediction? For one discussion of this topic, refer to: Domingos P, Pazzani M. On the optimality of the simple Bayesian classifier under zero-one loss. <span class="emphasis"><em>Machine Learning</em></span>. 1997; 29:103-130.</p></div></div><div class="section" title="Classification with Naive Bayes"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec20"/>Classification with Naive Bayes</h3></div></div></div><p>Let's extend our <a id="id281" class="indexterm"/>spam filter by adding a few additional terms to be monitored in addition to the term Viagra: Money, Groceries, and Unsubscribe. The Naive Bayes learner is trained by constructing a likelihood table for the appearance of these four words (labeled <span class="emphasis"><em>W<sub>1</sub></em></span>, <span class="emphasis"><em>W<sub>2</sub></em></span>, <span class="emphasis"><em>W<sub>3</sub></em></span>, and <span class="emphasis"><em>W<sub>4</sub></em></span>), as shown in the following diagram for 100 e-mails:</p><div class="mediaobject"><img src="graphics/B03905_04_09.jpg" alt="Classification with Naive Bayes"/></div><p>As new messages are received, we need to calculate the posterior probability to determine whether they are more likely to be spam or ham, given the likelihood of the words found in the message text. For example, suppose that a message contains the terms Viagra and Unsubscribe, but does not contain either Money or Groceries.</p><p>Using Bayes' theorem, we can define the problem as shown in the following formula. It captures the probability that a message is spam, given that <span class="emphasis"><em>Viagra = Yes</em></span>, <span class="emphasis"><em>Money = No</em></span>, <span class="emphasis"><em>Groceries = No</em></span>, and <span class="emphasis"><em>Unsubscribe = Yes</em></span>:</p><div class="mediaobject"><img src="graphics/B03905_04_10.jpg" alt="Classification with Naive Bayes"/></div><p>For a number of reasons, this formula is computationally difficult to solve. As additional features are added, tremendous amounts of memory are needed to store probabilities for all of the possible intersecting events; imagine the complexity of a Venn diagram for the events for four words, let alone for hundreds or more.</p><p>The work becomes much easier if we can exploit the fact that Naive Bayes assumes independence among events. Specifically, it assumes <span class="strong"><strong>class-conditional independence</strong></span>, which means that events are independent so long as they are conditioned on the same class value. Assuming conditional independence allows us to simplify the formula using the probability rule for independent events, which states that <span class="emphasis"><em>P(A ∩ B) = P(A) * P(B)</em></span>. Because the denominator does not depend on the class (spam or ham), it is treated as a constant value and can be ignored for the time being. This means that the conditional probability of spam can be expressed as:</p><div class="mediaobject"><img src="graphics/B03905_04_11.jpg" alt="Classification with Naive Bayes"/></div><p>And the probability that the message is ham can be expressed as:</p><div class="mediaobject"><img src="graphics/B03905_04_12.jpg" alt="Classification with Naive Bayes"/></div><p>Note that the equals <a id="id282" class="indexterm"/>symbol has been replaced by the proportional-to symbol (similar to a sideways, open-ended '8') to indicate the fact that the denominator has been omitted.</p><p>Using the values in the likelihood table, we can start filling numbers in these equations. The overall likelihood of spam is then:</p><p>(4/20) * (10/20) * (20/20) * (12/20) * (20/100) = 0.012</p><p>While the likelihood of ham is:</p><p>(1/80) * (66/80) * (71/80) * (23/80) * (80/100) = 0.002</p><p>Because <span class="emphasis"><em>0.012/0.002 = 6</em></span>, we can say that this message is six times more likely to be spam than ham. However, to convert these numbers into probabilities, we need to perform one last step to reintroduce the denominator that had been excluded. Essentially, we must rescale the likelihood of each outcome by dividing it by the total likelihood across all possible outcomes.</p><p>In this way, the probability of spam is equal to the likelihood that the message is spam divided by the likelihood that the message is either spam or ham:</p><p>0.012/(0.012 + 0.002) = 0.857</p><p>Similarly, the probability of ham is equal to the likelihood that the message is ham divided by the likelihood that the message is either spam or ham:</p><p>0.002/(0.012 + 0.002) = 0.143</p><p>Given the pattern of words found in this message, we expect that the message is spam with 85.7 percent probability and ham with 14.3 percent probability. Because these are mutually exclusive and exhaustive events, the probabilities sum to 1.</p><p>The Naive Bayes classification algorithm we used in the preceding example can be summarized by the following formula. The probability of level <span class="emphasis"><em>L</em></span> for class <span class="emphasis"><em>C</em></span>, given the evidence provided by features <span class="emphasis"><em>F<sub>1</sub></em></span> through <span class="emphasis"><em>F<sub>n</sub></em></span>, is equal to the product of the probabilities of each piece of evidence conditioned on the class level, the prior probability of the class level, and a scaling factor <span class="emphasis"><em>1/Z</em></span>, which converts the likelihood values into probabilities:</p><div class="mediaobject"><img src="graphics/B03905_04_13.jpg" alt="Classification with Naive Bayes"/></div><p>Although this equation <a id="id283" class="indexterm"/>seems intimidating, as the prior example illustrated, the series of steps is fairly straightforward. Begin by building a frequency table, use this to build a likelihood table, and multiply the conditional probabilities according to the Naive Bayes' rule. Finally, divide by the total likelihood to transform each class likelihood into a probability. After attempting this calculation a few times by hand, it will become second nature.</p></div><div class="section" title="The Laplace estimator"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec21"/>The Laplace estimator</h3></div></div></div><p>Before we <a id="id284" class="indexterm"/>employ Naive Bayes in more complex <a id="id285" class="indexterm"/>problems, there are some nuances to consider. Suppose that we received another message, this time containing all four terms: Viagra, Groceries, Money, and Unsubscribe. Using the Naive Bayes algorithm as before, we can compute the likelihood of spam as:</p><p>(4/20) * (10/20) * (0/20) * (12/20) * (20/100) = 0</p><p>The likelihood of ham is:</p><p>(1/80) * (14/80) * (8/80) * (23/80) * (80/100) = 0.00005</p><p>Therefore, the probability of spam is:</p><p>0/(0 + 0.00005) = 0</p><p>The probability of ham is:</p><p>0.00005/(0 + 0. 0.00005) = 1</p><p>These results suggest that the message is spam with 0 percent probability and ham with 100 percent probability. Does this prediction make sense? Probably not. The message contains several words usually associated with spam, including Viagra, which is rarely used in legitimate messages. It is therefore very likely that the message has been incorrectly classified.</p><p>This problem might arise if an event never occurs for one or more levels of the class. For instance, the term Groceries had never previously appeared in a spam message. Consequently, <span class="emphasis"><em>P(spam|groceries) = 0%</em></span>.</p><p>Because probabilities in the Naive Bayes formula are multiplied in a chain, this 0 percent value causes the posterior probability of spam to be zero, giving the word Groceries the ability to effectively nullify and overrule all of the other evidence. Even if the e-mail was otherwise overwhelmingly expected to be spam, the absence of the word Groceries in spam will always veto the other evidence and result in the probability of spam being zero.</p><p>A solution to this problem involves using something called the <span class="strong"><strong>Laplace estimator</strong></span>, which is named after the <a id="id286" class="indexterm"/>French mathematician <a id="id287" class="indexterm"/>Pierre-Simon Laplace. The Laplace estimator essentially adds a small number to each of the counts in the frequency table, which ensures that each feature has a nonzero probability of occurring with each class. Typically, the Laplace estimator is set to 1, which ensures that each class-feature combination is found in the data at least once.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip44"/>Tip</h3><p>The Laplace estimator can be set to any value and does not necessarily even have to be the same for each of the features. If you were a devoted Bayesian, you could use a Laplace estimator to reflect a presumed prior probability of how the feature relates to the class. In practice, given a large enough training dataset, this step is unnecessary and the value of 1 is almost always used.</p></div></div><p>Let's see how this affects our prediction for this message. Using a Laplace value of 1, we add one to each numerator in the likelihood function. The total number of 1 values must also be added to each conditional probability denominator. The likelihood of spam is therefore:</p><p>(5/24) * (11/24) * (1/24) * (13/24) * (20/100) = 0.0004</p><p>The likelihood of ham is:</p><p>(2/84) * (15/84) * (9/84) * (24/84) * (80/100) = 0.0001</p><p>This means that the probability of spam is 80 percent, and the probability of ham is 20 percent, which is a more plausible result than the one obtained when the term Groceries alone determined the result.</p></div><div class="section" title="Using numeric features with Naive Bayes"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec22"/>Using numeric features with Naive Bayes</h3></div></div></div><p>Because <a id="id288" class="indexterm"/>Naive Bayes uses frequency tables to learn <a id="id289" class="indexterm"/>the data, each feature must be categorical in order to create the combinations of class and feature values comprising of the matrix. Since numeric features do not have categories of values, the preceding algorithm does not work directly with numeric data. There are, however, ways that this can be addressed.</p><p>One easy and effective solution is to <span class="strong"><strong>discretize</strong></span> numeric features, which simply means that the numbers are put into categories known <a id="id290" class="indexterm"/>as <span class="strong"><strong>bins</strong></span>. For this reason, discretization is also <a id="id291" class="indexterm"/>sometimes called <span class="strong"><strong>binning</strong></span>. This method is ideal when there are large amounts of training data, a common condition while working with Naive Bayes.</p><p>There are several <a id="id292" class="indexterm"/>different ways to discretize a numeric feature. Perhaps the most common is to explore the data for natural categories or <span class="strong"><strong>cut points</strong></span> in the distribution of data. For example, suppose that you added a feature to the spam dataset that recorded the time of night or day the e-mail was sent, from 0 to 24 hours past midnight.</p><p>Depicted using a histogram, the time data might look something like the following diagram. In the early hours of the morning, the message frequency is low. The activity picks up during business hours and tapers off in the evening. This seems to create four natural bins of activity, as partitioned by the dashed lines indicating places where the numeric data are divided into levels of a new nominal feature, which could then be used with Naive Bayes:</p><div class="mediaobject"><img src="graphics/B03905_04_14.jpg" alt="Using numeric features with Naive Bayes"/></div><p>Keep in mind that the choice of four bins was somewhat arbitrary based on the natural distribution of data and a hunch about how the proportion of spam might change throughout the day. We might expect that spammers operate in the late hours of the night or they may operate during the day, when people are likely to check their e-mail. This said, to capture <a id="id293" class="indexterm"/>these trends, we could have just as easily <a id="id294" class="indexterm"/>used three bins or twelve.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip45"/>Tip</h3><p>If there are no obvious cut points, one option will be to discretize the feature using quantiles. You could divide the data into three bins with tertiles, four bins with quartiles, or five bins with quintiles.</p></div></div><p>One thing to keep in mind is that discretizing a numeric feature always results in a reduction of information as the feature's original granularity is reduced to a smaller number of categories. It is important to strike a balance here. Too few bins can result in important trends being obscured. Too many bins can result in small counts in the Naive Bayes frequency table, which can increase the algorithm's sensitivity to noisy data.</p></div></div></div></div>
<div class="section" title="Example &#x2013; filtering mobile phone spam with the Naive Bayes algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec22"/>Example – filtering mobile phone spam with the Naive Bayes algorithm</h1></div></div></div><p>As the <a id="id295" class="indexterm"/>worldwide use of mobile <a id="id296" class="indexterm"/>phones has grown, a new avenue for electronic junk mail has opened for disreputable marketers. These advertisers utilize Short Message Service (SMS) text messages to target potential consumers with unwanted advertising known as SMS spam. This type of spam is particularly troublesome because, unlike e-mail spam, many cellular phone users pay a fee per SMS received. Developing a classification algorithm that could filter SMS spam would provide a useful tool for cellular phone providers.</p><p>Since Naive <a id="id297" class="indexterm"/>Bayes has been used <a id="id298" class="indexterm"/>successfully for e-mail spam filtering, it seems likely that it could also be applied to SMS spam. However, relative to e-mail spam, SMS spam poses additional challenges for automated filters. SMS messages are often limited to 160 characters, reducing the amount of text that can be used to identify whether a message is junk. The limit, combined with small mobile phone keyboards, has led many to adopt a form of SMS shorthand lingo, which further blurs the line between legitimate messages and spam. Let's see how a simple Naive Bayes classifier handles these challenges.</p><div class="section" title="Step 1 – collecting data"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec40"/>Step 1 – collecting data</h2></div></div></div><p>To develop the <a id="id299" class="indexterm"/>Naive Bayes classifier, we will <a id="id300" class="indexterm"/>use data adapted from the SMS <a id="id301" class="indexterm"/>Spam Collection at <a class="ulink" href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/">http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/</a>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>To read more about how the SMS Spam Collection was developed, refer to: Gómez JM, Almeida TA, Yamakami A. On the validity of a new SMS spam collection. <span class="emphasis"><em>Proceedings of the 11<sup>th</sup> IEEE International Conference on Machine Learning and Applications</em></span>. 2012.</p></div></div><p>This dataset includes the text of SMS messages along with a label indicating whether the message is unwanted. Junk messages are labeled spam, while legitimate messages are labeled ham. Some examples of spam and ham are shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Sample SMS ham</p>
</th><th style="text-align: left" valign="bottom">
<p>Sample SMS spam</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Better. Made up for Friday and stuffed myself like a pig yesterday. Now I feel bleh. But, at least, its not writhing pain kind of bleh.</li><li class="listitem" style="list-style-type: disc">If he started searching, he will get job in few days. He has great potential and talent.</li><li class="listitem" style="list-style-type: disc">I got another job! The one at the hospital, doing data analysis or something, starts on Monday! Not sure when my thesis will finish.</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Congratulations ur awarded 500 of CD vouchers or 125 gift guaranteed &amp; Free entry 2 100 wkly draw txt MUSIC to 87066.</li><li class="listitem" style="list-style-type: disc">December only! Had your mobile 11mths+? You are entitled to update to the latest colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906.</li><li class="listitem" style="list-style-type: disc">Valentines Day Special! Win over £1000 in our quiz and take your partner on the trip of a lifetime! Send GO to 83600 now. 150 p/msg rcvd.</li></ul></div>
</td></tr></tbody></table></div><p>Looking at the preceding messages, did you notice any distinguishing characteristics of spam? One notable characteristic is that two of the three spam messages use the word "free," yet the word does not appear in any of the ham messages. On the other hand, two of the ham messages cite specific days of the week, as compared to zero in spam messages.</p><p>Our Naive Bayes classifier will take advantage of such patterns in the word frequency to determine whether the SMS messages seem to better fit the profile of spam or ham. While it's not inconceivable that the word "free" would appear outside of a spam SMS, a legitimate message is likely to provide additional words explaining the context. For instance, a ham message might state "are you free on Sunday?" Whereas, a spam message might use the <a id="id302" class="indexterm"/>phrase "free ringtones." The classifier will compute the probability of spam and ham, given the evidence provided by all the words in the message.</p></div><div class="section" title="Step 2 – exploring and preparing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec41"/>Step 2 – exploring and preparing the data</h2></div></div></div><p>The first step <a id="id303" class="indexterm"/>towards constructing our classifier <a id="id304" class="indexterm"/>involves processing the raw data for analysis. Text data are challenging to prepare, because it is necessary to transform the words and sentences into a form that a computer can understand. We will transform our data <a id="id305" class="indexterm"/>into a representation known as <span class="strong"><strong>bag-of-words</strong></span>, which ignores word order and simply provides a variable indicating whether the word appears at all.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip46"/>Tip</h3><p>The data used here has been modified slightly from the original in order to make it easier to work with in R. If you plan on following along with the example, download the <code class="literal">sms_spam.csv</code> file from the Packt website and save it in your R working directory.</p></div></div><p>We'll begin by importing the CSV data and saving it in a data frame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_raw &lt;- read.csv("sms_spam.csv", stringsAsFactors = FALSE)</strong></span>
</pre></div><p>Using the <code class="literal">str()</code> function, we see that the <code class="literal">sms_raw</code> data frame includes 5,559 total SMS messages with two features: <code class="literal">type</code> and <code class="literal">text</code>. The SMS type has been coded as either <code class="literal">ham</code> or <code class="literal">spam</code>. The <code class="literal">text</code> element stores the full raw SMS text.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(sms_raw)</strong></span>
<span class="strong"><strong>'data.frame':   5559 obs. of  2 variables:</strong></span>
<span class="strong"><strong> $ type: chr  "ham" "ham" "ham" "spam" ...</strong></span>
<span class="strong"><strong> $ text: chr  "Hope you are having a good week. Just checking in" "K..give back my thanks." "Am also doing in cbe only. But have to pay." "complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out"| __truncated__ ...</strong></span>
</pre></div><p>The <code class="literal">type</code> element is currently a character vector. Since this is a categorical variable, it would be better to convert it into a factor, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_raw$type &lt;- factor(sms_raw$type)</strong></span>
</pre></div><p>Examining this with the <code class="literal">str()</code> and <code class="literal">table()</code> functions, we see that <code class="literal">type</code> has now been appropriately recoded as a factor. Additionally, we see that 747 (about 13 percent) of SMS messages in our data were labeled as spam, while the others were labeled as ham:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(sms_raw$type)</strong></span>
<span class="strong"><strong> Factor w/ 2 levels "ham","spam": 1 1 1 2 2 1 1 1 2 1 ...</strong></span>
<span class="strong"><strong>&gt; table(sms_raw$type)</strong></span>
<span class="strong"><strong> ham spam</strong></span>
<span class="strong"><strong>4812  747</strong></span>
</pre></div><p>For now, we will <a id="id306" class="indexterm"/>leave the message text alone. As <a id="id307" class="indexterm"/>you will learn in the next section, processing the raw SMS messages will require the use of a new set of powerful tools designed specifically to process text data.</p><div class="section" title="Data preparation – cleaning and standardizing text data"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec23"/>Data preparation – cleaning and standardizing text data</h3></div></div></div><p>SMS <a id="id308" class="indexterm"/>messages are strings of text composed <a id="id309" class="indexterm"/>of words, spaces, numbers, and punctuation. Handling this type of complex data takes a lot of thought and effort. One needs to consider how to remove numbers and punctuation; handle uninteresting words such as <span class="emphasis"><em>and</em></span>, <span class="emphasis"><em>but</em></span>, and <span class="emphasis"><em>or</em></span>; and how to break apart sentences into individual words. Thankfully, this functionality has been provided by the members of the R community in a text mining package titled <code class="literal">tm</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>The <code class="literal">tm</code> package was originally created by Ingo Feinerer as a dissertation project at the Vienna University of Economics and Business. To learn more, see: Feinerer I, Hornik K, Meyer D. Text Mining Infrastructure in R. <span class="emphasis"><em>Journal of Statistical Software</em></span>. 2008; 25:1-54.</p></div></div><p>The <code class="literal">tm</code> package can be installed via the <code class="literal">install.packages("tm")</code> command and loaded with the <code class="literal">library(tm)</code> command. Even if you already have it installed, it may be worth re-running the install process to ensure that your version is up-to-date, as the <code class="literal">tm</code> package is still being actively developed. This occasionally results in changes to its functionality.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip47"/>Tip</h3><p>This chapter was written and tested using tm version 0.6-2, which was current as of July 2015. If you see differences in the output or if the code does not work, you may be using a different version. The Packt Publishing support page for this book will post solutions for future <code class="literal">tm</code> packages if significant changes are noted.</p></div></div><p>The first step in processing text data involves creating a <span class="strong"><strong>corpus</strong></span>, which is a collection of text documents. The <a id="id310" class="indexterm"/>documents can be short or long, from individual news articles, pages in a book or on the web, or entire books. In our case, the corpus will be a collection of SMS messages.</p><p>In order to create a corpus, we'll use the <code class="literal">VCorpus()</code> function in the <code class="literal">tm</code> package, which refers to a <a id="id311" class="indexterm"/>volatile corpus—volatile as it <a id="id312" class="indexterm"/>is stored in memory as opposed to being stored on disk (the <code class="literal">PCorpus()</code> function can be used to access a permanent corpus stored in a database). This function requires us to specify the source of documents for the corpus, which could be from a computer's filesystem, a database, the Web, or elsewhere. Since we already loaded the SMS message text into R, we'll use the <code class="literal">VectorSource()</code> reader function to create a source object from the existing <code class="literal">sms_raw$text</code> vector, which can then be supplied to <code class="literal">VCorpus()</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_corpus &lt;- VCorpus(VectorSource(sms_raw$text))</strong></span>
</pre></div><p>The resulting corpus object is saved with the name <code class="literal">sms_corpus</code>.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip48"/>Tip</h3><p>By specifying an optional <code class="literal">readerControl</code> parameter, the <code class="literal">VCorpus()</code> function offers functionality to import text from sources such as PDFs and Microsoft Word files. To learn more, examine the <span class="emphasis"><em>Data Import</em></span> section in the <code class="literal">tm</code> package vignette using the <code class="literal">vignette("tm")</code> command.</p></div></div><p>By printing the corpus, we see that it contains documents for each of the 5,559 SMS messages in the training data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; print(sms_corpus)</strong></span>
<span class="strong"><strong>&lt;&lt;VCorpus&gt;&gt;</strong></span>
<span class="strong"><strong>Metadata:  corpus specific: 0, document level (indexed): 0</strong></span>
<span class="strong"><strong>Content:  documents: 5559</strong></span>
</pre></div><p>Because the <code class="literal">tm</code> corpus is essentially a complex list, we can use list operations to select documents in the corpus. To receive a summary of specific messages, we can use the <code class="literal">inspect()</code> function with list operators. For example, the following command will view a summary of the first and second SMS messages in the corpus:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; inspect(sms_corpus[1:2])</strong></span>
<span class="strong"><strong>&lt;&lt;VCorpus&gt;&gt;</strong></span>
<span class="strong"><strong>Metadata:  corpus specific: 0, document level (indexed): 0</strong></span>
<span class="strong"><strong>Content:  documents: 2</strong></span>

<span class="strong"><strong>[[1]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument&gt;&gt;</strong></span>
<span class="strong"><strong>Metadata:  7</strong></span>
<span class="strong"><strong>Content:  chars: 49</strong></span>

<span class="strong"><strong>[[2]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument&gt;&gt;</strong></span>
<span class="strong"><strong>Metadata:  7</strong></span>
<span class="strong"><strong>Content:  chars: 23</strong></span>
</pre></div><p>To view the actual message text, the <code class="literal">as.character()</code> function must be applied to the desired messages. To view one message, use the <code class="literal">as.character()</code> function on a single list element, noting that the double-bracket notation is required:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; as.character(sms_corpus[[1]])</strong></span>
<span class="strong"><strong>[1] "Hope you are having a good week. Just checking in"</strong></span>
</pre></div><p>To view multiple documents, we'll need to use <code class="literal">as.character()</code> on several items in the <code class="literal">sms_corpus</code> object. To do so, we'll use the <code class="literal">lapply()</code> function, which is a part of a family of R functions that applies a procedure to each element of an R data structure. These functions, which include <code class="literal">apply()</code> and <code class="literal">sapply()</code> among others, are one of the key idioms of the R <a id="id313" class="indexterm"/>language. Experienced R <a id="id314" class="indexterm"/>coders use these much like the way <code class="literal">for</code> or <code class="literal">while</code> loops are used in other programming languages, as they result in more readable (and sometimes more efficient) code. The <code class="literal">lapply()</code> command to apply <code class="literal">as.character()</code> to a subset of corpus elements is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; lapply(sms_corpus[1:2], as.character)</strong></span>
<span class="strong"><strong>$`1`</strong></span>
<span class="strong"><strong>[1] "Hope you are having a good week. Just checking in"</strong></span>

<span class="strong"><strong>$`2`</strong></span>
<span class="strong"><strong>[1] "K..give back my thanks."</strong></span>
</pre></div><p>As noted earlier, the corpus contains the raw text of 5,559 text messages. In order to perform our analysis, we need to divide these messages into individual words. But first, we need to clean the text, in order to standardize the words, by removing punctuation and other characters that clutter the result. For example, we would like the strings <span class="emphasis"><em>Hello</em></span>!, <span class="emphasis"><em>HELLO</em></span>, and <span class="emphasis"><em>hello</em></span> to be counted as instances of the same word.</p><p>The <code class="literal">tm_map()</code> function provides a method to apply a transformation (also known as mapping) to a <code class="literal">tm</code> corpus. We will use this function to clean up our corpus using a series of transformations and save the result in a new object called <code class="literal">corpus_clean</code>.</p><p>Our first order of business will be to standardize the messages to use only lowercase characters. To this end, R provides a <code class="literal">tolower()</code> function that returns a lowercase version of text strings. In order to apply this function to the corpus, we need to use the <code class="literal">tm</code> wrapper function <code class="literal">content_transformer()</code> to treat <code class="literal">tolower()</code> as a transformation function that can be used to access the corpus. The full command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_corpus_clean &lt;- tm_map(sms_corpus,</strong></span>
<span class="strong"><strong>    content_transformer(tolower))</strong></span>
</pre></div><p>To check whether the command worked as advertised, let's inspect the first message in the original corpus and compare it to the same in the transformed corpus:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; as.character(sms_corpus[[1]])</strong></span>
<span class="strong"><strong>[1] "Hope you are having a good week. Just checking in"</strong></span>
<span class="strong"><strong>&gt; as.character(sms_corpus_clean[[1]])</strong></span>
<span class="strong"><strong>[1] "hope you are having a good week. just checking in"</strong></span>
</pre></div><p>As expected, uppercase letters have been replaced by lowercase versions of the same.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip49"/>Tip</h3><p>The <code class="literal">content_transformer()</code> function can be used to apply more sophisticated text processing and cleanup processes, such as <code class="literal">grep</code> pattern matching and replacement. Simply write a custom function and wrap it before applying via <code class="literal">tm_map()</code> as done earlier.</p></div></div><p>Let's continue <a id="id315" class="indexterm"/>our cleanup by removing <a id="id316" class="indexterm"/>numbers from the SMS messages. Although some numbers may provide useful information, the majority would likely be unique to individual senders and thus will not provide useful patterns across all messages. With this in mind, we'll strip all the numbers from the corpus as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removeNumbers)</strong></span>
</pre></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip50"/>Tip</h3><p>Note that the preceding code did not use the <code class="literal">content_transformer()</code> function. This is because <code class="literal">removeNumbers()</code> is built into <code class="literal">tm</code> along with several other mapping functions that do not need to be wrapped. To see the other built-in transformations, simply type <code class="literal">getTransformations()</code>.</p></div></div><p>Our next task is to remove filler words such as <span class="emphasis"><em>to</em></span>, <span class="emphasis"><em>and</em></span>, <span class="emphasis"><em>but</em></span>, and <span class="emphasis"><em>or</em></span> from our SMS messages. These terms are known as <span class="strong"><strong>stop words</strong></span> and are typically removed prior to text mining. This is due to the fact that although they appear very frequently, they do not provide much useful information for machine learning.</p><p>Rather than define a list of stop words ourselves, we'll use the <code class="literal">stopwords()</code> function provided by the <code class="literal">tm</code> package. This function allows us to access various sets of stop words, across several languages. By default, common English language stop words are used. To see the default list, type <code class="literal">stopwords()</code> at the command line. To see the other languages and options available, type <code class="literal">?stopwords</code> for the documentation page.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip51"/>Tip</h3><p>Even within a single language, there is no single definitive list of stop words. For example, the default English list in <code class="literal">tm</code> includes about 174 words while another option includes 571 words. You can even specify your own list of stop words if you prefer. Regardless of the list you choose, keep in mind the goal of this transformation, which is to eliminate all useless data while keeping as much useful information as possible.</p></div></div><p>The stop words alone are not a useful transformation. What we need is a way to remove any words that appear in the stop words list. The solution lies in the <code class="literal">removeWords()</code> function, which is a transformation included with the <code class="literal">tm</code> package. As we have done before, we'll use the <code class="literal">tm_map()</code> function to apply this mapping to the data, providing the <code class="literal">stopwords()</code> function as a parameter to indicate exactly the words we would like to remove. The full command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean,</strong></span>
<span class="strong"><strong>    removeWords, stopwords())</strong></span>
</pre></div><p>Since <code class="literal">stopwords()</code> simply returns a vector of stop words, had we chosen so, we could have replaced it <a id="id317" class="indexterm"/>with our own vector of words to be removed. In this way, we could expand or reduce the list of stop words to our liking or remove a completely different set of words entirely.</p><p>Continuing with <a id="id318" class="indexterm"/>our cleanup process, we can also eliminate any punctuation from the text messages using the built-in <code class="literal">removePunctuation()</code> transformation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, removePunctuation)</strong></span>
</pre></div><p>The <code class="literal">removePunctuation()</code> transformation strips punctuation characters from the text blindly, which can lead to unintended consequences. For example, consider what happens when it is applied as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; removePunctuation("hello...world")</strong></span>
<span class="strong"><strong>[1] "helloworld"</strong></span>
</pre></div><p>As shown, the lack of blank space after the ellipses has caused the words <span class="emphasis"><em>hello</em></span> and <span class="emphasis"><em>world</em></span> to be joined as a single word. While this is not a substantial problem for our analysis, it is worth noting for the future.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip52"/>Tip</h3><p>To work around the default behavior of <code class="literal">removePunctuation()</code>, simply create a custom function that replaces rather than removes punctuation characters:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; replacePunctuation &lt;- function(x) {</strong></span>
<span class="strong"><strong>    gsub("[[:punct:]]+", " ", x)</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>Essentially, this uses R's <code class="literal">gsub()</code> function to substitute any punctuation characters in <code class="literal">x</code> with a blank space. The <code class="literal">replacePunctuation()</code> function can then be used with <code class="literal">tm_map()</code> as with other transformations.</p></div></div><p>Another common standardization for text data involves reducing words to their root form in a process called <span class="strong"><strong>stemming</strong></span>. The stemming process takes words like <span class="emphasis"><em>learned</em></span>, <span class="emphasis"><em>learning</em></span>, and <span class="emphasis"><em>learns</em></span>, and strips the suffix in order to transform them into the base form, <span class="emphasis"><em>learn</em></span>. This allows machine learning algorithms to treat the related terms as a single concept rather than attempting to learn a pattern for each variant.</p><p>The <code class="literal">tm</code> package <a id="id319" class="indexterm"/>provides stemming <a id="id320" class="indexterm"/>functionality via integration with the <code class="literal">SnowballC</code> package. At the time of this writing, <code class="literal">SnowballC</code> was not installed by default with <code class="literal">tm</code>. Do so with <code class="literal">install.packages("SnowballC")</code> if it is not installed already.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>The <code class="literal">SnowballC</code> package is maintained by Milan Bouchet-Valat and provides an R interface to the C-based <code class="literal">libstemmer</code> library, which is based on M.F. Porter's "Snowball" word <a id="id321" class="indexterm"/>stemming algorithm, a widely used open source stemming method. For more detail, see <a class="ulink" href="http://snowball.tartarus.org">http://snowball.tartarus.org</a>.</p></div></div><p>The <code class="literal">SnowballC</code> package provides a <code class="literal">wordStem()</code> function, which for a character vector, returns the same vector of terms in its root form. For example, the function correctly stems the variants of the word <span class="emphasis"><em>learn</em></span>, as described previously:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(SnowballC)</strong></span>
<span class="strong"><strong>&gt; wordStem(c("learn", "learned", "learning", "learns"))</strong></span>
<span class="strong"><strong>[1] "learn"   "learn"   "learn"   "learn"</strong></span>
</pre></div><p>In order to apply the <code class="literal">wordStem()</code> function to an entire corpus of text documents, the <code class="literal">tm</code> package includes a <code class="literal">stemDocument()</code> transformation. We apply this to our corpus with the <code class="literal">tm_map()</code> function exactly as done earlier:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stemDocument)</strong></span>
</pre></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip54"/>Tip</h3><p>If you receive an error message while applying the <code class="literal">stemDocument()</code> transformation, please confirm that you have the <code class="literal">SnowballC</code> package installed. If after installing the package you still encounter the message that <code class="literal">all scheduled cores encountered errors</code>, you can also try forcing the <code class="literal">tm_map()</code> command to a single core, by adding an additional parameter to specify <code class="literal">mc.cores=1</code>.</p></div></div><p>After removing numbers, stop words, and punctuation as well as performing stemming, the text messages <a id="id322" class="indexterm"/>are left with the blank <a id="id323" class="indexterm"/>spaces that previously separated the now-missing pieces. The final step in our text cleanup process is to remove additional whitespace, using the built-in <code class="literal">stripWhitespace()</code> transformation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_corpus_clean &lt;- tm_map(sms_corpus_clean, stripWhitespace)</strong></span>
</pre></div><p>The following table shows the first three messages in the SMS corpus before and after the cleaning process. The messages have been limited to the most interesting words, and punctuation and capitalization have been removed:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>SMS messages before cleaning</p>
</th><th style="text-align: left" valign="bottom">
<p>SMS messages after cleaning</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top"><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; as.character(sms_corpus[1:3])</strong></span>

<span class="strong"><strong>[[1]] Hope you are having a good week. Just checking in</strong></span>

<span class="strong"><strong>[[2]] K..give back my thanks.</strong></span>

<span class="strong"><strong>[[3]] Am also doing in cbe only. But have to pay.</strong></span>
</pre></div>
</td><td style="text-align: left" valign="top"><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; as.character(sms_corpus_clean[1:3])</strong></span>

<span class="strong"><strong>[[1]] hope good week just check</strong></span>

<span class="strong"><strong>[[2]] kgive back thank</strong></span>

<span class="strong"><strong>[[3]] also cbe pay</strong></span>
</pre></div>
</td></tr></tbody></table></div></div><div class="section" title="Data preparation – splitting text documents into words"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec24"/>Data preparation – splitting text documents into words</h3></div></div></div><p>Now <a id="id324" class="indexterm"/>that the data are processed to our liking, the final step is to split the messages into individual components through a process called <span class="strong"><strong>tokenization</strong></span>. A token is a single element of a text string; in this case, the tokens are words.</p><p>As you might assume, the <code class="literal">tm</code> package provides functionality to tokenize the SMS message corpus. The <code class="literal">DocumentTermMatrix()</code> function will take a corpus and create a data structure called a <span class="strong"><strong>Document Term Matrix</strong></span> (<span class="strong"><strong>DTM</strong></span>) in which rows indicate documents (SMS messages) and columns indicate terms (words).</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip55"/>Tip</h3><p>The <code class="literal">tm</code> package also provides a data structure for a <span class="strong"><strong>Term Document Matrix</strong></span> (<span class="strong"><strong>TDM</strong></span>), which is simply a transposed DTM in which the rows are terms and the columns are documents. Why the need for both? Sometimes, it is more convenient to work with one or the other. For example, if the number of documents is small, while the word list is large, it may make sense to use a TDM because it is generally easier to display many rows than to display many columns. This said, the two are often interchangeable.</p></div></div><p>Each cell in the matrix stores a number indicating a count of the times the word represented by the column appears in the document represented by the row. The following illustration depicts only a small portion of the DTM for the SMS corpus, as the complete matrix has 5,559 rows and over 7,000 columns:</p><div class="mediaobject"><img src="graphics/B03905_04_15.jpg" alt="Data preparation – splitting text documents into words"/></div><p>The fact that each cell in the table is zero implies that none of the words listed on the top of the columns appear in any of the first five messages in the corpus. This highlights the reason why this data structure is <a id="id325" class="indexterm"/>called a <span class="strong"><strong>sparse matrix</strong></span>; the vast majority of the cells in the matrix are filled with zeros. Stated in real-world terms, although each message must contain at least one word, the probability of any one word appearing in a given message is small.</p><p>Creating a <a id="id326" class="indexterm"/>DTM sparse matrix, given a <code class="literal">tm</code> corpus, involves a single command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_dtm &lt;- DocumentTermMatrix(sms_corpus_clean)</strong></span>
</pre></div><p>This will create an <code class="literal">sms_dtm</code> object that contains the tokenized corpus using the default settings, which apply minimal processing. The default settings are appropriate because we have already prepared the corpus manually.</p><p>On the other hand, if we hadn't performed the preprocessing, we could do so here by providing a list of <code class="literal">control</code> parameter options to override the defaults. For example, to create a DTM directly from the raw, unprocessed SMS corpus, we can use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_dtm2 &lt;- DocumentTermMatrix(sms_corpus, control = list(</strong></span>
<span class="strong"><strong>    tolower = TRUE,</strong></span>
<span class="strong"><strong>    removeNumbers = TRUE,</strong></span>
<span class="strong"><strong>    stopwords = TRUE,</strong></span>
<span class="strong"><strong>    removePunctuation = TRUE,</strong></span>
<span class="strong"><strong>    stemming = TRUE</strong></span>
<span class="strong"><strong>  ))</strong></span>
</pre></div><p>This applies the same preprocessing steps to the SMS corpus in the same order as done earlier. However, comparing <code class="literal">sms_dtm</code> to <code class="literal">sms_dtm2</code>, we see a slight difference in the number of terms in the matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_dtm</strong></span>
<span class="strong"><strong>&lt;&lt;DocumentTermMatrix (documents: 5559, terms: 6518)&gt;&gt;</strong></span>
<span class="strong"><strong>Non-/sparse entries: 42113/36191449</strong></span>
<span class="strong"><strong>Sparsity           : 100%</strong></span>
<span class="strong"><strong>Maximal term length: 40</strong></span>
<span class="strong"><strong>Weighting          : term frequency (tf)</strong></span>

<span class="strong"><strong>&gt; sms_dtm2</strong></span>
<span class="strong"><strong>&lt;&lt;DocumentTermMatrix (documents: 5559, terms: 6909)&gt;&gt;</strong></span>
<span class="strong"><strong>Non-/sparse entries: 43192/38363939</strong></span>
<span class="strong"><strong>Sparsity           : 100%</strong></span>
<span class="strong"><strong>Maximal term length: 40</strong></span>
<span class="strong"><strong>Weighting          : term frequency (tf)</strong></span>
</pre></div><p>The reason for this discrepancy has to do with a minor difference in the ordering of the preprocessing steps. The <code class="literal">DocumentTermMatrix()</code> function applies its cleanup functions to the text strings only after they have been split apart into words. Thus, it uses a slightly different stop words removal function. Consequently, some words split differently than when they are cleaned before tokenization.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip56"/>Tip</h3><p>To force the two prior document term matrices to be identical, we can override the default stop words function with our own that uses the original replacement function. Simply replace <code class="literal">stopwords = TRUE</code> with the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>stopwords = function(x) { removeWords(x, stopwords()) }</strong></span>
</pre></div></div></div><p>The <a id="id327" class="indexterm"/>differences between these two cases illustrate an important principle of cleaning text data: the order of operations matters. With this in mind, it is very important to think through how early steps in the process are going to affect later ones. The order presented here will work in many cases, but when the process is tailored more carefully to specific datasets and use cases, it may require rethinking. For example, if there are certain terms you hope to exclude from the matrix, consider whether you should search for them before or after stemming. Also, consider how the removal of punctuation—and whether the punctuation is eliminated or replaced by blank space—affects these steps.</p></div><div class="section" title="Data preparation – creating training and test datasets"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec25"/>Data preparation – creating training and test datasets</h3></div></div></div><p>With our data <a id="id328" class="indexterm"/>prepared for analysis, we <a id="id329" class="indexterm"/>now need to split the data into training and test datasets, so that once our spam classifier is built, it can be evaluated on data it has not previously seen. But even though we need to keep the classifier blinded as to the contents of the test dataset, it is important that the split occurs after the data have been cleaned and processed; we need exactly the same preparation steps to occur on both the training and test datasets.</p><p>We'll divide the data into two portions: 75 percent for training and 25 percent for testing. Since the SMS messages are sorted in a random order, we can simply take the first 4,169 for training and leave the remaining 1,390 for testing. Thankfully, the DTM object acts very much like a data frame and can be split using the standard <code class="literal">[row, col]</code> operations. As our DTM stores SMS messages as rows and words as columns, we must request a specific range of rows and all columns for each:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_dtm_train &lt;- sms_dtm[1:4169, ]</strong></span>
<span class="strong"><strong>&gt; sms_dtm_test  &lt;- sms_dtm[4170:5559, ]</strong></span>
</pre></div><p>For convenience later on, it is also helpful to save a pair of vectors with labels for each of the rows in the training and testing matrices. These labels are not stored in the DTM, so we would need to pull them from the original <code class="literal">sms_raw</code> data frame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_train_labels &lt;- sms_raw[1:4169, ]$type</strong></span>
<span class="strong"><strong>&gt; sms_test_labels  &lt;- sms_raw[4170:5559, ]$type</strong></span>
</pre></div><p>To confirm that the subsets are representative of the complete set of SMS data, let's compare the proportion of spam in the training and test data frames:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; prop.table(table(sms_train_labels))</strong></span>
<span class="strong"><strong>      ham      spam</strong></span>
<span class="strong"><strong>0.8647158 0.1352842</strong></span>
<span class="strong"><strong>&gt; prop.table(table(sms_test_labels))</strong></span>
<span class="strong"><strong>      ham      spam</strong></span>
<span class="strong"><strong>0.8683453 0.1316547</strong></span>
</pre></div><p>Both the training <a id="id330" class="indexterm"/>data and test data contain <a id="id331" class="indexterm"/>about 13 percent spam. This suggests that the spam messages were divided evenly between the two datasets.</p></div><div class="section" title="Visualizing text data – word clouds"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec26"/>Visualizing text data – word clouds</h3></div></div></div><p>A <span class="strong"><strong>word cloud</strong></span> is a <a id="id332" class="indexterm"/>way to visually depict the frequency at which words <a id="id333" class="indexterm"/>appear in text data. The cloud is composed of words scattered somewhat randomly around the figure. Words appearing more often in the text are shown in a larger font, while less common terms are shown in smaller fonts. This type of figures grew in popularity recently, since it provides a way to observe trending topics on social media websites.</p><p>The <code class="literal">wordcloud</code> package provides a simple R function to create this type of diagrams. We'll use it to visualize the types of words in SMS messages, as comparing the clouds for spam and ham will help us gauge whether our Naive Bayes spam filter is likely to be successful. If you haven't already done so, install and load the package by typing <code class="literal">install.packages("wordcloud")</code> and <code class="literal">library(wordcloud)</code> at the R command line.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note07"/>Note</h3><p>The <code class="literal">wordcloud</code> package <a id="id334" class="indexterm"/>was written by Ian Fellows. For more information on this package, visit his blog at <a class="ulink" href="http://blog.fellstat.com/?cat=11">http://blog.fellstat.com/?cat=11</a>.</p></div></div><p>A word cloud can be created directly from a <code class="literal">tm</code> corpus object using the syntax:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)</strong></span>
</pre></div><p>This will create a word cloud from our prepared SMS corpus. Since we specified <code class="literal">random.order = FALSE</code>, the cloud will be arranged in a nonrandom order with higher frequency words placed closer to the center. If we do not specify <code class="literal">random.order</code>, the cloud would be arranged randomly by default. The <code class="literal">min.freq</code> parameter specifies the number of times a word must appear in the corpus before it will be displayed in the cloud. Since a frequency of 50 is about 1 percent of the corpus, this means that a word must be found in at least 1 percent of the SMS messages to be included in the cloud.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip57"/>Tip</h3><p>You might get a warning message noting that R was unable to fit all of the words in the figure. If so, try increasing <code class="literal">min.freq</code> to reduce the number of words in the cloud. It might also help to use the <code class="literal">scale</code> parameter to reduce the font size.</p></div></div><p>The resulting word <a id="id335" class="indexterm"/>cloud should appear similar to the following <a id="id336" class="indexterm"/>figure:</p><div class="mediaobject"><img src="graphics/B03905_04_16.jpg" alt="Visualizing text data – word clouds"/></div><p>A perhaps more interesting visualization involves comparing the clouds for SMS spam and ham. Since we did not construct separate corpora for spam and ham, this is an appropriate time to note a very helpful feature of the <code class="literal">wordcloud()</code> function. Given a vector of raw text strings, it will automatically apply common text preparation processes before displaying the cloud.</p><p>Let's use R's <code class="literal">subset()</code> function to take a subset of the <code class="literal">sms_raw</code> data by the SMS <code class="literal">type</code>. First, we'll create a subset where the message <code class="literal">type</code> is <code class="literal">spam</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; spam &lt;- subset(sms_raw, type == "spam")</strong></span>
</pre></div><p>Next, we'll do the same thing for the <code class="literal">ham</code> subset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; ham &lt;- subset(sms_raw, type == "ham")</strong></span>
</pre></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip58"/>Tip</h3><p>Be careful to note the double equals sign. Like many programming languages, R uses <code class="literal">==</code> to test equality. If you accidently use a single equals sign, you'll end up with a subset much larger than you expected!</p></div></div><p>We now have two data frames, <code class="literal">spam</code> and <code class="literal">ham</code>, each with a <code class="literal">text</code> feature containing the raw text strings for <a id="id337" class="indexterm"/>SMSes. Creating word clouds is as simple as before. This time, we'll use the <code class="literal">max.words</code> parameter to look at the 40 most common words in <a id="id338" class="indexterm"/>each of the two sets. The scale parameter allows us to adjust the maximum and minimum font size for words in the cloud. Feel free to adjust these parameters as you see fit. This is illustrated in the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))</strong></span>
<span class="strong"><strong>&gt; wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))</strong></span>
</pre></div><p>The resulting word clouds are shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B03905_04_17.jpg" alt="Visualizing text data – word clouds"/></div><p>Do you have a hunch about which one is the spam cloud and which represents ham?</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip59"/>Tip</h3><p>Because of the randomization process, each word cloud may look slightly different. Running the <code class="literal">wordcloud()</code> function several times allows you to choose the cloud that is the most visually appealing for presentation purposes.</p></div></div><p>As you probably <a id="id339" class="indexterm"/>guessed, the spam cloud is on the left. Spam messages include words such as <span class="emphasis"><em>urgent</em></span>, <span class="emphasis"><em>free</em></span>, <span class="emphasis"><em>mobile</em></span>, <span class="emphasis"><em>claim</em></span>, and <span class="emphasis"><em>stop</em></span>; these terms do not appear in the ham <a id="id340" class="indexterm"/>cloud at all. Instead, ham messages use words such as <span class="emphasis"><em>can</em></span>, <span class="emphasis"><em>sorry</em></span>, <span class="emphasis"><em>need</em></span>, and <span class="emphasis"><em>time</em></span>. These stark differences suggest that our Naive Bayes model will have some strong key words to differentiate between the classes.</p></div><div class="section" title="Data preparation – creating indicator features for frequent words"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec27"/>Data preparation – creating indicator features for frequent words</h3></div></div></div><p>The final <a id="id341" class="indexterm"/>step in the data preparation process is to transform the sparse matrix into a data structure that can be used to train a Naive Bayes classifier. Currently, the sparse matrix includes over 6,500 features; this is a feature for every word that appears in at least one SMS message. It's unlikely that all of these are useful for classification. To reduce the number of features, we will eliminate any word that appear in less than five SMS messages, or in less than about 0.1 percent of the records in the training data.</p><p>Finding frequent words requires use of the <code class="literal">findFreqTerms()</code> function in the <code class="literal">tm</code> package. This function takes a DTM and returns a character vector containing the words that appear for at least the specified number of times. For instance, the following command will display the words appearing at least five times in the <code class="literal">sms_dtm_train</code> matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; findFreqTerms(sms_dtm_train, 5)</strong></span>
</pre></div><p>The result of the function is a character vector, so let's save our frequent words for later on:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_freq_words &lt;- findFreqTerms(sms_dtm_train, 5)</strong></span>
</pre></div><p>A peek into the contents of the vector shows us that there are 1,136 terms appearing in at least five SMS messages:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(sms_freq_words)</strong></span>
<span class="strong"><strong> chr [1:1136] "abiola" "abl" "abt" "accept" "access" "account" "across" "act" "activ" ...</strong></span>
</pre></div><p>We now need to filter our DTM to include only the terms appearing in a specified vector. As done earlier, we'll use the data frame style <code class="literal">[row, col]</code> operations to request specific portions of the DTM, noting that the columns are named after the words the DTM contains. We can take advantage of this to limit the DTM to specific words. Since we want all the rows, but only the columns representing the words in the <code class="literal">sms_freq_words</code> vector, our commands are:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_dtm_freq_train&lt;- sms_dtm_train[ , sms_freq_words]</strong></span>
<span class="strong"><strong>&gt; sms_dtm_freq_test &lt;- sms_dtm_test[ , sms_freq_words]</strong></span>
</pre></div><p>The training <a id="id342" class="indexterm"/>and test datasets now include 1,136 features, which correspond to words appearing in at least five messages.</p><p>The Naive Bayes classifier is typically trained on data with categorical features. This poses a problem, since the cells in the sparse matrix are numeric and measure the number of times a word appears in a message. We need to change this to a categorical variable that simply indicates yes or no depending on whether the word appears at all.</p><p>The following defines a <code class="literal">convert_counts()</code> function to convert counts to <code class="literal">Yes</code>/<code class="literal">No</code> strings:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; convert_counts &lt;- function(x) {</strong></span>
<span class="strong"><strong>    x &lt;- ifelse(x &gt; 0, "Yes", "No")</strong></span>
<span class="strong"><strong>  }</strong></span>
</pre></div><p>By now, some of the pieces of the preceding function should look familiar. The first line defines the function. The <code class="literal">ifelse(x &gt; 0, "Yes", "No")</code> statement transforms the values in <code class="literal">x</code>, so that if the value is greater than <code class="literal">0</code>, then it will be replaced by <code class="literal">"Yes"</code>, otherwise it will be replaced by a <code class="literal">"No"</code> string. Lastly, the newly transformed <code class="literal">x</code> vector is returned.</p><p>We now need to apply <code class="literal">convert_counts()</code> to each of the columns in our sparse matrix. You may be able to guess the R function to do exactly this. The function is simply called <code class="literal">apply()</code> and is used much like <code class="literal">lapply()</code> was used previously.</p><p>The <code class="literal">apply()</code> function allows a function to be used on each of the rows or columns in a matrix. It uses a <code class="literal">MARGIN</code> parameter to specify either rows or columns. Here, we'll use <code class="literal">MARGIN = 2</code>, since we're interested in the columns (<code class="literal">MARGIN = 1</code> is used for rows). The commands to convert the training and test matrices are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_train &lt;- apply(sms_dtm_freq_train, MARGIN = 2,</strong></span>
<span class="strong"><strong>                                       convert_counts)</strong></span>
<span class="strong"><strong>&gt; sms_test &lt;- apply(sms_dtm_freq_test, MARGIN = 2,</strong></span>
<span class="strong"><strong>                                      convert_counts)</strong></span>
</pre></div><p>The result will be two character type matrixes, each with cells indicating <code class="literal">"Yes"</code> or <code class="literal">"No"</code> for whether the <a id="id343" class="indexterm"/>word represented by the column appears at any point in the message represented by the row.</p></div></div><div class="section" title="Step 3 – training a model on the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec42"/>Step 3 – training a model on the data</h2></div></div></div><p>Now that we <a id="id344" class="indexterm"/>have transformed the raw SMS messages into a format that can be represented by a statistical model, it is time to apply the Naive Bayes algorithm. The algorithm will use the presence or absence of words to estimate the probability that a given SMS message is spam.</p><p>The Naive Bayes implementation we will employ is in the <code class="literal">e1071</code> package. This package was developed in the statistics department of the Vienna University of Technology (TU Wien), and includes a variety of functions for machine learning. If you have not done so already, be sure to install and load the package using the <code class="literal">install.packages("e1071")</code> and <code class="literal">library(e1071)</code> commands before continuing.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip60"/>Tip</h3><p>Many machine learning approaches are implemented in more than one R package, and Naive Bayes is no exception. One other option is <code class="literal">NaiveBayes()</code> in the <code class="literal">klaR</code> package, which is nearly identical to the one in the <code class="literal">e1071</code> package. Feel free to use whichever option you prefer.</p></div></div><p>Unlike the k-NN algorithm we used for classification in the previous chapter, a Naive Bayes learner is trained and used for classification in separate stages. Still, as shown in the following table, these steps are is fairly straightforward:</p><div class="mediaobject"><img src="graphics/B03905_04_18.jpg" alt="Step 3 – training a model on the data"/></div><p>To build our <a id="id345" class="indexterm"/>model on the <code class="literal">sms_train</code> matrix, we'll use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_classifier &lt;- naiveBayes(sms_train, sms_train_labels)</strong></span>
</pre></div><p>The <code class="literal">sms_classifier</code> object now contains a <code class="literal">naiveBayes</code> classifier object that can be used to make predictions.</p></div><div class="section" title="Step 4 – evaluating model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec43"/>Step 4 – evaluating model performance</h2></div></div></div><p>To evaluate <a id="id346" class="indexterm"/>the SMS classifier, we need to test its predictions on unseen messages in the test data. Recall that the unseen message features are stored in a matrix named <code class="literal">sms_test</code>, while the class labels (spam or ham) are stored in a vector named <code class="literal">sms_test_labels</code>. The classifier that we trained has been named <code class="literal">sms_classifier</code>. We will use this classifier to generate predictions and then compare the predicted values to the true values.</p><p>The <code class="literal">predict()</code> function is used to make the predictions. We will store these in a vector named <code class="literal">sms_test_pred</code>. We will simply supply the function with the names of our classifier and test dataset, as shown:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_test_pred &lt;- predict(sms_classifier, sms_test)</strong></span>
</pre></div><p>To compare the predictions to the true values, we'll use the <code class="literal">CrossTable()</code> function in the <code class="literal">gmodels</code> package, which we used previously. This time, we'll add some additional parameters to eliminate unnecessary cell proportions and use the <code class="literal">dnn</code> parameter (dimension names) to relabel the rows and columns, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(gmodels)</strong></span>
<span class="strong"><strong>&gt; CrossTable(sms_test_pred, sms_test_labels,</strong></span>
<span class="strong"><strong>    prop.chisq = FALSE, prop.t = FALSE,</strong></span>
<span class="strong"><strong>    dnn = c('predicted', 'actual'))</strong></span>
</pre></div><p>This produces <a id="id347" class="indexterm"/>the following table:</p><div class="mediaobject"><img src="graphics/B03905_04_19.jpg" alt="Step 4 – evaluating model performance"/></div><p>Looking at the table, we can see that a total of only <span class="emphasis"><em>6 + 30 = 36</em></span> of the 1,390 SMS messages were incorrectly classified (2.6 percent). Among the errors were 6 out of 1,207 ham messages that were misidentified as spam, and 30 of the 183 spam messages were incorrectly labeled as ham. Considering the little effort we put into the project, this level of performance seems quite impressive. This case study exemplifies the reason why Naive Bayes is the standard for text classification; directly out of the box, it performs surprisingly well.</p><p>On the other <a id="id348" class="indexterm"/>hand, the six legitimate messages that were incorrectly classified as spam could cause significant problems for the deployment of our filtering algorithm, because the filter could cause a person to miss an important text message. We should investigate to see whether we can slightly tweak the model to achieve better performance.</p></div><div class="section" title="Step 5 – improving model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec44"/>Step 5 – improving model performance</h2></div></div></div><p>You may <a id="id349" class="indexterm"/>have noticed that we didn't set a value for the Laplace estimator while training our model. This allows words that appeared in zero spam or zero ham messages to have an indisputable say in the classification process. Just because the word "ringtone" only appeared in the spam messages in the training data, it does not mean that every message with this word should be classified as spam.</p><p>We'll build a Naive Bayes model as done earlier, but this time set <code class="literal">laplace = 1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_classifier2 &lt;- naiveBayes(sms_train, sms_train_labels,</strong></span>
<span class="strong"><strong>    laplace = 1)</strong></span>
</pre></div><p>Next, we'll make predictions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_test_pred2 &lt;- predict(sms_classifier2, sms_test)</strong></span>
</pre></div><p>Finally, we'll compare the predicted classes to the actual classifications using a cross tabulation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; CrossTable(sms_test_pred2, sms_test_labels,</strong></span>
<span class="strong"><strong>    prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,</strong></span>
<span class="strong"><strong>    dnn = c('predicted', 'actual'))</strong></span>
</pre></div><p>This results in the following table:</p><div class="mediaobject"><img src="graphics/B03905_04_20.jpg" alt="Step 5 – improving model performance"/></div><p>Adding the Laplace estimator reduced the number of false positives (ham messages erroneously classified as spam) from six to five and the number of false negatives from 30 to 28. Although <a id="id350" class="indexterm"/>this seems like a small change, it's substantial considering that the model's accuracy was already quite impressive. We'd need to be careful before tweaking the model too much in order to maintain the balance between being overly aggressive and overly passive while filtering spam. Users would prefer that a small number of spam messages slip through the filter than an alternative in which ham messages are filtered too aggressively.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Summary</h1></div></div></div><p>In this chapter, we learned about classification using Naive Bayes. This algorithm constructs tables of probabilities that are used to estimate the likelihood that new examples belong to various classes. The probabilities are calculated using a formula known as Bayes' theorem, which specifies how dependent events are related. Although Bayes' theorem can be computationally expensive, a simplified version that makes so-called "naive" assumptions about the independence of features is capable of handling extremely large datasets.</p><p>The Naive Bayes classifier is often used for text classification. To illustrate its effectiveness, we employed Naive Bayes on a classification task involving spam SMS messages. Preparing the text data for analysis required the use of specialized R packages for text processing and visualization. Ultimately, the model was able to classify over 97 percent of all the SMS messages correctly as spam or ham.</p><p>In the next chapter, we will examine two more machine learning methods. Each performs classification by partitioning data into groups of similar values.</p></div></body></html>
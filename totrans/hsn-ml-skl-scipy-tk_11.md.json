["```py\nx = np.random.uniform(low=5, high=20, size=100)\ne = np.random.normal(loc=0, scale=0.5, size=100)\ny = (x + e) ** 3\n```", "```py\nfrom sklearn.model_selection import train_test_split\nx = x.reshape((x.shape[0],1))\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n```", "```py\nclass YTransformer:\n\n    def __init__(self, power=1):\n        self.power = power\n\n    def fit(self, x, y):\n        pass\n\n    def transform(self, x, y):\n        return x, np.power(y, self.power)\n\n    def inverse_transform(self, x, y):\n        return x, np.power(y, 1/self.power)\n\n    def fit_transform(self, x, y):\n        return self.transform(x, y)\n```", "```py\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\nfor power in [1, 1/2, 1/3, 1/4, 1/5]:\n\n    yt = YTransformer(power)\n    _, y_train_t = yt.fit_transform(None, y_train)\n    _, y_test_t = yt.transform(None, y_test)\n\n    rgs = Ridge()\n\n    rgs.fit(x_train, y_train_t)\n    y_pred_t = rgs.predict(x_test)\n\n    _, y_pred = yt.inverse_transform(None, y_pred_t)\n\n    print(\n        'Transformed y^{:.2f}: MAE={:.0f}, R2={:.2f}'.format(\n            power,\n            mean_absolute_error(y_test, y_pred),\n            r2_score(y_test, y_pred),\n        )\n    )\n```", "```py\nTransformed y^1.00: MAE=559, R2=0.89\nTransformed y^0.50: MAE=214, R2=0.98\nTransformed y^0.33: MAE=210, R2=0.97\nTransformed y^0.25: MAE=243, R2=0.96\nTransformed y^0.20: MAE=276, R2=0.95\n```", "```py\nfrom sklearn.datasets import make_regression\n\nx, y = make_regression(\n    n_samples=500, n_features=8, n_informative=8, n_targets=3, noise=30.0\n)\n```", "```py\nfeature_names = [f'Feature # {i}' for i in range(x.shape[1])]\ntarget_names = [f'Target # {i}' for i in range(y.shape[1])]\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n```", "```py\nfrom sklearn.linear_model import SGDRegressor\n\nrgr = SGDRegressor()\nrgr.fit(x_train, y_train)\n```", "```py\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.linear_model import SGDRegressor\n\nrgr = MultiOutputRegressor(\n    estimator=SGDRegressor(), \n    n_jobs=-1\n)\nrgr.fit(x_train, y_train)\ny_pred = rgr.predict(x_test)\n```", "```py\ndf_pred = pd.DataFrame(y_pred, columns=target_names)\n```", "```py\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\nfor t in range(y_train.shape[1]):\n    print(\n        'Target # {}: MAE={:.2f}, R2={:.2f}'.format(\n            t,\n            mean_absolute_error(y_test[t], y_pred[t]),\n            r2_score(y_test[t], y_pred[t]),\n        )\n    )\n```", "```py\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.linear_model import Ridge\n\nrgr = RegressorChain(\n    base_estimator=Ridge(\n        alpha=1\n    ), \n    order=[0,1,2],\n)\nrgr.fit(x_train, y_train)\ny_pred = rgr.predict(x_test)\n```", "```py\npd.DataFrame(\n    zip(\n        rgr.estimators_[-1].coef_, \n        feature_names + target_names\n    ),\n    columns=['Coeff', 'Feature']\n)[\n    ['Feature', 'Coeff']\n].style.bar(\n    subset=['Coeff'], align='mid', color='#AAAAAA'\n)\n```", "```py\nfrom sklearn.datasets import make_classification\n\nx, y = make_classification(\n    n_samples=5000, n_features=15, n_informative=8, n_redundant=2, \n    n_classes=4, class_sep=0.5, \n)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score\n\nclf = OneVsRestClassifier(\n    estimator=LogisticRegression(solver='saga')\n)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsOneClassifier\n\nclf = OneVsOneClassifier(\n    estimator=LogisticRegression(solver='saga')\n)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\n\naccuracy_score(y_test, y_pred)\n```", "```py\nfrom sklearn.datasets import make_multilabel_classification\n\nx, y = make_multilabel_classification(\n    n_samples=500, n_features=8, n_classes=3, n_labels=2\n)\n```", "```py\nx.shape, y.shape # ((500, 8), (500, 3))\nnp.unique(y) # array([0, 1])\n```", "```py\ny[:,-1] = y[:,0]    \n```", "```py\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = MultiOutputClassifier(\n    estimator=GradientBoostingClassifier(\n        n_estimators=500,\n        learning_rate=0.01,\n        subsample=0.8,\n    ),\n    n_jobs=-1\n)\nclf.fit(x_train, y_train)\ny_pred_multioutput = clf.predict(x_test)\n```", "```py\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = ClassifierChain(\n    base_estimator=GradientBoostingClassifier(\n        n_estimators=500,\n        learning_rate=0.01,\n        subsample=0.8,\n    ),\n    order=[0,1,2]\n)\nclf.fit(x_train, y_train)\ny_pred_chain = clf.predict(x_test)\n```", "```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nx, y = make_classification(\n    n_samples=50000, n_features=15, n_informative=5, n_redundant=10, \n    n_classes=2, class_sep=0.001\n)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n```", "```py\nfrom sklearn.naive_bayes import GaussianNB\n\nclf = GaussianNB()\nclf.fit(x_train, y_train)\ny_pred_proba = clf.predict_proba(x_test)[:,-1]\n```", "```py\nfrom sklearn.calibration import calibration_curve\n\nfraction_of_positives, mean_predicted_value = calibration_curve(\n    y_test, y_pred_proba, n_bins=10\n)\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\nax.plot(\n    mean_predicted_value, fraction_of_positives, \"--\", \n    label='Uncalibrated GaussianNB', color='k'\n)\n\nfig.show()\n```", "```py\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\n\nclf_calib = CalibratedClassifierCV(GaussianNB(), cv=3, method='isotonic')\nclf_calib.fit(x_train, y_train)\ny_pred_calib = clf_calib.predict(x_test)\ny_pred_proba_calib = clf_calib.predict_proba(x_test)[:,-1]\n```", "```py\ndef precision_at_k_score(y_true, y_pred_proba, k=1000, pos_label=1):\n    topk = [\n        y_true_ == pos_label \n        for y_true_, y_pred_proba_ \n        in sorted(\n            zip(y_true, y_pred_proba), \n            key=lambda y: y[1], \n            reverse=True\n        )[:k]\n    ]\n    return sum(topk) / len(topk)\n```", "```py\nprecision_at_k_score(y_test, y_pred_proba, k=500)\n```"]
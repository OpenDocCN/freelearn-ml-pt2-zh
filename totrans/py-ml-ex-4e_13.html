<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer454">
    <h1 class="chapterNumber">13</h1>
    <h1 class="chapterTitle" id="_idParaDest-309">Advancing Language Understanding and Generation with the Transformer Models</h1>
    <p class="normal">In the previous chapter, we focused on RNNs and used them to deal with sequence learning tasks. However, RNNs may easily suffer from the vanishing gradient problem. In this chapter, we will explore the Transformer neural network architecture, which is designed for sequence-to-sequence tasks and is particularly well suited for <strong class="keyWord">Natural Language Processing</strong> (<strong class="keyWord">NLP</strong>). The key<a id="_idIndexMarker1282"/> innovation is the self-attention mechanism, allowing the model to weigh different parts of the input sequence differently, and enabling it to capture long-range dependencies more effectively than RNNs.</p>
    <p class="normal">We will learn two cutting-edge models utilizing the Transformer architecture and delve into their practical applications, such as sentiment analysis and text generation. Expect enhanced performance on tasks previously covered in the preceding chapter.</p>
    <p class="normal">We will cover the following topics in this chapter:</p>
    <ul>
      <li class="bulletList">Understanding self-attention</li>
      <li class="bulletList">Exploring the Transformer’s architecture</li>
      <li class="bulletList">Improving sentiment analysis with <strong class="keyWord">Bidirectional Encoder Representations from Transformers</strong> (<strong class="keyWord">BERT</strong>) and Transformers</li>
      <li class="bulletList">Generating text using <strong class="keyWord">Generative Pre-trained Transformers</strong> (<strong class="keyWord">GPT</strong>)</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-310">Understanding self-attention</h1>
    <p class="normal">The Transformer neural network architecture revolves around the self-attention mechanism. So, let’s first kick off the<a id="_idIndexMarker1283"/> chapter by looking at this. <strong class="keyWord">Self-attention</strong> is a mechanism used in machine learning, particularly in NLP and computer vision. It allows a model to weigh the importance of different parts of the input sequence.</p>
    <p class="normal">Self-attention is a specific type of attention mechanism. In traditional attention mechanisms, the importance weights are between two different sets of input data. For example, an attention-based English-to-French translation model may focus on specific parts (e.g., nouns, verbs) of the English source sentences that are relevant to the current French target word being generated. However, in self-attention, the importance weighting operates between any two elements within the same input sequence. It focuses on how different parts in the same sequence relate to each other. Used for English-to-French translation, the self-attention model analyzes how each English word interacts with every other English word. By understanding these relationships, the model can generate a more nuanced and accurate French translation.</p>
    <p class="normal">In the context of NLP, traditional RNNs process input sequences sequentially. Because of this sequential processing style, RNNs can only handle shorter sequences well, and capture shorter-range dependencies among tokens. On the contrary, a self-attention-powered model can simultaneously process all input tokens in a sequence. For a given token, the model assigns different attention weights to different tokens based on their relevance to the given token, regardless of their positions. As a result, it can capture the relationships between different tokens in the input, and their long-range dependencies. Self-attention-based models outperform RNNs in several sequence-to-sequence tasks such as machine translation, text summarization, and query answering.</p>
    <p class="normal">Let’s discuss how <strong class="keyWord">self-attention</strong> plays a key role in sequence learning tasks in the following examples:</p>
    <p class="normal">“I read<em class="italic"> Python Machine Learning by Example</em> by Hayden Liu and it is indeed a great book.” Apparently, <em class="italic">it</em> here refers to <em class="italic">Python Machine Learning by Example</em>. When the Transformer model processes this sentence, self-attention will associate <em class="italic">it</em> with <em class="italic">Python Machine Learning by Example</em>.</p>
    <p class="normal">We can use a self-attention-based model to summarize a document (e.g., this chapter). Self-attention isn’t limited by the order of sentences, unlike sequential learning RNNs, and it can identify relationships between sentences (even distant ones), which ensures the summary reflects the overall information.</p>
    <p class="normal">Given a token in an input sequence, self-attention allows the model to look at the other tokens in the sequence at<a id="_idIndexMarker1284"/> different attention levels. In the next section, we’ll look at a more detailed explanation of how the self-attention mechanism works.</p>
    <h2 class="heading-2" id="_idParaDest-311">Key, value, and query representations</h2>
    <p class="normal">The self-attention mechanism is applied to each token in a sequence. Its goal is to represent every token by an embedding vector that captures long-range context. The embedding vector of an input token is composed of three vectors: key, value, and query. For a given token, a self-attention-based model learns these three vectors in order to compute the embedding vector.</p>
    <p class="normal">The following describes the meaning of each of the three vectors. To aid comprehension, we use an analogy that likens a model’s understanding of a sequence to a detective investigating a crime scene with a bunch of clues. To solve the case (understand the meaning of the sequence), the detective needs to figure out which clues (tokens) are most important and how clues are connected (how tokens relate to each other):</p>
    <ul>
      <li class="bulletList">The key vector, <em class="italic">K</em>, represents <a id="_idIndexMarker1285"/>the core information of a token. It captures the key but not the detailed information a token holds. In our detective analogy, the<a id="_idIndexMarker1286"/> key vector of a clue might contain information about a witness who saw the crime, but not the details they saw.</li>
      <li class="bulletList">The value vector, <em class="italic">V</em>, holds the<a id="_idIndexMarker1287"/> full information of a token. In our<a id="_idIndexMarker1288"/> detective example, the value vector of a clue could be the detailed statement from the witness.</li>
      <li class="bulletList">Finally, the query vector, <em class="italic">Q</em>, represents the importance of understanding a given token in the context of the whole sequence. It is a question about a token’s relevance to the <a id="_idIndexMarker1289"/>current task. During the detective’s investigation, their <a id="_idIndexMarker1290"/>focus can change depending on what they are currently looking for. It can be the weapon used, the victim, the motivation, or something else. The query vector represents the detective’s current focus in the investigation.</li>
    </ul>
    <p class="normal">These three vectors are derived from the input token’s embeddings. Let’s discuss how they work together using the detective example again:</p>
    <ul>
      <li class="bulletList">First, we calculate the attention<a id="_idIndexMarker1291"/> scores based on the key and query vector. Based on a query vector, <em class="italic">Q</em>, the model analyses each token and computes the relevance score between its key vector, <em class="italic">K</em>, and the query vector, <em class="italic">Q</em>. A <a id="_idIndexMarker1292"/>high score indicates a high importance of the token to the context. In the detective example, they try to figure out how relevant a clue is to the current investigation focus. For instance, the detective would think clue A about the building is highly relevant when they are looking into the crime scene location. Note that the detective doesn’t look at the details of clue A yet, just like the attention scores are computed based on the key vector, not the value vector. The model (the detective) will use the value vector (details of a clue) in the next stage – embedding vector generation.</li>
      <li class="bulletList">Next, we generate the embedding <a id="_idIndexMarker1293"/>vector for a token using the value vectors, <em class="italic">V</em>, and the attention weights. The detective has decided how much weight (attention scores) is assigned to each clue, and now they will combine the details (value vector) of each clue to create a comprehensive understanding (embedding vector) of the crime scene.<div class="note">
          <p class="normal">The terminology, “<code class="inlineCode">query</code>,” “<code class="inlineCode">key</code>,” and “<code class="inlineCode">value</code>,” is inspired by the information retrieval systems.</p>
          <p class="normal">Let’s illustrate using a search engine example. Given a query, the search engine undergoes a matching process against the key of each document candidate and comes up with a ranking score individually. Based on the detailed information and the ranking score of a document, the search engine creates a search result page with the retrieval of specific associated values.</p>
        </div>
      </li>
    </ul>
    <p class="normal">We’ve discussed what key, value, and query vectors are in self-attention mechanisms, and how they work together to allow the model to capture important information from an input sequence. The generated context-aware embedding vector encapsulates the relationships between tokens in the sequence. I hope the detective’s crime scene analogy helped you gain a better understanding. In the next section, we will see how the context-aware embedding vector is generated mathematically.</p>
    <h3 class="heading-3" id="_idParaDest-312">Attention score calculation and embedding vector generation</h3>
    <p class="normal">We have a sequence of tokens (<em class="italic">x</em><sub class="subscript">1</sub>, <em class="italic">x</em><sub class="subscript">2</sub>, …<em class="italic">x</em><sub class="subscript">i</sub>, …<em class="italic">x</em><sub class="subscript">n</sub>). Here, <em class="italic">n</em> is the length of the sequence.</p>
    <p class="normal">For a given token, the calculation <a id="_idIndexMarker1294"/>of attention score begins by computing the similarity score between each token in the sequence and the token in question. The similarity score is computed by taking the dot product of the query vector of the current token and <a id="_idIndexMarker1295"/>the key vector of the other tokens:</p>
    <p class="center"><em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">ij</sub>=<em class="italic">q</em><sub class="subscript-italic" style="font-style: italic;">i</sub>∙<em class="italic">k</em><sub class="subscript-italic" style="font-style: italic;">j</sub></p>
    <p class="normal">Here, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">ij</sub> is the similarity score between token <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub> and <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">j</sub>, <em class="italic">q</em><sub class="subscript-italic" style="font-style: italic;">i</sub> is the query vector of <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub>, and <em class="italic">k</em><sub class="subscript-italic" style="font-style: italic;">j</sub> is the query vector of <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">j</sub>. The similarity score measures how relevant a token <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">j</sub> is to the current token <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub>.</p>
    <p class="normal">You may notice that the raw similarity scores do not directly reflect relative relevance (they can be negative or greater than 1). Recall that the <code class="inlineCode">softmax</code> function normalizes raw scores, converting them into probabilities that sum up to 1. Hence, we need to normalize them using a <code class="inlineCode">softmax</code> function. The normalized scores are the attention scores we are looking for:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_001.png"/></p>
    <p class="normal">Here, <em class="italic">a</em><sub class="subscript-italic" style="font-style: italic;">ij</sub> is the similarity score between token <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub> and <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">j</sub>, d is the dimension of the key vector, and the division of <img alt="" role="presentation" src="../Images/B21047_13_002.png"/> is for scaling. The attention weights (<em class="italic">a</em><sub class="subscript-italic" style="font-style: italic;">i</sub><sub class="subscript">1</sub>,<em class="italic">a</em><sub class="subscript-italic" style="font-style: italic;">i</sub><sub class="subscript">2</sub>,…,<em class="italic">a</em><sub class="subscript-italic" style="font-style: italic;">i</sub><sub class="subscript">n</sub>) convey the probability distribution over all other tokens in the sequence with respect to the current token.</p>
    <p class="normal">With the normalized attention weights available, we can now compute the embedding vector for the current token. The embedding vector <em class="italic">z</em><sub class="subscript-italic" style="font-style: italic;">i</sub> is a weighted sum of the value vectors <em class="italic">v</em> of all tokens in the sequence, where each weight is the attention score <em class="italic">a</em><sub class="subscript-italic" style="font-style: italic;">ij</sub> between the current token <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">i</sub> and the respective token <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">j</sub>:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_003.png"/></p>
    <p class="normal">This weighted sum vector is considered the context-aware representation of the current token, taking into account its<a id="_idIndexMarker1296"/> relationship with all other tokens.</p>
    <p class="normal">Take the sequence <em class="italic">python machine learning by example</em> as an example; we take the following steps to calculate the self-attention embedding vector for the first word, <em class="italic">python</em>:</p>
    <ol>
      <li class="numberedList" value="1">We calculate the dot products between each word in the sequence and the word <em class="italic">python</em>. They are <em class="italic">q</em><sub class="subscript">1</sub>∙<em class="italic">k</em><sub class="subscript">1</sub>, <em class="italic">q</em><sub class="subscript">1</sub>∙<em class="italic">k</em><sub class="subscript">2</sub>, <em class="italic">q</em><sub class="subscript">1</sub>∙<em class="italic">k</em><sub class="subscript">3</sub>, <em class="italic">q</em><sub class="subscript">1</sub>∙<em class="italic">k</em><sub class="subscript">4</sub>, and <em class="italic">q</em><sub class="subscript">1</sub>∙<em class="italic">k</em><sub class="subscript">5</sub>. Here, <em class="italic">q</em><sub class="subscript">1</sub> is the query vector for the first word, and <em class="italic">k</em><sub class="subscript">1</sub> to <em class="italic">k</em><sub class="subscript">5</sub> are the key vectors for the five words, respectively.</li>
      <li class="numberedList">We normalize the <a id="_idIndexMarker1297"/>resulting dot products with division and <code class="inlineCode">softmax</code> activation to find the attention weights:</li>
    </ol>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_004.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_005.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_006.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_007.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_008.png"/></p>
    <ol>
      <li class="numberedList" value="3">Then, we multiply the resulting attention weights by the value vectors, <em class="italic">v</em><sub class="subscript">1</sub>, <em class="italic">v</em><sub class="subscript">2</sub>, <em class="italic">v</em><sub class="subscript">3</sub>, <em class="italic">v</em><sub class="subscript">4</sub>, <em class="italic">v</em><sub class="subscript">5</sub>, and add up the results:</li>
    </ol>
    <p class="center"><em class="italic">z</em><sub class="subscript">1</sub> = <em class="italic">a</em><sub class="subscript">11</sub>.<em class="italic">v</em><sub class="subscript">1</sub>+<em class="italic">a</em><sub class="subscript">12</sub>.<em class="italic">v</em><sub class="subscript">2</sub>+<em class="italic">a</em><sub class="subscript">13</sub>.<em class="italic">v</em><sub class="subscript">3</sub>+<em class="italic">a</em><sub class="subscript">14</sub>.<em class="italic">v</em><sub class="subscript">4</sub>+<em class="italic">a</em><sub class="subscript">15</sub>.<em class="italic">v</em><sub class="subscript">5</sub></p>
    <p class="normal-one"><em class="italic">z</em><sub class="subscript">1</sub> is the context-aware embedding vector for the first word, <em class="italic">python</em>, in the sequence. We repeat this process for each remaining word in the sequence to obtain its context-aware <a id="_idIndexMarker1298"/>embedding.</p>
    <p class="normal-one">During training<a id="_idIndexMarker1299"/> in a self-attention mechanism, the key, query, and value vectors are created by three weight matrices, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">k</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">q</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">v</sub>, using a linear transformation:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_009.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_010.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_011.png"/></p>
    <p class="normal-one">Here, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">k</sub> is the weight matrix for the key transformation, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">q</sub> is the weight matrix for the query transformation, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">v</sub> is the weight matrix for the value transformation. These three weight matrices are learnable parameters. During the model training process, they get updated typically using gradient-based optimization algorithms.</p>
    <p class="normal">Now, let’s see how we can simulate the calculation of <em class="italic">z</em><sub class="subscript">1</sub> in PyTorch:</p>
    <ol>
      <li class="numberedList" value="1">First, assume we have the following integer representation mapping for the tokens in the input <code class="inlineCode">python machine learning by example</code>:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sentence = torch.tensor(</span>
        [0, # python
         8, # machine    
         1, # learning
         6, # by
         2] # example
    )
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Each integer corresponds<a id="_idIndexMarker1300"/> to the index of the respective token in the vocabulary.</p>
    <ol>
      <li class="numberedList" value="2">We also assume <a id="_idIndexMarker1301"/>we have embeddings ready to use for our simulated vocabulary:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">embed = torch.nn.Embedding(</span><span class="hljs-con-number">10</span><span class="language-python">, </span><span class="hljs-con-number">16</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sentence_embed = embed(sentence).detach()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sentence_embed</span>
tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, 
         -2.1152,
          0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, 
         -0.2473],
        [-0.8834, -0.4189, -0.8048,  0.5656,  0.6104,  0.4669,  1.9507, 
         -1.0631,
         -0.0773,  0.1164, -0.5940, -1.2439, -0.1021, -1.0335, -0.3126,  
          0.2458],
        [-1.3527, -1.6959,  0.5667,  0.7935,  0.5988, -1.5551, -0.3414,  
          1.8530,
          0.7502, -0.5855, -0.1734,  0.1835,  1.3894,  1.5863,  0.9463, 
         -0.8437],
        [ 1.6459, -1.3602,  0.3446,  0.5199, -2.6133, -1.6965, -0.2282,  
          0.2800,
          0.2469,  0.0769,  0.3380,  0.4544,  0.4569, -0.8654,  0.7813, 
         -0.9268],
         [-0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,  0.6408, 
           0.4412,
          -0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, 
          -1.5867]])    
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, our simulated vocabulary has 10 tokens, and the embedding size is 16. <code class="inlineCode">detach()</code> is used to create a new tensor that shares the same underlying data as the original tensor but is detached from the computation graph. Also, note that you may get different embedding results due to some non-deterministic operations.</p>
    <ol>
      <li class="numberedList" value="3">Next, we assume we have the following three weight matrices, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">k</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">q</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">v</sub>:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">d = sentence_embed.shape[</span><span class="hljs-con-number">1</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">w_key = torch.rand(d, d)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">w_query = torch.rand(d, d)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">w_value = torch.rand(d, d)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">For matrix operations, it is essential that vectors <em class="italic">Q</em> and <em class="italic">K</em> share the same dimensions to ensure they operate within a consistent feature space. However, vector <em class="italic">V</em> is allowed to<a id="_idIndexMarker1302"/> have different dimensions. In this example, we will maintain<a id="_idIndexMarker1303"/> uniform dimensions for all three vectors for the sake of simplicity. So, we choose 16 as the common dimension.</p>
    <ol>
      <li class="numberedList" value="4">Now, we can compute the key vector <em class="italic">k</em><sub class="subscript">1</sub>, query vector <em class="italic">q</em><sub class="subscript">1</sub>, and value vector <em class="italic">v</em><sub class="subscript">1</sub> for the <em class="italic">python</em> token accordingly:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">token1_embed = sentence_embed[</span><span class="hljs-con-number">0</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">key_1 = w_key.matmul(token1_embed)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">query_1 = w_query.matmul(token1_embed)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">value_1 = w_value.matmul(token1_embed)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Take a look at <em class="italic">k</em><sub class="subscript">1</sub>:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">key_1</span>
tensor([-1.1371, -0.5677, -0.9324, -0.3195, -2.8886, -1.2679, -1.1153,  
         0.2904, 0.3825,  0.3179, -0.4977, -3.8230,  0.3699, -0.3932, 
        -1.8788, -3.3556])
</code></pre>
    <ol>
      <li class="numberedList" value="5">We can also directly compute the key matrix (composed of key vectors for individual tokens) as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">keys = sentence_embed.matmul(w_key.T)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">keys[</span><span class="hljs-con-number">0</span><span class="language-python">]</span>
tensor([-1.1371, -0.5677, -0.9324, -0.3195, -2.8886, -1.2679, -1.1153,  
         0.2904, 0.3825,  0.3179, -0.4977, -3.8230,  0.3699, -0.3932, 
        -1.8788, -3.3556])
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Similarly, the value matrix can be directly computed as follows:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">values = sentence_embed.matmul(w_value.T)</span>
</code></pre>
    <ol>
      <li class="numberedList" value="6">With the key matrix and the query vector <em class="italic">q</em><sub class="subscript">1</sub>, we obtain the attention weight vector <em class="italic">a</em><sub class="subscript">1</sub>=(<em class="italic">a</em><sub class="subscript">11</sub>, <em class="italic">a</em><sub class="subscript">12</sub>, <em class="italic">a</em><sub class="subscript">13</sub>, <em class="italic">a</em><sub class="subscript">14</sub>, <em class="italic">a</em><sub class="subscript">15</sub>):
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch.nn.functional </span><span class="hljs-con-keyword">as</span><span class="language-python"> F</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">a1 = F.softmax(query_1.matmul(keys.T) / d ** </span><span class="hljs-con-number">0.5</span><span class="language-python">, dim=</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">a1</span>
tensor([3.2481e-01, 4.2515e-01, 6.8915e-06, 2.5002e-01, 1.5529e-05])
</code></pre>
      </li>
      <li class="numberedList">Finally, we multiply the<a id="_idIndexMarker1304"/> resulting attention weights by the value vectors to obtain the<a id="_idIndexMarker1305"/> context-aware embedding vector for the first token:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">z1 = a1.matmul(values)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">z1</span>
tensor([-0.7136, -1.1795, -0.5726, -0.4959, -0.6838, -1.6460, -0.3782, -1.0066,
        -0.4798, -0.8996, -1.2138, -0.3955, -1.3302, -0.3832, -0.8446, -0.8470])
</code></pre>
      </li>
    </ol>
    <p class="normal-one">This is the self-attention version of the embedding vector for the <em class="italic">python</em> token based on the three toy weight matrices, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">k</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">q</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">v</sub>.</p>
    <p class="normal">In practice, we typically employ more than one set of trainable weight matrices, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">k</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">q</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">v</sub>. That is why self-attention is often called <strong class="keyWord">multi-head self-attention</strong>. Each attention head has its own set of learnable <a id="_idIndexMarker1306"/>parameters for the key, query, and value transformations. Using multiple attention heads can capture different aspects of relationships within a sequence. Let’s dig into this next.</p>
    <h2 class="heading-2" id="_idParaDest-313">Multi-head attention</h2>
    <p class="normal">The single-head attention <a id="_idIndexMarker1307"/>mechanism is effective but may not capture diverse relationships within the sequence. Multi-head attention extends this by employing multiple sets of query, key, and value matrices (multiple “heads”). Each head operates <strong class="keyWord">independently</strong> and can attend to different parts of the input sequence <strong class="keyWord">in parallel</strong>. This allows the model to capture diverse relationships simultaneously.</p>
    <p class="normal">Using the previous example sequence, <em class="italic">python machine learning by example</em>, one attention head might focus on local dependencies, identifying “<code class="inlineCode">machine learning</code>" as a noun phrase; another attention head might emphasize semantic relationships, inferring that the “<code class="inlineCode">examples</code>" are about “<code class="inlineCode">machine learning</code>.” It’s like having multiple analysts examining the same sentence. Each analyst focuses on a different aspect (for instance, one on grammar, one on word order, and another on sentiment). By combining their insights, you get a more comprehensive understanding of the sentence.</p>
    <p class="normal">Finally, the outputs from all attention heads are concatenated and linearly transformed to produce the final attention output.</p>
    <p class="normal">In this section, we presented a self-attention mechanism featuring trainable parameters. In the<a id="_idIndexMarker1308"/> upcoming section, we will delve into the Transformer architecture, which centers around the self-attention mechanism.</p>
    <h1 class="heading-1" id="_idParaDest-314">Exploring the Transformer’s architecture</h1>
    <p class="normal">The Transformer architecture was <a id="_idIndexMarker1309"/>proposed as an alternative to RNNs for<a id="_idIndexMarker1310"/> sequence-to-sequence tasks. It heavily relies on the self-attention mechanism to process both input and output sequences.</p>
    <p class="normal">We’ll start by looking at the high-level architecture of the Transformer model (image based on that in the paper <em class="italic">Attention Is All You Need</em>, by Vaswani et al.):</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B21047_13_01.png"/></figure>
    <p class="packt_figref">Figure 13.1: Transformer architecture</p>
    <p class="normal">As you can see, the <a id="_idIndexMarker1311"/>Transformer consists of two parts: the <strong class="keyWord">encoder</strong> (the<a id="_idIndexMarker1312"/> big rectangle on the left-hand side) and the <strong class="keyWord">decoder</strong> (the big rectangle on the right-hand side). The encoder encrypts the input <a id="_idIndexMarker1313"/>sequence. It has a <strong class="keyWord">multi-head attention layer</strong> and a<a id="_idIndexMarker1314"/> regular feedforward layer. On the other hand, the decoder generates the output sequence. It has a masked multi-head attention (we will talk about this in detail later) layer, along with a multi-head attention layer and a regular feedforward layer.</p>
    <p class="normal">At step <em class="italic">t</em>, the Transformer model takes in input steps <em class="italic">x</em><sub class="subscript">1</sub>, <em class="italic">x</em><sub class="subscript">2</sub>, …, <em class="italic">x</em><sub class="subscript">t</sub> and output steps <em class="italic">y</em><sub class="subscript">1</sub>, <em class="italic">y</em><sub class="subscript">2</sub>, …, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">−1</sub>. It then predicts <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub>. This is<a id="_idIndexMarker1315"/> no different from the many-to-many RNN model. In the next section, we will explore the important elements of the Transformer that set it apart from RNNs, including the encoder-decoder structure, positional encoding, and layer normalization.</p>
    <h2 class="heading-2" id="_idParaDest-315">The encoder-decoder structure</h2>
    <p class="normal">The encoder-decoder structure is the key element in the Transformer architecture. It leverages the model’s ability to<a id="_idIndexMarker1316"/> handle sequence-to-sequence tasks. Below is a breakdown of the encoder component and the decoder component with an analogy to help you understand.</p>
    <p class="normal">The <strong class="keyWord">encoder</strong> component processes<a id="_idIndexMarker1317"/> the input sequence and creates a context representation. Typically, the encoder component is a stack of encoders. Each encoder consists of a self-attention layer and a feedforward neural network. We’ve covered that the self-attention allows each token to attend to other tokens in the sequence. Unlike sequential models like RNNs, relations between tokens (even the distant ones) are captured in the Transformer, and the feedforward neural network adds non-linearity to the model’s learning capacity. We’ve seen this in deep neural networks before.</p>
    <p class="normal">Imagine you want to order a meal at a restaurant. The encoder in the Transformer works in a similar way as you reading the menu and generating your own understanding. The encoder takes in the menu (input sequence of words). It analyzes the words using self-attention, just like you read the descriptions of each dish and their ingredients (relationships between words). After the encoder (it could be a stack of encoders) digests the information from the menu, it creates a condensed representation, the encoded context that captures the essence of the menu. This output of the encoder is like your own comprehension of the menu.</p>
    <p class="normal">For the encoder component composed of multiple identical encoder blocks, the output from each encoder serves as the input for the subsequent block in the stacked structure. This stacked approach offers a powerful way to capture more complex relationships and create a richer understanding <a id="_idIndexMarker1318"/>of the input sequence. We’ve seen a similar approach in RNNs. In case you are wondering, six encoder blocks were employed in the original design in <em class="italic">Attention Is All You Need</em>. The number of encoders is not magical and is subject to experimentation.</p>
    <p class="normal">The <strong class="keyWord">decoder</strong> component utilizes the <a id="_idIndexMarker1319"/>context representation provided by the encoder to generate the output sequence. Similar to the encoder, the decoder component also consists of multiple stacked decoder blocks. Similarly, each decoder block contains a self-attention layer and a feedforward neural network. However, the self-attention in the decoder is slightly different from the encoder one. It attends to the output sequence but it only considers the context it has already built. This means for a given token, the decoder self-attention only considers the relationships between previously processed tokens and the current tokens. Recall that the self-attention in the encoder can attend to the whole input sequence at once. Hence, we call the self-attention<a id="_idIndexMarker1320"/> in the decoder <strong class="keyWord">masked self-attention</strong>.</p>
    <p class="normal">Besides a masked self-attention<a id="_idIndexMarker1321"/> layer and a feedforward neural network, the<a id="_idIndexMarker1322"/> decoder block has an additional attention layer called <strong class="keyWord">encoder-decoder attention</strong>. It attends to the context representation provided by the encoder so that the generated output sequence is relevant to the encoded context.</p>
    <p class="normal">Going back to the restaurant ordering analogy. Now, you (the decoder) want to place an order (generate the output sequence). The decoder uses encoder-decoder self-attention to consider the encoded context (your understanding of the menu). Encoder-decoder self-attention ensures the output generation is based on your comprehension of the menu, not someone else’s. The decoder <a id="_idIndexMarker1323"/>uses <strong class="keyWord">masked self-attention</strong> to generate the output word by word (your order of dishes). Masked self-attention ensures you don’t “peek” at future dishes (words) you haven’t “ordered” (generated) yet. With each generated word (the dish you order), the decoder can refine its understanding of the desired output sequence (meal) based on the encoded context.</p>
    <p class="normal">Similar to the encoder, in the stacked decoder component, the output from each decoder block serves as the input for the subsequent block. Due to the encoder-decoder self-attention, the number of decoder blocks is usually the same as the encoder blocks.</p>
    <p class="normal">During training, the model is provided with both the input and the target output sequences. It learns to generate the target output sequence by minimizing the difference between its predictions and the actual target sequence.</p>
    <p class="normal">In this section, we’ve delved into the Transformer’s encoder-decoder structure. The encoder stack extracts a context representation of the input sequence. The decoder generates the output sequence one token at a time, attending to both the encoded context and previously <a id="_idIndexMarker1324"/>generated tokens.</p>
    <h2 class="heading-2" id="_idParaDest-316">Positional encoding</h2>
    <p class="normal">While powerful, self-attention struggles to differentiate between the importance of elements based solely on their <a id="_idIndexMarker1325"/>content. For instance, given the sentence, “The white fox jumps over the brown dog,” self-attention might assign similar<a id="_idIndexMarker1326"/> importance to “<code class="inlineCode">fox</code>" and “<code class="inlineCode">dog</code>" simply because they share similar grammatical roles (nouns). To address this limitation, <strong class="keyWord">positional encoding</strong> is introduced to inject positional information into self-attention.</p>
    <p class="normal">The positional encoding is a fixed-size vector that contains positional information of the token in the sequence. It is typically calculated based on mathematical functions. One common approach is to use a combination of sine and cosine functions as follows:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_012.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_013.png"/></p>
    <p class="normal">Here, <em class="italic">i</em> is the dimension index, <em class="italic">pos</em> is the position of the token, and <em class="italic">d</em> is the dimension of the embedding vector. <em class="italic">PE</em>(<em class="italic">pos</em>, 2<em class="italic">i</em>) denotes the 2<em class="italic">i</em><sup class="superscript">th</sup> dimension of the positional encoding for position <em class="italic">pos</em>; <em class="italic">PE</em>(<em class="italic">pos</em>, 2<em class="italic">i</em>+1) represents the 2<em class="italic">i</em>+1<sup class="superscript">th</sup> dimension of the positional encoding for position <em class="italic">pos</em>. <img alt="" role="presentation" src="../Images/B21047_13_014.png"/> introduces different frequencies for different dimensions.</p>
    <p class="normal">Let’s try to encode the positions of the words in a simple sentence, “<code class="inlineCode">Python machine learning</code>" using a four-dimensional <a id="_idIndexMarker1327"/>vector. For the first word, “Python,” we have the following:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_015.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_016.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_017.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_018.png"/></p>
    <p class="normal">For the second word “machine,” we have the following:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_019.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_020.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_021.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_13_022.png"/></p>
    <p class="normal">So, we have positional encoding <code class="inlineCode">[0, 1, 0, 1]</code> for “Python,” and <code class="inlineCode">[0.8, 0.5, 0, 1]</code> for “machine.” We will leave <a id="_idIndexMarker1328"/>the third word as an exercise for you.</p>
    <p class="normal">After the positional encoding vectors are computed for each position, they are then added to the embeddings of the corresponding tokens. As you can see in the Transformer architecture in <em class="italic">Figure 13.1</em>, the positional encoding is added to the input embedding before the input embedding is fed into the encoder component. Similarly, the positional encoding is added to the output embedding before the output embedding is fed into the decoder component. Now, when self-attention learns the relationships between tokens, it considers both the content (token themselves) and their positional information.</p>
    <p class="normal">Positional encoding is an important<a id="_idIndexMarker1329"/> complement to self-attention in Transformers. Since the Transformer doesn’t inherently understand the sequential order of tokens like RNNs do, the additional positional encoding allows it to capture sequential dependencies effectively. In the next section, we will look at another crucial component in Transformers, layer normalization.</p>
    <h2 class="heading-2" id="_idParaDest-317">Layer normalization</h2>
    <p class="normal">A Transformer has many layers (blocks of encoders and decoders that are composed of multi-head self-attention), and it can <a id="_idIndexMarker1330"/>suffer from exploding or vanishing gradients. This makes it difficult for the network to learn effectively during<a id="_idIndexMarker1331"/> training. <strong class="keyWord">Layer normalization</strong> helps address this by normalizing the outputs of each layer.</p>
    <p class="normal">Normalization is applied independently to each layer’s activations, including the self-attention layer and the feedforward network. This means that each layer’s outputs are kept within a specific range to prevent gradients from becoming too large or too small. As a result, layer normalization can stabilize the training process and improve the Transformer’s learning efficiency.</p>
    <p class="normal">We’ve gained a deep understanding of the Transformer architecture and its components, including the encoder, decoder, multi-head self-attention, masked self-attention, positional encoding, and layer normalization. Next, we will learn about models, BERT and GPT, that are based on the Transformer architecture, and will work on their applications, including sentiment analysis and text generation.</p>
    <h1 class="heading-1" id="_idParaDest-318">Improving sentiment analysis with BERT and Transformers</h1>
    <p class="normal"><strong class="keyWord">BERT</strong> (<a href="https://arxiv.org/abs/1810.04805v2"><span class="url">https://arxiv.org/abs/1810.04805v2</span></a>) is a model <a id="_idIndexMarker1332"/>based on the Transformer architecture. It has achieved significant success in<a id="_idIndexMarker1333"/> various language <a id="_idIndexMarker1334"/>understanding tasks in recent years.</p>
    <p class="normal">As its name implies, bidirectional is one <a id="_idIndexMarker1335"/>significant difference between BERT and earlier Transformer models. Traditional models often process sequence in a unidirectional manner, but BERT processes the entire context bidirectionally. This bidirectional context understanding makes the model more effective in capturing nuanced relationships in a sequence.</p>
    <p class="normal">BERT is basically a stack of trained Transformer’s encoders. It is pre-trained on large amounts of unlabeled text data in a self-supervised manner. During pre-training, it focuses on understanding the meaning of text in context. After pre-training, BERT can be fine-tuned for specific downstream tasks.</p>
    <p class="normal">Let’s first talk about the pre-training works.</p>
    <h2 class="heading-2" id="_idParaDest-319">Pre-training BERT</h2>
    <p class="normal">The goal of pre-training BERT is to capture<a id="_idIndexMarker1336"/> rich contextualized representations of <a id="_idIndexMarker1337"/>words. It involves training the model on <strong class="keyWord">a large corpus</strong> of unlabeled text data in a self-supervised manner. The pre-training process consists of two main tasks: the <strong class="keyWord">Masked Language Model</strong> (<strong class="keyWord">MLM</strong>) task and the <strong class="keyWord">Next Sentence Prediction</strong> (<strong class="keyWord">NSP</strong>) task. Here is the breakdown.</p>
    <h3 class="heading-3" id="_idParaDest-320">MLM</h3>
    <p class="normal">In MLM, random words in a<a id="_idIndexMarker1338"/> sentence are replaced with a special token <code class="inlineCode">[MASK]</code>. BERT takes the<a id="_idIndexMarker1339"/> modified sentence as input and tries to predict the original masked word based on the surrounding words. In this fill-in-the-blank game, BERT is trained to understand the meaning and context of words.</p>
    <p class="normal">It is worth noting that during the MLM task, the model is trained using the <strong class="keyWord">bidirectional</strong> context—both the left and right context of each masked word. This improves the masked word prediction accuracy. As a result, by going through a large number of training examples, BERT<a id="_idIndexMarker1340"/> gets better at understanding word meanings and capturing relationships in different <a id="_idIndexMarker1341"/>contexts.</p>
    <h3 class="heading-3" id="_idParaDest-321">NSP</h3>
    <p class="normal">The NSP task helps the model understand relationships between sentences and discourse-level information. During<a id="_idIndexMarker1342"/> training, pairs of sentences are randomly sampled from the<a id="_idIndexMarker1343"/> training corpus. For each pair, there is a 50% chance that the second sentence follows the first in the original text and a 50% chance that it doesn’t. Two sentences concatenated together to form a training sample. Special <code class="inlineCode">[CLS]</code> and <code class="inlineCode">[SEP]</code> tokens are used to format the training sample:</p>
    <ul>
      <li class="bulletList">The <code class="inlineCode">[CLS]</code> (classification) token is added at the beginning of the training sample. The output corresponding to the <code class="inlineCode">[CLS]</code> token is used to represent the entire input sequence for classification tasks.</li>
      <li class="bulletList">The <code class="inlineCode">[SEP]</code> (separator) token separates two concatenated input sentences.</li>
    </ul>
    <p class="normal">BERT’s pre-training process is depicted in the following diagram (note that “C” in the diagram is short for “Class”):</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B21047_13_02.png"/></figure>
    <p class="packt_figref">Figure 13.2: BERT pre-training</p>
    <p class="normal">As you can see in the<a id="_idIndexMarker1344"/> pre-training diagram, the model is trained on concatenated sentences to predict whether the second sentence follows the first one. It receives the <a id="_idIndexMarker1345"/>correct label (is next sentence or not) as feedback and adjusts its parameters to improve its prediction accuracy. Through this sentence matchmaking task, BERT learns to understand how sentences relate to each other and how ideas flow coherently within a text.</p>
    <p class="normal">Interestingly, the diagram shows that BERT’s pre-training combines MLM and NSP tasks simultaneously. The model predicts masked tokens within a sentence while also determining whether a sentence pair follows sequentially.</p>
    <p class="normal">While the MLM task focuses <a id="_idIndexMarker1346"/>on word-level contextualization, the NSP<a id="_idIndexMarker1347"/> task contributes to BERT’s broader understanding of sentence relationships. After pre-training, BERT can be <strong class="keyWord">fine-tuned </strong>for specific downstream NLP tasks. Let’s see how we do this next.</p>
    <h2 class="heading-2" id="_idParaDest-322">Fine-tuning of BERT</h2>
    <p class="normal">BERT is usually fine-tuned for targeted tasks such as sentiment analysis or named entity recognition. Task-specific layers are<a id="_idIndexMarker1348"/> added on top of the pre-trained model, and the model is trained on labeled data relevant to the task. Here’s the step-by-step process of fine-tuning:</p>
    <ol>
      <li class="numberedList" value="1">We first gather labeled data specific to the downstream task.</li>
      <li class="numberedList">Next, we need to select a <a id="_idIndexMarker1349"/>pre-trained BERT model. Various options are available, such as BERT-base and BERT-large. You should use the one suitable for the downstream task and computational capabilities.</li>
      <li class="numberedList">Based on the selected BERT model, we use the corresponding tokenizer to tokenize the input. Various tokenizers are available in the Hugging Face tokenizers package (<a href="https://huggingface.co/docs/tokenizers/python/latest/index.html"><span class="url">https://huggingface.co/docs/tokenizers/python/latest/index.html</span></a>). But you should use the one that matches the model.</li>
      <li class="numberedList">Here comes the fun part – architecture modification. We can add task-specific layers on top of the pre-trained BERT model. For instance, you can add a single neuron with a sigmoid activation for sentiment analysis.</li>
      <li class="numberedList">We then define the task-specific loss function and objective for training. Use the sentiment analysis example again; you can use the binary cross-entropy loss function.</li>
      <li class="numberedList">Finally, we train the modified BERT model on the labeled data.</li>
      <li class="numberedList">We usually perform hyperparameter tuning to find the optimal model configurations including learning rate, batch size, and regularization.</li>
    </ol>
    <p class="normal">Fine-tuning BERT leverages the knowledge acquired during pre-training and adapts it to a specific task. With this transfer learning strategy, BERT doesn’t need to start from scratch, so it learns new things faster and needs less data. In the next section, we’ll utilize BERT to improve<a id="_idIndexMarker1350"/> the sentiment prediction on movie reviews.</p>
    <h2 class="heading-2" id="_idParaDest-323">Fine-tuning a pre-trained BERT model for sentiment analysis</h2>
    <p class="normal">In <em class="chapterRef">Chapter 12</em><em class="italic">, Making Predictions with Sequences Using Recurrent Neural Networks</em>, we developed an LSTM model for movie review<a id="_idIndexMarker1351"/> sentiment prediction. We <a id="_idIndexMarker1352"/>will fine-tune a pre-trained BERT model for the same task in the following steps:</p>
    <ol>
      <li class="numberedList" value="1">First, we read the IMDb review data from the PyTorch built-in datasets:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> torchtext.datasets </span><span class="hljs-con-keyword">import</span><span class="language-python"> IMDB</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dataset = </span><span class="hljs-con-built_in">list</span><span class="language-python">(IMDB(split=</span><span class="hljs-con-string">'</span><span class="hljs-con-string">train'</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_dataset = </span><span class="hljs-con-built_in">list</span><span class="language-python">(IMDB(split=</span><span class="hljs-con-string">'test'</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-built_in">len</span><span class="language-python">(train_dataset), </span><span class="hljs-con-built_in">len</span><span class="language-python">(test_dataset))</span>
25000 25000
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We just load 25,000 training samples and 25,000 test samples. </p>
    <ol>
      <li class="numberedList" value="2">Then, we separate the raw data into text and label data, as we will need to tokenize the text data:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_texts = [train_sample[</span><span class="hljs-con-number">1</span><span class="language-python">] </span><span class="hljs-con-keyword">for</span><span class="language-python"> train_sample </span><span class="hljs-con-keyword">in</span><span class="language-python"> train_dataset]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_labels = [train_sample[</span><span class="hljs-con-number">0</span><span class="language-python">] </span><span class="hljs-con-keyword">for</span><span class="language-python"> train_sample </span><span class="hljs-con-keyword">in</span><span class="language-python"> train_dataset]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_texts = [test_sample[</span><span class="hljs-con-number">1</span><span class="language-python">] </span><span class="hljs-con-keyword">for</span><span class="language-python"> test_sample </span><span class="hljs-con-keyword">in</span><span class="language-python"> test_dataset]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_labels = [test_sample[</span><span class="hljs-con-number">0</span><span class="language-python">] </span><span class="hljs-con-keyword">for</span><span class="language-python"> test_sample </span><span class="hljs-con-keyword">in</span><span class="language-python"> test_dataset]</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We have finished data preparation and move on to the tokenization step.</p>
    <ol>
      <li class="numberedList" value="3">Now, we need to pick a suitable pre-trained model and the corresponding tokenizer. We choose the <code class="inlineCode">distilbert-base-uncased</code> model given that we have limited computational resources. Think of it as a smaller (“distilled”), faster version of BERT. It keeps most of the power of BERT but with fewer parameters. The “uncased” part just means that the model was trained on lowercase text. To make things work smoothly, we’ll use the <code class="inlineCode">distilbert-base-uncased</code> tokenizer that matches the model.</li>
      <li class="numberedList">If you have not installed Hugging Face’s transformers package yet, you can do so in the command line as follows:
        <pre class="programlisting con-one"><code class="hljs-con">pip install transformers==4.32.1
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Or, with the following:</p>
    <pre class="programlisting con-one"><code class="hljs-con">conda install -c huggingface transformers=4.32.1
</code></pre>
    <p class="normal-one">As of the current <a id="_idIndexMarker1353"/>writing, we are using version <code class="inlineCode">4.32.1</code>. Feel free to replicate the process using this <a id="_idIndexMarker1354"/>version, as the transformer package undergoes frequent updates.</p>
    <ol>
      <li class="numberedList" value="5">Pre-trained models from Hugging Face can be loaded and then downloaded and cached locally. We load the <code class="inlineCode">distilbert-base-uncased</code> tokenizer as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> transformers</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> transformers </span><span class="hljs-con-keyword">import</span><span class="language-python"> DistilBertTokenizerFast</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = DistilBertTokenizerFast.from_pretrained(</span><span class="hljs-con-string">'distilbert-base-uncased'</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We can also manually download a <code class="inlineCode">transformers</code> model ahead of time, and read it from the specified local path. To fetch the <code class="inlineCode">distilbert-base-uncased</code> pre-trained model and tokenizer, you can search <code class="inlineCode">distilbert-base-uncased</code> at <a href="https://huggingface.co/models"><span class="url">https://huggingface.co/models</span></a>, and go to the clickable link of <code class="inlineCode">distilbert-base-uncased</code>. Model and tokenizer files can be found in the <strong class="screenText">Files</strong> tab or at <a href="https://huggingface.co/distilbert-base-uncased/tree/main"><span class="url">https://huggingface.co/distilbert-base-uncased/tree/main</span></a> directly. Download all files <strong class="keyWord">except</strong> <code class="inlineCode">flax_model.msgpack</code>, <code class="inlineCode">model.safetensors</code>, <code class="inlineCode">rust_model.ot</code>, and <code class="inlineCode">tf_model.h5</code> (as we only need the PyTorch model), and put them in the folder called <code class="inlineCode">distilbert-base-uncased</code>. Finally, we will be able to load the tokenizer from the <code class="inlineCode">distilbert-base-uncased</code> path as follows:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = DistilBertTokenizerFast.from_pretrained(</span>
<span class="language-python">                                             </span><span class="hljs-con-string">'distilbert-base-uncased'</span><span class="language-python">,</span>
                                             local_files_only=True)
</code></pre>
    <ol>
      <li class="numberedList" value="6">Now, we tokenize the<a id="_idIndexMarker1355"/> input text from the train and test datasets:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_encodings = tokenizer(train_texts, truncation=</span><span class="hljs-con-literal">True</span><span class="language-python">,</span>
<span class="language-python">                                padding=</span><span class="hljs-con-literal">True</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_encodings = tokenizer(test_texts, truncation=</span><span class="hljs-con-literal">True</span><span class="language-python">, padding=</span><span class="hljs-con-literal">True</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Take a look at the encoding result of the first train sample:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_encodings[</span><span class="hljs-con-number">0</span><span class="language-python">]</span>
Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])
</code></pre>
    <p class="normal-one">Here, the resulting <a id="_idIndexMarker1356"/>encoding object can only hold up to 512 tokens. If the original text is longer, it will be truncated. Attributes are a list of information associated with the encoding process, including <code class="inlineCode">ids</code> (representing the token IDs) and <code class="inlineCode">attention_mask</code> (specifying which tokens should be attended to and which should be ignored).</p>
    <ol>
      <li class="numberedList" value="7">Next, we encapsulate all data fields, including the labels within a <code class="inlineCode">Dataset</code> class:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">class</span><span class="language-python"> </span><span class="hljs-con-title">IMDbDataset</span><span class="language-python">(torch.utils.data.Dataset):</span>
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx])
            for key, val in self.encodings.items()}
        item['labels'] = torch.tensor([0., 1.] 
                               if self.labels[idx] == 2
                               else [1., 0.])
        return item
    def __len__(self):
        return len(self.labels)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Note that we transform the positive label (denoted by “<code class="inlineCode">2</code>") and the negative label (denoted by “<code class="inlineCode">1</code>") into the<a id="_idIndexMarker1357"/> formats <code class="inlineCode">[0, 1]</code> and <code class="inlineCode">[1, 0]</code> respectively. We make this adjustment to align with the labeling format required by DistilBERT.</p>
    <ol>
      <li class="numberedList" value="8">We then generate the custom <code class="inlineCode">Dataset</code> objects for the train and test data:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_encoded_dataset = IMDbDataset(train_encodings, train_labels)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_encoded_dataset = IMDbDataset(test_encodings, test_labels)</span>
</code></pre>
      </li>
      <li class="numberedList">Based on the resulting <a id="_idIndexMarker1358"/>datasets, we create batch data loaders and get ready for model fine-tuning and evaluation:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">batch_size = </span><span class="hljs-con-number">32</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">train_dl = torch.utils.data.DataLoader(train_encoded_dataset,</span>
                                           batch_size=batch_size,
                                           shuffle=True)       
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_dl = torch.utils.data.DataLoader(test_encoded_dataset,</span>
                                          batch_size=batch_size,
                                          shuffle=False)
</code></pre>
      </li>
      <li class="numberedList">After completing the data preparation and tokenization, the next step is loading the pre-trained model and fine-tuning it with the dataset we’ve just prepared. The code for loading the pre-trained model is provided here:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> transformers </span><span class="hljs-con-keyword">import</span><span class="language-python"> DistilBertForSequenceClassification</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">device = torch.device(</span><span class="hljs-con-string">"cuda"</span><span class="language-python"> </span><span class="hljs-con-keyword">if</span><span class="language-python"> torch.cuda.is_available() </span><span class="hljs-con-keyword">else</span><span class="language-python"> </span><span class="hljs-con-string">"cpu"</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = DistilBertForSequenceClassification.from_pretrained(</span>
                                               'distilbert-base-uncased',
                                              local_files_only=True)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.to(device)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We load the pre-trained <code class="inlineCode">distilbert-base-uncased</code> model as mentioned earlier. We also ensure the model is placed on the specified computing device (GPU highly <a id="_idIndexMarker1359"/>recommended if available) for training and inference.</p>
    <p class="normal-one">The <code class="inlineCode">transformers</code> package offers a variety of pre-trained models. You can explore them at <a href="https://huggingface.co/docs/transformers/index#supported-models-and-frameworks"><span class="url">https://huggingface.co/docs/transformers/index#supported-models-and-frameworks</span></a>.</p>
    <ol>
      <li class="numberedList" value="11">We set the corresponding <code class="inlineCode">Adam</code> optimizer with a learning rate of <code class="inlineCode">0.00005</code> as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">5e-5</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Now, we define a<a id="_idIndexMarker1360"/> training function responsible for training (fine-tuning) the model for one iteration:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">train</span><span class="language-python">(</span><span class="hljs-con-params">model, dataloader, optimizer</span><span class="language-python">):</span>
    model.train()
    total_loss = 0
    for batch in dataloader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, 
                        labels=labels)
        loss = outputs['loss']
      
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()*len(batch)
    return total_loss/len(dataloader.dataset)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">This is similar to the train function we employed, except BERT needs both token IDs and <code class="inlineCode">attention_mask</code> as inputs.</p>
    <ol>
      <li class="numberedList" value="13">Similarly, we define the evaluation function responsible for evaluating the model accuracy:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">evaluate</span><span class="language-python">(</span><span class="hljs-con-params">model, dataloader</span><span class="language-python">):</span>
    model.eval()
    total_acc = 0
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs['logits']
            pred = torch.argmax(logits, 1)
             total_acc += (pred == torch.argmax(labels, 1)).float().sum().item()
    return  total_acc/len(dataloader.dataset)
</code></pre>
      </li>
      <li class="numberedList">We then train the <a id="_idIndexMarker1361"/>model for one iteration <a id="_idIndexMarker1362"/>and display the train loss and accuracy at the end of it:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">num_epochs = </span><span class="hljs-con-number">1</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(num_epochs):</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">    train_loss = train(model, train_dl, optimizer)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">    train_acc = evaluate(model, train_dl)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Epoch </span><span class="hljs-con-subst">{epoch+</span><span class="hljs-con-number">1</span><span class="hljs-con-subst">}</span><span class="hljs-con-string"> - loss: </span><span class="hljs-con-subst">{train_loss:</span><span class="hljs-con-number">.4</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string"> -</span>
<span class="hljs-con-string">                accuracy: </span>{train_acc:.4f}')
Epoch 1 - loss: 0.0242 - accuracy: 0.9642
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The training process takes a while and the training accuracy is 96%. You may train with more iterations if resources and time allow.</p>
    <ol>
      <li class="numberedList" value="15">Finally, we evaluate the performance on the test set:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">test_acc = evaluate(model, test_dl)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Accuracy on test set: </span><span class="hljs-con-subst">{</span><span class="hljs-con-number">100</span><span class="hljs-con-subst"> * test_acc:</span><span class="hljs-con-number">.2</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string"> %'</span><span class="language-python">)</span>
Accuracy on test set: 92.75 %
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We obtained a test accuracy of 93% with just one epoch in fine-tuning the pre-trained DistilBERT model. This marks a significant enhancement compared to the 86% test accuracy attained with LSTM in <em class="chapterRef">Chapter 12</em>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">If you are dealing with large models or datasets on GPU, it is recommended to monitor GPU memory usage<a id="_idIndexMarker1363"/> to avoid running out of GPU memory.</p>
      <p class="normal">In PyTorch, you can use <code class="inlineCode">torch.cuda.mem_get_info()</code> to check the GPU memory usage. It tells you information about the available and allocated GPU memory. Another trick is <code class="inlineCode">torch.cuda.empty_cache()</code>. It attempts to release all unused cached memory held by the GPU memory allocator back to the system. Finally, if you’re<a id="_idIndexMarker1364"/> done with a model, you can run <code class="inlineCode">del model</code> to free up the memory it was using.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-324">Using the Trainer API to train Transformer models</h2>
    <p class="normal">The Trainer API included in Hugging<a id="_idIndexMarker1365"/> Face is a shortcut for training Transformer-based models. It lets you fine-tune those pre-trained models on <a id="_idIndexMarker1366"/>your own tasks with an easy high-level interface. No more wrestling with tons of training code like we<a id="_idIndexMarker1367"/> did in the previous section.</p>
    <p class="normal">We will fine-tune the BERT model more conveniently using the <code class="inlineCode">Trainer</code> API in the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Load the pre-trained model again and create the corresponding optimizer:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = DistilBertForSequenceClassification.from_pretrained(</span>
                                                'distilbert-base-uncased',
                                                local_files_only=True)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.to(device)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optim = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">5e-5</span><span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">To execute the <code class="inlineCode">Trainer</code> scripts, it is necessary to have the accelerate package installed. Use the following command to install <code class="inlineCode">accelerate</code>:
        <pre class="programlisting con-one"><code class="hljs-con">conda install -c conda-forge accelerate
</code></pre>
      </li>
    </ol>
    <p class="normal-one">or</p>
    <pre class="programlisting con-one"><code class="hljs-con">pip install accelerate
</code></pre>
    <ol>
      <li class="numberedList" value="3">Next, we prepare the<a id="_idIndexMarker1368"/> necessary configurations <a id="_idIndexMarker1369"/>and initialize a <code class="inlineCode">Trainer</code> object for training the model:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> transformers </span><span class="hljs-con-keyword">import</span><span class="language-python"> Trainer, TrainingArguments</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">training_args = TrainingArguments(</span>
        output_dir='./results',
        num_train_epochs=1,   
        per_device_train_batch_size=32,
        logging_dir='./logs',
        logging_steps=50,
    )
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">trainer = Trainer(</span>
    model=model,
    args=training_args,
    train_dataset=train_encoded_dataset,
    optimizers=(optim, None)
)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, the <code class="inlineCode">TrainingArguments</code> configuration defines the number of training epochs, the batch size for training, and the number of steps between each logging of training metrics. We also tell the Trainer what model to use, which data to train on, and<a id="_idIndexMarker1370"/> what optimizer to use – the second element (<code class="inlineCode">None</code>) means there’s no learning rate scheduler this time.</p>
    <ol>
      <li class="numberedList" value="4">You may notice that the <code class="inlineCode">Trainer</code> initialization in the previous step does not involve evaluation metrics and test datasets. Let’s add them and rewrite the initialization:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> datasets </span><span class="hljs-con-keyword">import</span><span class="language-python"> load_metric</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> numpy </span><span class="hljs-con-keyword">as</span><span class="language-python"> np</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(</span><span class="hljs-con-string">"accuracy"</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">compute_metrics</span><span class="language-python">(</span><span class="hljs-con-params">eval_pred</span><span class="language-python">):</span>
        logits, labels = eval_pred
        pred = np.argmax(logits, axis=-1)
        return metric.compute(predictions=pred, 
                              references=np.argmax(labels, 1))
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">trainer = Trainer(</span>
        model=model,
        compute_metrics=compute_metrics,
        args=training_args,
        train_dataset=train_encoded_dataset,
        eval_dataset=test_encoded_dataset,
        optimizers=(optim, None)
    )
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The <code class="inlineCode">compute_metrics</code> function<a id="_idIndexMarker1371"/> calculates the accuracy based on the predicted and true labels. The <code class="inlineCode">Trainer</code> will use this metric to measure how well the model performs<a id="_idIndexMarker1372"/> on the specified test set (<code class="inlineCode">test_encoded_dataset</code>).</p>
    <ol>
      <li class="numberedList" value="5">Now, we train the model with just one line of code:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">trainer.train()</span>
Step     Training Loss
50       0.452500
100      0.321200
150      0.325800
200      0.258700
250      0.244300
300      0.239700
350      0.256700
400      0.234100
450      0.214500
500      0.240600
550      0.209900
600      0.228900
650      0.187800
700      0.194800
750      0.189500
TrainOutput(global_step=782, training_loss=0.25071206544061453, metrics={'train_runtime': 374.6696, 'train_samples_per_second': 66.725, 'train_steps_per_second': 2.087, 'total_flos': 3311684966400000.0, 'train_loss': 0.25071206544061453, 'epoch': 1.0})
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The model was just <a id="_idIndexMarker1373"/>trained for one epoch as we specified, and training loss was displayed for every 50 steps.</p>
    <ol>
      <li class="numberedList" value="6">With another line of<a id="_idIndexMarker1374"/> code, we can evaluate<a id="_idIndexMarker1375"/> the trained model on the test dataset:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(trainer.evaluate())</span>
{'eval_loss': 0.18415148556232452, 'eval_accuracy': 0.929, 'eval_runtime': 122.457, 'eval_samples_per_second': 204.153, 'eval_steps_per_second': 25.519, 'epoch': 1.0}
</code></pre>
      </li>
    </ol>
    <p class="normal">We obtained the same test accuracy of 93% utilizing the Trainer API, and it required significantly less code.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">Here are some best practices for fine-tuning BERT:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Data is king</strong>: You should prioritize high-quality and well-labeled data.</li>
        <li class="bulletList"><strong class="keyWord">Start small</strong>: You can begin with<a id="_idIndexMarker1376"/> smaller pre-trained models like BERT-base or DistilBERT. They’re less demanding on your computational power compared to larger models like BERT-large.</li>
        <li class="bulletList"><strong class="keyWord">Automate hyperparameter tuning</strong>: You may utilize automated hyperparameter tuning libraries (e.g., Hyperopt, Optuna) to search for optimal hyperparameters. This can save you time and let your computer do the heavy lifting.</li>
        <li class="bulletList"><strong class="keyWord">Implement early stopping</strong>: You should monitor validation loss during training. If it stops getting better after a while, hit the brakes. This early stopping strategy can prevent unnecessary training iterations. Remember, fine-tuning BERT can take some time and resources.</li>
      </ul>
    </div>
    <p class="normal">In this section, we discussed BERT, a model based on a Transformer encoder, and leveraged it to enhance<a id="_idIndexMarker1377"/> sentiment analysis. In the following section, we will explore another Transformer-based mode, <strong class="keyWord">GPT</strong>.</p>
    <h1 class="heading-1" id="_idParaDest-325">Generating text using GPT</h1>
    <p class="normal">BERT and GPT are both state-of-the-art <a id="_idIndexMarker1378"/>NLP models based on the Transformer architecture. However, they differ in their architectures, training objectives, and use cases. We will first learn more about GPT and then generate our own version of <em class="italic">War and Peace</em> with a fine-tuned GPT model.</p>
    <h2 class="heading-2" id="_idParaDest-326">Pre-training of GPT and autoregressive generation</h2>
    <p class="normal">GPT (<em class="italic">Improving Language Understanding by Generative Pre-training</em> by Alec Radford et al. 2018) is a <strong class="keyWord">decoder-only</strong> Transformer<a id="_idIndexMarker1379"/> architecture, while BERT is encoder only. This means GPT utilizes masked self-attention in the decoders and emphasizes predicting the next token in a sequence.</p>
    <p class="normal">Think of BERT like a super detective. It <a id="_idIndexMarker1380"/>gets a sentence with some words hidden (masked) and has to guess what they are based on the clues (surrounding words) in both directions, like looking at a crime scene from all angles. GPT, on the other hand, is more like a creative storyteller. It is pre-trained <a id="_idIndexMarker1381"/>using an <strong class="keyWord">autoregressive</strong> language model objective. It starts with a beginning word and keeps adding words one by one, using the previous words as inspiration, similar to how we write a story. This process repeats until the desired sequence length is reached.</p>
    <div class="note">
      <p class="normal">The word “autoregressive” means it generates text one token at a time in a sequential manner.</p>
    </div>
    <p class="normal">Both BERT and GPT are pre-trained on large-scale datasets. However, due to their training methods, they have different strengths and use cases for fine-tuning. BERT is a master at grasping how words and sentences connect. It can be fine-tuned for tasks like sentiment analysis and text classification. On the other hand, GPT is better at creating grammatically correct and smooth-flowing text. This makes it ideal for tasks like text generation, machine translation, and summarization.</p>
    <p class="normal">In the next section, we will write our own version of <em class="italic">War and Peace</em> as we did in <em class="chapterRef">Chapter 12</em><em class="italic">, Making Predictions with Sequences Using Recurrent Neural Networks</em>, but by fine-tuning a GPT model this time.</p>
    <h2 class="heading-2" id="_idParaDest-327">Writing your own version of War and Peace with GPT</h2>
    <p class="normal">For illustrative purposes, we’ll employ <a id="_idIndexMarker1382"/>GPT-2, a model that is more potent than GPT-1 yet smaller in size than GPT-3, and open source, to generate our own version of <em class="italic">War and Peace</em> in the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Before we start, let’s quickly look at how to generate text using the GPT-2 model:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> transformers </span><span class="hljs-con-keyword">import</span><span class="language-python"> pipeline, set_seed</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">generator = pipeline(</span><span class="hljs-con-string">'text-generation'</span><span class="language-python">, model=</span><span class="hljs-con-string">'gpt2'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">set_seed(</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">generator(</span><span class="hljs-con-string">"I love machine learning"</span><span class="language-python">,</span>
              max_length=20,
              num_return_sequences=3)
[{'generated_text': 'I love machine learning, so you should use machine learning as your tool for data production.\n\n'},
{'generated_text': 'I love machine learning. I love learning and I love algorithms. I love learning to control systems.'},
{'generated_text': 'I love machine learning, but it would be pretty difficult for it to keep up with the demands and'}]
</code></pre>
      </li>
    </ol>
    <p class="normal-one">This code snippet uses the GPT-2 model to generate text based on the prompt “<code class="inlineCode">I love machine learning</code>,” and it produces three alternative sequences with a maximum length of 20 tokens each.</p>
    <ol>
      <li class="numberedList" value="2">To generate our own version of <em class="italic">War and Peace</em>, we need to fine-tune the GPT-2 model based on the original <em class="italic">War and Peace</em> text. We first need to load the GPT-2 based tokenizer:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> transformers </span><span class="hljs-con-keyword">import</span><span class="language-python"> TextDataset, GPT2Tokenizer</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = GPT2Tokenizer.from_pretrained(</span><span class="hljs-con-string">'gpt2'</span>, local_files_only=True<span class="language-python">)</span>
</code></pre>
      </li>
      <li class="numberedList">Next, we create a <code class="inlineCode">Dataset</code> object for the tokenized <em class="italic">War and Peace</em> text:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">text_dataset = TextDataset(tokenizer=tokenizer,</span>
<span class="language-python">                               file_path=</span><span class="hljs-con-string">'warpeace_input.txt'</span><span class="language-python">,</span>
                               block_size=128)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We prepare a dataset (<code class="inlineCode">text_dataset</code>) by tokenizing the text from the <code class="inlineCode">'warpeace_input.txt</code>' file (which we used in <em class="chapterRef">Chapter 12</em>). We set the <code class="inlineCode">tokenizer</code> parameter to the previously created GPT-2 tokenizer, and <code class="inlineCode">block_size</code> to <code class="inlineCode">128</code>, specifying the maximum length of each sequence of tokens.</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">len</span><span class="language-python">(text_dataset)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-number">6176</span>
</code></pre>
    <p class="normal-one">We generated <code class="inlineCode">6176</code> training samples based on the original text.</p>
    <ol>
      <li class="numberedList" value="4">Recall in <em class="chapterRef">Chapter 12</em>, we had to create the training data manually. Thankfully, this time <a id="_idIndexMarker1383"/>we utilize the <code class="inlineCode">DataCollatorForLanguageModeling</code> class from Hugging Face to automatically generate the input sequence and output token. This data collator is specifically designed for language modeling tasks, where we’re trying to predict masked tokens. Let’s see how to create a data collator as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> transformers </span><span class="hljs-con-keyword">import</span><span class="language-python"> DataCollatorForLanguageModeling</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,</span>
<span class="language-python">                                                    mlm=</span><span class="hljs-con-literal">False</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The data collator helps organize and batch the input data for training the language model. We set the <code class="inlineCode">mlm</code> parameter to False. If set to <code class="inlineCode">True</code>, it would enable masked language modeling, where tokens are randomly masked for the model to predict as we did in BERT. In our case here, it’s turned off, meaning that the model is trained using an autoregressive approach. This approach is ideal for tasks like text generation, where the model learns to create new text one word at a time.</p>
    <ol>
      <li class="numberedList" value="5">After completing the tokenization and data collation, the next step is loading the pre-trained GPT-2 model and creating the corresponding optimizer:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span> torch
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> transformers </span><span class="hljs-con-keyword">import</span><span class="language-python"> GPT2LMHeadModel</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = GPT2LMHeadModel.from_pretrained(</span><span class="hljs-con-string">'gpt2'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.to(device)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optim = torch.optim.Adam(model.parameters(), lr=</span><span class="hljs-con-number">5e-5</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We use <code class="inlineCode">GPT2LMHeadModel</code> for text generation. It predicts the probability distribution of the next token in a sequence.</p>
    <ol>
      <li class="numberedList" value="6">Next, we prepare the necessary configurations and initialize a <code class="inlineCode">Trainer</code> object for training the model:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> transformers </span><span class="hljs-con-keyword">import</span><span class="language-python"> Trainer, TrainingArguments</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">training_args = TrainingArguments(</span>
        output_dir='./gpt_results',
        num_train_epochs=20,   
        per_device_train_batch_size=16,
        logging_dir='./gpt_logs',
        save_total_limit=1,
        logging_steps=500,
    )
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">trainer = Trainer(</span>
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    optimizers=(optim, None)
)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The model will be trained on the <code class="inlineCode">text_dataset</code> dataset divided into <code class="inlineCode">16</code> sample batches for <code class="inlineCode">20</code> epochs. We also set the limit on the total number of checkpoints to save. In<a id="_idIndexMarker1384"/> this case, only the latest checkpoint will be saved. This is to reduce space consumption. In the <code class="inlineCode">trainer</code>, we provide the <code class="inlineCode">DataCollatorForLanguageModeling</code> instance to organize the data for training.</p>
    <ol>
      <li class="numberedList" value="7">Now, we are ready to train the model:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">trainer.train()</span>
Step    Training Loss
500        3.414100
1000       3.149500
1500       3.007500
2000       2.882600
2500       2.779100
3000       2.699200
3500       2.621700
4000       2.548800
4500       2.495400
5000       2.447600
5500       2.401400
6000       2.367600
6500       2.335500
7000       2.315100
7500       2.300400
TrainOutput(global_step=7720, training_loss=2.640813370936893, metrics={'train_runtime': 1408.7655, 'train_samples_per_second': 87.68, 'train_steps_per_second': 5.48, 'total_flos': 8068697948160000.0, 'train_loss': 2.640813370936893, 'epoch': 20.0})
</code></pre>
      </li>
      <li class="numberedList">After the GPT-2 model is trained based on <em class="italic">War and Peace</em>, we finally use it to generate our own<a id="_idIndexMarker1385"/> version. We first develop the following function to generate text based on a given model and prompt text:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">generate_text</span><span class="language-python">(</span><span class="hljs-con-params">prompt_text, model, tokenizer, max_length</span><span class="language-python">):</span>
        input_ids = tokenizer.encode(prompt_text, 
                                     return_tensors="pt").to(device)
  
        # Generate response
        output_sequences = model.generate(
            input_ids=input_ids,
            max_length=max_length,
            num_return_sequences=1,
            no_repeat_ngram_size=2,
            top_p=0.9,
        )
        # Decode the generated responses
        responses = []
        for response_id in output_sequences:
            response = tokenizer.decode(response_id,
                                        skip_special_okens=True)
            responses.append(response)
        return responses
</code></pre>
      </li>
    </ol>
    <p class="normal-one">In the generation process, we first tokenize the input prompt text using the tokenizer and convert the tokenized sequence to a PyTorch tensor. Then, we generate text based on the tokenized input using the given model. Here, <code class="inlineCode">no_repeat_ngram_size</code> prevents repeating the same n-gram phrases to keep things fresh. Another <a id="_idIndexMarker1386"/>interesting setting, <code class="inlineCode">top_p</code>, controls the diversity of the generated text. It considers the tokens that are most probable instead of only the most probable one. Finally, we decode the generated response using the tokenizer, translating it back to human language.</p>
    <ol>
      <li class="numberedList" value="9">We use the same prompt, “<code class="inlineCode">the emperor</code>,” and here is our version of <em class="italic">War and Peace</em> in 100 words:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">prompt_text = </span><span class="hljs-con-string">"the emperor"</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">responses = generate_text(prompt_text, model, tokenizer, </span><span class="hljs-con-number">100</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> response </span><span class="hljs-con-keyword">in</span><span class="language-python"> responses:</span>
        print(response)
the emperor's, and the Emperor Francis, who was in attendance on him, was present.
The Emperor was present because he had received the news that the French troops were advancing on Moscow, that Kutuzov had been wounded, the Emperor's wife had died, a letter from Prince Andrew had come from Prince Vasili, Prince Bolkonski had seen at the palace, news of the death of the Emperor, but the most important news was that …
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We’ve successfully generated our own version of <em class="italic">War and Peace</em> using the fine-tuned GPT-2 model. It reads better than the LSTM version in <em class="chapterRef">Chapter 12</em>.</p>
    <p class="normal">GPT is a decoder-only Transformer architecture. It’s all about making things up on the fly, one token at a time. Unlike some other Transformer-based models, it doesn’t need a separate step (encoding) to understand what you feed it. This lets it focus on generating text that flows naturally, like a creative writer. Once GPT is trained on a massive pile of text, we can fine-tune it for specific tasks with smaller datasets. Think of it like teaching a master storyteller to write about a specific topic, like history or science fiction. In our example, we generated our own version of <em class="italic">War and Peace</em> by fine-tuning a GPT-2 model.</p>
    <div class="note">
      <p class="normal">While BERT focuses on bidirectional pre-training, GPT is autoregressive and predicts the next word in a sequence. You <a id="_idIndexMarker1387"/>may wonder whether there is a model that combines <a id="_idIndexMarker1388"/>aspects of both bidirectional understanding and auto-regressive generation. The answer is yes – <strong class="keyWord">Bidirectional and Auto-Regressive Transformers</strong> (<strong class="keyWord">BART</strong>). It was introduced by Facebook AI (<em class="italic">BART: Bidirectional and Autoregressive Transformers for Sequence-to-Sequence Learning</em> by Lewis et al., 2019), and designed to combine both strengths.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-328">Summary</h1>
    <p class="normal">This chapter was all about Transformer, a powerful neural network architecture designed for sequence-to-sequence tasks. Its key ingredient, self-attention, lets the model focus on the most important parts of the information it’s looking at in a sequence.</p>
    <p class="normal">We worked on two NLP projects: sentiment analysis and text generation using two state-of-the-art Transformer models, BERT and GPT. We observed an elevated performance compared to what we did in the last chapter. We also learned how to fine-tune these Transformers with the Hugging Face library, a one-stop shop for loading pre-trained models, performing different NLP tasks, and fine-tuning models on your own data. Plus, it throws in some bonus tools for chopping up text, checking how well the model did, and even generating some text of its own.</p>
    <p class="normal">In the next chapter, we will focus on another OpenAI cutting-edge model, CLIP, and will implement natural language-based image search.</p>
    <h1 class="heading-1" id="_idParaDest-329">Exercises</h1>
    <ol>
      <li class="numberedList" value="1">Can you compute the positional encoding for the third word “<code class="inlineCode">learning</code>" in the example sentence “<code class="inlineCode">python machine learning</code>" using a four-dimensional vector?</li>
      <li class="numberedList">Can you fine-tune a BERT model for topic classification? You can take the newsgroups dataset as an example.</li>
      <li class="numberedList">Can you fine-tune a BART model (<a href="https://huggingface.co/facebook/bart-base"><span class="url">https://huggingface.co/facebook/bart-base</span></a>) to write your own version of <em class="italic">War and Peace</em>?</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-330">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/yuxi"><span class="url">https://packt.link/yuxi</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code187846872178698968.png"/></p>
  </div>
</body></html>
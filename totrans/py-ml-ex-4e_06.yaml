- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting Stock Prices with Artificial Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuing the same project of stock price prediction from the last chapter,
    in this chapter, we will introduce and explain neural network models in depth.
    We will start by building the simplest neural network and go deeper by adding
    more computational units to it. We will cover neural network building blocks and
    other important concepts, including activation functions, feedforward, and backpropagation.
    We will also implement neural networks from scratch with scikit-learn, TensorFlow,
    and PyTorch. We will pay attention to how to learn with neural networks efficiently
    without overfitting, utilizing dropout and early stopping techniques. Finally,
    we will train a neural network to predict stock prices and see whether it can
    beat what we achieved with the three regression algorithms in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking the right activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventing overfitting in neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting stock prices with neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demystifying neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here comes probably the most frequently mentioned model in the media, **Artificial
    Neural Networks** (**ANNs**); more often, we just call them **neural networks**.
    Interestingly, the neural network has been (falsely) considered equivalent to
    machine learning or artificial intelligence by the general public.
  prefs: []
  type: TYPE_NORMAL
- en: An ANN is just one type of algorithm among many in machine learning, and machine
    learning is a branch of artificial intelligence. It is one of the ways we achieve
    **Artificial General Intelligence** (**AGI**), which is a hypothetical type of
    AI that can think, learn, and solve problems like a human.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, it is one of the most important machine learning models and has
    been rapidly evolving along with the revolution of **Deep Learning** (**DL**).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first understand how neural networks work.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with a single-layer neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by explaining different layers in a network, then move on to the activation
    function, and finally, training a network with backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Layers in neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A simple neural network is composed of three layers—the **input layer**, **hidden
    layer**, and **output layer— a**s shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a network  Description automatically generated with medium confidence](img/B21047_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: A simple shallow neural network'
  prefs: []
  type: TYPE_NORMAL
- en: A **layer** is a conceptual collection of **nodes** (also called **units**),
    which simulate neurons in a biological brain. The input layer represents the input
    features, **x**, and each node is a predictive feature, *x*. The output layer
    represents the target variable(s).
  prefs: []
  type: TYPE_NORMAL
- en: In binary classification, the output layer contains only one node, whose value
    is the probability of the positive class. In multiclass classification, the output
    layer consists of *n* nodes, where *n* is the number of possible classes and the
    value of each node is the probability of predicting that class. In regression,
    the output layer contains only one node, the value of which is the prediction
    result.
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer can be considered a composition of latent information extracted
    from the previous layer. There can be more than one hidden layer. Learning with
    a neural network with two or more hidden layers is called **deep learning**. In
    this chapter, we will focus on one hidden layer to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: Two adjacent layers are connected by conceptual edges (sort of like the synapses
    in a biological brain), which transmit signals from one neuron in a layer to another
    neuron in the next layer. The **edges** are parameterized by the weights, *W*,
    of the model. For example, *W*^((1)) in the preceding diagram connects the input
    and hidden layers and *W*^((2)) connects the hidden and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: In a standard neural network, data is conveyed only from the input layer to
    the output layer, through a hidden layer(s). Hence, this kind of network is called
    a **feedforward** neural network. Basically, logistic regression is a feedforward
    neural network with no hidden layer where the output layer connects directly with
    the input layer. Adding hidden layers between the input and output layers introduces
    non-linearity. This allows the neural networks to learn more about the underlying
    relationship between the input data and the target.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An **activation function** is a mathematical operation applied to the output
    of each neuron in a neural network. It determines whether the neuron should be
    activated (i.e., its output value should be propagated forward to the next layer)
    based on the input it receives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the input, *x*, is of *n* dimensions, and the hidden layer is composed
    of *H* hidden units. The weight matrix, *W*^((1)), connecting the input and hidden
    layers is of size *n* by *H*, where each column, ![](img/B21047_06_001.png), represents
    the coefficients associating the input with the *h*-th hidden unit. The output
    (also called **activation**) of the hidden layer can be expressed mathematically
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_06_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *f(z)* is an activation function. As its name implies, the activation
    function checks how activated each neuron is, simulating the way our brains work.
    Their primary purpose is to introduce non-linearity into the output of a neuron,
    allowing the network to learn and perform complex mappings between inputs and
    outputs. Typical activation functions include the logistic function (more often
    called the **sigmoid** function in neural networks) and the **tanh** function,
    which is considered a rescaled version of the logistic function, as well as **ReLU**
    (short for **Rectified Linear Unit**), which is often used in DL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_06_003.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_06_004.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_06_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We plot these three activation functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **logistic** (**sigmoid**) function where the output value is in the range
    of (`0, 1`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A graph with a line  Description automatically generated](img/B21047_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: The logistic function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization is produced by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The **tanh** function plot where the output value is in the range of `(-1,
    1)`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A graph with a line  Description automatically generated](img/B21047_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: The tanh function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization is produced by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The **ReLU** function plot where the output value is in the range of `(0, +inf)`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A graph with a line  Description automatically generated](img/B21047_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: The ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization is produced by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the output layer, let’s assume that there is one output unit (regression
    or binary classification) and that the weight matrix, *W*^((2)), connecting the
    hidden layer to the output layer is of size *H* by *1*. In regression, the output
    can be expressed mathematically as follows (for consistency, I here denote it
    as *a*^((3)) instead of *y*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_06_006.png)'
  prefs: []
  type: TYPE_IMG
- en: The **Universal Approximation Theorem** is a key concept in understanding how
    neural networks enable learning. It states that a feedforward neural network with
    a single hidden layer containing a finite number of neurons can approximate any
    continuous function to arbitrary precision, given a sufficiently large number
    of neurons in the hidden layer. During the training process, the neural network
    learns to approximate the target function by adjusting its parameters (weights).
    This is typically done using optimization algorithms, such as gradient descent,
    which iteratively update the parameters to minimize the difference between the
    predicted outputs and the true targets. Let’s see this process in detail in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, how can we obtain the optimal weights, *W = {W(1), W(2)}*, of the model?
    Similar to logistic regression, we can learn all weights using gradient descent
    with the goal of minimizing the **mean squared error** (**MSE**) cost or other
    loss function, *J(W)*. The difference is that the gradients, ![](img/B21047_06_007.png),
    are computed through **backpropagation**. After each forward pass through a network,
    a backward pass is performed to adjust the model’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the word *back* in the name implies, the computation of the gradient proceeds
    backward: the gradient of the final layer is computed first and the gradient of
    the first layer is computed last. As for *propagation*, it means that partial
    computations of the gradient on one layer are reused in the computation of the
    gradient on the previous layer. Error information is propagated layer by layer,
    instead of being calculated separately.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a single-layer network, the detailed steps of backpropagation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We travel through the network from the input to the output and compute the output
    values, *a*^((2)), of the hidden layer as well as the output layer, *a*^((3)).
    This is the feedforward step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the last layer, we calculate the derivative of the cost function with regard
    to the input to the output layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_06_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the hidden layer, we compute the derivative of the cost function with regard
    to the input to the hidden layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_06_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We compute the gradients by applying the **chain rule**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_06_010.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_06_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We update the weights with the computed gradients and learning rate ![](img/B21047_06_012.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21047_06_013.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B21047_06_014.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *m* is the number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: We repeatedly update all the weights by taking these steps with the latest weights
    until the cost function converges or the model goes through enough iterations.
  prefs: []
  type: TYPE_NORMAL
- en: The chain rule is a fundamental concept in calculus. It allows you to find the
    derivative of a composite function. You can read more in the mathematics course
    from Stanford University ([https://mathematics.stanford.edu/events/chain-rule-calculus](https://mathematics.stanford.edu/events/chain-rule-calculus)),
    or the differential calculus course, *Module 6, Applications of Differentiation*,
    from MIT ([https://ocw.mit.edu/courses/18-03sc-differential-equations-fall-2011/](https://ocw.mit.edu/courses/18-03sc-differential-equations-fall-2011/)).
  prefs: []
  type: TYPE_NORMAL
- en: This might not be easy to digest at first glance, so right after the next section,
    we will implement it from scratch, which will help you understand neural networks
    better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding more layers to a neural network: DL'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In real applications, a neural network usually comes with multiple hidden layers.
    That is how DL got its name—learning using neural networks with “stacked” hidden
    layers. An example of a DL model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: A deep neural network'
  prefs: []
  type: TYPE_NORMAL
- en: In a stack of multiple hidden layers, the input of one hidden layer is the output
    of its previous layer, as you can see from *Figure 6.5*. Features (signals) are
    extracted from each hidden layer. Features from different layers represent patterns
    from different levels. Going beyond shallow neural networks (usually with only
    one hidden layer), a DL model (usually with two or more hidden layers) with the
    right network architectures and parameters can better learn complex non-linear
    relationships from data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see some typical applications of DL so that you will be more motivated
    to get started with upcoming DL projects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computer vision** is widely considered the area with massive breakthroughs
    in DL. You will learn more about this in *Chapter 11*, *Categorizing Images of
    Clothing with Convolutional Neural Networks*, and *Chapter 14*, *Building an Image
    Search Engine Using CLIP: A Multimodal Approach*. For now, here is a list of common
    applications in computer vision:'
  prefs: []
  type: TYPE_NORMAL
- en: Image recognition, such as face recognition and handwritten digit recognition.
    Handwritten digit recognition, along with the common evaluation dataset MNIST,
    has become a “Hello, World!” project in DL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image-based search engines heavily utilize DL techniques in their image classification
    and image similarity encoding components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine vision, which is a critical part of autonomous vehicles, perceives camera
    views to make real-time decisions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Color restoration from black and white photos and art transfer that ingeniously
    blends two images of different styles. The artificial masterpieces in Google Arts
    & Culture ([https://artsandculture.google.com/](https://artsandculture.google.com/))
    are impressive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realistic image generation based on textual descriptions. This has applications
    in creating visual storytelling content and assisting in content creation for
    marketing and advertising.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**) is another field where you can see
    the dominant use of DL in its modern solutions. You will learn more about this
    in *Chapter 12*, *Making Predictions with Sequences Using Recurrent Neural Networks,*
    and*Chapter 13*, *Advancing Language Understanding and Generation with the Transformer
    Models*. But let’s quickly look at some examples now:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation, where DL has dramatically improved accuracy and fluency,
    for example, the sentence-based **Google Neural Machine Translation** (**GNMT**)
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text generation reproduces text by learning the intricate relationships between
    words in sentences and paragraphs with deep neural networks. You can become a
    virtual J. K. Rowling or Shakespeare if you train a model well on their works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image captioning, also known as image-to-text, leverages deep neural networks
    to detect and recognize objects in images and “describe” those objects in a comprehensible
    sentence. It couples recent breakthroughs in computer vision and NLP. Examples
    can be found at [https://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/](https://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/)
    (developed by Andrej Karpathy from Stanford University).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other common NLP tasks such as sentiment analysis and information retrieval
    and extraction, DL models have achieved state-of-the-art performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial Intelligence-Generated Content** (**AIGC**) is one of the recent
    breakthroughs. It uses DL technologies to create or assist in creating various
    types of content, such as articles, product descriptions, music, images, and videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to shallow networks, we learn all the weights in a deep neural network
    using gradient descent with the goal of minimizing the MSE cost, *J(W)*. And gradients,
    ![](img/B21047_06_007.png), are computed through backpropagation. The difference
    is that we backpropagate more than one hidden layer. In the next section, we will
    implement neural networks by starting with shallow networks and then moving on
    to deep ones.
  prefs: []
  type: TYPE_NORMAL
- en: Building neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This practical section will start with implementing a shallow network from scratch,
    followed by a deep network with two layers using scikit-learn. We will then implement
    a deep network with TensorFlow and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural networks from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate how activation functions work, we will use sigmoid as the activation
    function in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define the `sigmoid` function and its derivative function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can derive the derivative yourself if you are interested in verifying it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define the training function, which takes in the training dataset,
    the number of units in the hidden layer (we will only use one hidden layer as
    an example), and the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that besides weights, *W*, we also employ bias, *b*. Before training, we
    first randomly initialize weights and biases. In each iteration, we feed all layers
    of the network with the latest weights and biases, then calculate the gradients
    using the backpropagation algorithm, and finally, update the weights and biases
    with the resulting gradients. For training performance inspection, we print out
    the loss and the MSE for every 100 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the model, we will use California house prices as the example dataset
    again. As a reminder, data normalization is usually recommended whenever gradient
    descent is used. Hence, we will standardize the input data by removing the mean
    and scaling to unit variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With the scaled dataset, we can now train a one-layer neural network with `20`
    hidden units, a `0.1` learning rate, and `2000` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define a prediction function, which will take in a model and produce
    the regression results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we apply the trained model on the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Print out the predictions and their ground truths to compare them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: After successfully building a neural network model from scratch, we will move
    on to the implementation with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural networks with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will utilize the `MLPRegressor` class (**MLP** stands for **multi-layer
    perceptron**, a nickname for neural networks) to implement neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `hidden_layer_sizes` hyperparameter represents the number of hidden neurons.
    In this example, the network contains two hidden layers with `16` and `8` nodes,
    respectively. ReLU activation is used.
  prefs: []
  type: TYPE_NORMAL
- en: The Adam optimizer is a replacement for the stochastic gradient descent algorithm.
    It updates the gradients adaptively based on training data. For more information
    about Adam, check out the paper at [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).
  prefs: []
  type: TYPE_NORMAL
- en: 'We fit the neural network model on the training set and predict on the testing
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And we calculate the MSE on the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We’ve implemented a neural network with scikit-learn. Let’s do so with TensorFlow
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural networks with TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In TensorFlow 2.x, it is simple to initiate a deep neural network model using
    the Keras ([https://keras.io/](https://keras.io/)) module. Let’s implement neural
    networks with TensorFlow by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary modules and set a random seed, which is recommended
    for reproducible modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a Keras Sequential model by passing a list of layer instances
    to the constructor, including two fully connected hidden layers with `16` nodes
    and `8` nodes, respectively. And again, ReLU activation is used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We compile the model by using Adam as the optimizer with a learning rate of
    `0.01` and MSE as the learning goal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After defining the model, we now train it against the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We fit the model with `300` iterations. In each iteration, the training loss
    (MSE) is displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we use the trained model to predict the testing cases and print out
    the predictions and their MSE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, we add layer by layer to the neural network model in the TensorFlow
    Keras API. We start from the first hidden layer (with 16 nodes), then the second
    hidden layer (with 8 nodes), and finally, the output layer (with 1 unit, the target
    variable). It is quite similar to building with LEGO.
  prefs: []
  type: TYPE_NORMAL
- en: In the industry, neural networks are often implemented with PyTorch. Let’s see
    how to do it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural networks with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now implement neural networks with PyTorch by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary modules and set a random seed, which is recommended
    for reproducible modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a `torch.nn` Sequential model by passing a list of layer instances
    to the constructor, including two fully connected hidden layers with `16` nodes
    and `8` nodes, respectively. Again, ReLU activation is used in each fully connected
    layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize an Adam optimizer with a learning rate of `0.01` and MSE as the
    learning goal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After defining the model, we need to create tensor objects from the input NumPy
    arrays before using them to train the PyTorch model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can train the model against the PyTorch-compatible training set. We
    first define a training function that will be called in each epoch as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We fit the model with `500` iterations. In every 100 iterations, the training
    loss (MSE) is displayed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we use the trained model to predict the testing cases and print out
    the predictions and their MSE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It turns out that developing a neural network model with PyTorch is as simple
    as building with LEGO too.
  prefs: []
  type: TYPE_NORMAL
- en: Both PyTorch and TensorFlow are popular deep learning frameworks, and their
    popularity can vary depending on different factors such as application domains,
    research communities, industry adoption, and personal preferences. However, as
    of 2023, PyTorch has been more widely adopted and has a larger user base overall,
    according to Papers With Code ([https://paperswithcode.com/trends](https://paperswithcode.com/trends))
    and Google Trends ([https://trends.google.com/trends/explore?geo=US&q=tensorflow,pytorch&hl=en](https://trends.google.com/trends/explore?geo=US&q=tensorflow,pytorch&hl=en)).
    Hence, we will focus on PyTorch implementations for DL throughout the rest of
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at how to choose the right activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have used the ReLU and sigmoid activation functions in our implementations.
    You may wonder how to pick the right activation function for your neural networks.
    Detailed advice on when to choose a particular activation function is given next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear**: *f(z) = z*. You can interpret this as no activation function. We
    usually use it in the output layer in regression networks as we don’t need any
    transformation to the outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sigmoid** (logistic) transforms the output of a layer to a range between
    0 and 1\. You can interpret it as the probability of an output prediction. Therefore,
    we usually use it in the output layer in **binary classification** networks. Besides
    that, we sometimes use it in hidden layers. However, it should be noted that the
    sigmoid function is monotonic but its derivative is not. Hence, the neural network
    may get stuck at a suboptimal solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Softmax**: As was mentioned in *Chapter 4*, *Predicting Online Ad Click-Through
    with Logistic Regression*, softmax is a generalized logistic function used for
    multiclass classification. Hence, we use it in the output layer in **multiclass
    classification** networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tanh** is a better version of the sigmoid function with stronger gradients.
    As you can see in the plots earlier in the chapter, the derivatives in the tanh
    function are steeper than those for the sigmoid function. It has a range of `-1`
    to `1`. It is common to use the `tanh` function in hidden layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReLU** is probably the most frequently used activation function nowadays.
    It is the “default” one in hidden layers in feedforward networks. Its range is
    from `0` to infinity, and both the function itself and its derivative are monotonic.
    It has several benefits over `tanh`. One is sparsity, meaning that only a subset
    of neurons are activated at any given time. This can help reduce the computational
    cost of training and inference, as fewer neurons need to be computed. ReLU also
    mitigates the vanishing gradient problem, which occurs when gradients become very
    small during backpropagation, leading to slow or stalled learning. ReLU does not
    saturate for positive inputs, allowing gradients to flow more freely during training.
    One drawback of the ReLU function is the inability to appropriately map the negative
    part of the input where all negative inputs are transformed to 0\. To fix the
    “dying negative” problem in ReLU, **Leaky ReLU** was invented to introduce a small
    slope in the negative part. When *z < 0*, *f(z) = az*, where *a* is usually a
    small value, such as `0.01`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To recap, ReLU is usually in hidden layer activation. You can try Leaky ReLU
    if ReLU doesn’t work well. Sigmoid and tanh can be used in hidden layers but are
    not recommended in deep networks with many layers. For the output layer, linear
    activation (or no activation) is used in the regression network; sigmoid is for
    the binary classification network and softmax is for the multiple classification
    case.
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right activation is important, and so is avoiding overfitting in
    neural networks. Let’s see how to do this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing overfitting in neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network is powerful as it can derive hierarchical features from data
    with the right architecture (the right number of hidden layers and hidden nodes).
    It offers a great deal of flexibility and can fit a complex dataset. However,
    this advantage will become a weakness if the network is not given enough control
    over the learning process. Specifically, it may lead to overfitting if a network
    is only good at fitting to the training set but is not able to generalize to unseen
    data. Hence, preventing overfitting is essential to the success of a neural network
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are mainly three ways to impose restrictions on our neural networks:
    L1/L2 regularization, dropout, and early stopping. We practiced the first method
    in *Chapter 4*, *Predicting Online Ad Click-Through with Logistic Regression*,
    and will discuss the other two in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Dropout** means ignoring a certain set of hidden nodes during the learning
    phase of a neural network. And those hidden nodes are chosen randomly given a
    specified probability. In the forward pass during a training iteration, the randomly
    selected nodes are temporarily not used in calculating the loss; in the backward
    pass, the randomly selected nodes are not updated temporarily.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we choose three nodes in the network to ignore during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a network  Description automatically generated with medium confidence](img/B21047_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Three nodes to ignore in a neural network'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a regular layer has nodes fully connected to nodes from the previous
    layer and the following layer. It will lead to overfitting if a large network
    develops and memorizes the co-dependency between individual pairs of nodes. Dropout
    breaks this co-dependency by temporarily deactivating certain nodes in each iteration.
    Therefore, it effectively reduces overfitting and won’t disrupt learning at the
    same time.
  prefs: []
  type: TYPE_NORMAL
- en: The fraction of nodes being randomly chosen in each iteration is also called
    the dropout rate. In practice, we usually set a dropout rate no greater than 50%.
    If the dropout rate is too high, it can excessively hinder the model’s learning
    capacity, slowing down training and reducing the model’s ability to extract useful
    patterns from the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining the dropout rate empirically involves experimenting with different
    dropout rates and evaluating their effects on the model’s performance. Here’s
    a typical approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a low rate (e.g., `0.1` or `0.2`) and train the model on your dataset.
    Monitor the model’s performance metrics on a validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradually increase the dropout rate in small increments (e.g., by `0.1`) and
    retrain the model each time. Monitor the performance metrics after each training
    run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate performance obtained with different dropout rates. Be mindful of overfitting,
    as too high of a dropout rate can hinder model performance; if the dropout rate
    is too low, the model may not effectively prevent overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In PyTorch, we use the `torch.nn.Dropout` object to add dropout to a layer.
    An example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, 10% of nodes randomly picked from the first hidden
    layer are ignored in an iteration during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that dropout should only occur in the training phase. In the prediction
    phase, all nodes should be fully connected again. Hence, we have to switch the
    model to evaluation mode with the `.eval()` method to disable dropout before we
    evaluate the model or make predictions with the trained model. Let’s see it in
    the following California housing example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compile the model (with dropout) by using Adam as the optimizer with
    a learning rate of `0.01` and MSE as the learning goal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we can train the model (with dropout) for `1,000` iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In every 100 iterations, the training loss (MSE) is displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we use the trained model (with dropout) to predict the testing cases
    and print out the MSE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As mentioned earlier, don’t forget to run `model_with_dropout.eval()` before
    evaluating the model with dropout. Otherwise, the dropout layers will continue
    to randomly deactivate neurons, leading to inconsistent results between different
    model evaluations on the same data.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name implies, training a network with **early stopping** will end if
    the model performance doesn’t improve for a certain number of iterations. The
    model performance is measured on a validation set that is different from the training
    set, in order to assess how well it generalizes. During training, if the performance
    degrades after several (let’s say 50) iterations, it means the model is overfitting
    and not able to generalize well anymore. Hence, stopping the learning early in
    this case helps prevent overfitting. Usually, we evaluate the model against a
    validation set. If the metric on the validation set is not improving for more
    than *n* epochs, we stop the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will demonstrate how to apply early stopping in PyTorch using the California
    housing example as well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we recreate the model and optimizer as we did previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the early stopping criterion as the test loss doesn’t improve
    for more than `100` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we adopt early stopping and train the model for, at most, `500` iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Following every training step, we compute the test loss and compare it to the
    previously recorded best one. If it shows improvement, we save the current model
    using the `copy` module and reset the `epochs_no_improve` counter. However, if
    there is no improvement in the test loss for up to 100 consecutive iterations,
    we stop the training process as we have reached the tolerance threshold (`patience`).
    In our example, training stopped after epoch `224`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we use the previously recorded best model to predict the testing cases
    and print out the predictions and their MSE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is better than `0.0069`, which we obtained in the vanilla approach, and
    `0.0057`, which we achieved using dropout for overfitting prevention.
  prefs: []
  type: TYPE_NORMAL
- en: While the Universal Approximation Theorem guarantees that neural networks can
    represent any function, it doesn’t guarantee good generalization performance.
    Overfitting can occur if the model has too much capacity relative to the complexity
    of the underlying data distribution. Therefore, controlling the capacity of the
    model through techniques like regularization and early stopping is essential to
    ensure that the learned function generalizes well to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned about neural networks and their implementation, let’s
    utilize them to solve our stock price prediction problem.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting stock prices with neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will build the stock predictor with PyTorch in this section. We will start
    with feature generation and data preparation, followed by network building and
    training. After that, we will fine-tune the network to boost the stock predictor.
  prefs: []
  type: TYPE_NORMAL
- en: Training a simple neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We prepare data and train a simple neural work with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We load the stock data, generate features, and label the `generate_features`
    function we developed in *Chapter 5*, *Predicting Stock Prices with Regression
    Algorithms*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We construct the training set using data from 1990 to 2022 and the testing
    set using data from the first half of 2023:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to normalize features into the same or a comparable scale. We do so
    by removing the mean and rescaling to unit variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We rescale both sets with the scaler taught by the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create tensor objects from the input NumPy arrays before using
    them to train the PyTorch model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now build a neural network using the `torch.nn` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The network we begin with has one hidden layer with `32` nodes followed by a
    ReLU function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compile the model by using Adam as the optimizer with a learning rate of
    `0.3` and MSE as the learning goal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After defining the model, we perform training for `1,000` iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we use the trained model to predict the testing data and display metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We achieve an *R*² of `0.954` with a simple neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning the neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Can we do better? Of course, we haven’t fine-tuned the hyperparameters yet.
    We perform model fine-tuning in PyTorch with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorBoard provides functionality for logging various metrics and visualizations
    during model training and evaluation. You can use TensorBoard with PyTorch to
    track and visualize metrics such as loss, accuracy, gradients, and model architectures,
    among others. We rely on the `tensorboard` module in PyTorch `utils`, so we import
    it first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We want to tweak the number of hidden nodes in the hidden layer (again, we
    are using one hidden layer for this example), the number of training iterations,
    and the learning rate. We pick the following values of hyperparameters to experiment
    on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we experiment with two options for the number of hidden nodes, `16` and
    `32`; we use two options for the number of iterations, `300` and `1000`; and we
    use two options, `0.1` and `0.3`, for the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'After initializing the hyperparameters to optimize, we will iterate each hyperparameter
    combination, and train and validate the model using a given set of hyperparameters
    by calling the helper function `train_validate_model` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, in each hyperparameter combination, we build and fit a neural network
    model based on the given hyperparameters, including the number of hidden nodes,
    the learning rate, and the number of training iterations. There’s nothing much
    different here from what we did before. But when we train the model, we also update
    TensorBoard by logging the hyperparameters and metrics including the train loss
    and test loss with the `add_scalar` method.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorBoard writer object is straightforward. It provides visualization
    for the model graph and metrics during training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: At the end, we compute and display the *R*² of the prediction on the test set.
    We also log the test MSE and *R*² using the `add_hparams` method along with the
    given hyperparameter combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we fine-tune the neural network by iterating eight hyperparameter combinations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Experiment 4 with the combination of `(hidden_size=16, epochs=3000, learning_rate=0.3)`
    is the best performing one, where we achieve an *R*² of `0.977`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperparameter tuning for neural networks can significantly impact model performance.
    Here are some best practices for hyperparameter tuning in neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define a search space**: Determine which hyperparameters to tune and their
    ranges. Common hyperparameters include learning rate, batch size, number of hidden
    layers, number of neurons per layer, activation functions, dropout rates, and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use cross-validation**: This helps prevent overfitting and provides a more
    robust estimate of model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor performance metrics**: Track relevant metrics such as loss, accuracy,
    precision, recall, MSE, *R*², and so on during training and validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: Monitor the validation loss during training, and stop training
    when it starts to increase consistently while the training loss decreases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: Use regularization techniques such as L1 and L2 regularization
    and dropout to prevent overfitting and improve generalization performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiment with different architectures**: Try different network architectures,
    including the number of layers, the number of neurons per layer, and the activation
    functions. Experiment with deep vs. shallow networks and wide vs. narrow networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use parallelism**: If computational resources allow, parallelize the hyperparameter
    search process to speed up experimentation. Tools like TensorFlow’s `tf.distribute.Strategy`
    or PyTorch’s `torch.nn.DataParallel` can be used to distribute training across
    multiple GPUs or machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will notice that a new folder, `runs`, is created after these experiments
    start. It contains the training and validation performance for each experiment.
    After 8 experiments finish, it’s time to launch TensorBoard. We use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once it is launched, you will see the beautiful dashboard at `http://localhost:6006/`.
    You can see a screenshot of the expected result here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21047_06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Screenshot of TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: The time series of train and test loss provide valuable insights. They allow
    us to assess the progress of training and identify signs of overfitting or underfitting.
    Overfitting can be identified when the train loss decreases over time while the
    test loss remains stagnant or increases. On the other hand, underfitting is indicated
    by relatively high train and test loss values, indicating that the model fails
    to adequately fit the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we click on the **HPARAMS** tab to see the hyperparameter logs. You can
    see all the hyperparameter combinations and the respective metrics (MSE and *R*²)
    displayed in a table, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21047_06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Screenshot of TensorBoard for hyperparameter tuning'
  prefs: []
  type: TYPE_NORMAL
- en: Again, you can see experiment 4 yields the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we use the optimal model to make predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the prediction along with the ground truth as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with blue lines and numbers  Description automatically generated](img/B21047_06_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Prediction and ground truth of stock prices'
  prefs: []
  type: TYPE_NORMAL
- en: The fine-tuned neural network does a good job of predicting stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we further improved the neural network stock predictor by fine-tuning
    the hyperparameters. Feel free to use more hidden layers, or apply dropout or
    early stopping to see whether you can get a better result.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked on the stock prediction project again, but with neural
    networks this time. We started with a detailed explanation of neural networks,
    including the essential components (layers, activations, feedforward, and backpropagation),
    and transitioned to DL. We moved on to implementations from scratch with scikit-learn,
    TensorFlow, and PyTorch. We also learned about ways to avoid overfitting, such
    as dropout and early stopping. Finally, we applied what we covered in this chapter
    to solve our stock price prediction problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore NLP techniques and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, can you use more hidden layers in the neural network stock predictor
    and rerun the model fine-tuning? Can you get a better result?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Following the first exercise, can you apply dropout and/or early stopping and
    see if you can beat the current best *R*² of `0.977`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code187846872178698968.png)'
  prefs: []
  type: TYPE_IMG

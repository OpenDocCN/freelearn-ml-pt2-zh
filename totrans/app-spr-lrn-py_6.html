<html><head></head><body>
		<div id="_idContainer331" class="Content">
			<h1 id="_idParaDest-150"><em class="italics"><a id="_idTextAnchor168"/>Chapter 6</em></h1>
		</div>
		<div id="_idContainer332" class="Content">
			<h1 id="_idParaDest-151"><a id="_idTextAnchor169"/>Model Evaluation</h1>
		</div>
		<div id="_idContainer333" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Explain the importance of evaluating models</li>
				<li class="bullets">Evaluate regression and classification models using a number of metrics</li>
				<li class="bullets">Choose the right metric for evaluating and tuning a model</li>
				<li class="bullets">Explain the importance of hold-out datasets and types of sampling</li>
				<li class="bullets">Perform hyperparameter tuning to find the best model</li>
				<li class="bullets">Calculate feature importance and explain why they are important</li>
			</ul>
			<p>This chapter introduces us to how we can improve a model's performance by using hyperparameters and model evaluation metrics.</p>
		</div>
		<div id="_idContainer374" class="Content">
			<h2 id="_idParaDest-152"><a id="_idTextAnchor170"/><a id="_idTextAnchor171"/>Introduction</h2>
			<p>In the previous three chapters, we discussed the two types of supervised learning problems, regression and classification, followed by ensemble models, which were built from a combination of base models. We built several models and discussed how and why they work.</p>
			<p>However, that is not enough to take a model to production. Model development is an iterative process, and the model training step is followed by validation and updating steps:</p>
			<div>
				<div id="_idContainer334" class="IMG---Figure">
					<img src="image/C12622_06_01.jpg" alt="Figure 6.1: Machine learning model development process&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.1: Machine learning model development process</h6>
			<p>This chapter will explain the peripheral steps in the process shown in the preceding flowchart; we will discuss how to select the appropriate hyperparameters and how to perform model validation using the appropriate error metrics. Improving a model's performance happens by iteratively performing these two tasks.</p>
			<p>But why is it important to evaluate your model? Say you've trained your model and provided some hyperparameters, made predictions, and found its accuracy. That's the gist of it, but how do you make sure that your model is performing to the best of its ability? We need to ensure that the performance measure that you've come up with is actually representative of the model and that it will indeed perform well on an unseen test dataset.</p>
			<p>The essential part about making sure that the model is the best version of itself comes after the initial training: the process of evaluating and improving the performance of the model. This chapter will take you through the essential techniques required when it comes to this.</p>
			<p>In this chapter, we will first discuss why model evaluation is important, and introduce several evaluation metrics for both regression tasks and classification tasks that can be used to quantify the predictive performance of a model. This will be followed by a discussion on hold-out datasets and k-fold cross-validation and why it is imperative to have a test set that is independent of the validation set.</p>
			<p>After this, we'll look at tactics we can use to boost the performance of the model. In the previous chapter, we talked about how having a model with a high bias or a high variance can result in suboptimal performance, and how building an ensemble of models can help us build a robust system that makes more accurate predictions without increasing the overall variance. We also mentioned the following as techniques to avoid overfitting our model to the training data:</p>
			<ul>
				<li><strong class="bold">To get more data</strong>: A highly complex model can easily overfit to a small dataset but may not be able to as easily on a larger dataset.</li>
				<li><strong class="bold">Dimensionality reduction</strong>: Reducing the number of features can help make the model less complex.</li>
				<li><strong class="bold">Regularization</strong>: A new term is added to the cost function in order to adjust the coefficients (especially the high-degree coefficients in linear regression) toward a small value.</li>
			</ul>
			<p>In this chapter, we'll introduce learning curves and validation curves as a way to see how variations in training and validation errors allow us to see whether the model needs more data, and where the appropriate level of complexity is. This will be followed by a section on hyperparameter tuning in an effort to boost performance, and a brief introduction to feature importance.</p>
			<h3 id="_idParaDest-153"><a id="_idTextAnchor172"/>Exercise 49: Importing the Modules and Preparing Our Dataset</h3>
			<p>In this exercise, we will load the data and models that we trained as part of <em class="italics">Chapter 5</em>, <em class="italics">Ensemble Modeling</em>. We will use the stacked linear regression model from <em class="italics">Activity 14: Stacking with Standalone and Ensemble Algorithms</em>, and the random forest classification model to predict the survival of passengers from <em class="italics">Exercise 45: Building the Ensemble Model Using Random Forest</em>:</p>
			<ol>
				<li>Import the relevant libraries:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import pickle</p><p class="snippet">%matplotlib inline</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Load the processed data files from <em class="italics">Chapter 5</em>, <em class="italics">Ensemble Modeling</em>. We will use pandas' <strong class="inline">read_csv()</strong> method to read in our prepared datasets, which we will use in the exercises in this chapter. First, we'll read the house price data:<p class="snippet">house_prices_reg = pd.read_csv('houseprices_regression.csv')</p><p class="snippet">house_prices_reg.head()</p><p>We'll see the following output:</p><div id="_idContainer335" class="IMG---Figure"><img src="image/C12622_06_02.jpg" alt="Figure 6.2: First five rows of house_prices"/></div><h6>Figure 6.2: First five rows of house_prices</h6><p>Next, we'll read in the Titanic data:</p><p class="snippet">titanic_clf = pd.read_csv('titanic_classification.csv')</p><p class="snippet">titanic_clf.head()</p><p>We'll see the following output:</p><div id="_idContainer336" class="IMG---Figure"><img src="image/C12622_06_03.jpg" alt="Figure 6.3: First five rows of Titanic"/></div><h6>Figure 6.3: First five rows of Titanic</h6></li>
				<li>Next, load the model files that we will use for the exercises in this chapter by using the <strong class="inline">pickle</strong> library to load them from a binary file:<p class="snippet">with open('../Saved Models/titanic_regression.pkl', 'rb') as f:</p><p class="snippet">    reg = pickle.load(f)</p><p class="snippet">with open('../Saved Models/random_forest_clf.pkl', 'rb') as f:</p><p class="snippet">    rf = pickle.load(f)</p></li>
			</ol>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-154">Eva<a id="_idTextAnchor173"/>luation Metrics</h2>
			<p>Evaluating a machine learning model is an essential part of any project: once we have allowed our model to learn from the training data, the next step is to measure the performance of the model. We need to find a metric that can not only tell us how accurate the predictions made by the model are, but also allow us to compare the performance of a number of models so that we can select the one best suited for our use case.</p>
			<p>Defining a metric is usually one of the first things we should do when defining our problem statement and before we begin the EDA, since it's a good idea to plan ahead and think about how we intend to evaluate the performance of any model we build and how to judge whether it is performing optimally or not. Eventually, calculating the performance evaluation metric will fit into the machine learning pipeline.</p>
			<p>Needless to say, evaluation metrics will be different for regression tasks and classification tasks, since the output values in the former are continuous while the outputs in the latter are categorical. In this section, we'll look at the different metrics we can use to quantify the predictive performance of a model.</p>
			<h3 id="_idParaDest-155">Reg<a id="_idTextAnchor174"/>ression</h3>
			<p>For an input variable, <em class="italics">X</em>, a regression model gives us a predicted value,<img src="image/C12622_06_Eq1.png" alt=""/> , that can take on a range of values. The ideal scenario would be to have the model predict <img src="image/C12622_06_Eq11.png" alt=""/> values that are as close as possible to the actual value of <em class="italics">y</em>. Therefore, the smaller the difference between the two, the better the model performs. Regression metrics mostly involve looking at the numerical difference between the predicted value and actual value (that is, the residual or error value) for each data point, and subsequently aggregating these differences in some way.</p>
			<p>Let's look at the following plot, which plots the actual and predicted values for every point <em class="italics">X</em>:</p>
			<div>
				<div id="_idContainer339" class="IMG---Figure">
					<img src="image/C12622_06_04.jpg" alt="Figure 6.4: Residuals between actual and predicted outputs in a linear regression problem"/>
				</div>
			</div>
			<h6>Figure 6.4: Residuals between actual and predicted outputs in a linear regression problem</h6>
			<p>However, we can't just find the mean value of <img src="image/C12622_06_Eq2.png" alt=""/> over all data points, since there could be data points that have a prediction error that is positive or negative, and the aggregate would ultimately end up canceling out a lot of the errors and severely overestimate the performance of the model.</p>
			<p>Instead, we can consider the absolute error for each data point and find the <strong class="keyword">Mean Absolute Error</strong> (<strong class="keyword">MAE</strong>), which is given by the following formula:</p>
			<div>
				<div id="_idContainer341" class="IMG---Figure">
					<img src="image/C12622_06_05.jpg" alt="Figure 6.5: Mean Absolute Error"/>
				</div>
			</div>
			<h6>Figure 6.5: Mean Absolute Error</h6>
			<p>Here, <img src="image/C12622_06_Eq3.png" alt=""/> and <img src="image/C12622_06_Eq4.png" alt=""/> are the actual and predicted values, respectively, for the <em class="italics">i</em><em class="italics">th</em> data point.</p>
			<p>MAE is a <strong class="keyword">linear scoring function</strong>, which means that it gives each residual an equal weight when it aggregates the errors. The MAE can take on any value from zero to infinity and is indifferent to the direction (positive or negative) of errors. Since these are error metrics, a lower value (as close to zero as possible) is usually desirable.</p>
			<p>In order to not let the direction of the error affect the performance estimate, we can also take the square of the error terms. Taking the mean of the squared errors gives us the <strong class="keyword">Mean Squared Error</strong> (<strong class="keyword">MSE</strong>):</p>
			<div>
				<div id="_idContainer344" class="IMG---Figure">
					<img src="image/C12622_06_06.jpg" alt="Figure 6.6: Mean Squared Error"/>
				</div>
			</div>
			<h6>Figure 6.6: Mean Squared Error</h6>
			<p>While the MAE has the same units as the target variable, <em class="italics">y</em>, the units for the MSE will be the squared unit of <em class="italics">y</em>, which may make the MSE slightly less interpretable while judging the model in real-world terms. However, if we take the square root of the MSE, we get the <strong class="keyword">Root Mean Squared Error</strong> (<strong class="keyword">RMSE</strong>):</p>
			<div>
				<div id="_idContainer345" class="IMG---Figure">
					<img src="image/C12622_06_07.jpg" alt="Figure 6.7: Root Mean Squared Error"/>
				</div>
			</div>
			<h6>Figure 6.7: Root Mean Squared Error</h6>
			<p>Since the errors are squared before they are averaged, having even a few error values that are high can cause the RMSE value to significantly increase. This means that the RMSE is more useful than MAE for judging models in which we want to penalize large errors.</p>
			<p>Since MAE and RMSE have the same units as the target variable, it can be hard to judge whether a particular value of the MAE or RMSE is good or bad, since there is no scale to refer to. A metric that is commonly used to overcome this problem is the <strong class="keyword">R</strong><strong class="keyword">2</strong><strong class="keyword"> Score</strong>, or the <strong class="keyword">R-Squared Score</strong>:</p>
			<div>
				<div id="_idContainer346" class="IMG---Figure">
					<img src="image/C12622_06_08.jpg" alt="Figure 6.8: R-Squared score"/>
				</div>
			</div>
			<h6>Figure 6.8: R-Squared score</h6>
			<p>The R2 score has a lower limit of <em class="italics">-∞</em> and an upper limit of 1. The base model predicts the target variable to be equal to the mean of the target values in the training dataset, that is, where <img src="image/C12622_06_Eq41.png" alt=""/> is equal to <img src="image/C12622_06_Eq5.png" alt=""/> for all values of <em class="italics">i</em>. Keeping this in mind, a negative value of R2 would be one where  the trained model makes a prediction that is worse than the mean, and a value close to 1 would be achieved if the MSE of the model is close to zero.</p>
			<h3 id="_idParaDest-156">Exer<a id="_idTextAnchor175"/>cise 50: Regression Metrics</h3>
			<p>In this exercise, we will use the same model and processed dataset that we trained in <em class="italics">Activity 14: Stacking with Standalone and Ensemble Algorithms</em> in <em class="italics">Chapter 5</em>, <em class="italics">Ensemble Modeling</em>, to calculate regression metrics. We will use scikit-learn's implementation of MAE and MSE:</p>
			<ol>
				<li value="1">Import the metric functions:<p class="snippet">from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score</p><p class="snippet">from math import sqrt</p></li>
				<li>Use the loaded model to predict the output on the given data. We will use the same features as we did in <em class="italics">Activity 14: Stacking with Standalone and Ensemble Algorithms</em> in <em class="italics">Chapter 5</em>, <em class="italics">Ensemble Modeling</em>, and use the model to make a prediction on the loaded dataset. The column we saved as <em class="italics">y</em> is the target variable and we will create <em class="italics">X</em> and <em class="italics">y</em> accordingly:<p class="snippet">X = house_prices_reg.drop(columns=['y'])</p><p class="snippet">y = house_prices_reg['y'].values</p><p class="snippet">y_pred = reg.predict(X)</p></li>
				<li>Calculate the MAE, RMSE, and R2 scores. Let's print the values of the MAE and the RMSE from the predicted values. Also print the R2 score for the model:<p class="snippet">print('Mean Absolute Error = {}'.format(mean_absolute_error(y, y_pred)))</p><p class="snippet">print('Root Mean Squared Error = {}'.format(sqrt(mean_squared_error(y, y_pred))))</p><p class="snippet">print('R Squared Score = {}'.format(r2_score(y, y_pred)))</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer349" class="IMG---Figure">
					<img src="image/C12622_06_09.jpg" alt="Figure 6.9: Scores"/>
				</div>
			</div>
			<h6>Figure 6.9: Scores</h6>
			<p>We can see that the RMSE is much higher than the MAE. This shows that there are some data points where the residuals are particularly high, which is being highlighted by the larger RMSE value. But the R2 score is very close to 1, indicating that the model actually has close to ideal performance compared to a base model, which would predict a mean value.</p>
			<h3 id="_idParaDest-157">Class<a id="_idTextAnchor176"/>ification</h3>
			<p>For an input variable, <em class="italics">X</em>, a classification task gives us a predicted value, <img src="image/C12622_06_Eq12.png" alt=""/>, which can take on a limited set of values (two in the case of binary classification problems). Since the ideal scenario would be to predict a class for each data point that is the same as the actual class, there is no measure of how <em class="italics">close</em> or <em class="italics">far</em> the predicted class is from the actual class. Therefore, to judge the model's performance, it would be as simple as determining whether or not the model predicted the class correctly.</p>
			<p>Judging a classification model's performance can be done in two ways: using numerical metrics, or by plotting a curve and looking at the shape of the curve. Let's explore both of these in greater detail.</p>
			<p><strong class="bold">Numerical Metrics</strong></p>
			<p>The simplest and most basic way to judge the performance of the model is to calculate the proportion of the correct predictions to the total number of predictions, which gives us the <strong class="keyword">accuracy</strong>:</p>
			<div>
				<div id="_idContainer351" class="IMG---Figure">
					<img src="image/C12622_06_10.jpg" alt="Figure 6.10: Accuracy"/>
				</div>
			</div>
			<h6>Figure 6.10: Accuracy</h6>
			<p>Although the accuracy metric is appropriate no matter the number of classes, the next few metrics are discussed keeping in mind a binary classification problem. Additionally, accuracy may not be the best metric to judge the performance of a classification task in many cases.</p>
			<p>Let's look at an example of fraud detection: say the problem statement is to detect whether a particular email is fraudulent or not. Our dataset in this case is highly skewed (or imbalanced, that is, there are many more data points belonging to one class compared to the other class), with 100 out of 10,000 emails (1% of the total) having been classified as fraudulent (having class 1). Say we build two models:</p>
			<ul>
				<li>The first model simply predicts each email as not being fraud, that is, each of the 10,000 emails is classified with the class 0. In this case, 9,900 of the 10,000 were classified correctly, which means the model has 99% accuracy.</li>
				<li>The second model predicts the 100 fraud emails as being fraud, but also predicts another 100 emails incorrectly as fraud. In this case as well, 100 data points were misclassified out of 10,000, and the model has an accuracy of 99%.</li>
			</ul>
			<p>How do we compare these two models? The purpose of building a fraud detection model is to allow us to know <em class="italics">how well the fraud was detected</em>: it matters more that the fraudulent emails were correctly classified than if non-fraud emails were classified as fraudulent. Although both the models were equally high in accuracy, the second was actually more effective than the first.</p>
			<p>Since this cannot be captured using accuracy, we need the <strong class="keyword">confusion matrix</strong>, a table with four different combinations of predicted and actual values that essentially gives us a summary of the prediction results of a classification problem:</p>
			<div>
				<div id="_idContainer352" class="IMG---Figure">
					<img src="image/C12622_06_11.jpg" alt="Figure 6.11: Confusion matrix"/>
				</div>
			</div>
			<h6>Figure 6.11: Confusion matrix</h6>
			<p>Here's what the terms used in the matrix mean:</p>
			<ul>
				<li><strong class="keyword">True positives</strong> and <strong class="keyword">true negatives</strong>: These are the counts of the correctly predicted data points in the positive and negative classes respectively.</li>
				<li><strong class="keyword">False positives</strong>: These are also known as <strong class="keyword">Type 1 errors</strong> and refer to the count of the data points that actually belong to the negative class but were predicted to be positive. Continuing from the previous example, a false positive case would be if a normal email is classified as a fraudulent email.</li>
				<li><strong class="keyword">False negatives</strong>: These are also known as <strong class="keyword">Type 2 errors</strong> and refer to the count of the data points that actually belong to the positive class but were predicted to be negative. An example of a false negative case would be if a fraudulent email was classified as not being one.</li>
			</ul>
			<p>Two extremely important metrics can be derived from a confusion matrix: <strong class="keyword">precision</strong> and <strong class="keyword">recall</strong>.</p>
			<div>
				<div id="_idContainer353" class="IMG---Figure">
					<img src="image/C12622_06_12.jpg" alt="Figure 6.12: Precision"/>
				</div>
			</div>
			<h6>Figure 6.12: Precision</h6>
			<div>
				<div id="_idContainer354" class="IMG---Figure">
					<img src="image/C12622_06_13.jpg" alt="Figure 6.13: Recall"/>
				</div>
			</div>
			<h6>Figure 6.13: Recall</h6>
			<p>While precision tells us how many of the actual positives were correctly predicted to be positive (from the results the model says are relevant, how many are actually relevant?), recall tells us how many of the predicted positives were actually positive (from the real relevant results, how many are included in the model's list of relevant results?). These two metrics are especially useful when there is an imbalance between the two classes.</p>
			<p>There is usually a trade-off between the precision and recall of a model: if you have to recall all the relevant results, the model will generate more results that are not accurate, hence lowering the precision. On the other hand, having a higher percentage of relevant results from the generated results would involve including as few results as possible. In most cases, you would give a higher priority to either the precision or the recall, and this entirely depends on the problem statement. For example, since it matters more that all the fraudulent emails are correctly classified, recall would be an important metric that would need to be maximized.</p>
			<p>The next question that arises is how we take both precision and recall to evaluate our model using a single number instead of balancing two separate metrics. The <strong class="keyword">F</strong><strong class="keyword">1</strong><strong class="keyword"> score</strong> combines the two into a single number that can be used as a fair judge of the model and is equal to the harmonic mean of precision and recall:</p>
			<div>
				<div id="_idContainer355" class="IMG---Figure">
					<img src="image/C12622_06_14.jpg" alt="Figure 6.14: F1 Score"/>
				</div>
			</div>
			<h6>Figure 6.14: F1 Score</h6>
			<p>The value of the F1 score will always lie between 0 (if either precision or recall is zero) and 1 (if both precision and recall are 1). The higher the score, the better the model's performance is said to be. The F1 score gives equal weight to both measures and is a specific example of the general Fβ metric, where β can be adjusted to give more weight to either recall or precision using the following formula:</p>
			<div>
				<div id="_idContainer356" class="IMG---Figure">
					<img src="image/C12622_06_15.jpg" alt="Figure 6.15: F beta score"/>
				</div>
			</div>
			<h6>Figure 6.15: F beta score</h6>
			<p>A value of <em class="italics">β</em><em class="italics"> &lt; 1</em> focuses more on precision, while taking <em class="italics">β</em><em class="italics"> &gt; 1</em> focuses more on recall. The F1 score takes <em class="italics">β</em><em class="italics"> = 1</em> to give both equal weight.</p>
			<p><strong class="bold">Curve Plots</strong></p>
			<p>Sometimes, instead of predicting the class, we have the class probabilities at our disposal. Say, in a binary classification task, the class probabilities of both the positive (class 1) and negative (class 0) classes will always add up to unity (or 1), which means that if we take the classification probability as equal to the probability of class 1 and apply a threshold, we can essentially use it as a cut-off value to either round up (to 1) or down (to 0), which will give the output class.</p>
			<p>Usually, by varying the threshold, we can get data points that have classification probabilities closer to 0.5 from one class to another. For example, with a threshold of 0.5, a data point having a probability of 0.4 would be assigned class 0 and a data point having probability 0.6 would be assigned class 1. But if we change the threshold to 0.35 or 0.65, both those data points would be classified as 1 or 0.</p>
			<p>As it turns out, varying the probability changes the precision and recall values and this can be captured by plotting the <strong class="keyword">precision-recall curve</strong>. The plot has precision on the <em class="italics">Y</em> axis and recall on the <em class="italics">X</em> axis, and for a range of thresholds starting from 0 to 1 plots each <em class="italics">(recall, precision)</em> point. Connecting these points gives us the curve. The following graph shows an example:</p>
			<div>
				<div id="_idContainer357" class="IMG---Figure">
					<img src="image/C12622_06_16.jpg" alt="Figure 6.16: Precision-recall curve"/>
				</div>
			</div>
			<h6>Figure 6.16: Precision-recall curve</h6>
			<p>We know that in an ideal case, the values of precision and recall will be unity. This means that upon increasing the threshold from 0 to 1, the precision would stay constant at 1, but the recall would increase from 0 to 1 as more and more (relevant) data points would be classified correctly. Thus, in an ideal case, the precision-recall curve would essentially just be a square and the <strong class="keyword">area under the curve</strong> (<strong class="keyword">AUC</strong>) would be equal to one.</p>
			<p>Thus, we can see that, as with the F1 score, the AUC is another metric derived from the precision and recall behavior that uses a combination of their values to evaluate the performance of the model. We want the model to achieve an AUC as high and close to 1 as possible.</p>
			<p>The other main visualization technique for showing the performance of a classification model is the <strong class="keyword">Receiver Operating Characteristic</strong> (<strong class="keyword">ROC</strong>) curve. The ROC curve plots the relationship between the <strong class="keyword">True Positive Rate</strong> (<strong class="keyword">TPR</strong>) on the <em class="italics">Y</em> axis and the <strong class="keyword">False Positive Rate</strong> (<strong class="keyword">FPR</strong>) on the <em class="italics">X</em> axis across a varying classification probability threshold. TPR is exactly the same as the recall (and is also known as the <strong class="keyword">sensitivity</strong> of the model), and FPR is an equal complement of the <strong class="keyword">specificity</strong> (that is, <em class="italics">1 - FPR = Sensitivity</em>); both can be derived from the confusion matrix using these formulae:</p>
			<div>
				<div id="_idContainer358" class="IMG---Figure">
					<img src="image/C12622_06_17.jpg" alt="Figure 6.17: True positive rate"/>
				</div>
			</div>
			<h6>Figure 6.17: True positive rate</h6>
			<div>
				<div id="_idContainer359" class="IMG---Figure">
					<img src="image/C12622_06_18.jpg" alt="Figure 6.18: False positive rate&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.18: False positive rate</h6>
			<p>The following diagram shows an example of an ROC curve, plotted in the same way as the precision-recall curve: by varying the probability threshold such that each point on the curve represents a <em class="italics">(TPR, FPR)</em> data point corresponding to a specific probability threshold.</p>
			<div>
				<div id="_idContainer360" class="IMG---Figure">
					<img src="image/C12622_06_19.jpg" alt="Figure 6.19: ROC curve"/>
				</div>
			</div>
			<h6>Figure 6.19: ROC curve</h6>
			<p>ROC curves are more useful when the classes are fairly balanced, since they tend to present an overly optimistic picture of the model on datasets with a class imbalance via their use of true negatives in the false positive rate in the ROC curve (which is not present in the precision-recall curve).</p>
			<h3 id="_idParaDest-158">Ex<a id="_idTextAnchor177"/>ercise 51: Classification Metrics</h3>
			<p>In this exercise, we will use the random forest model we trained in <em class="italics">Chapter 5</em>, <em class="italics">Ensemble Modeling,</em> and use its predictions to generate the confusion matrix and calculate the precision, recall, and F1 scores, as a way of rating our model. We will use scikit-learn's implementations to calculate these metrics:</p>
			<ol>
				<li value="1">Import the relevant libraries and functions:<p class="snippet">from sklearn.metrics import (accuracy_score, confusion_matrix, precision_score,</p><p class="snippet">                             recall_score, f1_score)</p></li>
				<li>Use the model to predict classes for all data points. We will use the same features as we did earlier and use the random forest classifier to make a prediction on the loaded dataset. Every classifier in scikit-learn has a <strong class="inline">.predict_proba()</strong> function, which we will use here along with the standard <strong class="inline">.predict()</strong> function to give us the class probabilities and the classes respectively:<p class="snippet">X = titanic_clf.iloc[:, :-1]</p><p class="snippet">y = titanic_clf.iloc[:, -1]</p><p class="snippet">y_pred = rf.predict(X)</p><p class="snippet">y_pred_probs = rf.predict_proba(X)</p></li>
				<li>Calculate the accuracy:<p class="snippet">print('Accuracy Score = {}'.format(accuracy_score(y, y_pred)))</p><p>The output will be as follows:</p><div id="_idContainer361" class="IMG---Figure"><img src="image/C12622_06_20.jpg" alt="Figure 6.20: Accuracy score"/></div><h6>Figure 6.20: Accuracy score</h6></li>
				<li>Print the confusion matrix:<p class="snippet">print(confusion_matrix(y_pred=y_pred, y_true=y))</p><p>The output will be as follows:</p><div id="_idContainer362" class="IMG---Figure"><img src="image/C12622_06_21.jpg" alt="Figure 6.21: Confusion matrix"/></div><h6>Figure 6.21: Confusion matrix</h6><p>Here, we can see that the model seems to have a high number of false negatives, which means that we can expect the recall value for this model to be extremely low. Similarly, since the count of the false positives is just one, we can expect the model to have high precision.</p></li>
				<li>Calculate the precision and recall:<p class="snippet">print('Precision Score = {}'.format(precision_score(y, y_pred)))</p><p class="snippet">print('Recall Score = {}'.format(recall_score(y, y_pred)))</p><p>The output will be as follows:</p><div id="_idContainer363" class="IMG---Figure"><img src="image/C12622_06_22.jpg" alt="Figure 6.22: Precision and recall scores"/></div><h6>Figure 6.22: Precision and recall scores</h6></li>
				<li>Calculate the F1 score:<p class="snippet">print('F1 Score = {}'.format(f1_score(y, y_pred)))</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer364" class="IMG---Figure">
					<img src="image/C12622_06_23.jpg" alt="Figure 6.23: F1 score"/>
				</div>
			</div>
			<h6>Figure 6.23: F1 score</h6>
			<p>We can see that, since the recall is extremely low, this is affecting the F1 score as well, making it close to zero.</p>
			<p>Now that we have talked about the metrics we can use to measure the predictive performance of the model, let's talk about validation strategies, in which we will use a metric to evaluate the performance of the model in different cases and situations.</p>
			<h2 id="_idParaDest-159">Splitt<a id="_idTextAnchor178"/>ing the Dataset</h2>
			<p>A common mistake made when determining how well a model is performing is to calculate the prediction error on the data that the model was trained on and conclude that a model performs really well on the basis of a high prediction accuracy on the training dataset.</p>
			<p>This means that we are trying to test the model on data that the model has already <em class="italics">seen</em>, that is, the model has already learned the behavior of the training data because it was exposed to it—if asked to predict the behavior of the training data again, it would undoubtedly perform well. And the better the performance on the training data, the higher the chances that the model knows the data <em class="italics">too well</em>, so much so that it has even learned the noise and behavior of outliers in the data.</p>
			<p>Now, high training accuracy results in a model having high variance, as we saw in the previous chapter. In order to get an unbiased estimate of the model's performance, we need to find its prediction accuracy on data it has not already been exposed to during training. This is where the hold-out dataset comes into the picture.</p>
			<h3 id="_idParaDest-160">Hold-o<a id="_idTextAnchor179"/>ut Data</h3>
			<p>The <strong class="keyword">hold-out dataset</strong> refers to a sample of the dataset that has been held back from training the model on and is essentially <em class="italics">unseen</em> by the model. The hold-out data points will likely contain outliers and noisy data points that behave differently from those in the training dataset, given that noise is random. Thus, calculating the performance on the hold-out dataset would allow us to validate whether the model is overfitting or not, as well as giving us an unbiased view of the model's performance.</p>
			<p>We began our previous chapter by splitting the Titanic dataset into training and validation sets. What is this validation dataset, and how is it different from a test dataset? We often see the terms validation set and test set used interchangeably—although they both characterize a hold-out dataset, there are some differences in purpose:</p>
			<ul>
				<li><strong class="keyword">Validation data</strong>: After the model learns from the training data, its performance is evaluated on the validation dataset. However, in order to get the model to perform the best it can, we need to fine-tune the model and iteratively evaluate the updated model's performance repeatedly, and this is done on the validation dataset. The fine-tuned version of the model that performs best on the validation dataset is usually chosen to be the final model.<p>The model, thus, is exposed to the validation dataset multiple times, at each iteration of improvement, although does not essentially <em class="italics">learn</em> from the data. It can be said that the validation set does affect the model, although indirectly.</p></li>
				<li><strong class="keyword">Test data</strong>: The final model that was chosen is now evaluated on the test dataset. The performance measured on this dataset will be an unbiased measure that is reported as the final performance metric of the model. This final evaluation is done once the model has been completely trained on the combined training and validation datasets. There is no training or updating of the model performed after this metric has been calculated.<p>This means that the model is exposed to the test dataset only once, when calculating the final performance metric.</p></li>
			</ul>
			<p>It should be kept in mind that the validation dataset should never be used to evaluate the final performance of the model: our estimate of the true performance of a model will be positively biased if the model has seen and been modified subsequently in an effort to specifically improve the performance on the validation set.</p>
			<p>Having a single hold-out validation dataset does have some limitations, however:</p>
			<ul>
				<li>Since the model is only validated once in each iteration of improvement, it might be difficult to capture the uncertainty in prediction using this single evaluation.</li>
				<li>Dividing the data into training and validation sets decreases the size of the data upon which the model is trained, and this can lead to the model having high variance.</li>
				<li>The final model may <em class="italics">overfit</em> to this validation set since it was tuned in order to maximize performance on this dataset.</li>
			</ul>
			<p>These challenges can be overcome if we use a validation technique called K-fold cross-validation instead of using a single validation dataset.</p>
			<h3 id="_idParaDest-161">K-Fold<a id="_idTextAnchor180"/> Cross-Validation</h3>
			<p>K-fold cross-validation is a validation technique that helps us get an unbiased estimate of the model's performance by essentially rotating the validation set in <em class="italics">k</em> folds. This is how it works:</p>
			<ol>
				<li value="1">First, we choose the value of <em class="italics">k</em> and divide the data into <em class="italics">k</em> subsets.</li>
				<li>Then, we set aside the first subset as the validation set and use the remaining data to train the model.</li>
				<li>We measure the performance of the model on the validation subset.</li>
				<li>Then, we set aside the second subset as the validation subset and repeat the process.</li>
				<li>Once we have done this <em class="italics">k</em> times, we aggregate the performance metric values over all the folds and present the final metric.</li>
			</ol>
			<p>The following figure explains this visually:</p>
			<div>
				<div id="_idContainer365" class="IMG---Figure">
					<img src="image/C12622_06_24.jpg" alt="Figure 6.24: K-fold cross-validation"/>
				</div>
			</div>
			<h6>Figure 6.24: K-fold cross-validation</h6>
			<p>Although this method of validation is more computationally expensive, the benefits outweigh the costs. This approach makes sure that the model is validated on each example in the training dataset exactly once and that the performance estimate we achieve in the end is not biased in favor of a validation dataset, especially in the case of small datasets. A special case is <strong class="bold">leave-one-out</strong> cross-validation, where the value of <em class="italics">k</em> is equal to the number of data points.</p>
			<h3 id="_idParaDest-162">Samplin<a id="_idTextAnchor181"/>g</h3>
			<p>Now that we've looked at the strategies for splitting the dataset for training and validating the model, let's discuss how to allocate data points to these splits. There are two ways we can sample the data into the splits, and these are as follows:</p>
			<ul>
				<li><strong class="keyword">Random sampling</strong>: This is as simple as allocating random samples from the overall dataset into the training, validation, and/or test datasets. Randomly splitting the data only works when all the data points are independent of each other. For example, random splitting would not be the way to go if the data was in the form of a time-series, since the data points are ordered, and each depends on the previous one. Randomly splitting the data would destroy that order and not take into account this dependence.</li>
				<li><strong class="keyword">Stratified sampling</strong>: This is a way to ensure that each subset has the same distribution of values of the target variable as the original dataset. For example, if the original dataset has two classes in the ratio 3:7, stratified sampling ensures that each subset will also contain the two classes in the ratio 3:7.<p>Stratified sampling is important since testing our model on a dataset with a different distribution of target values from the dataset on which the model was trained can give us a performance estimate that is not representative of the model's actual performance.</p></li>
			</ul>
			<p>The size of the train, validation, and test samples also plays an important role in the model evaluation process. Keeping aside a large dataset to test the final performance of the model on will help us get an unbiased estimate of the model's performance and reduce the variance in prediction, but if the test set is so large that it compromises the model's ability to train due to a lack of training data, this will severely affect the model as well. This is a consideration that is especially relevant for smaller datasets.</p>
			<h3 id="_idParaDest-163">Exercis<a id="_idTextAnchor182"/>e 52: K-Fold Cross-Validation with Stratified Sampling</h3>
			<p>In this exercise, we'll implement K-fold cross-validation with stratified sampling on scikit-learn's random forest classifier. The <strong class="inline">StratifiedKFold</strong> class in scikit-learn implements a combination of the cross-validation and sampling together in one class, and we will use this in our exercise:</p>
			<ol>
				<li value="1">Import the relevant classes. We will import scikit-learn's <strong class="inline">StratifiedKFold</strong> class, which is a variation of <strong class="inline">KFold</strong> that returns stratified folds, along with the <strong class="inline">RandomForestClassifier</strong>:<p class="snippet">from sklearn.model_selection import StratifiedKFold</p><p class="snippet">from sklearn.ensemble import RandomForestClassifier</p></li>
				<li>Prepare data for training and initialize the k-fold object. Here, we will use five folds to evaluate the model, and hence will give the <strong class="inline">n_splits</strong> parameter a value of <strong class="inline">5</strong>:<p class="snippet">X = titanic_clf.iloc[:, :-1].values</p><p class="snippet">y = titanic_clf.iloc[:, -1].values</p><p class="snippet">skf = StratifiedKFold(n_splits=5)</p></li>
				<li>Train a classifier for each fold and record the score. The functioning of the <strong class="inline">StratifiedKFold</strong> class is similar to the <strong class="inline">KFold</strong> class that we used in the previous chapter, in <em class="italics">Exercise 48: Building a Stacked Model</em>: for each of the five folds, we will train on other four folds and predict on the fifth fold, and find the accuracy score for the predictions on the fifth fold. As we saw in the last chapter, the <strong class="inline">skf.split()</strong> function takes the dataset to split as input and returns an iterator comprising the index values used to subdivide the training data for training and validation for each row:<p class="snippet">scores = []</p><p class="snippet">for train_index, val_index in skf.split(X, y):</p><p class="snippet">    X_train, X_val = X[train_index], X[val_index]</p><p class="snippet">    y_train, y_val = y[train_index], y[val_index]</p><p class="snippet">    </p><p class="snippet">    rf_skf = RandomForestClassifier(**rf.get_params())</p><p class="snippet">    </p><p class="snippet">    rf_skf.fit(X_train, y_train)</p><p class="snippet">    y_pred = rf_skf.predict(X_val)</p><p class="snippet">    </p><p class="snippet">    scores.append(accuracy_score(y_val, y_pred))</p><p class="snippet">    </p><p class="snippet">print(scores)</p><p>The output will be as follows:</p><div id="_idContainer366" class="IMG---Figure"><img src="image/C12622_06_25.jpg" alt="Figure 6.25: Scores using random forest classifier"/></div><h6>Figure 6.25: Scores using random forest classifier</h6></li>
				<li>Print the aggregated accuracy score:<p class="snippet">print('Mean Accuracy Score = {}'.format(np.mean(scores)))</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer367" class="IMG---Figure">
					<img src="image/C12622_06_26.jpg" alt="Figure 6.26: Mean accuracy score"/>
				</div>
			</div>
			<h6>Figure 6.26: Mean accuracy score</h6>
			<h2 id="_idParaDest-164">Performan<a id="_idTextAnchor183"/>ce Improvement Tactics</h2>
			<p>Performance improvement for supervised machine learning models is an iterative process, and a continuous cycle of updating and evaluation is usually required to get the perfect model. While the previous sections in this chapter dealt with the evaluation strategies, this section will talk about model updating: we will discuss some ways we can determine what our model needs to give it that performance boost, and how to make that change in our model.</p>
			<h3 id="_idParaDest-165">Variation<a id="_idTextAnchor184"/> in Train and Test Error</h3>
			<p>In the previous chapter, we introduced the concepts of underfitting and overfitting, and mentioned a few ways to overcome them, later introducing ensemble models. But we didn't talk about how to identify whether our model was underfitting or overfitting to the training data.</p>
			<p>It's usually useful to look at the learning and validation curves.</p>
			<p><strong class="bold">Learning Curve</strong></p>
			<p>The learning curve shows the variation in the training and validation error with the training data increasing in size. By looking at the shape of the curves, we can get a good idea of whether or not more data will benefit the modeling and possibly improve the model's performance.</p>
			<p>Let's look at the following figure: the dotted curve represents the validation error and the solid curve represents the training error. The plot on the left shows the two curves converging to an error value that is quite high. This means that the model has a high bias and adding more data isn't likely to affect the model performance. So instead of wasting time and money collecting more data, all we need to do is increase model complexity.</p>
			<p>On the other hand, the plot on the right shows a high difference between the training and test errors, even with an increasing number of data points in the training set. The wide gap indicates a high variance in the system, which means the model is overfitting. In this case, adding more data points will probably help the model generalize better:</p>
			<div>
				<div id="_idContainer368" class="IMG---Figure">
					<img src="image/C12622_06_27.jpg" alt="Figure 6.27: Learning curve for increasing data size"/>
				</div>
			</div>
			<h6>Figure 6.27: Learning curve for increasing data size</h6>
			<p>But how will we recognize the perfect learning curve? When we have a model with low bias and low variance, we will see a curve like the one shown in the following figure. It shows a low training error (low bias) as well as a low gap between the validation and training curves (low variance) as they converge. In practice, the best possible learning curves we can see are those that converge to the value of some irreducible error value (which exists due to noise and outliers in the dataset):</p>
			<div>
				<div id="_idContainer369" class="IMG---Figure">
					<img src="image/C12622_06_28.jpg" alt="Figure 6.28: Variation in training and validation error with an increasing training data size for a low bias and variance model"/>
				</div>
			</div>
			<h6>Figure 6.28: Variation in training and validation error with an increasing training data size for a low bias and variance model</h6>
			<p><strong class="bold">Validation Curve</strong></p>
			<p>As we have discussed previously, the goal of a machine learning model is to be able to generalize to unseen data. Validation curves allow us to find the ideal point between an underfitted and an overfitted model where the model would generalize well. In the previous chapter, we talked a bit about how model complexity affects prediction performance: we said that as we move from an overly simplistic to an overly complex model, we go from having an underfitted model with high bias and low variance to an overfitted model with a low bias and high variance.</p>
			<p>A validation curve shows the variation in training and validation error with a varying value of a model parameter that has some degree of control over the model's complexity—this could be the degree of the polynomial in linear regression, or the depth of a decision tree classifier.</p>
			<div>
				<div id="_idContainer370" class="IMG---Figure">
					<img src="image/C12622_06_29.jpg" alt="Figure 6.29: Variation in training and validation with increasing model complexity"/>
				</div>
			</div>
			<h6>Figure 6.29: Variation in training and validation with increasing model complexity</h6>
			<p>The preceding figure shows how the validation and training error will vary with model complexity (of which the model parameter is an indicator). We can also see how the point in between the shaded regions is where the total error would be at a minimum, at the sweet spot between underfitting and overfitting. Finding this point will help us find the ideal value of the model's parameters that will help build a model with low bias as well as low variance.</p>
			<h3 id="_idParaDest-166">Hyperparamet<a id="_idTextAnchor185"/>er Tuning</h3>
			<p>We've talked about hyperparameter tuning several times before this; now let's discuss why it's so important. First, it should be noted that model parameters are different from model hyperparameters: while the former are internal to the model and are learned from the data, the latter define the architecture of the model itself.</p>
			<p>Some examples of hyperparameters are as follows:</p>
			<ul>
				<li>The degree of polynomial features to be used for a linear regressor</li>
				<li>The maximum depth allowed for a decision tree classifier</li>
				<li>The number of trees to be included in a random forest classifier</li>
				<li>The learning rate used for the gradient descent algorithm</li>
			</ul>
			<p>The design choices that define the architecture of the model can make a huge difference in how well the model performs. Usually, the default values for the hyperparameters work, but getting the perfect combination of values for the hyperparameters can really give the predictive power of the model a boost as the default values may be completely inappropriate for the problem we are trying to model. In the following figure, we see how varying the values of two hyperparameters can cause such a difference in the model score:</p>
			<div>
				<div id="_idContainer371" class="IMG---Figure">
					<img src="image/C12622_06_30.jpg" alt="Figure 6.30: Variation in model score (Z axis) across values of two model parameters (the X and Y axes)"/>
				</div>
			</div>
			<h6>Figure 6.30: Variation in model score (Z axis) across values of two model parameters (the X and Y axes)</h6>
			<p>Finding that perfect combination by exploring a range of possible values is what is referred to as <strong class="keyword">hyperparameter tuning</strong>. Since there is no loss function we can use to maximize the model performance, tuning the hyperparameters generally just involves experimenting with different combinations and choosing the one that performs best during validation.</p>
			<p>There are a few ways in which we can go about tuning our model's hyperparameters:</p>
			<ul>
				<li><strong class="keyword">Hand-tuning</strong>: When we manually choose the values of our hyperparameters, this is known as hand-tuning. It is usually inefficient, since solving a high-dimensional optimization problem by hand can not only be slow, but also would not allow the model to reach its peak performance as we probably wouldn't try out every single combination of hyperparameter values.</li>
				<li><strong class="keyword">Grid search</strong>: Grid search involves training and evaluating a model for each combination of the hyperparameter values provided and selecting the combination that produces the best performing model. Since this involves performing an exhaustive sampling of the hyperparameter space, it is quite computationally expensive and hence inefficient.</li>
				<li><strong class="keyword">Random search</strong>: While the first method was deemed inefficient because too few combinations were tried, the second one was deemed so because too many combinations were tried. Random search aims to solve this by selecting a random subset of hyperparameter combinations from the grid (specified previously), and training and evaluating a model only for those. Alternatively, we can also provide a statistical distribution for each hyperparameter from which the values can be randomly sampled.<p>The logic behind random search was proved by Bergstra and Bengio: if at least 5% of the points on the grid yield a close-to-optimal solution, then random search with 60 trials will find that region with a high probability.</p><h4>Note</h4><p class="callout">You can read the paper by Bergstra and Bengio at <a href="http://www.jmlr.org/papers/v13/bergstra12a.html">http://www.jmlr.org/papers/v13/bergstra12a.html</a>.</p></li>
				<li><strong class="keyword">Bayesian optimization</strong>: The previous two methods involved independently experimenting with combinations of hyperparameter values and recording the model performance for each. However, Bayesian optimization iterates over experiments sequentially and allows us to use the results of a previous experiment to improve the sampling method for the next experiment.</li>
			</ul>
			<h3 id="_idParaDest-167">Exercise 53: <a id="_idTextAnchor186"/>Hyperparameter Tuning with Random Search</h3>
			<p>Using scikit-learn's <strong class="inline">RandomizedSearchCV</strong> method, we can define a grid of hyperparameter ranges and randomly sample from the grid, performing K-fold cross-validation with each combination of values. In this exercise, we'll perform hyperparameter tuning with the random search method:</p>
			<ol>
				<li value="1">Import the class for random search:<p class="snippet">from sklearn.model_selection import RandomizedSearchCV</p></li>
				<li>Prepare data for training and initialize the classifier. Here, we will initialize our random forest classifier without passing any arguments, since this is just a base object that will be instantiated for each grid point on which to perform the random search:<p class="snippet">X = titanic_clf.iloc[:, :-1].values</p><p class="snippet">y = titanic_clf.iloc[:, -1].values</p><p class="snippet">rf_rand = RandomForestClassifier()</p></li>
				<li>Specify the parameters to sample from. Here, we will list down the different values for each hyperparameter that we would like to have in the grid:<p class="snippet">param_dist = {"n_estimators": list(range(10,210,10)),</p><p class="snippet">              "max_depth": list(range(3,20)),</p><p class="snippet">              "max_features": list(range(1, 10)),</p><p class="snippet">              "min_samples_split": list(range(2, 11)),</p><p class="snippet">              "bootstrap": [True, False],</p><p class="snippet">              "criterion": ["gini", "entropy"]}</p></li>
				<li>Run a randomized search. We initialize the random search object with the total number of trials we want to run, the parameter values dictionary, the scoring function, and the number of folds in the K-fold cross-validation. Then, we call the <strong class="inline">.fit()</strong> function to perform the search:<p class="snippet">n_iter_search = 60</p><p class="snippet">random_search = RandomizedSearchCV(rf_rand, param_distributions=param_dist, scoring='accuracy',</p><p class="snippet">                                   n_iter=n_iter_search, cv=5)</p><p class="snippet">random_search.fit(X, y)</p></li>
				<li>Print scores and hyperparameters for the top five models. Convert the <strong class="inline">results</strong> dictionary into a pandas DataFrame and sort the values by <strong class="inline">rank_test_score</strong>. Then, for the first five rows, print the rank, mean validation score, and the hyperparameters:<p class="snippet">results = pd.DataFrame(random_search.cv_results_).sort_values('rank_test_score')</p><p class="snippet">for i, row in results.head().iterrows():</p><p class="snippet">    print("Model rank: {}".format(row.rank_test_score))</p><p class="snippet">    print("Mean validation score: {:.3f} (std: {:.3f})".format(row.mean_test_score, row.std_test_score))</p><p class="snippet">    print("Model Hyperparameters: {}\n".format(row.params))</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer372" class="IMG---Figure">
					<img src="image/C12622_06_31.jpg" alt="Figure 6.31: Top five models’ scores and hyperparameters"/>
				</div>
			</div>
			<h6>Figure 6.31: Top five models' scores and hyperparameters</h6>
			<p>We can see that the model that performs best has only 70 trees, compared to the 160+ trees in the models ranked 2 to 4. Also, the model ranked 5 only has 10 trees and still has a performance comparable to that of the more complex models.</p>
			<h3 id="_idParaDest-168">Feature Import<a id="_idTextAnchor187"/>ance</h3>
			<p>While it is essential to focus on model performance, it's also important to understand how the features in our model contribute to the prediction:</p>
			<ul>
				<li>We need to be able to explain the model and how different variables affect the prediction to the relevant stakeholders who might demand insight into why our model is successful.</li>
				<li>The data might be biased and training a model on this data could hurt the model's performance and result in biased model evaluation, in which case the ability to interpret the model by finding the important features and analyzing them will help debug the performance of the model.</li>
				<li>In addition to the previous point, it must be noted that some model biases might just be socially or legally unacceptable. For example, if a model works well because it implicitly places high importance on a feature based on ethnicity, this might cause issues.</li>
			</ul>
			<p>Besides these points, finding feature importance can also help in feature selection. If the data has high dimensionality and the trained model has high variance, removing features that have low importance is one way to achieve lowered variance through dimensionality reduction.</p>
			<h3 id="_idParaDest-169">Exercise 54: F<a id="_idTextAnchor188"/>eature Importance Using Random Forest</h3>
			<p>In this exercise, we will find the feature importance from the random forest model we loaded earlier:</p>
			<ol>
				<li value="1">Find feature importance. Let's find the feature importance and save it in a pandas DataFrame with index equal to the column names, and sort this DataFrame in descending order:<p class="snippet">feat_imps = pd.DataFrame({'importance': rf.feature_importances_}, index=titanic_clf.columns[:-1])</p><p class="snippet">feat_imps.sort_values(by='importance', ascending=False, inplace=True)</p></li>
				<li>Plot the feature importance as a bar plot:<p class="snippet">feat_imps.plot(kind='bar', figsize=(10,7))</p><p class="snippet">plt.legend()</p><p class="snippet">plt.show()</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer373" class="IMG---Figure">
					<img src="image/C12622_06_32.jpg" alt="Figure 6.32: Histogram of features"/>
				</div>
			</div>
			<h6>Figure 6.32: Histogram of features</h6>
			<p>Here, we can see that the <strong class="inline">Sex</strong>, <strong class="inline">Fare</strong>, and <strong class="inline">Pclass</strong> features seem to have the highest importance, that is, they have the most effect on the target variable.</p>
			<h3 id="_idParaDest-170">Activity 15: Fi<a id="_idTextAnchor189"/>nal Test Project</h3>
			<p>In this activity, we'll use the <em class="italics">IBM HR Analytics Employee Attrition &amp; Performance</em> dataset (available at <a href="https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset">https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset</a>, and the accompanying source code at <a href="https://github.com/TrainingByPackt/Supervised-Learning-with-Python">https://github.com/TrainingByPackt/Supervised-Learning-with-Python</a>) to solve a classification problem wherein we have to predict whether or not an employee will leave the company given the features. In the employee attrition problem, we want to m<a id="_idTextAnchor190"/>aximize our recall, that is, we want to be able to identify all employees that will leave, even at the cost of predicting that a good employee will leave: this will help HR take the appropriate action for these employees so that they don't leave.</p>
			<p>Each row in the dataset represents a single employee, and the target variable we have here is <strong class="inline">Attrition</strong>, which has two values: <strong class="inline">1</strong> and <strong class="inline">0</strong>, representing a <em class="italics">Yes</em> and <em class="italics">No</em> with respect to whether the corresponding employee left. We will use a gradient boosting classifier from scikit-learn to train the model. This activity is meant as a final project that will help consolidate the practical aspects of the concepts learned in this book, and particularly in this chapter.</p>
			<p>We will find the most optimal set of hyperparameters for the model by using random search with cross-validation. Then, we will build the final classifier using the gradient boosting algorithm on a portion of the dataset and evaluate its performance using the classification metrics we have learned about on the remaining portion of the dataset. We will use the mean absolute error as the evaluation metric for this activity.</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Import the relevant libraries.</li>
				<li>Read the <strong class="inline">attrition_train.csv</strong> dataset.</li>
				<li>Read the <strong class="inline">categorical_variable_values.json</strong> file, which has details of categorical variables.</li>
				<li>Process the dataset to convert all features to numerical values.</li>
				<li>Choose a base model and define the range of hyperparameter values corresponding to the model to be searched over for hyperparameter tuning.</li>
				<li>Define the parameters with which to initialize the <strong class="inline">RandomizedSearchCV</strong> object and use K-fold cross-validation to find the best model hyperparameters.</li>
				<li>Split the dataset into training and validation sets and train a new model using the final hyperparameters on the training dataset.</li>
				<li>Calculate the accuracy, precision, and recall for predictions on the validation set, and print the confusion matrix.</li>
				<li>Experiment with varying thresholds to find the optimal point with high recall. Plot the precision-recall curve.</li>
				<li>Finalize a threshold that will be used for predictions on the test dataset.</li>
				<li>Read and process the test dataset to convert all features to numerical values.</li>
				<li>Predict the final values on the test dataset.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 373.</p></li>
			</ol>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor191"/>Summary</h2>
			<p>This chapter discussed why model evaluation is important in supervised machine learning and looked at several important metrics that are used to evaluate regression and classification tasks. We saw that while regression models were fairly straightforward to evaluate, the performance of classification models could be measured in a number of ways, depending on what we want the model to prioritize. Besides numerical metrics, we also looked at how to plot precision-recall and ROC curves to better interpret and evaluate model performance.</p>
			<p>After this, we talked about why evaluating a model by calculating the prediction error on the data that the model was trained on was a bad idea, and how testing a model on data that it has already <em class="italics">seen</em> would lead to the model having a high variance. With this, we introduced the concept of having a hold-out dataset and why K-fold cross-validation is a useful strategy to have, along with sampling techniques that ensure that the model training and evaluation process remains unbiased.</p>
			<p>The last section on performance improvement tactics started with a discussion on learning and validation curves, and how they can be interpreted to drive the model development process towards a better-performing model. This was followed by a section on hyperparameter tuning as an effort to boost performance, and a brief introduction to feature importance.</p>
		</div>
	</body></html>
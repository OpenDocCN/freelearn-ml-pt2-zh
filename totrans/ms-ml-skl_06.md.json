["```py\n>>> import numpy as np \n>>> from sklearn.cluster import KMeans \n>>> from scipy.spatial.distance import cdist \n>>> import matplotlib.pyplot as plt \n\n>>> cluster1 = np.random.uniform(0.5, 1.5, (2, 10))\n>>> cluster2 = np.random.uniform(3.5, 4.5, (2, 10))\n>>> X = np.hstack((cluster1, cluster2)).T\n>>> X = np.vstack((x, y)).T \n\n>>> K = range(1, 10) \n>>> meandistortions = [] \n>>> for k in K: \n>>>     kmeans = KMeans(n_clusters=k) \n>>>     kmeans.fit(X) \n>>>     meandistortions.append(sum(np.min(cdist(X, kmeans.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0]) \n\n>>> plt.plot(K, meandistortions, 'bx-') \n>>> plt.xlabel('k') \n>>> plt.ylabel('Average distortion') \n>>> plt.title('Selecting k with the Elbow Method') \n>>> plt.show()\n```", "```py\n>>> import numpy as np\n>>> from sklearn.cluster import KMeans\n>>> from sklearn import metrics\n>>> import matplotlib.pyplot as plt\n>>> plt.subplot(3, 2, 1)\n>>> x1 = np.array([1, 2, 3, 1, 5, 6, 5, 5, 6, 7, 8, 9, 7, 9])\n>>> x2 = np.array([1, 3, 2, 2, 8, 6, 7, 6, 7, 1, 2, 1, 1, 3])\n>>> X = np.array(zip(x1, x2)).reshape(len(x1), 2)\n>>> plt.xlim([0, 10])\n>>> plt.ylim([0, 10])\n>>> plt.title('Instances')\n>>> plt.scatter(x1, x2)\n>>> colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'b']\n>>> markers = ['o', 's', 'D', 'v', '^', 'p', '*', '+']\n>>> tests = [2, 3, 4, 5, 8]\n>>> subplot_counter = 1\n>>> for t in tests:\n>>>     subplot_counter += 1\n>>>     plt.subplot(3, 2, subplot_counter)\n>>>     kmeans_model = KMeans(n_clusters=t).fit(X)\n>>>     for i, l in enumerate(kmeans_model.labels_):\n>>>         plt.plot(x1[i], x2[i], color=colors[l], marker=markers[l], ls='None')\n>>>     plt.xlim([0, 10])\n>>>     plt.ylim([0, 10])\n>>>     plt.title('K = %s, silhouette coefficient = %.03f' % (\n>>>         t, metrics.silhouette_score(X, kmeans_model.labels_, metric='euclidean')))\n>>> plt.show()\n```", "```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.cluster import KMeans\n>>> from sklearn.utils import shuffle\n>>> import mahotas as mh\n```", "```py\n>>> original_img = np.array(mh.imread('img/tree.jpg'), dtype=np.float64) / 255\n>>> original_dimensions = tuple(original_img.shape)\n>>> width, height, depth = tuple(original_img.shape)\n>>> image_flattened = np.reshape(original_img, (width * height, depth))\n```", "```py\n>>> image_array_sample = shuffle(image_flattened, random_state=0)[:1000]\n>>> estimator = KMeans(n_clusters=64, random_state=0)\n>>> estimator.fit(image_array_sample)\n```", "```py\n>>> cluster_assignments = estimator.predict(image_flattened)\n```", "```py\n>>> compressed_palette = estimator.cluster_centers_\n>>> compressed_img = np.zeros((width, height, compressed_palette.shape[1]))\n>>> label_idx = 0\n>>> for i in range(width):\n>>>     for j in range(height):\n>>>         compressed_img[i][j] = compressed_palette[cluster_assignments[label_idx]]\n>>>         label_idx += 1\n>>> plt.subplot(122)\n>>> plt.title('Original Image')\n>>> plt.imshow(original_img)\n>>> plt.axis('off')\n>>> plt.subplot(121)\n>>> plt.title('Compressed Image')\n>>> plt.imshow(compressed_img)\n>>> plt.axis('off')\n>>> plt.show()\n```", "```py\n>>> import numpy as np\n>>> import mahotas as mh\n>>> from mahotas.features import surf\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.metrics import *\n>>> from sklearn.cluster import MiniBatchKMeans\n>>> import glob\n```", "```py\n>>> all_instance_filenames = []\n>>> all_instance_targets = []\n>>> for f in glob.glob('cats-and-dogs-img/*.jpg'):\n>>>     target = 1 if 'cat' in f else 0\n>>>     all_instance_filenames.append(f)\n>>>     all_instance_targets.append(target)\n>>> surf_features = []\n>>> counter = 0\n>>> for f in all_instance_filenames:\n>>>     print 'Reading image:', f\n>>>     image = mh.imread(f, as_grey=True)\n>>>     surf_features.append(surf.surf(image)[:, 5:])\n\n>>> train_len = int(len(all_instance_filenames) * .60)\n>>> X_train_surf_features = np.concatenate(surf_features[:train_len])\n>>> X_test_surf_feautres = np.concatenate(surf_features[train_len:])\n>>> y_train = all_instance_targets[:train_len]\n>>> y_test = all_instance_targets[train_len:]\n```", "```py\n>>> n_clusters = 300\n>>> print 'Clustering', len(X_train_surf_features), 'features'\n>>> estimator = MiniBatchKMeans(n_clusters=n_clusters)\n>>> estimator.fit_transform(X_train_surf_features)\n```", "```py\n>>> X_train = []\n>>> for instance in surf_features[:train_len]:\n>>>     clusters = estimator.predict(instance)\n>>>     features = np.bincount(clusters)\n>>>     if len(features) < n_clusters:\n>>>         features = np.append(features, np.zeros((1, n_clusters-len(features))))\n>>>     X_train.append(features)\n\n>>> X_test = []\n>>> for instance in surf_features[train_len:]:\n>>>     clusters = estimator.predict(instance)\n>>>     features = np.bincount(clusters)\n>>>     if len(features) < n_clusters:\n>>>         features = np.append(features, np.zeros((1, n_clusters-len(features))))\n>>>     X_test.append(features)\n```", "```py\n>>> clf = LogisticRegression(C=0.001, penalty='l2')\n>>> clf.fit_transform(X_train, y_train)\n>>> predictions = clf.predict(X_test)\n>>> print classification_report(y_test, predictions)\n>>> print 'Precision: ', precision_score(y_test, predictions)\n>>> print 'Recall: ', recall_score(y_test, predictions)\n>>> print 'Accuracy: ', accuracy_score(y_test, predictions)\n\nReading image: dog.9344.jpg\n...\nReading image: dog.8892.jpg\nClustering 756914 features\n             precision    recall  f1-score   support\n\n          0       0.71      0.76      0.73       392\n          1       0.75      0.70      0.72       408\n\navg / total       0.73      0.73      0.73       800\n\nPrecision:  0.751322751323\nRecall:  0.696078431373\nAccuracy:  0.7275\n```"]
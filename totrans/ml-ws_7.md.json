["```py\n    import seaborn as sns\n    titanic = sns.load_dataset('titanic')\n    titanic.head(10)\n    ```", "```py\n    X = titanic.drop('survived',axis = 1)\n    Y = titanic['survived']\n    ```", "```py\n    X.shape\n    ```", "```py\n    (891, 14)\n    ```", "```py\n    Y.shape\n    ```", "```py\n    (891,)\n    ```", "```py\n    import seaborn as sns\n    from sklearn.preprocessing import LabelEncoder\n    titanic = sns.load_dataset('titanic')\n    X = titanic[['sex','age','fare','class',\\\n                 'embark_town','alone']].copy()\n    X.shape\n    ```", "```py\n    (891, 6)\n    ```", "```py\n    print(\"Sex: \" + str(X['sex'].isnull().sum()))\n    print(\"Age: \" + str(X['age'].isnull().sum()))\n    print(\"Fare: \" + str(X['fare'].isnull().sum()))\n    print(\"Class: \" + str(X['class'].isnull().sum()))\n    print(\"Embark town: \" + str(X['embark_town'].isnull().sum()))\n    print(\"Alone: \" + str(X['alone'].isnull().sum()))\n    ```", "```py\n    Sex: 0\n    Age: 177\n    Fare: 0\n    Class: 0\n    Embark town: 2\n    Alone: 0\n    ```", "```py\n    mean = X['age'].mean()\n    mean =round(mean)\n    X['age'].fillna(mean,inplace = True)\n    ```", "```py\n    features = [\"age\", \"fare\"]\n    for feature in features:\n        min_ = X[feature].mean() - (3 * X[feature].std())\n        max_ = X[feature].mean() + (3 * X[feature].std())\n        X = X[X[feature] <= max_]\n        X = X[X[feature] >= min_]\n        print(feature,    \":\", X.shape)\n    ```", "```py\n    age: (884, 6)\n    fare: (864, 6)\n    ```", "```py\n    features = [\"sex\", \"class\", \"embark_town\", \"alone\"]\n    for feature in features:\n        count_ = X[feature].value_counts()\n        print(feature)\n        print(count_, \"\\n\")\n    ```", "```py\n    enc = LabelEncoder()\n    X[\"sex\"] = enc.fit_transform(X['sex'].astype('str'))\n    X[\"class\"] = enc.fit_transform(X['class'].astype('str'))\n    X[\"embark_town\"] = enc.fit_transform(X['embark_town'].\\\n                                         astype('str'))\n    X[\"alone\"] = enc.fit_transform(X['alone'].astype('str'))\n    ```", "```py\n    X.head()\n    ```", "```py\n    X = (X - X.min()) / (X.max() - X.min())\n    X.head(10)\n    ```", "```py\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import numpy as np\n    ```", "```py\n    data = pd.read_csv(\"wholesale_customers_data.csv\")\n    ```", "```py\n    data.isnull().sum()\n    ```", "```py\n    Channel             0\n    Region              0\n    Fresh               0\n    Milk                0\n    Grocery             0\n    Frozen              0\n    Detergents_Paper    0\n    Delicassen          0\n    dtype: int64\n    ```", "```py\n    outliers = {}\n    for i in range(data.shape[1]):\n        min_t = data[data.columns[i]].mean() \\\n                - (3 * data[data.columns[i]].std())\n        max_t = data[data.columns[i]].mean() \\\n                + (3 * data[data.columns[i]].std())\n        count = 0\n        for j in data[data.columns[i]]:\n            if j < min_t or j > max_t:\n                count += 1\n        outliers[data.columns[i]] = [count,data.shape[0]-count]\n    print(outliers)\n    ```", "```py\n    {'Channel': [0, 440], 'Region': [0, 440], 'Fresh': [7, 433], 'Milk': [9, 431], 'Grocery': [7, 433], 'Frozen': [6, 434], 'Detergents_Paper': [10, 430], 'Delicassen': [4, 436]}\n    ```", "```py\n    plt.hist(data[\"Fresh\"])\n    plt.show()\n    ```", "```py\n    plt.figure(figsize=(8,8))\n    plt.pie(outliers[\"Detergents_Paper\"],autopct=\"%.2f\")\n    plt.show()\n    ```", "```py\n    data_standardized = (data - data.mean())/data.std()\n    data_standardized.head()\n    ```", "```py\n    from sklearn.cluster import KMeans\n    ```", "```py\n    ideal_k = []\n    for i in range(1,21):\n        est_kmeans = KMeans(n_clusters=i, random_state=0)\n        est_kmeans.fit(data_standardized)\n        ideal_k.append([i,est_kmeans.inertia_])\n    ideal_k = np.array(ideal_k)\n    ```", "```py\n    plt.plot(ideal_k[:,0],ideal_k[:,1])\n    plt.show()\n    ```", "```py\n    est_kmeans = KMeans(n_clusters=6, random_state = 0)\n    est_kmeans.fit(data_standardized)\n    pred_kmeans = est_kmeans.predict(data_standardized)\n    ```", "```py\n    plt.subplots(1, 2, sharex='col', \\\n                 sharey='row', figsize=(16,8))\n    plt.scatter(data.iloc[:,5], data.iloc[:,3], \\\n                c=pred_kmeans, s=20)\n    plt.xlim([0, 20000])\n    plt.ylim([0, 20000])\n    plt.xlabel('Frozen')\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.iloc[:,4], data.iloc[:,3], \\\n                c=pred_kmeans, s=20)\n    plt.xlim([0, 20000])\n    plt.ylim([0,20000])\n    plt.xlabel('Grocery')\n    plt.ylabel('Milk')\n    plt.show()\n    ```", "```py\n    from sklearn.cluster import MeanShift\n    ```", "```py\n    est_meanshift = MeanShift(0.4)\n    est_meanshift.fit(data_standardized)\n    pred_meanshift = est_meanshift.predict(data_standardized)\n    ```", "```py\n    plt.subplots(1, 2, sharex='col', \\\n                 sharey='row', figsize=(16,8))\n    plt.scatter(data.iloc[:,5], data.iloc[:,3], \\\n                c=pred_meanshift, s=20)\n    plt.xlim([0, 20000])\n    plt.ylim([0,20000])\n    plt.xlabel('Frozen')\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.iloc[:,4], data.iloc[:,3], \\\n                c=pred_meanshift, s=20)\n    plt.xlim([0, 20000])\n    plt.ylim([0,20000])\n    plt.xlabel('Grocery')\n    plt.ylabel('Milk')\n    plt.show()\n    ```", "```py\n    from sklearn.cluster import DBSCAN\n    ```", "```py\n    est_dbscan = DBSCAN(eps=0.8)\n    pred_dbscan = est_dbscan.fit_predict(data_standardized)\n    ```", "```py\n    plt.subplots(1, 2, sharex='col', \\\n                 sharey='row', figsize=(16,8))\n    plt.scatter(data.iloc[:,5], data.iloc[:,3], \\\n                c=pred_dbscan, s=20)\n    plt.xlim([0, 20000])\n    plt.ylim([0,20000])\n    plt.xlabel('Frozen')\n    plt.subplot(1, 2, 1)\n    plt.scatter(data.iloc[:,4], data.iloc[:,3], \\\n                c=pred_dbscan, s=20)\n    plt.xlim([0, 20000])\n    plt.ylim([0,20000])\n    plt.xlabel('Grocery')\n    plt.ylabel('Milk')\n    plt.show()\n    ```", "```py\n    from sklearn.metrics import silhouette_score\n    from sklearn.metrics import calinski_harabasz_score\n    ```", "```py\n    kmeans_score = silhouette_score(data_standardized, \\\n                                    pred_kmeans, \\\n                                    metric='euclidean')\n    meanshift_score = silhouette_score(data_standardized, \\\n                                       pred_meanshift, \\\n                                       metric='euclidean')\n    dbscan_score = silhouette_score(data_standardized, \\\n                                    pred_dbscan, \\\n                                    metric='euclidean')\n    print(kmeans_score, meanshift_score, dbscan_score)\n    ```", "```py\n    kmeans_score = calinski_harabasz_score(data_standardized, \\\n                                           pred_kmeans)\n    meanshift_score = calinski_harabasz_score(data_standardized, \\\n                                              pred_meanshift)\n    dbscan_score = calinski_harabasz_score(data_standardized, \\\n                                           pred_dbscan)\n    print(kmeans_score, meanshift_score, dbscan_score)\n    ```", "```py\n    from sklearn.datasets import load_digits\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.model_selection import KFold\n    ```", "```py\n    digits = load_digits()\n    X = pd.DataFrame(digits.data)\n    Y = pd.DataFrame(digits.target)\n    print(X.shape, Y.shape)\n    ```", "```py\n    (1797, 64) (1797, 1)\n    ```", "```py\n    X_new, X_test, \\\n    Y_new, Y_test = train_test_split(X, Y, test_size=0.2)\n    print(X_new.shape, Y_new.shape, X_test.shape, Y_test.shape)\n    ```", "```py\n    (1437, 64) (1437, 1) (360, 64) (360, 1)\n    ```", "```py\n    dev_size = X_test.shape[0]/X_new.shape[0]\n    print(dev_size)\n    ```", "```py\n    X_train, X_dev, \\\n    Y_train, Y_dev = train_test_split(X_new, Y_new, \\\n                                      test_size = dev_size)\n    print(X_train.shape, Y_train.shape, X_dev.shape, \\\n          Y_dev.shape, X_test.shape, Y_test.shape)\n    ```", "```py\n    (1077, 64) (1077, 1) (360, 64) (360, 1) (360, 64) (360, 1)\n    ```", "```py\n    X_new_2, X_test_2, \\\n    Y_new_2, Y_test_2 = train_test_split(X, Y, test_size=0.1)\n    ```", "```py\n    kf = KFold(n_splits = 10)\n    splits = kf.split(X_new_2)\n    ```", "```py\n    for train_index, dev_index in splits:\n        X_train_2, X_dev_2 = X_new_2.iloc[train_index,:], \\\n                             X_new_2.iloc[dev_index,:]\n        Y_train_2, Y_dev_2 = Y_new_2.iloc[train_index,:], \\\n                             Y_new_2.iloc[dev_index,:]\n    ```", "```py\n    print(X_train_2.shape, Y_train_2.shape, X_dev_2.shape, \\\n          Y_dev_2.shape, X_test_2.shape, Y_test_2.shape)\n    ```", "```py\n    (1456, 64) (1456, 1) (161, 64) (161, 1) (180, 64) (180, 1)\n    ```", "```py\n    from sklearn.datasets import load_digits\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn import tree\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    ```", "```py\n    digits = load_digits()\n    X = pd.DataFrame(digits.data)\n    Y = pd.DataFrame(digits.target)\n    ```", "```py\n    X_train, X_test, \\\n    Y_train, Y_test = train_test_split(X,Y, test_size = 0.2,\\\n                                       random_state = 0)\n    ```", "```py\n    model = tree.DecisionTreeClassifier(random_state = 0)\n    model = model.fit(X_train, Y_train)\n    Y_pred = model.predict(X_test)\n    ```", "```py\n    confusion_matrix(Y_test, Y_pred)\n    ```", "```py\n    accuracy = accuracy_score(Y_test, Y_pred)\n    print(\"accuracy:\", accuracy)\n    ```", "```py\n    Y_test_2 = Y_test[:]\n    Y_test_2[Y_test_2 != 6] = 1\n    Y_test_2[Y_test_2 == 6] = 0\n    Y_pred_2 = Y_pred\n    Y_pred_2[Y_pred_2 != 6] = 1\n    Y_pred_2[Y_pred_2 == 6] = 0\n    precision = precision_score(Y_test_2, Y_pred_2)\n    print(\"precision:\", precision)\n    recall = recall_score(Y_test_2, Y_pred_2)\n    print(\"recall:\", recall)\n    ```", "```py\n    precision: 0.9841269841269841\n    recall: 0.9810126582278481\n    ```", "```py\n    from sklearn.datasets import load_digits\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    import numpy as np\n    from sklearn import tree\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    digits = load_digits()\n    X = pd.DataFrame(digits.data)\n    Y = pd.DataFrame(digits.target)\n    ```", "```py\n    X_new, X_test, \\\n    Y_new, Y_test = train_test_split(X, Y, test_size = 0.1,\\\n                                     random_state = 101)\n    test_size = X_test.shape[0] / X_new.shape[0]\n    X_train, X_dev, \\\n    Y_train, Y_dev = train_test_split(X_new, Y_new, \\\n                                      test_size= test_size, \\\n                                      random_state = 101)\n    print(X_train.shape, Y_train.shape, X_dev.shape, \\\n          Y_dev.shape, X_test.shape, Y_test.shape)\n    ```", "```py\n    (1437, 64) (1437, 1) (180, 64) (180, 1) (180, 64) (180, 1)\n    ```", "```py\n    np.random.seed(101)\n    indices_train = np.random.randint(0, len(X_train), 90)\n    indices_dev = np.random.randint(0, len(X_dev), 90)\n    X_train_dev = pd.concat([X_train.iloc[indices_train,:], \\\n                             X_dev.iloc[indices_dev,:]])\n    Y_train_dev = pd.concat([Y_train.iloc[indices_train,:], \\\n                             Y_dev.iloc[indices_dev,:]])\n    print(X_train_dev.shape, Y_train_dev.shape)\n    ```", "```py\n    (180, 64) (180, 1)\n    ```", "```py\n    model = tree.DecisionTreeClassifier(random_state = 101)\n    model = model.fit(X_train, Y_train)\n    ```", "```py\n    sets = [\"Training\", \"Train/dev\", \"Validation\", \"Testing\"]\n    X_sets = [X_train, X_train_dev, X_dev, X_test]\n    Y_sets = [Y_train, Y_train_dev, Y_dev, Y_test]\n    scores = {}\n    for i in range(0, len(X_sets)):\n        pred = model.predict(X_sets[i])\n        score = accuracy_score(Y_sets[i], pred)\n        scores[sets[i]] = score\n    print(scores)\n    ```", "```py\n    {'Training': 1.0, 'Train/dev': 0.9444444444444444, 'Validation': 0.8833333333333333, 'Testing': 0.8833333333333333}\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.naive_bayes import GaussianNB\n    ```", "```py\n    data = pd.read_csv(\"census_income_dataset_preprocessed.csv\")\n    X = data.drop(\"target\", axis=1)\n    Y = data[\"target\"]\n    ```", "```py\n    X_new, X_test, \\\n    Y_new, Y_test = train_test_split(X, Y, test_size=0.1, \\\n                                     random_state=101)\n    test_size = X_test.shape[0] / X_new.shape[0]\n    X_train, X_dev, \\\n    Y_train, Y_dev = train_test_split(X_new, Y_new, \\\n                                      test_size=test_size, \\\n                                      random_state=101)\n    print(X_train.shape, Y_train.shape, X_dev.shape, \\\n          Y_dev.shape, X_test.shape, Y_test.shape)\n    ```", "```py\n    (26047, 9) (26047,) (3257, 9) (3257,) (3257, 9) (3257,)\n    ```", "```py\n    model_NB = GaussianNB()\n    model_NB.fit(X_train,Y_train)\n    ```", "```py\n    pred_1 = model_NB.predict([[39,6,13,4,0,2174,0,40,38]])\n    print(pred_1)\n    ```", "```py\n    [0]\n    ```", "```py\n    from sklearn.tree import DecisionTreeClassifier\n    ```", "```py\n    model_tree = DecisionTreeClassifier(random_state=101)\n    model_tree.fit(X_train,Y_train)\n    ```", "```py\n    pred_2 = model_tree.predict([[39,6,13,4,0,2174,0,40,38]])\n    print(pred_2)\n    ```", "```py\n    [0]\n    ```", "```py\n    from sklearn.svm import SVC\n    ```", "```py\n    model_svm = SVC()\n    model_svm.fit(X_train, Y_train)\n    ```", "```py\n    pred_3 = model_svm.predict([[39,6,13,4,0,2174,0,40,38]])\n    print(pred_3)\n    ```", "```py\n    [0]\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    data = pd.read_csv(\"census_income_dataset_preprocessed.csv\")\n    X = data.drop(\"target\", axis=1)\n    Y = data[\"target\"]\n    ```", "```py\n    X_new, X_test, \\\n    Y_new, Y_test = train_test_split(X, Y, test_size=0.1, \\\n                                     random_state=101)\n    test_size = X_test.shape[0] / X_new.shape[0]\n    X_train, X_dev, \\\n    Y_train, Y_dev = train_test_split(X_new, Y_new, \\\n                                      test_size=test_size, \\\n                                      random_state=101)\n    print(X_train.shape, X_dev.shape, X_test.shape, \\\n          Y_train.shape, Y_dev.shape, Y_test.shape)\n    ```", "```py\n    (26047, 9) (3257, 9) (3257, 9) (26047,) (3257,) (3257,)\n    ```", "```py\n    model = MLPClassifier(random_state=101)\n    model = model.fit(X_train, Y_train)\n    ```", "```py\n    sets = [\"Training\", \"Validation\", \"Testing\"]\n    X_sets = [X_train, X_dev, X_test]\n    Y_sets = [Y_train, Y_dev, Y_test]\n    accuracy = {}\n    for i in range(0,len(X_sets)):\n        pred = model.predict(X_sets[i])\n        score = accuracy_score(Y_sets[i], pred)\n        accuracy[sets[i]] = score\n    print(accuracy)\n    ```", "```py\n    {'Training': 0.8465909090909091, 'Validation': 0.8246314496314496, 'Testing': 0.8415719987718759}\n    ```", "```py\n    import pandas as pd\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.metrics import precision_score\n    ```", "```py\n    data = pd.read_csv(\"bank-full-dataset.csv\")\n    data.head(10)\n    ```", "```py\n    data.isnull().sum()\n    ```", "```py\n    data = data.drop([\"contact\", \"poutcome\"], axis=1)\n    ```", "```py\n    enc = LabelEncoder()\n    features_to_convert=[\"job\",\"marital\",\"default\",\\\n                         \"housing\",\"loan\",\"month\",\"y\"]\n    for i in features_to_convert:\n        data[i] = enc.fit_transform(data[i].astype('str'))\n    ```", "```py\n    data['education'] = data['education'].fillna('unknown')\n    encoder = ['unknown','primary','secondary','tertiary']\n    for i, word in enumerate(encoder):\n        data['education'] = data['education'].astype('str').\\\n                            str.replace(word, str(i))\n    data['education'] = data['education'].astype('int64')\n    data.head()\n    ```", "```py\n    outliers = {}\n    for i in range(data.shape[1]):\n        min_t = data[data.columns[i]].mean() \\\n                - (3 * data[data.columns[i]].std())\n        max_t = data[data.columns[i]].mean() \\\n                + (3 * data[data.columns[i]].std())\n        count = 0\n        for j in data[data.columns[i]]:\n            if j < min_t or j > max_t:\n                count += 1\n        outliers[data.columns[i]] = [count, data.shape[0]]\n    print(outliers)\n    ```", "```py\n    {'age': [381, 45211], 'job': [0, 45211], 'marital': [0, 45211], 'education': [0, 45211], 'default': [815, 45211], 'balance': [745, 45211], 'housing': [0, 45211], 'loan': [0, 45211], 'day': [0, 45211], 'month': [0, 45211], 'duration': [963, 45211], 'campaign': [840, 45211], 'pdays': [1723, 45211], 'previous': [582, 45211], 'y': [0, 45211]}\n    ```", "```py\n    X = data.drop(\"y\", axis = 1)\n    Y = data[\"y\"]\n    ```", "```py\n    X_new, X_test, \\\n    Y_new, Y_test = train_test_split(X, Y, test_size=0.2,\\\n                                     random_state = 0)\n    test_size = X_test.shape[0] / X_new.shape[0]\n    X_train, X_dev, \\\n    Y_train, Y_dev = train_test_split(X_new, Y_new, \\\n                                      test_size=test_size,\\\n                                      random_state = 0)\n    print(X_train.shape, Y_train.shape, X_dev.shape, \\\n        Y_dev.shape, X_test.shape, Y_test.shape)\n    ```", "```py\n    (27125, 14) (27125,) (9043, 14) (9043,) (9043, 14) (9043,)\n    ```", "```py\n    model_tree = DecisionTreeClassifier(random_state = 2)\n    model_tree.fit(X_train, Y_train)\n    ```", "```py\n    model_NN = MLPClassifier(random_state = 2)\n    model_NN.fit(X_train, Y_train)\n    ```", "```py\n    X_sets = [X_train, X_dev, X_test]\n    Y_sets = [Y_train, Y_dev, Y_test]\n    precision = []\n    for i in range(0, len(X_sets)):\n        pred = model_tree.predict(X_sets[i])\n        score = precision_score(Y_sets[i], pred)\n        precision.append(score)\n    print(precision)\n    ```", "```py\n    [1.0, 0.43909348441926344, 0.4208059981255858]\n    ```", "```py\n    X_sets = [X_train, X_dev, X_test]\n    Y_sets = [Y_train, Y_dev, Y_test]\n    precision = []\n    for i in range(0, len(X_sets)):\n        pred = model_NN.predict(X_sets[i])\n        score = precision_score(Y_sets[i], pred)\n        precision.append(score)\n    print(precision)\n    ```", "```py\n    [0.35577647236029525, 0.35199283475145543, 0.3470483005366726]\n    ```", "```py\n    model_tree = DecisionTreeClassifier(random_state = 2, \\\n                                        min_samples_leaf=100, \\\n                                        max_depth=100)\n    model_tree.fit(X_train, Y_train)\n    ```", "```py\n    model_NN = \\\n        MLPClassifier(random_state = 2, max_iter=1000,\\\n                      hidden_layer_sizes = [100,100,50,25,25], \\\n                      tol=1e-4)\n    model_NN.fit(X_train, Y_train)\n    ```", "```py\n    X_sets = [X_train, X_dev, X_test]\n    Y_sets = [Y_train, Y_dev, Y_test]\n    precision = []\n    for i in range(0, len(X_sets)):\n        pred = model_tree.predict(X_sets[i])\n        score = precision_score(Y_sets[i], pred)\n        precision.append(score)\n    print(precision)\n    ```", "```py\n    [0.6073670992046881, 0.5691158156911582, 0.5448113207547169]\n    ```", "```py\n    X_sets = [X_train, X_dev, X_test]\n    Y_sets = [Y_train, Y_dev, Y_test]\n    precision = []\n    for i in range(0, len(X_sets)):\n        pred = model_NN.predict(X_sets[i])\n        score = precision_score(Y_sets[i], pred)\n        precision.append(score)\n    print(precision)\n    ```", "```py\n    [0.759941089837997, 0.5920398009950248, 0.5509259259259259]\n    ```", "```py\n    path = os.getcwd() + \"/final_model.pkl\"\n    file = open(path, \"wb\")\n    pickle.dump(model_NN, file)\n    ```", "```py\n    from sklearn.neural_network import MLPClassifier\n    import pickle\n    import os\n    ```", "```py\n    path = os.getcwd() + \"/final_model.pkl\"\n    file = open(path, \"rb\")\n    model = pickle.load(file)\n    ```", "```py\n    pred = model.predict([[42,2,0,0,1,2,1,0,5,8,380,1,-1,0]])\n    print(pred)\n    ```", "```py\n    import pickle\n    import os\n    ```", "```py\n    Class NN_Model(object):\n        def __init__(self):\n            path = os.getcwd() + \"/model_exercise.pkl\"\n            file = open(path, \"rb\")\n            self.model = pickle.load(file)\n        def predict(self, age, job, marital, education, \\\n                    default, balance, housing, loan, day, \\\n                    month, duration, campaign, pdays, previous):\n            X = [[age, job, marital, education, default, \\\n                  balance, housing, loan, day, month, \\\n                  duration, campaign, pdays, previous]]\n            return self.model.predict(X)\n    ```", "```py\n    from trainedModel import NN_Model\n    model = NN_Model()\n    age = 42\n    job = 2\n    marital = 0\n    education = 0\n    default = 1\n    balance = 2\n    housing = 1\n    loan = 0\n    day = 5\n    month = 8\n    duration = 380\n    campaign = 1\n    pdays = -1\n    previous = 0\n    ```", "```py\n    pred = model.predict(age=age, job=job, marital=marital, \\\n                         education=education, default=default, \\\n                         balance=balance, housing=housing, \\\n                         loan=loan, day=day, month=month, \\\n                         duration=duration, campaign=campaign, \\\n                         pdays=pdays, previous=previous)\n    print(pred)\n    ```", "```py\n    [0]\n    ```"]
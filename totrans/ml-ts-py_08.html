<html><head></head><body>
  <div id="_idContainer344">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-137" class="chapterTitle">Probabilistic Models for Time-Series</h1>
    <p class="normal">Probability is a<a id="_idIndexMarker690"/> measure of how likely something is to occur. In sales forecasting, an estimate of uncertainty is crucial because these forecasts, providing insights into cash flow, margin, and revenue, drive business decisions on which depend the financial stability and the livelihoods of employees. This is where probabilistic models for time-series come in. They help us make decisions when an estimate of certainty is important.</p>
    <p class="normal">In this chapter, I'll introduce Prophet, Markov models, and Fuzzy time-series models. At the end, we'll go through an applied exercise with these methods.</p>
    <p class="normal">Another application for probabilistic modeling is estimating counterfactuals, where we can estimate treatment effects in experiments. We'll discuss the concept of Bayesian Structural Time-Series Models, and we'll run through a practical example with a time-series in the practice section.</p>
    <p class="normal">We're going to cover the following topics:</p>
    <ul>
      <li class="bullet">Probabilistic Models for Time-Series</li>
      <li class="bullet">Prophet</li>
      <li class="bullet">Markov Models</li>
      <li class="bullet">Fuzzy Modeling</li>
      <li class="bullet">Bayesian Structural Time-Series Models</li>
      <li class="bullet">Python Exercise:<ul>
          <li class="bullet-l2">Prophet</li>
          <li class="bullet-l2">Markov Switching Model</li>
          <li class="bullet-l2">Fuzzy Time-Series</li>
          <li class="bullet-l2">Bayesian Structural Time-Series Models</li>
        </ul>
      </li>
    </ul>
    <p class="normal">We'll start with an introduction to probabilistic time-series predictions.</p>
    <h1 id="_idParaDest-138" class="title">Probabilistic Models for Time-Series</h1>
    <p class="normal">As mentioned <a id="_idIndexMarker691"/>in the introduction, probabilistic models can help us make decisions under uncertainty, and in situations where estimates have to <a id="_idIndexMarker692"/>come with quantified confidence, such as in financial forecasting, this can be crucial. For predictions of sales or cash flow, attaching probabilities to model predictions can make it easier for financial controllers and managers to act on the new information.</p>
    <p class="normal">Some well-known algorithms include Prophet, explicitly designed for monitoring operational metrics and <strong class="keyword">key </strong><strong class="keyword"><a id="_idIndexMarker693"/></strong><strong class="keyword">performance indicators</strong> (<strong class="keyword">KPIs</strong>), and Markov models. Others are stochastic deep learning models such as <strong class="keyword">DeepAR</strong> and <strong class="keyword">DeepState</strong>. Since <a id="_idIndexMarker694"/>we are dealing with deep learning models in <em class="chapterRef">Chapter 10</em>, <em class="italic">Deep Learning Models</em>, we'll not deal with them in detail in this chapter.</p>
    <p class="normal">The Prophet model<a id="_idIndexMarker695"/> comes from Facebook (Taylor and Letham, 2017) and is based on a decomposable model with interpretable parameters. A guiding design principle was that parameters can be intuitively adjusted by analysts.</p>
    <p class="normal">Both Prophet and the <a id="_idIndexMarker696"/>Silverkite algorithm, which we introduced in <em class="chapterRef">Chapter 7</em>, <em class="italic">Machine Learning Models for Time-Series</em>, aim for accurate predictions with time-series that can have changing trends, seasonality, and recurring events (such as holidays), and short-term effects, and are therefore well suited for many applications in data science, where the focus is on tasks such as resource planning, optimizing financial decisions, and tracking progress for operational analysis – typical tasks for operations research.</p>
    <p class="normal">Other types of models of particular interest within the application of time-series include Markov models, which we'll discuss in a dedicated section.</p>
    <p class="normal"><strong class="keyword">Bayesian Structural Time-Series</strong> (<strong class="keyword">BSTS</strong>) models, which<a id="_idIndexMarker697"/> we mentioned in <em class="chapterRef">Chapter 6</em>, <em class="italic">Unsupervised Models for Time-Series</em>, allow the quantification of the posterior uncertainty of the individual components, control the variance of the components, and impose prior beliefs on the model. The BSTS model is a technique that can be used for feature selection, time-series forecasting, and inferring causal relationships. This last point, causal inference, is another use case for probabilistic models in time-series. Understanding the impact of interventions can be important, for example, with A/B tests.</p>
    <p class="normal">The following plot illustrates the<a id="_idIndexMarker698"/> popularity of a few selected probabilistic libraries suitable for time-series predictions:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_01.png" alt="probabilistic-star_history.png"/></figure>
    <p class="packt_figref">Figure 9.1: Libraries for the probabilistic modeling of time-series</p>
    <p class="normal">You can see<a id="_idIndexMarker699"/> that, of these three libraries, <code class="Code-In-Text--PACKT-">pyFTS</code> outranks the other two. I haven't included the <code class="Code-In-Text--PACKT-">statsmodels</code> library, which includes a few probabilistic models. Neither have I included Prophet. Both <code class="Code-In-Text--PACKT-">statsmodels</code> and Prophet would have outstripped HMMs by far, a library for Hidden Markov Models, and Pints, a library for noisy time-series.</p>
    <p class="normal">Neither have I included neural network or deep learning libraries such as TensorFlow Probability or Gluon-TS. Deep learning will be the topic of <em class="chapterRef">Chapter 10</em>, <em class="italic">Deep Learning for Time-Series</em>.</p>
    <p class="normal">Let's start with a forecasting model in Prophet!</p>
    <h1 id="_idParaDest-139" class="title">Prophet</h1>
    <p class="normal">Facebook's Prophet is both a Python/R library and the algorithm that comes with it. The algorithm was published in 2017 ("Forecasting at Scale" by Sean Taylor and Benjamin Letham). The <a id="_idIndexMarker700"/>authors write that the problems of forecasting and anomaly detection in practice involve the complexity of handling a variety of idiosyncratic forecasting problems at Facebook with piecewise trends, multiple seasonalities, and floating holidays, and building trust across the organization in these forecasts.</p>
    <p class="normal">With these<a id="_idIndexMarker701"/> goals in mind, Prophet was designed to be scalable to many time-series, flexible enough for a wide range of business-relevant, possibly idiosyncratic time-series, and at the same time intuitive enough to be configurable by domain experts who might have little knowledge of time-series methods.</p>
    <p class="normal">The Prophet algorithm<a id="_idIndexMarker702"/> is similar to the <strong class="keyword">Generalized Additive Model</strong> (<strong class="keyword">GAM</strong>) and formalizes the relationship between the forecast for the three model components, trend (growth), seasonality, and holiday, as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_001.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">The error<a id="_idIndexMarker703"/> term epsilon represents the residual—idiosyncratic changes not accommodated by the model. All functions use time as the regressor. The three effects are additive; however, Sean Taylor and Benjamin Letham advise that multiplicative seasonality, where the seasonal effect is a factor that multiplies g(t), can be accomplished through a log transform.</p>
    <p class="normal">The trend or growth function can be linear or logistic for saturating growth. Both can incorporate piecewise effects through change points. The seasonality models periodic effects based on the Fourier series.</p>
    <p class="normal">Change point selection in Prophet is automated. Parameters are optimized via the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm, as implemented in the Stan platform for statistical modeling.</p>
    <p class="normal">Probabilistic models bring the advantage of providing a measure of certainty with the prediction; however, their predictions are not necessarily better than those of non-probabilistic models. Benchmark results of Prophet against other models have seen mixed results.</p>
    <p class="normal">In their 2020 paper "<em class="italic">A Worrying Analysis of Probabilistic Time-Series Models for Sales Forecasting</em>", Seungjae Jung and others validated probabilistic time-series models on a large-scale dataset. The univariate time-series consists of the daily sales from an e-commerce website.</p>
    <p class="normal">They compared two deep-learning probabilistic models, DeepAR and DeepState, and Prophet to baseline models that <a id="_idIndexMarker704"/>comprised a <strong class="keyword">moving average</strong> (<strong class="keyword">MA</strong>), <strong class="keyword">linear regression</strong> (<strong class="keyword">LR</strong>), a <strong class="keyword">multi-layer perceptron</strong> (<strong class="keyword">MLP</strong>), and <strong class="keyword">Seasonal ARIMA</strong> (<strong class="keyword">SARIMA</strong>). You <a id="_idIndexMarker705"/>should remember from <em class="chapterRef">Chapter 5</em>, <em class="italic">Moving Averages and Autoregressive Models</em>, that the MA is a simple unweighted <a id="_idIndexMarker706"/>mean of preceding days. They tried 72 different<a id="_idIndexMarker707"/> hyperparameters for prophet and all baseline models.</p>
    <p class="normal">They <a id="_idIndexMarker708"/>found that probabilistic models in their test failed to <a id="_idIndexMarker709"/>outperform even the simplest baseline models such as MLP and LR in terms of <strong class="keyword">root mean squared error</strong> (<strong class="keyword">RMSE</strong>) and <strong class="keyword">mean absolute percentage error</strong> (<strong class="keyword">MAPE</strong>). Overall, Prophet performed the worst of all models. As always, model performance depends on the dataset and the task at hand—there's no silver bullet.</p>
    <p class="normal">Let's see how Markov Models work!</p>
    <h1 id="_idParaDest-140" class="title">Markov Models</h1>
    <p class="normal">A Markov chain<a id="_idIndexMarker710"/> is a probabilistic model describing a sequence of possible events that satisfies the Markov property.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="keyword">Markov property</strong>: In a <a id="_idIndexMarker711"/>sequence or stochastic process that possesses the Markov property, the probability of each event depends only on the immediately preceding state (rather than earlier states). These sequences <a id="_idIndexMarker712"/>or processes <a id="_idIndexMarker713"/>can also be called <strong class="keyword">Markovian</strong>, or a <strong class="keyword">Markov Process</strong>.</p>
      <p class="Information-Box--PACKT-">Named after Russian mathematician Andrey Markov, the Markov property is very desirable since it significantly reduces the complexity of a problem. In forecasting, instead of taking into account all previous states, t-1, t-2, …, 0, only t-1 is considered.</p>
      <p class="Information-Box--PACKT-">Similarly, the <strong class="keyword">Markov assumption</strong>, for <a id="_idIndexMarker714"/>a mathematical or machine learning model is that the sequence satisfies the Markov property. In models such as the Markov chain and Hidden Markov model, the process or sequence is assumed to be a Markov process.</p>
    </div>
    <p class="normal">In a <strong class="keyword">discrete-time Markov chain</strong> (<strong class="keyword">DTMC</strong>), the sequence (or chain) transitions between states at <a id="_idIndexMarker715"/>discrete time steps. Markov chains can also be operating at continuous time steps. This<a id="_idIndexMarker716"/> less common model is called a <strong class="keyword">continuous-time Markov chain</strong> (<strong class="keyword">CTMC</strong>). </p>
    <p class="normal">In a <strong class="keyword">hidden Markov model</strong> (<strong class="keyword">HMM</strong>), it is assumed that the process X follows unobservable <a id="_idIndexMarker717"/>states Y, which <a id="_idIndexMarker718"/>is another process whose behavior depends on X. The HMM models this latent or hidden process Y based on X.</p>
    <p class="normal">Yet another Markov-type model is the nonlinear regime-switching model (also: Markov switching model). Invented by James Hamilton in 1989, the regime-switching model specifically addresses situations of abrupt changes, where more conventional linear models would struggle to capture distinct behaviors. The regime-switching model is an autoregressive model, where the mean of the process switches between regimes.</p>
    <p class="normal">For the practical example, we'll follow a <code class="Code-In-Text--PACKT-">statsmodels</code> library implementation and build a model to replicate Hamilton's 1989 model. Hamilton modeled a time-series of the real gross national <a id="_idIndexMarker719"/>product (RGNP), a macroeconomic measure of the value of economic output adjusted for price changes, between 1951 and 1984. </p>
    <p class="normal">We'll use a Markov switching model of order 4, which can be written as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_002.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">For each state (or period), the regime transitions according to the following matrix of transition probabilities:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_003.png" alt="" style="height: 2.8em;"/></figure>
    <p class="normal"><img src="../Images/B17577_09_004.png" alt="" style="height: 1em;"/> is the probability of transitioning from regime i to regime j. </p>
    <p class="normal">In this case, we are modeling two regimes.</p>
    <p class="normal">In the next section, we'll discuss a Fuzzy approach to time-series modeling.</p>
    <h1 id="_idParaDest-141" class="title">Fuzzy Modeling</h1>
    <p class="normal">Fuzzy logic <a id="_idIndexMarker720"/>and fuzzy set theory were developed by Lotfi Zadeh in the 1960s and 70s while a professor at the University of California, Berkeley. Born to Persian and Jewish Russian parents in Baku, Azerbaijan, he completed his schooling in Tehran, Iran, and later moved to the USA, where he studied at MIT and Columbia. As a result, he was familiar with how concepts are understood in different cultures and expressed in different languages. This inspired his research approach to approximate reasoning and linguistic variables that he formalized as fuzzy theory.</p>
    <p class="normal"><strong class="keyword">Fuzzy set theory</strong> is an <a id="_idIndexMarker721"/>approach that can deal with problems relating to ambiguous, subjective, and imprecise judgments. Vagueness is inherent in everyday language, and fuzziness was invented to express this and work with it in an intuitive manner. Fuzzy logic expresses subjective belief and vagueness. It can and has been claimed that probability theory is a subset of fuzzy logic.</p>
    <p class="normal"><strong class="keyword">Fuzzy sets</strong> are sets whose elements have degrees of membership. In Fuzzy logic, instead of binary (Boolean) truth values, True and False, the unit interval [0, 1] is used as a basis for rules of inference. More formally, the membership function, which expresses the degree of <a id="_idIndexMarker722"/>certainty that an element belongs to the set, is characterized by a membership mapping function:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_005.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">For example, a well-known algorithm, <strong class="keyword">fuzzy c-means</strong> (James Bezdek, 1981), based on k-means, returns degrees of membership to clusters. This means that each point can belong to each cluster, but to varying degrees. This fuzzy set membership is in contrast to the so-called crisp partitioning typically returned by other clustering algorithms, where a point is either a member of a cluster or not.</p>
    <p class="normal">For fuzzy logic, all set operations, such as equality, sub- and superset, union, and intersection, had to be redefined. The union between two fuzzy sets (or relations) is defined as the max operation on each point, while the intersection is defined as the min operation. More formally, the union between two sets A and B, <img src="../Images/B17577_09_006.png" alt="" style="height: 1em;"/>, is defined as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_007.png" alt="" style="height: 1.8em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_09_008.png" alt="" style="height: 1em;"/> is the membership function for point x.</p>
    <p class="normal">Song and Chissom (1993) proposed a first-order, time-invariant fuzzy time-series model to forecast enrollments at the University of Alabama. This is formalized as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_009.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_09_010.png" alt="" style="height: 1em;"/> is the enrollment in year t, <em class="italic">R</em> is the union of fuzzy relations, and <img src="../Images/B17577_09_011.png" alt="" style="height: 0.16em;"/> is the fuzzy Max-Min composition operator.</p>
    <p class="normal">The <strong class="keyword">Max-Min composition operation</strong>, <img src="../Images/B17577_09_012.png" alt="" style="height: 0.8em;"/>, is obtained by taking the minimum term-by-term <a id="_idIndexMarker723"/>of the ith row of A and the jth column of B, and taking the maximum of these n minimums.</p>
    <p class="normal">This is illustrated in the diagram below (from the Matrix Multiplication page on Wikipedia):</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_02.png" alt="atrix multiplication diagram 2.svg"/></figure>
    <p class="packt_figref">Figure 9.2: The Max-Min composition operator</p>
    <p class="normal">The values at the positions marked with circles are calculated as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_013.png" alt="" style="height: 1.5em;"/></figure>
    <figure class="mediaobject"><span class="mediaobject"><img src="../Images/B17577_09_014.png" alt="" style="height: 1.5em;"/></span></figure>
    <p class="normal">In Song and Chissom's approach, relations between values at time t and values preceding it are extracted and carried forward for the forecast. A necessary preprocessing step in their algorithm is the conversion of time-series X into a fuzzy time-series Y. This is called fuzzification and consists of constraining an input from the set of real values to fuzzy memberships of a discrete set. This quantization can be performed by vector quantization methods <a id="_idIndexMarker724"/>such as Kohonen Self-Organizing Maps (SOM), an unsupervised machine learning method that produces a low-dimensional representation.</p>
    <p class="normal">While fuzzy<a id="_idIndexMarker725"/> time-series models have not enjoyed widespread application, they have been shown to be competitive in some applications to more traditional approaches, such as SARIMA (for example, Maria Elena et al., 2012). They work on discrete and continuous time-series and produce interpretable models for forecasting.</p>
    <p class="normal">In the following section, we'll do a few practice examples for probabilistic time-series predictions.</p>
    <h1 id="_idParaDest-142" class="title">Bayesian Structural Time-Series Models</h1>
    <p class="normal">In causal <a id="_idIndexMarker726"/>inference, we want to analyze the effect of a treatment. The treatment can be any action that interacts with the system or environment that we care about, from changing the colors of a button on a website to the release of a product. We have the choice of taking the action (for example, releasing the product), thereby observing the outcome under treatment, or not taking the action, where we observe the outcome under no treatment. This is illustrated in the diagram here:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_03.png" alt="../causal%20(1).png"/></figure>
    <p class="packt_figref">Figure 9.3: Causal effect of a treatment</p>
    <p class="normal">In the diagram, an action is taken or not (medicine is administered to a patient), and depending on whether the action is taken we see the patient recovering (cycling) or going into intensive care.</p>
    <p class="normal">A causal effect is the difference between what happens under treatment and what happens under no treatment. The problem with this is that we can't observe both potential outcomes at the same time.</p>
    <p class="normal">However, we <a id="_idIndexMarker727"/>can run an experiment to observe both potential outcomes under treatment and potential outcomes under no treatment, such as in an A/B test, where the treatment is given only to a subset of the total population and the treatment condition B can be compared against the control condition A.</p>
    <p class="normal">We can tabulate potential outcomes like this:</p>
    <table id="table001-6" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Unit</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Treatment status, <img src="../Images/B17577_09_015.png" alt="" style="height: 0.44em;"/></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Outcome under treatment, <img src="../Images/B17577_09_016.png" alt="" style="height: 0.49em;"/></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Outcome under no treatment, <img src="../Images/B17577_09_017.png" alt="" style="height: 0.49em;"/></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Covariates, <img src="../Images/B17577_09_018.png" alt="" style="height: 0.44em;"/></p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <figure class="mediaobject">1</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject">1</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject"><img src="../Images/B17577_09_04.png" alt="heck mark, Segoe UI Symbol font, character code 2714 hex."/></figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject">estimate</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject"><img src="../Images/B17577_09_04.png" alt="heck mark, Segoe UI Symbol font, character code 2714 hex."/></figure>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <figure class="mediaobject">2</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject">1</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject"><img src="../Images/B17577_09_04.png" alt="heck mark, Segoe UI Symbol font, character code 2714 hex."/></figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject">estimate</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject"><img src="../Images/B17577_09_04.png" alt="heck mark, Segoe UI Symbol font, character code 2714 hex."/></figure>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <figure class="mediaobject">3</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject">0</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject">estimate</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject"><img src="../Images/B17577_09_04.png" alt="heck mark, Segoe UI Symbol font, character code 2714 hex."/></figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject"><img src="../Images/B17577_09_04.png" alt="heck mark, Segoe UI Symbol font, character code 2714 hex."/></figure>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <figure class="mediaobject">4</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject">0</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject">estimate</figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject"><img src="../Images/B17577_09_04.png" alt="heck mark, Segoe UI Symbol font, character code 2714 hex."/></figure>
          </td>
          <td class="No-Table-Style">
            <figure class="mediaobject"><img src="../Images/B17577_09_04.png" alt="heck mark, Segoe UI Symbol font, character code 2714 hex."/></figure>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Figure 9.4: Potential outcomes from an experiment</p>
    <p class="normal">In the first column, <em class="italic">Unit</em>, we see the sample indexes. Each row refers to a separate unit or sample in the population. The second column (<em class="italic">Treatment status</em>) encodes if the treatment was administered (1) or not (0). In the third and fourth columns, <em class="italic">Outcome under treatment</em> and <em class="italic">Outcome under no treatment</em>, respectively, are registered.</p>
    <p class="normal">The marks <a id="_idIndexMarker728"/>show what should be obvious: when there is a treatment, we can observe the outcomes under treatment, but not the outcomes under no treatment. Inversely, when there is no treatment, we can observe the outcomes under no treatment, but not the outcomes under treatment.</p>
    <p class="normal">Finally, in the last column, there are additional variables that can help us in our model that are available irrespective of treatment or no treatment.</p>
    <p class="normal">With <strong class="keyword">Bayesian Structural Time-Series</strong> (<strong class="keyword">BSTS</strong>), the focus is on estimating the treatment effect in the absence of an experiment. We can estimate or impute the counterfactuals, which are the unknown potential outcomes of an experiment. This allows to compare the outcomes under treatment against outcomes under no treatment, and therefore to quantify the causal treatment effect.</p>
    <p class="normal">The model consists of three main components:</p>
    <ol>
      <li class="numbered">Kalman filter</li>
      <li class="numbered">Variable selection</li>
      <li class="numbered">Bayesian model averaging</li>
    </ol>
    <p class="normal">Kalman filters are used for time-series decomposition. This allows the modeling of trends, seasonality, and holidays. In the Bayesian variable selection step (the spike-and-slab technique), the most important regression predictors are selected. Finally, in the model averaging, the prediction results are combined.</p>
    <p class="normal">The application of BSTS models for change point and anomaly detection was described in the paper "<em class="italic">Predicting the Present with Bayesian Structural Time-Series</em>" (2013), by Steven L. Scott and Hal Varian.</p>
    <p class="normal">A paper outlining the application of BSTS for estimating the causal effect of interventions was published in 2015 by Google Research ("<em class="italic">Inferring causal impact using Bayesian structural time-series models</em>", by Kay H. Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, and Steven L. Scott). </p>
    <p class="normal">The mathematical details of this are beyond the scope of this chapter. Fortunately, we can use a Python library to apply BSTS models. We'll run through an example in a practice section of this chapter.</p>
    <p class="normal">We can practice now some of the theory that we have learned so far in this chapter.</p>
    <h1 id="_idParaDest-143" class="title">Python Exercise</h1>
    <p class="normal">Let's put into <a id="_idIndexMarker729"/>practice what we've learned in this chapter so far. We'll be doing a model in Prophet, a Markov Switching model, a Fuzzy time-series model, and a BSTS model.</p>
    <p class="normal">Let's get started with Prophet!</p>
    <h2 id="_idParaDest-144" class="title">Prophet</h2>
    <p class="normal">First, let's<a id="_idIndexMarker730"/> make sure we have everything installed that we need. Let's quickly install the required libraries. We can do this from the terminal (or similarly from the Anaconda navigator):</p>
    <pre class="programlisting con"><code class="hljs-con">pip install -U pandas-datareader plotly
</code></pre>
    <p class="normal">You'll need a <a id="_idIndexMarker731"/>recent version of pandas-datareader, otherwise you might get a <code class="Code-In-Text--PACKT-">RemoteDataError</code>.</p>
    <p class="normal">We'll use the Prophet model through Facebook's Prophet library. Let's install it:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install prophet
</code></pre>
    <p class="normal">Once this is done, we are set to go.</p>
    <p class="normal">In this example, we'll use the daily Yahoo closing stock values in this chapter that we used in several examples in <em class="chapterRef">Chapter 7</em>, <em class="italic">Machine Learning Models for Time-Series</em>. </p>
    <p class="normal">To recap, we can download the daily Yahoo stock history from 2001 to 2021 in pandas-datareader as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> pandas_datareader.data <span class="hljs-keyword">import</span> DataReader
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
yahoo_data = DataReader(<span class="hljs-string">'JPM'</span>,  <span class="hljs-string">'yahoo'</span>, datetime(<span class="hljs-number">2001</span>,<span class="hljs-number">6</span>,<span class="hljs-number">1</span>), datetime(<span class="hljs-number">2021</span>,<span class="hljs-number">6</span>,<span class="hljs-number">1</span>))
yahoo_df = yahoo_data[<span class="hljs-string">'Adj Close'</span>].to_frame().reset_index(<span class="hljs-string">'Date'</span>)
</code></pre>
    <p class="normal">This gives us a pandas DataFrame with two columns, the adjusted daily closing value and the date. Let's quickly check the datatypes of these two columns:</p>
    <pre class="programlisting code"><code class="hljs-code">yahoo_df.dtypes
</code></pre>
    <p class="normal">These are the datatypes:</p>
    <pre class="programlisting con"><code class="hljs-con">Date         datetime64[ns]
Adj Close           float64
dtype: object
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">Date</code> column is datetime in nanoseconds. <code class="Code-In-Text--PACKT-">Adj Close</code> is of type float.</p>
    <p class="normal">We'll feed this into the <code class="Code-In-Text--PACKT-">fit()</code> method for training:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> prophet <span class="hljs-keyword">import</span> Prophet
forecaster = Prophet()
forecaster.fit(
  yahoo_df.rename(columns={<span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">Date</strong></span><span class="hljs-string">"</span>: <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">ds</strong></span><span class="hljs-string">"</span>, <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">Adj Close</strong></span><span class="hljs-string">"</span>: <span class="hljs-string">"</span><span class="code-highlight"><strong class="hljs-slc">y</strong></span><span class="hljs-string">"</span>})
)
</code></pre>
    <p class="normal">We have to <a id="_idIndexMarker732"/>rename our columns <code class="Code-In-Text--PACKT-">ds</code> and <code class="Code-In-Text--PACKT-">y</code> in order to stick to the Prophet conventions. We have a trained Prophet model now.</p>
    <p class="normal">We'll then<a id="_idIndexMarker733"/> create a new DataFrame that will have future dates. We'll be able to stick this DataFrame into the <code class="Code-In-Text--PACKT-">predict()</code> method of the Prophet model:</p>
    <pre class="programlisting code"><code class="hljs-code">future = forecaster.make_future_dataframe(periods=<span class="hljs-number">90</span>)
</code></pre>
    <p class="normal">The forecast is calling the <code class="Code-In-Text--PACKT-">predict()</code> method with this new DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">forecast = forecaster.predict(future)
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">forecast</code> DataFrame contains the upper and lower confidence intervals alongside the forecast. The <code class="Code-In-Text--PACKT-">ds</code> columns is the date corresponding to the forecast.</p>
    <p class="normal">Let's plot the forecasts against the actual data:</p>
    <pre class="programlisting code"><code class="hljs-code">forecaster.plot(forecast, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>));
</code></pre>
    <p class="normal">Here's the plot:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.5: Forecast versus actual time-series (Prophet)</p>
    <p class="normal">You might <a id="_idIndexMarker734"/>want to compare this plot to the one in <em class="chapterRef">Chapter 7</em>,<em class="chapterRef"> </em><em class="italic">Machine Learning Models for Time-Series</em>. The actual data is thick and bold, while the forecast is thinner. The<a id="_idIndexMarker735"/> upper and lower confidence intervals are around the forecast.</p>
    <p class="normal">We can inspect the forecasts by looking at the DataFrame:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.6: Table of forecasts (Prophet)</p>
    <p class="normal">It is quite easy to get a first model, and there are many ways to tweak it.</p>
    <h2 id="_idParaDest-145" class="title">Markov Switching Model</h2>
    <p class="normal">For the <a id="_idIndexMarker736"/>Markov Switching model, we'll <a id="_idIndexMarker737"/>use the <code class="Code-In-Text--PACKT-">statsmodels</code> library. If you don't have it installed yet, you can install it like this:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install statsmodels
</code></pre>
    <p class="normal">We'll use a dataset with <code class="Code-In-Text--PACKT-">statsmodels</code> in this example. This is based on the <code class="Code-In-Text--PACKT-">statsmodels</code> tutorial on Markov switching autoregression models. We can get the dataset from the Stata Press publishing house on their website:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> statsmodels.tsa.regime_switching.tests.test_markov_autoregression <span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sn
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
dta = pd.read_stata(<span class="hljs-string">'https://www.stata-press.com/data/r14/rgnp.dta'</span>).iloc[<span class="hljs-number">1</span>:]
dta.index = pd.DatetimeIndex(dta.date, freq=<span class="hljs-string">'QS'</span>)
dta_hamilton = dta.rgnp 
</code></pre>
    <p class="normal">This gives us a pandas series of the RGNP, and the index annotates the dates. Let's quickly plot this:</p>
    <pre class="programlisting code"><code class="hljs-code">dta_hamilton.plot(title=<span class="hljs-string">'Growth rate of RGNP'</span>) 
</code></pre>
    <p class="normal">We get the following plot:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_07.png" alt="RGNP.png"/></figure>
    <p class="packt_figref">Figure 9.7: Growth rate of RGNP</p>
    <p class="normal">We'll <a id="_idIndexMarker738"/>model domestic recessions and expansions. The model will include transition probabilities between these two regimes<a id="_idIndexMarker739"/> and predict probabilities of expansion or recession at each time point.</p>
    <p class="normal">Let's fit the 4<sup class="Superscript--PACKT-">th</sup> order Markov switching model. We'll specify two regimes:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm
mod_hamilton = sm.tsa.MarkovAutoregression(dta_hamilton, k_regimes=<span class="hljs-number">2</span>, order=<span class="hljs-number">4</span>, switching_ar=<span class="hljs-literal">False</span>)
res_hamilton = mod_hamilton.fit()
</code></pre>
    <p class="normal">We now have the model fitted via maximum likelihood estimation to the RGNP data. We've set <code class="Code-In-Text--PACKT-">switching_ar=False</code> because the <code class="Code-In-Text--PACKT-">statsmodels</code> implementation defaults to switching autoregressive coefficients.</p>
    <p class="normal">Let's have a look at the <code class="Code-In-Text--PACKT-">statsmodels</code> model summary:</p>
    <pre class="programlisting code"><code class="hljs-code">print(res_hamilton.summary())
</code></pre>
    <p class="normal">We get the following output (truncated):</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_08.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_qPxkLo/Screenshot 2021-07-11 at 18.36.54.png"/></figure>
    <p class="packt_figref">Figure 9.8: Markov Switching Model Results</p>
    <p class="normal">We can see that we have two sets of parameters, one each for the two regimes. We also get measures of the statistical model quality (such as AIC and BIC). </p>
    <p class="normal">At the bottom of the same output, we can see the regime transition parameters:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_09.png" alt="../../Desktop/Screenshot%202021-07-11%20at%2018.40.01.pn"/></figure>
    <p class="packt_figref">Figure 9.9: Regime transition parameters</p>
    <p class="normal">These are<a id="_idIndexMarker740"/> the regime transitions we<a id="_idIndexMarker741"/> mentioned in the theory section on Markov Switching models.</p>
    <p class="normal">Let's see the lengths of recession and expansion:</p>
    <pre class="programlisting code"><code class="hljs-code">res_hamilton.expected_durations
</code></pre>
    <p class="normal">The output <code class="Code-In-Text--PACKT-">array([ 4.07604793, 10.4258926 ])</code> is in financial quarters. Therefore, a recession is expected to take about four quarters (1 year) and an expansion 10 quarters (two and a half years).</p>
    <p class="normal">Next, we'll plot the probability of recession at each point in time. However, this is more informative if we overlay indicators of recession by the National Bureau of Economic Research (NBER), which we can load up with pandas-dataloader:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pandas_datareader.data <span class="hljs-keyword">import</span> DataReader
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
usrec = DataReader(<span class="hljs-string">'USREC'</span>, <span class="hljs-string">'fred'</span>, start=datetime(<span class="hljs-number">1947</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), end=datetime(<span class="hljs-number">2013</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">This gives us a DataFrame in which recessions are indicated. Here are the first five rows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_10.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_P2RnuW/Screenshot 2021-07-11 at 19.00.46.png"/></figure>
    <p class="packt_figref">Figure 9.10: Recession indicators by NBER</p>
    <p class="normal">In the <a id="_idIndexMarker742"/>first five rows, there was no recession according to NBER indicators.</p>
    <p class="normal">We'll now <a id="_idIndexMarker743"/>plot NBER recession indicators against the model regime predictions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
_, ax = plt.subplots(<span class="hljs-number">1</span>) ax.plot(res_hamilton.filtered_marginal_probabilities[<span class="hljs-number">0</span>]) ax.fill_between(
  usrec.index, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, where=usrec[<span class="hljs-string">'USREC'</span>].values,
  color=<span class="hljs-string">'gray'</span>, alpha=<span class="hljs-number">0.3</span>
)
ax.<span class="hljs-built_in">set</span>(
  xlim=(dta_hamilton.index[<span class="hljs-number">4</span>], dta_hamilton.index[-<span class="hljs-number">1</span>]),
  ylim=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>),
  title=<span class="hljs-string">'Filtered probability of recession'</span>
)
</code></pre>
    <p class="normal">This gives us actual recession data against model predictions:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_11.png" alt="filtered_probability_of_recession.png"/></figure>
    <p class="packt_figref">Figure 9.11: Filtered probability of recession</p>
    <p class="normal">We can see there <a id="_idIndexMarker744"/>seems to be quite a good match between the model predictions and actual recession indicators.</p>
    <p class="normal">Unfortunately, the <code class="Code-In-Text--PACKT-">statsmodels</code> implementation<a id="_idIndexMarker745"/> doesn't provide the functionality for forecasting or out-of-sample prediction, so we'll end the brief demo here.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">Statsmodels</code> includes other datasets for regime-switching models to play around with.</p>
    <p class="normal">In the following practice <a id="_idIndexMarker746"/>section, we'll apply Song and Chissom's model to a time-series forecasting problem using the <code class="Code-In-Text--PACKT-">pyFTS</code> library for fuzzy time-series developed in the MINDS laboratory at the Universidade Federal de Minas Gerais (UFMG), Brazil.</p>
    <h2 id="_idParaDest-146" class="title">Fuzzy Time-Series</h2>
    <p class="normal">In this section, we'll <a id="_idIndexMarker747"/>load two time-series of ticker symbols, the NASDAQ and the S&amp;P 500 indices, over time and forecast them <a id="_idIndexMarker748"/>using Song and Chissom's 1993 algorithm. This follows closely the example tutorial in the library.</p>
    <p class="normal">First, we'll install the library from the terminal (or similarly from the Anaconda navigator):</p>
    <pre class="programlisting con"><code class="hljs-con">pip install pyFTS SimpSOM
</code></pre>
    <p class="normal">Then, we'll define our datasets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pyFTS.data <span class="hljs-keyword">import</span> NASDAQ, SP500
datasets = {
  <span class="hljs-string">"SP500"</span>: SP500.get_data()[<span class="hljs-number">11500</span>:<span class="hljs-number">16000</span>],
  <span class="hljs-string">"NASDAQ"</span>: NASDAQ.get_data()
}
</code></pre>
    <p class="normal">Both datasets, the entries in our <code class="Code-In-Text--PACKT-">datasets</code> dictionary, are vectors of roughly 4,000 scalar values. We'll take about 50% of these points for training, and we'll set this as a constant:</p>
    <pre class="programlisting code"><code class="hljs-code">train_split = <span class="hljs-number">2000</span>
</code></pre>
    <p class="normal">The model assumes <a id="_idIndexMarker749"/>a stationary process, so we'll need to preprocess our time-series by temporal differencing as discussed in <em class="chapterRef">Chapter 2</em>, <em class="italic">Exploratory Time-Series Analysis with Time-Series</em>.</p>
    <p class="normal">We'll define a<a id="_idIndexMarker750"/> first-order differencing operation for preprocessing:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pyFTS.common <span class="hljs-keyword">import</span> Transformations
tdiff = Transformations.Differential(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Let's plot our time-series and the transformation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
fig, ax = plt.subplots(nrows=<span class="hljs-number">2</span>, ncols=<span class="hljs-number">2</span>)
<span class="hljs-keyword">for</span> count, (dataset_name, dataset) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(datasets.items()):
  dataset_diff = tdiff.apply(dataset)
  ax[<span class="hljs-number">0</span>][count].plot(dataset)
  ax[<span class="hljs-number">1</span>][count].plot(dataset_diff)
  ax[<span class="hljs-number">0</span>][count].set_title(dataset_name)
</code></pre>
    <p class="normal">The plots of the original and transformed time-series look like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_12.png" alt="nasdaq_sp500.png"/></figure>
    <p class="packt_figref">Figure 9.12: NASDAQ and S&amp;P 500—the original and transformed time-series</p>
    <p class="normal">In the GitHub repository <a id="_idIndexMarker751"/>for this, you can see the Augmented Dickey-Fuller unit root test applied to the transformed time-series. This<a id="_idIndexMarker752"/> test for stationarity gives us the green light, and we continue with our model.</p>
    <p class="normal">The next step is training our models for the two transformed (differenced) time-series:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pyFTS.models <span class="hljs-keyword">import</span> song
<span class="hljs-keyword">from</span> pyFTS.partitioners <span class="hljs-keyword">import</span> Grid
models = {}
<span class="hljs-keyword">for</span> count, (dataset_name, dataset) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(datasets.items()):
  partitioner_diff = Grid.GridPartitioner(data=dataset, npart=<span class="hljs-number">15</span>, transformation=tdiff)
  model = song.ConventionalFTS(partitioner=partitioner_diff)
  model.name = dataset_name
  model.append_transformation(tdiff)
  model.fit(
    dataset[:train_split], 
    order=<span class="hljs-number">1</span>
  )
  models[dataset_name] = model
</code></pre>
    <p class="normal">We iterate over the datasets and train a separate model for each, which we save into a dictionary, <code class="Code-In-Text--PACKT-">models</code>. The training consists of extracting relations from the training set.</p>
    <p class="normal">As part of the model training, the preprocessed time-series is quantized as discussed in the theory section on fuzzy time-series models of this chapter.</p>
    <p class="normal">We can plot our forecasts from the two models:</p>
    <pre class="programlisting code"><code class="hljs-code">_, ax = plt.subplots(nrows=<span class="hljs-number">2</span>, ncols=<span class="hljs-number">1</span>, figsize=[<span class="hljs-number">12</span>, <span class="hljs-number">6</span>])
<span class="hljs-keyword">for</span> count, (dataset_name, dataset) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(datasets.items()):
    ax[count].plot(dataset[train_split:train_split+<span class="hljs-number">200</span>])
    model = models[dataset_name]
    forecasts = model.predict(dataset[train_split:train_split+<span class="hljs-number">200</span>], steps_ahead=<span class="hljs-number">1</span>)
    ax[count].plot(forecasts)
    ax[count].set_title(dataset_name)
    
plt.tight_layout()
</code></pre>
    <p class="normal">Again, we iterate <a id="_idIndexMarker753"/>over the two datasets. This time, we plot the original values in the test set (200 points) against <a id="_idIndexMarker754"/>estimated values predicted one step ahead. Please note that the models are not updated based on new data during the prediction.</p>
    <p class="normal">This is our plot comparing the forecasts against the actual values in the test set:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_13.png" alt="fuzzy_predicted_actual.png"/></figure>
    <p class="packt_figref">Figure 9.13: Fuzzy time-series forecast versus actual (S&amp;P 500, NASDAQ).</p>
    <p class="normal">Looking at these charts, the predictions look quite promising, but let's look at some hard numbers!</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">PyFTS</code> has a convenience function to extract RMSE, MAPE, and finally, Theil's U, a measure of correlation. We introduced these measures in <em class="chapterRef">Chapter 2</em>, <em class="italic">Exploratory Time-Series Analysis with Time-Series</em>.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pyFTS.benchmarks <span class="hljs-keyword">import</span> Measures
rows = []
<span class="hljs-keyword">for</span> count, (dataset_name, dataset) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(datasets.items()):
    row = [dataset_name]
    test = dataset[train_split:train_split+<span class="hljs-number">200</span>]
    model = models[dataset_name]
    row.extend(Measures.get_point_statistics(test, model))
    rows.append(row)
    
pd.DataFrame(
  rows,columns=[<span class="hljs-string">"Dataset"</span>, <span class="hljs-string">"RMSE"</span>, <span class="hljs-string">"MAPE"</span>, <span class="hljs-string">"Theil's U"</span>]
).set_index(<span class="hljs-string">"Dataset"</span>)
</code></pre>
    <p class="normal">We get these statistics:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_14.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_PJYAmO/Screenshot 2021-07-11 at 23.36.47.png"/></figure>
    <p class="packt_figref">Figure 9.14: Model statistics for fuzzy time-series modeling of NASDAQ and S&amp;P 500</p>
    <p class="normal">I'll leave it as <a id="_idIndexMarker755"/>an exercise for the reader<a id="_idIndexMarker756"/> to compare these two models with others based on these error metrics.</p>
    <h2 id="_idParaDest-147" class="title">Bayesian Structural Time-Series Modeling</h2>
    <p class="normal">In this example, we'll<a id="_idIndexMarker757"/> apply BSTS modeling to <a id="_idIndexMarker758"/>understand the causal effect of treatment in time-series.</p>
    <p class="normal">First, we'll install the library:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install tfcausalimpact
</code></pre>
    <p class="normal">Now, we'll load a dataset, and we'll estimate the consequence of a treatment.</p>
    <p class="normal">Here, we'll be estimating the impact of the emissions scandal of September 2015 for Volkswagen. We'll work with the stock values of three big companies, Volkswagen, BMW, and Allianz. The <a id="_idIndexMarker759"/>dataset comes with the Python Causal Impact (tfcausalimpact) library:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> causalimpact <span class="hljs-keyword">import</span> CausalImpact
data = pd.read_csv(<span class="code-highlight"><strong class="hljs-slc">"https://raw.githubusercontent.com/WillianFuks/tfcausalimpact/master/tests/fixtures/volks_data.csv"</strong></span>, header=<span class="hljs-number">0</span>, sep=<span class="code-highlight"><strong class="hljs-slc">'</strong></span><span class="hljs-string"> </span><span class="code-highlight"><strong class="hljs-slc">'</strong></span>, index_col=<span class="code-highlight"><strong class="hljs-slc">'Date'</strong></span>, parse_dates=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Now we have the stock values. Let's plot them:</p>
    <pre class="programlisting code"><code class="hljs-code">data.plot()
</code></pre>
    <p class="normal">Here are the stocks over time:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_15.png" alt="emission_scandal_stocks.png"/></figure>
    <p class="packt_figref">Figure 9.15: Stock values of three big companies (Volkswagen, BMW, Allianz)</p>
    <p class="normal">We can <a id="_idIndexMarker760"/>see a sharp drop in the value of Volkswagen shares in late 2015. Let's try to find out the actual impact of the emission scandal. We can build our model like this:</p>
    <pre class="programlisting code"><code class="hljs-code">pre_period = [<span class="hljs-built_in">str</span>(np.<span class="hljs-built_in">min</span>(data.index.values)), <span class="hljs-string">"2015-09-13"</span>]
post_period = [<span class="hljs-string">"2015-09-20"</span>, <span class="hljs-built_in">str</span>(np.<span class="hljs-built_in">max</span>(data.index.values))]
ci = CausalImpact(data.iloc[:, <span class="hljs-number">0</span>], pre_period, post_period, model_args={<span class="hljs-string">'nseasons'</span>: <span class="hljs-number">52</span>, <span class="hljs-string">'fit_method'</span>: <span class="hljs-string">'vi'</span>})
</code></pre>
    <p class="normal">The <a id="_idIndexMarker761"/>model statistics provide us with the causal impact estimate:</p>
    <pre class="programlisting code"><code class="hljs-code">print(ci.summary())
</code></pre>
    <p class="normal">We see these stats here:</p>
    <pre class="programlisting con"><code class="hljs-con">Posterior Inference {Causal Impact}
                          Average              Cumulative
Actual                    126.91               10026.07
Prediction (s.d.)         171.28 (17.33)       13531.49 (1369.17)
95%<span class="bash"> CI                    [136.07, 204.01]     [10749.78, 16116.83]</span>
Absolute effect (s.d.)    -44.37 (17.33)       -3505.42 (1369.17)
95%<span class="bash"> CI                    [-77.1, -9.16]       [-6090.76, -723.71]</span>
Relative effect (s.d.)    -25.91% (10.12%)     -25.91% (10.12%)
95%<span class="bash"> CI                    [-45.01%, -5.35%]    [-45.01%, -5.35%]</span>
<span class="bash">Posterior tail-area probability p: 0.01</span>
<span class="bash">Posterior probability of a causal effect: 99.2%</span>
</code></pre>
    <p class="packt_figref">Figure 9.16: Causal impact estimates and model statistics</p>
    <p class="normal">As <a id="_idIndexMarker762"/>discussed before, the Causal Impact model developed by Google works by fitting a BSTS model to observed data, which is later used to predict what the results would be had no intervention happened in a given time period.</p>
    <p class="normal">The total estimated effect is about 44 points—the stock price would be 44 points higher if not for the emissions scandal. The impact summary report gives us this analysis (excerpt):</p>
    <pre class="programlisting gen"><code class="hljs">During the post-intervention period, the response variable had
an average value of approx. 126.91. By contrast, in the absence of an intervention, we would have expected an average response of 171.28. The 95% interval of this counterfactual prediction is [136.07, 204.01].
Subtracting this prediction from the observed response yields
an estimate of the causal effect the intervention had on the
response variable. This effect is -44.37 with a 95% interval of [-77.1, -9.16]. For a discussion of the significance of this effect, see below.
</code></pre>
    <p class="packt_figref">Figure 9.17: Causal impact analysis report</p>
    <p class="normal">This gives us a very good idea of what the model estimates. </p>
    <p class="normal">We can plot the effect as well:</p>
    <pre class="programlisting code"><code class="hljs-code">ci.plot(panels=[<span class="hljs-string">"original"</span>]
</code></pre>
    <p class="normal">The plot is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_09_16.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_OzvijR/Screenshot 2021-07-18 at 17.03.38.png"/></figure>
    <p class="packt_figref">Figure 9.18: Causal impact graph</p>
    <p class="normal">Again, we<a id="_idIndexMarker763"/> see the original time-series against the predicted counterfactual value. </p>
    <p class="normal">The emissions <a id="_idIndexMarker764"/>scandal wiped out a massive amount of value from Volkswagen. The 44 points can give us a monetary value of how much cheating emissions tests cost Volkswagen.</p>
    <h1 id="_idParaDest-148" class="title">Summary</h1>
    <p class="normal">In this chapter, we've discussed how probabilistic models for time-series can help us make decisions with an estimate of uncertainty in the context of financial forecasting. These forecasts drive business decisions for financial planning.</p>
    <p class="normal">I've introduced Prophet, Markov models, and Fuzzy time-series models. We've discussed the components of Facebook's Prophet model. For Markov models, we've discussed the main ideas, such as the Markov property, and we've discussed more details regarding Switching Models. Then I've explained some basics of fuzzy set theory and how this is applied to time-series.</p>
    <p class="normal">Finally, we've delved into the intuition and some of the theory of BSTS models in the context of estimating treatment effects in experiments.</p>
    <p class="normal">Finally, we went through an applied exercise with each method. In the BSTS practice, we've looked at the effect of the Volkswagen emissions scandal.</p>
  </div>
</body></html>
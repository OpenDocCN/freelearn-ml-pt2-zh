["```py\n    import langdetect\n    import matplotlib.pyplot\n    import nltk\n    import numpy\n    import pandas\n    import pyLDAvis\n    import pyLDAvis.sklearn\n    import regex\n    import sklearn\n    ```", "```py\n    nltk.download('wordnet')\n    nltk.download('stopwords')\n    ```", "```py\n    %matplotlib inline\n    ```", "```py\n    path = \"News_Final.csv\"\n    df = pandas.read_csv(path, header=0)\n    ```", "```py\n    def dataframe_quick_look(df, nrows):\n        print(\"SHAPE:\\n{shape}\\n\".format(shape=df.shape))\n        print(\"COLUMN NAMES:\\n{names}\\n\".format(names=df.columns))\n        print(\"HEAD:\\n{head}\\n\".format(head=df.head(nrows)))\n    dataframe_quick_look(df, nrows=2)\n    ```", "```py\n    print(\"TOPICS:\\n{topics}\\n\".format(topics=df[\"Topic\"]\\\n          .value_counts()))\n    ```", "```py\n    TOPICS:\n    economy      33928\n    obama        28610\n    microsoft    21858\n    palestine     8843\n    Name: Topic, dtype: int64\n    ```", "```py\n    raw = df[\"Headline\"].tolist()\n    print(\"HEADLINES:\\n{lines}\\n\".format(lines=raw[:5]))\n    print(\"LENGTH:\\n{length}\\n\".format(length=len(raw)))\n    ```", "```py\n    example = raw[5]\n    print(example)\n    ```", "```py\n    def do_language_identifying(txt):\n        try: the_language = langdetect.detect(txt)\n        except: the_language = 'none'\n        return the_language\n    print(\"DETECTED LANGUAGE:\\n{lang}\\n\"\\\n          .format(lang=do_language_identifying(example)))\n    ```", "```py\n    DETECTED LANGUAGE:\n    en\n    ```", "```py\n    example = example.split(\" \")\n    print(example)\n    ```", "```py\n    example = ['URL' if bool(regex.search(\"http[s]?://\", i)) \\\n               else i for i in example]\n    print(example)\n    ```", "```py\n    example = [regex.sub(\"[^\\\\w\\\\s]|\\n\", \"\", i) for i in example]\n    print(example)\n    ```", "```py\n    example = [regex.sub(\"^[0-9]*$\", \"\", i) for i in example]\n    print(example)\n    ```", "```py\n    example = [i.lower() if i not in [\"URL\"] else i for i in example]\n    print(example)\n    ```", "```py\n    example = [i for i in example if i not in [\"URL\",\"\"]]\n    print(example)\n    ```", "```py\n    list_stop_words = nltk.corpus.stopwords.words(\"english\")\n    list_stop_words = [regex.sub(\"[^\\\\w\\\\s]\", \"\", i) \\\n                       for i in list_stop_words]\n    print(list_stop_words)\n    ```", "```py\n    example = [i for i in example if i not in list_stop_words]\n    print(example)\n    ```", "```py\n    def do_lemmatizing(wrd):\n        out = nltk.corpus.wordnet.morphy(wrd)\n        return (wrd if out is None else out)\n    example = [do_lemmatizing(i) for i in example]\n    print(example)\n    ```", "```py\n    example = [i for i in example if len(i) >= 5]\n    print(example)\n    ```", "```py\n    Exercise7.01-Exercise7.12.ipynb\n    def do_headline_cleaning(txt):\n          # identify language of tweet\n          # return null if language not English\n        lg = do_language_identifying(txt)\n        if lg != 'en': \n            return None\n          # split the string on whitespace\n        out = txt.split(\" \")\n          # identify urls\n          # replace with URL\n        out = ['URL' if bool(regex.search(\"http[s]?://\", i)) \\\n               else i for i in out]\n          # remove all punctuation\n        out = [regex.sub(\"[^\\\\w\\\\s]|\\n\", \"\", i) for i in out]\n          # remove all numerics\n        out = [regex.sub(\"^[0-9]*$\", \"\", i) for i in out]\n    The complete code for this step can be found at https://packt.live/34gLGKa.\n    ```", "```py\n    tick = time()\n    clean = list(map(do_headline_cleaning, raw))\n    print(time()-tick)\n    ```", "```py\n    clean = list(filter(None.__ne__, clean))\n    print(\"HEADLINES:\\n{lines}\\n\".format(lines=clean[:5]))\n    print(\"LENGTH:\\n{length}\\n\".format(length=len(clean)))\n    ```", "```py\n    clean_sentences = [\" \".join(i) for i in clean]\n    print(clean_sentences[0:10])\n    ```", "```py\n    number_words = 10\n    number_docs = 10\n    number_features = 1000\n    ```", "```py\n    vectorizer1 = sklearn.feature_extraction.text\\\n                  .CountVectorizer(analyzer=\"word\",\\\n                                   max_df=0.5,\\\n                                   min_df=20,\\\n                                   max_features=number_features)\n    clean_vec1 = vectorizer1.fit_transform(clean_sentences)\n    print(clean_vec1[0])\n    ```", "```py\n    feature_names_vec1 = vectorizer1.get_feature_names()\n    ```", "```py\n    def perplexity_by_ntopic(data, ntopics):\n        output_dict = {\"Number Of Topics\": [], \\\n                       \"Perplexity Score\": []}\n        for t in ntopics:\n            lda = sklearn.decomposition.LatentDirichletAllocation(\\\n                  n_components=t, \\\n                  learning_method=\"online\", \\\n                  random_state=0)\n            lda.fit(data)\n            output_dict[\"Number Of Topics\"].append(t)\n            output_dict[\"Perplexity Score\"]\\\n                       .append(lda.perplexity(data))\n            output_df = pandas.DataFrame(output_dict)\n            index_min_perplexity = output_df[\"Perplexity Score\"]\\\n                                   .idxmin()\n            output_num_topics = output_df.loc[\\\n            index_min_perplexity,  # index \\\n            \"Number Of Topics\"  # column\n            ]\n        return (output_df, output_num_topics)\n    ```", "```py\n    df_perplexity, optimal_num_topics = \\\n    perplexity_by_ntopic(clean_vec1, ntopics=[1, 2, 3, 4, 6, 8, 10])\n    print(df_perplexity)\n    ```", "```py\n    df_perplexity.plot.line(\"Number Of Topics\", \"Perplexity Score\")\n    ```", "```py\n    lda = sklearn.decomposition.LatentDirichletAllocation\\\n          (n_components=optimal_num_topics,\\\n           learning_method=\"online\",\\\n           random_state=0)\n    lda.fit(clean_vec1)\n    ```", "```py\n    lda_transform = lda.transform(clean_vec1)\n    print(lda_transform.shape)\n    print(lda_transform)\n    ```", "```py\n    (92946, 3)\n    [[0.04761958 0.90419577 0.04818465]\n     [0.04258906 0.04751535 0.90989559]\n     [0.16656181 0.04309434 0.79034385]\n     ...\n     [0.0399815  0.51492894 0.44508955]\n     [0.06918206 0.86099065 0.06982729]\n     [0.48210053 0.30502833 0.21287114]]\n    ```", "```py\n    lda_components = lda.components_\n    print(lda_components.shape)\n    print(lda_components)\n    ```", "```py\n    (3, 1000)\n    [[3.35570079e-01 1.98879573e+02 9.82489014e+00 ... 3.35388004e-01\n      2.04173562e+02 4.03130268e-01]\n     [2.74824227e+02 3.94662558e-01 3.63412044e-01 ... 3.45944379e-01\n      1.77517291e+02 4.61625408e+02]\n     [3.37041234e-01 7.36749100e+01 2.05707096e+02 ... 2.31714093e+02\n      1.21765267e+02 7.71397922e-01]]\n    ```", "```py\n    Exercise7.01-Exercise7.12.ipynb\n    def get_topics(mod, vec, names, docs, ndocs, nwords):\n        # word to topic matrix\n        W = mod.components_\n        W_norm = W / W.sum(axis=1)[:, numpy.newaxis]\n        # topic to document matrix\n        H = mod.transform(vec)\n        W_dict = {}\n        H_dict = {}\n    The complete code for this step can be found at https://packt.live/34gLGKa.\n    ```", "```py\n    W_df, H_df = get_topics(mod=lda, \\\n                            vec=clean_vec1, \\\n                            names=feature_names_vec1, \\\n                            docs=raw, \\\n                            ndocs=number_docs, \\\n                            nwords=number_words)\n    ```", "```py\n    print(W_df)\n    ```", "```py\n    print(H_df)\n    ```", "```py\n    lda_plot = pyLDAvis.sklearn\\\n              .prepare(lda, clean_vec1, vectorizer1, R=10)\n    pyLDAvis.display(lda_plot)\n    ```", "```py\n    Exercise7.01-Exercise7.12.ipynb\n    def plot_tsne(data, threshold):\n        # filter data according to threshold\n        index_meet_threshold = numpy.amax(data, axis=1) >= threshold\n        lda_transform_filt = data[index_meet_threshold]\n        # fit tsne model\n        # x-d -> 2-d, x = number of topics\n        tsne = sklearn.manifold.TSNE(n_components=2, \\\n                                     verbose=0, \\\n                                     random_state=0, \\\n                                     angle=0.5, \\\n                                     init='pca')\n        tsne_fit = tsne.fit_transform(lda_transform_filt)\n        # most probable topic for each headline\n        most_prob_topic = []\n    The complete code for this step can be found at https://packt.live/34gLGKa.\n    ```", "```py\n    index_meet_threshold = numpy.amax(data, axis=1) >= threshold\n    lda_transform_filt = data[index_meet_threshold]\n    ```", "```py\n    tsne = sklearn.manifold.TSNE(n_components=2, \\\n                                 verbose=0, \\\n                                 random_state=0, \\\n                                 angle=0.5, \\\n                                 init='pca')\n    tsne_fit = tsne.fit_transform(lda_transform_filt)\n    ```", "```py\n    most_prob_topic = []\n    for i in range(tsne_fit.shape[0]):\n        most_prob_topic.append(lda_transform_filt[i].argmax())\n    ```", "```py\n    print(\"LENGTH:\\n{}\\n\".format(len(most_prob_topic)))\n    unique, counts = numpy.unique(numpy.array(most_prob_topic), \\\n                                  return_counts=True)\n    print(\"COUNTS:\\n{}\\n\".format(numpy.asarray((unique, counts)).T))\n    ```", "```py\n    color_list = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n    for i in list(set(most_prob_topic)):\n        indices = [idx for idx, val in enumerate(most_prob_topic) \\\n                   if val == i]\n        matplotlib.pyplot.scatter(x=tsne_fit[indices, 0], \\\n                                  y=tsne_fit[indices, 1], \\\n                                  s=0.5, c=color_list[i], \\\n                                  label='Topic' + str(i), \\\n                                  alpha=0.25)\n    matplotlib.pyplot.xlabel('x-tsne')\n    matplotlib.pyplot.ylabel('y-tsne')\n    matplotlib.pyplot.legend(markerscale=10)\n    ```", "```py\n    plot_tsne(data=lda_transform, threshold=0.75)\n    ```", "```py\n    lda4 = sklearn.decomposition.LatentDirichletAllocation(\\\n           n_components=4,  # number of topics data suggests \\\n           learning_method=\"online\", \\\n           random_state=0)\n    lda4.fit(clean_vec1)\n    ```", "```py\n    W_df4, H_df4 = get_topics(mod=lda4, \\\n                              vec=clean_vec1, \\\n                              names=feature_names_vec1, \\\n                              docs=raw, \\\n                              ndocs=number_docs, \\\n                              nwords=number_words)\n    ```", "```py\n    print(W_df4)\n    ```", "```py\n    print(H_df4)\n    ```", "```py\n    lda4_plot = pyLDAvis.sklearn\\\n               .prepare(lda4, clean_vec1, vectorizer1, R=10)\n    pyLDAvis.display(lda4_plot)\n    ```", "```py\n    vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer\\\n                  (analyzer=\"word\",\\\n                   max_df=0.5, \\\n                   min_df=20, \\\n                   max_features=number_features,\\\n                   smooth_idf=False)\n    clean_vec2 = vectorizer2.fit_transform(clean_sentences)\n    print(clean_vec2[0])\n    ```", "```py\n    feature_names_vec2 = vectorizer2.get_feature_names()\n    feature_names_vec2\n    ```", "```py\n    ['abbas',\n     'ability',\n     'accelerate',\n     'accept',\n     'access',\n     'accord',\n     'account',\n     'accused',\n     'achieve',\n     'acknowledge',\n     'acquire',\n     'acquisition',\n     'across',\n     'action',\n     'activist',\n     'activity',\n     'actually',\n    ```", "```py\n    nmf = sklearn.decomposition.NMF(n_components=4, \\\n                                    init=\"nndsvda\", \\\n                                    solver=\"mu\", \\\n                                    beta_loss=\"frobenius\", \\\n                                    random_state=0, \\\n                                    alpha=0.1, \\\n                                    l1_ratio=0.5)\n    nmf.fit(clean_vec2)\n    ```", "```py\n    W_df, H_df = get_topics(mod=nmf, \\\n                            vec=clean_vec2, \\\n                            names=feature_names_vec2, \\\n                            docs=raw, \\\n                            ndocs=number_docs, \\\n                            nwords=number_words)\n    ```", "```py\n    print(W_df)\n    ```", "```py\n    print(H_df)\n    ```", "```py\n    nmf_transform = nmf.transform(clean_vec2)\n    print(nmf_transform.shape)\n    print(nmf_transform)\n    ```", "```py\n    (92946, 4)\n    [[5.12653315e-02 3.60582233e-15 3.19729419e-34 8.17267206e-16]\n     [7.43734737e-04 2.04138105e-02 6.85552731e-15 2.11679327e-03]\n     [2.92397552e-15 1.94083984e-02 4.76691813e-21 1.24269313e-18]\n     ...\n     [9.83404082e-06 3.41225477e-03 6.14009658e-04 3.23919592e-02]\n     [6.51294966e-07 1.32359509e-07 3.32509174e-08 6.14671536e-02]\n     [4.53925928e-05 1.16401194e-04 1.84755839e-02 2.00616344e-03]]\n    ```", "```py\n    plot_tsne(data=nmf_transform, threshold=0)\n    ```"]
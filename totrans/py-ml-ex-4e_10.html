<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer382">
    <h1 class="chapterNumber">10</h1>
    <h1 class="chapterTitle" id="_idParaDest-221">Machine Learning Best Practices</h1>
    <p class="normal">After working on multiple projects covering important machine learning concepts, techniques, and widely used algorithms, you have a broad picture of the machine learning ecosystem, as well as solid experience in tackling practical problems using machine learning algorithms and Python. However, there will be issues once we start working on projects from scratch in the real world. This chapter aims to get us ready for it with 21 best practices to follow throughout the entire machine learning solution workflow.</p>
    <p class="normal">We will cover the following topics in this chapter:</p>
    <ul>
      <li class="bulletList">Machine learning solution workflow</li>
      <li class="bulletList">Best practices in the data preparation stage</li>
      <li class="bulletList">Best practices in the training set generation stage</li>
      <li class="bulletList">Best practices in the model training, evaluation, and selection stage</li>
      <li class="bulletList">Best practices in the deployment and monitoring stage</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-222">Machine learning solution workflow</h1>
    <p class="normal">In <a id="_idIndexMarker964"/>general, the main tasks involved in solving a machine learning problem can be summarized into four areas, as follows:</p>
    <ul>
      <li class="bulletList">Data preparation</li>
      <li class="bulletList">Training set generation</li>
      <li class="bulletList">Model training, evaluation, and selection</li>
      <li class="bulletList">Deployment and monitoring</li>
    </ul>
    <p class="normal">Starting from <a id="_idIndexMarker965"/>data sources and ending with the final machine learning system, a machine learning solution basically follows the paradigm shown here:</p>
    <figure class="mediaobject"><img alt="A picture containing text, screenshot, diagram, font  Description automatically generated" src="../Images/B21047_10_01.png"/></figure>
    <p class="packt_figref">Figure 10.1: The life cycle of a machine learning solution</p>
    <p class="normal">In the following sections, we will learn about the typical tasks, common challenges, and best practices for each of these four stages.</p>
    <h1 class="heading-1" id="_idParaDest-223">Best practices in the data preparation stage</h1>
    <p class="normal">No machine<a id="_idIndexMarker966"/> learning system can be built without<a id="_idIndexMarker967"/> data. Therefore, <strong class="keyWord">data collection</strong> should be our<a id="_idIndexMarker968"/> first focus.</p>
    <h2 class="heading-2" id="_idParaDest-224">Best practice 1 – Completely understanding the project goal</h2>
    <p class="normal">Before<a id="_idIndexMarker969"/> starting to collect data, we should make sure that the goal of the project and the business problem are completely understood, as this will guide us on what data sources to look into, and where sufficient domain knowledge and expertise is also required. For example, in a previous chapter, <em class="chapterRef">Chapter 5</em>, <em class="italic">Predicting Stock Prices with Regression Algorithms</em>, our goal was to predict the future prices of the stock index, so we first collected data on its past performance, instead of the past performance of an irrelevant European stock. In <em class="chapterRef">Chapter 3</em>, <em class="italic">Predicting Online Ad Click-Through with Tree-Based Algorithms</em>, for example, the business problem was to optimize advertising, targeting efficiency measured by click-through rate, so we collected the clickstream data of who clicked or did not click on what ad on what page, instead of merely using how many ads were displayed in a web domain.</p>
    <h2 class="heading-2" id="_idParaDest-225">Best practice 2 – Collecting all fields that are relevant</h2>
    <p class="normal">With a set<a id="_idIndexMarker970"/> goal in mind, we can narrow down potential data sources to investigate. Now the question becomes: is it necessary to collect the data of all fields available in a data source, or is a subset of attributes enough? It would be perfect if we knew in advance which attributes were key indicators or key predictive factors. However, it is in fact very difficult to ensure that the attributes hand-picked by a domain expert will yield the best prediction results. Hence, for each data source, it is recommended to collect all of the fields that are related to the project, especially in cases where recollecting the data is time-consuming, or even impossible.</p>
    <p class="normal">For example, in the stock price prediction example, we collected the data of all fields, including <strong class="keyWord">Open</strong>, <strong class="keyWord">High</strong>, <strong class="keyWord">Low</strong>, and <strong class="keyWord">Volume</strong>, even though we were initially not certain of how useful <strong class="keyWord">high</strong> and <strong class="keyWord">low</strong> predictions would be. Retrieving the stock data is quick and easy, however. In another example, if we ever want to collect data ourselves by scraping online articles for topic classification, we should store as much information as possible. Otherwise, if any piece of information is not collected but is later found to be valuable, such as hyperlinks in an article, the article might already have been removed from the web page; if it still exists, rescraping those pages can be costly.</p>
    <p class="normal">After collecting the <a id="_idIndexMarker971"/>datasets that we think are useful, we need to ensure the data quality by inspecting its <strong class="keyWord">consistency</strong> and <strong class="keyWord">completeness</strong>. Consistency refers to how the distribution of data changes over<a id="_idIndexMarker972"/> time. Completeness <a id="_idIndexMarker973"/>means how much data is present across fields and samples. They are explained in detail in the following two practices.</p>
    <h2 class="heading-2" id="_idParaDest-226">Best practice 3 – Maintaining the consistency and normalization of field values</h2>
    <p class="normal">In a dataset<a id="_idIndexMarker974"/> that already exists, or in one that we collect <a id="_idIndexMarker975"/>from scratch, we often see different values representing the same meaning. For example, we see <em class="italic">American</em>, <em class="italic">US</em>, and <em class="italic">U.S.A</em> in the <code class="inlineCode">Country</code> field, and <em class="italic">male</em> and <em class="italic">M</em> in the <code class="inlineCode">Gender</code> field. It is necessary to unify or standardize the values in a field, otherwise, it will mess up the algorithms in later stages as different feature values will be treated differently even if they have the same meaning. For example, we keep only the three options <em class="italic">M</em>, <em class="italic">F</em>, and <em class="italic">gender-diverse</em> in the <code class="inlineCode">Gender</code> field, and replace other alternative values. It is also a great practice to keep track of what values are mapped to the default value of a field.</p>
    <p class="normal">In addition, the format of values in the same field should also be consistent. For instance, in the <em class="italic">age</em> field, there could be true age values, such as <em class="italic">21</em> and <em class="italic">35</em>, and incorrect age values, such as <em class="italic">1990</em> and <em class="italic">1978</em>; in the <em class="italic">rating</em> field, both cardinal numbers and English numerals could be found, such as <em class="italic">1</em>, <em class="italic">2</em>, and <em class="italic">3</em>, and <em class="italic">one</em>, <em class="italic">two</em>, and <em class="italic">three</em>. Transformation and reformatting should be conducted in order to ensure data consistency.</p>
    <h2 class="heading-2" id="_idParaDest-227">Best practice 4 – Dealing with missing data</h2>
    <p class="normal">Due to various <a id="_idIndexMarker976"/>reasons, datasets in the real world are rarely completely clean and often contain missing or corrupted values. They are usually presented as blanks, <em class="italic">Null</em>, <em class="italic">-1, 999999</em>, <em class="italic">unknown</em>, or any other placeholder. Samples with missing data not only provide incomplete predictive information but also confuse the machine learning model as it cannot tell whether <em class="italic">-1</em> or <em class="italic">unknown</em> holds a meaning. It is important to pinpoint and deal with missing data in order to avoid jeopardizing the performance of models in the later stages.</p>
    <p class="normal">Here are three basic strategies that we can use to tackle the missing data issue:</p>
    <ul>
      <li class="bulletList">Discarding samples containing any missing values.</li>
      <li class="bulletList">Discarding fields containing missing values in any sample.</li>
      <li class="bulletList">Inferring the missing values based on the known part of the attribute. This process<a id="_idIndexMarker977"/> is called <strong class="keyWord">missing data imputation</strong>. Typical imputation methods include replacing missing values with the mean or median value of the field across all samples, or the most frequent value for categorical data.</li>
    </ul>
    <p class="normal">The first two strategies are simple to implement; however, they come at the expense of the data lost, especially when the original dataset is not large enough. The third strategy doesn’t abandon any data but does try to fill in the blanks.</p>
    <p class="normal">Let’s look at how<a id="_idIndexMarker978"/> each strategy is applied in an example where we <a id="_idIndexMarker979"/>have a dataset (age, income) consisting of six samples – (30, 100), (20, 50), (35, <em class="italic">unknown</em>), (25, 80), (30, 70), and (40, 60):</p>
    <ul>
      <li class="bulletList">If we process this dataset using the first strategy, it becomes (30, 100), (20, 50), (25, 80), (30, 70), and (40, 60).</li>
      <li class="bulletList">If we employ the second strategy, the dataset becomes (30), (20), (35), (25), (30), and (40), where only the first field remains.</li>
      <li class="bulletList">If we decide to complete the unknown value instead of skipping it, the sample (35, <em class="italic">unknown</em>) can be transformed into (35, 72) with the mean of the rest of the values in the second field, or (35, 70), with the median value in the second field.</li>
    </ul>
    <p class="normal">In <a id="_idIndexMarker980"/>scikit-learn, the <code class="inlineCode">SimpleImputer</code> class (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html</span></a>) provides a nicely written imputation transformer. We can use it for the following small example:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> numpy </span><span class="hljs-con-keyword">as</span><span class="language-python"> np</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.impute </span><span class="hljs-con-keyword">import</span><span class="language-python"> SimpleImputer</span>
</code></pre>
    <p class="normal">Represent the unknown value with <code class="inlineCode">np.nan</code> in <code class="inlineCode">numpy</code>, as detailed in the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_origin = [[</span><span class="hljs-con-number">30</span><span class="language-python">, </span><span class="hljs-con-number">100</span><span class="language-python">],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">               [</span><span class="hljs-con-number">20</span><span class="language-python">, </span><span class="hljs-con-number">50</span><span class="language-python">],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">               [</span><span class="hljs-con-number">35</span><span class="language-python">, np.nan],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">               [</span><span class="hljs-con-number">25</span><span class="language-python">, </span><span class="hljs-con-number">80</span><span class="language-python">],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">               [</span><span class="hljs-con-number">30</span><span class="language-python">, </span><span class="hljs-con-number">70</span><span class="language-python">],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">               [</span><span class="hljs-con-number">40</span><span class="language-python">, </span><span class="hljs-con-number">60</span><span class="language-python">]]</span>
</code></pre>
    <p class="normal">Initialize the<a id="_idIndexMarker981"/> imputation transformer with the mean value and<a id="_idIndexMarker982"/> obtain the mean value from the original data:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">imp_mean = SimpleImputer(missing_values=np.nan, strategy=</span><span class="hljs-con-string">'mean'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">imp_mean.fit(data_origin)</span>
</code></pre>
    <p class="normal">Complete the missing value as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_mean_imp = imp_mean.transform(data_origin)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(data_mean_imp)</span>
[[ 30. 100.]
 [ 20. 50.]
 [ 35. 72.]
 [ 25. 80.]
 [ 30. 70.]
 [ 40. 60.]]
</code></pre>
    <p class="normal">Similarly, initialize the imputation transformer with the median value, as detailed in the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">imp_median = SimpleImputer(missing_values=np.nan, strategy=</span><span class="hljs-con-string">'median'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">imp_median.fit(data_origin)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_median_imp = imp_median.transform(data_origin)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(data_median_imp)</span>
[[ 30. 100.]
 [ 20. 50.]
 [ 35. 70.]
 [ 25. 80.]
 [ 30. 70.]
 [ 40. 60.]]
</code></pre>
    <p class="normal">When new samples come in, the missing values (in any attribute) can be imputed using the trained transformer, for example, with the mean value, as shown here:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">new = [[</span><span class="hljs-con-number">20</span><span class="language-python">, np.nan],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">       [</span><span class="hljs-con-number">30</span><span class="language-python">, np.nan],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">       [np.nan, </span><span class="hljs-con-number">70</span><span class="language-python">],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">       [np.nan, np.nan]]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">new_mean_imp = imp_mean.transform(new)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(new_mean_imp)</span>
[[ 20. 72.]
 [ 30. 72.]
 [ 30. 70.]
 [ 30. 72.]]
</code></pre>
    <p class="normal">Note that <code class="inlineCode">30</code> in the age field is the mean of those six age values in the original dataset.</p>
    <p class="normal">Now that we<a id="_idIndexMarker983"/> have seen how imputation works, as well as<a id="_idIndexMarker984"/> its implementation, let’s explore how the strategy of imputing missing values and discarding missing data affects the prediction results through the following example:</p>
    <ol>
      <li class="numberedList" value="1">First, we load the diabetes dataset, as shown here:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn </span><span class="hljs-con-keyword">import</span><span class="language-python"> datasets</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">dataset = datasets.load_diabetes()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_full, y = dataset.data, dataset.target</span>
</code></pre>
      </li>
      <li class="numberedList">Simulate a corrupted dataset by adding 25% missing values:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">m, n = X_full.shape</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">m_missing = </span><span class="hljs-con-built_in">int</span><span class="language-python">(m * </span><span class="hljs-con-number">0.25</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(m, m_missing)</span>
442 110
</code></pre>
      </li>
      <li class="numberedList">Randomly select the <code class="inlineCode">m_missing</code> samples, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">np.random.seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">missing_samples = np.array([</span><span class="hljs-con-literal">True</span><span class="language-python">] * m_missing + [</span><span class="hljs-con-literal">False</span><span class="language-python">] * (m - m_missing))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">np.random.shuffle(missing_samples)</span>
</code></pre>
      </li>
      <li class="numberedList">For each missing sample, randomly select 1 out of <code class="inlineCode">n</code> features:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">missing_features = np.random.randint(low=</span><span class="hljs-con-number">0</span><span class="language-python">, high=n, size=m_missing)</span>
</code></pre>
      </li>
      <li class="numberedList">Represent missing values with <code class="inlineCode">nan</code>, as shown here:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_missing = X_full.copy()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_missing[np.where(missing_samples)[</span><span class="hljs-con-number">0</span><span class="language-python">], missing_features] = np.nan</span>
</code></pre>
      </li>
      <li class="numberedList">Then, we deal with this corrupted dataset by discarding the samples containing a missing value:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_rm_missing = X_missing[~missing_samples, :]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_rm_missing = y[~missing_samples]</span>
</code></pre>
      </li>
      <li class="numberedList">Measure the effects of using this strategy by estimating the averaged regression score <em class="italic">R</em><sup class="superscript">2</sup>, with a regression forest model in a cross-validation manner. Estimate <em class="italic">R</em><sup class="superscript">2</sup> on the dataset with the missing samples removed, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.ensemble </span><span class="hljs-con-keyword">import</span><span class="language-python"> RandomForestRegressor</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.model_selection </span><span class="hljs-con-keyword">import</span><span class="language-python"> cross_val_score</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">regressor = RandomForestRegressor(random_state=</span><span class="hljs-con-number">42</span><span class="language-python">,</span>
                                  max_depth=10, n_estimators=100)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">score_rm_missing = cross_val_score(regressor,X_rm_missing,</span>
                                          y_rm_missing).mean()
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Score with the data set with missing samples removed: </span><span class="hljs-con-subst">{score_rm_missing:</span><span class="hljs-con-number">.2</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
Score with the data set with missing samples removed: 0.38
</code></pre>
      </li>
      <li class="numberedList">Now we<a id="_idIndexMarker985"/> approach the corrupted <a id="_idIndexMarker986"/>dataset differently by imputing missing values with the mean, as shown here:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">imp_mean = SimpleImputer(missing_values=np.nan, strategy=</span><span class="hljs-con-string">'mean'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_mean_imp = imp_mean.fit_transform(X_missing)</span>
</code></pre>
      </li>
      <li class="numberedList">Similarly, measure the effects of using this strategy by estimating the averaged <em class="italic">R</em><sup class="superscript">2</sup>, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">regressor = RandomForestRegressor(random_state=</span><span class="hljs-con-number">42</span><span class="language-python">,</span>
                                      max_depth=10,
                                      n_estimators=100)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">score_mean_imp = cross_val_score(regressor, X_mean_imp, y).mean()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Score with the data set with missing values replaced by mean: </span><span class="hljs-con-subst">{score_mean_imp:</span><span class="hljs-con-number">.2</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
Score with the data set with missing values replaced by mean: 0.41
</code></pre>
      </li>
      <li class="numberedList">An imputation strategy works better than discarding in this case. So, how far is the imputed dataset from the original full one? We can check it again by estimating the averaged regression score on the original dataset, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">regressor = RandomForestRegressor(random_state=</span><span class="hljs-con-number">42</span><span class="language-python">,</span>
                                      max_depth=10,
                                      n_estimators=500)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">score_full = cross_val_score(regressor, X_full, y).mean()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Score with the full data set: </span><span class="hljs-con-subst">{score_full:</span><span class="hljs-con-number">.2</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
Score with the full data set: 0.42
</code></pre>
      </li>
    </ol>
    <p class="normal">It turns out that<a id="_idIndexMarker987"/> little information is compromised in the <a id="_idIndexMarker988"/>imputed dataset.</p>
    <p class="normal">However, there is no guarantee that an imputation strategy always works better, and sometimes, dropping samples with missing values can be more effective. Hence, it is a great practice to compare the performance of different strategies via cross-validation, as we have done previously.</p>
    <h2 class="heading-2" id="_idParaDest-228">Best practice 5 – Storing large-scale data</h2>
    <p class="normal">With the <a id="_idIndexMarker989"/>ever-growing size of data, oftentimes, we can’t simply fit the data on our single local machine and need to store it on the cloud or distributed filesystems. As this is mainly a book on machine learning with Python, we will just touch on some basic areas that you can look into. The two main strategies for storing big data are <strong class="keyWord">scale up</strong> and <strong class="keyWord">scale out</strong>:</p>
    <ul>
      <li class="bulletList">A <strong class="keyWord">scale-up</strong> approach<a id="_idIndexMarker990"/> increases storage capacity if data exceeds the current system capacity, such as by adding more disks. This is useful in fast-access platforms.</li>
      <li class="bulletList">In a <strong class="keyWord">scale-out</strong> approach, storage capacity <a id="_idIndexMarker991"/>grows incrementally with additional nodes in a <a id="_idIndexMarker992"/>storage cluster. Hadoop Distributed File System (HDFS) (<a href="https://hadoop.apache.org/"><span class="url">https://hadoop.apache.org/</span></a>) and Spark (<a href="https://spark.apache.org/"><span class="url">https://spark.apache.org/</span></a>) are used to store and process big data in<a id="_idIndexMarker993"/> scale-out clusters, where data is spread across hundreds or even thousands of nodes. Also, there are <a id="_idIndexMarker994"/>cloud-based distributed file services, such as S3 in Amazon Web Services (<a href="https://aws.amazon.com/s3/"><span class="url">https://aws.amazon.com/s3/</span></a>), Google <a id="_idIndexMarker995"/>Cloud Storage in Google Cloud (<a href="https://cloud.google.com/storage/"><span class="url">https://cloud.google.com/storage/</span></a>), and <a id="_idIndexMarker996"/>Storage in Microsoft Azure (<a href="https://azure.microsoft.com/en-us/services/storage/"><span class="url">https://azure.microsoft.com/en-us/services/storage/</span></a>). They are massively scalable and are designed for secure and durable storage.</li>
    </ul>
    <p class="normal">Besides choosing the right storage system to increase capacity, you also need to pay attention to the following practices:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Data partitioning</strong>: Divide your data into smaller partitions or shards. This distributes the load across multiple servers or nodes, enabling better parallel processing and retrieval.</li>
      <li class="bulletList"><strong class="keyWord">Data compression and encoding</strong>: Implement data compression techniques to reduce storage space and optimize data retrieval times.</li>
      <li class="bulletList"><strong class="keyWord">Replication and redundancy</strong>: Replicate data across multiple storage nodes or geographical locations to ensure data availability and fault tolerance.</li>
      <li class="bulletList"><strong class="keyWord">Security and access control</strong>: Implement robust access control mechanisms to ensure <a id="_idIndexMarker997"/>that only authorized personnel can <a id="_idIndexMarker998"/>access sensitive data.</li>
    </ul>
    <p class="normal">With well-prepared data, it is safe to move on to the training set generation stage. Let’s see the next section.</p>
    <h1 class="heading-1" id="_idParaDest-229">Best practices in the training set generation stage</h1>
    <p class="normal">Typical<a id="_idIndexMarker999"/> tasks in this stage can be summarized into two<a id="_idIndexMarker1000"/> major <a id="_idIndexMarker1001"/>categories: <strong class="keyWord">data preprocessing</strong> and <strong class="keyWord">feature engineering</strong>.</p>
    <p class="normal">To begin, data preprocessing usually involves categorical feature encoding, feature scaling, feature selection, and dimensionality reduction.</p>
    <h2 class="heading-2" id="_idParaDest-230">Best practice 6 – Identifying categorical features with numerical values</h2>
    <p class="normal">In general, categorical<a id="_idIndexMarker1002"/> features are easy to spot, as they convey qualitative information, such as risk level, occupation, and interests. However, it gets tricky if the feature takes on a discreet and countable (limited) number of numerical values, for instance, 1 to 12 representing months of the year, and 1 and 0 indicating true and false.</p>
    <p class="normal">The key to identifying whether such a feature is categorical or numerical is whether it provides a mathematical or ranking implication; if it does, it is a numerical feature, such as a product rating from 1 to 5; otherwise, it is categorical, such as the month, or day of the week.</p>
    <h2 class="heading-2" id="_idParaDest-231">Best practice 7 – Deciding whether to encode categorical features</h2>
    <p class="normal">If a feature is<a id="_idIndexMarker1003"/> considered categorical, we need to decide whether we should encode it. This depends on what prediction algorithm(s) we will use in later stages. Naïve Bayes and tree-based algorithms can directly work with categorical features, while other algorithms in general cannot, in which case encoding is essential.</p>
    <p class="normal">As the output of the feature generation stage is the input of the model training stage, <em class="italic">steps taken in the feature generation stage should be compatible with the prediction algorithm</em>. Therefore, we should look at the two stages of feature generation and predictive model training as a whole, instead of two isolated components. The next two practical tips also reinforce this point.</p>
    <h2 class="heading-2" id="_idParaDest-232">Best practice 8 – Deciding whether to select features and, if so, how to do so</h2>
    <p class="normal">You have <a id="_idIndexMarker1004"/>seen, in <em class="chapterRef">Chapter 4</em>, <em class="italic">Predicting Online Ad Click-Through with Logistic Regression</em>, how feature selection can be performed using L1-based<a id="_idIndexMarker1005"/> regularized logistic regression and random forest. The benefits of feature selection include the following:</p>
    <ul>
      <li class="bulletList">Reducing the training time of prediction models as redundant or irrelevant features are eliminated</li>
      <li class="bulletList">Reducing overfitting for the same preceding reason</li>
      <li class="bulletList">Likely improving performance, as prediction models will learn from data with more significant features</li>
    </ul>
    <p class="normal">Note that we used the word <em class="italic">likely</em> because there is no absolute certainty that feature selection will increase prediction accuracy. It is, therefore, good practice to compare the performances of conducting feature selection and not doing so via cross-validation. For example, by executing the following steps, we can measure the effects of feature selection by <a id="_idIndexMarker1006"/>estimating the averaged classification accuracy with an <code class="inlineCode">SVC</code> model in a cross-validation manner:</p>
    <ol>
      <li class="numberedList" value="1">First, we load the handwritten digits dataset from <code class="inlineCode">scikit-learn</code>, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.datasets </span><span class="hljs-con-keyword">import</span><span class="language-python"> load_digits</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">dataset = load_digits()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X, y = dataset.data, dataset.target</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(X.shape)</span>
(1797, 64)
</code></pre>
      </li>
      <li class="numberedList">Next, estimate the accuracy of the original dataset, which is 64-dimensional, as detailed here:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.svm </span><span class="hljs-con-keyword">import</span><span class="language-python"> SVC</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.model_selection </span><span class="hljs-con-keyword">import</span><span class="language-python"> cross_val_score</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">classifier = SVC(gamma=</span><span class="hljs-con-number">0.005</span><span class="language-python">, random_state=</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">score = cross_val_score(classifier, X, y).mean()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Score with the original data set: </span><span class="hljs-con-subst">{score:</span><span class="hljs-con-number">.2</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
Score with the original data set: 0.90
</code></pre>
      </li>
      <li class="numberedList">Then, conduct feature selection based on random forest and sort the features based on their importance scores:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.ensemble </span><span class="hljs-con-keyword">import</span><span class="language-python"> RandomForestClassifier</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">random_forest = RandomForestClassifier(n_estimators=</span><span class="hljs-con-number">100</span><span class="language-python">,</span>
                                           criterion='gini',
                                           n_jobs=-1,
                                           random_state=42)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">random_forest.fit(X, y)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">feature_sorted = np.argsort(random_forest.feature_importances_)</span>
</code></pre>
      </li>
      <li class="numberedList">Now select a <a id="_idIndexMarker1007"/>different number of top features to construct a new dataset, and estimate the accuracy on each dataset, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">K = [</span><span class="hljs-con-number">10</span><span class="language-python">, </span><span class="hljs-con-number">15</span><span class="language-python">, </span><span class="hljs-con-number">25</span><span class="language-python">, </span><span class="hljs-con-number">35</span><span class="language-python">, </span><span class="hljs-con-number">45</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> k </span><span class="hljs-con-keyword">in</span><span class="language-python"> K:</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    top_K_features = feature_sorted[-k:]</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    X_k_selected = X[:, top_K_features]</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-comment"># Estimate accuracy on the data set with k</span>
          selected features
<span class="hljs-con-meta">...</span> <span class="language-python">    classifier = SVC(gamma=</span><span class="hljs-con-number">0.005</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    score_k_features = cross_val_score(classifier,</span>
                              X_k_selected, y).mean()
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Score with the dataset of top </span><span class="hljs-con-subst">{k}</span><span class="hljs-con-string"> features:</span>
              {score_k_features:.2f}')
<span class="hljs-con-meta">...</span>
Score with the dataset of top 10 features: 0.86
Score with the dataset of top 15 features: 0.92
Score with the dataset of top 25 features: 0.95
Score with the dataset of top 35 features: 0.93
Score with the dataset of top 45 features: 0.90
</code></pre>
      </li>
    </ol>
    <p class="normal">If we use<a id="_idIndexMarker1008"/> the top 25 features selected by random forest, the SVM classification performance can increase from <code class="inlineCode">0.9</code> to <code class="inlineCode">0.95</code>.</p>
    <h2 class="heading-2" id="_idParaDest-233">Best practice 9 – Deciding whether to reduce dimensionality and, if so, how to do so</h2>
    <p class="normal">Feature selection and<a id="_idIndexMarker1009"/> dimensionality are different in the sense that the former chooses features from the original data space, while the latter does so from a projected space from the original space. Dimensionality reduction has the following advantages that are similar to feature selection:</p>
    <ul>
      <li class="bulletList">Reducing the training time of prediction models, as redundant or correlated features are merged into new ones</li>
      <li class="bulletList">Reducing overfitting for the same reason</li>
      <li class="bulletList">Likely improving performance, as prediction models will learn from data with less redundant or correlated features</li>
    </ul>
    <p class="normal">Again, it is<a id="_idIndexMarker1010"/> not guaranteed that dimensionality reduction<a id="_idIndexMarker1011"/> will yield better prediction results. In order to examine its effects, integrating dimensionality reduction in the model training stage is recommended. Reusing the preceding handwritten <a id="_idIndexMarker1012"/>digits example, we can measure the effects of <strong class="keyWord">Principal Component Analysis</strong> (<strong class="keyWord">PCA</strong>)-based dimensionality reduction, where we keep a different number of top components to construct a new dataset, and estimate the accuracy on each dataset:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.decomposition </span><span class="hljs-con-keyword">import</span><span class="language-python"> PCA</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-comment"># Keep different number of top components</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">N = [</span><span class="hljs-con-number">10</span><span class="language-python">, </span><span class="hljs-con-number">15</span><span class="language-python">, </span><span class="hljs-con-number">25</span><span class="language-python">, </span><span class="hljs-con-number">35</span><span class="language-python">, </span><span class="hljs-con-number">45</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> n </span><span class="hljs-con-keyword">in</span><span class="language-python"> N:</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    pca = PCA(n_components=n)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    X_n_kept = pca.fit_transform(X)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-comment"># Estimate accuracy on the data set with top n components</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    classifier = SVC(gamma=</span><span class="hljs-con-number">0.005</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    score_n_components =</span>
                   cross_val_score(classifier, X_n_kept, y).mean()
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Score with the dataset of top </span><span class="hljs-con-subst">{n}</span><span class="hljs-con-string"> components: </span>
                                    {score_n_components:.2f}')
Score with the dataset of top 10 components: 0.94
Score with the dataset of top 15 components: 0.95
Score with the dataset of top 25 components: 0.93
Score with the dataset of top 35 components: 0.91
Score with the dataset of top 45 components: 0.90
</code></pre>
    <p class="normal">If we use the top 15 features generated by PCA, the SVM classification performance can increase from <code class="inlineCode">0.9</code> to <code class="inlineCode">0.95</code>.</p>
    <h2 class="heading-2" id="_idParaDest-234">Best practice 10 – Deciding whether to rescale features</h2>
    <p class="normal">As seen in <em class="chapterRef">Chapter 5</em>, <em class="italic">Predicting Stock Prices with Regression Algorithms</em>, and <em class="chapterRef">Chapter 6</em>, <em class="italic">Predicting Stock Prices with Artificial Neural Networks</em>, SGD-based linear regression, SVR, and the neural <a id="_idIndexMarker1013"/>network model require features to be standardized by removing the mean and scaling to unit variance. So, when is feature scaling needed, and when is it not?</p>
    <p class="normal">In general, Naïve Bayes and tree-based algorithms are not sensitive to features at different scales, as they look at each feature independently.</p>
    <p class="normal">In most cases, an algorithm that involves any form of distance (or separation in spaces) of samples in<a id="_idIndexMarker1014"/> learning requires scaled/standardized inputs, such as SVC, SVR, k-means clustering, and <strong class="keyWord">k-nearest neighbors</strong> (<strong class="keyWord">KNN</strong>) algorithms. Feature scaling is also a must for any algorithm using SGD for optimization, such as linear or logistic regression with gradient descent, and neural networks.</p>
    <p class="normal">We have so <a id="_idIndexMarker1015"/>far covered tips regarding data preprocessing and will next discuss best practices of feature engineering as another major aspect of training set generation. We will do so from two perspectives.</p>
    <h2 class="heading-2" id="_idParaDest-235">Best practice 11 – Performing feature engineering with domain expertise</h2>
    <p class="normal">If we are<a id="_idIndexMarker1016"/> lucky enough to possess sufficient domain knowledge, we can apply it in creating domain-specific features; we utilize our business experience and insights to identify what is in the data and formulate new data that correlates to the prediction target. For example, in <em class="chapterRef">Chapter 5</em>, <em class="italic">Predicting Stock Prices with Regression Algorithms</em>, we designed and constructed feature sets for the prediction of stock prices based on factors that investors usually look at when making investment decisions.</p>
    <p class="normal">While particular domain knowledge is required, sometimes we can still apply some general tips in this category. For example, in fields related to customer analytics, such as marketing and advertising, the time of the day, day of the week, and month are usually important signals. Given a data point with the value <em class="italic">2020/09/01</em> in the <code class="inlineCode">Date</code> column and <em class="italic">14:34:21</em> in the <code class="inlineCode">Time</code> column, we can create new features including <em class="italic">afternoon</em>, <em class="italic">Tuesday</em>, and <em class="italic">September</em>. In retail, information covering a period of time is usually aggregated to provide better insights. The number of times a customer visited a store in the past three months, or the average number of products purchased weekly in the previous year, for instance, can be good predictive indicators for customer behavior prediction.</p>
    <h2 class="heading-2" id="_idParaDest-236">Best practice 12 – Performing feature engineering without domain expertise</h2>
    <p class="normal">If, unfortunately, we<a id="_idIndexMarker1017"/> have very little domain knowledge, how can <a id="_idIndexMarker1018"/>we generate features? Don’t panic. There are several generic approaches that you can follow, such as binarization, discretization, interaction, and polynomial transformation.</p>
    <h3 class="heading-3" id="_idParaDest-237">Binarization and discretization</h3>
    <p class="normal"><strong class="keyWord">Binarization</strong> is<a id="_idIndexMarker1019"/> the <a id="_idIndexMarker1020"/>process of converting a numerical feature to a binary one with a preset threshold. For example, in spam email detection, for the feature (or term) <em class="italic">prize</em>, we can generate a new feature, <code class="inlineCode">whether_term_prize_occurs</code>: any term frequency value greater than 1 becomes 1; otherwise, it is 0. The feature <em class="italic">number of visits per week</em> can be used to produce a new feature, <code class="inlineCode">is_frequent_visitor</code>, by judging whether the value is greater than or equal to 3. We implement such binarization using scikit-learn, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.preprocessing </span><span class="hljs-con-keyword">import</span><span class="language-python"> Binarizer</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X = [[</span><span class="hljs-con-number">4</span><span class="language-python">], [</span><span class="hljs-con-number">1</span><span class="language-python">], [</span><span class="hljs-con-number">3</span><span class="language-python">], [</span><span class="hljs-con-number">0</span><span class="language-python">]]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">binarizer = Binarizer(threshold=</span><span class="hljs-con-number">2.9</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_new = binarizer.fit_transform(X)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(X_new)</span>
[[1]
 [0]
 [1]
 [0]]
</code></pre>
    <p class="normal"><strong class="keyWord">Discretization</strong> is the process<a id="_idIndexMarker1021"/> of converting a numerical feature to a categorical feature with limited possible values. Binarization can be viewed as a special case of discretization. For example, we can generate an <em class="italic">age group</em> feature: “<em class="italic">18-24</em>” for ages from 18 to 24, “<em class="italic">25-34</em>” for ages from 25 to 34, “<em class="italic">34-54</em>”, and “<em class="italic">55+</em>”.</p>
    <h3 class="heading-3" id="_idParaDest-238">Interaction</h3>
    <p class="normal">This includes<a id="_idIndexMarker1022"/> the sum, multiplication, or any operations of two numerical features, and the joint condition check of two categorical features. For example, <em class="italic">the number of visits per week</em> and <em class="italic">the number of products purchased per week</em> can be used to generate <em class="italic">the number of products purchased per visit</em> feature; <em class="italic">interest and occupation</em>, such as <em class="italic">sports</em> and <em class="italic">engineer</em>, can form <em class="italic">occupation AND interest</em>, such as <em class="italic">engineer interested in sports</em>.</p>
    <h3 class="heading-3" id="_idParaDest-239">Polynomial transformation</h3>
    <p class="normal">This is the process of generating<a id="_idIndexMarker1023"/> polynomial and interaction features. For two features, <em class="italic">a</em> and <em class="italic">b</em>, the two degrees of polynomial features generated are <em class="italic">a</em><sup class="superscript-italic" style="font-style: italic;">2</sup>, <em class="italic">ab</em>, and <em class="italic">b</em><sup class="superscript-italic" style="font-style: italic;">2</sup>. In scikit-learn, we can use the <code class="inlineCode">PolynomialFeatures</code> class (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html</span></a>) to perform polynomial transformation, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.preprocessing </span><span class="hljs-con-keyword">import</span><span class="language-python"> PolynomialFeatures</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X = [[</span><span class="hljs-con-number">2</span><span class="language-python">, </span><span class="hljs-con-number">4</span><span class="language-python">],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">     [</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">3</span><span class="language-python">],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">     [</span><span class="hljs-con-number">3</span><span class="language-python">, </span><span class="hljs-con-number">2</span><span class="language-python">],</span>
<span class="hljs-con-meta">...</span> <span class="language-python">     [</span><span class="hljs-con-number">0</span><span class="language-python">, </span><span class="hljs-con-number">3</span><span class="language-python">]]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">poly = PolynomialFeatures(degree=</span><span class="hljs-con-number">2</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_new = poly.fit_transform(X)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(X_new)</span>
[[ 1. 2. 4. 4. 8. 16.]
 [ 1. 1. 3. 1. 3. 9.]
 [ 1. 3. 2. 9. 6. 4.]
 [ 1. 0. 3. 0. 0. 9.]]
</code></pre>
    <p class="normal">Note the resulting new features consist of <em class="italic">1</em> (bias, intercept), <em class="italic">a</em>, <em class="italic">b</em>, <em class="italic">a</em><sup class="superscript-italic" style="font-style: italic;">2</sup>, <em class="italic">ab</em>, and <em class="italic">b</em><sup class="superscript-italic" style="font-style: italic;">2</sup>.</p>
    <h2 class="heading-2" id="_idParaDest-240">Best practice 13 – Documenting how each feature is generated</h2>
    <p class="normal">We have covered <a id="_idIndexMarker1024"/>the rules of feature engineering with <a id="_idIndexMarker1025"/>domain knowledge, and in general, there is one more thing worth noting: documenting how each feature is generated. It sounds trivial, but oftentimes we just forget about how a feature is obtained or created. We usually need to go back to this stage after some failed trials in the model training stage and attempt to create more features with the hope of improving performance. We have to be clear on what and how features are generated, in order to remove those that do not quite work out, and to add new ones that have more potential.</p>
    <h2 class="heading-2" id="_idParaDest-241">Best practice 14 – Extracting features from text data</h2>
    <p class="normal">We will <a id="_idIndexMarker1026"/>start with a traditional approach to extract features from text, tf, and tf-idf. Then, we will continue with a modern approach: word embedding. Specifically, we will look at word embedding using <code class="inlineCode">Word2Vec</code> models, and embedding layers in neural network models.</p>
    <h3 class="heading-3" id="_idParaDest-242">tf and tf-idf</h3>
    <p class="normal">We have worked intensively with text data in <em class="chapterRef">Chapter 7</em>, <em class="keystroke">Mining the 20 Newsgroups Dataset with Text Analysis Techniques</em>, and <em class="chapterRef">Chapter 8</em>, <em class="italic">Discovering Underlying Topics in the Newsgroups Dataset with Clustering and Topic Modeling</em>, where we extracted features from text based <a id="_idIndexMarker1027"/>on <strong class="keyWord">term frequency</strong> (<strong class="keyWord">tf</strong>) and <strong class="keyWord">term frequency-inverse document frequency</strong> (<strong class="keyWord">tf-idf</strong>). Both methods consider each document of words (terms) a collection <a id="_idIndexMarker1028"/>of words, or a <strong class="keyWord">bag of words</strong> (<strong class="keyWord">BoW</strong>), disregarding <a id="_idIndexMarker1029"/>the order of the words but keeping multiplicity. A tf approach simply uses the counts of tokens, while tf-idf extends tf by assigning each tf a weighting factor that is inversely proportional to the document frequency. With the idf factor incorporated, tf-idf diminishes the weight of common terms (such as “get” and “make”) that occur frequently, and emphasizes terms that rarely occur but convey important meaning. Hence, oftentimes, features extracted from tf-idf are more representative than those from tf.</p>
    <p class="normal">As you may remember, a document is represented by a very sparse vector where only present terms have non-zero values. The vector’s dimensionality is usually high, which is determined by the size of the vocabulary and the number of unique terms. Also, such a one-hot encoding approach treats each term as an independent item and does not consider the relationship across words (referred to as “context” in linguistics).</p>
    <h3 class="heading-3" id="_idParaDest-243">Word embedding</h3>
    <p class="normal">On the contrary, another <a id="_idIndexMarker1030"/>approach, called <strong class="keyWord">word embedding</strong>, is able to capture the meanings of words and their context. In this approach, a word is represented by a vector of float numbers. Its dimensionality is a lot lower than the size of the vocabulary and is usually several hundred only.</p>
    <p class="normal">The embedding vectors are of real values, where each dimension encodes an aspect of meaning for the words in the vocabulary. This helps preserve the semantic information of the words, as opposed to discarding it as in the one-hot encoding approach using tf or tf-idf. An interesting phenomenon is that vectors from semantically similar words are proximate to each other in geometric space. For example, both the words <em class="italic">clustering and grouping</em> refer to unsupervised clustering in the context of machine learning, hence their embedding vectors are close together.</p>
    <p class="normal">Here are some popular ways to obtain<a id="_idIndexMarker1031"/> word embeddings:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Word2Vec</strong>: Train your <a id="_idIndexMarker1032"/>own Word2Vec embeddings on your specific corpus using the Skip-gram or Continuous Bag of Words (CBOW) models. We covered this in <em class="chapterRef">Chapter 7</em>, <em class="italic">Mining the 20 Newsgroups Dataset with Text Analysis Techniques</em>. Libraries like Gensim in Python provide easy-to-use interfaces for training Word2Vec embeddings. We will present a simple example shortly.</li>
      <li class="bulletList"><strong class="keyWord">Pre-trained embeddings</strong>: Use<a id="_idIndexMarker1033"/> pre-trained<a id="_idIndexMarker1034"/> word embeddings<a id="_idIndexMarker1035"/> that <a id="_idIndexMarker1036"/>are trained on large corpora. We also <a id="_idIndexMarker1037"/>talked about this in <em class="italic">Chapter 7</em>. Popular<a id="_idIndexMarker1038"/> examples include:<ul>
          <li class="bulletList level-2">FastText</li>
          <li class="bulletList level-2"><strong class="keyWord">GloVe</strong> (<strong class="keyWord">Global Vectors for Word Representation</strong>)</li>
          <li class="bulletList level-2"><strong class="keyWord">BERT</strong> (<strong class="keyWord">Bidirectional Encoder Representations from Transformers</strong>)</li>
          <li class="bulletList level-2"><strong class="keyWord">GPT</strong> (<strong class="keyWord">Generative Pre-trained Transformer</strong>)</li>
          <li class="bulletList level-2"><strong class="keyWord">USE</strong> (<strong class="keyWord">Universal Sentence Encoder</strong>) embeddings</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">Training custom models with an embedding layer</strong>: If you have a specific domain or dataset, you can train your own word embeddings using custom neural network models.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-244">Word2Vec embedding</h3>
    <p class="normal">Prior to delving into training a <a id="_idIndexMarker1039"/>custom model for word embeddings, let’s begin with the following example of training a basic <code class="inlineCode">Word2Vec</code> model using <code class="inlineCode">gensim</code>:</p>
    <p class="normal">We first import the <code class="inlineCode">gensim</code> module:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> gensim.models </span><span class="hljs-con-keyword">import</span><span class="language-python"> Word2Vec</span>
</code></pre>
    <p class="normal">We define some sample sentences for training:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sentences = [</span>
    ["i", "love", "machine", "learning", "by", "example"],
    ["machine", "learning", "and", "deep", "learning", "are", "fascinating"],
    ["word", "embedding", "is", "essential", "for", "many", "nlp", "tasks"],
    ["word2vec", "produces", "word", "embeddings"]
]
</code></pre>
    <p class="normal">In practice, you will need to format the sentences in plain text into a list of word lists just like the <code class="inlineCode">sentences</code> object.</p>
    <p class="normal">We then create a Word2Vec model with various parameters, such as <code class="inlineCode">vector_size</code> (embedding dimension), <code class="inlineCode">window</code> (context window size), <code class="inlineCode">min_count</code> (minimum frequency of words), and <code class="inlineCode">sg</code> (training algorithm – <code class="inlineCode">0</code> for CBOW, <code class="inlineCode">1</code> for Skip-gram):</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = Word2Vec(sentences=sentences, vector_size=</span><span class="hljs-con-number">100</span><span class="language-python">, window=</span><span class="hljs-con-number">5</span><span class="language-python">,</span>
                     min_count=1, sg=0)
</code></pre>
    <p class="normal">After training, we <a id="_idIndexMarker1040"/>access word vectors using the model’s <code class="inlineCode">wv</code> property. Here, we display the embedding vector for the word <em class="italic">machine:</em></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vector = model.wv[</span><span class="hljs-con-string">"machine"</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">"Vector for 'machine':"</span><span class="language-python">, vector)</span>
Vector for 'machine': [ 9.2815855e-05  3.0779743e-03 -6.8117767e-03 -1.3753572e-03 7.6693585e-03  7.3465472e-03 -3.6724545e-03  2.6435424e-03 -8.3174659e-03  6.2051434e-03 -4.6373457e-03 -3.1652437e-03 9.3113342e-03  8.7273103e-04  7.4911476e-03 -6.0739564e-03 5.1591368e-03  9.9220201e-03 -8.4587047e-03 -5.1362212e-03 -7.0644980e-03 -4.8613679e-03 -3.7768795e-03 -8.5355258e-03 7.9550967e-03 -4.8430962e-03  8.4243221e-03  5.2609886e-03 -6.5501807e-03  3.9575580e-03  5.4708594e-03 -7.4282014e-03 -7.4055856e-03 -2.4756377e-03 -8.6252270e-03 -1.5801827e-03 -4.0236043e-04  3.3001360e-03  1.4415972e-03 -8.8241365e-04 -5.5940133e-03  1.7302597e-03 -8.9826871e-04  6.7939684e-03 3.9741215e-03  4.5290575e-03  1.4341431e-03 -2.6994087e-03 -4.3666936e-03 -1.0321270e-03  1.4369689e-03 -2.6467817e-03 -7.0735654e-03 -7.8056543e-03 -9.1217076e-03 -5.9348154e-03 -1.8470082e-03 -4.3242811e-03 -6.4605214e-03 -3.7180765e-03 4.2892280e-03 -3.7388816e-03  8.3797537e-03  1.5337169e-03 -7.2427099e-03  9.4338059e-03  7.6304432e-03  5.4950463e-03 -6.8496312e-03  5.8225882e-03  4.0093577e-03  5.1861661e-03 4.2569390e-03  1.9407619e-03 -3.1710821e-03  8.3530620e-03 9.6114948e-03  3.7916750e-03 -2.8375010e-03  6.6632601e-06 1.2186278e-03 -8.4594022e-03 -8.2233679e-03 -2.3177716e-04 1.2370384e-03 -5.7435711e-03 -4.7256653e-03 -7.3463405e-03 8.3279097e-03  1.2112247e-04 -4.5090448e-03  5.7024667e-03 9.1806483e-03 -4.0998533e-03  7.9661217e-03  5.3769764e-03 5.8786790e-03  5.1239668e-04  8.2131373e-03 -7.0198057e-03]
</code></pre>
    <p class="normal">Keep in mind that <a id="_idIndexMarker1041"/>this is a basic example. In practice, you might need to preprocess your data more thoroughly, adjust hyperparameters, and train on a larger corpus for better embeddings.</p>
    <h4 class="heading-4">Embedding layers in custom neural networks</h4>
    <p class="normal">In a complete deep <a id="_idIndexMarker1042"/>neural network for NLP tasks, we would typically combine an embedding layer with other layers, like fully connected (dense) layers, or recurrent layers (we will talk about recurrent layers in <em class="chapterRef">Chapter 12</em><em class="italic">, Making Predictions with Sequences Using Recurrent Neural Networks</em>) to build a more sophisticated model. The embedding layer allows the network to learn meaningful representations for words in the input data.</p>
    <p class="normal">Let’s look at a simplified example of using an embedding layer for word embeddings. In PyTorch, we use the <code class="inlineCode">nn.Embedding</code> module (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"><span class="url">https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html</span></a>) for embedding layers:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch.nn </span><span class="hljs-con-keyword">as</span><span class="language-python"> nn</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">input_data = torch.LongTensor([[</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">2</span><span class="language-python">, </span><span class="hljs-con-number">3</span><span class="language-python">, </span><span class="hljs-con-number">4</span><span class="language-python">], [</span><span class="hljs-con-number">5</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">6</span><span class="language-python">, </span><span class="hljs-con-number">3</span><span class="language-python">]])</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-comment"># Define the embedding layer</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vocab_size = </span><span class="hljs-con-number">10</span><span class="language-python">  </span><span class="hljs-con-comment"># Total number of unique words</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">embedding_dim = </span><span class="hljs-con-number">3</span><span class="language-python">  </span><span class="hljs-con-comment"># Dimensionality of the embeddings</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">embedding_layer = nn.Embedding(vocab_size, embedding_dim)</span>
<span class="hljs-con-meta">...</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">embedded_data = embedding_layer(input_data) </span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">"Embedded Data:\n"</span><span class="language-python">, embedded_data)</span>
Embedded Data:
tensor([[[-1.2462,  0.4035,  0.4463],
        [-0.5218,  0.8302, -0.6920],
        [-0.4720, -1.2894,  1.0763],
        [-2.2879, -0.4834,  0.3416]],
       [[ 1.5886, -0.3489, -0.4579],
        [-1.2462,  0.4035,  0.4463],
        [-1.2322, -0.5981, -0.1349],
        [-0.4720, -1.2894,  1.0763]]], grad_fn=&lt;EmbeddingBackward0&gt;)
</code></pre>
    <p class="normal">In this example, we<a id="_idIndexMarker1043"/> first import the necessary modules from PyTorch. We define some sample input data containing word indices (for example, 1 represents <em class="italic">I</em>, 2 represents <em class="italic">love</em>, 3 represents <em class="italic">machine</em>, and 4 represents <em class="italic">learning</em>). Then, we define the embedding layer using <code class="inlineCode">nn.Embedding</code> with <code class="inlineCode">vocab_size</code> as the total number of unique words in the vocabulary, and <code class="inlineCode">embedding_dim</code> as the desired dimensionality of the embeddings. The embedding layer is usually the first layer of a neural network model after the input layer. Upon completion of network training, when we pass the input data through the embedding layer, it returns embedded vectors for each input word index. The shape of output <code class="inlineCode">embedded_data</code> will be <code class="inlineCode">(sample size, sequence length, embedding_dim)</code>, which is <code class="inlineCode">(2, 4, 3)</code> in our case.</p>
    <p class="normal">Once again, this is a simplified example. In practice, the embedding layers are involved in more complex architectures with additional layers, in order to process and interpret the embeddings for specific tasks, such as classification, sentiment analysis, or sequence generation. Keep an eye out for the upcoming <em class="chapterRef">Chapter 12</em>.</p>
    <p class="normal">Curious about the choice between tf-idf and word embeddings? In conventional NLP applications, such as simple text classification and topic modeling, tf, or tf-idf, remains an exceptional method for feature extraction. In more complicated areas, such as text summarization, machine translation, named entity resolution, question answering, and information retrieval, word embeddings are extensively utilized and yield significantly enhanced features compared to conventional methods.</p>
    <p class="normal">Now that you have reviewed the best practices for data and feature generation, let’s look at model training next.</p>
    <h1 class="heading-1" id="_idParaDest-245">Best practices in the model training, evaluation, and selection stage</h1>
    <p class="normal">Given a <a id="_idIndexMarker1044"/>supervised machine learning <a id="_idIndexMarker1045"/>problem, the first question many people ask is usually <em class="italic">What is the best classification or regression algorithm to solve it?</em> However, there is no one-size-fits-all solution and no free lunch. No one could know which algorithm will work best before trying multiple ones and fine-tuning the optimal one. We will be looking into best practices around this in this section.</p>
    <h2 class="heading-2" id="_idParaDest-246">Best practice 15 – Choosing the right algorithm(s) to start with</h2>
    <p class="normal">Due<a id="_idIndexMarker1046"/> to the fact that there are<a id="_idIndexMarker1047"/> several parameters to tune for an algorithm, exhausting all algorithms and fine-tuning each one can be extremely time-consuming and computationally expensive. We should instead shortlist one to three algorithms to start with using the general guidelines that follow (note we herein focus on classification, but the theory transcends to regression, and there is usually a counterpart algorithm in regression).</p>
    <p class="normal">There are several things we need to be clear about before shortlisting potential algorithms, as described in the following:</p>
    <ul>
      <li class="bulletList">The size of the training dataset</li>
      <li class="bulletList">The dimensionality of the dataset</li>
      <li class="bulletList">Whether the data is linearly separable</li>
      <li class="bulletList">Whether features are independent</li>
      <li class="bulletList">Tolerance and trade-off of bias and variance</li>
      <li class="bulletList">Whether online learning is required</li>
    </ul>
    <p class="normal">Now, let’s look at how we choose the right algorithm to start with, taking into account the aforementioned perspectives.</p>
    <h3 class="heading-3" id="_idParaDest-247">Naïve Bayes</h3>
    <p class="normal">This is a very simple algorithm. For a relatively small training dataset, if features are independent, Naïve Bayes <a id="_idIndexMarker1048"/>will usually perform well. For a large dataset, Naïve Bayes will still work well as feature independence can be assumed in this case, regardless of the truth. The training of Naïve Bayes is usually faster than any other algorithm due to its computational simplicity. However, this may lead to a high bias (but low variance).</p>
    <h3 class="heading-3" id="_idParaDest-248">Logistic regression</h3>
    <p class="normal">This is probably the most widely used classification <a id="_idIndexMarker1049"/>algorithm, and the first algorithm that a machine learning practitioner usually tries when given a classification problem. It performs well when data is linearly separable or approximately <strong class="keyWord">linearly separable</strong>. Even if it is not linearly separable, it might be possible to convert the linearly non-separable features into separable ones and apply logistic regression afterward.</p>
    <p class="normal">In the following instance, data <a id="_idIndexMarker1050"/>in the original space is not linearly separable, but it becomes separable in a transformed space<a id="_idIndexMarker1051"/> created from the interaction of two features:</p>
    <figure class="mediaobject"><img alt="A screenshot of a cell phone  Description automatically generated with low confidence" src="../Images/B21047_10_02.png"/></figure>
    <p class="packt_figref">Figure 10.2: Transforming features from linearly non-separable to separable</p>
    <p class="normal">Also, logistic<a id="_idIndexMarker1052"/> regression is extremely scalable to<a id="_idIndexMarker1053"/> large datasets with SGD optimization, which makes it efficient in solving big data problems. Plus, it makes online learning feasible. Although <a id="_idIndexMarker1054"/>logistic regression is a low-bias, high-variance algorithm, we overcome the potential overfitting by adding L1, L2, or a mix of the two regularizations.</p>
    <h3 class="heading-3" id="_idParaDest-249">SVM</h3>
    <p class="normal">This is versatile enough to <a id="_idIndexMarker1055"/>adapt to the linear separability of data. For a separable dataset, SVM with a linear kernel performs<a id="_idIndexMarker1056"/> comparably to logistic regression. Beyond this, SVM also works well for a non-separable dataset if equipped with a non-linear kernel, such as RBF. Logistic regression may face challenges in high-dimensional datasets, while SVM still performs well. A good example of this can be in news classification, where the feature dimensionality is in the tens of thousands. In general, very high accuracy can be achieved by SVM with the right kernel and parameters. However, this might be at the expense of intense computation and high memory consumption.</p>
    <h3 class="heading-3" id="_idParaDest-250">Random forest (or decision tree)</h3>
    <p class="normal">The linear separability of the<a id="_idIndexMarker1057"/> data does not matter to this algorithm, and it works directly with categorical features without encoding, which provides great ease of use. Also, the trained model is very easy to interpret and explain to non-machine learning practitioners, which cannot be achieved with most other algorithms. Additionally, random forest boosts the decision tree algorithm, which can reduce overfitting by ensembling a collection of separate trees. Its performance is comparable to SVM, while fine-tuning a random forest model is less difficult compared to SVM and neural networks.</p>
    <h3 class="heading-3" id="_idParaDest-251">Neural networks</h3>
    <p class="normal">These are<a id="_idIndexMarker1058"/> extremely powerful, especially with the <a id="_idIndexMarker1059"/>development of deep learning. However, finding the right topology (layers, nodes, activation functions, and so on) is not easy, not to mention the time-consuming model of training and tuning. Hence, they are not recommended as an algorithm to start with for general machine learning problems. However, for computer vision and many NLP tasks, the neural network is still the go-to model. In summary, here are some scenarios where using neural networks is particularly beneficial:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Complex patterns</strong>: When the <a id="_idIndexMarker1060"/>task involves learning complex patterns or relationships within the data that may be difficult for traditional algorithms to capture.</li>
      <li class="bulletList"><strong class="keyWord">Large amounts of data</strong>: Neural networks tend to perform well when you have a substantial amount of data available for training, as they are capable of learning from large datasets.</li>
      <li class="bulletList"><strong class="keyWord">Unstructured data</strong>: Neural networks excel in handling unstructured data types like images, audio, and text, where traditional methods might struggle to extract meaningful features. For NLP tasks like sentiment analysis, machine translation, named entity recognition, and text generation, neural networks, especially recurrent and transformer models, have shown remarkable performance. In image classification, object detection, segmentation, and image generation tasks, deep neural networks have revolutionized<a id="_idIndexMarker1061"/> computer vision.</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-252">Best practice 16 – Reducing overfitting</h2>
    <p class="normal">We touched <a id="_idIndexMarker1062"/>on ways to avoid overfitting <a id="_idIndexMarker1063"/>when discussing the pros and cons of algorithms in the last practice. We herein formally summarize them, as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">More data, if possible</strong>: Increase the size of your training dataset. More data can help the model learn relevant patterns and reduce its tendency to memorize noise.</li>
      <li class="bulletList"><strong class="keyWord">Simplification, if possible</strong>: The more complex the model is, the higher the chance of overfitting. Complex models include a tree or forest with excessive depth, a linear regression with a high degree of polynomial transformation, and an SVM with a complicated kernel.</li>
      <li class="bulletList"><strong class="keyWord">Cross-validation</strong>: A good habit that we have built over all of the chapters in this book.</li>
      <li class="bulletList"><strong class="keyWord">Regularization</strong>: This adds penalty terms to reduce the error caused by fitting the model perfectly on the given training set.</li>
      <li class="bulletList"><strong class="keyWord">Early stopping</strong>: Monitor the model’s performance on a validation set during training. Stop training when the performance on the validation set starts to degrade, indicating that the model is starting to overfit.</li>
      <li class="bulletList"><strong class="keyWord">Dropout</strong>: In neural networks, apply dropout layers during training. Dropout randomly drops out a fraction of neurons during each forward pass, preventing reliance on specific neurons.</li>
      <li class="bulletList"><strong class="keyWord">Feature selection</strong>: Select a subset of relevant features. Removing irrelevant or redundant features can prevent the model from fitting noise.</li>
      <li class="bulletList"><strong class="keyWord">Ensemble learning</strong>: This involves combining a collection of weak models to form a stronger one.</li>
    </ul>
    <p class="normal">So, how <a id="_idIndexMarker1064"/>can we tell whether a model suffers <a id="_idIndexMarker1065"/>from overfitting, or the other extreme, underfitting? Let’s see the next section.</p>
    <h2 class="heading-2" id="_idParaDest-253">Best practice 17 – Diagnosing overfitting and underfitting</h2>
    <p class="normal">A <strong class="keyWord">learning curve</strong><strong class="keyWord"><a id="_idIndexMarker1066"/></strong> is usually used to evaluate the bias and variance of a model. A learning curve is a graph that compares the cross-validated training and testing scores over a<a id="_idIndexMarker1067"/> given number of training samples.</p>
    <p class="normal">For a model that fits well on the training samples, the performance of the training samples should be beyond what’s desired. Ideally, as the number of training samples increases, the model performance on the testing samples will improve; eventually, the performance on the testing samples will become close to that of the training samples.</p>
    <p class="normal">When the performance on the testing samples converges at a value much lower than that of the training performance, overfitting can be concluded. In this case, the model fails to generalize to instances that have not been seen.</p>
    <p class="normal">For a model that does not even fit well on the training samples, underfitting is easily spotted: both performances on the training and testing samples are below the desired performance in the learning curve.</p>
    <p class="normal">Here is an example of the learning curve in an ideal case:</p>
    <figure class="mediaobject"><img alt="A picture containing text, screenshot, line, diagram  Description automatically generated" src="../Images/B21047_10_03.png"/></figure>
    <p class="packt_figref">Figure 10.3: Ideal learning curve</p>
    <p class="normal">An example <a id="_idIndexMarker1068"/>of the learning curve for an <a id="_idIndexMarker1069"/>overfitted model is shown in the following diagram:</p>
    <figure class="mediaobject"><img alt="A diagram of training data  Description automatically generated with low confidence" src="../Images/B21047_10_04.png"/></figure>
    <p class="packt_figref">Figure 10.4: Overfitting learning curve</p>
    <p class="normal">The learning<a id="_idIndexMarker1070"/> curve for an underfitted <a id="_idIndexMarker1071"/>model may look like the following diagram:</p>
    <figure class="mediaobject"><img alt="A picture containing text, screenshot, line, diagram  Description automatically generated" src="../Images/B21047_10_05.png"/></figure>
    <p class="packt_figref">Figure 10.5: Underfitting learning curve</p>
    <p class="normal">To generate<a id="_idIndexMarker1072"/> the learning curve, you can<a id="_idIndexMarker1073"/> utilize the <code class="inlineCode">learning_curve</code> module (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve</span></a>) from scikit-learn, and the <code class="inlineCode">plot_learning_curve</code> function defined at <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"><span class="url">https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html</span></a>.</p>
    <h2 class="heading-2" id="_idParaDest-254">Best practice 18 – Modeling on large-scale datasets</h2>
    <p class="normal">We gained <a id="_idIndexMarker1074"/>experience working with large datasets in <em class="chapterRef">Chapter 4</em>, <em class="italic">Predicting Online Ad Click-Through with Logistic Regression</em>. There are a few tips that can help you model on large-scale data more efficiently.</p>
    <p class="normal">First, start with a small subset, for instance, a subset that can fit on your local machine. This can help speed up early experimentation. Obviously, you don’t want to train on the entire dataset just to find out whether SVM or random forest works better. Instead, you can randomly sample data points and quickly run a few models on the selected set.</p>
    <p class="normal">The second tip is choosing scalable algorithms, such as logistic regression, linear SVM, and SGD-based optimization. This is quite intuitive.</p>
    <p class="normal">Here are other<a id="_idIndexMarker1075"/> best practices for modeling<a id="_idIndexMarker1076"/> on large-scale datasets:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Sampling and subset selection</strong>: When starting model development, work with smaller subsets of your data to iterate and experiment quickly. Once your model architecture and parameters are tuned, scale up to the full dataset.</li>
      <li class="bulletList"><strong class="keyWord">Distributed computing</strong>: Utilize distributed computing frameworks like Apache Spark to handle large-scale data processing and model training across multiple nodes or clusters.</li>
      <li class="bulletList"><strong class="keyWord">Feature engineering</strong>: Focus on relevant features and avoid unnecessary dimensions. Use dimensionality reduction techniques like PCA or t-SNE to reduce feature space if needed.</li>
      <li class="bulletList"><strong class="keyWord">Parallelization</strong>: Explore techniques to parallelize training, like data parallelism or model parallelism, to leverage multiple GPUs or distributed systems.</li>
      <li class="bulletList"><strong class="keyWord">Memory management</strong>: Optimize memory usage by using data generators, streaming data from storage, and releasing memory when no longer needed.</li>
      <li class="bulletList"><strong class="keyWord">Optimized libraries</strong>: Choose libraries and frameworks that are optimized for large-scale data, such as TensorFlow, PyTorch, scikit-learn, and XGBoost.</li>
      <li class="bulletList"><strong class="keyWord">Incremental learning</strong>: For streaming data or dynamic datasets, consider incremental learning<a id="_idIndexMarker1077"/> techniques that update the model as new data arrives.</li>
    </ul>
    <p class="normal">Last but not least, don’t forget to save the trained model. Training on a large dataset takes a long time, which you would want to avoid redoing, if possible. We will explore saving and loading models in detail in <em class="italic">Best practice 19 – Saving, loading, and reusing models</em>, which is a part of the deployment and monitoring stage.</p>
    <h1 class="heading-1" id="_idParaDest-255">Best practices in the deployment and monitoring stage</h1>
    <p class="normal">After performing all <a id="_idIndexMarker1078"/>processes in the previous three<a id="_idIndexMarker1079"/> stages, we now have a well-established data preprocessing pipeline and a correctly trained prediction model. The last stage of a machine learning system involves saving those resulting models from previous stages and deploying them on new data, as well as monitoring their performance and updating the prediction models regularly. We also need to implement monitoring and logging to track model performance, training progress, and potential issues during training.</p>
    <h2 class="heading-2" id="_idParaDest-256">Best practice 19 – Saving, loading, and reusing models</h2>
    <p class="normal">When <a id="_idIndexMarker1080"/>machine learning is deployed, new <a id="_idIndexMarker1081"/>data should go through the same data preprocessing procedures (scaling, feature <a id="_idIndexMarker1082"/>engineering, feature selection, dimensionality reduction, and so on) as in the previous stages. The preprocessed data is then fed into the trained model. We simply cannot rerun the entire process and retrain the model every time new data comes in. Instead, we should save the established preprocessing models and trained prediction models after the corresponding stages have been completed. In deployment mode, these models are loaded in advance and are used to produce prediction results from the new data. Let’s explore methods for saving and loading models using pickle, TensorFlow, and PyTorch below.</p>
    <h3 class="heading-3" id="_idParaDest-257">Saving and restoring models using pickle</h3>
    <p class="normal">We start <a id="_idIndexMarker1083"/>with using <code class="inlineCode">pickle</code>. This can be illustrated via the diabetes example, where we standardize the data and employ an <code class="inlineCode">SVR</code> model, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">dataset = datasets.load_diabetes()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X, y = dataset.data, dataset.target</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">num_new = </span><span class="hljs-con-number">30</span><span class="language-python"> </span><span class="hljs-con-comment"># the last 30 samples as new data set</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_train = X[:-num_new, :]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_train = y[:-num_new]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_new = X[-num_new:, :]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_new = y[-num_new:]</span>
</code></pre>
    <p class="normal">Preprocess the training data with scaling, as shown in the following commands:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.preprocessing </span><span class="hljs-con-keyword">import</span><span class="language-python"> StandardScaler</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">scaler = StandardScaler()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">scaler.fit(X_train)</span>
</code></pre>
    <p class="normal">Now save the established standardizer, the <code class="inlineCode">scaler</code> object with <code class="inlineCode">pickle</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> pickle</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">pickle.dump(scaler, </span><span class="hljs-con-built_in">open</span><span class="language-python">(</span><span class="hljs-con-string">"scaler.p"</span><span class="language-python">, </span><span class="hljs-con-string">"wb"</span><span class="language-python"> ))</span>
</code></pre>
    <p class="normal">This generates<a id="_idIndexMarker1084"/> a <code class="inlineCode">scaler.p</code> file.</p>
    <p class="normal">Move on to training an <code class="inlineCode">SVR</code> model on the scaled data, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_scaled_train = scaler.transform(X_train)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.svm </span><span class="hljs-con-keyword">import</span><span class="language-python"> SVR</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">regressor = SVR(C=</span><span class="hljs-con-number">20</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">regressor.fit(X_scaled_train, y_train)</span>
</code></pre>
    <p class="normal">Save the trained <code class="inlineCode">regressor</code> object with <code class="inlineCode">pickle</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">pickle.dump(regressor, </span><span class="hljs-con-built_in">open</span><span class="language-python">(</span><span class="hljs-con-string">"</span><span class="hljs-con-string">regressor.p"</span><span class="language-python">, </span><span class="hljs-con-string">"wb"</span><span class="language-python">))</span>
</code></pre>
    <p class="normal">This generates a <code class="inlineCode">regressor.p</code> file.</p>
    <p class="normal">In the deployment <a id="_idIndexMarker1085"/>stage, we first load the saved standardizer and the <code class="inlineCode">regressor</code> object from the preceding two files, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">my_scaler = pickle.load(</span><span class="hljs-con-built_in">open</span><span class="language-python">(</span><span class="hljs-con-string">"scaler.p"</span><span class="language-python">, </span><span class="hljs-con-string">"rb"</span><span class="language-python"> ))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">my_regressor = pickle.load(</span><span class="hljs-con-built_in">open</span><span class="language-python">(</span><span class="hljs-con-string">"regressor.p"</span><span class="language-python">, </span><span class="hljs-con-string">"rb"</span><span class="language-python">))</span>
</code></pre>
    <p class="normal">Then, we <a id="_idIndexMarker1086"/>preprocess the new data using the standardizer and make a prediction with the <code class="inlineCode">regressor</code> object just loaded, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_scaled_new = my_scaler.transform(X_new)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = my_regressor.predict(X_scaled_new)</span>
</code></pre>
    <h3 class="heading-3" id="_idParaDest-258">Saving and restoring models in TensorFlow</h3>
    <p class="normal">I will also demonstrate<a id="_idIndexMarker1087"/> how to save and restore models in<a id="_idIndexMarker1088"/> TensorFlow. As an example, we will train a simple logistic regression model on the cancer dataset, save the trained model, and reload it in the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Import the necessary TensorFlow modules and load the cancer dataset from <code class="inlineCode">scikit-learn</code> and rescale the data:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> tensorflow </span><span class="hljs-con-keyword">as</span><span class="language-python"> tf</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> tensorflow </span><span class="hljs-con-keyword">import</span><span class="language-python"> keras</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn </span><span class="hljs-con-keyword">import</span><span class="language-python"> datasets</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">cancer_data = datasets.load_breast_cancer()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X = cancer_data.data</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X = scaler.fit_transform(X)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y = cancer_data.target</span>
</code></pre>
      </li>
      <li class="numberedList">Build a simple logistic regression model using the Keras Sequential API, along with several specified parameters:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">learning_rate = </span><span class="hljs-con-number">0.005</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">n_iter = </span><span class="hljs-con-number">10</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tf.random.set_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = keras.Sequential([</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    keras.layers.Dense(units=</span><span class="hljs-con-number">1</span><span class="language-python">, activation=</span><span class="hljs-con-string">'sigmoid'</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">])</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.</span><span class="hljs-con-built_in">compile</span><span class="language-python">(loss=</span><span class="hljs-con-string">'binary_crossentropy'</span><span class="language-python">,</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        optimizer=tf.keras.optimizers.Adam(learning_rate))</span>
</code></pre>
      </li>
      <li class="numberedList">Train the TensorFlow model against the data:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.fit(X, y, epochs=n_iter)</span>
Epoch 1/10 
18/18 [==============================] - 0s 943us/step - loss: 0.2288 
Epoch 2/10 
18/18 [==============================] - 0s 914us/step - loss: 0.1591 
Epoch 3/10 
18/18 [==============================] - 0s 825us/step - loss: 0.1303 
Epoch 4/10 
18/18 [==============================] - 0s 865us/step - loss: 0.1147 
Epoch 5/10 
18/18 [==============================] - 0s 795us/step - loss: 0.1042 
Epoch 6/10 
18/18 [==============================] - 0s 796us/step - loss: 0.0971 
Epoch 7/10 
18/18 [==============================] - 0s 862us/step - loss: 0.0917 
Epoch 8/10 
18/18 [==============================] - 0s 913us/step - loss: 0.0871 
Epoch 9/10 
18/18 [==============================] - 0s 795us/step - loss: 0.0835 
Epoch 10/10 
18/18 [==============================] - 0s 767us/step - loss: 0.0806
</code></pre>
      </li>
      <li class="numberedList">Display <a id="_idIndexMarker1089"/>the <a id="_idIndexMarker1090"/>model’s architecture:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.summary()</span>
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense (Dense)                multiple                  31
=================================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
_________________________________________________________
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We will see if we can retrieve the same model later.</p>
    <ol>
      <li class="numberedList" value="5">Hopefully, the previous steps look familiar to you. If not, feel free to review our TensorFlow implementation. Now we save the model to a path:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">path = </span><span class="hljs-con-string">'./model_tf'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model.save(path)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">After this, you will see that a folder called <code class="inlineCode">model_tf</code> is created. The folder contains the trained model’s architecture, weights, and training configuration.</p>
    <ol>
      <li class="numberedList" value="6">Finally, we load the model from the previous path and display the loaded model’s path:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">new_model = tf.keras.models.load_model(path)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">new_model.summary()</span>
Model: "sequential"
_________________________________________________________
Layer (type)                 Output Shape              Param #
=========================================================
dense (Dense)                multiple                  31
=========================================================
Total params: 31
Trainable params: 31
Non-trainable params: 0
_________________________________________________________
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We just <a id="_idIndexMarker1091"/>loaded back the exact same model.</p>
    <h3 class="heading-3" id="_idParaDest-259">Saving and restoring models in PyTorch</h3>
    <p class="normal">Finally, let’s see<a id="_idIndexMarker1092"/> how to save and restore models in PyTorch. Similarly, we <a id="_idIndexMarker1093"/>will train a simple logistic regression model on the same cancer dataset, save the trained model, and reload it in the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Convert the data <code class="inlineCode">torch</code> tensors used for modeling:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X_torch = torch.FloatTensor(X)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y_torch = torch.FloatTensor(y.reshape(y.shape[</span><span class="hljs-con-number">0</span><span class="language-python">], </span><span class="hljs-con-number">1</span><span class="language-python">))</span>
</code></pre>
      </li>
      <li class="numberedList">Build a simple logistic regression model using the <code class="inlineCode">nn.sequential</code> module, along with the loss function and optimizer:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.manual_seed(</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = nn.Sequential(nn.Linear(X.shape[</span><span class="hljs-con-number">1</span><span class="language-python">], </span><span class="hljs-con-number">1</span><span class="language-python">),</span>
                          nn.Sigmoid())
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">loss_function = nn.BCELoss()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span>
</code></pre>
      </li>
      <li class="numberedList">Reuse the <code class="inlineCode">train_step</code> function we developed previously in <em class="chapterRef">Chapter 6</em>, <em class="italic">Predicting Stock Prices with Artificial Neural Networks</em>, and train the <code class="inlineCode">PyTorch</code> model against the data for 10 iterations:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">train_step</span><span class="language-python">(</span><span class="hljs-con-params">model, X_train, y_train, loss_function, optimizer</span><span class="language-python">):</span>
        pred_train = model(X_train)
        loss = loss_function(pred_train, y_train)
        model.zero_grad()
        loss.backward()
        optimizer.step()
    return loss.item()
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> epoch </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(n_iter):</span>
        loss = train_step(model, X_torch, y_torch, loss_function, optimizer)
        <span class="hljs-con-built_in">print</span>(<span class="hljs-con-string">f"Epoch {epoch} - loss: {loss}"</span>)
Epoch 0 - loss: 0.8387020826339722
Epoch 1 - loss: 0.7999904751777649
Epoch 2 - loss: 0.76298588514328
Epoch 3 - loss: 0.7277476787567139
Epoch 4 - loss: 0.6943162679672241
Epoch 5 - loss: 0.6627081036567688
Epoch 6 - loss: 0.6329135298728943
Epoch 7 - loss: 0.6048969030380249
Epoch 8 - loss: 0.5786024332046509
Epoch 9 - loss: 0.5539639592170715
</code></pre>
      </li>
      <li class="numberedList">Display the model’s architecture:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(model)</span>
Sequential(
  (0): Linear(in_features=30, out_features=1, bias=True)
  (1): Sigmoid()
)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We will see if we can retrieve the same model later.</p>
    <ol>
      <li class="numberedList" value="5">Hopefully, the<a id="_idIndexMarker1094"/> previous steps look familiar to<a id="_idIndexMarker1095"/> you. If not, feel free to review our PyTorch implementation. Now we save the model to a path:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">path = </span><span class="hljs-con-string">'./model.pth '</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.save(model, path)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">After this, you will see that a folder called <code class="inlineCode">model.pth</code> is created. The folder contains the entire trained model’s architecture, weights, and training configuration.</p>
    <ol>
      <li class="numberedList" value="6">Finally, we<a id="_idIndexMarker1096"/> load the model from the previous path and display the loaded model’s path:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">new_model = torch.load(path)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(new_model)</span>
Sequential(
  (0): Linear(in_features=30, out_features=1, bias=True)
  (1): Sigmoid()
)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We just <a id="_idIndexMarker1097"/>loaded back the exact same model.</p>
    <h2 class="heading-2" id="_idParaDest-260">Best practice 20 – Monitoring model performance</h2>
    <p class="normal">The machine learning<a id="_idIndexMarker1098"/> system is now up and running. To make sure everything is on the right track, we need to conduct performance checks on a regular basis. To do so, besides making a prediction in real time, we should also record the ground truth at the same time. Here are some best practices for monitoring model performance:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Define evaluation metrics</strong>: Choose appropriate evaluation metrics that align with your problem’s goals. Accuracy, precision, recall, F1-score, AUC-ROC, <em class="italic">R</em><sup class="superscript-italic" style="font-style: italic;">2</sup>, and mean squared error are some common metrics.</li>
      <li class="bulletList"><strong class="keyWord">Baseline performance</strong>: Establish a baseline model or a simple rule-based approach to compare your model’s performance. This provides context for understanding whether your model is adding value.</li>
      <li class="bulletList"><strong class="keyWord">Learning curves</strong>: Plot learning curves showing training and validation loss or evaluation metrics over epochs. This helps identify overfitting or underfitting issues, as mentioned in <em class="italic">Best practice 17 – Diagnosing overfitting and underfitting</em>.</li>
    </ul>
    <p class="normal">Continuing with the diabetes example from earlier in the chapter, we conduct a performance check as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.metrics </span><span class="hljs-con-keyword">import</span><span class="language-python"> r2_score</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Health check on the model, R^2: </span><span class="hljs-con-subst">{r2_score(y_new,</span>
          predictions):.3f}')
Health check on the model, R^2: 0.613
</code></pre>
    <p class="normal">We should log the performance and set up an alert for any decayed performance.</p>
    <h2 class="heading-2" id="_idParaDest-261">Best practice 21 – Updating models regularly</h2>
    <p class="normal">If the performance is<a id="_idIndexMarker1099"/> getting worse, chances are that the pattern of data has changed. We can work around this by updating the model. Depending on whether online learning is feasible or not with the model, the model can be modernized with the new set of data (online updating) or retrained completely with the most recent data. Here are some best practices for the last <a id="_idIndexMarker1100"/>section of the chapter:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Monitor model performance</strong>: Continuously monitor model performance metrics. If there’s a significant drop, it’s a sign that the model needs updating.</li>
      <li class="bulletList"><strong class="keyWord">Scheduled updates</strong>: Implement a schedule for model updates based on the frequency of data changes and business needs. This ensures that the model remains relevant, without unnecessary updates.</li>
      <li class="bulletList"><strong class="keyWord">Online updating</strong>: For models that support online learning, update the model incrementally with new data. This applies to models based on gradient descent algorithms, or Naïve Bayes. Online updating minimizes the need for retraining the entire model and adapts it to changing patterns over time.</li>
      <li class="bulletList"><strong class="keyWord">Version control</strong>: Maintain version control of models and datasets to track changes and facilitate rollback if necessary. This helps in comparing model performance over time and reverting to previous versions if updates lead to performance degradation.</li>
      <li class="bulletList"><strong class="keyWord">Regular auditing</strong>: Periodically review model performance, reevaluate business goals, and update your evaluation metrics if needed.</li>
    </ul>
    <p class="normal">Remember that <a id="_idIndexMarker1101"/>monitoring should be an ongoing process, starting from model development through deployment and maintenance. It ensures that your machine learning models remain effective, trustworthy, and aligned with your business objectives.</p>
    <h1 class="heading-1" id="_idParaDest-262">Summary</h1>
    <p class="normal">The purpose of this chapter is to prepare you for real-world machine learning problems. We started with the general workflow that a machine learning solution follows: data preparation, training set generation, algorithm training, evaluation and selection, and finally, system deployment and monitoring. We then went, in depth, through the typical tasks, common challenges, and best practices for each of these four stages.</p>
    <p class="normal">Practice makes perfect. The most important best practice is practice itself. Get started with a real-world project to deepen your understanding and apply what you have learned so far.</p>
    <p class="normal">In the next chapter, we will start our deep learning journey by categorizing clothing images using convolutional neural networks.</p>
    <h1 class="heading-1" id="_idParaDest-263">Exercises</h1>
    <ol>
      <li class="numberedList" value="1">Can you use word embedding to extract text features and develop a multiclass classifier to classify the newsgroup data? (Note that you might not be able to get better results with word embedding than tf-idf, but it is good practice.)</li>
      <li class="numberedList">Can you find several challenges in Kaggle (<a href="https://www.kaggle.com"><span class="url">www.kaggle.com</span></a>) and practice what you have learned throughout the entire book?</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-264">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/yuxi"><span class="url">https://packt.link/yuxi</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code1878468721786989681.png"/></p>
  </div>
</body></html>
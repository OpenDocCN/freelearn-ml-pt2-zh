["```py\ndef load():\n import numpy as np\n from scipy import sparse\n\n data = np.loadtxt('data/ml-100k/u.data')\n ij = data[:, :2]\n ij -= 1  # original data is in 1-based system\n values = data[:, 2]\n reviews = sparse.csc_matrix((values, ij.T)).astype(float)\n return reviews.toarray()\n\n```", "```py\n>>> reviews = load()\n>>> U,M = np.where(reviews)\n\n```", "```py\n>>> import random\n>>> test_idxs = np.array(random.sample(range(len(U)), len(U)//10))\n\n```", "```py\n>>> train = reviews.copy()\n>>> train[U[test_idxs], M[test_idxs]] = 0\n\n```", "```py\n>>> test = np.zeros_like(reviews)\n>>> test[U[test_idxs], M[test_idxs]] = reviews[U[test_idxs], M[test_idxs]]\n\n```", "```py\nclass NormalizePositive(object):\n\n```", "```py\n def __init__(self, axis=0):\n self.axis = axis\n\n```", "```py\n def fit(self, features, y=None):\n\n```", "```py\n if self.axis == 1:\n features = features.T\n #  count features that are greater than zero in axis 0:\n binary = (features > 0)\n count0 = binary.sum(axis=0)\n\n # to avoid division by zero, set zero counts to one:\n count0[count0 == 0] = 1.\n\n # computing the mean is easy:\n self.mean = features.sum(axis=0)/count0\n\n # only consider differences where binary is True:\n diff = (features - self.mean) * binary\n diff **= 2\n # regularize the estimate of std by adding 0.1\n self.std = np.sqrt(0.1 + diff.sum(axis=0)/count0)\n return self\n\n```", "```py\n def transform(self, features):\n if self.axis == 1:\n features = features.T\n binary = (features > 0)\n features = features - self.mean\n features /= self.std\n features *= binary\n if self.axis == 1:\n features = features.T\n return features\n\n```", "```py\n def inverse_transform(self, features, copy=True):\n if copy:\n features = features.copy()\n if self.axis == 1:\n features = features.T\n features *= self.std\n features += self.mean\n if self.axis == 1:\n features = features.T\n return features\n\n```", "```py\n def fit_transform(self, features):\n return self.fit(features).transform(features)\n\n```", "```py\n>>> from matplotlib import pyplot as plt\n>>> # Build an instance of the object we defined above\n>>> norm = NormalizePositive(axis=1)\n>>> binary = (train > 0)\n>>> train = norm.fit_transform(train)\n>>> # plot just 200x200 area for space reasons\n>>> plt.imshow(binary[:200, :200], interpolation='nearest')\n\n```", "```py\n>>> from scipy.spatial import distance\n>>> # compute all pair-wise distances:\n>>> dists = distance.pdist(binary, 'correlation')\n>>> # Convert to square form, so that dists[i,j]\n>>> # is distance between binary[i] and binary[j]:\n>>> dists = distance.squareform(dists)\n\n```", "```py\n>>> neighbors = dists.argsort(axis=1)\n\n```", "```py\n>>> # We are going to fill this matrix with results\n>>> filled = train.copy()\n>>> for u in range(filled.shape[0]):\n...     # n_u is neighbors of user\n...     n_u = neighbors[u, 1:]\n...     # t_u is training data\n\n...     for m in range(filled.shape[1]):\n...         # get relevant reviews in order!\n...         revs = [train[neigh, m]\n...                    for neigh in n_u\n...                         if binary  [neigh, m]]\n...         if len(revs):\n...             # n is the number of reviews for this movie\n...             n = len(revs)\n...             # consider half of the reviews plus one\n...             n //= 2\n...             n += 1\n...             revs = revs[:n]\n...             filled[u,m] = np.mean(revs )\n\n```", "```py\n>>> predicted = norm.inverse_transform(filled)\n\n```", "```py\n>>> from sklearn import metrics\n>>> r2 = metrics.r2_score(test[test > 0], predicted[test > 0])\n>>> print('R2 score (binary neighbors): {:.1%}'.format(r2))\nR2 score (binary neighbors): 29.5%\n\n```", "```py\n>>> reviews = reviews.T\n>>> # use same code as before â€¦\n>>> r2 = metrics.r2_score(test[test > 0], predicted[test > 0])\n>>> print('R2 score (binary movie neighbors): {:.1%}'.format(r2))\nR2 score (binary movie neighbors): 29.8%\n\n```", "```py\n>>> reg = ElasticNetCV(alphas=[\n 0.0125, 0.025, 0.05, .125, .25, .5, 1., 2., 4.])\n\n```", "```py\n>>> filled = train.copy()\n\n```", "```py\n>>> for u in range(train.shape[0]):\n...     curtrain = np.delete(train, u, axis=0)\n...     # binary records whether this rating is present\n...     bu = binary[u]\n...     # fit the current user based on everybody else\n...     reg.fit(curtrain[:,bu].T, train[u, bu])\n...     # Fill in all the missing ratings\n...     filled[u, ~bu] = reg.predict(curtrain[:,~bu].T)\n\n```", "```py\n>>> predicted = norm.inverse_transform(filled)\n>>> r2 = metrics.r2_score(test[test > 0], predicted[test > 0])\n>>> print('R2 score (user regression): {:.1%}'.format(r2))\nR2 score (user regression): 32.3%\n\n```", "```py\n>>> train,test = load_ml100k.get_train_test(random_state=12)\n>>> # Now split the training again into two subgroups\n>>> tr_train,tr_test = load_ml100k.get_train_test(train, random_state=34)\n>>> # Call all the methods we previously defined:\n>>> # these have been implemented as functions:\n>>> tr_predicted0 = regression.predict(tr_train)\n>>> tr_predicted1 = regression.predict(tr_train.T).T\n>>> tr_predicted2 = corrneighbours.predict(tr_train)\n>>> tr_predicted3 = corrneighbours.predict(tr_train.T).T\n>>> tr_predicted4 = norm.predict(tr_train)\n>>> tr_predicted5 = norm.predict(tr_train.T).T\n\n>>> # Now assemble these predictions into a single array:\n>>> stack_tr = np.array([\n...     tr_predicted0[tr_test > 0],\n...     tr_predicted1[tr_test > 0],\n...     tr_predicted2[tr_test > 0],\n...     tr_predicted3[tr_test > 0],\n...     tr_predicted4[tr_test > 0],\n...     tr_predicted5[tr_test > 0],\n...     ]).T\n\n>>> # Fit a simple linear regression\n>>> lr = linear_model.LinearRegression()\n>>> lr.fit(stack_tr, tr_test[tr_test > 0])\n\n```", "```py\n>>> stack_te = np.array([\n...     tr_predicted0.ravel(),\n...     tr_predicted1.ravel(),\n...     tr_predicted2.ravel(),\n...     tr_predicted3.ravel(),\n...     tr_predicted4.ravel(),\n...     tr_predicted5.ravel(),\n...     ]).T\n>>> predicted = lr.predict(stack_te).reshape(train.shape)\n\n```", "```py\n>>> r2 = metrics.r2_score(test[test > 0], predicted[test > 0])\n>>> print('R2 stacked: {:.2%}'.format(r2))\nR2 stacked: 33.15%\n\n```", "```py\n>>> from collections import defaultdict\n>>> from itertools import chain\n\n>>> # File is downloaded as a compressed file\n>>> import gzip\n>>> # file format is a line per transaction\n>>> # of the form '12 34 342 5...'\n>>> dataset = [[int(tok) for tok in line.strip().split()]\n...         for line in gzip.open('retail.dat.gz')]\n>>> # It is more convenient to work with sets\n>>> dataset = [set(d) for d in dataset]\n>>> # count how often each product was purchased:\n>>> counts = defaultdict(int)\n>>> for elem in chain(*dataset):\n...     counts[elem] += 1\n\n```", "```py\n>>> minsupport = 80\n\n```", "```py\n>>> valid = set(k for k,v in counts.items()\n...           if (v >= minsupport))\n\n```", "```py\n>>>  itemsets = [frozenset([v]) for v in valid]\n\n```", "```py\n>>> freqsets = []\n>>> for i in range(16):\n...     nextsets = []\n...     tested = set()\n...     for it in itemsets:\n...         for v in valid:\n...             if v not in it:\n...                 # Create a new candidate set by adding v to it\n...                 c = (it | frozenset([v]))\n...                 # check If we have tested it already\n...                 if c in tested:\n...                     continue\n...                 tested.add(c)\n...\n...                 # Count support by looping over dataset\n...                 # This step is slow.\n...                 # Check `apriori.py` for a better implementation.\n...                 support_c = sum(1 for d in dataset if d.issuperset(c))\n...                 if support_c > minsupport:\n...                     nextsets.append(c)\n...     freqsets.extend(nextsets)\n...     itemsets = nextsets\n...     if not len(itemsets):\n...         break\n>>> print(\"Finished!\")\nFinished!\n\n```", "```py\n>>> minlift = 5.0\n>>> nr_transactions = float(len(dataset))\n>>> for itemset in freqsets:\n...       for item in itemset:\n...         consequent = frozenset([item])\n...         antecedent = itemset-consequent\n...         base = 0.0\n...         # acount: antecedent count\n...         acount = 0.0\n... \n...         # ccount : consequent count\n...         ccount = 0.0\n...         for d in dataset:\n...           if item in d: base += 1\n...           if d.issuperset(itemset): ccount += 1\n...           if d.issuperset(antecedent): acount += 1\n...         base /= nr_transactions\n...         p_y_given_x = ccount/acount\n...         lift = p_y_given_x / base\n...         if lift > minlift:\n...             print('Rule {0} ->  {1} has lift {2}'\n...                   .format(antecedent, consequent,lift))\n\n```"]
["```py\n>>> import numpy as np\n>>> X = [[2, 0, -1.4],\n>>>     [2.2, 0.2, -1.5],\n>>>     [2.4, 0.1, -1],\n>>>     [1.9, 0, -1.2]]\n>>> print np.cov(np.array(X).T)\n[[ 0.04916667  0.01416667  0.01916667]\n [ 0.01416667  0.00916667 -0.00583333]\n [ 0.01916667 -0.00583333  0.04916667]]\n```", "```py\n>>> import numpy as np\n>>> w, v = np.linalg.eig(np.array([[1, -2], [2, -3]]))\n>>> w; v\narray([-0.99999998, -1.00000002])\narray([[ 0.70710678,  0.70710678],\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.datasets import load_iris\n```", "```py\n>>> data = load_iris()\n>>> y = data.target\n>>> X = data.data\n>>> pca = PCA(n_components=2)\n>>> reduced_X = pca.fit_transform(X)\n```", "```py\n>>> red_x, red_y = [], []\n>>> blue_x, blue_y = [], []\n>>> green_x, green_y = [], []\n>>> for i in range(len(reduced_X)):\n>>>     if y[i] == 0:\n>>>         red_x.append(reduced_X[i][0])\n>>>         red_y.append(reduced_X[i][1])\n>>>     elif y[i] == 1:\n>>>         blue_x.append(reduced_X[i][0])\n>>>         blue_y.append(reduced_X[i][1])\n>>>     else:\n>>>         green_x.append(reduced_X[i][0])\n>>>         green_y.append(reduced_X[i][1])\n>>> plt.scatter(red_x, red_y, c='r', marker='x')\n>>> plt.scatter(blue_x, blue_y, c='b', marker='D')\n>>> plt.scatter(green_x, green_y, c='g', marker='.')\n>>> plt.show()\n```", "```py\n>>> from os import walk, path\n>>> import numpy as np\n>>> import mahotas as mh\n>>> from sklearn.cross_validation import train_test_split\n>>> from sklearn.cross_validation import cross_val_score\n>>> from sklearn.preprocessing import scale\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.metrics import classification_report\n>>> X = []\n>>> y = []\n```", "```py\n>>> for dir_path, dir_names, file_names in walk('data/att-faces/orl_faces'):\n>>>     for fn in file_names:\n>>>         if fn[-3:] == 'pgm':\n>>>             image_filename = path.join(dir_path, fn)\n>>>             X.append(scale(mh.imread(image_filename, as_grey=True).reshape(10304).astype('float32')))\n>>>             y.append(dir_path)\n>>> X = np.array(X)\n```", "```py\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y)\n>>> pca = PCA(n_components=150)\n```", "```py\n>>> X_train_reduced = pca.fit_transform(X_train)\n>>> X_test_reduced = pca.transform(X_test)\n>>> print 'The original dimensions of the training data were', X_train.shape\n>>> print 'The reduced dimensions of the training data are', X_train_reduced.shape\n>>> classifier = LogisticRegression()\n>>> accuracies = cross_val_score(classifier, X_train_reduced, y_train)\n```", "```py\n>>> print 'Cross validation accuracy:', np.mean(accuracies), accuracies\n>>> classifier.fit(X_train_reduced, y_train)\n>>> predictions = classifier.predict(X_test_reduced)\n>>> print classification_report(y_test, predictions)\n```", "```py\nThe original dimensions of the training data were (300, 10304)\nThe reduced dimensions of the training data are (300, 150)\nCross validation accuracy: 0.833841819347 [ 0.82882883  0.83        0.84269663]\n             precision    recall  f1-score   support\n\ndata/att-faces/orl_faces/s1       1.00      1.00      1.00         2\ndata/att-faces/orl_faces/s10       1.00      1.00      1.00         2\ndata/att-faces/orl_faces/s11       1.00      0.60      0.75         5\n...\ndata/att-faces/orl_faces/s9       1.00      1.00      1.00         2\n\navg / total       0.92      0.89      0.89       100\n```"]
["```py\nfrom sklearn.linear_model import LinearRegression\n\n# Initialize and train the model\nreg = LinearRegression()\nreg.fit(df_taxi[['Kilometres']], df_taxi['Paid (incl. tips)'])\n\n# Make predictions\ndf_taxi['Paid (Predicted)'] = reg.predict(df_taxi[['Kilometres']])\n```", "```py\nfig, axs = plt.subplots(1, 2, figsize=(16, 5))\n\ndf_taxi.set_index('Kilometres')['Meter'].plot(\n   title='Meter', kind='line', ax=axs[0]\n)\n\ndf_taxi.set_index('Kilometres')['Paid (incl. tips)'].plot(\ntitle='Paid (incl. tips)', label='actual', kind='line',  ax=axs[1]\n)\ndf_taxi.set_index('Kilometres')['Paid (Predicted)'].plot(\n    title='Paid (incl. tips)', label='estimated', kind='line', ax=axs[1]\n)\n\nfig.show()\n```", "```py\nprint(\n    'Amount Paid = {:.1f} + {:.1f} * Distance'.format(\n        reg.intercept_, reg.coef_[0], \n    )\n) \n```", "```py\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\ndf_dataset = pd.DataFrame(\n    boston.data,\n    columns=boston.feature_names,\n)\ndf_dataset['target'] = boston.target\n```", "```py\ndf_dataset.isnull().sum().sum() # Luckily, the result is zero\n```", "```py\nfig, axs = plt.subplots(1, 2, figsize=(16, 8))\n\ndf_dataset['target'].plot(\n    title='Distribution of target prices', kind='hist', ax=axs[0]\n)\ndf_dataset[boston.feature_names].mean().plot(\n    title='Mean of features', kind='bar', ax=axs[1]\n)\n\nfig.show()\n```", "```py\ndf_dataset['target'].describe(percentiles=[.9, .95, .99])\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df_dataset, test_size=0.4)\n\nx_train = df_train[boston.feature_names]\nx_test = df_test[boston.feature_names]\ny_train = df_train['target']\ny_test = df_test['target']\n```", "```py\nfrom sklearn.dummy import DummyRegressor\n\nbaselin = DummyRegressor(strategy='mean')\nbaselin.fit(x_train, y_train)\n\ny_test_baselin = baselin.predict(x_test)\n\n```", "```py\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\nreg.fit(x_train, y_train)\n\ny_test_pred = reg.predict(x_test)\n```", "```py\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\nprint(\n    'R2 Regressor = {:.2f} vs Baseline = {:.2f}'.format(\n        r2_score(y_test, y_test_pred), \n        r2_score(y_test, y_test_baselin)\n     )\n)\nprint(\n    'MAE Regressor = {:.2f} vs Baseline = {:.2f}'.format(\n        mean_absolute_error(y_test, y_test_pred), \n        mean_absolute_error(y_test, y_test_baselin)\n    )\n)\nprint(\n    'MSE Regressor = {:.2f} vs Baseline = {:.2f}'.format(\n        mean_squared_error(y_test, y_test_pred), \n        mean_squared_error(y_test, y_test_baselin)\n    )\n)\n```", "```py\nR2 Regressor = 0.74 vs Baseline = -0.00\nMAE Regressor = 3.19 vs Baseline = 6.29\nMSE Regressor = 19.70 vs Baseline = 76.11\n```", "```py\ndf_feature_importance = pd.DataFrame(\n    {\n        'Features': x_train.columns,\n        'Coeff': reg.coef_,\n        'ABS(Coeff)': abs(reg.coef_),\n    }\n).set_index('Features').sort_values('Coeff', ascending=False)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nreg = LinearRegression()\n\nscaler.fit(x_train)\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\nreg.fit(x_train_scaled, y_train)\ny_test_pred = reg.predict(x_test_scaled)\n```", "```py\n# scaler.fit(x_train)\n# x_train_scaled = scaler.transform(x_train)\nx_train_scaled = scaler.fit_transform(x_train)\n```", "```py\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=3)\nx_train_poly = poly.fit_transform(x_train)\nx_test_poly = poly.transform(x_test)\n```", "```py\nfeature_translator = [\n    (f'x{i}', feature) for i, feature in enumerate(x_train.columns, 0)\n]\n\ndef translate_feature_names(s):\n    for key, val in feature_translator:\n        s = s.replace(key, val)\n    return s\n\npoly_features = [\n    translate_feature_names(f) for f in poly.get_feature_names()\n]\n\nx_train_poly = pd.DataFrame(x_train_poly, columns=poly_features)\nx_test_poly = pd.DataFrame(x_test_poly, columns=poly_features)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression(fit_intercept=False)\nreg.fit(x_train_poly, y_train)\n\ny_test_pred = reg.predict(x_test_poly)\n```", "```py\nR2 Regressor = -84.887 vs Baseline = -0.0\nMAE Regressor = 37.529 vs Baseline = 6.2\nMSE Regressor = 6536.975 vs Baseline = 78.1\n```", "```py\nfrom sklearn.linear_model import Ridge, Lasso\n\nreg = Lasso(fit_intercept=False)\nreg.fit(x_train_poly, y_train)\n\ny_test_pred = reg.predict(x_test_poly)\n```", "```py\nR2 Regressor = 0.787 vs Baseline = -0.0\nMAE Regressor = 2.381 vs Baseline = 6.2\nMSE Regressor = 16.227 vs Baseline = 78.\n```", "```py\nfrom sklearn.linear_model import LassoCV\n\n# Make a list of 50 values between 0.000001 & 1,000,000\nalphas = np.logspace(-6, 6, 50)\n\n# We will do 5-fold cross validation\nreg = LassoCV(alphas=alphas, fit_intercept=False, cv=5)\nreg.fit(x_train_poly, y_train)\n\ny_train_pred = reg.predict(x_train_poly)\ny_test_pred = reg.predict(x_test_poly)\n```", "```py\nprint(f\"LassoCV: Chosen alpha = {reg.alpha_}\")\n```", "```py\n# n_folds equals to 5 here\nn_folds = reg.mse_path_.shape[1]\n\n# Calculate the mean and standard error for MSEs\nmse_mean = reg.mse_path_.mean(axis=1)\nmse_std = reg.mse_path_.std(axis=1)\n# Std Error = Std Deviation / SQRT(number of samples)\nmse_std_error = mse_std / np.sqrt(n_folds)\n```", "```py\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n# We multiply by 1.96 for a 95% Confidence Interval\npd.DataFrame(\n    {\n        'alpha': reg.alphas_,\n        'Mean MSE': mse_mean,\n        'Upper Bound MSE': mse_mean + 1.96 * mse_std_error,\n        'Lower Bound MSE': mse_mean - 1.96 * mse_std_error,\n    }\n).set_index('alpha')[\n    ['Mean MSE', 'Upper Bound MSE', 'Lower Bound MSE']\n].plot(\n    title='Regularization plot (MSE vs alpha)', \n    marker='.', logx=True, ax=ax\n)\n\n# Color the confidence interval \nplt.fill_between(\n    reg.alphas_, \n    mse_mean + 1.96 * mse_std_error, \n    mse_mean - 1.96 * mse_std_error, \n)\n\n# Print a vertical line for the chosen alpha\nax.axvline(reg.alpha_, linestyle='--', color='k')\nax.set_xlabel('Alpha')\nax.set_ylabel('Mean Squared Error')\n```", "```py\nimport numpy as np\nimport pandas as pd\n\ndf_noisy = pd.DataFrame(\n    {\n        'x': np.random.random_integers(0, 30, size=150),\n        'noise': np.random.normal(loc=0.0, scale=5.0, size=150)\n    }\n)\n\ndf_noisy['y'] = df_noisy['x'] + df_noisy['noise']\n```", "```py\ndf_noisy.plot(\n    kind='scatter', x='x', y='y'\n)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import BayesianRidge\n\nlr = LinearRegression()\nbr = BayesianRidge()\n\nlr.fit(df_noisy[['x']], df_noisy['y'])\ndf_noisy['y_lr_pred'] = lr.predict(df_noisy[['x']])\n\nbr.fit(df_noisy[['x']], df_noisy['y'])\ndf_noisy['y_br_pred'], df_noisy['y_br_std'] = br.predict(df_noisy[['x']], return_std=True)\n```", "```py\nfig, axs = plt.subplots(1, 3, figsize=(16, 6), sharex=True, sharey=True)\n\n# We plot the data 3 times\ndf_noisy.sort_values('x').plot(\n    title='Data', kind='scatter', x='x', y='y', ax=axs[0]\n)\ndf_noisy.sort_values('x').plot(\n    kind='scatter', x='x', y='y', ax=axs[1], marker='o', alpha=0.25\n)\ndf_noisy.sort_values('x').plot(\n    kind='scatter', x='x', y='y', ax=axs[2], marker='o', alpha=0.25\n)\n\n# Here we plot the Linear Regression predictions\ndf_noisy.sort_values('x').plot(\n    title='LinearRegression', kind='scatter', x='x', y='y_lr_pred', \n    ax=axs[1], marker='o', color='k', label='Predictions'\n)\n\n# Here we plot the Bayesian Ridge predictions\ndf_noisy.sort_values('x').plot(\n    title='BayesianRidge', kind='scatter', x='x', y='y_br_pred', \n    ax=axs[2], marker='o', color='k', label='Predictions'\n)\n\n# Here we plot the range around the expected values\n# We multiply by 1.96 for a 95% Confidence Interval\naxs[2].fill_between(\n    df_noisy.sort_values('x')['x'], \n    df_noisy.sort_values('x')['y_br_pred'] - 1.96 * \n                df_noisy.sort_values('x')['y_br_std'], \n    df_noisy.sort_values('x')['y_br_pred'] + 1.96 * \n                df_noisy.sort_values('x')['y_br_std'],\n    color=\"k\", alpha=0.2, label=\"Predictions +/- 1.96 * Std Dev\"\n)\n\nfig.show()\n```", "```py\nimport numpy as np\nimport pandas as pd\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\ntheta = np.arange(-10, 10, 0.05)\ny = 1 / (1 + np.exp(-1 * theta))\n\npd.DataFrame(\n    {\n        'theta': theta,\n        'y': y\n    }\n).plot(\n    title='Logistic Function', \n    kind='scatter', x='theta', y='y', \n    ax=ax\n)\n\nfig.show()\n```", "```py\nfrom sklearn import datasets\niris = datasets.load_iris()\n\ndf = pd.DataFrame(\n    iris.data,\n    columns=iris.feature_names\n)\n\ndf['target'] = pd.Series(\n    iris.target\n)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\n\nnum_folds = 6\n\nclf = LogisticRegression(\n    solver='lbfgs', multi_class='multinomial', max_iter=1000\n)\naccuracy_scores = cross_validate(\n    clf, df[iris.feature_names], df['target'], \n    cv=num_folds, scoring=['accuracy']\n)\n\naccuracy_mean = pd.Series(accuracy_scores['test_accuracy']).mean()\naccuracy_std = pd.Series(accuracy_scores['test_accuracy']).std()\naccuracy_sterror = accuracy_std / np.sqrt(num_folds)\n\nprint(\n     'Logistic Regression: Accuracy ({}-fold): {:.2f} ~ {:.2f}'.format(\n         num_folds,\n         (accuracy_mean - 1.96 * accuracy_sterror),\n         (accuracy_mean + 1.96 * accuracy_sterror),\n    )\n)\n```", "```py\n# We need to fit the model again before getting its coefficients\nclf.fit(df[iris.feature_names], df['target'])\n\n# We use dictionary comprehension instead of a for-loop\ndf_coef = pd.DataFrame(\n    {\n        f'Coef [Class {class_id}]': clf.coef_[class_id]\n        for class_id in range(clf.coef_.shape[0])\n    },\n    index=iris.feature_names\n)\ndf_coef.loc['intercept', :] = clf.intercept_\n```", "```py\ndef plot_decision_boundary(clf, x, y, ax, title):\n\n   cmap='Paired_r' \n   feature_names = x.columns\n   x, y = x.values, y.values\n\n   x_min, x_max = x[:,0].min(), x[:,0].max()\n   y_min, y_max = x[:,1].min(), x[:,1].max()\n\n   step = 0.02\n\n   xx, yy = np.meshgrid(\n      np.arange(x_min, x_max, step),\n      np.arange(y_min, y_max, step)\n   )\n   Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n   Z = Z.reshape(xx.shape)\n\n   ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)\n   ax.contour(xx, yy, Z, colors='k', linewidths=0.7)\n   ax.scatter(x[:,0], x[:,1], c=y, edgecolors='k')\n   ax.set_title(title)\n   ax.set_xlabel(feature_names[0])\n   ax.set_ylabel(feature_names[1])\n```", "```py\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.3, random_state=22)\n```", "```py\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\ntwo_features = ['petal width (cm)', 'petal length (cm)']\n\nclf_lr = LogisticRegression()\nclf_lr.fit(df_train[two_features], df_train['target'])\naccuracy = accuracy_score(\n    df_test['target'], \n    clf_lr.predict(df_test[two_features])\n)\nplot_decision_boundary(\n    clf_lr, df_test[two_features], df_test['target'], ax=axs[0], \n    title=f'Logistic Regression Classifier\\nAccuracy: {accuracy:.2%}'\n)\n\nclf_dt = DecisionTreeClassifier(max_depth=3)\nclf_dt.fit(df_train[two_features], df_train['target'])\naccuracy = accuracy_score(\n    df_test['target'], \n    clf_dt.predict(df_test[two_features])\n)\nplot_decision_boundary(\n    clf_dt, df_test[two_features], df_test['target'], ax=axs[1], \n    title=f'Decision Tree Classifier\\nAccuracy: {accuracy:.2%}'\n)\n\nfig.show()\n```"]
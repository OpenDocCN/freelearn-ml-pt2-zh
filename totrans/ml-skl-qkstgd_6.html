<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Classification and Regression with Trees</h1>
                </header>
            
            <article>
                
<p class="mce-root">Tree based algorithms are very popular for two reasons: they are interpretable, and they make sound predictions that have won many machine learning competitions on online platforms, such as Kaggle. Furthermore, they have many use cases outside of machine learning for solving problems, both simple and complex.</p>
<p>Building a tree is an approach to decision-making used in almost all industries. Trees can be used to solve both classification- and regression-based problems, and have several use cases that make them the go-to solution!</p>
<p>This chapter is broadly divided into the following<span> two </span>sections:</p>
<ul>
<li>Classification trees</li>
<li>Regression trees</li>
</ul>
<p>Each section will cover the fundamental theory of different types of tree based algorithms, along with their implementation in scikit-learn. By the end of this chapter, you will have learned how to aggregate several algorithms into an <strong>ensemble</strong> and have them vote on what the best prediction is.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You will be required to have <span class="fontstyle0">Python 3.6 or greater, </span><span class="fontstyle0">Pandas ≥ 0.23.4,</span><span class="fontstyle2"> </span><span class="fontstyle0">Scikit-learn ≥ 0.20.0, and </span><span class="fontstyle0">Matplotlib ≥ 3.0.0 </span></span><span class="fontstyle0">installed on your system.</span></p>
<p class="mce-root">The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb">https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb</a><a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb">.</a></p>
<p class="mce-root"/>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2SrPP7R">http://bit.ly/2SrPP7R</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification trees</h1>
                </header>
            
            <article>
                
<p>Classification trees are used to predict a category or class. This is similar to the classification algorithms that you have learned about <span>previously</span> <span>in this book, such as the k-nearest neighbors algorithm or logistic regression.</span></p>
<p>Broadly speaking, there are three tree based algorithms that are used to solve classification problems:</p>
<ul>
<li>The decision tree classifier</li>
<li>The random forest classifier</li>
<li>The AdaBoost classifier</li>
</ul>
<p>In this section, you will learn how each of these tree based algorithms works, in order to classify a row of data as a particular class or category.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The decision tree classifier</h1>
                </header>
            
            <article>
                
<p>The decision tree is the simplest tree based algorithm, and serves as the foundation for the other two algorithms. Let's consider the following simple decision tree:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/93f38146-379b-4b23-8502-5318cb631c9f.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A simple decision tree</div>
<p>A decision tree, in simple terms, is a set of rules that help us classify observations into distinct groups. In the previous diagram, the rule could be written as the following:</p>
<pre class="CDPAlignLeft CDPAlign">If (value of feature is less than 50); then (put the triangles in the left-hand box <span>and</span> put the circles in the right-hand box).</pre>
<p>The preceding decision tree perfectly divides the observations into two distinct groups. This is a characteristic of the ideal decision tree. The first box on the top is called the <strong>root</strong> of the tree, and is the most important feature of the tree when it comes to deciding how to group the observations.</p>
<p>The boxes under the root node are known as the <strong>children</strong>. In the <span>preceding</span> tree, the <strong>children</strong> are also the <strong>leaf</strong> nodes. The <strong>leaf</strong> is the last set of boxes, usually in the bottommost part of the tree. As you might have guessed, the decision tree represents a regular tree, but inverted, or upside down.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Picking the best feature</h1>
                </header>
            
            <article>
                
<p>How does the decision tree decide which feature is the best? The best feature is one that offers the best possible split, and divides the tree into two or more distinct groups, depending on the number of classes or categories that we have in the data. Let's have a look at the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/019a959a-1aad-4ef8-841b-bfc4290a4c20.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A decision tree showing a good split</div>
<p>In the preceding diagram, the following happens:</p>
<ol>
<li>The tree splits the data from the root node into two distinct groups.</li>
<li>In the left-hand group, we see that there are two triangles and one circle.</li>
<li>In the right-hand group, we see that there are two circles and one triangle.</li>
<li>Since the tree got the majority of each class into one group, we can say that the tree has done a good job when it comes to splitting the data into distinct groups.</li>
</ol>
<p>Let's take a look at another example—this time, one in which the split is bad. Consider the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ff259ed3-b1df-482f-aab2-76d4b8185849.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A decision tree with a bad split</div>
<p>In the preceding diagram, the following happens:</p>
<ol>
<li>The tree splits the data in the root node into four distinct groups. This is bad in itself, as it is clear that there are only two categories (circle <span>and</span> triangle).</li>
<li>Furthermore, each group has one triangle and one circle.</li>
<li>There is no majority class or category in any one of the four groups. Each group has 50% of one category; therefore, the tree cannot come to a conclusive decision, unless it relies on more features, which then increases the complexity of the tree.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Gini coefficient</h1>
                </header>
            
            <article>
                
<p>The metric that the decision tree uses to decide if the root node is called the <em>Gini coefficient</em>. The higher the value of this coefficient, the better the job that this particular feature does at splitting the data into distinct groups. In order to learn how to compute the Gini coefficient for a feature, let's consider the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c9879aa0-1163-476e-bbcf-1e026de8d171.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Computing the Gini coefficient</div>
<p>In the preceding diagram, the following happens:</p>
<ol>
<li>The feature splits the data into two groups.</li>
<li>In the left-hand group, we have two triangles and one circle.</li>
<li>Therefore, the Gini for the left-hand group is (2 triangles/3 total data points)^2+ (1 circle/3 total data points)^2.</li>
<li>To calculate this, do the following: <img class="fm-editor-equation" src="assets/c7c76b50-1651-43c4-92b6-066af865a3eb.png" style="width:6.00em;height:1.25em;"/>0.55.</li>
<li>A value of 0.55 for the Gini coefficient indicates that the root of this tree splits the data in such a way that each group has a majority category.</li>
<li>A perfect root feature would have a Gini coefficient of 1. This means that each group has only one class/category.</li>
<li>A bad root feature would have a Gini coefficient of 0.5, which indicates that there is no distinct class/category in a group.</li>
</ol>
<p>In reality, the decision tree is built in a recursive manner, with the tree picking a random attribute for the root and then computing the Gini coefficient for that attribute. It does this until it finds the attribute that best splits the data in a node into groups that have distinct classes and categories.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the decision tree classifier in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how to implement the decision tree classifier in scikit-learn. We will work with the same fraud detection dataset. The first step is to load the dataset into the Jupyter Notebook. We can do this by using the following code:</p>
<pre>import pandas as pd<br/><br/>df = pd.read_csv('fraud_prediction.csv')</pre>
<p>The next step is to split the data into training and test sets. We can do this using the <span>following</span> code:</p>
<pre>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)</pre>
<p>We can now build the initial decision tree classifier on the training data, and test its accuracy on the test data, by using the <span>following</span> code:</p>
<pre>from sklearn.tree import DecisionTreeClassifier<br/><br/>dt = DecisionTreeClassifier(criterion = 'gini', random_state = 50)<br/><br/>#Fitting on the training data<br/><br/>dt.fit(X_train, y_train)<br/><br/>#Testing accuracy on the test data<br/><br/>dt.score(X_test, y_test)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>In the preceding code, we do the following:</p>
<ol>
<li>First, we import <kbd>DecisionTreeClassifier</kbd> from scikit-learn.</li>
<li>We then initialize a <kbd>DecisionTreeClassifier</kbd> object with two arguments. The first, <kbd>criterion</kbd>, is the metric with which the tree picks the most important features in a recursive manner, which, in this case, is the Gini coefficient. The second is <kbd>random_state</kbd>, which is set to 50 so that the model produces the same result every time we run it.</li>
<li>Finally, we fit the model on the training data and evaluate its accuracy on the test data.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning for the decision tree</h1>
                </header>
            
            <article>
                
<p>The decision tree has a plethora of hyperparameters that require fine-tuning in order to derive the best possible model that reduces the generalization error as much as possible. In this section, we will focus on two specific hyperparameters:</p>
<ul>
<li><strong>Max depth</strong>: This is the maximum number of children nodes that can grow out from the decision tree until the tree is cut off. For example, if this is set to 3, then the tree will use three children nodes and cut the tree off before it can grow any more.</li>
<li><strong>Min samples leaf:</strong> This is the minimum number of samples, or data points, that are required to be present in the leaf node. The leaf node is the last node of the tree. If this parameter is, for example, set to a value of 0.04, it tells the tree that it must grow until the last node contains 4% of the total samples in the data.</li>
</ul>
<p>In order to optimize the ideal hyperparameter and to extract the best possible decision tree, we use the <kbd>GridSearchCV</kbd> module from scikit-learn. We can set this up using the <span>following</span> code:</p>
<pre>from sklearn.model_selection import GridSearchCV<br/><br/>#Creating a grid of different hyperparameters<br/><br/>grid_params = {<br/>    'max_depth': [1,2,3,4,5,6],<br/>    'min_samples_leaf': [0.02,0.04, 0.06, 0.08]<br/>}<br/><br/>#Building a 10 fold Cross Validated GridSearchCV object<br/><br/>grid_object = GridSearchCV(estimator = dt, param_grid = grid_params, scoring = 'accuracy', cv = 10, n_jobs = -1)</pre>
<p class="mce-root"/>
<p>In the preceding code, we do the following:</p>
<ol>
<li>We first import the <kbd>GridSearchCV</kbd> module from scikit-learn.</li>
<li>Next, we create a dictionary of possible values for the hyperparameters and store it as <kbd>grid_params</kbd>.</li>
<li>Finally, we create a <kbd>GridSearchCV</kbd> object with the decision tree classifier as the estimator; that is, the dictionary of hyperparameter values.</li>
<li>We set the <kbd>scoring</kbd> argument as <kbd>accuracy</kbd>, since we want to extract the accuracy of the best model found by <kbd>GridSearchCV</kbd>.</li>
</ol>
<p>We then fit this grid object to the training data using the <span>following</span> code:</p>
<pre>#Fitting the grid to the training data<br/><br/>grid_object.fit(X_train, y_train)</pre>
<p>We can then extract the best set of parameters using the <span>following</span> code:</p>
<pre>#Extracting the best parameters<br/><br/>grid_object.best_params_</pre>
<p>The output of the preceding code indicates that a maximum depth of 1 and a minimum number of samples at the leaf node of 0.02 are the best parameters for this data. We can use these optimal parameters and construct a new decision tree using <span>the</span> <span>following</span> <span>code</span>:</p>
<pre>#Extracting the best parameters<br/><br/>grid_object.best_params_</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing the decision tree</h1>
                </header>
            
            <article>
                
<p>One of the best aspects of building and implementing a decision tree in order to solve problems is that it can be interpreted quite easily, using a decision tree diagram that explains how the algorithm that you built works. In order to visualize a simple decision tree for the fraud detection dataset, we use <span>the</span> <span>following</span> <span>code</span>:</p>
<pre>#Package requirements <br/><br/>import pandas as pd<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.externals.six import StringIO <br/>from IPython.display import Image <br/>from sklearn.tree import export_graphviz<br/>import pydotplus<br/>from sklearn import tree</pre>
<p>We start by importing the required packages. The new packages here are the following:</p>
<ul>
<li><kbd>StringIO</kbd></li>
<li><kbd>Image</kbd></li>
<li><kbd>export_graphviz</kbd></li>
<li><kbd>pydotplus</kbd></li>
<li><kbd>tree</kbd></li>
</ul>
<p>The installations of the packages were covered in <a href="d81461f2-02a5-4154-a9b1-7a1f91882534.xhtml" target="_blank">Chapter 1</a>, <em>Introducing Machine Learning with scikit-learn</em>.</p>
<p>Then, we read in the dataset and initialize a decision tree classifier, as shown in <span>the</span> <span>following</span> <span>code</span>:</p>
<pre>#Reading in the data<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>#Initializing the DT classifier<br/><br/>dt = DecisionTreeClassifier(criterion = 'gini', random_state = 50, max_depth= 5)</pre>
<p>Next, we fit the tree on the features and target, and then extract the feature names separately:</p>
<pre>#Fitting the classifier on the data<br/><br/>dt.fit(features, target)<br/><br/>#Extracting the feature names<br/><br/>feature_names = df.drop('isFraud', axis = 1)</pre>
<p>We can then visualize the decision tree using <span>the</span> <span>following</span> <span>code</span>:</p>
<pre>#Creating the tree visualization<br/><br/>data = tree.export_graphviz(dt, out_file=None, feature_names= feature_names.columns.values, proportion= True)<br/><br/>graph = pydotplus.graph_from_dot_data(data) <br/><br/># Show graph<br/>Image(graph.create_png())</pre>
<p>In the preceding code, we do the following:</p>
<ol>
<li>We use the <kbd>tree.export_graphviz()</kbd> function in order to construct the decision tree object, and store it in a variable called <kbd>data</kbd>.</li>
<li>This function uses a couple of arguments: <kbd>dt</kbd> is t<span>he decision tree that you built;</span> <kbd>out_file</kbd> is set to <kbd>None</kbd>, as we do not want to send the tree visualization to any file outside our Jupyter Notebook; the <kbd>feature_names</kbd> are those we defined earlier; and <kbd>proportion</kbd> is, set to <kbd>True</kbd> (this will be explained in more detail later).</li>
<li>We then construct a graph of the data contained within the tree so that we can visualize this decision tree graph by using the <kbd>pydotplus.<br/>
graph_from_dot_data()</kbd> function on the <kbd>data</kbd> variable, which contains data about the decision tree.</li>
<li>Finally, we visualize the decision tree using the <kbd>Image()</kbd> function, by passing the graph of the decision tree to it.</li>
</ol>
<p>This results in a decision tree like that illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/39330691-35d8-4b9e-a633-54f1aa59bf15.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The resultant decision tree</div>
<p>The tree might seem pretty complex to interpret at first, but it's not! In order to interpret this tree, let's consider the root node and the first two children only. This is illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6cd82ac6-bd08-4705-9ffb-c72ef6898597.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A snippet of the decision tree</div>
<p>In the preceding diagram, note the following:</p>
<ul>
<li>In the root node, the tree has identified the 'step' feature as the feature with the highest Gini value.</li>
<li>The root node makes the split in such a way that 0.71, or 71%, of the data falls into the non-fraudulent transactions, while 0.29, or 29%, of the transactions fall into the fraudulent transactions category.</li>
<li>If the step is greater than or equal to 7.5 (the right-hand side), then all of the transactions are classified as fraudulent.</li>
<li>If the step is less than or equal to 7.5 (the left-hand side), then 0.996, or 99.6%, of the transactions are classified as non-fraudulent, while 0.004, or 0.4%, of the transactions are classified as fraudulent.</li>
<li>If the amount is greater than or equal to 4,618,196.0, then all of the transactions are classified as fraudulent.</li>
<li>If the amount is less than or equal to 4,618,196.0, then <span>0.996, or 99.6%, of the transactions are classified as non-fraudulent, while 0.004, or 0.4%, of the transactions are classified as fraudulent.</span></li>
</ul>
<p>Note how the decision tree is simply a set of If-then rules, constructed in a nested manner.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The random forests classifier</h1>
                </header>
            
            <article>
                
<p>Now that you understand the core principles of the decision tree at a very foundational level, we will next explore what random forests are. Random forests are a form of <em>ensemble</em> learning. An ensemble learning method is one that makes use of multiple machine learning models to make a decision.</p>
<p>Let's consider the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/85f521c4-4bb7-43af-afbb-d53dece02a4f.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The concept of ensemble learning</div>
<p class="mce-root"/>
<p>The random forest algorithm operates as follows:</p>
<ol>
<li>Assume that you initially have a dataset with 100 features.</li>
<li>From this, we will build a decision tree with 10 features initially. The features are selected randomly.</li>
<li>Now, using a random selection of the remaining 90 features, we construct the next decision tree, again with 10 features.</li>
<li>This process continues until there are no more features left to build a decision tree with.</li>
<li>At this point in time, we have 10 decision trees, each with 10 features.</li>
<li>Each decision tree is known as the <strong>base estimator</strong> of the random forest.</li>
<li>Thus, we have a forest of trees, each built using a random set of 10 features.</li>
</ol>
<p>The next step for the algorithm is to make the prediction. In order to better understand how the random forest algorithm makes predictions, consider the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/33763702-7826-4c87-a6b2-f1d7a923426d.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The process of making predictions in random forests</div>
<p>In the preceding diagram, the following occurs:</p>
<ol>
<li>Let's assume that there are 10 decision trees in the random forest.</li>
<li>Each decision tree makes a single prediction for the data that comes in.</li>
<li>If six trees predict class A, and four trees predict class B, then the final prediction of the random forest algorithm is class A, as it had the majority vote.</li>
<li>This process of voting on a prediction, based on the outputs of multiple models, is known as ensemble learning.</li>
</ol>
<p>Now that you have learned how the algorithm works internally, we can implement it using scikit-learn!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the random forest classifier in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, we will implement the random forest classifier in scikit-learn. The first step is to read in the data, and split it into training and test sets. This can be done by using the following code:</p>
<pre>import pandas as pd<br/><br/>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the index<br/><br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)</pre>
<p>The next step is to build the random forest classifier. We can do that using the <span>following</span> code:</p>
<pre>from sklearn.ensemble import RandomForestClassifier<br/><br/>#Initiliazing an Random Forest Classifier with default parameters<br/><br/>rf_classifier = RandomForestClassifier(random_state = 50)<br/><br/>#Fitting the classifier on the training data<br/><br/>rf_classifier.fit(X_train, y_train)<br/><br/>#Extracting the scores<br/><br/>rf_classifier.score(X_test, y_test)</pre>
<p>In the preceding code block, we do the following:</p>
<ol>
<li>We first import <kbd>RandomForestClassifier</kbd> from scikit-learn.</li>
<li>Next, we initialize a random forest classifier model.</li>
<li>We then fit this model to our training data, and evaluate its accuracy on the test data.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning for random forest algorithms</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to optimize the hyperparameters of the random forest algorithm. Since random forests are fundamentally based on multiple decision trees, the hyperparameters are very similar. In order to optimize the hyperparameters, we use the <span>following</span> code:</p>
<pre>from sklearn.model_selection import GridSearchCV<br/><br/>#Creating a grid of different hyperparameters<br/><br/>grid_params = {<br/> 'n_estimators': [100,200, 300,400,5000],<br/> 'max_depth': [1,2,4,6,8],<br/> 'min_samples_leaf': [0.05, 0.1, 0.2]<br/>}<br/><br/>#Building a 3 fold Cross-Validated GridSearchCV object<br/><br/>grid_object = GridSearchCV(estimator = rf_classifier, param_grid = grid_params, scoring = 'accuracy', cv = 3, n_jobs = -1)<br/><br/>#Fitting the grid to the training data<br/><br/>grid_object.fit(X_train, y_train)<br/><br/>#Extracting the best parameters<br/><br/>grid_object.best<em>params<br/><br/></em>#Extracting the best model<br/><br/>rf_best = grid_object.best<em>estimator_<br/></em></pre>
<p>In the preceding code block, we do the following:</p>
<ol>
<li>We first import the <kbd>GridSearchCV</kbd> package.</li>
<li>We initialize a dictionary of hyperparameter values. The <kbd>max_depth</kbd> and <kbd>min_samples_leaf</kbd> values are similar to those of the decision tree.</li>
<li>However, <kbd>n_estimators</kbd> is a new parameter, covering the total number of trees that you want your random forest algorithm to consider while making the final prediction.</li>
<li>We then build and fit the <kbd>gridsearch</kbd> object to the training data and extract the optimal parameters.</li>
<li>The best model is then extracted using these optimal hyperparameters.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The AdaBoost classifier</h1>
                </header>
            
            <article>
                
<p>In the section, you will learn how the AdaBoost classifier works internally, and how the concept of boosting might be used to give you better results. Boosting is a form of ensemble machine learning, in which a machine learning model learns from the mistakes of the models that were previously built, thereby increasing its final prediction accuracy.</p>
<p>AdaBoost stands for Adaptive Boosting, and is a boosting algorithm in which a lot of importance is given to the rows of data that the initial predictive model got wrong. This ensures that the next predictive model will not make the same mistakes.</p>
<p>The process by which the AdaBoost algorithm works is illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e0a5e241-8f39-4f5c-a0df-b6a79aab1801.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An outline of the AdaBoost algorithm</div>
<p>In the preceding diagram of the AdaBoost algorithm, the following happens:</p>
<ol>
<li>The first decision tree is built and outputs a set of predictions.</li>
<li>The predictions that the first decision tree got wrong are given a weight of <kbd>w</kbd>. This means that, if the weight is set to 2, then two instances of that particular sample are introduced into the dataset.</li>
<li>This enables decision tree 2 to learn at a faster rate, since we have more samples of the data in which an error was made beforehand.</li>
<li>This process is repeated until all the trees are built.</li>
<li>Finally, the predictions of all the trees are gathered, and a weighted vote is initiated in order to determine the final prediction.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the AdaBoost classifier in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how we can implement the AdaBoost classifier in scikit-learn in order to predict if a transaction is fraudulent or not. As usual, the first step is to import the data and split it into training and testing sets.</p>
<p>This can be done with the following code:</p>
<pre>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the index<br/><br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)</pre>
<p>The next step is to build the AdaBoost classifier. We can do this using the following code:</p>
<pre>from sklearn.ensemble import AdaBoostClassifier<br/><br/>#Initialize a tree (Decision Tree with max depth = 1)<br/><br/>tree = DecisionTreeClassifier(max_depth=1, random_state = 42)<br/><br/>#Initialize an AdaBoost classifier with the tree as the base estimator<br/><br/>ada_boost = AdaBoostClassifier(base_estimator = tree, n_estimators=100)<br/><br/>#Fitting the AdaBoost classifier to the training set<br/><br/>ada_boost.fit(X_train, y_train)<br/><br/>#Extracting the accuracy scores from the classifier<br/><br/>ada_boost.score(X_test, y_test)</pre>
<p>In the preceding code block, we do the following:</p>
<ol>
<li>We first import the <kbd>AdaBoostClassifier</kbd> package from scikit-learn.</li>
<li>Next, we initialize a decision tree that forms the base of our AdaBoost classifier.</li>
<li>We then build the AdaBoost classifier, with the base estimator as the decision tree, and we specify that we want 100 decision trees in total.</li>
<li>Finally, we fit the classifier to the training data, and extract the accuracy scores from the test data.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning for the AdaBoost classifier</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to tune the hyperparameters of the AdaBoost classifier. The AdaBoost classifier has only one parameter of interest—the number of base estimators, or decision trees.</p>
<p>We can optimize the hyperparameters of the AdaBoost classifier using the following code:</p>
<pre>from sklearn.model_selection import GridSearchCV<br/><br/>#Creating a grid of hyperparameters<br/><br/>grid_params = {<br/>    'n_estimators': [100,200,300]<br/>}<br/><br/>#Building a 3 fold CV GridSearchCV object<br/><br/>grid_object = GridSearchCV(estimator = ada_boost, param_grid = grid_params, scoring = 'accuracy', cv = 3, n_jobs = -1)<br/><br/>#Fitting the grid to the training data<br/><br/>grid_object.fit(X_train, y_train)<br/><br/>#Extracting the best parameters<br/><br/>grid_object.best<em>params<br/></em><br/>#Extracting the best model<em><br/></em><br/>ada_best = grid_object.best_estimator_</pre>
<p>In the preceding code, we do the following:</p>
<ol>
<li>We first import the <kbd>GridSearchCV</kbd> package.</li>
<li>We initialize a dictionary of hyperparameter values. In this case, <kbd>n_estimators</kbd> is the number of decision trees.</li>
<li>We then build and fit the <kbd>gridsearch</kbd> object to the training data and extract the best parameters.</li>
<li>The best model is then extracted using these optimal hyperparameters.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression trees</h1>
                </header>
            
            <article>
                
<p>You have learned how trees are used in order to classify a prediction as belonging to a particular class or category. However, trees can also be used to solve problems related to predicting numeric outcomes. In this section, you will learn about the three types of tree based algorithms that you can implement in scikit-learn in order to predict numeric outcomes, instead of classes:</p>
<ul>
<li>The decision tree regressor</li>
<li>The random forest regressor</li>
<li>The gradient boosted tree</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The decision tree regressor</h1>
                </header>
            
            <article>
                
<p>When we have data that is non-linear in nature, a linear regression model might not be the best model to choose. In such situations, it makes sense to choose a model that can fully capture the non-linearity of such data. A decision tree regressor can be used to predict numeric outcomes, just like that of the linear regression model.</p>
<p>In the case of the decision tree regressor, we use the mean squared error, instead of the Gini metric, in order to determine how the tree is built. You will learn about the <span>mean squared error</span> in detail in <a href="99286f39-a802-4285-a217-547b2ff62d71.xhtml" target="_blank">Chapter 8</a>, <em>Performance Evaluation Methods</em>. In a nutshell, the <span>mean squared error</span> is used to tell us about the <span>prediction</span> error rate.</p>
<p>Consider the tree shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/42ed3bf2-2be0-462f-8fdf-66e18617c5db.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An example decision tree for regression</div>
<p>When considering the preceding diagram of the decision tree, note the following:</p>
<ul>
<li>We are trying to predict the amount of a mobile transaction using the tree.</li>
<li>When the tree tries to decide on a split, it chooses the node in such a way that the target value is closest to the mean values of the target in that node.</li>
<li>You will notice that, as you go down the tree to the left, along the <kbd>True</kbd> cases, the <span>mean squared error</span> of the nodes decreases.</li>
<li>Therefore, the nodes are built in a recursive fashion, such that it reduces the overall <span>mean squared error</span>, thereby obtaining the <kbd>True</kbd> value.</li>
<li>In the preceding tree, if the old balance of origination is less than 600,281, then the amount (here, coded as <kbd>value</kbd>) is 80,442, and if it's greater than 600,281, then the amount is 1,988,971.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the decision tree regressor in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how to implement the decision tree regressor in scikit-learn. The first step is to import the data, and create the features and target variables. We can do this using the following code:</p>
<pre>import pandas as pd<br/><br/>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the index<br/><br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Creating the features <br/><br/>features = df.drop('amount', axis = 1).values<br/>target = df['amount'].values</pre>
<p>Note how, in the case of regression, the target variable is the amount, and not the <kbd>isFraud</kbd> column.</p>
<p>Next, we split the data into training and test sets, and build the decision tree regressor, as shown in the following code:</p>
<pre>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeRegressor<br/><br/>#Splitting the data into training and test sets<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42)<br/><br/>#Building the decision tree regressor <br/><br/>dt_reg = DecisionTreeRegressor(max_depth = 10, min_samples_leaf = 0.2, random_state= 50)<br/><br/>#Fitting the tree to the training data<br/><br/>dt_reg.fit(X_train, y_train)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the preceding code, we do the following:</p>
<ol>
<li>We first import the required packages and split the data into training and test sets.</li>
<li>Next, we build the decision tree regressor using the <kbd>DecisionTreeRegressor()</kbd> function.</li>
<li>We specify two hyperparameter arguments: <kbd>max_depth</kbd>, which tells the algorithm how many branches the tree must have, and <kbd>min_sample_leaf</kbd>, which tells the tree about the minimum number of samples that each node must have. The latter is set to 20%, or 0.2 of the total data, in this case.</li>
<li><kbd>random_state</kbd> is set to 50 to ensure that the same tree is built every time we run the code.</li>
<li>We then fit the tree to the training data.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing the decision tree regressor</h1>
                </header>
            
            <article>
                
<p>Just as we visualized the decision tree classifier, we can also visualize the decision tree regressor. Instead of showing you the classes or categories to which the node of a tree belongs, you will now be shown the value of the target variable.</p>
<p>We can visualize the decision tree regressor by using the following code:</p>
<pre>#Package requirements <br/><br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.externals.six import StringIO <br/>from IPython.display import Image <br/>from sklearn.tree import export_graphviz<br/>import pydotplus<br/>from sklearn import tree<br/><br/>#Extracting the feature names<br/><br/>feature_names = df.drop('amount', axis = 1)<br/><br/>#Creating the tree visualization<br/><br/>data = tree.export_graphviz(dt_reg, out_file=None, feature_names= feature_names.columns.values, proportion= True)<br/><br/>graph = pydotplus.graph_from_dot_data(data) <br/><br/># Show graph<br/>Image(graph.create_png())</pre>
<p>The code follows the exact same methodology as that of the decision tree classifier, and will not be discussed in detail here. This produces a decision tree regressor like that in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e84a5850-7db2-49e0-b8fa-b16ba817ec8d.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A visualization of the decision tree regressor</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The random forest regressor</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">The random forest regressor takes the decision tree regressor as the base estimator, and makes predictions in a method similar to that of the random forest classifier, as illustrated by the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c0365b85-8717-4817-9b84-c3324b1c7c59.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Making the final prediction in the random forest regressor</div>
<p>The only difference between the random forest classifier and the random forest regressor is the fact that, in the case of the latter, the base estimator is a decision tree regressor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the random forest regressor in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how you can implement the random forest regressor in scikit-learn. The first step is to import the data and split it into training and testing sets. This can be done using the following code:</p>
<pre>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the index<br/><br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Creating the features and target arrays<br/><br/>features = df.drop('amount', axis = 1).values<br/>target = df['amount'].values<br/><br/>#Splitting the data into training and test sets<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42)</pre>
<p>The next step is to build the random forest regressor. We can do this using the following code:</p>
<pre>from sklearn.ensemble import RandomForestRegressor<br/><br/>#Initiliazing an Random Forest Regressor with default parameters<br/><br/>rf_reg = RandomForestRegressor(max_depth = 10, min_samples_leaf = 0.2, random_state = 50)<br/><br/>#Fitting the regressor on the training data<br/><br/>rf_reg.fit(X_train, y_train)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the preceding code, we do the following:</p>
<ol>
<li>We first import the <kbd>RandomForestRegressor</kbd> module from scikit-learn.</li>
<li>We then initialize a random forest regressor object, called <kbd>rf_reg</kbd>, with a maximum depth of 10 for each decision tree, and the minimum number of data and samples in each tree as 20% of the total data.</li>
<li>We then fit the tree to the training set.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The gradient boosted tree</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how the gradient boosted tree is used for regression, and how you can implement this using scikit-learn.</p>
<p>In the AdaBoost classifier that you learned about earlier in this chapter, weights are added to the examples that the classifier predicted in correctly. In the gradient boosted tree, however, instead of weights, the residual errors are used as labels in each tree in order to make future predictions. This concept is illustrated for you in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0d3486ae-2764-4bc3-8d70-ac0625150960.png" style=""/></div>
<p>Here is what occurs in the preceding diagram:</p>
<ol>
<li>The first decision tree is trained with the data that you have, and the target variable <strong>Y</strong>.</li>
<li>We then compute the residual error for this tree.</li>
<li>The residual error is given by the difference between the predicted value and the actual value.</li>
<li>The second tree is now trained, using the residuals as the target.</li>
<li>This process of building multiple trees is iterative, and continues for the number of base estimators that we have.</li>
<li>The final prediction is made by adding the target value predicted by the first tree to the product of the shrinkage and the residuals for all the other trees.</li>
</ol>
<ol start="7">
<li>The shrinkage is a factor with which we control the rate at which we want this gradient boosting <span>process</span> to take place.</li>
<li>A small value of shrinkage (learning rate) implies that the algorithm will learn more quickly, and therefore, must be compensated with a larger number of base estimators (that is, decision trees) in order to prevent overfitting.</li>
<li>A larger value of shrinkage (learning rate) implies that the algorithm will learn more slowly, and thus requires fewer trees in order to reduce the computational time.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the gradient boosted tree in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how we can implement the gradient boosted regressor in scikit-learn. The first step, as usual, is to import the dataset, define the features and target arrays, and split the data into training and test sets. This can be done using the following code:</p>
<pre>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the index<br/><br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Creating the features <br/><br/>features = df.drop('amount', axis = 1).values<br/>target = df['amount'].values<br/><br/>#Splitting the data into training and test sets<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42)</pre>
<p>The next step is to build the gradient boosted regressor. This can be done using the following code:</p>
<pre>from sklearn.ensemble import GradientBoostingRegressor<br/><br/>#Initializing an Gradient Boosted Regressor with default parameters<br/><br/>gb_reg = GradientBoostingRegressor(max_depth = 5, n_estimators = 100, learning_rate = 0.1, random_state = 50)<br/><br/>#Fitting the regressor on the training data<br/><br/>gb_reg.fit(X_train, y_train)</pre>
<p>In the preceding code, we do the following:</p>
<ol>
<li>We first import <kbd>GradientBoostingRegressor</kbd> from scikit-learn.</li>
<li>We the build a <span>gradient boosted regressor</span> object with three main arguments: the maximum depth of each tree, the total number of trees, and the learning rate.</li>
<li>We then fit the regressor on the training data.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensemble classifier</h1>
                </header>
            
            <article>
                
<p>The concept of ensemble learning was explored in this chapter, when we learned about <span>random forest</span>s, AdaBoost, <span><span>and</span></span> <span>gradient boosted trees. However, this concept can be extended to classifiers outside of trees.</span></p>
<p>If we had built a logistic regression, random forest, <span>and</span> k-nearest neighbors classifiers, <span>and</span> we wanted to group them all together and extract the final prediction through majority voting, then we could do this by using the ensemble classifier.</p>
<p>This concept can be better understood with the aid of the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/15463931-a9d9-4b97-aec4-75fa2bfc4f6f.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Ensemble learning with a voting classifier to predict fraud transactions</div>
<p>When examining the <span>preceding</span> diagram, note the following:</p>
<ul>
<li>The random forest classifier predicted that a particular transaction was fraudulent, while the other two classifiers predicted that the transaction was not fraudulent.</li>
<li>The <span>voting classifier</span> sees that two out of three (that is, a majority) of the predictions are <strong>Not Fraud</strong>, and hence, outputs the final prediction as <strong>Not Fraud</strong>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the voting classifier in scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how to implement the voting classifier in scikit-learn. The first step is to import the data, create the feature and target arrays, and create the training and testing splits. This can be done using the following code:</p>
<pre>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the index<br/><br/>df = df.drop(['Unnamed: 0'], axis = 1)<br/><br/>#Splitting the data into training and test sets<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42)</pre>
<p>Next, we will build two classifiers that include the voting classifier: the decision tree classifier <span>and</span> the random forest classifier. This can be done using the following code:</p>
<pre>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier<br/><br/>#Initializing the DT classifier<br/><br/>dt = DecisionTreeClassifier(criterion = 'gini', random_state = 50)<br/><br/>#Fitting on the training data<br/><br/>dt.fit(X_train, y_train)<br/><br/>#Initiliazing an Random Forest Classifier with default parameters<br/><br/>rf_classifier = RandomForestClassifier(random_state = 50)<br/><br/>#Fitting the classifier on the training data<br/><br/>rf_classifier.fit(X_train, y_train)</pre>
<p>Next, we will build the voting classifier by using the following code:</p>
<pre>from sklearn.ensemble import VotingClassifier<br/><br/>#Creating a list of models<br/><br/>models = [('Decision Tree', dt), ('Random Forest', rf_classifier)]<br/><br/>#Initialize a voting classifier <br/><br/>voting_model = VotingClassifier(estimators = models)<br/><br/>#Fitting the model to the training data<br/><br/>voting_model.fit(X_train, y_train)<br/><br/>#Evaluating the accuracy on the test data<br/><br/>voting_model.score(X_test, y_test)</pre>
<p>In the <span>preceding</span> code, we do the following:</p>
<ol>
<li>We first import the <kbd>VotingClassifier</kbd> module from scikit-learn.</li>
<li>Next, we create a list of all the models that we want to use in our voting classifier.</li>
<li>In the list of classifiers, each model is stored in a tuple, along with the model's name in a string and the model itself.</li>
<li>We then initialize a <span>voting classifier</span> with the list of models built in step 2.</li>
<li>Finally, the model is fitted to the training data and the accuracy is extracted from the test data.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>While this chapter was rather long, you have entered the world of tree based algorithms, <span>and</span> left with a wide arsenal of tools that you can implement in order to solve both small- <span>and</span> large-scale problems. To summarize, you have learned the following:</p>
<ul>
<li>How to use decision trees for classification <span>and</span> regression</li>
<li>How to use random forests for classification <span>and</span> regression</li>
<li>How to use AdaBoost for classification</li>
<li>How to use gradient boosted trees for regression</li>
<li>How the voting classifier can be used to build a single model out of different models</li>
</ul>
<p>In the upcoming chapter, you will learn how you can work with data that does not have a target variable or labels, and how to perform unsupervised machine learning in order to solve such problems!</p>


            </article>

            
        </section>
    </body></html>
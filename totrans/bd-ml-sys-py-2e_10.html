<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 10. Computer Vision"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Computer Vision</h1></div></div></div><p>Image analysis and computer vision have always been important in industrial and scientific applications. With the popularization of cell phones with powerful cameras and Internet connections, images now are increasingly generated by consumers. Therefore, there are opportunities to make use of computer vision to provide a better user experience in new contexts.</p><p>In this chapter, we will look at how to apply techniques you have learned in the rest of this book to this specific type of data. In particular, we will learn how to use the mahotas computer vision package to extract features from images. These features can be used as input to the same classification methods we studied in other chapters. We will apply these techniques to publicly available datasets of photographs. We will also see how the same features can be used on another problem, that is, the problem of finding similar looking images.</p><p>Finally, at the end of this chapter, we will learn about using local features. These are relatively new methods (the first of these methods to achieve state-of-the-art performance, the <a id="id514" class="indexterm"/>
<span class="strong"><strong>scale-invariant feature transform</strong></span> (<span class="strong"><strong>SIFT</strong></span>), was introduced in 1999) and achieve very good results in many tasks.</p><div class="section" title="Introducing image processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec59"/>Introducing image processing</h1></div></div></div><p>From the<a id="id515" class="indexterm"/> point of<a id="id516" class="indexterm"/> view of the computer, an image is a large rectangular array of pixel values. Our goal is to process this image and to arrive at a decision for our application.</p><p>The first step will be to load the image from disk, where it is typically stored in an image-specific format such as PNG or JPEG, the former being a lossless compression format, and the latter a lossy compression one that is optimized for visual assessment of photographs. Then, we may wish to perform preprocessing on the images (for example, normalizing them for illumination variations).</p><p>We will have a classification problem as a driver for this chapter. We want to be able to learn a support vector machine (or other) classifier that can be trained from images. Therefore, we will use an intermediate representation, extracting numeric features from the images before<a id="id517" class="indexterm"/> applying machine learning.</p><div class="section" title="Loading and displaying images"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec82"/>Loading and displaying images</h2></div></div></div><p>In order to<a id="id518" class="indexterm"/> manipulate images, we will use a package called mahotas. You can obtain mahotas from <a class="ulink" href="https://pypi.python.org/pypi/mahotas">https://pypi.python.org/pypi/mahotas</a> and read its manual at <a class="ulink" href="http://mahotas.readthedocs.org">http://mahotas.readthedocs.org</a>. Mahotas is an open source package (MIT license, so it can be used in any project) that was developed by one of the authors of this book. Fortunately, it is based on NumPy. The NumPy knowledge you have acquired so far can be used for image processing. There are other image packages, such as scikit-image (skimage), the ndimage (n-dimensional image) module in SciPy, and the Python bindings for OpenCV. All of these work natively with NumPy arrays, so you can even mix and match functionality from different packages to build a combined pipeline.</p><p>We start by importing <a id="id519" class="indexterm"/>mahotas, with the <code class="literal">mh</code> abbreviation, which we will use throughout this chapter, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import mahotas as mh</strong></span>
</pre></div><p>Now, we can load an image file using <code class="literal">imread</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; image = mh.imread('scene00.jpg')</strong></span>
</pre></div><p>The <code class="literal">scene00.jpg</code> file (this file is contained in the dataset available on this book's companion code repository) is a color image of height <code class="literal">h</code> and width <code class="literal">w</code>; the image will be an array of shape <code class="literal">(h, w, 3)</code>. The first dimension is the height, the second is the width, and the third is red/green/blue. Other systems put the width in the first dimension, but this is the convention that is used by all NumPy-based packages. The type of the array will typically be <code class="literal">np.uint8</code> (an unsigned 8-bit integer). These are the images that your camera takes or that your monitor can fully display.</p><p>Some specialized equipment, used in scientific and technical applications, can take images with higher bit resolution (that is, with more sensitivity to small variations in brightness). Twelve or sixteen bits are common in this type of equipment. Mahotas can deal with all these types, including floating point images. In many computations, even if the original data is composed of unsigned integers, it is advantageous to convert to floating point numbers in order to simplify handling of rounding and overflow issues.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>Mahotas can use a variety of different input/output backends. Unfortunately, none of them can load all image formats that exist (there are hundreds, with several variations of each). However, loading PNG and JPEG images is supported by all of them. We will focus on these common formats and refer you to the mahotas documentation on how to read uncommon formats.</p></div></div><p>We can display the image on screen using matplotlib, the plotting library we have already used several times, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from matplotlib import pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.imshow(image)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>As shown<a id="id520" class="indexterm"/> in the<a id="id521" class="indexterm"/> following, this code shows the image using the convention that the first dimension is the height and the second the width. It correctly handles color images as well. When using Python for numerical computation, we benefit from the whole ecosystem working well together: mahotas works with NumPy arrays, which can be displayed with matplotlib; later we will compute features from images to use with scikit-learn.</p><div class="mediaobject"><img src="images/2772OS_10_01.jpg" alt="Loading and displaying images"/></div></div><div class="section" title="Thresholding"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec83"/>Thresholding</h2></div></div></div><p>
<span class="strong"><strong>Thresholding</strong></span> is<a id="id522" class="indexterm"/> a <a id="id523" class="indexterm"/>very simple operation: we transform all pixel values above a certain threshold to <code class="literal">1</code> and all those below it to <code class="literal">0</code> (or by using Booleans, transform it to <code class="literal">True</code> and <code class="literal">False</code>). The important question in thresholding is to select a good value to use as the threshold limit. Mahotas implements a few methods for choosing a threshold value from the image. One is called <span class="strong"><strong>Otsu</strong></span>, after <a id="id524" class="indexterm"/>its inventor. The first necessary step is to convert the image to grayscale, with <code class="literal">rgb2gray</code> in the <code class="literal">mahotas.colors</code> submodule.</p><p>Instead of <code class="literal">rgb2gray</code>, we could also have just the mean value of the red, green, and blue channels, by callings <code class="literal">image.mean(2)</code>. The result, however, would not be the same, as <code class="literal">rgb2gray</code> uses different weights for the different colors to give a subjectively more pleasing result. Our eyes are not equally sensitive to the three basic colors.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; image = mh.colors.rgb2grey(image, dtype=np.uint8)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.imshow(image) # Display the image</strong></span>
</pre></div><p>By default, matplotlib will display this single-channel image as a false color image, using red for high values and blue for low values. For natural images, a grayscale is more appropriate. You can select it with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; plt.gray()</strong></span>
</pre></div><p>Now the image is shown in gray scale. Note that only the way in which the pixel values are interpreted and shown has changed and the image data is untouched. We can continue our processing by computing the threshold value:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; thresh = mh.thresholding.otsu(image)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Otsu threshold is {}.'.format(thresh))</strong></span>
<span class="strong"><strong>Otsu threshold is 138.</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.imshow(image &gt; thresh)</strong></span>
</pre></div><p>When applied to the previous screenshot, this method finds the threshold to be 138, which separates the ground from the sky above, as shown in the following image:</p><div class="mediaobject"><img src="images/2772OS_10_03.jpg" alt="Thresholding"/></div></div><div class="section" title="Gaussian blurring"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec84"/>Gaussian blurring</h2></div></div></div><p>Blurring <a id="id525" class="indexterm"/>your image may seem odd, but it often serves to reduce noise, which helps with further processing. With mahotas, it is just a function call:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; im16 = mh.gaussian_filter(image, 16)</strong></span>
</pre></div><p>Notice that we did not convert the grayscale image to unsigned integers: we just made use of the floating point result as it is. The second argument to the <code class="literal">gaussian_filter</code> function is the size of the filter (the standard deviation of the filter). Larger values result in more blurring, as shown in the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_10_04.jpg" alt="Gaussian blurring"/></div><p>We can use<a id="id526" class="indexterm"/> the screenshot on the left and threshold with Otsu (using the same previous code). Now, the boundaries are smoother, without the jagged edges, as shown in the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_10_05.jpg" alt="Gaussian blurring"/></div></div><div class="section" title="Putting the center in focus"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec85"/>Putting the center in focus</h2></div></div></div><p>The<a id="id527" class="indexterm"/> final example shows how to mix NumPy operators with a tiny bit of filtering to get an interesting result. We start with the Lena image and split it into the color channels:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; im = mh.demos.load('lena')</strong></span>
</pre></div><p>This is an image of a young woman that has been often for image processing demos. It is shown in the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_10_06.jpg" alt="Putting the center in focus"/></div><p>To split the red, green, and blue channels, we use the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; r,g,b = im.transpose(2,0,1)</strong></span>
</pre></div><p>Now, we filter <a id="id528" class="indexterm"/>the three channels separately and build a composite image out of it with <code class="literal">mh.as_rgb</code>. This function takes three two-dimensional arrays, performs contrast stretching to make each be an 8-bit integer array, and then stacks them, returning a color RGB image:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; r12 = mh.gaussian_filter(r, 12.)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; g12 = mh.gaussian_filter(g, 12.)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; b12 = mh.gaussian_filter(b, 12.)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; im12 = mh.as_rgb(r12, g12, b12)</strong></span>
</pre></div><p>Now, we blend the two images from the center away to the edges. First, we need to build a weights array <code class="literal">W</code>, which will contain at each pixel a normalized value, which is its distance to the center:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; h, w = r.shape # height and width</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; Y, X = np.mgrid[:h,:w]</strong></span>
</pre></div><p>We used <a id="id529" class="indexterm"/>the <code class="literal">np.mgrid</code> object, which returns arrays of size <code class="literal">(h, w)</code>, with values corresponding to the <span class="emphasis"><em>y</em></span> and <span class="emphasis"><em>x</em></span> coordinates, respectively. The next steps are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; Y = Y - h/2. # center at h/2</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; Y = Y / Y.max() # normalize to -1 .. +1</strong></span>

<span class="strong"><strong>&gt;&gt;&gt; X = X - w/2.</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; X = X / X.max()</strong></span>
</pre></div><p>We now use a Gaussian function to give the center region a high value:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; C = np.exp(-2.*(X**2+ Y**2))</strong></span>

<span class="strong"><strong>&gt;&gt;&gt; # Normalize again to 0..1</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; C = C - C.min()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; C = C / C.ptp()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; C = C[:,:,None] # This adds a dummy third dimension to C</strong></span>
</pre></div><p>Notice how all of these manipulations are performed using NumPy arrays and not some mahotas-specific methodology. Finally, we can combine the two images to have the center be in sharp focus and the edges softer:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; ringed = mh.stretch(im*C + (1-C)*im12)</strong></span>
</pre></div><div class="mediaobject"><img src="images/2772OS_10_17.jpg" alt="Putting the center in focus"/></div></div><div class="section" title="Basic image classification"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec86"/>Basic image classification</h2></div></div></div><p>We will <a id="id530" class="indexterm"/>start with a small dataset that was collected especially for this book. It has three classes: buildings, natural scenes (landscapes), and pictures of texts. There are 30 images in each category, and they were all taken using a cell phone camera with minimal composition. The images are similar to those that would be uploaded to a modern website by users with no photography training. This dataset is available from this book's website or the GitHub code repository. Later in this chapter, we will look at a harder dataset with more images and more categories.</p><p>When classifying images, we start with a large rectangular array of numbers (pixel values). Nowadays, millions of pixels are common. We could try to feed all these numbers as features into the learning algorithm. This is not a very good idea. This is because the relationship of each pixel (or even each small group of pixels) to the final result is very indirect. Also, having millions of pixels, but only as a small number of example images, results in a very hard statistical learning problem. This is an extreme form of the P greater than N type of problem we discussed in <a class="link" href="ch07.html" title="Chapter 7. Regression">Chapter 7</a>, <span class="emphasis"><em>Regression</em></span>. Instead, a good approach is to compute features from the image and use those features for classification.</p><p>Having said that, I will point out that, in fact, there are a few methods that do work directly from the pixel values. They have feature computation submodules inside them. They may even attempt to learn good features automatically. These methods are the topic of current research. They typically work best with very large datasets (millions of images).</p><p>We previously<a id="id531" class="indexterm"/> used an example of the scene class. The following are examples of the text and building classes:</p><div class="mediaobject"><img src="images/2772OS_10_10.jpg" alt="Basic image classification"/></div></div><div class="section" title="Computing features from images"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec87"/>Computing features from images</h2></div></div></div><p>With <a id="id532" class="indexterm"/>mahotas, it is very easy to compute features from images. There is a submodule named <code class="literal">mahotas.features</code>, where feature computation functions are available.</p><p>A commonly used set of texture features is the Haralick. As with many methods in image processing, the name is due to its inventor. These features are texture-based: they distinguish between images that are smooth from those that are patterned, and between different patterns. With mahotas, it is very easy to compute them as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; haralick_features = mh.features.haralick(image)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; haralick_features_mean = np.mean(haralick_features, axis=0)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; haralick_features_all = np.ravel(haralick_features)</strong></span>
</pre></div><p>The <code class="literal">mh.features.haralick</code> function returns a 4x13 array. The first dimension refers to four possible directions in which to compute the features (vertical, horizontal, diagonal, and the anti-diagonal). If we are not interested in the direction specifically, we can use the average over all the directions (shown in the earlier code as <code class="literal">haralick_features_mean</code>). Otherwise, we can use all the features separately (using <code class="literal">haralick_features_all</code>). This decision should be informed by the properties of the dataset. In our case, we reason that the horizontal and vertical directions should be kept separately. Therefore, we will use <code class="literal">haralick_features_all</code>.</p><p>There are a few other feature sets implemented in mahotas. Linear binary patterns are another texture-based feature set, which is very robust against illumination changes. There are other types of features, including local features, which we will discuss later in this chapter.</p><p>With these <a id="id533" class="indexterm"/>features, we use a standard classification method such as logistic regression as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from glob import glob</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; images = glob('SimpleImageDataset/*.jpg')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; features = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; labels = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for im in images:</strong></span>
<span class="strong"><strong>...   labels.append(im[:-len('00.jpg')])</strong></span>
<span class="strong"><strong>...   im = mh.imread(im)</strong></span>
<span class="strong"><strong>...   im = mh.colors.rgb2gray(im, dtype=np.uint8)</strong></span>
<span class="strong"><strong>...   features.append(mh.features.haralick(im).ravel())</strong></span>


<span class="strong"><strong>&gt;&gt;&gt; features = np.array(features)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; labels = np.array(labels)</strong></span>
</pre></div><p>The three classes have very different textures. Buildings have sharp edges and big blocks where the color is similar (the pixel values are rarely exactly the same, but the variation is slight). Text is made of many sharp dark-light transitions, with small black areas in a sea of white. Natural scenes have smoother variations with fractal-like transitions. Therefore, a classifier based on texture is expected to do well.</p><p>As a classifier, we are going to use a logistic regression classifier with preprocessing of the features as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.pipeline import Pipeline</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; clf = Pipeline([('preproc', StandardScaler()),</strong></span>
<span class="strong"><strong>                    ('classifier', LogisticRegression())])</strong></span>
</pre></div><p>Since our dataset is small, we can use leave-one-out regression as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn import cross_validation</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; cv = cross_validation.LeaveOneOut(len(images))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; scores = cross_validation.cross_val_score(</strong></span>
<span class="strong"><strong>...     clf, features, labels, cv=cv)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Accuracy: {:.1%}'.format(scores.mean()))</strong></span>
<span class="strong"><strong>Accuracy: 81.1%</strong></span>
</pre></div><p>Eighty-one percent is not bad for the three classes (random guessing would correspond to 33 percent). We <a id="id534" class="indexterm"/>can do better, however, by writing our own features.</p></div><div class="section" title="Writing your own features"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec88"/>Writing your own features</h2></div></div></div><p>A feature is <a id="id535" class="indexterm"/>nothing magical. It is simply a number that we computed from an image. There are several feature sets already defined in the literature. These often have the added advantage that they have been designed and studied to be invariant to many unimportant factors. For example, linear binary patterns are completely invariant to multiplying all pixel values by a number or adding a constant to all these values. This makes this feature set robust against illumination changes of images.</p><p>However, it is also possible that your particular use case would benefit from a few specially designed features.</p><p>A simple type of feature that is not shipped with mahotas is a color histogram. Fortunately, this feature is easy to implement. A color histogram partitions the color space into a set of bins, and then counts how many pixels fall into each of the bins.</p><p>The images are in RGB format, that is, each pixel has three values: R for red, G for green, and B for blue. Since each of these components is an 8-bit value, the total is 17 million different colors. We are going to reduce this number to only 64 colors by grouping colors into bins. We will write a function to encapsulate this algorithm as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def chist(im):</strong></span>
</pre></div><p>To bin the colors, we first divide the image by 64, rounding down the pixel values as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    im = im // 64</strong></span>
</pre></div><p>This makes the pixel values range from 0 to 3, which gives a total of 64 different colors.</p><p>Separate the red, green, and blue channels as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    r,g,b = im.transpose((2,0,1))</strong></span>
<span class="strong"><strong>    pixels = 1 * r + 4 * b + 16 * g</strong></span>
<span class="strong"><strong>    hist = np.bincount(pixels.ravel(), minlength=64)</strong></span>
<span class="strong"><strong>    hist = hist.astype(float)</strong></span>
</pre></div><p>Convert to log scale, as seen in the following code snippet. This is not strictly necessary, but makes for better features. We use <code class="literal">np.log1p</code>, which computes <span class="emphasis"><em>log(h+1)</em></span>. This ensures that zero values are kept as zero values (mathematically, the logarithm of zero is not defined, and NumPy prints a warning if you attempt to compute it).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    hist = np.log1p(hist)</strong></span>
<span class="strong"><strong>    return hist</strong></span>
</pre></div><p>We can adapt the previous processing code to use the function we wrote very easily:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; features = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for im in images:</strong></span>
<span class="strong"><strong>...   image = mh.imread(im)</strong></span>
<span class="strong"><strong>...   features.append(chist(im))</strong></span>
</pre></div><p>Using the same <a id="id536" class="indexterm"/>cross-validation code we used earlier, we obtain 90 percent accuracy. The best results, however, come from combining all the features, which we can implement as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; features = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for im in images:</strong></span>
<span class="strong"><strong>...   imcolor = mh.imread(im)</strong></span>
<span class="strong"><strong>...   im = mh.colors.rgb2gray(imcolor, dtype=np.uint8)</strong></span>
<span class="strong"><strong>...   features.append(np.concatenate([</strong></span>
<span class="strong"><strong>...           mh.features.haralick(im).ravel(),</strong></span>
<span class="strong"><strong>...           chist(imcolor),</strong></span>
<span class="strong"><strong>...       ]))</strong></span>
</pre></div><p>By using all of these features, we get 95.6 percent accuracy, as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; scores = cross_validation.cross_val_score(</strong></span>
<span class="strong"><strong>...     clf, features, labels, cv=cv)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Accuracy: {:.1%}'.format(scores.mean()))</strong></span>
<span class="strong"><strong>Accuracy: 95.6%</strong></span>
</pre></div><p>This is a perfect illustration of the principle that good algorithms are the easy part. You can always use an implementation of state-of-the-art classification from scikit-learn. The real secret and added value often comes in feature design and engineering. This is where knowledge of your dataset is valuable.</p></div><div class="section" title="Using features to find similar images"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec89"/>Using features to find similar images</h2></div></div></div><p>The<a id="id537" class="indexterm"/> basic concept of representing an image by a relatively small number of features can be used for more than just classification. For example, we can also use it to find similar images to a given query image (as we did before with text documents).</p><p>We will compute the same features as before, with one important difference: we will ignore the bordering area of the picture. The reason is that due to the amateur nature of the compositions, the edges of the picture often contain irrelevant elements. When the features are computed over the whole image, these elements are taken into account. By simply ignoring them, we get slightly better features. In the supervised example, it is not as important, as the learning algorithm will then learn which features are more informative and weigh them accordingly. When working in an unsupervised fashion, we need to be more careful to ensure that our features are capturing important elements of the data. This is implemented in the loop as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; features = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for im in images:</strong></span>
<span class="strong"><strong>...   imcolor = mh.imread(im)</strong></span>
<span class="strong"><strong>...   # ignore everything in the 200 pixels closest to the borders</strong></span>
<span class="strong"><strong>...   imcolor = imcolor[200:-200, 200:-200]</strong></span>
<span class="strong"><strong>...   im = mh.colors.rgb2gray(imcolor, dtype=np.uint8)</strong></span>
<span class="strong"><strong>...   features.append(np.concatenate([</strong></span>
<span class="strong"><strong>...           mh.features.haralick(im).ravel(),</strong></span>
<span class="strong"><strong>...           chist(imcolor),</strong></span>
<span class="strong"><strong>...       ]))</strong></span>
</pre></div><p>We now <a id="id538" class="indexterm"/>normalize the features and compute the distance matrix as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; sc = StandardScaler()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; features = sc.fit_transform(features)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from scipy.spatial import distance</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; dists = distance.squareform(distance.pdist(features))</strong></span>
</pre></div><p>We will plot just a subset of the data (every 10th element) so that the query will be on top and the returned "nearest neighbor" at the bottom, as shown in the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; fig, axes = plt.subplots(2, 9)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for ci,i in enumerate(range(0,90,10)):</strong></span>
<span class="strong"><strong>...     left = images[i]</strong></span>
<span class="strong"><strong>...     dists_left = dists[i]</strong></span>
<span class="strong"><strong>...     right = dists_left.argsort()</strong></span>
<span class="strong"><strong>...     # right[0] is same as left[i], so pick next closest</strong></span>
<span class="strong"><strong>...     right = right[1]</strong></span>
<span class="strong"><strong>...     right = images[right]</strong></span>
<span class="strong"><strong>...     left = mh.imread(left)</strong></span>
<span class="strong"><strong>...     right = mh.imread(right)</strong></span>
<span class="strong"><strong>...     axes[0, ci].imshow(left)</strong></span>
<span class="strong"><strong>...     axes[1, ci].imshow(right)</strong></span>
</pre></div><p>The result is shown in the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_10_02.jpg" alt="Using features to find similar images"/></div><p>It is clear that the system is not perfect, but can find images that are at least visually similar to the <a id="id539" class="indexterm"/>queries. In all but one case, the image found comes from the same class as the query.</p></div><div class="section" title="Classifying a harder dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec90"/>Classifying a harder dataset</h2></div></div></div><p>The previous dataset<a id="id540" class="indexterm"/> was an easy dataset for classification using texture features. In fact, many of the problems that are interesting from a business point of view are relatively easy. However, sometimes we may be faced with a tougher problem and need better and more modern techniques to get good results.</p><p>We will now test a public dataset, which has the same structure: several photographs split into a small number of classes. The classes are animals, cars, transportation, and natural scenes.</p><p>When compared to the three class problem we discussed previously, these classes are harder to tell apart. Natural scenes, buildings, and texts have very different textures. In this dataset, however, texture and color are not as clear marker, of the image class. The following is one example from the animal class:</p><div class="mediaobject"><img src="images/2772OS_10_07.jpg" alt="Classifying a harder dataset"/></div><p>And here <a id="id541" class="indexterm"/>is another example from the car class:</p><div class="mediaobject"><img src="images/2772OS_10_21.jpg" alt="Classifying a harder dataset"/></div><p>Both objects are against natural backgrounds, and with large smooth areas inside the objects. This is a harder problem than the simple dataset, so we will need to use more advanced methods. The first improvement will be to use a slightly more powerful classifier. The logistic regression that scikit-learn provides is a penalized form of logistic regression, which contains an adjustable parameter, <code class="literal">C</code>. By default, <code class="literal">C = 1.0</code>, but this may not be optimal. We can use grid search to find a good value for this parameter as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.grid_search import GridSearchCV</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; C_range = 10.0 ** np.arange(-4, 3)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; grid = GridSearchCV(LogisticRegression(), param_grid={'C' : C_range})</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; clf = Pipeline([('preproc', StandardScaler()),</strong></span>
<span class="strong"><strong>...                ('classifier', grid)])</strong></span>
</pre></div><p>The data is <a id="id542" class="indexterm"/>not organized in a random order inside the dataset: similar images are close together. Thus, we use a cross-validation schedule that considers the data shuffled so that each fold has a more representative training set, as shown in the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; cv = cross_validation.KFold(len(features), 5,</strong></span>
<span class="strong"><strong>...                      shuffle=True, random_state=123)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; scores = cross_validation.cross_val_score(</strong></span>
<span class="strong"><strong>...    clf, features, labels, cv=cv)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Accuracy: {:.1%}'.format(scores.mean()))</strong></span>
<span class="strong"><strong>Accuracy: 72.1%</strong></span>
</pre></div><p>This is not so bad for four classes, but we will now see if we can do better by using a different set of features. In fact, we will see that we need to combine these features with other methods to get the best possible results.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Local feature representations"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec60"/>Local feature representations</h1></div></div></div><p>A<a id="id543" class="indexterm"/> relatively recent development in the computer vision world has been the development of local-feature based methods. Local features are computed on a small region of the image, unlike the previous features we considered, which had been computed on the whole image. Mahotas supports computing a type of these features, <span class="strong"><strong>Speeded Up Robust Features</strong></span> (<span class="strong"><strong>SURF</strong></span>). There<a id="id544" class="indexterm"/> are <a id="id545" class="indexterm"/>several others, the most well-known being the original proposal of SIFT. These features are designed to be robust against rotational or illumination changes (that is, they only change their value slightly when illumination changes).</p><p>When using these features, we have to decide where to compute them. There are three possibilities that are commonly used:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Randomly</li><li class="listitem" style="list-style-type: disc">In a grid</li><li class="listitem" style="list-style-type: disc">Detecting interesting areas of the image (a technique known as keypoint detection or interest point detection)</li></ul></div><p>All of these are valid and will, under the right circumstances, give good results. Mahotas supports all three. Using interest point detection works best if you have a reason to expect that your interest point will correspond to areas of importance in the image.</p><p>We will be using the interest point method. Computing the features with mahotas is easy: import the right submodule and call the <code class="literal">surf.surf</code> function as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from mahotas.features import surf</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; image = mh.demos.load('lena')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; image = mh.colors.rgb2gray(im, dtype=np.uint8)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; descriptors = surf.surf(image, descriptor_only=True)</strong></span>
</pre></div><p>The <code class="literal">descriptors_only=True</code> flag<a id="id546" class="indexterm"/> means that we are only interested in the descriptors themselves, and not in their pixel location, size, or orientation. Alternatively, we could have used the dense sampling method, using the <code class="literal">surf.dense</code> function as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from mahotas.features import surf</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; descriptors = surf.dense(image, spacing=16)</strong></span>
</pre></div><p>This returns <a id="id547" class="indexterm"/>the value of the descriptors computed on points that are at a distance of 16 pixels from each other. Since the position of the points is fixed, the metainformation on the interest points is not very interesting and is not returned by default. In either case, the result (descriptors) is an n-times-64 array, where <span class="emphasis"><em>n</em></span> is the number of points sampled. The number of points depends on the size of your images, their content, and the parameters you pass to the functions. In this example, we are using the default settings, and we obtain a few hundred descriptors per image.</p><p>We cannot directly feed these descriptors to a support vector machine, logistic regressor, or similar classification system. In order to use the descriptors from the images, there are several solutions. We could just average them, but the results of doing so are not very good as they throw away all location specific information. In that case, we would have just another global feature set based on edge measurements.</p><p>The solution we will use here is the <a id="id548" class="indexterm"/>
<span class="strong"><strong>bag of words</strong></span> model, which is a very recent idea. It was published in this form first in 2004. This is one of those obvious-in-hindsight ideas: it is very simple to implement and achieves very good results.</p><p>It may seem strange to speak of <span class="emphasis"><em>words</em></span> when dealing with images. It may be easier to understand if you think that you have not written words, which are easy to distinguish from each other, but orally spoken audio. Now, each time a word is spoken, it will sound slightly different, and different speakers will have their own pronunciation. Thus, a word's waveform will not be identical every time it is spoken. However, by using clustering on these waveforms, we can hope to recover most of the structure so that all the instances of a given word are in the same cluster. Even if the process is not perfect (and it will not be), we can still talk of grouping the waveforms into words.</p><p>We perform the same operation with image data: we cluster together similar looking regions from all images and call these <a id="id549" class="indexterm"/>
<span class="strong"><strong>visual words</strong></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>The number of words used does not usually have a big impact on the final performance of the algorithm. Naturally, if the number is extremely small (10 or 20, when you have a few thousand images), then the overall system will not perform well. Similarly, if you have too many words (many more than the number of images, for example), the system will also not perform well. However, in between these two extremes, there is often a very large plateau, where you can choose the number of words without a big impact on the result. As a rule of thumb, using a value such as 256, 512, or 1,024 if you have very many images should give you a good result.</p></div></div><p>We are<a id="id550" class="indexterm"/> going <a id="id551" class="indexterm"/>to start by computing the features as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; alldescriptors = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for im in images:</strong></span>
<span class="strong"><strong>...   im = mh.imread(im, as_grey=True)</strong></span>
<span class="strong"><strong>...   im = im.astype(np.uint8)</strong></span>
<span class="strong"><strong>...   alldescriptors.append(surf.dense(image, spacing=16))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; # get all descriptors into a single array</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; concatenated = np.concatenate(alldescriptors)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Number of descriptors: {}'.format(</strong></span>
<span class="strong"><strong>...        len(concatenated)))</strong></span>
<span class="strong"><strong>Number of descriptors: 2489031</strong></span>
</pre></div><p>This results in over 2 million local descriptors. Now, we use k-means clustering to obtain the centroids. We could use all the descriptors, but we are going to use a smaller sample for extra speed, as shown in the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; # use only every 64th vector</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; concatenated = concatenated[::64]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.cluster import KMeans</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; k = 256</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; km = KMeans(k)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; km.fit(concatenated)</strong></span>
</pre></div><p>After this is done (which will take a while), the <code class="literal">km</code> object contains information about the centroids. We now go back to the descriptors and build feature vectors as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; sfeatures = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for d in alldescriptors:</strong></span>
<span class="strong"><strong>...   c = km.predict(d)</strong></span>
<span class="strong"><strong>...   sfeatures.append(</strong></span>
<span class="strong"><strong>...       np.array([np.sum(c == ci) for ci in range(k)])</strong></span>
<span class="strong"><strong>...   )</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; # build single array and convert to float</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sfeatures = np.array(sfeatures, dtype=float)</strong></span>
</pre></div><p>The end result of this loop is that <code class="literal">sfeatures[fi, fj]</code> is the number of times that the image <code class="literal">fi</code> contains the element <code class="literal">fj</code>. The same could have been computed faster with the <code class="literal">np.histogram</code> function, but getting the arguments just right is a little tricky. We convert the result to floating point as we do not want integer arithmetic (with its rounding semantics).</p><p>The <a id="id552" class="indexterm"/>result is <a id="id553" class="indexterm"/>that each image is now represented by a single array of features, of the same size (the number of clusters, in our case 256). Therefore, we can use our standard classification methods as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; scores = cross_validation.cross_val_score(</strong></span>
<span class="strong"><strong>...    clf, sfeatures, labels, cv=cv)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Accuracy: {:.1%}'.format(scores.mean()))</strong></span>
<span class="strong"><strong>Accuracy: 62.6%</strong></span>
</pre></div><p>This is worse than before! Have we gained nothing?</p><p>In fact, we have, as we can combine all features together to obtain 76.1 percent accuracy, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; combined = np.hstack([features, features])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; scores = cross_validation.cross_val_score(</strong></span>
<span class="strong"><strong>...    clf, combined, labels, cv=cv)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print('Accuracy: {:.1%}'.format(scores.mean()))</strong></span>
<span class="strong"><strong>Accuracy: 76.1%</strong></span>
</pre></div><p>This is the best result we have, better than any single feature set. This is due to the fact that the local SURF features are different enough to add new information to the global image features we had before and improve the combined result.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec61"/>Summary</h1></div></div></div><p>We learned the classical feature-based approach to handling images in a machine learning context: by converting from a million pixels to a few numeric features, we are able to directly use a logistic regression classifier. All of the technologies that we learned in the other chapters suddenly become directly applicable to image problems. We saw one example in the use of image features to find similar images in a dataset.</p><p>We also learned how to use local features, in a bag of words model, for classification. This is a very modern approach to computer vision and achieves good results while being robust to many irrelevant aspects of the image, such as illumination, and even uneven illumination in the same image. We also used clustering as a useful intermediate step in classification rather than as an end in itself.</p><p>We focused on mahotas, which is one of the major computer vision libraries in Python. There are others that are equally well maintained. Skimage (scikit-image) is similar in spirit, but has a different set of features. OpenCV is a very good C++ library with a Python interface. All of these can work with NumPy arrays and you can mix and match functions from different libraries to build complex computer vision pipelines.</p><p>In the next chapter, you will learn a different form of machine learning: dimensionality reduction. As we saw in several earlier chapters, including when using images in this chapter, it is very easy to computationally generate many features. However, often we want to have a reduced number of features for speed and visualization, or to improve our results. In the next chapter, we will see how to achieve this.</p></div></div>
</body></html>
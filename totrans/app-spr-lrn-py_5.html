<html><head></head><body>
		<div id="_idContainer309" class="Content">
			<h1 id="_idParaDest-127"><em class="italics"><a id="_idTextAnchor141"/>Chapter 5</em></h1>
		</div>
		<div id="_idContainer310" class="Content">
			<h1 id="_idParaDest-128"><a id="_idTextAnchor142"/>Ensemble Modeling</h1>
		</div>
		<div id="_idContainer311" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of the chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Explain the concepts of bias and variance and how they lead to underfitting and overfitting</li>
				<li class="bullets">Explain the concepts behind bootstrapping</li>
				<li class="bullets">Implement a bagging classifier using decision trees</li>
				<li class="bullets">Implement adaptive boosting and gradient boosting models</li>
				<li class="bullets">Implement a stacked ensemble using a number of classifiers</li>
			</ul>
			<p>This chapter covers bias and variance, and underfitting and overfitting, and then introduces ensemble modeling.</p>
		</div>
		<div id="_idContainer330" class="Content">
			<h2 id="_idParaDest-129"><a id="_idTextAnchor143"/><a id="_idTextAnchor144"/>Introduction</h2>
			<p>In the previous chapters, we discussed the two types of supervised learning problems: regression and classification. We looked at a number of algorithms for each type and delved into how those algorithms worked.</p>
			<p>But there are times when these algorithms, no matter how complex they are, just don't seem to perform well on the data that we have. There could be a variety of causes and reasons – perhaps the data is not good enough, perhaps there really is no trend where we are trying to find one, or perhaps the model itself is too complex.</p>
			<p>Wait. What? How can a model being <em class="italics">too complex</em> be a problem? Oh, but it can! If a model is too complex and there isn't enough data, the model could fit so well to the data that it learns even the noise and outliers, which is never what we want.</p>
			<p>Oftentimes, where a single complex algorithm can give us a result that is way off, aggregating the results from a group of models can give us a result that's closer to the actual truth. This is because there is a high likelihood that the errors from all the individual models would cancel out when we take them all into account when making a prediction.</p>
			<p>This approach to grouping multiple algorithms to give an aggregated prediction is what <strong class="bold">ensemble modeling</strong> is based on. The ultimate goal of an ensemble method is to combine several underperforming <strong class="bold">base estimators</strong> (that is, individual algorithms) in such a way that the overall performance of the system improves and the <strong class="bold">ensemble</strong> of algorithms results in a model that is more robust and can generalize well compared to an individual algorithm.</p>
			<p>In this chapter, we'll discuss how building an ensemble model can help us build a robust system that makes accurate predictions without increasing variance. We will start by talking about some reasons a model may not perform well, and then move on to discussing the concepts of bias and variance, as well as overfitting and underfitting. We will introduce ensemble modeling as a solution for these performance issues and discuss different ensemble methods that could be used to overcome different types of problems when it comes to underperforming models.</p>
			<p>The chapter will discuss three types of ensemble methods. Namely, bagging, boosting, and stacking. Each of these will be discussed right from the basic theory to discussions on which use cases each type deals with well and which use cases each type might not be a good fit for. This chapter will also walk you through a number of exercises to implement the models using the scikit-learn library in Python.</p>
			<h3 id="_idParaDest-130"><a id="_idTextAnchor145"/>Exercise 43: Importing Modules and Preparing the Dataset</h3>
			<p>In this exercise, we'll import all the modules we will need for this chapter and get our dataset in shape for the exercises to come:</p>
			<ol>
				<li>Import all the modules required to manipulate the data and evaluate the model:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">%matplotlib inline</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">from sklearn.metrics import accuracy_score</p><p class="snippet">from sklearn.model_selection import KFold</p></li>
				<li>The dataset that we will use in this chapter is the Titanic dataset, which was introduced in the previous chapters as well. Read the dataset and print the first five rows:<p class="snippet">data = pd.read_csv('titanic.csv')</p><p class="snippet">data.head()</p><p>The output is as follows:</p><div id="_idContainer312" class="IMG---Figure"><img src="image/C12622_05_01.jpg" alt="Figure 5.1: The first five rows"/></div><h6>Figure 5.1: The first five rows</h6></li>
				<li>In order to make the dataset ready for use, we will add a <strong class="inline">preprocess</strong> function, which will preprocess the dataset to get it into a format that is ingestible by the scikit-learn library.<p>This chapter assumes that the dataset has already been preprocessed and is ready for use, but we will add a <strong class="inline">preprocess</strong> function, which will preprocess the dataset to get it into a format that is ingestible by the <strong class="inline">Scikit-learn</strong> library.</p><p>First, we create a <strong class="inline">fix_age</strong> function to preprocess the <strong class="inline">age</strong> function and get an integer value. If the age is null, the function returns a value of <em class="italics">-1</em> to differentiate it from available values, and if the value is a fraction less than <em class="italics">1</em>, multiply the age value by <em class="italics">100</em>. We then apply this function to the <strong class="inline">age</strong> column.</p><p>Then, we convert the <strong class="inline">Sex</strong> column into a binary variable with <em class="italics">1</em> for female and <em class="italics">0</em> for male values, and subsequently create dummy binary columns for the <strong class="inline">Embarked</strong> column using pandas' <strong class="inline">get_dummies</strong> function. Following this, we combine the DataFrame containing the dummy columns with the remaining numerical columns to create the final DataFrame, which is returned by the function:</p><p class="snippet">def preprocess(data):</p><p class="snippet">    def fix_age(age):</p><p class="snippet">        if np.isnan(age):</p><p class="snippet">            return -1</p><p class="snippet">        elif age &lt; 1:</p><p class="snippet">            return age*100</p><p class="snippet">        else:</p><p class="snippet">            return age</p><p class="snippet">    </p><p class="snippet">    data.loc[:, 'Age'] = data.Age.apply(fix_age)</p><p class="snippet">    data.loc[:, 'Sex'] = data.Sex.apply(lambda s: int(s == 'female'))</p><p class="snippet">        </p><p class="snippet">    embarked = pd.get_dummies(data.Embarked, prefix='Emb')[['Emb_C','Emb_Q','Emb_S']]</p><p class="snippet">    cols = ['Pclass','Sex','Age','SibSp','Parch','Fare']</p><p class="snippet">    </p><p class="snippet">    return pd.concat([data[cols], embarked], axis=1).values</p></li>
				<li>Split the dataset into training and validation sets.<p>We split the dataset into two parts – one on which we will train the models during the exercises (<strong class="inline">train</strong>), and another on which we will make predictions to evaluate the performance of each of those models (<strong class="inline">val</strong>). We will use the function we wrote in the previous step to preprocess the training and validation datasets separately.</p><p>Here, the <strong class="inline">Survived</strong> binary variable is the target variable that determines whether or not the individual in each row survived the sinking of the Titanic, so we create <strong class="inline">y_train</strong> and <strong class="inline">y_val</strong> as the dependent variable columns from both the splits:</p><p class="snippet">train, val = train_test_split(data, test_size=0.2, random_state=11)</p><p class="snippet">x_train = preprocess(train)</p><p class="snippet">y_train = train['Survived'].values</p><p class="snippet">x_val = preprocess(val)</p><p class="snippet">y_val = val['Survived'].values</p></li>
			</ol>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-131">O<a id="_idTextAnchor146"/>verfitting and Underfitting</h2>
			<p>Let's say we fit a supervised learning algorithm to our data and subsequently use the model to perform a prediction on a hold-out validation set. The performance of this model will be considered to be good based on how well it generalizes, that is, the predictions it makes for data points in an independent validation dataset.</p>
			<p>Sometimes we find that the model is not able to make accurate predictions and gives poor performance on the validation data. This poor performance can be<a id="_idTextAnchor147"/> the result of a model that is too simple to model the data appropriately, or a model that is too complex to generalize to the validation dataset. In the former case, the model has a <strong class="bold">high bias</strong> and results in <strong class="bold">underfitting</strong>, while in the latter case, the model has a <strong class="bold">high variance</strong> and results in <strong class="bold">overfitting</strong>.</p>
			<p><strong class="bold">Bias</strong></p>
			<p>The bias in the prediction of a machine learning model represents the difference between the predicted values and the true values. A model is said to have a high bias if the average predicted values are far off from the true values and is conversely said to have a low bias if the average predicted values are close to the true values.</p>
			<p>A high bias indicates that the model cannot capture the complexity in the data and is unable to identify the relevant relationships between the inputs and outputs.</p>
			<p><strong class="bold">Variance</strong></p>
			<p>The variance in prediction of a machine learning model represents how scattered the predicted values are compared to the true values. A model is said to have high variance if the predictions are scattered and unstable and is conversely said to have low variance if the predictions are consistent and not very scattered.</p>
			<p>A high variance indicates the model's inability to generalize and make accurate predictions on data points previously unseen by the model:</p>
			<div>
				<div id="_idContainer313" class="IMG---Figure">
					<img src="image/C12622_05_02.jpg" alt="Figure 5.2: Visual representation of data points having high and low bias and variance"/>
				</div>
			</div>
			<h6>Figure 5.2: Visual representation of data points having high and low bias and variance</h6>
			<h3 id="_idParaDest-132">Un<a id="_idTextAnchor148"/>derfitting</h3>
			<p>Let's say that we fit a simple model on the training dataset, one with a low model complexity, such as a simple linear model. We have fit a function that's able to represent the relationship between the X and Y data points in the training data to some extent, but we see that the training error is still high.</p>
			<div>
				<div id="_idContainer314" class="IMG---Figure">
					<img src="image/C12622_05_03.jpg" alt="Figure 5.3: Underfitting versus an ideal fit in regression"/>
				</div>
			</div>
			<h6>Figure 5.3: Underfitting versus an ideal fit in regression</h6>
			<p>For example, look at the two regression plots shown in <em class="italics">Figure 5.3</em>; while the first plot shows a model that fits a straight line to the data, the second plot shows a model that attempts to fit a relatively more complex polynomial to the data, one that seems to represent the mapping between X and Y quite well.</p>
			<p>We can say that the first model demonstrates underfitting, since it shows the characteristics of a high bias and low variance; that is, while it is unable to capture the complexity in the mapping between the inputs and outputs, it is consistent in its predictions. This model will have a high prediction error on both the training data and validation data.</p>
			<h3 id="_idParaDest-133">Ove<a id="_idTextAnchor149"/>rfitting</h3>
			<p>Let's say that we trained a highly complex model that is able to make predictions on the training dataset almost perfectly. We have managed to fit a function to represent the relationship between the X and Y data points in the training data such that prediction error on the training data is extremely low:</p>
			<div>
				<div id="_idContainer315" class="IMG---Figure">
					<img src="image/C12622_05_04.jpg" alt="Figure 5.4: An ideal fit versus overfitting in regression"/>
				</div>
			</div>
			<h6>Figure 5.4: An ideal fit versus overfitting in regression</h6>
			<p>Looking at the two plots in <em class="italics">Figure 5.4</em>, we can see that the second plot shows a model that attempts to fit a highly complex function to the data points, compared to the plot on the left, which represents the ideal fit for the given data.</p>
			<p>It is evident that, when we try to use the first model to predict the Y values for X data points that did not appear in the training set, we will see that the predictions are way off from the corresponding true values. This is a case of overfitting: the phenomenon where the model fits the data <em class="italics">too well</em> so that it is unable to generalize to new data points, since the model learns even the random noise and outliers in the training data.</p>
			<p>This model shows the characteristics of high variance and low bias: while the average predicted values would be close to the true values, they would be quite scattered compared to the true values.</p>
			<h3 id="_idParaDest-134">Over<a id="_idTextAnchor150"/>coming the Problem of Underfitting and Overfitting</h3>
			<p>From<a id="_idTextAnchor151"/> the previous sections, we can see that, as we move from an overly simplistic to an overly complex model, we go from having an underfitting model with high bias and low variance to an overfitting model with a low bias and high variance. The goal of any supervised machine learning algorithm is to achieve low bias and low variance and find that sweet spot between underfitting and overfitting. This will help the algorithm generalize well from the training data to validation data points as well, resulting in good prediction performance on data the model has never seen.</p>
			<p>The best way to improve performance when the model underfits the data is to increase the model complexity so as to identify the relevant relationships in the data. This can be done by adding new features, or by creating an ensemble of high-bias models. However, in this case, adding more data to train on would not help, as the constraining factor is model complexity and more data will not help to reduce the model's bias.</p>
			<p>Overfitting is, however, more difficult to tackle. Here are some common techniques used to overcome the problem posed by overfitting:</p>
			<ul>
				<li><strong class="bold">To get more data</strong>: A highly complex model can easily overfit to a small dataset but not be able to as easily on a larger dataset.</li>
				<li><strong class="bold">Dimensionality Reduction</strong>: Reducing the number of features can help make the model less complex.</li>
				<li><strong class="bold">Regularization</strong>: A new term is added to the cost function to adjust the coefficients (especially the high-degree coefficients in linear regression) toward a low value.</li>
				<li><strong class="bold">Ensemble modeling</strong>: Aggregating the predictions of several overfitting models can effectively eliminate high variance in prediction and perform better than individual models that overfit to the training data.</li>
			</ul>
			<p>We will talk in more detail about the nuances and considerations involved in the first three in <em class="italics">Chapter 6</em>, <em class="italics">Model Evaluation</em>; this chapter will focus on different ensemble modeling techniques. Some of the common types of ensembles are:</p>
			<ul>
				<li><strong class="bold">Bagging</strong>: A shorter term for <strong class="bold">bootstrap aggregation</strong>, this technique is also used to decrease the model's variance and avoid overfitting. It involves taking a subset of features and data points at a time, training a model on each subset, and subsequently aggregating the results from all the models into a final prediction.</li>
				<li><strong class="bold">Boosting</strong>: This technique is used to reduce bias rather than to reduce variance, and involves incrementally training new models that focus on the misclassified data points in the previous model.</li>
				<li><strong class="bold">Stacking</strong>: The aim of this technique is to increase the predictive power of the classifier, as it involves training multiple models and then using a combiner algorithm to make the final prediction by using the predictions from all these models additional inputs.</li>
			</ul>
			<p>Let's start with bagging, and then move on to boosting and stacking.</p>
			<h2 id="_idParaDest-135">Bagg<a id="_idTextAnchor152"/>ing</h2>
			<p>The term bagging is derived from a technique called bootstrap aggregation. In order to implement a successful predictive model, it's important to know in what situation we could benefit from using bootstrapping methods to build ensemble models. In this section, we'll talk about a way to use bootstrap methods to create an ensemble model that minimizes variance and look at how we can build an ensemble of decision trees, that is, the Random Forest algorithm. But what is bootstrapping and how does it help us build robust ensemble models?</p>
			<h3 id="_idParaDest-136">Boot<a id="_idTextAnchor153"/>strapping</h3>
			<p>The bootstrap method refers to random sampling with replacement, that is, drawing multiple samples (each known as a resample) from the dataset consisting of randomly chosen data points, where there can be an overlap in the data points contained in each resample and each data point has an equal probability of being selected from the overall dataset:</p>
			<div>
				<div id="_idContainer316" class="IMG---Figure">
					<img src="image/C12622_05_05.jpg" alt="Figure 5.5: Randomly choosing data points"/>
				</div>
			</div>
			<h6>Figure 5.5: Randomly choosing data points</h6>
			<p>From the previous diagram, we can see that each of the five bootstrapped samples taken from the primary dataset is different and has different characteristics. As such, training models on each of these resamples would result in different predictions.</p>
			<p>The following are the advantages of bootstrapping:</p>
			<ul>
				<li>Each resample can contain different characteristics from that of the entire dataset, allowing us a different perspective of how the data behaves.</li>
				<li>Algorithms that utilize bootstrapping can be more robust and handle unseen data better, especially on smaller datasets that have a tendency to cause overfitting.</li>
				<li>The bootstrap method can test the stability of a prediction by testing models using datasets with different variations and characteristics, resulting in a model that is more robust.</li>
			</ul>
			<h3 id="_idParaDest-137">Boots<a id="_idTextAnchor154"/>trap Aggregation</h3>
			<p>Now that we are aware of what bootstrapping is, what exactly does a bagging ensemble do? It is essentially an ensemble model that generates multiple versions of a predictor on each resample and uses these to get an aggregated predictor. The aggregation step gives us a <em class="italics">meta prediction</em>, which involves taking an average over the models when predicting a continuous numerical value for regression problems, and also involves taking a vote when predicting a class for classification problems.</p>
			<p>The following diagram gives us a visual representation of how a bagging estimator is built from the bootstrap sampling shown in <em class="italics">Figure 5.5</em>:</p>
			<div>
				<div id="_idContainer317" class="IMG---Figure">
					<img src="image/C12622_05_06.jpg" alt="Figure 5.6: Bagging estimator built from bootstrap sampling"/>
				</div>
			</div>
			<h6>Figure 5.6: Bagging estimator built from bootstrap sampling</h6>
			<p>Since each model is essentially independent of the others, all the base models can be trained in parallel, considerably speeding up the training process and allowing us to take advantage of the computational power we have on our hands today.</p>
			<p>Bagging essentially helps reduce the variance of the entire ensemble by introducing randomization into its construction procedure and is usually used with a base predictor that has a tendency to overfit the training data. The primary point of consideration here would be the stability (or lack thereof) of the training dataset: Bagging can improve accuracy in cases where a slight perturbation in the data could result in a significant change in the trained model.</p>
			<p>scikit-learn uses <strong class="inline">BaggingClassifier</strong> and <strong class="inline">BaggingRegressor</strong> to implement generic bagging ensembles for classification and regression tasks respectively. The primary inputs to these are the base estimators to use on each resample, along with the number of estimators to use (that is, the number of resamples).</p>
			<h3 id="_idParaDest-138">Exerci<a id="_idTextAnchor155"/>se 44: Using the Bagging Classifier</h3>
			<p>In this exercise, we will use scikit-learn's <strong class="inline">BaggingClassifier</strong> as our ensemble with <strong class="inline">DecisionTreeClassifier</strong> as the base estimator. We know that decision trees are prone to overfitting, and so will have a high variance and low bias, both being important characteristics for the base estimators to be used in bagging ensembles:</p>
			<ol>
				<li value="1">Import the base and ensemble classifiers:<p class="snippet">from sklearn.tree import DecisionTreeClassifier</p><p class="snippet">from sklearn.ensemble import BaggingClassifier</p></li>
				<li>Specify the hyperparameters and initialize the model.<p>Here, we will first specify the hyperparameters of the base estimator, for which we are using the decision tree classifier with the entropy or information gain as the splitting criterion. We will not specify any limits on the depth of the tree or size/number of leaves to each tree to grow fully. Following this, we will define the hyperparameters for the bagging classifier and pass the base estimator object to the classifier as a hyperparameter.</p><p>We will take 50 base estimators for our example, which will run in parallel and utilize all the processes available in the machine (which is done by specifying <strong class="inline">n_jobs=-1</strong>). Additionally, we will specify <strong class="inline">max_samples</strong> as 0.5, indicating that the number of datapoints in the bootstrap should be half that in the total dataset. We will also set a random state (to any arbitrary value, which will stay constant throughout) to maintain the reproducibility of the results:</p><p class="snippet">dt_params = {</p><p class="snippet">    'criterion': 'entropy',</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p><p class="snippet">dt = DecisionTreeClassifier(**dt_params)</p><p class="snippet">bc_params = {</p><p class="snippet">    'base_estimator': dt,</p><p class="snippet">    'n_estimators': 50,</p><p class="snippet">    'max_samples': 0.5,</p><p class="snippet">    'random_state': 11,</p><p class="snippet">    'n_jobs': -1</p><p class="snippet">}</p><p class="snippet">bc = BaggingClassifier(**bc_params)</p></li>
				<li>Fit the bagging classifier model to the training data and calculate the prediction accuracy.<p>Let's fit the bagging classifier and find the meta predictions for both the training and validation set. Following this, let's find the prediction accuracy on the training and validation datasets:</p><p class="snippet">bc.fit(x_train, y_train)</p><p class="snippet">bc_preds_train = bc.predict(x_train)</p><p class="snippet">bc_preds_val = bc.predict(x_val)</p><p class="snippet">print('Bagging Classifier:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on validation data = {:.4f}'.format(</p><p class="snippet">    accuracy_score(y_true=y_train, y_pred=bc_preds_train),</p><p class="snippet">    accuracy_score(y_true=y_val, y_pred=bc_preds_val)</p><p class="snippet">))</p><p>The output is as follows:</p><div id="_idContainer318" class="IMG---Figure"><img src="image/C12622_05_07.jpg" alt="Figure 5.7: Prediction accuracy of the bagging classifier"/></div><h6>Figure 5.7: Prediction accuracy of the bagging classifier</h6></li>
				<li>Fit the decision tree model to the training data to compare prediction accuracy.<p>Let's also fit the decision tree (from the object we initialized in <em class="italics">step two</em>) so that we will be able to compare the prediction accuracies of the ensemble with that of the base predictor:</p><p class="snippet">dt.fit(x_train, y_train)</p><p class="snippet">dt_preds_train = dt.predict(x_train)</p><p class="snippet">dt_preds_val = dt.predict(x_val)</p><p class="snippet">print('Decision Tree:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on validation data = {:.4f}'.format(</p><p class="snippet">    accuracy_score(y_true=y_train, y_pred=dt_preds_train),</p><p class="snippet">    accuracy_score(y_true=y_val, y_pred=dt_preds_val)</p><p class="snippet">))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer319" class="IMG---Figure">
					<img src="image/C12622_05_08.jpg" alt="Figure 5.8: Prediction accuracy of the decision tree"/>
				</div>
			</div>
			<h6>Figure 5.8: Prediction accuracy of the decision tree</h6>
			<p>Here, we can see that, although the decision tree has a much higher training accuracy than the bagging classifier, its accuracy on the validation dataset is lower, a clear signal that the decision tree is overfitting to the training data. The bagging ensemble, on the other hand, reduces the overall variance and results in a much more accurate prediction.</p>
			<h3 id="_idParaDest-139">Random F<a id="_idTextAnchor156"/>orest</h3>
			<p>An issue that is commonly faced with decision trees is that the split on each node is performed using a <strong class="bold">greedy</strong> algorithm that minimizes the entropy of the leaf nodes. Keeping this in mind, the base estimator decision trees in a bagging classifier can still be similar in terms of the features they split on, and so can have predictions that are quite similar. However, bagging is only useful in reducing the variance in predictions if the predictions from the base models are not correlated.</p>
			<p>The Random Forest algorithm attempts to overcome this problem by not only bootstrapping the data points in the overall training dataset, but also bootstrapping the features available for each tree to split on. This ensures that when the greedy algorithm is searching for the <em class="italics">best</em> feature to split on, the overall <em class="italics">best</em> feature may not always be available in the bootstrapped features for the base estimator and so would not be chosen – resulting in base trees that have different structures. This simple tweak lets the best estimators be trained in such a way that the predictions from each tree in the forest have a lower probability of being correlated to the predictions from other trees.</p>
			<p>Each base estimator in the Random Forest has a random sample of data points as well as a random sample of features. And since the ensemble is made up of decision trees, the algorithm is called a Random Forest.</p>
			<h3 id="_idParaDest-140">Exercise<a id="_idTextAnchor157"/> 45: Building the Ensemble Model Using Random Forest</h3>
			<p>The two primary parameters that Random Forest takes is the fraction of features and the fraction of data points to bootstrap to train each base decision tree on.</p>
			<p>In this exercise, we will use scikit-learn's <strong class="inline">RandomForestClassifier</strong> to build the ensemble model:</p>
			<ol>
				<li value="1">Import the ensemble classifier:<p class="snippet">from sklearn.ensemble import RandomForestClassifier</p></li>
				<li>Specify the hyperparameters and initialize the model.<p>Here, we will use entropy as the splitting criterion for the decision trees in a forest comprising 100 trees. As before, we will not specify any limits on the depth of the trees or size/number of leaves. Unlike the bagging classifier, which took <strong class="inline">max_samples</strong> as an input during initialization, the Random Forest algorithm takes in only <strong class="inline">max_features</strong>, indicating the number (or fraction) of features in the bootstrap sample. We will specify the value for this as 0.5, so that only three out of six features are considered for each tree:</p><p class="snippet">rf_params = {</p><p class="snippet">    'n_estimators': 100,</p><p class="snippet">    'criterion': 'entropy',</p><p class="snippet">    'max_features': 0.5,</p><p class="snippet">    'min_samples_leaf': 10,</p><p class="snippet">    'random_state': 11,</p><p class="snippet">    'n_jobs': -1</p><p class="snippet">}</p><p class="snippet">rf = RandomForestClassifier(**rf_params)</p></li>
				<li>Fit the Random Forest classifier model to the training data and calculate the prediction accuracy.<p>Let's fit the Random Forest model and find the meta predictions for both the training and validation set. Following this, let's find the prediction accuracy on the training and validation datasets:</p><p class="snippet">rf.fit(x_train, y_train)</p><p class="snippet">rf_preds_train = rf.predict(x_train)</p><p class="snippet">rf_preds_val = rf.predict(x_val)</p><p class="snippet">print('Random Forest:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on validation data = {:.4f}'.format(</p><p class="snippet">    accuracy_score(y_true=y_train, y_pred=rf_preds_train),</p><p class="snippet">    accuracy_score(y_true=y_val, y_pred=rf_preds_val)</p><p class="snippet">))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer320" class="IMG---Figure">
					<img src="image/C12622_05_09.jpg" alt="Figure 5.9: Accuracy on training and validation using Random Forest"/>
				</div>
			</div>
			<h6>Figure 5.9: Accuracy on training and validation using Random Forest</h6>
			<p>If we compare the prediction accuracies of Random Forest on our dataset to that of the bagging classifier, we can see that the accuracy on the validation set is pretty much the same, although the latter has higher accuracy on the training dataset.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor158"/>Boosting</h2>
			<p>The second ensemble technique we'll be looking at is <strong class="bold">boosting</strong>, which involves incrementally training new models that focus on the misclassified data points in the previous model and utilizes weighted averages to turn weak models (underfitting models having high bias) into stronger models. Unlike bagging, where each base estimator could be trained independently of the others, the training of each base estimator in a boosted algorithm depends on the previous one.</p>
			<p>Although boosting also uses the concept of bootstrapping, it's done differently from bagging, since each sample of data is weighted, implying that some bootstrapped samples can be used for training more often than other samples. When training each model, the algorithm keeps track of which features are most useful and which data samples have the most prediction error; these are given higher weightage and are considered to require more iterations to properly train the model.</p>
			<p>When predicting the output, the boosting ensemble takes a weighted average of the predictions from each base estimator, giving a higher weight to the ones that had lower errors during the training stage. This means that, for the data points that are misclassified by the model in an iteration, the weights for those data points are increased so that the next model is more likely to classify it correctly.</p>
			<p>As was the case with bagging, the results from all the boosting base estimators are aggregated to produce a meta prediction. However, unlike bagging, the accuracy of a boosted ensemble increases significantly with the number of base estimators in the boosted ensemble:</p>
			<div>
				<div id="_idContainer321" class="IMG---Figure">
					<img src="image/C12622_05_10.jpg" alt="Figure 5.10: A boosted ensemble"/>
				</div>
			</div>
			<h6>Figure 5.10: A boosted ensemble</h6>
			<p>In the diagram, we can see that, after each iteration, the misclassified points have increased weights (represented by larger icons) so that the next base estimator that is trained is able to focus on those points. The final predictor has aggregated the decision boundaries from each of its base estimators.</p>
			<h3 id="_idParaDest-142">Adaptive <a id="_idTextAnchor159"/>Boosting</h3>
			<p>Let's talk about a boosting technique called <strong class="bold">adaptive boosting</strong>, which is best used to boost the performance of decision stumps for binary classification problems. Decision stumps are essentially decision trees with a maximum depth of one (only one split is made on a single feature), and, as such, are weak learners. The primary principle that adaptive boosting works on is the same: to improve the areas where the base estimator fails to turn an ensemble of weak learners to a strong learner.</p>
			<p>To start with, the first base estimator takes a bootstrap of data points from the main training set and fits a decision stump to classify the sampled points, after which the trained decision tree stump is fit to the complete training data. For the samples that are misclassified, the weights are increased so that there is a higher probability of these data points being selected in the bootstrap for the next base estimator. A decision stump is again trained on the new bootstrap to classify the data points in the sample. Subsequently, the mini ensemble comprising the two base estimators is used to classify the data points in the entire training set. The misclassified data points from the second round are given a higher weight to improve their probability of selection, and so on until the ensemble reaches the limit on the number of base estimators it should contain.</p>
			<p>One drawback of adaptive boosting is that the algorithm is easily influenced by noisy data points and outliers since it tries to fit every point perfectly. As such, it is prone to overfitting if the number of estimators is very high.</p>
			<h3 id="_idParaDest-143">Exercise <a id="_idTextAnchor160"/>46: Adaptive Boosting</h3>
			<p>In this exercise, we'll use scikit-learn's implementation of adaptive boosting for classification, <strong class="inline">AdaBoostClassifier</strong>:</p>
			<ol>
				<li value="1">Import the classifier:<p class="snippet">from sklearn.ensemble import AdaBoostClassifier</p></li>
				<li>Specify the hyperparameters and initialize the model.<p>Here, we will first specify the hyperparameters of the base estimator, for which we are using the decision tree classifier with a maximum depth of one, that is, a decision stump. Following this, we will define the hyperparameters for the AdaBoost classifier and pass the base estimator object to the classifier as a hyperparameter:</p><p class="snippet">dt_params = {</p><p class="snippet">    'max_depth': 1,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p><p class="snippet">dt = DecisionTreeClassifier(**dt_params)</p><p class="snippet">ab_params = {</p><p class="snippet">    'n_estimators': 100,</p><p class="snippet">    'base_estimator': dt,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p><p class="snippet">ab = AdaBoostClassifier(**ab_params)</p></li>
				<li>Fit the model to the training data.<p>Let's fit the AdaBoost model and find the meta predictions for both the training and validation set. Following this, let's find the prediction accuracy on the training and validation datasets:</p><p class="snippet">ab.fit(x_train, y_train)</p><p class="snippet">ab_preds_train = ab.predict(x_train)</p><p class="snippet">ab_preds_val = ab.predict(x_val)</p><p class="snippet">print('Adaptive Boosting:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on validation data = {:.4f}'.format(</p><p class="snippet">    accuracy_score(y_true=y_train, y_pred=ab_preds_train),</p><p class="snippet">    accuracy_score(y_true=y_val, y_pred=ab_preds_val)</p><p class="snippet">))</p><p>The output is as follows:</p><div id="_idContainer322" class="IMG---Figure"><img src="image/C12622_05_11.jpg" alt="Figure 5.11: Accuracy of training and validation data using adaptive boosting"/></div><h6>Figure 5.11: Accuracy of training and validation data using adaptive boosting</h6></li>
				<li>Calculate the prediction accuracy of the model on the training and validation data for a varying number of base estimators.<p>Earlier, we claimed that the accuracy tends to increase with an increasing number of base estimators, but also that the model has a tendency to overfit if too many base estimators are used. Let's calculate the prediction accuracies so that we can find the point where the model begins to overfit the training data:</p><p class="snippet">ab_params = {</p><p class="snippet">    'base_estimator': dt,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p><p class="snippet">n_estimator_values = list(range(10, 210, 10))</p><p class="snippet">train_accuracies, val_accuracies = [], []</p><p class="snippet">for n_estimators in n_estimator_values:</p><p class="snippet">    ab = AdaBoostClassifier(n_estimators=n_estimators, **ab_params)</p><p class="snippet">    ab.fit(x_train, y_train)</p><p class="snippet">    ab_preds_train = ab.predict(x_train)</p><p class="snippet">    ab_preds_val = ab.predict(x_val)</p><p class="snippet">    </p><p class="snippet">    train_accuracies.append(accuracy_score(y_true=y_train, y_pred=ab_preds_train))</p><p class="snippet">    val_accuracies.append(accuracy_score(y_true=y_val, y_pred=ab_preds_val))</p></li>
				<li>Plot a line graph to visualize the trend of the prediction accuracies on both the training and validation datasets:<p class="snippet">plt.figure(figsize=(10,7))</p><p class="snippet">plt.plot(n_estimator_values, train_accuracies, label='Train')</p><p class="snippet">plt.plot(n_estimator_values, val_accuracies, label='Validation')</p><p class="snippet">plt.ylabel('Accuracy score')</p><p class="snippet">plt.xlabel('n_estimators')</p><p class="snippet">plt.legend()</p><p class="snippet">plt.show() </p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer323" class="IMG---Figure">
					<img src="image/C12622_05_12.jpg" alt="Figure 5.12: The trend of the prediction accuracies"/>
				</div>
			</div>
			<h6>Figure 5.12: The trend of the prediction accuracies</h6>
			<p>As was mentioned earlier, we can see that the training accuracy almost consistently increases as the number of decision tree stumps increases from 10 to 200. However, the validation accuracy fluctuates between 0.84 and 0.86 and begins to drop as the number of decision stumps goes higher. This happens because the AdaBoost algorithm is trying fit the noisy data points and outliers as well.</p>
			<h3 id="_idParaDest-144">Gradient Bo<a id="_idTextAnchor161"/>osting</h3>
			<p>Gradient boosting is an extension to the boosting method that visualizes boosting as an optimization problem. A loss function is defined that is representative of the error residuals (the difference between the predicted and true values), and the gradient descent algorithm is used to optimize the loss function.</p>
			<p>In the first step, a base estimator (which would be a weak learner) is added and trained on the entire training dataset. The loss associated with the prediction is calculated, and, in order to reduce the error residuals, the loss function is updated to add more base estimators for the data points where the existing estimators are performing poorly. Subsequently, the algorithm iteratively adds new base estimators and computes the loss to allow the optimization algorithm to update the model and minimize the residuals themselves.</p>
			<p>In the case of adaptive boosting, decision stumps were used as the weak learners for the base estimators. However, for gradient boosting methods, larger trees can be used, but the weak learners should still be constrained by providing a limit to the maximum number of layers, nodes, splits, or leaf nodes. This ensures that the base estimators are still weak learners, but they can be constructed in a greedy manner.</p>
			<p>From <em class="italics">Chapter 3</em>, <em class="italics">Regression Analysis</em>, we know that the gradient descent algorithm can be used to minimize a set of parameters, such as the coefficients in a regression equation. When building an ensemble, however, we have decision trees instead of parameters that need to be optimized. After calculating the loss at each step, the gradient descent algorithm then has to modify the parameters of the new tree that's to be added to the ensemble in such a way that reduces the loss. This approach is more commonly known as <strong class="bold">functional gradient descent.</strong></p>
			<h3 id="_idParaDest-145">Exercise 47<a id="_idTextAnchor162"/>: GradientBoostingClassifier</h3>
			<p>The two primary parameters that Random Forest takes is the fraction of features and the fraction of data points to bootstrap to train each base decision tree on.</p>
			<p>In this exercise, we will use scikit-learn's <strong class="inline">GradientBoostingClassifier</strong> to build the boosting ensemble model:</p>
			<ol>
				<li value="1">Import the ensemble classifier:<p class="snippet">from sklearn.ensemble import GradientBoostingClassifier</p></li>
				<li>Specify the hyperparameters and initialize the model.<p>Here, we will use 100 decision trees as the base estimator, with each tree having a maximum depth of three and a minimum of five samples in each of its leaves. Although we are not using decision stumps, as in the previous example, the tree is still small and would be considered a weak learner:</p><p class="snippet">gbc_params = {</p><p class="snippet">    'n_estimators': 100,</p><p class="snippet">    'max_depth': 3,</p><p class="snippet">    'min_samples_leaf': 5,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p><p class="snippet">gbc = GradientBoostingClassifier(**gbc_params)</p></li>
				<li>Fit the gradient boosting model to the training data and calculate the prediction accuracy.<p>Let's fit the ensemble model and find the meta predictions for both the training and validation set. Following this, we will find the prediction accuracy on the training and validation datasets:</p><p class="snippet">gbc.fit(x_train, y_train)</p><p class="snippet">gbc_preds_train = gbc.predict(x_train)</p><p class="snippet">gbc_preds_val = gbc.predict(x_val)</p><p class="snippet">print('Gradient Boosting Classifier:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on validation data = {:.4f}'.format(</p><p class="snippet">    accuracy_score(y_true=y_train, y_pred=gbc_preds_train),</p><p class="snippet">    accuracy_score(y_true=y_val, y_pred=gbc_preds_val)</p><p class="snippet">))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer324" class="IMG---Figure">
					<img src="image/C12622_05_13.jpg" alt="Figure 5.13: Prediction accuracy on the training and validation datasets"/>
				</div>
			</div>
			<h6>Figure 5.13: Prediction accuracy on the training and validation datasets</h6>
			<p>We can see that the gradient boosting ensemble has greater accuracy on both the training and validation datasets compared to those for the adaptive boosting ensemble.</p>
			<h3 id="_idParaDest-146"><a id="_idTextAnchor163"/>Stacking</h3>
			<p>Stacking, or stacked generalization (also called meta ensembling), is a model ensembling technique that involves combining information from multiple predictive models and using them as features to generate a new model. The stacked model will most likely outperform each of the individual models due the smoothing effect it adds, as well as due to its ability to "choose" the base model that performs best in certain scenarios. Keeping this in mind, stacking is usually most effective when each of the base models is significantly different from each other.</p>
			<p>Stacking uses the predictions of the base models as additional features when training the final model – these are known as <strong class="bold">meta features</strong>. The stacked model essentially acts as a classifier that determines where each model is performing well and where it is performing poorly.</p>
			<p>However, you cannot simply train the base models on the full training data, generate predictions on the full validation dataset, and then output these for second-level training. This runs the risk of your base model predictions already having "seen" the test set and therefore overfitting when feeding these predictions.</p>
			<p>It is important to note that the value of the meta features for each row cannot be predicted using a model that contained that row in the training data, as we then run the risk of overfitting since the base predictions would have already "seen" the target variable for that row. The common practice is to divide the training data into <em class="italics">k</em> subsets so that, when finding the meta features for each of those subsets, we only train the model on the remaining data. Doing this also avoids the problem of overfitting the data the model has already "seen":</p>
			<div>
				<div id="_idContainer325" class="IMG---Figure">
					<img src="image/C12622_05_14.jpg" alt="Figure 5.14: A stacking ensemble"/>
				</div>
			</div>
			<h6>Figure 5.14: A stacking ensemble</h6>
			<p>The preceding diagram shows how this is done: we divide the training data into <em class="italics">k</em> folds and find the predictions from the base models on each fold by training the model on the remaining <em class="italics">k-1</em> folds. So, once we have the meta predictions for each of the folds, we can use those meta predictions along with the original features to train the stacked model.</p>
			<h3 id="_idParaDest-147">Exercise 48:<a id="_idTextAnchor164"/> Building a Stacked Model</h3>
			<p>In this exercise, we will use a support vector machine (scikit-learn's <strong class="inline">LinearSVC</strong>) and k-nearest neighbors (scikit-learn's <strong class="inline">KNeighborsClassifier</strong>) as the base predictors, and the stacked model will be a logistic regression classifier.</p>
			<ol>
				<li value="1">Import the base models and the model used for stacking:<p class="snippet"># Base models</p><p class="snippet">from sklearn.neighbors import KNeighborsClassifier</p><p class="snippet">from sklearn.svm import LinearSVC</p><p class="snippet"># Stacking model</p><p class="snippet">from sklearn.linear_model import LogisticRegression</p></li>
				<li>Create a new training set with additional columns for predictions from base predictors.<p>We need to create two new columns for predicted values from each model to be used as features for the ensemble model in both the test and train set. Since NumPy arrays are immutable, we will create a new array that will have the same number of rows as the training dataset, and two columns more than those in the training dataset. Once the dataset is created, let's print it to see what it looks like:</p><p class="snippet">x_train_with_metapreds = np.zeros((x_train.shape[0], x_train.shape[1]+2))</p><p class="snippet">x_train_with_metapreds[:, :-2] = x_train</p><p class="snippet">x_train_with_metapreds[:, -2:] = -1</p><p class="snippet">print(x_train_with_metapreds)</p><p>The output is as follows:</p><div id="_idContainer326" class="IMG---Figure"><img src="image/C12622_05_15.jpg" alt="Figure 5.15: The new columns for the predicted values"/></div><h6>Figure 5.15: The new columns for the predicted values</h6><p>As we can see, there are two extra columns filled with <em class="italics">-1</em> values at the end of each row.</p></li>
				<li>Train base models using the k-fold strategy.<p>Let's take <em class="italics">k=5</em>. For each of the five folds, train on the other four folds and predict on the fifth fold. These predictions should then be added into the placeholder columns for base predictions in the new NumPy array.</p><p>First, we initialize the <strong class="inline">KFold</strong> object with the value of <strong class="inline">k</strong> and a random state to maintain reproducibility. The <strong class="inline">kf.split()</strong> function takes the dataset to split as an input and returns an iterator, each element in the iterator corresponding to the list of indices in the training and validation folds respectively. These index values in each loop over the iterator can be used to subdivide the training data for training and prediction for each row.</p><p>Once the data is adequately divided, we train the two base predictors on four-fifths of the data and predict the values on the remaining one-fifth of the rows. These predictions are then inserted into the two placeholder columns we initialized with <strong class="inline">-1</strong> in <em class="italics">step 2</em>:</p><p class="snippet">kf = KFold(n_splits=5, random_state=11)</p><p class="snippet">for train_indices, val_indices in kf.split(x_train):</p><p class="snippet">    kfold_x_train, kfold_x_val = x_train[train_indices], x_train[val_indices]</p><p class="snippet">    kfold_y_train, kfold_y_val = y_train[train_indices], y_train[val_indices]</p><p class="snippet">   </p><p class="snippet">    svm = LinearSVC(random_state=11, max_iter=1000)</p><p class="snippet">    svm.fit(kfold_x_train, kfold_y_train)</p><p class="snippet">    svm_pred = svm.predict(kfold_x_val)</p><p class="snippet">    </p><p class="snippet">    knn = KNeighborsClassifier(n_neighbors=4)</p><p class="snippet">    knn.fit(kfold_x_train, kfold_y_train)</p><p class="snippet">    knn_pred = knn.predict(kfold_x_val)</p><p class="snippet">    </p><p class="snippet">    x_train_with_metapreds[val_indices, -2] = svm_pred</p><p class="snippet">    x_train_with_metapreds[val_indices, -1] = knn_pred</p></li>
				<li>Create a new validation set with additional columns for predictions from base predictors.<p>As we did in <em class="italics">step 2</em>, we will add two placeholder columns for the base model predictions in the validation dataset as well:</p><p class="snippet">x_val_with_metapreds = np.zeros((x_val.shape[0], x_val.shape[1]+2))</p><p class="snippet">x_val_with_metapreds[:, :-2] = x_val</p><p class="snippet">x_val_with_metapreds[:, -2:] = -1</p><p class="snippet">print(x_val_with_metapreds)</p><p>The output is as follows:</p><div id="_idContainer327" class="IMG---Figure"><img src="image/C12622_05_16.jpg" alt="Figure 5.16: Additional columns for predictions from base predictors"/></div><h6>Figure 5.16: Additional columns for predictions from base predictors</h6></li>
				<li>Fit base models on the complete training set to get meta features for the validation set.<p>Next, we will train the two base predictors on the complete training dataset to get the meta prediction values for the validation dataset. This is similar to what we did for each fold in <em class="italics">step 3</em>:</p><p class="snippet">svm = LinearSVC(random_state=11, max_iter=1000)</p><p class="snippet">svm.fit(x_train, y_train)</p><p class="snippet">knn = KNeighborsClassifier(n_neighbors=4)</p><p class="snippet">knn.fit(x_train, y_train)</p><p class="snippet">svm_pred = svm.predict(x_val)</p><p class="snippet">knn_pred = knn.predict(x_val)</p><p class="snippet">x_val_with_metapreds[:, -2] = svm_pred</p><p class="snippet">x_val_with_metapreds[:, -1] = knn_pred</p></li>
				<li>Train the stacked model and use the final predictions to calculate accuracy.<p>The last step is to train the logistic regression model on all the columns of the training dataset plus the meta predictions from the base estimators. We use the model to find the prediction accuracies for both the training and validation datasets:</p><p class="snippet">lr = LogisticRegression(random_state=11)</p><p class="snippet">lr.fit(x_train_with_metapreds, y_train)</p><p class="snippet">lr_preds_train = lr.predict(x_train_with_metapreds)</p><p class="snippet">lr_preds_val = lr.predict(x_val_with_metapreds)</p><p class="snippet">print('Stacked Classifier:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on validation data = {:.4f}'.format(</p><p class="snippet">    accuracy_score(y_true=y_train, y_pred=lr_preds_train),</p><p class="snippet">    accuracy_score(y_true=y_val, y_pred=lr_preds_val)</p><p class="snippet">))</p><p>The output is as follows:</p><div id="_idContainer328" class="IMG---Figure"><img src="image/C12622_05_17.jpg" alt="Figure 5.17: Accuracy using a stacked classifier"/></div><h6>Figure 5.17: Accuracy using a stacked classifier</h6></li>
				<li>Compare the accuracy with that of base models.<p>To get a sense of the performance boost from stacking, we calculate the accuracies of the base predictors on the training and validation datasets and compare it to that of the stacked model:</p><p class="snippet">print('SVM:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on validation data = {:.4f}'.format(</p><p class="snippet">    accuracy_score(y_true=y_train, y_pred=svm.predict(x_train)),</p><p class="snippet">    accuracy_score(y_true=y_val, y_pred=svm_pred)</p><p class="snippet">))</p><p class="snippet">print('kNN:\n&gt; Accuracy on training data = {:.4f}\n&gt; Accuracy on validation data = {:.4f}'.format(</p><p class="snippet">    accuracy_score(y_true=y_train, y_pred=knn.predict(x_train)),</p><p class="snippet">    accuracy_score(y_true=y_val, y_pred=knn_pred)</p><p class="snippet">))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer329" class="IMG---Figure">
					<img src="image/C12622_05_18.jpg" alt="Figure 5.18: Accuracy of training and validation data using SVM and K-NN"/>
				</div>
			</div>
			<h6>Figure 5.18: Accuracy of training and validation data using SVM and K-NN</h6>
			<p>As we can see, not only does the stacked model give us a validation accuracy that is significantly higher than either of the base predictors, but it also has the highest accuracy, of nearly 89%, of all the ensemble models discussed in this chapter.</p>
			<h3 id="_idParaDest-148">Activity 14: Sta<a id="_idTextAnchor165"/>cking with Standalone and Ensemble Algorithms</h3>
			<p>In this activity, we'll use the <em class="italics">Kaggle House Prices: Advanced Regression Techniques database </em>(available at <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a> or on GitHub at <a href="https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python">https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python</a>), that we did EDA on in <em class="italics">Chapter 2</em>, <em class="italics">Exploratory Data Analysis and Visualization</em>. This dataset is aimed toward solving a regression problem (that is, the target variable takes on a range of continuous values). In this activity, we will use decision trees, K-nearest neighbors, Random Forest, and gradient boosting algorithms to train individual regressors on the data. Then, we will build a stacked linear regression model that uses all these algorithms and compare the performance of each. We will use the <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>) as the evaluation metric for this activity.</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Import the relevant libraries.</li>
				<li>Read the data.</li>
				<li>Preprocess the dataset to remove null values and one-hot encode categorical variables to prepare the data for modeling.</li>
				<li>Divide the dataset into train and validation DataFrames.</li>
				<li>Initialize dictionaries in which to store the train and validation MAE values.</li>
				<li>Train a decision tree model with the following hyperparameters and save the scores:<p class="snippet">dt_params = {</p><p class="snippet">    'criterion': 'mae',</p><p class="snippet">    'min_samples_leaf': 10,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p></li>
				<li>Train a k-nearest neighbors model with the following hyperparameters and save the scores:<p class="snippet">knn_params = {</p><p class="snippet">    'n_neighbors': 5</p><p class="snippet">}</p></li>
				<li>Train a Random Forest model with the following hyperparameters and save the scores:<p class="snippet">rf_params = {</p><p class="snippet">    'n_estimators': 50,</p><p class="snippet">    'criterion': 'mae',</p><p class="snippet">    'max_features': 'sqrt',</p><p class="snippet">    'min_samples_leaf': 10,</p><p class="snippet">    'random_state': 11,</p><p class="snippet">    'n_jobs': -1</p><p class="snippet">}</p></li>
				<li>Train a gradient boosting model with the following hyperparameters and save the scores:<p class="snippet">gbr_params = {</p><p class="snippet">    'n_estimators': 50,</p><p class="snippet">    'criterion': 'mae',</p><p class="snippet">    'max_features': 'sqrt',</p><p class="snippet">    'min_samples_leaf': 10,</p><p class="snippet">    'random_state': 11</p><p class="snippet">}</p></li>
				<li>Prepare the training and validation datasets, with the four meta estimators having the same hyperparameters that were used in the previous steps.</li>
				<li>Train a linear regression model as the stacked model.</li>
				<li>Visualize the train and validation errors for each individual model and the stacked model.<h4>Note</h4><p class="callout">The solut<a id="_idTextAnchor166"/>ion for this activity can be found on page 364.</p></li>
			</ol>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor167"/>Summary</h2>
			<p>In this chapter, we started off with a discussion on overfitting and underfitting and how these can affect the performance of a model on unseen data. The chapter looked at ensemble modeling as a solution for these and went on to discuss different ensemble methods that could be used, and how they could decrease the overall bias or variance encountered when making predictions.</p>
			<p>We first discussed bagging algorithms and introduced the concept of bootstrapping. Then, we looked at Random Forest as a classic example of a Bagged ensemble and solved exercises that involved building a bagging classifier and Random Forest classifier on the previously seen Titanic dataset.</p>
			<p>We then moved on to discussing boosting algorithms, how they successfully reduce bias in the system, and gained an understanding of how to implement adaptive boosting and gradient boosting. The last ensemble method we discussed was stacking, which, as we saw from the exercise, gave us the best accuracy score of all the ensemble methods we implemented.</p>
			<p>Although building an ensemble model is a great way to decrease bias and variance, and they generally outperform any single model by itself, they themselves come with their own problems and use cases. While bagging is great when trying to avoid overfitting, boosting can reduce both bias and variance, though it may still have a tendency to overfit. Stacking, on the other hand, is a good choice for when one model performs well on a portion of the data while another model performs better on another portion of the data.</p>
			<p>In the next chapter, we will explore more ways to overcome the problems of overfitting and underfitting in detail by looking at validation techniques, that is, ways to judge our model's performance, and how to use different metrics as indicators to build the best possible model for our use case.</p>
		</div>
	</body></html>
["```py\n    %%sh\n    aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Camera_v1_00.tsv.gz /tmp\n    ```", "```py\n    data = pd.read_csv(\n        '/tmp/amazon_reviews_us_Camera_v1_00.tsv.gz', \n        sep='\\t', compression='gzip', \n        error_bad_lines=False, dtype='str')\n    data.dropna(inplace=True)\n    ```", "```py\n    print(data.shape)\n    print(data.columns)\n    ```", "```py\n    (1800755, 15)\n    Index(['marketplace','customer_id','review_id','product_id','product_parent', 'product_title','product_category',  'star_rating','helpful_votes','total_votes','vine', 'verified_purchase','review_headline','review_body', \n    'review_date'], dtype='object')\n    ```", "```py\n    data = data[:100000]\n    data = data[['star_rating', 'review_body']]\n    ```", "```py\n    data['label'] = data.star_rating.map({\n        '1': '__label__negative__',\n        '2': '__label__negative__',\n        '3': '__label__neutral__',\n        '4': '__label__positive__',\n        '5': '__label__positive__'})\n    data = data.drop(['star_rating'], axis=1)\n    ```", "```py\n    data = data[['label', 'review_body']]\n    ```", "```py\n    !pip -q install nltk\n    import nltk\n    nltk.download('punkt')\n    data['review_body'] = data['review_body'].apply(nltk.word_tokenize)\n    ```", "```py\n    data['review_body'] = \n        data.apply(lambda row: \"\".join(row['review_body'])\n            .lower(), axis=1)\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    training, validation = train_test_split(data, test_size=0.05)\n    np.savetxt('/tmp/training.txt', training.values, fmt='%s')\n    np.savetxt('/tmp/validation.txt', validation.values, fmt='%s')\n    ```", "```py\n    __label__neutral__ really works for me , especially on the streets of europe . wished it was less expensive though . the rain cover at the base really works . the padding which comes in contact with your back though will suffocate & make your back sweaty .\n    ```", "```py\n    import sagemaker\n    session = sagemaker.Session()\n    prefix = 'amazon-reviews-camera'\n    input_data = session.upload_data(\n        path='/tmp/amazon_reviews_us_Camera_v1_00.tsv.gz', \n        key_prefix=prefix)\n    ```", "```py\n    from sagemaker.sklearn.processing import SKLearnProcessor\n    sklearn_processor = SKLearnProcessor(\n       framework_version='0.23-1',\n       role= sagemaker.get_execution_role(),\n       instance_type='ml.c5.2xlarge',\n       instance_count=1)\n    ```", "```py\n    from sagemaker.processing import ProcessingInput, ProcessingOutput\n    sklearn_processor.run(\n        code='preprocessing.py',\n        inputs=[\n            ProcessingInput(\n                source=input_data,\n                destination='/opt/ml/processing/input')\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name='train_data',\n                source='/opt/ml/processing/train'),\n            ProcessingOutput(\n                output_name='validation_data',\n                source='/opt/ml/processing/validation')\n        ],\n        arguments=[\n            '--filename', 'amazon_reviews_us_Camera_v1_00.tsv.gz',\n            '--num-reviews', '100000',\n            '--split-ratio', '0.05'\n        ]\n    )\n    ```", "```py\n    import argparse, os, subprocess, sys\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    def install(package):\n        subprocess.call([sys.executable, \"-m\", \"pip\",  \n                         \"install\", package]) \n    if __name__=='__main__':\n        install('nltk')\n        import nltk\n    ```", "```py\n        parser = argparse.ArgumentParser()\n        parser.add_argument('--filename', type=str)\n        parser.add_argument('--num-reviews', type=int)\n        parser.add_argument('--split-ratio', type=float, \n                            default=0.1)\n        args, _ = parser.parse_known_args()\n        filename = args.filename\n        num_reviews = args.num_reviews\n        split_ratio = args.split_ratio\n    ```", "```py\n        input_data_path = \n        os.path.join('/opt/ml/processing/input', filename)\n        data = pd.read_csv(input_data_path, sep='\\t', \n               compression='gzip', error_bad_lines=False,\n               dtype='str')\n        # Process data\n        . . . \n    ```", "```py\n        training, validation = train_test_split(\n                               data, \n                               test_size=split_ratio)\n        training_output_path = os.path.join('\n        /opt/ml/processing/train', 'training.txt')    \n        validation_output_path = os.path.join(\n        '/opt/ml/processing/validation', 'validation.txt')\n        np.savetxt(training_output_path, \n        training.values, fmt='%s')\n        np.savetxt(validation_output_path, \n        validation.values, fmt='%s')\n    ```", "```py\n    %%sh\n    pip -q install spacy\n    python -m spacy download en_core_web_sm\n    python -m spacy validate\n    ```", "```py\n    data = pd.read_csv(\n        '/tmp/amazon_reviews_us_Camera_v1_00.tsv.gz', \n        sep='\\t', compression='gzip', \n        error_bad_lines=False, dtype='str')\n    data.dropna(inplace=True)\n    ```", "```py\n    data = data[:100000]\n    data = data[['review_body']]\n    ```", "```py\n    import spacy\n    spacy_nlp = spacy.load('en_core_web_sm')\n    def tokenize(text):\n        tokens = spacy_nlp.tokenizer(text)\n        tokens = [ t.text for t in tokens ]\n        return \" \".join(tokens).lower()\n    data['review_body'] = \n        data['review_body'].apply(tokenize)\n    ```", "```py\n    import numpy as np\n    np.savetxt('/tmp/training.txt', data.values, fmt='%s')\n    ```", "```py\n    Ok\n    perfect , even sturdier than the original !\n    ```", "```py\n    %%sh\n    pip -q install nltk gensim\n    ```", "```py\n    num_lines = 1000000\n    data = pd.read_csv('abcnews-date-text.csv.gz', \n                       compression='gzip', error_bad_lines=False, \n                       dtype='str', nrows=num_lines)\n    ```", "```py\n    data = data.sample(frac=1)\n    data = data.drop(['publish_date'], axis=1)\n    ```", "```py\n    import string\n    import nltk\n    from nltk.corpus import stopwords\n    #from nltk.stem.snowball import SnowballStemmer\n    from nltk.stem import WordNetLemmatizer \n    nltk.download('stopwords')\n    stop_words = stopwords.words('english')\n    #stemmer = SnowballStemmer(\"english\")\n    wnl = WordNetLemmatizer()\n    def process_text(text):\n        for p in string.punctuation:\n            text = text.replace(p, '')\n            text = ''.join([c for c in text if not \n                            c.isdigit()])\n            text = text.lower().split()\n            text = [w for w in text if not w in \n                    stop_words]\n            #text = [stemmer.stem(w) for w in text]\n            text = [wnl.lemmatize(w) for w in text]\n            return text\n    data['headline_text'] = \n        data['headline_text'].apply(process_text)\n    ```", "```py\n    from gensim import corpora\n    dictionary = corpora.Dictionary(data['headline_text'])\n    print(dictionary)\n    ```", "```py\n    Dictionary(83131 unique tokens: ['aba', 'broadcasting', 'community', 'decides', 'licence']...)\n    ```", "```py\n    dictionary.filter_extremes(keep_n=512)\n    ```", "```py\n    with open('vocab.txt', 'w') as f:\n        for index in range(0,len(dictionary)):\n            f.write(dictionary.get(index)+'\\n')\n    ```", "```py\n    data['tokens'] = data.apply(lambda row: dictionary.doc2bow(row['headline_text']), axis=1)\n    data = data.drop(['headline_text'], axis=1)\n    ```", "```py\n    from scipy.sparse import lil_matrix\n    num_lines = data.shape[0]\n    num_columns = len(dictionary)\n    token_matrix = lil_matrix((num_lines,num_columns))\n                   .astype('float32')\n    ```", "```py\n    def add_row_to_matrix(line, row):\n        for token_id, token_count in row['tokens']:\n            token_matrix[line, token_id] = token_count\n        return\n    ```", "```py\n    line = 0\n    for _, row in data.iterrows():\n        add_row_to_matrix(line, row)\n        line+=1\n    ```", "```py\n    import io, boto3\n    import sagemaker\n    import sagemaker.amazon.common as smac\n    buf = io.BytesIO()\n    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n    buf.seek(0)\n    bucket = sagemaker.Session().default_bucket()\n    prefix = 'headlines-lda-ntm'\n    train_key = 'reviews.protobuf'\n    obj = '{}/{}'.format(prefix, train_key))\n    s3 = boto3.resource('s3')\n    s3.Bucket(bucket).Object(obj).upload_fileobj(buf)\n    s3_train_path = 's3://{}/{}'.format(bucket,obj)\n    ```", "```py\n    $ aws s3 ls s3://sagemaker-eu-west-1-123456789012/amazon-reviews-ntm/training.protobuf\n    43884300 training.protobuf\n    ```", "```py\n{\"source\":\"With great power come great responsibility. The second you create AWS resources, you're responsible for them: security of course, but also cost and scaling. This makes monitoring and alerting all the more important, which is why we built services like Amazon CloudWatch, AWS Config and AWS Systems Manager.\",\"my-text-classification-job\":0,\"my-text-classification-job-metadata\":{\"confidence\":0.84,\"job-name\":\"labeling-job/my-text-classification-job\",\"class-name\":\"aws_service\",\"human-annotated\":\"yes\",\"creation-date\":\"2020-05-11T12:44:50.620065\",\"type\":\"groundtruth/text-classification\"}}\n```", "```py\n    import pandas as pd\n    bucket = 'sagemaker-book'\n    prefix = 'chapter2/classif/output/my-text-classification-job/manifests/output'\n    manifest = 's3://{}/{}/output.manifest'.format(bucket, prefix)\n    data = pd.read_json(manifest, lines=True)\n    ```", "```py\n    def get_label(metadata):\n        return metadata['class-name']\n    data['label'] = \n    data['my-text-classification-job-metadata'].apply(get_label)\n    data = data[['label', 'source']]\n    ```", "```py\n    import sagemaker\n    session = sagemaker.Session()\n    bucket = session.default_bucket()\n    prefix = 'amazon-reviews'\n    s3_train_path = session.upload_data(path='/tmp/training.txt', bucket=bucket, key_prefix=prefix+'/input/train')\n    s3_val_path = session.upload_data(\n        path='/tmp/validation.txt', bucket=bucket,   \n        key_prefix=prefix+'/input/validation')\n    s3_output = 's3://{}/{}/output/'.format(bucket, \n        prefix)\n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    region_name = session.boto_session.region_name\n    container = retrieve('blazingtext', region)\n    bt = sagemaker.estimator.Estimator(container, \n         sagemaker.get_execution_role(),\n         instance_count=1,\n         instance_type='ml.p3.2xlarge',\n         output_path=s3_output)\n    ```", "```py\n    bt.set_hyperparameters(mode='supervised')\n    ```", "```py\n    from sagemaker import TrainingInput\n    train_data = TrainingInput(\n        s3_train_path, content_type='text/plain')\n    validation_data = TrainingInput(\n        s3_val_path, content_type='text/plain')\n    s3_channels = {'train': train_data, \n                   'validation': validation_data}\n    bt.fit(inputs=s3_channels)\n    ```", "```py\n    bt_predictor = bt.deploy(initial_instance_count=1, \n                             instance_type='ml.t2.medium')\n    ```", "```py\n    import json\n    sentences = ['This is a bad camera it doesnt work at all , i want a refund  . ' , 'The camera works , the pictures are decent quality, nothing special to say about it . ' , 'Very happy to have bought this , exactly what I needed . ']\n    payload = {\"instances\":sentences, \n               \"configuration\":{\"k\": 3}}\n    bt_predictor.serializer =  \n        sagemaker.serializers.JSONSerializer()\n    response = bt_predictor.predict(json.dumps(payload))\n    ```", "```py\n    [{'prob': [0.9758228063583374, 0.023583529517054558, 0.0006236258195713162], 'label': ['__label__negative__', '__label__neutral__', '__label__positive__']}, \n    {'prob': [0.5177792906761169, 0.2864232063293457, 0.19582746922969818], 'label': ['__label__neutral__', '__label__positive__', '__label__negative__']}, \n    {'prob': [0.9997835755348206, 0.000205090589588508, 4.133415131946094e-05], 'label': ['__label__positive__', '__label__neutral__', '__label__negative__']}]\n    ```", "```py\n    bt_predictor.delete_endpoint()\n    ```", "```py\nbt.set_hyperparameters(mode='skipgram', subwords=True)\n```", "```py\n$ git clone https://github.com/facebookresearch/fastText.git\n$ cd fastText\n$ make\n```", "```py\n    $ aws s3 cp s3://sagemaker-eu-west-1-123456789012/amazon-reviews/output/JOB_NAME/output/model.tar.gz .\n    $ tar xvfz model.tar.gz\n    ```", "```py\n    $ ./fasttext predict model.bin -\n    ```", "```py\n    This is a bad camera it doesnt work at all , i want a refund  .\n    __label__negative__\n    The camera works , the pictures are decent quality, nothing\n    special to say about it .\n    __label__neutral__\n    Very happy to have bought this , exactly what I needed\n    __label__positive__\n    ```", "```py\n    $ aws s3 cp s3://sagemaker-eu-west-1-123456789012/amazon-reviews-word2vec/output/JOB_NAME/output/model.tar.gz .\n    $ tar xvfz model.tar.gz\n    ```", "```py\n    $ ./fasttext nn vectors.bin\n    Query word? Telephoto\n    telephotos 0.951023\n    75-300mm 0.79659\n    55-300mm 0.788019\n    18-300mm 0.782396\n    . . .\n    ```", "```py\n    $ ./fasttext analogies vectors.bin\n    Query triplet (A - B + C)? nikon d3300 canon\n    xsi 0.748873\n    700d 0.744358\n    100d 0.735871\n    ```", "```py\n    import sagemaker\n    session = sagemaker.Session()\n    bucket = session.default_bucket()\n    prefix = reviews-lda-ntm'\n    train_key = 'reviews.protobuf'\n    obj = '{}/{}'.format(prefix, train_key)\n    s3_train_path = 's3://{}/{}'.format(bucket,obj)\n    s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    region_name = session.boto_session.region_name\n    container = retrieve('lda', region)\n    lda = sagemaker.estimator.Estimator(container, \n          role = sagemaker.get_execution_role(), \n          instance_count=1,                                \n          instance_type='ml.c5.2xlarge', \n          output_path=s3_output)\n    ```", "```py\n    lda.set_hyperparameters(num_topics=5,\n       feature_dim=len(dictionary),\n       mini_batch_size=num_lines,\n       alpha0=0.1)\n    ```", "```py\n    lda.fit(inputs={'train': s3_train_path})\n    ```", "```py\n    lda_predictor = lda.deploy(\n        initial_instance_count=1,    \n        instance_type='ml.t2.medium')\n    ```", "```py\n    def process_samples(samples, dictionary):\n        num_lines = len(samples)\n        num_columns = len(dictionary)\n        sample_matrix = lil_matrix((num_lines,  \n                        num_columns)).astype('float32')\n        for line in range(0, num_lines):\n            s = samples[line]\n            s = process_text(s)\n            s = dictionary.doc2bow(s)\n            for token_id, token_count in s:\n                sample_matrix[line, token_id] = token_count\n            line+=1\n        buf = io.BytesIO()\n        smac.write_spmatrix_to_sparse_tensor(\n            buf,\n            sample_matrix,\n            None)\n        buf.seek(0)\n        return buf\n    ```", "```py\n    samples = [ \"Major tariffs expected to end Australian barley trade to China\", \"Satellite imagery sparks more speculation on North Korean leader Kim Jong-un\", \"Fifty trains out of service as fault forces Adelaide passengers to 'pack like sardines\", \"Germany's Bundesliga plans its return from lockdown as football world watches\", \"All AFL players to face COVID-19 testing before training resumes\" ]\n    ```", "```py\n    lda_predictor.serializer =  \n        sagemaker.serializers.CSVSerializer()\n    response = lda_predictor.predict(\n               process_samples(samples, dictionary))\n    print(response)\n    ```", "```py\n    {'predictions': [\n    {'topic_mixture': [0,0.22,0.54,0.23,0,0,0,0,0,0]}, \n    {'topic_mixture': [0.51,0.49,0,0,0,0,0,0,0,0]}, {'topic_mixture': [0.38,0,0.22,0,0.40,0,0,0,0,0]}, {'topic_mixture': [0.38,0.62,0,0,0,0,0,0,0,0]}, {'topic_mixture': [0,0.75,0,0,0,0,0,0.25,0,0]}]}\n    ```", "```py\n    import numpy as np\n    vecs = [r['topic_mixture'] for r in response['predictions']]\n    for v in vecs:\n        top_topic = np.argmax(v)\n        print(\"topic %s, %2.2f\"%(top_topic,v[top_topic]))\n    ```", "```py\n    topic 2, 0.54\n    topic 0, 0.51\n    topic 4, 0.40\n    topic 1, 0.62\n    topic 1, 0.75 \n    ```", "```py\n    lda_predictor.delete_endpoint()\n    ```", "```py\n    s3_auxiliary_path =    \n        session.upload_data(path='vocab.txt',   \n        key_prefix=prefix + '/input/auxiliary')\n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    region_name = session.boto_session.region_name\n    container = retrieve('ntm', region)\n    ```", "```py\n    ntm.set_hyperparameters(num_topics=10, \n                            feature_dim=len(dictionary),\n                            optimizer='adam',\n                            mini_batch_size=256,\n                            num_patience_epochs=10)\n    ```", "```py\n    ntm.fit(inputs={'train': s3_training_path, \n                    'auxiliary': s3_auxiliary_path})\n    ```", "```py\n(num_topics:10) [wetc 0.42, tu 0.86]\n```", "```py\n[0.51, 0.84] stabbing charged guilty pleads murder fatal man assault bail jailed alleged shooting arrested teen girl accused boy car found crash\n```", "```py\n[0.36, 0.85] seeker asylum climate live front hears change export carbon tax court wind challenge told accused rule legal face stand boat\n```", "```py\n[0.39, 0.78] seeker crew hour asylum cause damage truck country firefighter blaze crash warning ta plane near highway accident one fire fatal\n```", "```py\n[0.54, 0.93] cup world v league one match win title final star live victory england day nrl miss beat team afl player\n```", "```py\n[0.35, 0.77] coast korea gold north east central pleads west south guilty queensland found qld rain beach cyclone northern nuclear crop mine\n```", "```py\n[0.38, 0.88] iraq troop bomb trade korea nuclear kill soldier iraqi blast pm president china pakistan howard visit pacific u abc anti\n```", "```py\n[0.25, 0.88] news hour country rural national abc ta sport vic abuse sa nsw weather nt club qld award business\n```", "```py\n[0.62, 0.90] share dollar rise rate market fall profit price interest toll record export bank despite drop loss post high strong trade\n```", "```py\n[0.41, 0.90] issue election vote league hunt interest poll parliament gun investigate opposition raid arrest police candidate victoria house northern crime rate\n```", "```py\n[0.37, 0.84] missing search crop body found wind rain continues speaks john drought farm farmer smith pacific crew river find mark tourist\n```", "```py\ntopics = ['crime','legal','disaster','sports','unknown1',\n          'international','local','finance','politics', \n          'unknown2']\nsamples = [ \"Major tariffs expected to end Australian barley trade to China\", \"US woman wanted over fatal crash asks for release after coronavirus halts extradition\", \"Fifty trains out of service as fault forces Adelaide passengers to 'pack like sardines\", \"Germany's Bundesliga plans its return from lockdown as football world watches\", \"All AFL players to face COVID-19 testing before training resumes\" ]\n```", "```py\nimport numpy as np\nfor r in response['predictions']:\n    sorted_indexes = np.argsort(r['topic_weights']).tolist()\n    sorted_indexes.reverse()\n    top_topics = [topics[i] for i in sorted_indexes]\n    top_weights = [r['topic_weights'][i] \n                   for i in sorted_indexes]\n    pairs = list(zip(top_topics, top_weights))\n    print(pairs[:3])\n```", "```py\n[('finance', 0.30),('international', 0.22),('sports', 0.09)]\n[('unknown1', 0.19),('legal', 0.15),('politics', 0.14)]\n[('crime', 0.32), ('legal', 0.18), ('international', 0.09)]\n[('sports', 0.28),('unknown1', 0.09),('unknown2', 0.08)]\n[('sports', 0.27),('disaster', 0.12),('crime', 0.11)]\n```"]
- en: 'Chapter 12: Automating Machine Learning Workflows'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to deploy machine learning models in
    different configurations, using both the `boto3` SDK. We used their APIs in **Jupyter**
    **Notebooks** – the preferred way to experiment and iterate quickly.
  prefs: []
  type: TYPE_NORMAL
- en: However, running notebooks for production tasks is not a good idea. Even if
    your code has been carefully tested, what about monitoring, logging, creating
    other AWS resources, handling errors, rolling back, and so on? Doing all of this
    right would require a lot of extra work and code, opening the possibility for
    more bugs. A more industrial approach is required.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you'll first learn how to provision SageMaker resources with
    **AWS** **CloudFormation** and **AWS** **Cloud Development Kit** (**CDK**) – two
    AWS services purposely built to bring repeatability, predictability, and robustness.
    You'll see how you can preview changes before applying them, in order to avoid
    uncontrolled and potentially destructive operations.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you'll learn how to automate end-to-end machine learning workflows with
    two other services – **AWS** **Step Functions** and **Amazon** **SageMaker Pipelines**.
    You'll see how to build workflows with simple APIs, and how to visualize results
    in **SageMaker Studio**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Automating with AWS CloudFormation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating with AWS CDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building end-to-end workflows with AWS Step Functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building end-to-end workflows with Amazon SageMaker Pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS free tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to install and configure the **AWS** **Command Line Interface**
    (**CLI**) for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working `pandas`, `numpy`, and more).
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in this book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Automating with AWS CloudFormation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS CloudFormation has long been the preferred way to automate infrastructure
    builds and operations on AWS ([https://aws.amazon.com/cloudformation](https://aws.amazon.com/cloudformation)).
    You could certainly write a book on the topic, but we'll stick to the basics in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in using CloudFormation is to write a template – that is, a **JSON**
    or **YAML** text file describing the **resources** that you want to build, such
    as an **EC2** instance or an **S3** bucket. Resources are available for almost
    all AWS services, and SageMaker is no exception. If we look at [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_SageMaker.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_SageMaker.html),
    we see that we can create SageMaker Studio applications, deploy endpoints, and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: A template can (and should) include parameters and outputs. The former help
    make templates as generic as possible. The latter provide information that can
    be used by downstream applications, such as endpoint URLs or bucket names.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've written your template file, you pass it to CloudFormation to create
    a **stack** – that is, a collection of AWS resources. CloudFormation will parse
    the template and create all resources automatically. Dependencies are also managed
    automatically, and resources will be created in the correct order. If a stack
    can't be created correctly, CloudFormation will roll it back, deleting resources
    that have been built so far.
  prefs: []
  type: TYPE_NORMAL
- en: A stack can be updated by applying a newer template revision. CloudFormation
    will analyze changes, and will create, delete, update, or replace resources accordingly.
    Thanks to **change sets**, you can verify changes before they are performed, and
    then decide whether to proceed or not.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, a stack can be deleted, and CloudFormation will automatically tear
    down all its resources, which is a great way to clean up your builds without leaving
    any cruft behind.
  prefs: []
  type: TYPE_NORMAL
- en: Let's run a first example, where we deploy a model to a real-time endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This stack will be equivalent to calling the `boto3` API we studied in [*Chapter
    11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237), *Deploying Machine Learning
    Models*: `create_model()`, `create_endpoint_configuration()`, and `create_endpoint()`.
    Accordingly, we''ll define three CloudFormation resources (a model, an endpoint
    configuration, and an endpoint) and their parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a new YAML file named `endpoint-one-model.yml`, we first define the
    input parameters for the stack in the `Parameters` section. Each parameter has
    a name, a description, and a type. Optionally, we can provide default values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `Resources` section, we define a model resource, using the `Ref` built-in
    function to reference the appropriate input parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define an endpoint configuration resource. We use the `GetAtt` built-in
    function to get the name of the model resource. Of course, this requires that
    the model resource already exists, and CloudFormation will make sure that resources
    are created in the right order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we define an endpoint resource. Likewise, we use `GetAtt` to find
    the name of the endpoint configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `Outputs` section, we return the CloudFormation identifier of the endpoint,
    as well as its name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that the template is complete (`endpoint-one-model.yml`), we can create
    a stack.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please make sure that your IAM role has permission to invoke CloudFormation
    APIs. If not, please add the `AWSCloudFormationFullAccess` managed policy to the
    role.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model to a real-time endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s use the `boto3` API to create a stack deploying a **TensorFlow** model.
    We''ll reuse a model trained with **Keras** on **Fashion MNIST**:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As our template is completely region-independent, you can use any region that
    you want. Just make sure that you have trained a model there, and that you're
    using the appropriate container image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll need `boto3` clients for SageMaker and CloudFormation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We describe the training job to find the location of its artifact, and its
    execution role:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set the container to use for deployment. In some cases, this is unnecessary,
    as the same container is used for training and deployment. For **TensorFlow**
    and other frameworks, SageMaker uses two different containers. You can find more
    information at [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we read our template, create a new stack, and pass the required parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Jumping to the CloudFormation console, we see that the stack is being created,
    as shown in the following screenshot. Notice that resources are created in the
    right order: model, endpoint configuration, and endpoint:![Figure 12.1 – Viewing
    stack creation'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.1 – Viewing stack creation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As we would expect, we also see the endpoint in SageMaker Studio, as shown
    in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Viewing endpoint creation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_12_2.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.2 – Viewing endpoint creation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the stack creation is complete, we can use its output to find the name
    of the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the stack status and the endpoint name autogenerated by CloudFormation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**CREATE_COMPLETE**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Endpoint-MTaOIs4Vexpt**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can test the endpoint as usual. Then, we can delete the stack and its resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: However, let's not delete the stack right away. Instead, we're going to update
    it using a change set.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying a stack with a change set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we''re going to update the number of instances backing the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a new change set using the same template and parameters, except `InstanceCount`,
    which we set to `2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We see details on the change set in the CloudFormation console, as shown in
    the next screenshot. We could also use the `describe_change_set()` API:![Figure
    12.3 – Viewing a change set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_3.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.3 – Viewing a change set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This tells us that the endpoint configuration and the endpoint need to be modified,
    and possibly replaced. As we already know from [*Chapter 11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237),
    *Deploying Machine Learning Models*, a new endpoint will be created and applied
    in a non-disruptive fashion to the existing endpoint.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When working with CloudFormation, it's critical that you understand the **replacement
    policy** for your resources. Details are available in the documentation for each
    resource type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By clicking on the `execute_change_set()` API. As expected, the endpoint is
    immediately updated, as shown in the following screenshot:![Figure 12.4 – Updating
    the endpoint
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.4 – Updating the endpoint
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the update is complete, we see the sequence of events in the CloudFormation
    console, as shown in the next screenshot. A new endpoint configuration has been
    created and applied to the endpoint. The previous endpoint configuration has been
    deleted:![Figure 12.5 – Updating the stack
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_5.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.5 – Updating the stack
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can check that the endpoint is now backed by two instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the number of instances backing the Production Variant:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's keep working with change sets and add a second production variant to the
    endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a second production variant to the endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our initial template only defined a single production variant. We''ll update
    it and add another one (`endpoint-two-models.yml`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Parameters` section, we add entries for a second model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We do the same in the `Resources` section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Moving back to our notebook, we get information on another training job. We
    then create a change set, reading the updated template and passing all required
    parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Looking at the CloudFormation console, we see the changes caused by the change
    set. Create a new model and modify the endpoint configuration and the endpoint:![Figure
    12.6 – Viewing the change set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.6 – Viewing the change set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We execute the change set. Once it''s complete, we see that the endpoint now
    supports two production variants. Note that the instance count is back to its
    initial value, as we defined it as `1` in the updated template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Viewing production variants'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – Viewing production variants
  prefs: []
  type: TYPE_NORMAL
- en: The new production variant has a weight of `0`, so it won't be used for prediction.
    Let's see how we can gradually introduce it using **canary deployment**.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing canary deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Canary deployment is a popular technique for gradual application deployment
    ([https://martinfowler.com/bliki/CanaryRelease.html](https://martinfowler.com/bliki/CanaryRelease.html)),
    and it can also be used for machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply put, we''ll use a series of stack updates to gradually increase the
    weight of the second production variant in 10% increments, until it completely
    replaces the first production variant. We''ll also create a CloudWatch alarm monitoring
    the latency of the second production variant – if the alarm is triggered, the
    change set will be rolled back:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a CloudWatch alarm monitoring the 60-second average latency of the
    second production variant. We set the threshold at 500 milliseconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We find the ARN of the alarm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we loop over weights and update the stack. Change sets are unnecessary
    here, as we know exactly what''s going to happen from a resource perspective.
    We set our CloudWatch alarm as a **rollback trigger**, giving it five minutes
    to go off after each update before moving on to the next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's all it takes. Pretty cool, don't you think?
  prefs: []
  type: TYPE_NORMAL
- en: This cell will run for a couple of hours, so don't stop it. In another notebook,
    the next step is to start sending some traffic to the endpoint. For the sake of
    brevity, I won't include the code, which is identical to the one we used in [*Chapter
    7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending Machine Learning
    Services with Built-in Frameworks*. You'll find the notebook in the GitHub repository
    for this book (`Chapter12/cloudformation/Predict Fashion MNIST images.ipynb`).
  prefs: []
  type: TYPE_NORMAL
- en: Now, all we have to do is sit back, have a cup of tea, and enjoy the fact that
    our model is being deployed safely and automatically. As endpoint updates are
    seamless, client applications won't notice a thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a couple of hours, deployment is complete. The next screenshot shows
    invocations for both variants over time. As we can see, traffic was gradually
    shifted from the first variant to the second one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Monitoring canary deployment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Monitoring canary deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Latency stayed well under our 500-millisecond limit, and the alarm wasn''t
    triggered, as shown in the next screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Viewing the CloudWatch alarm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9 – Viewing the CloudWatch alarm
  prefs: []
  type: TYPE_NORMAL
- en: This example can serve as a starting point for your own deployments. For example,
    you could add an alarm monitoring `4xx` or `5xx` HTTP errors. You could also monitor
    a business metric directly impacted by prediction latency and accuracy, such as
    click-through rate, conversion rate, and so on. A useful thing to add would be
    an alarm notification (email, SMS, or even a Lambda function) in order to trigger
    downstream actions, should model deployment fail. The possibilities are endless!
  prefs: []
  type: TYPE_NORMAL
- en: When you're done, *don't forget to delete the stack*, either in the CloudFormation
    console or with the `delete_stack()` API. This will automatically clean up all
    AWS resources created by the stack.
  prefs: []
  type: TYPE_NORMAL
- en: '**Blue-green deployment** is another popular technique. Let''s see how we can
    implement it on SageMaker.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing blue-green deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Blue-green deployment requires two production environments ([https://martinfowler.com/bliki/BlueGreenDeployment.html](https://martinfowler.com/bliki/BlueGreenDeployment.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: The live production environment (`blue`) running version `n`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A copy of this environment (`green`) running version `n+1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at two possible scenarios, which we could implement using the same
    APIs we've used for canary deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing blue-green deployment with a single endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Starting from an existing endpoint running the current version of the model,
    we would carry out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new endpoint configuration with two production variants: one for the
    current model and one for the new model. Initial weights would be set to `1` and
    `0` respectively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply it to the endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run tests on the new production variant, selecting it explicitly with the `TargetVariant`
    parameter in `invoke_endpoint()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When tests are satisfactory, update weights to `0` and `1`. This will seamlessly
    switch traffic to the new model. If anything goes wrong, revert the weights to
    `1` and `0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the deployment is complete, update the endpoint to delete the first production
    variant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a simple and robust solution. However, updating an endpoint takes several
    minutes, making the whole process not as quick as one may want. Let's see how
    we can fix this problem by using two endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing blue-green deployment with two endpoints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Starting from an existing endpoint running the current version of the model,
    we would implement the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a second endpoint running the new version of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run tests on this new endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the tests are satisfactory, switch all traffic to the new endpoint. This
    could be achieved in different ways; for example, updating a parameter in your
    business application, or updating a private DNS entry. If anything goes wrong,
    revert to the previous setting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the deployment is complete, delete the old endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This setup is a little more complex, but it lets you switch instantly from one
    model version to the next, both for deployments and rollbacks.
  prefs: []
  type: TYPE_NORMAL
- en: CloudFormation is a fantastic tool for automation, and any time spent learning
    it will pay dividends. Yet some AWS users prefer writing code to writing templates,
    which is why we introduced the CDK.
  prefs: []
  type: TYPE_NORMAL
- en: Automating with AWS CDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS CDK is a multi-language SDK that lets you write code to define AWS infrastructure
    ([https://github.com/aws/aws-cdk](https://github.com/aws/aws-cdk)). Using the
    CDK CLI, you can then provision this infrastructure, using CloudFormation under
    the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the CDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CDK is natively implemented with `npm` tool is installed on your machine
    ([https://www.npmjs.com/get-npm](https://www.npmjs.com/get-npm)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing the CDK is then as simple as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a CDK application and deploy an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a CDK application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll deploy the same model that we deployed with CloudFormation. I''ll use
    Python, and you could also use **JavaScript**, **TypeScript**, **Java**, and .**NET**.
    API documentation is available at [https://docs.aws.amazon.com/cdk/api/latest/python/](https://docs.aws.amazon.com/cdk/api/latest/python/):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a Python application named `endpoint`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This automatically creates a virtual environment, which we need to activate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This also creates a default `app.py` file for our CDK code, a `cdk.json` file
    for application configuration, and a `requirements.txt` file to install dependencies.
    Instead, we''ll use the files present in the GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `requirements.txt` file, we install the CDK package for S3 and SageMaker.
    Each service requires a different package. For example, we would add `aws_cdk.aws_s3`
    for S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then install requirements as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `cdk.json` file, we store the application context. Namely, key-value
    pairs that can be read by the application for configuration ([https://docs.aws.amazon.com/cdk/latest/guide/context.html](https://docs.aws.amazon.com/cdk/latest/guide/context.html)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the preferred way to pass values to your application. You should manage
    this file with version control in order to keep track of how stacks were built.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can view the context of our application with the `cdk context` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Viewing CDK context'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.10 – Viewing CDK context
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to write the actual application.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a CDK application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All code goes in the `app.py` file, which we implement in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We extend the `core.Stack` class to create our own stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We add a `CfnModel` object, reading the appropriate context values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We add a `CfnEndpointConfig` object, using the built-in `get_att()` function
    to associate it to the model. This creates a dependency that CloudFormation will
    use to build resources in the right order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We add a `CfnEndpoint` object, using the built-in `get_att()` function to associate
    it to the endpoint configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we instantiate the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our code is complete!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a CDK application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now deploy the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can list the available stacks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also see the actual CloudFormation template. It should be extremely
    similar to the template we wrote in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Deploying the stack is equally simple, as shown in the next screenshot:![Figure
    12.11 – Deploying an endpoint
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.11 – Deploying an endpoint
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Looking at CloudFormation, we see that the stack is created using a change set.
    A few minutes later, the endpoint is in service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Editing `app.py`, we set the initial instance count to `2`. We then ask CDK
    to deploy the stack, but without executing the change set, as shown in the next
    screenshot:![Figure 12.12 – Creating a change set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.12 – Creating a change set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we're happy with the change set, we can execute it in the CloudFormation
    console, or run the previous command again without `--no-execute`. As expected,
    and as shown in the next screenshot, the endpoint is updated:![Figure 12.13 –
    Updating the endpoint
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.13 – Updating the endpoint
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When we''re done, we can destroy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the CDK is an interesting alternative to writing templates directly,
    while still benefiting from the rigor and the robustness of CloudFormation.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we haven't done yet is to automate an end-to-end workflow, from training
    to deployment. Let's do this with AWS Step Functions.
  prefs: []
  type: TYPE_NORMAL
- en: Building end-to-end workflows with AWS Step Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**AWS Step Functions** let you define and run workflows based on **state machines**
    ([https://aws.amazon.com/step-functions/](https://aws.amazon.com/step-functions/)).
    A state machine is a combination of steps, which can be sequential, parallel,
    or conditional. Each step receives an input from its predecessor, performs an
    operation, and passes the output to its successor. Step Functions are integrated
    with many AWS services, such as Amazon SageMaker, **AWS** **Lambda**, container
    services, **Amazon** **DynamoDB**, **Amazon** **EMR**, **AWS** **Glue**, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: State machines can be defined using JSON and the **Amazon States Language**,
    and you can visualize them in the service console. State machine execution is
    fully managed, so you don't need to provision any infrastructure to run.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to SageMaker, Step Functions has a dedicated Python SDK, oddly
    named the **Data Science SDK** ([https://github.com/aws/aws-step-functions-data-science-sdk-python](https://github.com/aws/aws-step-functions-data-science-sdk-python)).
  prefs: []
  type: TYPE_NORMAL
- en: Let's run an example where we automate training and deployment for a **scikit-learn**
    model trained on the **Boston Housing** dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up permissions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, please make sure that the IAM role for your user or for your notebook
    instance has permission to invoke Step Functions APIs. If not, please add the
    `AWSStepFunctionsFullAccess` managed policy to the role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to create a service role for Step Functions, allowing it to invoke
    AWS APIs on our behalf:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the IAM console ([https://console.aws.amazon.com/iam/home#/roles](https://console.aws.amazon.com/iam/home#/roles)),
    we click on **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We select **AWS service** and **Step Functions**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We click through the next screens until we can enter the role name. Let's call
    it `StepFunctionsWorkflowExecutionRole`, and click on **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting this role, we click on its **Permission** tab, then on **Add inline
    policy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting the JSON tab, we replace the empty policy with the content of the
    `Chapter12/step_functions/service-role-policy.json` file, and we click on **Review
    policy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We name the policy `StepFunctionsWorkflowExecutionPolicy` and click on **Create
    policy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We write down the ARN on the role, and we close the IAM console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The setup is now complete. Now, let's create a workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our first workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this workflow, we''ll go through the following step sequence: train the
    model, create it, use it for a batch transform, create an endpoint configuration,
    and deploy the model to an endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We upload the training set to S3, as well as a test set where we removed the
    target attribute. We''ll use the latter for a batch transform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We configure our estimator as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also define the transformer that we''ll use for batch transform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We import the Step Functions objects required by the workflow. You can find
    the API documentation at [https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the input of the workflow. We''ll pass it a training job name, a
    model name, and an endpoint name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first step of the workflow is the training step. We pass it the estimator,
    the location of the dataset in S3, and a training job name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is the model creation step. We pass it the location of the model
    trained in the previous step, and a model name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is running a batch transform on the test dataset. We pass the
    `transformer` object, the location of the test dataset in S3, and its content
    type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is creating the endpoint configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last step is creating the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that all steps have been defined, we chain them in sequential order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now build our workflow, using the workflow definition and the input definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can visualize the state machine, an easy way to check that we built it as
    expected, as shown in the next screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 12.14 – Viewing the state machine'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_12_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.14 – Viewing the state machine
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '­­We create the workflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It's visible in the Step Functions console, as shown in the following screenshot.
    We can see both its graphical representation and its JSON definition based on
    the Amazon States Language. We could edit the workflow as well if needed:![Figure
    12.15 – Viewing the state machine in the console
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.15 – Viewing the state machine in the console
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We run the workflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can track its progress with `render_progress()` and the `list_events()` API.
    We can also see it in the console, as shown in the next screenshot. Note that
    we also see the input and output of each step, which is a great way to troubleshoot
    problems:![Figure 12.16 – Running the state machine
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.16 – Running the state machine
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When the workflow is complete, you can test the endpoint as usual. *Don't forget
    to delete it in the SageMaker console when you're done*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example shows how simple it is to build a SageMaker workflow with this
    SDK. Still, we could improve it by running batch transform and endpoint creation
    in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Adding parallel execution to a workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next screenshot shows the workflow we''re going to build. The steps themselves
    are exactly the same. We''re only going to modify the way they''re chained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.17 – Viewing the parallel state machine'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.17 – Viewing the parallel state machine
  prefs: []
  type: TYPE_NORMAL
- en: 'We will get started using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our workflow has two branches – one for batch transform and one for the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a `Parallel` step in order to allow parallel execution of these two
    branches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We put everything together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it! We can now create and run this workflow just like in the previous
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the Step Functions console, we see that the workflow does run the
    two branches in parallel. There is a minor problem, however. The endpoint creation
    step is shown as complete, although the endpoint is still being created. You can
    see in the SageMaker console that the endpoint is listed as `Creating`. This could
    cause a problem if a client application tried to invoke the endpoint right after
    the workflow completes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's improve this by adding an extra step, waiting for the endpoint to be in
    service. We can easily do this with a Lambda function, allowing us to run our
    own code anywhere in a workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a Lambda function to a workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you've never looked at **AWS** **Lambda** ([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda)),
    you're missing out! Lambda is at the core of serverless architectures, where you
    can write and deploy short functions running on fully managed infrastructure.
    These functions can be triggered by all sorts of AWS events, and they can also
    be invoked on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up permissions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating a Lambda function is simple. The only prerequisite is to create an
    `DescribeEndpoint` API, as well as permission to create a log in CloudWatch. Let''s
    use the `boto3` API for this. You can find more information at [https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html](https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define a **trust policy** for the role, allowing it to be assumed
    by the Lambda service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a role and attach the trust policy to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a policy listing the APIs that are allowed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the policy and add it to the role:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The IAM setup is now complete.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a Lambda function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now write a short Lambda function. It receives a JSON event as input,
    which stores the ARN of the endpoint being created by the `EndpointStep` step.
    It simply extracts the endpoint name from the ARN, creates a `boto3` waiter, and
    waits until the endpoint is in service. The following screenshot shows the code
    in the Lambda console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.18 – Our Lambda function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.18 – Our Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy this function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a deployment package for the Lambda function and upload it to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create the function with a timeout of 15 minutes, the longest possible runtime
    for a Lambda function. Endpoints are typically deployed in less than 10 minutes,
    so this should be more than enough:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the Lambda function has been created, we can easily add it to the
    existing workflow. We define a `LambdaStep` and add it to the endpoint branch.
    Its payload is the endpoint ARN, extracted from the output of the `EndpointStep`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the workflow again, we see in the following screenshot that this new
    step receives the endpoint ARN as input and waits for the endpoint to be in service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.19 – Running the state machine with Lambda'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.19 – Running the state machine with Lambda
  prefs: []
  type: TYPE_NORMAL
- en: There are many other ways you can use Lambda functions with SageMaker. You can
    extract training metrics, predict test sets on an endpoint, and more. The possibilities
    are endless.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's automate end-to-end workflows with Amazon SageMaker Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Building end-to-end workflows with Amazon SageMaker Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon SageMaker Pipelines** lets us create and run end-to-end machine learning
    **workflows** based on SageMaker steps for training, tuning, batch transform,
    and processing scripts, using SageMaker APIs SDK that are very similar to the
    ones we used in Step Functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to Step Functions, SageMaker Pipelines adds the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to write, run, visualize and manage your workflows directly in SageMaker
    Studio, without having to jump to the AWS console.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **model registry**, which makes it easier to manage model versions, deploy
    only approved versions, and track **lineage**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLOps templates** – a collection of CloudFormation templates published via
    **AWS Service Catalog** that help you automate the deployment of your models.
    Built-in templates are provided, and you can add your own. You (or your Ops team)
    can learn more at [https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One thing that SageMaker Pipelines lacks is integration with other AWS services.
    At the time of writing, SageMaker Pipelines only supports **SQS**, whereas Step
    Functions supports many compute and big data services. With SageMaker Pipelines,
    the assumption is either that your training data has already been processed, or
    that you'll process it with SageMaker Processing steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we know what SageMaker Pipelines is, let''s run a complete example
    based on the Amazon Reviews dataset and the BlazingText algorithm we used in [*Chapter
    6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108), *Training Natural Language
    Processing Models*, and [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Advanced Training Techniques*, putting together many of the services we learned
    about so far. Our pipeline will contain the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A processing step, where we prepare the dataset with **SageMaker Processing**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ingestion step, where we load the processed data set in **SageMaker Feature
    Store**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataset building step, where we use **Amazon Athena** to query the offline
    store and save a dataset to S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A training step, where we train a BlazingText model on the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model creation step, where we save the trained model as a SageMaker model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model registration step, where we add the model to the SageMaker Pipelines
    model registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In real life, you should not initially worry about automation. You should first
    experiment with Jupyter Notebooks and iterate on all these steps. Then, as the
    project matures, you should start automating each step, eventually assembling
    them as a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: My recommendation is to first automate each processing step, with individual
    SageMaker Processing jobs. Not only will this come in handy in the development
    phase, but it will also create a simple and step-by-step path to full automation.
    Indeed, once steps run fine with SageMaker Processing, it takes little effort
    to combine them with SageMaker Pipelines. In fact, you can use the exact same
    Python script. You'll only have to write code with the Pipelines SDK. As you'll
    see in a minute, it's very similar to the Processing SDK.
  prefs: []
  type: TYPE_NORMAL
- en: This is the approach I've followed with the following example. In the GitHub
    repository, you'll find SageMaker Processing notebooks for the data processing,
    ingestion, and dataset building steps, as well as another notebook for the end-to-end
    workflow. Here, we'll focus on the latter. Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Defining workflow parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like CloudFormation templates, you can (and should) define parameters in
    your workflows. This makes them easier to reuse in other projects. Parameters
    can be strings, integers, and floats, with an optional default value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create parameters for the AWS region and for the instances we''d like to
    use for processing and training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We also create parameters for the location of input data, the model name, and
    the model status to set in the model registry (more on this later).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's define the data processing step.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the dataset with SageMaker Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We reuse the processing script we wrote in [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)
    (`preprocessing.py`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a `SKLearnProcessor` object with the parameters we just defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define the data processing step. Remember that it creates two outputs:
    one in BlazingText format, and one for ingestion in SageMaker Feature Store. As
    mentioned earlier, the SageMaker Pipelines syntax is extremely close to the SageMaker
    Processing syntax (inputs, outputs, and arguments).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's define the ingestion step.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting the dataset in SageMaker Feature Store with SageMaker Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We reuse the processing script we wrote in [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206)
    (`ingesting.py`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define a name for the feature group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define a processing step, setting the data input to the output of the
    first processing job. To illustrate step chaining, we define an output pointing
    to a file saved by the script, which contains the name of the feature group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's take care of the dataset building step.
  prefs: []
  type: TYPE_NORMAL
- en: Building a dataset with Amazon Athena and SageMaker Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We reuse the processing script we wrote in [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206)
    (`querying.py`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the input to the output of the ingestion step, in order to retrieve
    the name of the feature group. We also define two outputs for the training and
    validation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's move on to the training step.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'No surprises here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define an `Estimator` module for this job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define the training step, passing the training and validation datasets
    as inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's take care of the model creation and model registration steps (the
    last ones in the pipeline).
  prefs: []
  type: TYPE_NORMAL
- en: Creating and registering a model in SageMaker Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model has been trained, we need to create it as a SageMaker model and
    register it in the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create the model, passing the location of the training container and of
    the model artifact:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then register the model in the model registry, passing the list of allowed
    instance types for deployment, as well as the approval status. We associate it
    to a model package group that will hold this model, as well as further versions
    we train in the future:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: All the steps are now defined, so let's assemble them in a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We simply put together all the steps and their parameters. Then, we create
    the pipeline (or update it if it existed previously):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We're all set. Let's run our pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: Running a pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It takes a single line of code to fire up a pipeline execution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We assign values to the data location and model name parameters (the other
    ones have default values):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In SageMaker Studio, we go **SageMaker resources** / **Pipelines**, and we see
    the pipeline executing, as shown in the next screenshot:![Figure 12.20 – Executing
    a pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_12_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.20 – Executing a pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After an hour and a half, the pipeline is complete, as shown in the next screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.21 – Visualizing a pipeline'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_12_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.21 – Visualizing a pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, for each step of the pipeline, we can see the lineage of all artifacts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For example, the output for the training step is shown in the next image. We
    see exactly which datasets and which container were used to train the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.22 – Viewing the lineage for the training step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.22 – Viewing the lineage for the training step
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we can deploy this model.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model from the model registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Going to **SageMaker resources** / **Model registry**, we also see that the
    model has been registered in the model registry, as shown in the next screenshot.
    If we train further versions of the model, they will also appear here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.23 – Viewing a model in the model registry'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_12_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.23 – Viewing a model in the model registry
  prefs: []
  type: TYPE_NORMAL
- en: As its status is `Pending`, it can't be deployed for now. We need to change
    it to `Approved` in order to allow deployment. This is a safe way to guarantee
    that only good models are deployed, once all appropriate tests have been performed.
  prefs: []
  type: TYPE_NORMAL
- en: We right-click on the model and select `Approved`. We also note the model ARN,
    which is visible in the **Settings** tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can deploy and test the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in our Jupyter Notebook, we create a `ModelPackage` object pointing at
    the model version we''d like to deploy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We call `deploy()` as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a `Predictor` and send a test sample for prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the probabilities for all three classes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we're done, we can delete the endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a full clean-up, you should also delete the pipeline, the feature store,
    and the model package group. You'll find a clean-up notebook in the GitHub repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, SageMaker Pipelines provides you with robust and powerful tools
    to build, run, and track end-to-end machine learning workflows. These tools are
    nicely integrated in SageMaker Studio, which should help you to be more productive
    and get high-quality models in production quicker
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you first learned how to deploy and update endpoints with AWS
    CloudFormation. You also saw how it can be used to implement canary deployment
    and blue-green deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you learned about the AWS CDK, an SDK specifically built to easily generate
    and deploy CloudFormation templates using a variety of programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you built complete end-to-end machine learning workflows with AWS Step
    Functions and Amazon SageMaker Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, you'll learn about additional SageMaker capabilities
    that help you optimize the cost and performance of predictions.
  prefs: []
  type: TYPE_NORMAL

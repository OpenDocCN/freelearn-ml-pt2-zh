["```py\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'\n\nheader = [\n    'symboling',\n    'normalized-losses',\n    'make',\n    # ... some list items are omitted for brevity \n    'highway-mpg',\n    'price',\n\n]\n```", "```py\ndf = pd.read_csv(url, names=header, na_values='?')\n```", "```py\ncols_with_missing = df.isnull().sum()\ncols_with_missing[\n    cols_with_missing > 0\n]\n```", "```py\nnormalized-losses    41\nnum-of-doors          2\nbore                  4\nstroke                4\nhorsepower            2\npeak-rpm              2\nprice                 4\n```", "```py\ndf = df[~df['price'].isnull()]\n```", "```py\ndf.drop(labels=['normalized-losses'], axis=1, inplace=True)\n```", "```py\nfeatures = {\n    'categorical': [\n        'make', 'fuel-type', 'aspiration', 'num-of-doors', \n        'body-style', 'drive-wheels', 'engine-location', \n        'engine-type', 'num-of-cylinders', 'fuel-system',\n\n    ],\n    'numerical': [\n        'symboling', 'wheel-base', 'length', 'width', 'height', \n        'curb-weight', 'engine-size', 'bore', 'stroke', \n        'compression-ratio', 'horsepower', 'peak-rpm', \n        'city-mpg', 'highway-mpg', \n    ],\n}\n\nfeatures['all'] = features['categorical'] + features['numerical']\n\ntarget = 'price'\n```", "```py\ndf[target] = df[target].astype(np.float64) / 1000\n```", "```py\n df[features['categorical']].sample(n=3, random_state=42)\n```", "```py\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.25, random_state=22)\n```", "```py\nx_train = df_train[features['all']]\nx_test = df_test[features['all']]\n\ny_train = df_train[target]\ny_test = df_test[target]\n```", "```py\ny_train.plot(\n    title=\"Distribution of Car Prices (in 1000's)\",\n    kind='hist', \n)\n```", "```py\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\nx_train = imp.fit_transform(x_train)\nx_test = imp.transform(x_test)\n```", "```py\nclass ColumnNamesKeeper:\n\n    def __init__(self, transformer):\n        self._columns = None\n        self.transformer = transformer\n\n    def fit(self, x, y=None):\n        self._columns = x.columns\n        self.transformer.fit(x)\n\n    def transform(self, x, y=None):\n        x = self.transformer.transform(x)\n        return pd.DataFrame(x, columns=self._columns)\n\n    def fit_transform(self, x, y=None):\n        self.fit(x, y)\n        return self.transform(x)\n```", "```py\nfrom sklearn.impute import SimpleImputer\n\nimp = ColumnNamesKeeper(\n    SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n)\n\nx_train = imp.fit_transform(x_train)\nx_test = imp.transform(x_test)\n```", "```py\nfrom category_encoders.ordinal import OrdinalEncoder\nenc = OrdinalEncoder(\n    cols=features['categorical'],\n    handle_unknown='value'\n)\nx_train = enc.fit_transform(x_train)\nx_test = enc.transform(x_test)\n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\nrgr = RandomForestRegressor(n_jobs=-1)\nrgr.fit(x_train, y_train)\ny_test_pred = rgr.predict(x_test)\n```", "```py\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n)\n\nprint(\n    'R2: {:.2f}, MSE: {:.2f}, RMSE: {:.2f}, MAE {:.2f}'.format(\n        r2_score(y_test, y_test_pred),\n        mean_squared_error(y_test, y_test_pred),\n        np.sqrt(mean_squared_error(y_test, y_test_pred)),\n        mean_absolute_error(y_test, y_test_pred),\n    )\n)\n```", "```py\n# R2: 0.90, MSE: 4.54, RMSE: 2.13, MAE 1.35\n```", "```py\ndf_pred = pd.DataFrame(\n    {\n        'actuals': y_test,\n        'predictions': y_test_pred,\n    }\n)\n\ndf_pred['error'] = np.abs(y_test - y_test_pred)\n\nfig, axs = plt.subplots(1, 2, figsize=(16, 5), sharey=False)\n\ndf_pred.plot(\n    title='Actuals vs Predictions',\n    kind='scatter',\n    x='actuals',\n    y='predictions',\n    ax=axs[0],\n)\n\ndf_pred['error'].plot(\n    title='Distribution of Error',\n    kind='hist',\n    ax=axs[1],\n)\n\nfig.show()\n```", "```py\nmae = []\nn_estimators_options = [5, 500, 5000]\n\nfor n_estimators in n_estimators_options:\n\n    rgr = RandomForestRegressor(\n        n_estimators=n_estimators,\n        bootstrap=True,\n        max_features=0.75,\n        max_samples=0.75,\n        n_jobs=-1,\n    )\n\n    rgr.fit(x_train, y_train)\n    y_test_pred = rgr.predict(x_test)\n    mae.append(mean_absolute_error(y_test, y_test_pred))\n```", "```py\ndf_feature_importances = pd.DataFrame(\n    {\n        'Feature': x_train.columns,\n        'Importance': rgr.feature_importances_,\n    }\n).sort_values(\n    'Importance', ascending=False\n)\n```", "```py\nfrom sklearn.inspection import plot_partial_dependence\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 7), sharey=False)\n\ntop_features = df_feature_importances['Feature'].head(6)\n\nplot_partial_dependence(\n    rgr, x_train, \n    features=top_features,\n    n_cols=3, \n    n_jobs=-1,\n    line_kw={'color': 'k'},\n    ax=ax\n) \n\nax.set_title('Partial Dependence')\n\nfig.show()\n```", "```py\nfrom sklearn.datasets import make_hastie_10_2\nx, y = make_hastie_10_2(n_samples=6000, random_state=42)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(\n    n_estimators=100,\n    oob_score=True,\n    n_jobs=-1,\n)\n\nclf.fit(x_train, y_train)\ny_pred_proba = clf.predict_proba(x_test)[:,1]\n```", "```py\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thr = roc_curve(y_test, y_pred_proba)\n```", "```py\npd.DataFrame(\n    {'FPR': fpr, 'TPR': tpr}\n).set_index('FPR')['TPR'].plot(\n    title=f'Receiver Operating Characteristic (ROC)',\n    label='Random Forest Classifier',\n    kind='line',\n)\n```", "```py\nfrom sklearn.metrics import auc\nauc_values = auc(fpr, tpr)\n```", "```py\nfrom sklearn.impute import SimpleImputer\nfrom category_encoders.one_hot import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nnumerical_mputer = ColumnNamesKeeper(\n    SimpleImputer(\n        missing_values=np.nan, \n        strategy='median'\n    )\n)\n\ncategorical_mputer = ColumnNamesKeeper(\n    SimpleImputer(\n        missing_values=np.nan, \n        strategy='most_frequent'\n    )\n)\n\nminmax_scaler = ColumnNamesKeeper(\n    MinMaxScaler()\n) \n\nonehot_encoder = OneHotEncoder(\n    cols=features['categorical'],\n    handle_unknown='value'\n)\n```", "```py\nnumerical_pipeline = Pipeline(\n    [\n        ('numerical_mputer', numerical_mputer), \n        ('minmax_scaler', minmax_scaler)\n    ]\n)\n\ncategorical_pipeline = Pipeline(\n    [\n        ('categorical_mputer', categorical_mputer), \n        ('onehot_encoder', onehot_encoder)\n    ]\n)\n```", "```py\nx_train_knn = pd.concat(\n    [\n        numerical_pipeline.fit_transform(df_train[features['numerical']]), \n        categorical_pipeline.fit_transform(df_train[features['categorical']]),\n    ],\n    axis=1\n)\n\nx_test_knn = pd.concat(\n    [\n        numerical_pipeline.transform(df_test[features['numerical']]), \n        categorical_pipeline.transform(df_test[features['categorical']]),\n    ],\n    axis=1\n)\n```", "```py\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nrgr = BaggingRegressor(\n    base_estimator=KNeighborsRegressor(\n        n_neighbors=1\n    ),\n    n_estimators=400,\n)\n\nrgr.fit(x_train_knn, df_train[target])\ny_test_pred = rgr.predict(x_test_knn)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nrgr = GradientBoostingRegressor(\n    n_estimators=1000, learning_rate=0.01, max_depth=3, loss='ls'\n)\n\nrgr.fit(x_train, y_train)\ny_test_pred = rgr.predict(x_test)\n```", "```py\n# R2: 0.92, MSE: 3.93, RMSE: 1.98, MAE: 1.42\n```", "```py\ndef calculate_deviance(estimator, x_test, y_test):\n\n    train_errors = estimator.train_score_\n    test_errors = [\n        estimator.loss_(y_test, y_pred_staged) \n        for y_pred_staged in estimator.staged_predict(x_test)\n    ]\n\n    return pd.DataFrame(\n        {\n            'n_estimators': range(1, estimator.estimators_.shape[0]+1),\n            'train_error': train_errors,\n            'test_error': test_errors,\n        }\n    ).set_index('n_estimators')\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nrgr = GradientBoostingRegressor(n_estimators=250, learning_rate=0.02, loss='ls')\nrgr.fit(x_train, y_train)\n```", "```py\nfig, ax = plt.subplots(1, 1, figsize=(16, 5), sharey=False)\n\ndf_deviance = calculate_deviance(rgr, x_test, y_test)\n\ndf_deviance['train_error'].plot(\n    kind='line', color='k', linestyle=':', ax=ax\n)\n\ndf_deviance['test_error'].plot(\n    kind='line', color='k', linestyle='-', ax=ax\n)\n\nfig.show()\n```", "```py\ndef lr_changer(n, estimator, params):\n    if n % 10:\n        estimator.learning_rate = 0.01\n    else:\n        estimator.learning_rate = 0.1\n    return False\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\nrgr = GradientBoostingRegressor(n_estimators=50, learning_rate=0.01, loss='ls')\nrgr.fit(x_train, y_train, monitor=lr_changer)\n```", "```py\nx_sample = np.arange(-10, 10, 0.05)\ny_sample = np.random.normal(loc=0, scale=25, size=x_sample.shape[0]) \ny_sample *= x_sample \n\npd_random_samples = pd.DataFrame(\n    {\n        'x': x_sample,\n        'y': y_sample\n    }\n)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8), sharey=False)\n\npd_random_samples.plot(\n    title='Regression Ranges [10th & 90th Quantiles]', \n    kind='scatter', x='x', y='y', color='k', alpha=0.95, ax=ax\n)\n\nfor quantile in [0.1, 0.9]:\n\n    rgr = GradientBoostingRegressor(n_estimators=10, loss='quantile', alpha=quantile)\n    rgr.fit(pd_random_samples[['x']], pd_random_samples['y'])\n    pd_random_samples[f'pred_q{quantile}'] = rgr.predict(pd_random_samples[['x']])\n\n    pd_random_samples.plot(\n        kind='line', x='x', y=f'pred_q{quantile}', \n        linestyle='-', alpha=0.75, color='k', ax=ax\n    )\n\nax.legend(ncol=1, fontsize='x-large', shadow=True)\n\nfig.show()\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nrgr_min = GradientBoostingRegressor(n_estimators=50, loss='quantile', alpha=0.25)\nrgr_max = GradientBoostingRegressor(n_estimators=50, loss='quantile', alpha=0.75)\n\nrgr_min.fit(x_train, y_train, monitor=lr_changer)\nrgr_max.fit(x_train, y_train, monitor=lr_changer)\n\ny_test_pred_min = rgr_min.predict(x_test)\ny_test_pred_max = rgr_max.predict(x_test)\n\ndf_pred_range = pd.DataFrame(\n    {\n        'Actuals': y_test,\n        'Pred_min': y_test_pred_min,\n        'Pred_max': y_test_pred_max,\n    }\n)\n```", "```py\ndf_pred_range['Actuals in Range?'] = df_pred_range.apply(\n    lambda row: 1 if row['Actuals'] >= row['Pred_min'] and row['Actuals'] <= row['Pred_max'] else 0, axis=1\n)\n```", "```py\nfrom sklearn.ensemble import AdaBoostRegressor\n\nrgr = AdaBoostRegressor(n_estimators=100)\nrgr.fit(x_train, y_train)\ny_test_pred = rgr.predict(x_test)\n```", "```py\npd.DataFrame(\n    [\n        (n, mean_squared_error(y_test, y_pred_staged))\n        for n, y_pred_staged in enumerate(rgr.staged_predict(x_test), 1)\n    ],\n    columns=['n', 'Test Error']\n).set_index('n').plot()\n\nfig.show()\n```", "```py\nfrom sklearn.ensemble import RandomTreesEmbedding\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\n\nrgr = make_pipeline(RandomTreesEmbedding(), Ridge())\nrgr.fit(x_train, y_train)\ny_test_pred = rgr.predict(x_test)\n\nprint(f'MSE: {mean_squared_error(y_test, y_test_pred)}')\n```"]
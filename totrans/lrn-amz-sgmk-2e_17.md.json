["```py\n    model_name = 'sagemaker-xgboost-2020-06-09-08-33-24-782'\n    endpoint_config_name = 'xgboost-one-model-epc'\n    endpoint_name = 'xgboost-one-model-ep'\n    production_variants = [{\n        'VariantName': 'variant-1',\n        'ModelName': model_name,\n        'InitialInstanceCount': 2,\n        'InitialVariantWeight': 1,\n        'InstanceType': 'ml.m5.large'}]\n    sm.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=production_variants)\n    sm.create_endpoint(\n        EndpointName=endpoint_name,\n        EndpointConfigName=endpoint_config_name)\n    ```", "```py\n    app = boto3.client('application-autoscaling')\n    app.register_scalable_target(\n     ServiceNamespace='sagemaker',\n     ResourceId=\n         'endpoint/xgboost-one-model-ep/variant/variant-1',\n     ScalableDimension=\n        'sagemaker:variant:DesiredInstanceCount',\n     MinCapacity=2,\n     MaxCapacity=10)\n    ```", "```py\n    policy_name = 'xgboost-scaling-policy'\n    app.put_scaling_policy(\n     PolicyName=policy_name,\n     ServiceNamespace='sagemaker',\n     ResourceId=\n       'endpoint/xgboost-one-model-ep/variant/variant-1',\n     ScalableDimension=\n       'sagemaker:variant:DesiredInstanceCount',\n     PolicyType='TargetTrackingScaling',\n    ```", "```py\n     TargetTrackingScalingPolicyConfiguration={\n       'TargetValue': 1000.0,\n       'PredefinedMetricSpecification': {\n           'PredefinedMetricType': \n           'SageMakerVariantInvocationsPerInstance'\n        },\n       'ScaleInCooldown': 60,\n       'ScaleOutCooldown': 60\n     }\n    )\n    ```", "```py\n    test_sample = '0.00632, 18.00, 2.310, 0, 0.5380, 6.5750, 65.20, 4.0900, 1, 296.0, 15.30, 396.90, 4.98'\n    smrt=boto3.Session().client(service_name='runtime.sagemaker') \n    while True:\n        smrt.invoke_endpoint(EndpointName=endpoint_name,\n                             ContentType='text/csv',\n                             Body=test_sample)\n    ```", "```py\n    app.delete_scaling_policy(\n     PolicyName=policy_name,\n     ServiceNamespace='sagemaker',\n     ScalableDimension='sagemaker:variant :DesiredInstanceCount',\n     ResourceId='endpoint/xgboost-one-model-ep/variant/variant-1')\n    sm.delete_endpoint(EndpointName=endpoint_name)\n    sm.delete_endpoint_config(\n      EndpointConfigName=endpoint_config_name)\n    ```", "```py\n    import sagemaker, boto3\n    sess = sagemaker.Session()\n    bucket = sess.default_bucket()\n    prefix = 'sklearn-boston-housing-mme'\n    training = sess.upload_data(path='housing.csv', \n                                key_prefix=prefix + \n                                '/training')\n    output = 's3://{}/{}/output/'.format(bucket,prefix)\n    ```", "```py\n    from sagemaker.sklearn import SKLearn\n    jobs = {}\n    for test_size in [0.2, 0.1, 0.05]:\n        sk = SKLearn(entry_point=\n                    'sklearn-boston-housing.py',\n            role=sagemaker.get_execution_role(),\n            framework_version='0.23-1',\n            instance_count=1,\n            instance_type='ml.m5.large',\n            output_path=output,\n            hyperparameters={ 'normalize': True,\n                              'test-size': test_size }\n        )\n        sk.fit({'training':training}, wait=False)\n        jobs[sk.latest_training_job.name] = {}\n        jobs[sk.latest_training_job.name]['test-size'] =   \n            test_size\n    ```", "```py\n    import boto3\n    sm = boto3.client('sagemaker')\n    for j in jobs.keys():\n        job = sm.describe_training_job(TrainingJobName=j)\n        jobs[j]['artifact'] =\n            job['ModelArtifacts']['S3ModelArtifacts']\n        jobs[j]['key'] = '/'.join(\n            job['ModelArtifacts']['S3ModelArtifacts']\n            .split('/')[3:])\n    ```", "```py\n    %%sh -s \"$bucket\" \"$prefix\"\n    aws s3 rm --recursive s3://$1/$2/models\n    ```", "```py\n    s3 = boto3.client('s3')\n    for j in jobs.keys():\n        copy_source = { 'Bucket': bucket, \n                        'Key': jobs[j]['key'] }\n        s3.copy_object(CopySource=copy_source,  \n                       Bucket=bucket, \n                       Key=prefix+'/models/'+j+'.tar.gz')\n    response = s3.list_objects(Bucket=bucket, \n                               Prefix=prefix+'/models/')\n    for o in response['Contents']:\n        print(o['Key'])\n    ```", "```py\n    sklearn-boston-housing-mme/models/sagemaker-scikit-learn-2021-09-01-07-52-22-679\n    sklearn-boston-housing-mme/models/sagemaker-scikit-learn-2021-09-01-07-52-26-399\n    sklearn-boston-housing-mme/models/sagemaker-scikit-learn-2021-09-01-08-05-33-229\n    ```", "```py\n    script = 'sklearn-boston-housing.py'\n    script_archive = 's3://{}/{}/source/source.tar.gz'.\n                     format(bucket, prefix)\n    ```", "```py\n    %%sh -s \"$script\" \"$script_archive\"\n    tar cvfz source.tar.gz $1\n    aws s3 cp source.tar.gz $2\n    ```", "```py\n    import time\n    model_name = prefix+'-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n    response = sm.create_model(\n      ModelName = model_name,\n      ExecutionRoleArn = role,\n      Containers = [{\n        'Image': sk.image_uri,\n        'ModelDataUrl':'s3://{}/{}/models/'.format(bucket, \n                        prefix),\n        'Mode': 'MultiModel',\n        'Environment': {\n            'SAGEMAKER_PROGRAM' : script,\n            'SAGEMAKER_SUBMIT_DIRECTORY' : script_archive\n        }\n      }]\n    )\n    ```", "```py\n    epc_name = prefix+'-epc'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n    response = sm.create_endpoint_config(\n        EndpointConfigName = epc_name,\n        ProductionVariants=[{\n            'InstanceType': 'ml.m5.large',\n            'InitialInstanceCount': 1,\n            'InitialVariantWeight': 1,\n            'ModelName': model_name,\n            'VariantName': 'variant-1'}]\n    )\n    ```", "```py\n    ep_name = prefix+'-ep'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n    response = sm.create_endpoint(\n        EndpointName=ep_name,\n        EndpointConfigName=epc_name)\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from io import BytesIO\n    data = pd.read_csv('housing.csv')\n    payload = data[:10].drop(['medv'], axis=1)\n    buffer = BytesIO()\n    np.save(buffer, payload.values)\n    ```", "```py\n    smrt = boto3.client('runtime.sagemaker')\n    for j in jobs.keys():\n        model_name=j+'.tar.gz'\n        response = smrt.invoke_endpoint(\n            EndpointName=ep_name,\n            TargetModel=model_name,\n            Body=buffer.getvalue(),\n            ContentType='application/x-npy')\n        print(response['Body'].read())\n    ```", "```py\n    sm.delete_endpoint(EndpointName=ep_name)\n    sm.delete_endpoint_config(EndpointConfigName=epc_name)\n    ```", "```py\n    import time\n    endpoint_name = 'c5-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n    c5_predictor = ic.deploy(initial_instance_count=1,\n                             instance_type='ml.c5.large',\n                             endpoint_name=endpoint_name,\n                             wait=False)\n    endpoint_name = 'g4-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n    g4_predictor = ic.deploy(\n        initial_instance_count=1,\n        instance_type='ml.g4dn.xlarge',\n        endpoint_name=endpoint_name,\n        wait=False)\n    ```", "```py\n    with open(file_name, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    def predict_images(predictor, iterations=1000):\n        total = 0\n        for i in range(0, iterations):\n            tick = time.time()\n            response = runtime.invoke_endpoint(\n                EndpointName=predictor.endpoint_name,                                 \n                ContentType='application/x-image',\n                Body=payload)\n            tock = time.time()\n            total += tock-tick\n        return total/iterations\n    predict_images(c5_predictor)\n    predict_images(g4_predictor)\n    ```", "```py\n    endpoint_name = 'c5-medium-'\n       +time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n    c5_medium_predictor = ic.deploy(\n        initial_instance_count=1,\n        instance_type='ml.c5.large',\n        accelerator_type='ml.eia2.medium',\n        endpoint_name=endpoint_name,\n        wait=False)\n    predict_images(c5_medium_predictor)\n    ```", "```py\n    ic.set_hyperparameters(\n        num_layers=18,                       \n        use_pretrained_model=0,\n        num_classes=2\n        num_training_samples=22500,\n        mini_batch_size=128,\n        precision_dtype='float16',\n        epochs=10)                   \n    ```", "```py\n    ic_predictor = ic.deploy(initial_instance_count=1,\n        instance_type='ml.c5.4xlarge',                         \n        endpoint_name=ic_endpoint_name)\n    ```", "```py\n    output_path = 's3://{}/{}/output-neo/'\n                  .format(bucket, prefix)\n    ic_neo_model = ic.compile_model(\n        target_instance_family='ml_c5',\n        input_shape={'data':[1, 3, 224, 224]},\n        role=role,\n        framework='mxnet',\n        framework_version='1.5.1',\n        output_path=output_path)\n    ```", "```py\n    ic_neo_model.image = get_image_uri(\n        session.boto_region_name, \n        'image-classification-neo', \n        repo_version='latest')\n    ic_neo_predictor = ic_neo_model.deploy(\n        endpoint_name=ic_neo_endpoint_name,\n        initial_instance_count=1,\n        instance_type='ml.c5.4xlarge')\n    ```", "```py\n    predict_images(ic_predictor)\n    predict_images(ic_neo_predictor)\n    ```", "```py\n$ aws s3 ls s3://sagemaker-eu-west-1-123456789012/dogscats/output-neo/\nmodel-ml_c5.tar.gz\n```", "```py\n$ aws s3 cp s3://sagemaker-eu-west-1-123456789012/dogscats/output-neo/model-ml_c5.tar.gz .\n$ tar xvfz model-ml_c5.tar.gz\ncompiled.meta\nmodel-shapes.json\ncompiled.params\ncompiled_model.json\ncompiled.so\n```", "```py\n$ file compiled.so\ncompiled.so: ELF 64-bit LSB shared object, x86-64\n$ nm compiled.so | grep conv | head -3\n0000000000005880 T fused_nn_contrib_conv2d_NCHWc\n00000000000347a0 T fused_nn_contrib_conv2d_NCHWc_1\n0000000000032630 T fused_nn_contrib_conv2d_NCHWc_2\n```", "```py\n    output_path = 's3://{}/{}/output-neo/'\n                  .format(bucket, prefix)\n    ic_neo_model = ic.compile_model(\n        target_instance_family='rasp3b',\n        input_shape={'data':[1, 3, 224, 224]},\n        role=role,\n        framework='mxnet',\n        framework_version='1.5.1',\n        output_path=output_path)\n    ```", "```py\n    $ aws s3 cp s3://sagemaker-eu-west-1-123456789012/dogscats/output-neo/model-rasp3b.tar.gz .\n    $ scp model-rasp3b.tar.gz pi@raspberrypi:~\n    ```", "```py\n    $ mkdir resnet50\n    $ tar xvfz model-rasp3b.tar.gz -C resnet50\n    ```", "```py\n    $ wget https://neo-ai-dlr-release.s3-us-west-2.amazonaws.com/v1.9.0/rasp3b/dlr-1.9.0-py3-none-any.whl \n    $ pip3 install dlr-1.9.0-py3-none-any.whl\n    ```", "```py\n    import numpy as np\n    from PIL import Image\n    def process_image(filename):\n        image = Image.open(filename)\n        image = image.resize((224,224))   \n        image = np.asarray(image)         # (224,224,3)\n        image = np.moveaxis(image, 2, 0). # (3,224,224)\n        image = image[np.newaxis, :].     # (1,3,224,224)\n        return image\n    ```", "```py\n    from dlr import DLRModel\n    model = DLRModel('resnet50')\n    ```", "```py\n    image = process_image('dog.jpg')\n    #image = process_image('cat.png')\n    input_data = {'data': image}\n    ```", "```py\n    import time\n    total = 0\n    for i in range(0,100):\n        tick = time.time()\n        out = model.run(input_data)\n        print(out[0])\n        tock = time.time()\n        total+= tock-tick\n    print(total)\n    ```"]
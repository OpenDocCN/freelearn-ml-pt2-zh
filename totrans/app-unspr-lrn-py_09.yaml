- en: '*Chapter 9*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hotspot Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand some of the applications of spatial modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy hotspot models in the appropriate context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build kernel density estimation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform hotspot analysis and visualize the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will learn about kernel density estimation and learn how
    to perform hotspot analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s consider an imaginary scenario: a new disease has begun spreading through
    numerous communities in the country that you live in and the government is trying
    to figure out how to confront this health emergency. Critical to any plan to confront
    this health emergency is epidemiological knowledge, including where the patients
    are located and how the disease is moving. The ability to locate and quantify
    problem areas (which are classically referred to as hotspots) can help health
    professionals, policy makers, and emergency response teams craft the most effective
    and efficient strategies for combating the disease. This scenario highlights one
    of the many applications of hotspot modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hotspot modeling is an approach that is used to identify how a population is
    distributed across a geographical area; for example, how the population of individuals
    infected with the previously mentioned disease is spread across the country. The
    creation of this distribution relies on the availability of representative sample
    data. Note that the population can be anything definable in geographical terms,
    which includes, but is not limited to, crime, disease-infected individuals, people
    with certain demographic characteristics, or hurricanes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: A fabricated example of fire location data showing some potential
    hotspots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.1: A fabricated example of fire location data showing some potential
    hotspots'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hotspot analysis is incredibly popular, and this is mainly because of how easy
    it is to visualize the results and to read and interpret the visualizations. Newspapers,
    websites, blogs, and TV shows all leverage hotspot analysis to support the arguments,
    chapters, and topics included in them or on them. While it might not be as well-known
    as the most popular machine learning models, the main hotspot analysis algorithm,
    known as **kernel density estimation**, is arguably one of the most widely used
    analytical techniques. Kernel density estimation is a hotspot analysis technique
    that is used to estimate the true population distribution of specific geographical
    events.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Spatial statistics** is a branch of statistics that focuses on the analysis
    of data that has spatial properties, including geographic or topological coordinates.
    It is similar to time series analysis in that the goal is to analyze data that
    changes across some dimension. In the case of time series analysis, the dimension
    across which the data changes is time, whereas in the spatial statistics case,
    the data changes across the spatial dimension. There are a number of techniques
    that are included under the spatial statistics umbrella, but the technique we
    are concerned with here is kernel density estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: As is the goal of most statistical analyses, in spatial statistics, we are trying
    to take samples of geographic data and use them to generate insights and make
    predictions. The analysis of earthquakes is one arena in which spatial statistical
    analyses are commonly deployed. By collecting earthquake location data, maps that
    identify areas of high and low earthquake likelihood can be generated, which can
    help scientists determine both where future earthquakes are likely to occur and
    what to expect in terms of intensity.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Density Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kernel density estimation uses the idea of the **probability density function**
    (**PDF**), which is one of the foundational concepts in statistics. The probability
    density function is a function that describes the behavior of a continuous **random
    variable**. That is, it expresses the likelihood, or probability, that the random
    variable takes on some range of values. Consider the heights of males in the United
    States as an example. By using the probability density function of the heights
    of males in the United States, we can determine the probability that some United
    States-based male is between 1.9 and 1.95 meters tall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2: The standard normal distribution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.2: The standard normal distribution'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Possibly the most popular density function in statistics is the standard normal
    distribution, which is simply the normal distribution centered at zero with the
    standard deviation equal to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of the density function, what is typically available to statisticians
    or data scientists are randomly collected sample values coming from a population
    distribution that is unknown. This is where kernel density estimation comes in;
    it is a technique that is used for estimating the unknown probability density
    function of a random variable using sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3: A mixture of three normal distributions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.3: A mixture of three normal distributions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using Hotspot Analysis in Business
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already mentioned some of the ways in which hotspot modeling can be
    leveraged to meaningfully impact industry. The following use cases are common
    applications of hotspot modeling.
  prefs: []
  type: TYPE_NORMAL
- en: When reporting on infectious diseases, health organizations and media companies
    typically use hotspot analysis to communicate where the diseases are located and
    the likelihood of contracting the disease based on geographic location. Using
    hotspot analysis, this information could be reliably computed and disseminated.
    Hotspot analysis is great for dealing with health data because the visualizations
    are very straightforward. This means that the chances of data being misinterpreted
    either intentionally or unintentionally are relatively low.
  prefs: []
  type: TYPE_NORMAL
- en: Hotspot analysis can also be used to predict where certain events are likely
    to occur geographically. One research area that is leveraging the predictive capabilities
    of hotspot analysis more and more is the environmental sciences, which includes
    the study of natural disasters and extreme weather events. Earthquakes, for example,
    are notorious for being difficult to predict, because the time between significant
    earthquakes can be large, and the machinery needed to track and measure earthquakes
    to the degree required to make these predictions is relatively new.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of public policy and resource deployment, hotspot analysis can be very
    impactful when dealing with the analysis of population demographics. Determining
    where resources, both monetary and personnel, should be deployed can be challenging;
    however, given that resources are often demographic-specific, hotspot analysis
    is a useful technique since it can be used to determine the distribution of certain
    demographic characteristics. By demographic characteristics we mean that we could
    find the geographic distribution of high school graduates, immigrants from a specific
    global region, or individuals making $100,000 or more annually.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Density Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main methodological approaches to hotspot analysis is kernel density
    estimation. Kernel density estimation builds an estimated density using sample
    data and two parameters known as the **kernel function** and the **bandwidth value**.
    The estimated density is, like any distribution, essentially a guideline for the
    behavior of a random variable. Here, we mean how frequently the random variable
    takes on any specific value, ![](img/C12626_09_Formula_01.png). When dealing with
    hotspot analysis where the data is typically geographic, the estimated density
    answers the question *How frequently do specific longitude and latitude pairs
    appear?*. If a specific longitude and latitude pair, ![](img/C12626_09_Formula_02.png),
    and other nearby pairs occur with high frequency, then the estimated density built
    using the sample data will be expected to show that the area around the longitude
    and latitude pair has a high likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel density estimation is referred to as a smoothing algorithm, because the
    process of estimating a density is the process of estimating the underlying shape
    of the data by disregarding the eccentricities and anomalies in the sample data.
    Stated another way, kernel density estimation removes the noise from the data.
    The only assumption of the model is that the data truly belongs to some interpretable
    and meaningful density from which insights can be derived and acted upon. That
    is, there exists a true underlying distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, more than any other topic in this book, kernel density estimation
    embodies the basic idea of statistics, which is to use sample data of finite size
    to make inferences about the population. We assume that the sample data contains
    clusters of data points and that these clusters imply regions of high likelihood
    in the true population. A benefit of creating a quality estimate of the true population
    density is that the estimated density can then be used to sample more data from
    the population.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this brief introduction, you probably have the following two questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the bandwidth value?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the kernel function?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We answer both of these questions next.
  prefs: []
  type: TYPE_NORMAL
- en: The Bandwidth Value
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most crucial parameter in kernel density estimation is called the **bandwidth
    value** and its impact on the quality of the estimate cannot be overestimated.
    A high-level definition of the bandwidth value is that it is a value that determines
    the degree of smoothing. If the bandwidth value is low, then the estimated density
    will feature limited smoothing, which means that the density will capture all
    the noise in the sample data. If the bandwidth value is high, then the estimated
    density will be very smooth. An overly smooth density will remove characteristics
    of the true density from the estimated density, which are legitimate and not simply
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: In more statistical or machine learning languages, the bandwidth parameter controls
    the bias-variance trade-off. That is, high variance is the result of low bandwidth
    values because the density is sensitive to the variance of the sample data. Low
    bandwidth values limit any ability the model may have had to adapt to and work
    around gaps in the sample data that are not present in the population. Densities
    estimated using low bandwidth values tend to overfit the data (this is also known
    as under-smoothed densities). When high bandwidth values are used, then the resulting
    density is underfit and the estimated density has a high bias (this is also known
    as over-smoothed densities).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 46: The Effect of the Bandwidth Value'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will fit nine different models with nine different bandwidth
    values to sample data created in the exercise. The goal here is to solidify our
    understanding of the impact the bandwidth parameter can have and make clear that
    if an accurate estimated density is sought, then the bandwidth value needs to
    be selected with care. Note that finding an optimal bandwidth value will be the
    topic of the next section. All exercises will be done in a Jupyter notebook utilizing
    Python 3; ensure that all package installation is done using `pip`. The easiest
    way to install the `basemap` module from `mpl_toolkits` is by using *Anaconda*.
    Instructions for downloading and installing *Anaconda* can be found at the beginning
    of this book:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load all of the libraries that are needed for the exercises in this chapter.
    Here, the `matplotlib` library is used to create basic graphics; the `basemap`
    library is used to create graphics involving location data; the `numpy` library
    is used for working with arrays and matrices; the `pandas` library is used for
    working with DataFrames; the `scipy` library is used for scientific computing
    in Python; the `seaborn` library is used for creating much more attractive and
    complicated graphics; and the `sklearn` library is used to access data, manipulate
    the data, and run models. Additionally, ensure that the graphics be run inline
    and set to `seaborn`, so that all graphics appear as `seaborn` graphics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create some sample data (`vals`) by mixing three normal distributions. In addition
    to the sample data, define the true density curve (`true_density`) and the range
    over which the data will be plotted (`x_vec`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a list of tuples that will guide the creation of the multiplot graphic.
    Each tuple contains the row and column indices of the specific subplot, and the
    bandwidth value used to create the estimated density in that particular subplot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create nine plots each using a different bandwidth value. The first plot, with
    the index of (0, 0), will have the lowest bandwidth and the last plot, with the
    index of (2, 2), will have the highest bandwidth. These values are not the absolute
    lowest or absolute highest bandwidth values, rather they are only the minimum
    and maximum of the list defined in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.4: A 3 x 3 matrix of subplots; each of which features an estimated
    density created using one of nine bandwidth values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.4: A 3 x 3 matrix of subplots; each of which features an estimated
    density created using one of nine bandwidth values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that on the lower end, the density curves clearly overfit the data. As
    the bandwidth values increase, the estimated density becomes smoother until it
    noticeably underfits the data. Visually, it looks like the optimal bandwidth may
    be around 1.6.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to design an algorithm to identify the optimal bandwidth value,
    so that the estimated density is the most reasonable and, therefore, the most
    reliable and actionable.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Optimal Bandwidth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned in the previous exercise, we can come quite close to selecting
    the optimal bandwidth by simply comparing several densities visually. However,
    this is neither the most efficient method of selecting parameter values nor the
    most reliable.
  prefs: []
  type: TYPE_NORMAL
- en: There are two standard approaches to optimizing the bandwidth value, and both
    of these will appear in future exercises and activities. The first approach is
    a plug-in method (or a formulaic approach) that is deterministic and not optimized
    on the sample data. Plug-in methods are generally much faster to implement, simpler
    to code, and easier to explain. However, these methods have one big downside,
    which is that their accuracy tends to suffer compared to approaches that are optimized
    on the sample data. These methods also have distributional assumptions. The most
    popular plug-in methods are Silverman's Rule and Scott's Rule. By default, the
    `seaborn` package (which will be used in future exercises) uses Scott's Rule as
    the method to determine the bandwidth value.
  prefs: []
  type: TYPE_NORMAL
- en: The second, and arguably the more robust, approach to finding an optimal bandwidth
    value is by searching a predefined grid of bandwidth values. Grid search is an
    empirical approach that is used frequently in machine learning and predictive
    modeling to optimize model hyperparameters. The process starts by defining the
    bandwidth grid, which is simply the collection of bandwidth values to be evaluated.
    Use each bandwidth value in the grid to create an estimated density; then, score
    the estimated density using the pseudo-log-likelihood value. The optimal bandwidth
    value is that which has the maximum pseudo-log-likelihood value. Think of the
    pseudo-log-likelihood value as the result of the probability of getting data points
    where we did get data points and the probability of not getting points where we
    did not get any data points. Ideally, both of these probabilities would be large.
    Consider the case where the probability of getting data points where we did get
    points is low. In this situation, the implication would be that the data points
    in the sample were anomalous because, under the true distribution, getting points
    where we did would not be expected with a high likelihood value.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now implement the grid search approach to optimize the bandwidth value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 47: Selecting the Optimal Bandwidth Using Grid Search'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will create an estimated density for the sample data created
    in *Exercise 46*, *The Effect of the Bandwidth Value* with an optimal bandwidth
    value identified using grid search and cross-validation. To run the grid search
    with cross-validation, we will leverage `sklearn`, which we have used throughout
    this book. This exercise is a continuation of Exercise 1 as we are using the same
    sample data and continuing our exploration of the bandwidth value:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a grid of bandwidth values and the grid search cross-validation model.
    Ideally, the leave-one-out approach to cross-validation should be used, but for
    the sake of having the model run in a reasonable amount of time, we will do a
    10-fold cross-validation. Fit the model on the sample data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the optimal bandwidth value from the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The optimal bandwidth value should be approximately two. We can interpret the
    optimal bandwidth value as the bandwidth value producing the maximum pseudo-log-likelihood
    value. Note that depending on the values included in the grid, the optimal bandwidth
    value can change.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the histogram of the sample data overlaid by both the true and estimated
    densities. In this case, the estimated density will be the optimal estimated density:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.5: A histogram of the random sample with the true density and the
    optimal estimated density overlaid'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.5: A histogram of the random sample with the true density and the
    optimal estimated density overlaid'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The estimated density is neither overfit or underfit to any noticeable degree
    and it definitely captures the three clusters. Arguably, it could map to the true
    density better, but this is just an estimated density generated by a model that
    has limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now move onto the second question: what is the kernel function and what
    role does it play?'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The other parameter to be set is the kernel function. The kernel is a non-negative
    function that controls the shape of the density. Like topic models, we are working
    in a non-negative environment because it does not make sense to have negative
    likelihoods or probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel function controls the shape of the estimated density by weighting
    the points in a systematic way. This systematic methodology for weighting is fairly
    simple; data points that are in close proximity to many other data points are
    up-weighted, whereas data points that are alone or far away from any other data
    points are down-weighted. Up-weighted data points will correspond to points of
    higher likelihood in the final estimated density.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many functions can be used as kernels, but six frequent choices are Gaussian,
    Tophat, Epanechnikov, Exponential, Linear, and Cosine. Each of these functions
    represents a unique distributional shape. Note that in each of the formulas the
    parameter, *h*, represents the bandwidth value:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaussian:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.6: The formula for the Gaussian kernel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.6: The formula for the Gaussian kernel'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Tophat:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.7: The formula for the Tophat kernel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.7: The formula for the Tophat kernel'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Epanechnikov:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.8: The formula for the Epanechnikov kernel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.8: The formula for the Epanechnikov kernel'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exponential:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.9: The formula for the Exponential kernel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.9: The formula for the Exponential kernel'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Linear:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.10: The formula for the Linear kernel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.10: The formula for the Linear kernel'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Cosine:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.11: The formula for the Cosine kernel'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.11: The formula for the Cosine kernel'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the distributional shapes of the six kernel functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12: The general shapes of the six kernel functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.12: The general shapes of the six kernel functions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The choice of kernel function is not completely insignificant, but it is definitely
    not nearly as important as the choice of bandwidth value. A reasonable book of
    action would be to use the gaussian kernel for all density estimation problems,
    which is what we will do in the following exercises and activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 48: The Effect of the Kernel Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of this exercise is to understand how the choice of kernel function
    affects the quality of the density estimation. Like we did when exploring the
    bandwidth value effect, we will hold all other parameters constant, use the same
    data generated in the first two exercises, and run six different kernel density
    estimation models using the six kernel functions previously specified. Clear differences
    should be noticeable between the six estimated densities, but these differences
    should be slightly less dramatic than the differences between the densities estimated
    using the different bandwidth values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a list of tuples along the same lines as the one defined previously.
    Each tuple includes the row and column indices of the subplot, and the kernel
    function to be used to create the density estimation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.13: A 3 x 2 matrix of subplots, each of which features an estimated
    density'
  prefs: []
  type: TYPE_NORMAL
- en: created using one of six kernel functions
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.13: A 3 x 2 matrix of subplots, each of which features an estimated
    density created using one of six kernel functions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Out of the six kernel functions, the gaussian kernel produced the most reasonable
    estimated density. Beyond that, notice that the difference between the estimated
    densities with different kernels is less than the difference between the estimated
    densities with different bandwidth values. This goes to the previously made claim
    that the bandwidth value is the more important parameter and should be the focus
    during the model building process.
  prefs: []
  type: TYPE_NORMAL
- en: With our understanding mostly formed, let's discuss the derivation of kernel
    density estimation in a high-level fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Density Estimation Derivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's skip the formal mathematical derivation in favor of the popular derivation
    by intuition. Kernel density estimation turns each data point in the sample into
    its own distribution whose width is controlled by the bandwidth value. The individual
    distributions are then summed to create the desired density estimate. This concept
    is fairly easy to demonstrate; however, before doing that in the next exercise,
    let's try to think through it in an abstract way. For geographic regions containing
    many sample data points, the individual densities will overlap and, through the
    process of summing those densities, will create points of high likelihood in the
    estimated density. Similarly, for geographic regions containing few to no sample
    data points, the individual densities will not overlap and, therefore, will correspond
    to points of low likelihood in the estimated density.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 49: Simulating the Derivation of Kernel Density Estimation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal here is to demonstrate the concept of summing individual distributions
    to create an overall estimated density for a random variable. We will establish
    the concept incrementally by starting with one sample data point and then work
    up to many sample data points. Additionally, different bandwidth values will be
    applied, so our understanding of the effect of the bandwidth value on these individual
    densities will solidify further:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function that will evaluate the normal distribution. The input values
    are the grid representing the range of the random variable, *X*, the sampled data
    point, *m*, and the bandwidth, *b*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot a single sample data point as a histogram and as an individual density
    with varying bandwidth values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.14: Showing one data point and its individual density at various
    bandwidth values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.14: Showing one data point and its individual density at various bandwidth
    values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we see what has already been established, which is that lower bandwidth
    values produce very narrow densities that tend to overfit the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Reproduce the work done in *Step 2*, but now scale up to 16 data points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.15: Showing 16 data points, their individual densities at various  andwidth
    values, and the sum of their individual densities'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.15: Showing 16 data points, their individual densities at various
    bandwidth values, and the sum of their individual densities'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Again, unsurprisingly, the plot utilizing the smallest bandwidth value features
    a wildly overfitted estimated density. That is, the estimated density captures
    all the noise in the sample data. Of these three densities, the second one, where
    the bandwidth value was set to 0.35, is the most reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 21: Estimating Density in One Dimension'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this first activity, we will be generating some fake sample data and estimating
    the density function using kernel density estimation. The bandwidth value will
    be optimized using grid search cross-validation. The goal is to solidify our understanding
    of this useful methodology by running the model in a simple one-dimension case.
    We will once again leverage Jupyter notebooks to do our work.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that the sample data we will be creating describes the price of homes
    in a state in the United States. Momentarily ignore the values in the following
    sample data. The question is, *What does the distribution of home prices look
    like, and can we extract the probability of a house having a price that falls
    in some specific range?* These questions and more are answerable using kernel
    density estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new notebook and install all the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample 1,000 data points from the standard normal distribution. Add 3.5 to each
    of the last 625 values of the sample (that is, the indices between 375 and 1,000).
    Set a random state of 100\. To do this, set a random state of 100 using `numpy.random.RandomState`
    to guarantee the same sampled values, and then randomly generate the data points
    using the `randn(1000)` call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the 1,000-point sample data as a histogram and add a scatterplot below
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a grid of bandwidth values. Then, define and fit a grid search cross-validation
    algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the optimal bandwidth value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replot the histogram from *Step 3* and overlay the estimated density.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.16: A histogram of the random sample with the optimal estimated
    density overlaid'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.16: A histogram of the random sample with the optimal estimated density
    overlaid'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 374.
  prefs: []
  type: TYPE_NORMAL
- en: Hotspot Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start, hotspots are areas of higher concentrations of data points, such as
    particular neighborhoods where the crime rate is abnormally high or swaths of
    the country that are impacted by an above-average number of tornadoes. Hotspot
    analysis is the process of finding these hotspots, should any exist, in a population
    using sampled data. This process is generally done by leveraging kernel density
    estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hotspot analysis can be described in four high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect the data**: The data should include the locations of the objects
    or events. As we have briefly mentioned, the amount of data needed to run and
    achieve actionable results is relatively flexible. The optimal state is to have
    a sample dataset that is representative of the population.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Identify the base map**: The next step is to identify which base map would
    best suit the analytical and presentational needs of the project. On this base
    map, the results of the model will be overlaid, so that the locations of the hotspots
    can be easily articulated in much more digestible terms, such as city, neighborhood,
    or region.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Execute the model**: In this step, you select and execute one or multiple
    methodologies of extracting spatial patterns to identify hotspots. For us, this
    method will be – no surprise – kernel density estimation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create the visualization**: The hotspot maps are generated by overlaying
    the model results on the base map to support whatever business questions are outstanding.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the principal issues with hotspot analysis from a usability standpoint
    is that the statistical significance of a hotspot is not particularly easy to
    ascertain. Most questions about statistical significance revolve around the existence
    of the hotspots. That is, do the fluctuations in likelihood of occurrence actually
    amount to statistically significant fluctuations? It is important to note that
    statistical significance is not required to perform kernel density estimation
    and that we will not be dealing with significance at all going forward.
  prefs: []
  type: TYPE_NORMAL
- en: While the term hotspot is traditionally reserved to describe a cluster of location
    data points, it is not limited to location data. Any data type can have hotspots
    regardless of whether or not they are referred to as hotspots. In one of the following
    exercises, we will model some non-location data to find hotspots, which will be
    regions of feature space having a high or low likelihood of occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 50: Loading Data and Modeling with Seaborn'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will work with the `seaborn` library to fit and visualize
    kernel density estimation models. This is done on both location and non-location
    data. Before getting into the modeling, we load the data, which is the California
    housing dataset that is automatically loaded with `sklearn`. Taken from the United
    States census in 1990, this dataset describes the housing situation in California
    during that time. One row of data describes one census block group. The definition
    of a census block group is irrelevant to this exercise, so we will bypass the
    definition here in favor of more hands-on coding and modeling. It is important
    to mention that all the variables are aggregated to the census block. For example,
    `MedInc` is the median income of households in each census block. Additional information
    on this dataset is available at [https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset](https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the California housing dataset using `fetch_california_housing()`. Convert
    the data to a DataFrame using `pandas` and print the first five rows of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.17: The first five rows of the California housing dataset from sklearn'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.17: The first five rows of the California housing dataset from sklearn'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Filter the DataFrame based on the `HouseAge` feature, which is the median home
    age of each census block. Keep only the rows with `HouseAge` less than or equal
    to 15 and name the DataFrame `dfLess15`. Print out the first five rows of the
    DataFrame; Then, reduce the DataFrame down to just the longitude and latitude
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.18: The first five rows of the dataset filtered down to those rows
    that have a value of 15 or less in the HouseAge column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.18: The first five rows of the dataset filtered down to those rows
    that have a value of 15 or less in the HouseAge column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use `seaborn` to fit and visualize the kernel density estimation model built
    on the longitude and latitude data points. The `seaborn` approach to fitting these
    models uses Scott''s Rule. There are four inputs to the model, which are the names
    of the two columns over which the estimated density is sought (that is, the longitude
    and latitude), the DataFrame to which those columns belong, and the method of
    density estimation (that is, the `kde` or kernel density estimation):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.19: A joint plot containing both the two-dimensional estimated density
    plus the marginal densities for the dfLess15 dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.19: A joint plot containing both the two-dimensional estimated density
    plus the marginal densities for the dfLess15 dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: If we overlay these results on a map of California, we will see that the hotspots
    are southern California, including Los Angeles and San Diego, the bay area, including
    San Francisco, and to a small degree the area known as the central valley. A benefit
    of this `seaborn` graphic is that we get the two-dimensional estimated density
    and the marginal densities for both longitude and latitude.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create another filtered DataFrame based on the `HouseAge` feature; this time
    keep only the rows with `HouseAge` greater than 40 and name the DataFrame `dfMore40`.
    Additionally, remove all the columns except for longitude and latitude. Then,
    print the first five rows of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.20: The top of the dataset filtered to the rows containing values
    greater than 40 in the HouseAge column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.20: The top of the dataset filtered to the rows containing values
    greater than 40 in the HouseAge column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Repeat the process from *Step 3*, but now using this new filtered DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.21: A joint plot containing both the two-dimensional estimated density
    plus the marginal densities for the dfMore40 dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.21: A joint plot containing both the two-dimensional estimated density
    plus the marginal densities for the dfMore40 dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This estimated density is much more compact in that the data is clustered almost
    entirely in two areas. Those areas are Los Angeles and the bay area. Comparing
    this to the plot in *Step 3*, we notice that housing development has spread out
    across the state. Additionally, newer housing developments occur with much higher
    frequencies in a larger number of census blocks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s again create another filtered DataFrame. This time only keeping rows
    where `HouseAge` is less than or equal to five and name the DataFrame `dfLess5`.
    Plot `Population` and `MedInc` as a scatterplot, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.22: A scatterplot of the median income against population for values
    of five or less in the HouseAge column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.22: A scatterplot of the median income against population for values
    of five or less in the HouseAge column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use yet another `seaborn` function to fit a kernel density estimation model.
    Again, the optimal bandwidth is found using Scott''s Rule. Replot the histogram
    and overlay the estimated density, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.23: The same scatterplot as created in Step 6 with the estimated
    density overlaid'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.23: The same scatterplot as created in Step 6 with the estimated density
    overlaid'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, the estimated density shows that census blocks with smaller populations
    have lower median incomes at higher likelihoods than they have high median incomes.
    The point of this step is to showcase how kernel density estimation can be used
    on non-location data.
  prefs: []
  type: TYPE_NORMAL
- en: When presenting the results of hotspot analysis, some type of map should be
    involved since hotspot analysis is generally done on location data. Acquiring
    maps on which estimated densities can be overlaid is not an easy process. Due
    to copyright issues, we will use very basic maps, called basemaps, on which we
    can overlay our estimated densities. It will be left to you to extend the knowledge
    you acquire in this chapter to fancier and more detailed maps. Mapping environments
    can also be complicated and time-consuming to download and install.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 51: Working with Basemaps'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This exercise leverages the `basemap` module of `mpl_toolkits`. `basemap` is
    a mapping library, which can be used to create basic maps or outlines of geographic
    regions. These maps can have the results of kernel density estimation overlaid,
    so that we can clearly see where the hotspots are located.
  prefs: []
  type: TYPE_NORMAL
- en: First, check whether `basemap` is installed by running `import mpl_toolkits.basemap`
    in a Jupyter notebook. If it loads without error, then you are ready and need
    to take no further action. If the call fails, then install `basemap` using `pip`
    by running `python3 -m pip install basemap`. You should be good to go after restarting
    any already-open notebooks. Note that the `pip` installation will only work if
    Anaconda is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this exercise is to remodel and replot the location data from *Exercise
    50*, *Loading Data and Modeling with Seaborn*, using the kernel density estimation
    functions of `sklearn` and the mapping capabilities of `basemap`. Extract the
    longitude and latitude values from the filtered DataFrame called `dfLess15`, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Form the grid of locations over which the estimated density will be laid. The
    grid of locations is the two-dimensional location equivalent of the one-dimensional
    vector defining the range of the random variable in Exercise 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.24: The x and y components of the grid representing the dfLess15
    dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.24: The x and y components of the grid representing the dfLess15 dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Define and fit a kernel density estimation model. Set the bandwidth value to
    0.05 in order to save runtime; then, create likelihood values for each point on
    the location grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that if you print out the shape of the likelihood values, it is 3,287
    rows by 3,287 columns, which is 10,804,369 likelihood values. This is the same
    number of values in the preestablished longitude and latitude grid, called `xy15`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create an outline of California and overlay the estimated density computed
    in *Step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.25: The estimated density of dfLess15 overlaid onto an outline of
    California'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.25: The estimated density of dfLess15 overlaid onto an outline of
    California'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The 0.05 value was set to purposefully overfit the data slightly. You'll notice
    that instead of the larger clusters that make up the density in *Exercise 50,
    Loading Data and Modeling with Seaborn* the estimated density here is made up
    of much smaller clusters. This slightly overfit density might be a bit more helpful
    than the previous version of the density because it gives you a clearer view of
    where the high likelihood census blocks are truly located. One of the high-likelihood
    areas in the previous density was southern California, but southern California
    is a huge area with an enormous population and many municipalities. Bear in mind
    that when using the results for business decisions, certain levels of specificity
    might be required and should be provided if the sample data can support results
    with that level of specificity or granularity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Repeat *Step 1*, but with the `dfMore40` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.26: The x and y components of the grid representing the dfMore40
    dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_09_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.26: The x and y components of the grid representing the dfMore40 dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Repeat *Step 2* using the grid established in *Step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat *Step 3* using the estimated density computed in *Step 5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.27: The estimated density of dfMore40 overlaid onto an outline of
    California'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_09_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.27: The estimated density of dfMore40 overlaid onto an outline of
    California'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This estimated density is again a redo of the one that we did in *Exercise 50*,
    *Loading Data and Modeling with Seaborn*. While the density from *Step 3* will
    provide more detail for a person interested in real estate or the census, this
    density does not actually look that different from its corollary density in *Exercise
    50*, *Loading Data and Modeling with Seaborn*. The clusters are primarily around
    Los Angeles and San Francisco with almost no points anywhere else.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 22: Analyzing Crime in London'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will perform hotspot analysis with kernel density estimation
    on London crime data from [https://data.police.uk/data/](https://data.police.uk/data/).
    Due to the difficulties of working with map data, we will visualize the results
    of the analysis using `seaborn`. However, if you feel brave and were able to run
    all the plots in *Exercise 51*, *Working with Basemaps* you are encouraged to
    try using maps.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation for performing hotspot analysis on this crime data is two-fold.
    We are asked first to determine where certain types of crimes are occurring in
    high likelihood, so that police resources can be allocated for maximum impact.
    Then, as a follow up, we are asked to ascertain whether the hotspots for certain
    types of crime are changing over time. Both of these questions are answerable
    using kernel density estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is downloaded from [https://data.police.uk/data/](https://data.police.uk/data/).
  prefs: []
  type: TYPE_NORMAL
- en: You can download it from the Packt GitHub at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson09/Activity21-Activity22](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson09/Activity21-Activity22).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, to download the data directly from the source, go to the preceding
    police website, check the box for **Metropolitan Police Service**, and then set
    the date range to **July 2018** to **Dec 2018**. Next, click **Generate file**
    followed by **Download now** and name the downloaded file **metro-jul18-dec18**.
    Make sure that you know how or can retrieve the path to the downloaded directory.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains public sector information licensed under the Open Government
    License v3.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the crime data. Use the path where you saved the downloaded directory,
    create a list of the year-month tags, use the `read_csv` command to load the individual
    files iteratively, and then concatenate these files together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print diagnostics of the complete (six months) and concatenated dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subset the DataFrame down to four variables (`Longitude`, `Latitude`, `Month`,
    and `Crime type`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `jointplot` function from `seaborn`, fit and visualize three kernel
    density estimation models for bicycle theft in July, September, and December 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Step 4*; this time, use shoplifting crimes for the months of August,
    October, and November 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Step 5*; this time, use burglary crimes for the months of July, October,
    and December 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output from the last part of *Step 6* will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.28: The estimated joint and marginal densities for burglaries in
    December 20 ](img/C12626_09_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.28: The estimated joint and marginal densities for burglaries in December
    2018'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To clarify one more time, the densities found in this activity should have been
    overlaid on maps so that we could see exactly what areas these densities cover.
    Attempting to overlay the results on maps on your own would be encouraged if you
    have the appropriate mapping platforms at your disposal. If not, you could go
    to the mapping services available online and use the longitude and latitude pairs
    to gain insight into the specific locations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 377.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kernel density estimation is a classic statistical technique that is in the
    same family of techniques as the histogram. It allows the user to extrapolate
    out from sample data to make insights and predictions about the population of
    particular objects or events. This extrapolation comes in the form of a probability
    density function, which is nice because the results read as likelihoods or probabilities.
    The quality of this model is dependent on two parameters: the bandwidth value
    and the kernel function. As discussed, the most crucial component of leveraging
    kernel density estimation successfully is the setting of an optimal bandwidth.
    Optimal bandwidths are most frequently identified using grid search cross-validation
    with pseudo-log-likelihood as the scoring metric. What makes kernel density estimation
    great is both its simplicity and its applicability to so many fields.'
  prefs: []
  type: TYPE_NORMAL
- en: It is routine to find kernel density estimation models in criminology, epidemiology,
    meteorology, and real estate to only name a few. Regardless of your area of business,
    kernel density estimation should be applicable.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we explored the best practices for using unsupervised learning
    techniques in tandem with Python libraries and extracting meaningful information
    from unstructured data. Now you can confidently build your own models using Python.
  prefs: []
  type: TYPE_NORMAL

["```py\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom packtml.utils.plotting import plot_learning_curve\nfrom packtml.regression import SimpleLinearRegression\n%matplotlib inline\n\nboston = load_boston()\nplot_learning_curve(\n        model=SimpleLinearRegression, X=boston.data, y=boston.target,\n        metric=mean_squared_error, n_folds=3,\n        train_sizes=(50, 150, 250, 300),\n        seed=42, y_lim=(0, 45))\\\n    .show\n```", "```py\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error\nfrom packtml.utils.plotting import plot_learning_curve\nfrom packtml.decision_tree import CARTRegressor\n%matplotlib inline\n\nboston = load_boston()\nplot_learning_curve(\n        model=CARTRegressor, X=boston.data, y=boston.target,\n        metric=mean_squared_error, n_folds=3,\n        train_sizes=(25, 150, 225, 350),\n        seed=42, random_state=21, max_depth=50)\\\n    .show\n```", "```py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\n\nfrom packtml.decision_tree.metrics import gini_impurity, InformationGain\nimport numpy as np\n\n# #############################################################################\n# Build the example from the slides\ny = np.array([0, 0, 0, 1, 1, 1, 1])\nuncertainty = gini_impurity(y)\nprint(\"Initial gini impurity: %.4f\" % uncertainty)\n\n# now get the information gain of the split from the slides\ndirections = np.array([\"right\", \"left\", \"left\", \"left\",\n                       \"right\", \"right\", \"right\"])\nmask = directions == \"left\"\nprint(\"Information gain from the split we created: %.4f\"\n      % InformationGain(\"gini\")(target=y, mask=mask, uncertainty=uncertainty))\n```", "```py\nfrom packtml.decision_tree.metrics import gini_impurity, InformationGain\nimport numpy as np\n```", "```py\nfrom __future__ import absolute_import\n\nfrom packtml.decision_tree.cart import RandomSplitter\nfrom packtml.decision_tree.metrics import InformationGain\nimport numpy as np\n```", "```py\n# Build the example from the slides (3.3)\nX = np.array([[21, 3], [ 4, 2], [37, 2]])\ny = np.array([1, 0, 1])\n```", "```py\n# this is the splitting class; we'll use gini as the criteria\nrandom_state = np.random.RandomState(42)\nsplitter = RandomSplitter(random_state=random_state,\n                          criterion=InformationGain('gini'),\n                          n_val_sample=3)\n# find the best:\nbest_feature, best_value, best_gain = splitter.find_best(X, y)\nprint(\"Best feature=%i, best value=%r, information gain: %.3f\"\n      % (best_feature, best_value, best_gain))\n```", "```py\n# 1\\. metrics.InformationGain & metrics.VarianceReduction\n# 2\\. RandomSplitter\n# 3\\. LeafNode\n# 4\\. BaseCART\n```", "```py\n__all__ = [\n    'entropy',\n    'gini_impurity',\n    'InformationGain',\n    'VarianceReduction'\n]\n```", "```py\ndef entropy(y):\n    \"\"\"Compute the entropy of class labels.\n\n    This computes the entropy of training samples. A high entropy means\n    a relatively uniform distribution, while low entropy indicates a\n    varying distribution (many peaks and valleys).\n\n    References\n    ----------\n    .. [1] http://www.cs.csi.cuny.edu/~imberman/ai/Entropy%20and%20Information%20Gain.htm\n    \"\"\"\n    return _clf_metric(y, 'entropy')\n\ndef gini_impurity(y):\n    \"\"\"Compute the Gini index on a target variable.\n\n    The Gini index gives an idea of how mixed two classes are within a leaf\n    node. A perfect class separation will result in a Gini impurity of 0 (that is,\n    \"perfectly pure\").\n    \"\"\"\n    return _clf_metric(y, 'gini')\n```", "```py\ndef _clf_metric(y, metric):\n    \"\"\"Internal helper. Since this is internal, so no validation performed\"\"\"\n    # get unique classes in y\n    y = np.asarray(y)\n    C, cts = np.unique(y, return_counts=True)\n\n    # a base case is that there is only one class label\n    if C.shape[0] == 1:\n        return 0.\n\n    pr_C = cts.astype(float) / y.shape[0] # P(Ci)\n\n    # 1 - sum(P(Ci)^2)\n    if metric == 'gini':\n        return 1\\. - pr_C.dot(pr_C) # np.sum(pr_C ** 2)\n    elif metric == 'entropy':\n        return np.sum(-pr_C * np.log2(pr_C))\n\n    # shouldn't ever get to this point since it is internal\n    else:\n        raise ValueError(\"metric should be one of ('gini', 'entropy'), \"\n                         \"but encountered %s\" % metric)\n```", "```py\nclass BaseCriterion(object):\n    \"\"\"Splitting criterion.\n\n    Base class for InformationGain and VarianceReduction. WARNING - do\n    not invoke this class directly. Use derived classes only! This is a\n    loosely-defined abstract class used to prescribe a common interface\n    for sub-classes.\n    \"\"\"\n    def compute_uncertainty(self, y):\n        \"\"\"Compute the uncertainty for a vector.\n\n        A subclass should override this function to compute the uncertainty\n        (that is, entropy or gini) of a vector.\n        \"\"\"\n\nclass InformationGain(BaseCriterion):\n    \"\"\"Compute the information gain after a split.\n\n    The information gain metric is used by CART trees in a classification\n    context. It measures the difference in the gini or entropy before and\n    after a split to determine whether the split \"taught\" us anything.\n```", "```py\ndef __init__(self, metric):\n        # let fail out with a KeyError if an improper metric\n        self.crit = {'gini': gini_impurity,\n                     'entropy': entropy}[metric]\n```", "```py\ndef __call__(self, target, mask, uncertainty):\n        \"\"\"Compute the information gain of a split.\n\n        Parameters\n        ----------\n        target : np.ndarray\n            The target feature\n\n        mask : np.ndarray\n            The value mask\n\n        uncertainty : float\n            The gini or entropy of rows pre-split\n        \"\"\"\n        left, right = target[mask], target[~mask]\n        p = float(left.shape[0]) / float(target.shape[0])\n\n        crit = self.crit # type: callable\n        return uncertainty - p * crit(left) - (1 - p) * crit(right)\n```", "```py\nclass VarianceReduction(BaseCriterion):\n    \"\"\"Compute the variance reduction after a split.\n\n    Variance reduction is a splitting criterion used by CART trees in the\n    context of regression. It examines the variance in a target before and\n    after a split to determine whether we've reduced the variability in the\n    target.\n    \"\"\"\n    def compute_uncertainty(self, y):\n        \"\"\"Compute the variance of a target.\"\"\"\n        return np.var(y)\n\n    def __call__(self, target, mask, uncertainty):\n        left, right = target[mask], target[~mask]\n        return uncertainty - (self.compute_uncertainty(left) +\n                              self.compute_uncertainty(right))\n```", "```py\n def __init__(self, random_state, criterion, n_val_sample=25):\n        self.random_state = random_state\n        self.criterion = criterion # BaseCriterion from metrics\n        self.n_val_sample = n_val_sample\n```", "```py\ndef find_best(self, X, y):\n        criterion = self.criterion\n        rs = self.random_state\n\n        # keep track of the best info gain\n        best_gain = 0.\n\n        # keep track of best feature and best value on which to split\n        best_feature = None\n        best_value = None\n\n        # get the current state of the uncertainty (gini or entropy)\n        uncertainty = criterion.compute_uncertainty(y)\n```", "```py\n# iterate over each feature\nfor col in xrange(X.shape[1]):\n    feature = X[:, col]\n```", "```py\n # For each of n_val_sample iterations, select a random value\n # from the feature and create a split. We store whether we've seen\n # the value before; if we have, continue. Continue until we've seen\n # n_vals unique values. This allows us to more likely select values\n # that are high frequency (retains distributional data implicitly)\n for v in rs.permutation(feature):\n```", "```py\n# if we've hit the limit of the number of values we wanted to\n# examine, break out\nif len(seen_values) == n_vals:\n   break\n# if we've already tried this value, continue\nelif v in seen_values: # O(1) lookup\n     continue\n# otherwise, it's a new value we've never tried splitting on.\n# add it to the set.\nseen_values.add(v)\n\n# create the mask (these values \"go left\")\nmask = feature >= v # type: np.ndarray\n```", "```py\n# skip this step if this doesn't divide the dataset\nif np.unique(mask).shape[0] == 1: # all True or all False\n    continue\n```", "```py\n# compute how good this split was\ngain = criterion(y, mask, uncertainty=uncertainty)\n\n# if the gain is better, we keep this feature & value &\n# update the best gain we've seen so far\nif gain > best_gain:\n    best_feature = col\n    best_value = v\n    best_gain = gain\n\n# if best feature is None, it means we never found a viable split...\n# this is likely because all of our labels were perfect. In this case,\n# we could select any feature and the first value and define that as\n# our left split and nothing will go right.\nif best_feature is None:\n    best_feature = 0\n    best_value = np.squeeze(X[:, best_feature])[0]\n    best_gain = 0.\n\n# we need to know the best feature, the best value, and the best gain\nreturn best_feature, best_value, best_gain\n```", "```py\nclass LeafNode(object):\n    \"\"\"A tree node class.\n\n    Tree node that store the column on which to split and the value above\n    which to go left vs. right. Additionally, it stores the target statistic\n    related to this node. For instance, in a classification scenario:\n\n        >>> X = np.array([[ 1, 1.5 ],\n        ...               [ 2, 0.5 ],\n        ...               [ 3, 0.75]])\n        >>> y = np.array([0, 1, 1])\n        >>> node = LeafNode(split_col=0, split_val=2,\n        ...                 class_statistic=_most_common(y))\n```", "```py\n   def __init__(self, split_col, split_val, split_gain, class_statistic):\n\n        self.split_col = split_col\n        self.split_val = split_val\n        self.split_gain = split_gain\n\n        # the class statistic is the mode or the mean of the targets for\n        # this split\n        self.class_statistic = class_statistic\n\n        # if these remain None, it's a terminal node\n        self.left = None\n        self.right = None\n\n    def create_split(self, X, y):\n        \"\"\"Split the next X, y.\n```", "```py\n# If values in the split column are greater than or equal to the\n# split value, we go left.\nleft_mask = X[:, self.split_col] >= self.split_val\n\n# Otherwise we go to the right\nright_mask = ~left_mask # type: np.ndarray\n\n# If the left mask is all False or all True, it means we've achieved\n# a perfect split.\nall_left = left_mask.all()\nall_right = right_mask.all()\n\n# create the left split. If it's all right side, we'll return None\nX_left = X[left_mask, :] if not all_right else None\ny_left = y[left_mask] if not all_right else None\n\n# create the right split. If it's all left side, we'll return None.\nX_right = X[right_mask, :] if not all_left else None\ny_right = y[right_mask] if not all_left else None\n\nreturn X_left, X_right, y_left, y_right\n```", "```py\ndef is_terminal(self):\n     \"\"\"Determine whether the node is terminal.\n\n     If there is no left node and no right node, it's a terminal node.\n     If either is non-None, it is a parent to something.\n     \"\"\"\n     return self.left is None and self.right is None\n```", "```py\n   def predict_record(self, record):\n        \"\"\"Find the terminal node in the tree and return the class statistic\"\"\"\n        # First base case, this is a terminal node:\n        has_left = self.left is not None\n        has_right = self.right is not None\n        if not has_left and not has_right:\n            return self.class_statistic\n\n        # Otherwise, determine whether the record goes right or left\n        go_left = record[self.split_col] >= self.split_val\n\n        # if we go left and there is a left node, delegate the recursion to the\n        # left side\n        if go_left and has_left:\n            return self.left.predict_record(record)\n\n        # if we go right, delegate to the right\n        if not go_left and has_right:\n            return self.right.predict_record(record)\n\n        # if we get here, it means one of two things:\n        # 1\\. we were supposed to go left and didn't have a left\n        # 2\\. we were supposed to go right and didn't have a right\n        # for both of these, we return THIS class statistic\n        return self.class_statistic\n```", "```py\nclass _BaseCART(BaseSimpleEstimator):\n    def __init__(self, X, y, criterion, min_samples_split, max_depth,\n                 n_val_sample, random_state):\n        # make sure max_depth > 1\n        if max_depth < 2:\n            raise ValueError(\"max depth must be > 1\")\n\n        # check the input arrays, and if it's classification validate the\n        # target values in y\n        X, y = check_X_y(X, y, accept_sparse=False, dtype=None, copy=True)\n        if is_classifier(self):\n            check_classification_targets(y)\n\n        # hyper parameters so we can later inspect attributes of the model\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_val_sample = n_val_sample\n        self.random_state = random_state\n\n        # create the splitting class\n        random_state = check_random_state(random_state)\n        self.splitter = RandomSplitter(random_state, criterion, n_val_sample)\n\n        # grow the tree depth first\n        self.tree = self._find_next_split(X, y, 0)\n```", "```py\n    def _find_next_split(self, X, y, current_depth):\n        # base case 1: current depth is the limit, the parent node should\n        # be a terminal node (child = None)\n        # base case 2: n samples in X <= min_samples_split\n        if current_depth == self.max_depth or \\\n                X.shape[0] <= self.min_samples_split:\n            return None\n```", "```py\n# create the next split\nsplit_feature, split_value, gain = \\\n     self.splitter.find_best(X, y)\n\n# create the next node based on the best split feature and value\n# that we just found. Also compute the \"target stat\" (mode of y for\n# classification problems or mean of y for regression problems) and\n# pass that to the node in case it is the terminal node (that is, the\n# decision maker)\nnode = LeafNode(split_feature, split_value, gain, self._target_stat(y))\n# Create the splits based on the criteria we just determined, and then\n# recurse down left, right sides\nX_left, X_right, y_left, y_right = node.create_split(X, y)\n\n# if either the left or right is None, it means we've achieved a\n# perfect split. It is then a terminal node and will remain None.\nif X_left is not None:\nnode.left = self._find_next_split(X_left, y_left,\n                                  current_depth + 1)\n```", "```py\ndef predict(self, X):\n    # Check the array\n    X = check_array(X, dtype=np.float32) # type: np.ndarray\n\n    # For each record in X, find its leaf node in the tree (O(log N))\n    # to get the predictions. This makes the prediction operation\n    # O(N log N) runtime complexity\n    predictions = [self.tree.predict_record(row) for row in X]\n    return np.asarray(predictions)\n```", "```py\nfrom packtml.decision_tree import CARTClassifier\nfrom packtml.utils.plotting import add_decision_boundary_to_axis\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\n```", "```py\n# Create a classification dataset\nrs = np.random.RandomState(42)\ncovariance = [[1, .75], [.75, 1]]\nn_obs = 500\nx1 = rs.multivariate_normal(mean=[0, 0], cov=covariance, size=n_obs)\nx2 = rs.multivariate_normal(mean=[1, 3], cov=covariance, size=n_obs)\n\nX = np.vstack((x1, x2)).astype(np.float32)\ny = np.hstack((np.zeros(n_obs), np.ones(n_obs)))\n\n# split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n```", "```py\n# Fit a simple decision tree classifier and get predictions\nshallow_depth = 2\nclf = CARTClassifier(X_train, y_train, max_depth=shallow_depth, criterion='gini',\n                     random_state=42)\npred = clf.predict(X_test)\nclf_accuracy = accuracy_score(y_test, pred)\nprint(\"Test accuracy (depth=%i): %.3f\" % (shallow_depth, clf_accuracy))\n\n# Fit a deeper tree and show accuracy increases\nclf2 = CARTClassifier(X_train, y_train, max_depth=25, criterion='gini',\n                      random_state=42)\npred2 = clf2.predict(X_test)\nclf2_accuracy = accuracy_score(y_test, pred2)\nprint(\"Test accuracy (depth=25): %.3f\" % clf2_accuracy)\n```", "```py\nfrom packtml.decision_tree import CARTRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\n```", "```py\n# Create a classification dataset\nrs = np.random.RandomState(42)\nX = np.sort(5 * rs.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\n\n# split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n```", "```py\n# Fit a simple decision tree regressor and get predictions\nclf = CARTRegressor(X_train, y_train, max_depth=3, random_state=42)\npred = clf.predict(X_test)\nclf_mse = mean_squared_error(y_test, pred)\nprint(\"Test MSE (depth=3): %.3f\" % clf_mse)\n\n# Fit a deeper tree and show accuracy increases\nclf2 = CARTRegressor(X_train, y_train, max_depth=10, random_state=42)\npred2 = clf2.predict(X_test)\nclf2_mse = mean_squared_error(y_test, pred2)\nprint(\"Test MSE (depth=10): %.3f\" % clf2_mse)\n```", "```py\nx = X_train.ravel()\nxte = X_test.ravel()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 8))\naxes[0].scatter(x, y_train, alpha=0.25, c='r')\naxes[0].scatter(xte, pred, alpha=1.)\naxes[0].set_title(\"Shallow tree (depth=3) test MSE: %.3f\" % clf_mse)\n\naxes[1].scatter(x, y_train, alpha=0.4, c='r')\naxes[1].scatter(xte, pred2, alpha=1.)\naxes[1].set_title(\"Deeper tree (depth=10) test MSE: %.3f\" % clf2_mse)\n\n# if we're supposed to save it, do so INSTEAD OF showing it\nif len(sys.argv) > 1:\n    plt.savefig(sys.argv[1])\nelse:\n    plt.show()\n```", "```py\nfrom __future__ import absolute_import\n\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.utils.validation import check_X_y\nfrom sklearn.utils.multiclass import check_classification_targets\n```", "```py\n Parameters\n ----------\n X : array-like, shape=(n_samples, n_features)\n     The training array. Should be a numpy array or array-like structure\n     with only finite values.\n\ny : array-like, shape=(n_samples,)\n    The target vector.\nk : int, optional (default=10)\n    The number of neighbors to identify. The higher the ``k`` parameter,\n    the more likely you are to *under*-fit your data. The lower the ``k``\n    parameter, the more likely you are to *over*-fit your model.\n```", "```py\n    def __init__(self, X, y, k=10):\n        # check the input array\n        X, y = check_X_y(X, y, accept_sparse=False, dtype=np.float32,\n                         copy=True)\n\n        # make sure we're performing classification here\n        check_classification_targets(y)\n\n        # Save the K hyper-parameter so we can use it later\n        self.k = k\n\n        # kNN is a special case where we have to save the training data in\n        # order to make predictions in the future\n        self.X = X\n        self.y = y\n```", "```py\ndef predict(self, X):\n    # Compute the pairwise distances between each observation in\n    # the dataset and the training data. This can be relatively expensive\n    # for very large datasets!!\n    train = self.X\n    dists = euclidean_distances(X, train)\n```", "```py\n# Arg sort to find the shortest distance for each row. This sorts\n# elements in each row (independent of other rows) to determine the\n# order required to sort the rows.\n# that is:\n# >>> P = np.array([[4, 5, 1], [3, 1, 6]])\n# >>> np.argsort(P, axis=1)\n# array([[2, 0, 1],\n# [1, 0, 2]])\nnearest = np.argsort(dists, axis=1)\n```", "```py\n\n # We only care about the top K, really, so get sorted and then truncate\n # that is:\n # array([[1, 2, 1],\n # ...\n # [0, 0, 0]])\n predicted_labels = self.y[nearest][:, :self.k]\n```", "```py\n # We want the most common along the rows as the predictions\n # that is:\n # array([1, ..., 0])\n return mode(predicted_labels, axis=1)[0].ravel()\n```", "```py\nfrom __future__ import absolute_import\n\nfrom packtml.clustering import KNNClassifier\nfrom packtml.utils.plotting import add_decision_boundary_to_axis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_iris\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport sys\n```", "```py\n# Create a classification sub-dataset using iris\niris = load_iris()\nX = iris.data[:, :2] # just use the first two dimensions\ny = iris.target\n\n# split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n```", "```py\n# Fit a k-nearest neighbor model and get predictions\nk=10\nclf = KNNClassifier(X_train, y_train, k=k)\npred = clf.predict(X_test)\nclf_accuracy = accuracy_score(y_test, pred)\nprint(\"Test accuracy: %.3f\" % clf_accuracy)\n```"]
- en: Classifying Text Using Naive Bayes
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器进行文本分类
- en: '"Language is a process of free creation; its laws and principles are fixed,
    but the manner in which the principles of generation are used is free and infinitely
    varied. Even the interpretation and use of words involves a process of free creation."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “语言是一个自由创造的过程；它的规律和原则是固定的，但这些生成原则的运用方式是自由且无限变化的。甚至单词的解释和使用也涉及自由创造的过程。”
- en: – Noam Chomsky
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: – 诺姆·乔姆斯基
- en: 'Not all information exists in tables. From Wikipedia to social media, there
    are billions of written words that we would like our computers to process and
    extract bits of information from. The sub-field of machine learning that deals
    with textual data goes by names such as **Text Mining** and **Natural Language
    Processing** (**NLP**). These different names reflect the fact that the field
    inherits from multiple disciplines. On the one hand, we have computer science
    and statistics, and on the other hand, we have linguistics. I''d argue that the
    influence of linguistics was stronger when the field was at its infancy, but in
    later stages, practitioners came to favor mathematical and statistical tools,
    as they require less human intervention and can get away without humans manually
    codifying linguistic rules into the algorithms:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有信息都存在于表格中。从维基百科到社交媒体，成千上万的文字信息需要我们的计算机进行处理和提取。处理文本数据的机器学习子领域有着如**文本挖掘**和**自然语言处理**（**NLP**）等不同的名称。这些名称反映了该领域从多个学科继承而来。一方面，我们有计算机科学和统计学，另一方面，我们有语言学。我认为，在该领域初期，语言学的影响较大，但随着发展，实践者们更倾向于使用数学和统计工具，因为它们需要较少的人工干预，并且不需要人工将语言规则编入算法中：
- en: '"Every time I fire a linguist, the performance of our speech recognition system
    goes up."'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: “每次我解雇一个语言学家，我们的语音识别系统性能都会提升。”
- en: – Fred Jelinek
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: – 弗雷德·杰里内克
- en: Having said that, it is essential to have a basic understanding of how things
    have progressed over time and not jump to the bleeding-edge solutions right away.
    This enables us to pick our tools wisely while being aware of the tradeoffs we
    are making. Thus, we will start this chapter by processing textual data and presenting
    it to our algorithms in formats they understand. This preprocessing stage has
    an important effect on the performance of the downstream algorithms. Therefore,
    I will make sure to shed light on the pros and cons of each method explained here.
    Once the data is ready, we will use a **Naive Bayes** classifier to detect the
    sentiment of different Twitter users based on the messages they send to multiple
    airway services.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，了解事物随着时间的进展是如何发展的，避免直接跳入前沿解决方案，这一点至关重要。这使我们能够在意识到权衡取舍的基础上明智地选择工具。因此，我们将从处理文本数据开始，并以算法能够理解的格式呈现数据。这个预处理阶段对下游算法的性能有着重要影响。因此，我会确保阐明每种方法的优缺点。一旦数据准备好，我们将使用**朴素贝叶斯**分类器根据用户发送给多个航空公司服务的消息，检测不同Twitter用户的情感。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Splitting sentences into tokens
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将句子拆分成词元
- en: Token normalization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词元归一化
- en: Using bag of words to represent tokens
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用词袋模型表示词元
- en: Using n-grams to represent tokens
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用n-gram模型表示词元
- en: Using Word2Vec to represent tokens
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec表示词元
- en: Text classification with a Naive Bayes classifier
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器进行文本分类
- en: Splitting sentences into tokens
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将句子拆分成词元
- en: '"A word after a word after a word is power."'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: “一个字接一个字，形成了力量。”
- en: – Margaret Atwood
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: – 玛格丽特·阿特伍德
- en: So far, the data we have dealt with has either been table data with columns
    as features or image data with pixels as features. In the case of text, things
    are less obvious. Shall we use sentences, words, or characters as our features?
    Sentences are very specific. For example, it is very unlikely to have the exact
    same sentence appearing in two or more Wikipedia articles. Therefore, if we use
    sentences as features, we will end up with tons of features that do not generalize
    well.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理的数据要么是带有列作为特征的表格数据，要么是带有像素作为特征的图像数据。而在文本的情况下，问题变得不那么明确。我们应该使用句子、单词，还是字符作为特征？句子非常具体。例如，两篇维基百科文章中出现完全相同的句子的可能性非常小。因此，如果我们将句子作为特征，最终会得到大量的特征，这些特征的泛化能力较差。
- en: Characters, on the other hand, are limited. For example, there are only 26 letters
    in the English language. This small variety is likely to limit the ability of
    the separate characters to carry enough information for the downstream algorithms
    to extract. As a result, words are typically used as features for most tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，字符是有限的。例如，英语中只有 26 个字母。这种小的变化可能限制了单个字符携带足够信息的能力，无法让下游算法提取出有效的特征。因此，单词通常作为大多数任务的特征。
- en: Later in this chapter, we will see that fairly specific tokens are still possible,
    but let's stick to words as features for now. Finally, we do not want to limit
    ourselves to dictionary words; Twitter hashtags, numbers, and URLs can also be
    extracted from text and treated as features. That's why we prefer to use the term
    *token* instead *word*, since it is more generic. The process where a stream of
    text is split into tokens is called tokenization, and we are going to learn about
    that in the next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后我们会看到，尽管可以得到相当具体的标记，但现在让我们暂时仅把单词作为特征。最后，我们并不想局限于字典中的单词；Twitter 标签、数字和 URL
    也可以从文本中提取并作为特征。因此，我们更倾向于使用 *token* 而不是 *word* 这个术语，因为 *token* 更为通用。将文本流分割成标记的过程称为分词，我们将在下一节中学习这个过程。
- en: Tokenizing with string split
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用字符串分割进行分词
- en: Different tokenization methods lead to different results. To demonstrate these
    differences, let's take the following three lines of text and see how can we tokenize
    them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的分词方法会导致不同的结果。为了演示这些差异，让我们以以下三行文本为例，看看如何对它们进行分词。
- en: 'Here I write the lines of text as strings and put them into a list:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将文本行作为字符串写入并放入一个列表中：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'One obvious way to do this is to use Python''s built-in `split()` method as
    follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种明显的方法是使用 Python 内置的 `split()` 方法，如下所示：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When no parameters are given, `split()` uses white spaces to split strings
    based on. Thus, we get the following output:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有提供参数时，`split()` 会根据空格来分割字符串。因此，我们得到以下输出：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You may notice that the punctuation was kept as part of the tokens. The question
    mark was left at the end of `tokenize`, and the period remained attached to `boss`.
    The hashtag is made of two words, but since there are no spaces between them,
    it was kept as a single token along with its leading hash sign.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，标点符号被保留为标记的一部分。问号被保留在 `tokenize` 的末尾，句号也附着在 `boss` 后面。井号标签由两个单词组成，但由于它们之间没有空格，它被作为一个整体标记保留，并带有前导的井号符号。
- en: Tokenizing using regular expressions
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式进行分词
- en: 'We may also use regular expressions to treat sequences of letters and numbers
    as tokens, and split our sentences accordingly. The pattern used here, `"\w+"`,
    refers to any sequence of one or more alphanumeric characters or underscores.
    Compiling our patterns gives us a regular expression object that we can use for
    matching. Finally, we loop over each line and use the regular expression object
    to split it into tokens:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用正则表达式将字母和数字序列视为标记，并相应地分割我们的句子。这里使用的模式 `"\w+"` 表示任何一个或多个字母数字字符或下划线的序列。编译我们的模式会得到一个正则表达式对象，我们可以用它来进行匹配。最后，我们遍历每一行并使用正则表达式对象将其拆分为标记：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, the punctuation has been removed, but the URL has been split into four
    tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，标点符号已被去除，但 URL 被分割成了四个标记。
- en: Scikit-learn uses regular expressions for tokenization by default. However,
    the following pattern, `r"(?u)\b\w\w+\b"`, is used instead of `r"\w+"`. This pattern
    ignores all punctuation and words shorter than two letters. So, the "a" token
    would be omitted. You can still overwrite the default pattern by providing your
    custom one.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 默认使用正则表达式进行分词。然而，`r"(?u)\b\w\w+\b"` 这个模式被用来代替 `r"\w+"`。这个模式会忽略所有标点符号和短于两个字母的单词。因此，"a"
    这个词会被省略。你仍然可以通过提供自定义模式来覆盖默认模式。
- en: Using placeholders before tokenizing
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用占位符进行分词前的处理
- en: 'To deal with the previous problem, we may decide to replace the numbers, URLs,
    and hashtags with placeholders before tokenizing our sentences. This is useful
    if we don''t really care to differentiate between their content. A URL may be
    just a URL to me, regardless of where it leads to. The following function converts
    its input into lower case, then replaces any URL it finds with a `_url_` placeholder.
    Similarly, it converts the hashtags and numbers into their corresponding placeholders.
    Finally, the input is split based on white spaces, and the resulting tokens are
    returned:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决前面的问题，我们可以决定在对句子进行分词之前，将数字、URL和标签（hashtags）替换为占位符。如果我们不在意区分它们的内容，这样做是有用的。对我来说，URL可能只是一个URL，无论它指向哪里。以下函数将输入转换为小写字母，然后将找到的任何URL替换为`_url_`占位符。类似地，它将标签和数字转换为相应的占位符。最后，输入根据空白字符进行分割，并返回结果的词元：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives us the following output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下输出：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the new placeholder tells us that a URL existed in the second
    sentence, but it doesn't really care where the URL links to. If we have another
    sentence with a different URL, it will just get the same placeholder as well.
    The same goes for the numbers and hashtags.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，新的占位符告诉我们第二个句子中存在一个URL，但它并不关心该URL指向哪里。如果我们有另一个包含不同URL的句子，它也会得到相同的占位符。数字和标签也是一样的。
- en: Depending on your use case, this may not be ideal if your hashtags carry information
    that you would not like to lose. Again, this is a tradeoff you have to make based
    on your use case. Usually, you can intuitively tell which technique is more suitable
    for the problem at hand, but sometimes evaluating a model after multiple tokenization
    techniques can be the only way to tell which one is more suitable. Finally, in
    practice, you may use libraries such as **NLTK** and **spaCy** to tokenize your
    text. They already have the necessary regular expressions under the hood. We will
    be using spaCy later on in this chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的使用情况，如果你的标签包含你不想丢失的信息，这种方法可能并不理想。同样，这是一个你必须根据具体应用做出的权衡。通常，你可以直观地判断哪种技术更适合当前问题，但有时，评估经过多次分词技术后的模型，可能是唯一判断哪种方法更合适的方式。最后，在实际应用中，你可能会使用**NLTK**和**spaCy**等库来对文本进行分词。它们已经在后台实现了必要的正则表达式。在本章稍后的部分，我们将使用spaCy。
- en: Note how I converted the sentence into lower case before processing it. This
    is called normalization. Without normalization, a capitalized word and a lowercase
    version of it will be seen as two different tokens. This is not ideal, since *Boy*
    and *boy* are conceptually the same, hence normalization is usually required.
    Scikit-learn converts input text to lower case by default.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我在处理句子之前将其转换为小写字母。这被称为归一化。如果没有归一化，首字母大写的单词和它的小写版本会被视为两个不同的词元。这不是理想的，因为*Boy*和*boy*在概念上是相同的，因此通常需要进行归一化。Scikit-learn默认会将输入文本转换为小写字母。
- en: Vectorizing text into matrices
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本向量化为矩阵
- en: In text mining, a dataset is usually called a **corpus**. Each data sample in
    it is usually called a **document**. Documents are made of **tokens**, and a set
    of distinct tokens is called a **vocabulary**. Putting this information into a
    matrix is called **vectorization**. In the following sections, we are going to
    see the different kinds of vectorizations that we can get.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本挖掘中，一个数据集通常被称为**语料库**。其中的每个数据样本通常被称为**文档**。文档由**词元**组成，一组不同的词元被称为**词汇表**。将这些信息放入矩阵中称为**向量化**。在接下来的章节中，我们将看到我们可以获得的不同类型的向量化方法。
- en: Vector space model
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量空间模型
- en: We still miss our beloved feature matrices, where we expect each token to have
    its own column and each document to be represented by a separate row. This kind
    of representation for textual data is known as the **vecto****r****space mo****del**.
    From a linear-algebraic point of view, the documents in this representation are
    seen as vectors (rows), and the different terms are the dimensions of this space
    (columns), hence the name vector space model. In the next section, we will learn
    how to vectorize our documents.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然缺少我们心爱的特征矩阵，在这些矩阵中，我们期望每个词元（token）有自己的列，每个文档由单独的一行表示。这种文本数据的表示方式被称为**向量空间模型**。从线性代数的角度来看，这种表示中的文档被视为向量（行），而不同的词项是该空间的维度（列），因此称为向量空间模型。在下一节中，我们将学习如何将文档向量化。
- en: Bag of words
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋模型
- en: 'We need to convert the documents into tokens and put them into the vector space
    model. `CountVectorizer` can be used here to tokenize the documents and put them
    into the desired matrix. Here, we are going to use it with the help of the tokenizer
    we created in the previous section. As usual, we import and initialize `CountVectorizer`,
    and then we use its `fit_transform` method to convert our documents. We also specified
    that we want to use the tokenizer we built in the previous section:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将文档转换为标记，并将它们放入向量空间模型中。此处可以使用`CountVectorizer`对文档进行标记化并将其放入所需的矩阵中。在这里，我们将使用我们在上一节创建的分词器。像往常一样，我们导入并初始化`CountVectorizer`，然后使用其`fit_transform`方法来转换我们的文档。我们还指定希望使用我们在上一节构建的分词器：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Most of the cells in the returned matrix are zeros. To save space, it is saved
    as a sparse matrix; however, we can turn it into a dense matrix using its `todense()`
    method. The vectorizer holds the set of encountered vocabulary, which can be retrieved
    using `get_feature_names()`. Using this information, we can convert `x` into a
    DataFrame as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的矩阵中大部分单元格都是零。为了节省空间，它被保存为稀疏矩阵；然而，我们可以使用其`todense()`方法将其转换为稠密矩阵。向量化器保存了遇到的词汇表，可以使用`get_feature_names()`来检索。通过这些信息，我们可以将`x`转换为DataFrame，如下所示：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This gives us the following matrix:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们以下矩阵：
- en: '![](img/d6c4ca9d-16c6-4fef-9180-13492778d67f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6c4ca9d-16c6-4fef-9180-13492778d67f.png)'
- en: Each cell contains the number of times each token appears in each document.
    However, the vocabulary does not follow any order; therefore, it is not possible
    to tell the order of the tokens in each document from this matrix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单元格包含每个标记在每个文档中出现的次数。然而，词汇表没有遵循任何顺序；因此，从这个矩阵中无法判断每个文档中标记的顺序。
- en: Different sentences, same representation
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同的句子，相同的表示
- en: 'Take these two sentences with opposite meanings:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 取这两句话，它们有相反的意思：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we use the count vectorizer to represent them, we will end up with the following
    matrix:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用计数向量化器来表示它们，我们将得到以下矩阵：
- en: '![](img/2f1e5f0c-b2aa-4795-b849-0fe56cb9ca8a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f1e5f0c-b2aa-4795-b849-0fe56cb9ca8a.png)'
- en: As you can see, the order of the tokens in the sentences is lost. That is why
    this method is known as **bag of words** – the result is like a bag that words
    are just put into without any order. Obviously, this makes it impossible to tell
    which of the two people is happy and which is not. To fix this problem, we may
    need to use **n-grams**, as we will do in the following section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，句子中标记的顺序丢失了。这就是为什么这种方法被称为**词袋模型（bag of words）**——结果就像一个袋子，单词只是被放入其中，没有任何顺序。显然，这使得无法分辨哪一个人是开心的，哪一个不是。为了解决这个问题，我们可能需要使用**n-grams**，正如我们将在下一节中所做的那样。
- en: N-grams
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N-grams
- en: 'Rather than treating each term as a token, we can treat the combinations of
    each two consecutive terms as a single token. All we have to do is to set `ngram_range`
    in `CountVectorizer` to `(2,2)`, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将每个术语视为一个标记，我们可以将每两个连续术语的组合视为一个单独的标记。我们要做的就是将`CountVectorizer`中的`ngram_range`设置为`(2,2)`，如下所示：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using similar code to that used in the previous section, we can put the resulting
    `x` into a DataFrame and get the following matrix:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与上一节相似的代码，我们可以将结果的`x`放入DataFrame中并得到以下矩阵：
- en: '![](img/2c2c898e-bdec-4f61-ab94-ef3bbe0bdc6b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c2c898e-bdec-4f61-ab94-ef3bbe0bdc6b.png)'
- en: Now we can tell who is happy and who is not. When using word pairs, this is
    known as **bigrams**. We can also do 3-grams (with three consecutive words), 4-grams,
    or any other number of grams. Setting `ngram_range` to (1,1) takes us back to
    the original representation where each separate word is a token, which is **unigrams**.
    We can also mix unigrams with bigrams by setting `ngram_range`**to (1,2). In brief,
    this range tells the tokenizer the minimum and maximum values for*n* to use in
    our n-grams.**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以知道谁是开心的，谁不是。当使用词对时，这被称为**大ram（bigrams）**。我们还可以使用3-gram（由三个连续单词组成），4-gram或任何其他数量的gram。将`ngram_range`设置为(1,1)将使我们回到原始表示形式，其中每个单独的单词是一个标记，这就是**单gram（unigrams）**。我们还可以通过将`ngram_range`设置为(1,2)来混合单gram和大gram。简而言之，这个范围告诉分词器用于我们n-gram的最小值和最大值*n*。
- en: '**If you set *n* to a high value – say, 8 – this means that sequences of eight
    words are treated as tokens. Now, how likely do you think it is that a sequence
    of eight words will appear more than once in your dataset? Most likely, you will
    see it once in your training set and never again into the test set. That''s why
    *n* is usually set to something between 2 and 3, with some unigrams also being
    used to capture rare words.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你将*n*设置为一个较高的值——比如 8——这意味着八个单词的序列将被当作标记。那么，你认为一个包含八个单词的序列在你的数据集中出现的概率有多大？大概率是它只会在训练集中出现一次，而在测试集中从未出现过。这就是为什么*n*通常设置为
    2 到 3 之间的数值，并且有时会使用一些 unigram 来捕捉稀有词汇。**'
- en: Using characters instead of words
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用字符代替单词
- en: Up until now, words have been the atoms of our textual universe. However, some
    situations may require us to tokenize our documents based on characters instead.
    In situations where word boundaries are not clear, such as in hashtags and URLs,
    the use of characters as tokens may help. Natural languages tend to have different
    frequencies for their characters. The letter **e** is the most commonly used character
    in the English language, and character combinations such as **th**, **er**, and
    **on** are also very common. Other languages, such as French and Dutch, have different
    character frequencies. If our aim is to classify documents based on their languages,
    the use of characters instead of words can come in handy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，单词一直是我们文本宇宙中的原子。然而，有些情况可能需要我们基于字符来进行文档的标记化。在单词边界不明确的情况下，比如标签和 URL，使用字符作为标记可能会有所帮助。自然语言的字符频率通常不同。字母**e**是英语中使用最频繁的字符，字符组合如**th**、**er**和**on**也非常常见。其他语言，如法语和荷兰语，也有不同的字符频率。如果我们的目标是基于语言来分类文档，使用字符而不是单词可能会派上用场。
- en: 'The very same `CountVectorizer` can help us tokenize our documents into characters.
    We can also combine this with the `n-grams` setting to get subsequences within
    words, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的`CountVectorizer`可以帮助我们将文档标记化为字符。我们还可以将其与`n-grams`设置结合，以获取单词中的子序列，如下所示：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can put the resulting `x` into a DataFrame, as we did earlier, to get the
    following matrix:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前一样将结果`x`放入 DataFrame 中，从而得到如下矩阵：
- en: '![](img/ea3c937d-0287-467f-8625-0844a79f8a9a.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea3c937d-0287-467f-8625-0844a79f8a9a.png)'
- en: All our tokens are made of four characters now. Whitespaces are also treated
    as characters, as you can see. With characters, it is more common to go for higher
    values of *n*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们所有的标记都由四个字符组成。如你所见，空格也被视作字符。使用字符时，通常会选择更高的*n*值。
- en: Capturing important words with TF-IDF
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 TF-IDF 捕捉重要词汇
- en: Another discipline that we borrow lots of ideas from here is the **information
    retrieval** field. It's the field responsible for the algorithms that run search
    engines such as Google, Bing, and DuckDuckGo.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里借鉴的另一个学科是**信息检索**领域。它是负责运行搜索引擎算法的领域，比如 Google、Bing 和 DuckDuckGo。
- en: 'Now, take the following quotation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看下面这段引文：
- en: '"From a linguistic point of view, you can''t really take much objection to
    the notion that a show is a show is a show."'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: “从语言学的角度来看，你真的不能对‘一个节目就是一个节目’这一概念提出太多反对意见。”
- en: – Walter Becker
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: – 沃尔特·贝克尔
- en: The word **linguistic** and the word **that** both appeared exactly once in
    the previous quotation. Nevertheless, we would only worry about the word **linguistic**,
    not the word **that**, if we were searching for this quotation on the internet.
    We know that it is more significant, although it appeared only once, just as many
    times as **that**. The word **show** appeared three times. From a count vectorizer's
    point of view, it should carry three times more information than the word **linguistic**.
    I assume you also disagree with the vectorizer about that. Those issues are fundamentally
    the raison d'être of **Term Frequency**-**Inverse Document Frequency***(**TF-IDF**).
    The IDF part not only involves weighting the value of the words based on how frequently
    they appear in a certain document, but also discounting weights from them if they
    happen to be very common in other documents. The word **that** is so common across
    other documents that it shouldn't be given as much value as **linguistic**. Furthermore,
    IDF uses a logarithmic scale to better represent the information a word carries
    based on its frequency in a document.*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**linguistic**和**that**这两个词在前述引用中都出现过一次。然而，如果我们在互联网上搜索这段引文，我们只会关注**linguistic**这个词，而不是**that**这个词。我们知道，尽管它只出现了一次，和**that**出现的次数一样多，但它更为重要。**show**这个词出现了三次。从计数向量化器的角度来看，它应该比**linguistic**包含更多的信息。我猜你也不同意向量化器的看法。这些问题从根本上来说是**词频-逆文档频率**（**TF-IDF**）的存在原因。IDF部分不仅涉及根据单词在某个文档中出现的频率来加权单词的值，还会在这些单词在其他文档中非常常见时对它们的权重进行折扣。**that**这个词在其他文档中如此常见，以至于它不应该像**linguistic**一样被赋予那么高的权重。此外，IDF使用对数尺度来更好地表示一个词根据它在文档中的频率所携带的信息。*'
- en: '*Let''s use the following three documents to demonstrate how TF-IDF works:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们使用以下三个文档来演示TF-IDF是如何工作的：'
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`TfidfVectorizer` has an almost identical interface to that of`CountVectorizer`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer`的接口与`CountVectorizer`几乎完全相同：'
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is a comparison for the outputs of the two vectorizers side by side:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两种向量化器输出的并排比较：
- en: '![](img/224edc67-6a5f-4823-bfe1-b259b922069c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/224edc67-6a5f-4823-bfe1-b259b922069c.png)'
- en: As you can see, unlike in `CountVectorizer`, not all words were treated equally
    by `TfidfVectorizer`. More emphasis was given to the fruit names compared to the
    other, less informative words that happened to appear in all three sentences.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，与`CountVectorizer`不同，`TfidfVectorizer`并没有对所有单词进行平等对待。相比于其他出现在所有三句话中的不太有信息量的词，更多的强调被放在了水果名称上。
- en: Both `CountVectorizer` and**`TfidfVectorizer`**have a parameter called `stop_words`.
    It can be used to specify tokens to be ignored. You can provide your own list
    of less informative words, such as **a**, **an**, and **the**. You can also provide
    the `english`*keyword to specify the common stop words in the English language.
    Having said that, it is important to note that some words can be informative for
    one task but not for another. Furthermore, IDF usually does what you need it to
    do automatically and gives low weights to non-informative words. That is why I
    usually prefer not to manually remove stop words, instead trying things such as
    `TfidfVectorizer`, feature selection, and regularization**first.*******
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`和**`TfidfVectorizer`**都有一个名为`stop_words`的参数。它可以用来指定需要忽略的词元。你可以提供自己的不太有信息量的词列表，例如**a**、**an**和**the**。你也可以提供`english`*关键字来指定英语中常见的停用词。话虽如此，需要注意的是，一些词对于某个任务来说可能有信息量，但对另一个任务则可能没有。此外，IDF通常会自动完成你需要它做的工作，并且给非信息性词语赋予较低的权重。这就是为什么我通常不手动去除停用词，而是尝试使用`TfidfVectorizer`、特征选择和正则化**优先**的方法。*******'
- en: '*******Besides its original use case,`TfidfVectorizer` is commonly used as
    a preprocessing step for text classification. Nevertheless, it usually gives good
    results when longer documents are to be classified. For short documents, it may
    produce noisy transformation, and it is advised to give`CountVectorizer` a try
    in such cases.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*******除了它的原始用途，`TfidfVectorizer`通常作为文本分类的预处理步骤。然而，当需要对较长的文档进行分类时，它通常能给出不错的结果。对于较短的文档，它可能会产生嘈杂的转化，建议在这种情况下尝试使用`CountVectorizer`。'
- en: In a basic search engine, when someone types a query, it gets converted into
    the same vector space where all the documents to be searched exist, using TF-IDF.
    Once the search query and the documents exist as vectors in the same space, a
    simple distance measure such as cosine distance can be used to find the closest
    documents to the query. Modern search engines vary from this basic idea, but it
    is a good base to build your understanding of information retrieval on.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个基础的搜索引擎中，当有人输入查询时，它会通过 TF-IDF 转换为与所有待搜索文档存在于同一向量空间中的形式。一旦查询和文档作为向量存在于同一空间中，就可以使用简单的距离度量方法，如余弦距离，来查找与查询最接近的文档。现代搜索引擎在这个基础概念上有所变化，但这是构建信息检索理解的良好基础。
- en: Representing meanings with word embedding
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用词嵌入表示意义
- en: 'As documents are collections of tokens, their vector representations are basically
    the sum of the vectors of the tokens they contain. As we have seen earlier, the
    **I like apples** document was represented by `CountVectorizer` using the vector
    [1,1,1,0,0]:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文档是由词元组成的，它们的向量表示基本上是包含的词元向量之和。正如我们之前看到的，**I like apples**文档通过`CountVectorizer`被表示为向量[1,1,1,0,0]：
- en: '![](img/7838de63-46e3-4cae-84cc-810e05362a52.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7838de63-46e3-4cae-84cc-810e05362a52.png)'
- en: From this representation, we can also deduce that the terms **I**, **like**,
    **apples**, and **oranges** are represented by the following four five-dimensional
    vectors, [0,1,0,0,0], [0,0,1,0,0], [1,0,0,0,0], and [0,0,0,1,0]. We have a five-dimensional
    space, given our vocabulary of five terms. Each term has a magnitude of 1 in one
    dimension and 0 in the other four dimensions. From a linear algebraic point of
    view, all five terms are orthogonal (perpendicular) to each other. Nevertheless,
    **apples**, **pears**, and **oranges** are all fruits, and conceptually they have
    some similarity that was not captured by this model. Therefore, we would ideally
    like to represent them with vectors that are closer to each other, unlike these
    orthogonal vectors. The same issue here applied to `TfidfVectorizer`, by the way***.***
    This was the driver for researchers to come up with better representations, and
    word embedding is the coolest kid on the natural language processing block nowadays,
    as it tries to capture meaning better than traditional vectorizers. In the next
    section, we will get to know one popular embedding technique, Word2Vec.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从这种表示方式出发，我们还可以推断出**I**、**like**、**apples**和**oranges**分别由以下四个五维向量表示：[0,1,0,0,0]，[0,0,1,0,0]，[1,0,0,0,0]和[0,0,0,1,0]。我们有一个五维空间，基于我们五个词的词汇表。每个词在一个维度上的值为1，其他四个维度上的值为0。从线性代数的角度来看，所有五个词是正交的（垂直的）。然而，**apples**、**pears**和**oranges**都是水果，在概念上它们有一定的相似性，但这种相似性并没有被这个模型捕捉到。因此，我们理想的做法是使用相互接近的向量来表示它们，而不是这些正交的向量。顺便提一下，`TfidfVectorizer`也存在类似问题***。***
    这促使研究人员提出了更好的表示方法，而词嵌入如今成为自然语言处理领域的热门技术，因为它比传统的向量化方法更好地捕捉了意义。在下一节中，我们将了解一种流行的嵌入技术——Word2Vec。
- en: Word2Vec
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec
- en: Without getting into the details too much, Word2Vec uses neural networks to
    predict words from their context, that is, from their surrounding words. By doing
    so, it learns better representations for the different words, and these representations
    incorporate the meanings of the words they represent. Unlike the previously mentioned
    vectorizers, the dimensionality of the word representation is not directly linked
    to the size of our vocabulary. We get to choose the length of our embedding vectors.
    Once each word is represented by a vector, the document's representation is usually
    the summation of all the vectors of its words. Averaging is also an option instead
    of summation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入细节，Word2Vec 使用神经网络从上下文中预测单词，也就是说，从单词的周围词汇中进行预测。通过这种方式，它学习了更好的单词表示，并且这些表示包含了它们所代表的单词的意义。与前面提到的向量化方法不同，单词表示的维度与我们词汇表的大小没有直接关系。我们可以选择嵌入向量的长度。一旦每个单词被表示为一个向量，文档的表示通常是所有单词向量的和。平均值也是一个替代选择，而不是求和。
- en: Since the size of our vectors is independent of the size of the vocabulary of
    the documents we are dealing with, researchers can reuse a pre-trained Word2Vec
    model that wasn't made specifically for their particular problem. This ability
    to re-use pre-trained models is known as transfer learning. Some researchers can
    train an embedding on a huge amount of documents using expensive machines and
    release the resulting vectors for the entire world to use. Then, the next time
    we deal with a specific natural language processing task, all we need to do is
    to get these vectors and use them to represent our new documents. spaCy ([https://spacy.io/](https://spacy.io/))
    is an open source software library that comes with word vectors for different
    languages.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们向量的大小与我们处理的文档的词汇量无关，研究人员可以重新使用未专门为他们特定问题训练的预训练Word2Vec模型。这种重新使用预训练模型的能力被称为迁移学习。一些研究人员可以使用昂贵的机器在大量文档上训练嵌入，并发布得到的向量供全世界使用。然后，下次我们处理特定的自然语言处理任务时，我们所需要做的就是获取这些向量并用它们来表示我们新的文档。spaCy
    ([https://spacy.io/](https://spacy.io/))是一个开源软件库，提供了不同语言的词向量。
- en: 'In the following few lines of code, we will install spaCy, download its language
    model data, and use it to convert words into vectors:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几行代码中，我们将安装spaCy，下载它的语言模型数据，并使用它将单词转换为向量：
- en: 'To use spaCy, we can install the library and download its pre-trained models
    for the English language by running the following commands in our terminal:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用spaCy，我们可以安装这个库并通过运行以下命令在终端中下载其英语预训练模型：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we can assign the downloaded vectors to our five words as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以将下载的向量分配给我们的五个单词，如下所示：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the representation for **apples**:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是**苹果**的表示：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: I promised you that the representations for **apples**, **oranges**, and **pears**
    would not be orthogonal as in the case with `CountVectorizer`. However, with 300
    dimensions, it is hard for me to visually prove that. Luckily, we have already
    learned how to calculate the cosine of the angle between two vectors. Orthogonal
    vectors should have 90^o angles between them, whose cosines are equal to 0\. The
    cosine for the zero angle between two vectors going in the exact same direction
    is 1.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾承诺你，**苹果**、**橙子**和**梨**的表示不会像`CountVectorizer`那样正交。然而，使用300个维度时，我很难直观地证明这一点。幸运的是，我们已经学会了如何计算两个向量之间的余弦角度。正交向量之间的角度应该是90°，其余弦值为0。而两个方向完全相同的向量之间的零角度的余弦值为1。
- en: 'Here, we calculate the cosine between all the five vectors we got from spaCy.
    I used some pandas and seaborn styling to make the numbers clearer:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算了来自spaCy的五个向量之间的余弦相似度。我使用了一些pandas和seaborn的样式，使数字更清晰：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, I showed the results in the following DataFrame:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我在下面的DataFrame中展示了结果：
- en: '![](img/54477c03-db2a-4444-8191-78cc4fc479e5.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54477c03-db2a-4444-8191-78cc4fc479e5.png)'
- en: Clearly, the new representation understands that fruit names are more similar
    to each other than they are to words such as **I** and **like**. It also considered
    **apples** and **pears** to be very similar to each other, as opposed to **oranges**.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，新的表示方法理解到水果名称之间的相似度远高于它们与像**I**和**like**这样的词的相似度。它还认为**苹果**和**梨**非常相似，而**橙子**则不然。
- en: You may have noticed that Word2Vec suffers from the same problem as unigrams;
    words are encoded without much attention being paid to their context. The representation
    for the word "book" in "I will read a book" is the same as its representation
    in "I will book a flight." That's why newer techniques, such as **Embeddings from
    Language Models** (**ELMo**), **Bidirectional Encoder Representations from Transformers**
    (**BERT**) and OpenAI's recent **GPT-3** are gaining more popularity nowadays
    as they respect the words' context. I expect them to be included in more libraries
    soon for anyone to easily use them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，Word2Vec存在与一元词相同的问题；词语的编码并没有太多关注它们的上下文。在句子“I will read a book”和“I will
    book a flight”中，单词“book”的表示是一样的。这就是为什么像**语言模型嵌入**（**ELMo**）、**双向编码器表示从变换器**（**BERT**）以及OpenAI最近的**GPT-3**等新技术现在越来越受欢迎的原因，因为它们尊重词语的上下文。我预计它们很快会被更多的库所采用，供大家轻松使用。
- en: The embedding concept is recycled and reused by machine learning practitioners
    everywhere nowadays. Apart from its use in natural language processing, it is
    used for feature reduction and in recommendation systems. For instance, every
    time a customer adds an item to their online shopping cart, if we treat the cart
    as a sentence and the items as words, we end up with item embeddings (**Item2Vec**).
    These new representations for the items can easily be plugged into a downstream
    classifier or a recommender system.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Before moving to text classification, we need to stop and spend some time first
    to learn about the classifier we are going to use – the **Naive Bayes classifier**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Naive Bayes
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naive Bayes classifier is commonly used in classifying textual data. In
    the following sections, we are going to see its different flavors and learn how
    to configure their parameters. But first, to understand the Naive Bayes classifier,
    we need to first go through Thomas Bayes' theorem, which he published in the 18^(th)
    century.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The Bayes rule
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When talking about classifiers, we can describe the probability of a certain
    sample belonging to a certain class using conditional probability, *P(y|x)*. This
    is the probability of a sample belonging to class *y* given its features, *x*.
    The pipe sign (|) is what we use to refer to conditional probability, that is,
    *y* given *x*. The Bayes rule is capable of expressing this conditional probability
    in terms of *P(x|y)*, *P(x)*, and *P(y)*, using the following formula:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5af08cd0-2cfe-4726-95b2-787a4098441d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'Usually, we ignore the denominator part of the equation and convert it into
    a proportion as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82a8bcff-9301-4949-8174-c274048cad51.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: The probability of a class, *P(y)*, is known as the prior probability. It's
    basically the number of samples that belong to a certain class out of all training
    samples. The conditional probability, *P(x|y)*, is known as the likelihood. It's
    what we calculate from the training samples. Once the two probabilities are known
    at training time, we can use them to predict the chance of a new sample belonging
    to a certain class at prediction time, *P(y|x)*, also known as the posterior probability.
    Calculating the likelihood part of the equation is not as simple as we expect.
    So, in the next section, we are going to discuss the assumption we can make to
    ease this calculation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the likelihood naively
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A data sample is made of multiple features, which means that in reality, the
    *x* part of *P(x|y)* is made of *x[1]*, *x[2]*, *x[3]*, .... *x[k]*, where *k*
    is the number of features. Thus, the conditional probability can be expressed
    as *P(x[1], x[2], x[3], .... x[k]|y)*. In practice, this means that we need to
    calculate this conditional probability for all possible combinations of *x*. The
    main drawback of this is the lack of generalization of our models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the following toy example to make things clearer:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '| **Text** | **Does the text suggest that the writer likes fruit?** |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| I like apples | Yes |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| I like oranges | Yes |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| I hate pears | No |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: If the previous table is our training data, the likelihood probability, *P(x|y)*,
    for the first sample is the probability of seeing the three words **I**, **like**,
    and **apples** together, given the target, **Yes**. Similarly, for the second
    sample, it is the probability of seeing the three words **I**, **like**, and **oranges**
    together, given the target, **Yes**. The same goes for the third sample, where
    the target is **No** instead of **Yes**. Now, say we are given a new sample, **I
    hate apples**. The problem is that we have never seen these three words together
    before. You might say, "But we've seen each individual word of the sentence before,
    just separately!" That's correct, but our formula only cares about combinations
    of words. It cannot learn anything from each separate feature on its own.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: You may recall from [Chapter 4](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=27&action=edit),
    *Preparing Your Data*, that *P(x[1], x[2], x[3], .... x[k]|y)* can only be expressed
    as *P(x[1]|y)* P(x[2]|y)x[3]* .. * P(x[k]|y)* if *x[1], x[2], x[3], .... x[k]*
    are independent. Their independence is not something we can be sure of, yet we
    still make this naive assumption in order to make the model more generalizable.
    As a result of this assumption and dealing with separate words, we can now learn
    something about the phrase **I hate apples**, despite not seeing it before. This
    naive yet useful assumption of independence is what gave the classifier's name
    its "naive" prefix.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes implementations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In scikit-learn, there are various Naive Bayes implementations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The **multinomial Naive Bayes** classifier is the most commonly used implementation
    for text classification. Its implementation is most similar to what we saw in
    the previous section.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Bernoulli Naive Bayes****classifier assumes the features to be binary.
    Rather than counting how many times a term appears in each document, in the Bernoulli
    version, we only care whether a term exists or not. The way the likelihood is
    calculated explicitly penalizes the non-occurrence of the terms in the documents,
    and it might perform better on some datasets, especially those with shorter documents.**
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***   **Gaussian Naive Bayes** is used with continuous features. It assumes
    the features to be normally distributed and calculates the likelihood probabilities
    using maximum likelihood estimation. This implementation is useful for other cases
    aside from text analysis.**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**Furthermore, you can also read about two other implementations, **complement
    Naive Bayes** and **categorical Naive Bayes**, in the scikit-learn user guide
    ([https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html)).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Additive smoothing
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a term not seen during training appears during prediction, we set its probability
    to 0\. This sounds logical, yet it is a problematic decision to make given our
    naive assumption. Since *P(x[1], x[2], x[3], .... x[k]|y)* is equal to *P(x[1]|y)*
    P(x[2]|y)*P(x[3]|y) * .. * P(x[k]|y),* setting the conditional probability for
    any term to zero will set the entire *P(x[1], x[2], x[3], .... x[k]|y)* to zero
    as a result. To avoid this problem, we pretend that a new document that contains
    the whole vocabulary was added to each class. Conceptually, this new hypothetical
    document takes a portion of the probability mass assigned to the terms we have
    seen and reassigns it to the unseen terms. The `alpha` parameter controls how
    much of the probability mass we want to reassign to the unseen terms. Setting
    `alpha` to 1 is called **Laplace smoothing**, while setting it to values between
    0 and 1 is called **Lidstone****smoothing**.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'I find myself using Laplace smoothing a lot when calculating ratios. In addition
    to preventing us from dividing by zero, it also helps to deal with uncertainties.
    Let me explain further using the following two examples:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: 10,000 people saw a link, and 9,000 of them clicked on it. We
    can obviously estimate the click-through rate to be 90%.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example 2**: If our data has only one person, and that person saw the link
    and clicked on it, would we be confident enough to say that the click-through
    rate was 100%?'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous examples, if we pretended that there were two additional users,
    where only one of them clicked on the link, the click-through rate in the first
    example would become 9,001 out of 10,002, which is still almost 90%. In the second
    example, though, we would be dividing 2 by 3, which would leave 60%, instead of
    the 100% calculated earlier. Laplace smoothing and Lidstone smoothing can be linked
    to the Bayesian way of thinking. Those two users, where 50% of them clicked on
    the link, are our prior belief. Initially, we do not know much, so we assume a
    50% click-through rate. Now, in the first example, we have enough data to overrule
    this prior belief, while in the second case, the fewer data points were only able
    to move the prior so much.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: That's enough theory for now – let's use everything we have learned so far to
    tell whether some reviewers are happy about their movie-watching experience or
    not.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Classifying text using a Naive Bayes classifier
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to get a list of sentences and classify them based
    on the user's sentiment. We want to tell whether the sentence carries a positive
    or a negative sentiment. *Dimitrios Kotzias et al* created this dataset for their
    research paper, *From Group to Individual Labels using Deep Features*. They collected
    a list of random sentences from three different websites, where each sentence
    is labeled with either 1 (positive sentiment) or 0 (negative sentiment).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: In total, there are 2,745 sentences in the data set. In the following sections,
    we are going to download the dataset, preprocess it, and classify the sentences
    in it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can just open the browser, download the CSV files into a local folder, and
    use pandas to load the files into DataFrames. However, I prefer to use Python
    to download the files, rather than the browser. I don't do this out of geekiness,
    but to ensure the reproducibility of my entire process by putting it into code.
    Anyone can just run my Python code and get the same results, without having to
    read a lousy documentation file, find a link to the compressed file, and follow
    the instructions to get the data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to download the data we need:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a folder to store the downloaded data into it. The following
    code checks whether the required folder exists or not. If it is not there, it
    creates it into the current working directory:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then we need to install the `requests` library using `pip`, as we will use
    it to download the data:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we download the compressed data as follows:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we can uncompress the data and store it into the data folder we have just
    created. We will be using the `zipfile` module to uncompress our data. The `ZipFile`
    method expects to read a file object. Thus, we use `BytesIO` to convert the content
    of the response into a file-like object. Then we extract the content of the zip
    file into our folder as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that our data is written into 3 separate files in our data folder, we can
    load each one of the 3 files into a separate data frame. Then, we can combine
    the 3 data frames into a single data frame as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can display the distribution of the sentiment labels using the following
    code:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As we can see, the two classes are more or less equal. It is a good practice
    to check the distribution of your classes before running any classification task:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a79736b-17ac-4eb2-bed3-ed7cbe53ea12.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'We can also display a few sample sentences using the following code, after
    tweaking pandas'' settings to display more characters per cell:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'I set the `random_state` to an arbitrary value to make sure we both get the
    same samples as below:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62567e34-644b-408c-bb6c-8ef36434604f.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: Preparing the data
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we need to prepare the data for our classifier to use it:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'As we usually do, we start by splitting the DataFrame into training and testing
    sets. I kept 40% of the data set for testing, and also set `random_state` to an
    arbitrary value to make sure we both get the same random split:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we get our labels from the sentiment column as follows:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As for the textual features, let''s convert them using `CountVectorizer`. We
    will include unigrams as well as bigrams and trigrams. We can also ignore rare
    words by setting `min_df` to `3` to exclude words appearing in fewer than three
    documents. This is a useful practice for removing spelling mistakes and noisy
    tokens. Finally, we can strip accents from letters and convert them to `ASCII`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the end, we can use the Naive Bayes classifier to classify our data. We
    set `fit_prior=True` for the model to use the distribution of the class labels
    in the training data as its prior:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This time, our old good accuracy score may not be informative enough. We want
    to know how accurate we are per class. Furthermore, depending on our use case,
    we may need to tell whether the model was able to identify all the negative tweets,
    even if it did that at the expense of misclassifying some positive tweets. To
    be able to get this information, we need to use the `precision` and `recall` scores.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Precision, recall, and F1 score
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Out of the samples that were assigned to the positive class, the percentage
    of them that were actually positive is the**precision** of this class. For the
    positive tweets, the percentage of them that the classifier correctly predicted
    to be positive is the **recall** for this class. As you can see, the precision
    and recall are calculated per class. Here is how we formally express the **precision
    score** in terms of true positives and false positives:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eeb004df-1cbb-438e-9d77-7389cd083b56.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: The **recall score** is expressed in terms of true positives and false negatives*:*
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8d8af97-42d7-4c94-a66d-33da59706fa2.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'To summarize the two previous scores into one number, the *F[1] score* can
    be used. It combines the precision and recall scores using the following formula:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb6db0cd-d232-4b46-b24b-27bfe55625de.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: 'Here we calculate the three aforementioned metrics for our classifier:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To make it clear, I put the resulting metrics into the following table. Keep
    in mind that the support is just the number of samples in each class:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a748541e-1612-4be1-863d-635d2eee912e.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: We have equivalent scores given that the sizes of the two classes are almost
    equal. In cases where the classes are imbalanced, it is more common to see one
    class achieving a higher precision or a higher recall compared to the other.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Since these metrics are calculated per class label, we can also get their macro
    averages. For this example here, the macro average precision score will be the
    average of **0.81**, and **0.77**, which is **0.79**. A micro average, on the
    other hand, calculates these scores globally based on the overall number of true
    positive, false positive, and false negative samples.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapters, we used a grid search to find the optimal hyperparameters
    for our estimators. Now, we have multiple things to optimize at once. One the
    one hand, we want to optimize the Naive Bayes hyperparameters, but on the other
    hand, we also want to optimize the parameters of the vectorizer used at the preprocessing
    step. Since a grid search expects one object only, scikit-learn provides a `pipeline`
    wrapper where we can combine multiple transformers and estimators into one.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, the pipeline is made of a set of sequential steps. Here
    we start with `CountVectorizer` and have `MultinomialNB` as the second and final
    step:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: All objects but the one in the last step are expected to be `transformers`;
    that is, they should have the `fit`, `transform`, and `fit_transform` methods.
    The object in the last step is expected to be `estimator`, meaning it should have
    the `fit` and `predict` methods. You can also build your custom transformers and
    estimators and use them in the pipeline as long as they have the expected methods.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our pipeline ready, we can plug it into `GridSearchCV` to find
    the optimal hyperparameters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing for different scores
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '"What gets measured gets managed."'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: – Peter Drucker
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: When we used `GridSearchCV` before, we did not specify which metric we want
    to optimize our hyperparameters for. The classifier's accuracy was used by default.
    Alternatively, you can also choose to optimize your hyperparameters for the precision
    score or the recall score. We will set our grid search here to optimize for the
    macro precision score.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by setting the different hyperparameters that we want to search within.
    Since we are using a pipeline here, we prefix each hyperparameter with the name
    of the step it is designated for, in order for the pipeline to assign the parameter
    to the correct step:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: By default, the priors, `P(y)`, in the Bayes rule are set based on the number
    of samples in each class. However, we can set them to be constant for all classes
    by setting `fit_prior=False`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we run `GridSearchCV` while letting it know that we care about precision
    the most:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This gives us the following hyperparameters:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '`ngram_range`: (1, 3)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: 1'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit_prior`: False'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We get a macro precision of 80.5% and macro recall of 80.5%.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Due to the balanced class distributions, it was expected for the prior not to
    add much value. We also get similar precision and recall scores. Thus, it doesn't
    make sense now to re-run the grid search again for an optimized recall. We will
    most likely get identical results anyway. Nevertheless, things will likely be
    different when you deal with highly imbalanced classes, and you want to maximize
    the recall of one class at the expense of the others.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to use word embeddings to represent our tokens.
    Let's see if this form of transfer learning will help our classifier perform better.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom transformer
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before ending this chapter, we can also create a custom transformer based on
    the `Word2Vec` embedding and use it in our classification pipeline instead of
    `CountVectorizer`. In order to be able to use our custom transformer in the pipeline,
    we need to make sure it has `fit`, `transform`, and `fit_transform` methods.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our new transformer, whichwe will call `WordEmbeddingVectorizer`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `fit` method here is impotent—it does not do anything since we are using
    a pre-trained model from spaCy. We can use the newly created transformer as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Instead of the Naive Bayes classifier, we can also use this transformer with
    other classifiers, such as `LogisticRegression` or `Multi-layer Perceptron`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The `apply` function in pandas can be slow, especially when dealing with high
    volumes of data. I like to use a library called `tqdm`, which allows me to replace
    the `apply()` method with `progress_apply()`, which then displays a progress bar
    while running. All you have to do after importing the library is run `tqdm.pandas()`;
    this adds the `progress_apply()` method to the pandas Series and DataFrame objects.
    Fun fact: the word `tqdm`means *progress* in Arabic.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Personally, I find the field of natural language processing very exciting. The
    vast majority of our knowledge as humans is contained in books, documents, and
    web pages. Knowing how to automatically extract this information and organize
    it with the help of machine learning is essential to our scientific progress and
    endeavors in automation. This is why multiple scientific fields, such as information
    retrieval, statistics, and linguistics, borrow ideas from each other and try to
    solve the same problem from different angles. In this chapter, we also borrowed
    ideas from all these fields and learned how to represent textual data in formats
    suitable to machine learning algorithms. We also learned about the utilities that
    scikit-learn provides to aid in building and optimizing end-to-end solutions.
    We also encountered concepts such as transfer learning, and we were able to seamlessly
    incorporate spaCy's language models into scikit-learn.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: From the next chapter, we are going to deal with slightly advanced topics. In
    the next chapter, we will learn about artificial neural networks (multi-layer
    perceptron). This is a very hot topic nowadays, and understanding its main concepts
    helps anyone who wants to get deeper into deep learning. Since neural networks
    are commonly used in image processing, we will seize the opportunity to build
    on what we learned in [Chapter 5](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=28&action=edit),
    Image Processing with Nearest Neighbors and expand our image processing knowledge
    even further.************
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL

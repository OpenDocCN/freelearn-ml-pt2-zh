<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;From the Perceptron to Artificial Neural Networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. From the Perceptron to Artificial Neural Networks</h1></div></div></div><p>In <a class="link" href="ch08.html" title="Chapter 8. The Perceptron">Chapter 8</a>, <span class="emphasis"><em>The Perceptron</em></span>, we introduced the perceptron, which is a linear model for binary classification. You learned that the perceptron is not a universal function approximator; its decision boundary must be a hyperplane. In the previous chapter we introduced the support vector machine, which redresses some of the perceptron's limitations by using kernels to efficiently map the feature representations to a higher dimensional space in which the <a class="indexterm" id="id522"/>instances are linearly separable. In this chapter, we will discuss <span class="strong"><strong>artificial neural networks</strong></span>, which are powerful nonlinear models for classification and regression that use a different strategy to overcome the perceptron's limitations.</p><p>If the perceptron is analogous to a <a class="indexterm" id="id523"/>neuron, an artificial neural network, or <span class="strong"><strong>neural net</strong></span>, is analogous to a brain. As billions of neurons with trillions of synapses comprise a human brain, an artificial neural network is a directed graph of perceptrons or other artificial neurons. The graph's edges are weighted; these weights are the parameters of the model that must be learned.</p><p>Entire books describe individual aspects of artificial neural networks; this chapter will provide an overview of their structure and training. At the time of writing, some artificial neural networks have been developed for scikit-learn, but they are not available in Version 0.15.2. Readers can follow the examples in this chapter by checking out a fork of scikit-learn 0.15.1 that includes the neural network module. The implementations in this fork are likely to be merged into future versions of scikit-learn without any changes to the API described in this chapter.</p><div class="section" title="Nonlinear decision boundaries"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec65"/>Nonlinear decision boundaries</h1></div></div></div><p>Recall from <a class="link" href="ch08.html" title="Chapter 8. The Perceptron">Chapter 8</a>, <span class="emphasis"><em>The Perceptron</em></span>, that while some Boolean functions such as AND, OR, and NAND <a class="indexterm" id="id524"/>can be approximated by the perceptron, the linearly inseparable function XOR cannot, as shown in the following plots:</p><div class="mediaobject"><img alt="Nonlinear decision boundaries" src="graphics/8365OS_10_01.jpg"/></div><p>Let's review XOR in more detail to develop an intuition for the power of artificial neural networks. In contrast to AND, which outputs 1 when both of its inputs are equal to 1, and OR, which outputs 1 when at least one of the inputs are equal to 1, the output of XOR is 1 when exactly one of its inputs are equal to 1. We could view XOR as outputting 1 when two conditions are true. The first condition is that at least one of the inputs must be equal to 1; this is the same condition that OR tests. The second condition is that not both of the inputs are equal to 1; NAND tests this condition. We can produce the same output as XOR by processing the input with both OR and NAND and then verifying that the outputs of both functions are equal to 1 using AND. That is, the functions OR, NAND, and AND can be composed to produce the same output as XOR.</p><p>The following tables <a class="indexterm" id="id525"/>provide the truth tables for XOR, OR, AND, and NAND for the inputs <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span>. From these tables we can verify that inputting the output of OR and NAND to AND produces the same output as inputting <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> to XOR:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>A</p>
</th><th style="text-align: left" valign="bottom">
<p>B</p>
</th><th style="text-align: left" valign="bottom">
<p>A AND B</p>
</th><th style="text-align: left" valign="bottom">
<p>A NAND B</p>
</th><th style="text-align: left" valign="bottom">
<p>A OR B</p>
</th><th style="text-align: left" valign="bottom">
<p>A XOR B</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr></tbody></table></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>A</p>
</th><th style="text-align: left" valign="bottom">
<p>B</p>
</th><th style="text-align: left" valign="bottom">
<p>A OR B</p>
</th><th style="text-align: left" valign="bottom">
<p>A NAND B</p>
</th><th style="text-align: left" valign="bottom">
<p>(A OR B) AND (A NAND B)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr></tbody></table></div><p>Instead of trying to represent XOR with a single perceptron, we will build an artificial neural network from multiple artificial neurons that each approximate a linear function. Each instance's feature representation will be input to two neurons; one neuron will represent NAND and the other will represent OR. The output of these neurons will be received by a third neuron <a class="indexterm" id="id526"/>that represents AND to test whether both of XOR's conditions are true.</p></div></div>
<div class="section" title="Feedforward and feedback artificial neural networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec66"/>Feedforward and feedback artificial neural networks</h1></div></div></div><p>Artificial neural networks <a class="indexterm" id="id527"/>are described by three components. The first is the model's <span class="strong"><strong>architecture</strong></span>, or topology, which describes the layers of neurons and <a class="indexterm" id="id528"/>structure of the connections between them. The second component is the activation function used by the artificial neurons. The third component is the learning algorithm that finds the optimal values of the weights.</p><p>There are <a class="indexterm" id="id529"/>two main types of artificial neural networks. <span class="strong"><strong>Feedforward neural networks</strong></span> are the most common type of neural net, and are defined by their directed acyclic graphs. Signals only travel in one direction—towards the output layer—in <a class="indexterm" id="id530"/>feedforward neural networks. Conversely, <span class="strong"><strong>feedback neural networks</strong></span>, or recurrent neural networks, do contain cycles. The feedback cycles can represent an internal state for the network that can cause the network's behavior to change over time based on its input. Feedforward neural networks are commonly used to learn a function to map an input to an output. The temporal behavior of feedback neural networks makes them suitable for processing sequences of inputs. Because feedback neural networks are not implemented in scikit-learn, we will limit our discussion to only feedforward neural networks.</p><div class="section" title="Multilayer perceptrons"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec37"/>Multilayer perceptrons</h2></div></div></div><p>The <span class="strong"><strong>multilayer perceptron</strong></span> (<span class="strong"><strong>MLP</strong></span>) is the one of the most commonly used artificial neural networks. The name is a slight misnomer; a <a class="indexterm" id="id531"/>multilayer perceptron is not a single perceptron with multiple layers, but rather multiple layers of artificial neurons that can be perceptrons. The layers of the MLP form a directed, acyclic graph. Generally, each layer is fully connected to the <a class="indexterm" id="id532"/>subsequent layer; the output of each artificial neuron in a layer is an input to every artificial neuron in the <a class="indexterm" id="id533"/>next layer towards the output. MLPs have three or more layers of artificial neurons.</p><p>The <span class="strong"><strong>input layer</strong></span> consists of <a class="indexterm" id="id534"/>simple input neurons. The input neurons are <a class="indexterm" id="id535"/>connected to at least one <span class="strong"><strong>hidden layer</strong></span> of artificial neurons. The hidden layer represents latent variables; the input and output of this layer cannot be observed in the training data. Finally, the last hidden layer is connected to <a class="indexterm" id="id536"/>an <span class="strong"><strong>output </strong></span>
<a class="indexterm" id="id537"/>
<span class="strong"><strong>layer</strong></span>. The following diagram depicts the architecture of a multilayer perceptron with three layers. The neurons labeled <span class="strong"><strong>+1</strong></span> are bias neurons and are not depicted in most architecture diagrams.</p><div class="mediaobject"><img alt="Multilayer perceptrons" src="graphics/8365OS_10_02.jpg"/></div><p>The artificial neurons, or <span class="strong"><strong>units</strong></span>, in the <a class="indexterm" id="id538"/>hidden layer commonly use nonlinear activation functions such as the hyperbolic tangent function and the logistic function, which are given by the following equations:</p><div class="mediaobject"><img alt="Multilayer perceptrons" src="graphics/8365OS_10_03.jpg"/></div><div class="mediaobject"><img alt="Multilayer perceptrons" src="graphics/8365OS_10_04.jpg"/></div><p>As with other supervised models, our goal is to find the values of the weights that minimize the value of a cost function. The mean squared error cost function is commonly used with multilayer perceptrons. It is given by the following equation, where <span class="emphasis"><em>m</em></span> is the number of training instances:</p><div class="mediaobject"><img alt="Multilayer perceptrons" src="graphics/8365OS_10_05.jpg"/></div></div><div class="section" title="Minimizing the cost function"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec38"/>Minimizing the cost function</h2></div></div></div><p>The <span class="strong"><strong>backpropagation</strong></span> <a class="indexterm" id="id539"/>algorithm is commonly used in conjunction with an <a class="indexterm" id="id540"/>optimization algorithm such as gradient descent to minimize the value of the cost function. The algorithm takes its name from a portmanteau of <span class="emphasis"><em>backward propagation</em></span>, and refers to the direction in which errors flow through the layers of the network. Backpropagation can theoretically be used to train a feedforward network with any number of hidden units arranged in any number of layers, though computational power constrains this capability.</p><p>Backpropagation is similar to gradient descent in that it uses the gradient of the cost function to update the values of the model parameters. Unlike the linear models we have previously seen, neural nets contain hidden units that represent latent variables; we can't tell what the hidden units should do from the training data. If we do not know what the hidden units should do, we cannot calculate their errors and we cannot calculate the gradient of cost function with respect to their weights. A naive solution to overcome this is to randomly perturb the weights for the hidden units. If a random change to one of the weights decreases the value of the cost function, we save the change and randomly change the value of another weight. An obvious problem with this solution is its prohibitive computational cost. Backpropagation provides a more efficient solution.</p><p>We will step through training a feedforward neural network using backpropagation. This network has two input units, two hidden layers that both have three hidden units, and two output units. The input units are both fully connected to the first hidden layer's units, called <code class="literal">Hidden1</code>, <code class="literal">Hidden2</code>, and <code class="literal">Hidden3</code>. The edges connecting the units are initialized to small random weights.</p></div><div class="section" title="Forward propagation"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec39"/>Forward propagation</h2></div></div></div><p>During the forward <a class="indexterm" id="id541"/>propagation stage, the features are input to the network and fed through the subsequent layers to produce the output activations. First, we compute the activation for the unit <code class="literal">Hidden1</code>. We find the weighted sum of input to <code class="literal">Hidden1</code>, and then process the sum with the activation function. Note that <code class="literal">Hidden1</code> receives a constant input from a bias unit that is not depicted in the diagram in addition to the inputs from the input units. In the following diagram, <span class="inlinemediaobject"><img alt="Forward propagation" src="graphics/8365OS_10_42.jpg"/></span> is the activation function:</p><div class="mediaobject"><img alt="Forward propagation" src="graphics/8365OS_10_21.jpg"/></div><p>Next, we compute the <a class="indexterm" id="id542"/>activation for the second hidden unit. Like the first hidden unit, it receives weighted inputs from both of the input units and a constant input from a bias unit. We then process the weighted sum of the inputs, or <span class="strong"><strong>preactivation</strong></span>, with the <a class="indexterm" id="id543"/>activation function as shown in the following figure:</p><div class="mediaobject"><img alt="Forward propagation" src="graphics/8365OS_10_22.jpg"/></div><p>We then compute the <a class="indexterm" id="id544"/>activation for <code class="literal">Hidden3</code> in the same manner:</p><div class="mediaobject"><img alt="Forward propagation" src="graphics/8365OS_10_23.jpg"/></div><p>Having computed the activations of all of the hidden units in the first layer, we proceed to the second hidden layer. In <a class="indexterm" id="id545"/>this network, the first hidden layer is fully connected to the second hidden layer. Similar to the units in the first hidden layer, the units in the second hidden layer receive a constant input from bias units that are not depicted in the diagram. We proceed to compute the activation of <code class="literal">Hidden4</code>:</p><div class="mediaobject"><img alt="Forward propagation" src="graphics/8365OS_10_24.jpg"/></div><p>We next compute the activations of <code class="literal">Hidden5</code> and <code class="literal">Hidden6</code>. Having computed the activations of all of the hidden units in the second hidden layer, we proceed to the output layer in the following figure. The <a class="indexterm" id="id546"/>activation of <code class="literal">Output1</code> is the weighted sum of the second hidden layer's activations processed through an activation function. Similar to the hidden units, the output units both receive a constant input from a bias unit:</p><div class="mediaobject"><img alt="Forward propagation" src="graphics/8365OS_10_25.jpg"/></div><p>We calculate the activation <a class="indexterm" id="id547"/>of <code class="literal">Output2</code> in the same manner:</p><div class="mediaobject"><img alt="Forward propagation" src="graphics/8365OS_10_26.jpg"/></div><p>We computed the activations of all of the units in the network, and we have now completed forward propagation. The <a class="indexterm" id="id548"/>network is not likely to approximate the true function well using the initial random values of the weights. We must now update the values of the weights so that the network can better approximate our function.</p></div><div class="section" title="Backpropagation"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec40"/>Backpropagation</h2></div></div></div><p>We can calculate the error <a class="indexterm" id="id549"/>of the network only at the output units. The hidden units represent latent variables; we cannot observe their true values in the training data and thus, we have nothing to compute their error against. In order to update their weights, we must propagate the network's errors backwards through its layers. We will begin with <code class="literal">Output1</code>. Its error is equal to the difference between the true and predicted outputs, multiplied by the partial derivative of the unit's activation:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_27.jpg"/></div><p>We then calculate the <a class="indexterm" id="id550"/>error of the second output unit:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_28.jpg"/></div><p>We computed the errors of the output layer. We can now propagate these errors backwards to the second hidden layer. First, we will compute the error of hidden unit <code class="literal">Hidden4</code>. We multiply the <a class="indexterm" id="id551"/>error of <code class="literal">Output1</code> by the value of the weight connecting <code class="literal">Hidden4</code> and <code class="literal">Output1</code>. We similarly weigh the error of <code class="literal">Output2</code>. We then add these errors and calculate the product of their sum and the partial derivative of <code class="literal">Hidden4</code>:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_38.jpg"/></div><p>We similarly compute <a class="indexterm" id="id552"/>the errors of  <code class="literal">Hidden5</code>:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_29.jpg"/></div><p>We then compute <a class="indexterm" id="id553"/>the <code class="literal">Hidden6</code> error in the following figure:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_30.jpg"/></div><p>We calculated the error of the second hidden layer with respect to the output layer. Next, we will continue to propagate <a class="indexterm" id="id554"/>the errors backwards towards the input layer. The error of the hidden unit <code class="literal">Hidden1</code> is the product of its partial derivative and the weighted sums of the errors in the second hidden layer:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_31.jpg"/></div><p>We similarly compute the <a class="indexterm" id="id555"/>error for hidden unit <code class="literal">Hidden2</code>:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_32.jpg"/></div><p>We similarly compute the <a class="indexterm" id="id556"/>error for <code class="literal">Hidden3</code>:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_33.jpg"/></div><p>We computed the errors of the first hidden layer. We can now use these errors to update the values of the weights. We will first update the weights for the edges connecting the input units to <code class="literal">Hidden1</code> as well as the weight for the edge connecting the bias unit to <code class="literal">Hidden1</code>. We will increment the <a class="indexterm" id="id557"/>value of the weight connecting <code class="literal">Input1</code> and <code class="literal">Hidden1</code> by the product of the learning rate, error of <code class="literal">Hidden1</code>, and the value of <code class="literal">Input1</code>. </p><p>We will similarly increment the value of <code class="literal">Weight2</code> by the product of the learning rate, error of <code class="literal">Hidden1</code>, and the value of <code class="literal">Input2</code>. Finally, we will increment the value of the weight connecting the bias unit to <code class="literal">Hidden1</code> by the product of the learning rate, error of <code class="literal">Hidden1</code>, and one.</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_34.jpg"/></div><p>We will then update the <a class="indexterm" id="id558"/>values of the weights connecting hidden unit <code class="literal">Hidden2</code> to the input units and the bias unit using the same method:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_35.jpg"/></div><p>Next, we will update the <a class="indexterm" id="id559"/>values of the weights connecting the input layer to <code class="literal">Hidden3</code>:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_36.jpg"/></div><p>Since the values of the weights connecting the input layer to the first hidden layer <a class="indexterm" id="id560"/>is updated, we can continue to the weights connecting the first hidden layer to the second hidden layer. We will increment the value of <code class="literal">Weight7</code> by the product of the learning rate, error of <code class="literal">Hidden4</code>, and the output of <code class="literal">Hidden1</code>. We continue to similarly update the values of weights <code class="literal">Weight8</code> to <code class="literal">Weight15</code>:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_37.jpg"/></div><p>The weights for <code class="literal">Hidden5</code> and <code class="literal">Hidden6</code> are updated in the same way. We updated the values of the weights connecting the two hidden layers. We can now update the values of the weights connecting the <a class="indexterm" id="id561"/>second hidden layer and the output layer. We increment the values of weights <code class="literal">W16</code> through <code class="literal">W21</code> using the same method that we used for the weights in the previous layers:</p><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_40.jpg"/></div><div class="mediaobject"><img alt="Backpropagation" src="graphics/8365OS_10_41.jpg"/></div><p>After incrementing the value of <code class="literal">Weight21</code> by the product of the learning rate, error of <code class="literal">Output2</code>, and the activation of <code class="literal">Hidden6</code>, we have finished updating the values of the weights for the network. We can now perform another forward pass using the new values of the weights; the value of the cost function produced using the updated weights should be smaller. We will repeat this <a class="indexterm" id="id562"/>process until the model converges or another stopping criterion is satisfied. Unlike the linear models we have discussed, backpropagation does not optimize a convex function. It is possible that backpropagation will converge on parameter values that specify a local, rather than global, minimum. In practice, local optima are frequently adequate for many applications.</p></div></div>
<div class="section" title="Approximating XOR with Multilayer perceptrons"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec67"/>Approximating XOR with Multilayer perceptrons</h1></div></div></div><p>Let's train a <a class="indexterm" id="id563"/>multilayer perceptron to approximate the XOR function. At the time of writing, multilayer perceptrons have been implemented as part <a class="indexterm" id="id564"/>of a 2014 Google Summer of Code project, but have not been merged or released. Subsequent versions of scikit-learn are likely to include this implementation of multilayer perceptrons without any changes to the API described in this section. In the interim, a fork of scikit-learn 0.15.1 that includes the multilayer perceptron implementation can be cloned from <a class="ulink" href="https://github.com/IssamLaradji/scikit-learn.git">https://github.com/IssamLaradji/scikit-learn.git</a>.</p><p>First, we will create a toy binary classification dataset that represents XOR and split it into training and testing sets:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.cross_validation import train_test_split
&gt;&gt;&gt; from sklearn.neural_network import MultilayerPerceptronClassifier
&gt;&gt;&gt; y = [0, 1, 1, 0] * 1000
&gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0], [1, 1]] * 1000
&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)</pre></div><p>Next we instantiate <code class="literal">MultilayerPerceptronClassifier</code>. We specify the architecture of the network through the <code class="literal">n_hidden</code> keyword argument, which takes a list of the number of hidden units in each hidden layer. We create a hidden layer with two units that use the logistic activation function. The <code class="literal">MultilayerPerceptronClassifier</code> class automatically creates two input units and one output unit. In multi-class problems the classifier will create one output unit for each of the possible classes.</p><p>Selecting an architecture is challenging. There are some rules of thumb to choose the numbers of hidden <a class="indexterm" id="id565"/>units and layers, but these tend to be supported only by anecdotal evidence. The optimal number of hidden units depends on the number of training instances, the noise in the training data, the complexity of the function that is being approximated, the hidden units' activation function, the learning algorithm, and the regularization employed. In practice, architectures can only be <a class="indexterm" id="id566"/>evaluated by comparing their performances through cross validation.</p><p>We train the network by calling the fit() method:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; clf = MultilayerPerceptronClassifier(n_hidden=[2],
&gt;&gt;&gt;                                      activation='logistic',
&gt;&gt;&gt;                                      algorithm='sgd',
&gt;&gt;&gt;                                      random_state=3)
&gt;&gt;&gt; clf.fit(X_train, y_train)</pre></div><p>Finally, we print some predictions for manual inspection and evaluate the model's accuracy on the test set. The network perfectly approximates the XOR function on the test set:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; print 'Number of layers: %s. Number of outputs: %s' % (clf.n_layers_, clf.n_outputs_)
&gt;&gt;&gt; predictions = clf.predict(X_test)
&gt;&gt;&gt; print 'Accuracy:', clf.score(X_test, y_test)
&gt;&gt;&gt; for i, p in enumerate(predictions[:10]):
&gt;&gt;&gt;     print 'True: %s, Predicted: %s' % (y_test[i], p)
Number of layers: 3. Number of outputs: 1
Accuracy: 1.0
True: 1, Predicted: 1
True: 1, Predicted: 1
True: 1, Predicted: 1
True: 0, Predicted: 0
True: 1, Predicted: 1
True: 0, Predicted: 0
True: 0, Predicted: 0
True: 1, Predicted: 1
True: 0, Predicted: 0
True: 1, Predicted: 1</pre></div></div>
<div class="section" title="Classifying handwritten digits"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec68"/>Classifying handwritten digits</h1></div></div></div><p>In the previous chapter <a class="indexterm" id="id567"/>we used a support vector machine to classify the handwritten digits in the MNIST dataset. In this section we will classify the images using an artificial neural network:</p><div class="informalexample"><pre class="programlisting">from sklearn.datasets import load_digits
from sklearn.cross_validation import train_test_split, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network.multilayer_perceptron import MultilayerPerceptronClassifier</pre></div><p>First we use the <code class="literal">load_digits</code> convenience function to load the MNIST dataset. We will fork additional processes during cross validation, which requires execution from a <code class="literal">main-</code>protected block:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; if __name__ == '__main__':
&gt;&gt;&gt;     digits = load_digits()
&gt;&gt;&gt;     X = digits.data
&gt;&gt;&gt;     y = digits.target</pre></div><p>Scaling the features is particularly important for artificial neural networks and will help some learning algorithms to converge more quickly. Next, we create a <code class="literal">Pipeline </code>class that scales the data before fitting a <code class="literal">MultilayerPerceptronClassifier</code>. This network contains an input layer, a hidden layer with 150 units, a hidden layer with 100 units, and an output layer. We also increased the value of the regularization hyperparameter <code class="literal">alpha</code> argument. Finally, we print the accuracies of the three cross validation folds. The code is as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt;     pipeline = Pipeline([
&gt;&gt;&gt;         ('ss', StandardScaler()),
&gt;&gt;&gt;         ('mlp', MultilayerPerceptronClassifier(n_hidden=[150, 100], alpha=0.1))
&gt;&gt;&gt;     ])
&gt;&gt;&gt;     print cross_val_score(pipeline, X, y, n_jobs=-1)
Accuracies [ 0.95681063  0.96494157  0.93791946]</pre></div><p>The mean accuracy is comparable to the accuracy of the support vector classifier. Adding more hidden units or <a class="indexterm" id="id568"/>hidden layers and grid searching to tune the hyperparameters could further improve the accuracy.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec69"/>Summary</h1></div></div></div><p>In this chapter, we introduced artificial neural networks, powerful models for classification and regression that can represent complex functions by composing several artificial neurons. In particular, we discussed directed acyclic graphs of artificial neurons called feedforward neural networks. Multilayer perceptrons are a type of feedforward network in which each layer is fully connected to the subsequent layer. An MLP with one hidden layer and a finite number of hidden units is a universal function approximator. It can represent any continuous function, though it will not necessarily be able to learn appropriate weights automatically. We described how the hidden layers of a network represent latent variables and how their weights can be learned using the backpropagation algorithm. Finally, we used scikit-learn's multilayer perceptron implementation to approximate the function XOR and to classify handwritten digits.</p><p>This chapter concludes the book. We discussed a variety of models, learning algorithms, and performance measures, as well as their implementations in scikit-learn. In the first chapter, we described machine learning programs as those that learn from experience to improve their performance at a task. Then, we worked through examples that demonstrated some of the most common experiences, tasks, and performance measures in machine learning. We regressed the prices of pizzas onto their diameters and classified spam and ham text messages. We clustered colors to compress images and clustered the SURF descriptors to recognize photographs of cats and dogs. We used principal component analysis for facial recognition, built a random forest to block banner advertisements, and used support vector machines and artificial neural networks for optical character recognition. Thank you for reading; I hope that you will be able to use scikit-learn and this book's examples to apply machine learning to your own experiences.</p></div></body></html>
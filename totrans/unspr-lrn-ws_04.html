<html><head></head><body>
		<div>
			<div id="_idContainer063" class="Content">
			</div>
		</div>
		<div id="_idContainer064" class="Content">
			<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>3. Neighborhood Approaches and DBSCAN</h1>
		</div>
		<div id="_idContainer079" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will see how neighborhood approaches to clustering work from start to end and implement the <strong class="bold">Density-Based Spatial Clustering of Applications with Noise</strong> (<strong class="bold">DBSCAN</strong>) algorithm from scratch by using packages. We will also identify the most suitable algorithm to solve your problem from k-means, hierarchical clustering, and DBSCAN. By the end of this chapter, we will see how the DBSCAN clustering approach will serve us best in the sphere of highly complex data.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor060"/>Introduction</h1>
			<p>In previous chapters, we evaluated a number of different approaches to data clustering, including k-means and hierarchical clustering. While k-means is the simplest form of clustering, it is still extremely powerful in the right scenarios. In situations where k-means can't capture the complexity of the dataset, hierarchical clustering proves to be a strong alternative.</p>
			<p>One of the key challenges in unsupervised learning is that you will be presented with a collection of feature data but no complementary labels telling you what a target state will be. While you may not get a discrete view of what the target labels are, you can get some semblance of structure out of the data by clustering similar groups together and seeing what is similar within groups. The first approach we covered to achieve this goal of clustering similar data points is k-means. K-means clustering works best for simple data challenges where speed is paramount. Simply looking at the closest data point (cluster centroid) does not require a lot of computational overhead; however, there is also a greater challenge posed when it comes to higher-dimensional datasets. K-means clustering is also not ideal if you are unaware of the potential number of clusters you are looking for. An example we worked with in <em class="italic">Chapter 2</em>, <em class="italic">Hierarchical Clustering</em>, entailed looking at chemical profiles to determine which wines belonged together in a disorganized shipment. This exercise only worked well because we knew that three wine types were ordered; however, k-means would have been less successful if you had no idea regarding what the original order constituted.</p>
			<p>The second clustering approach we explored was hierarchical clustering. This method can work in two ways – either agglomerative or divisive. Agglomerative clustering works with a bottom-up approach, treating each data point as its own cluster and recursively grouping them together with linkage criteria. Divisive clustering works in the opposite way by treating all data points as one large class and recursively breaking them down into smaller clusters. This approach has the benefit of fully understanding the entire data distribution, as it calculates splitting potential; however, it is typically not implemented in practice due to its greater complexity. Hierarchical clustering is a strong contender for your clustering needs when it comes to not knowing anything about the data. Using a dendrogram, you can visualize all the splits in your data and consider what number of clusters makes sense after the fact. This can be really helpful in your specific use case; however, it also comes at a higher computational cost than is associated with k-means. </p>
			<p>In this chapter, we will cover a clustering approach that will serve us best in the sphere of highly complex data: <strong class="bold">Density-Based Spatial Clustering of Applications with Noise</strong> (<strong class="bold">DBSCAN</strong>). Canonically, this method has always been seen as a high performer in datasets that have a lot of densely interspersed data. Let's walk through why it does so well in these use cases.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor061"/>Clusters as Neighborhoods</h1>
			<p>Until now, we have explored the concept of likeness being described as a function of Euclidean distance – data points that are closer to any one point can be seen as similar, while those that are further away in Euclidean space can be seen as dissimilar. This notion is seen once again in the DBSCAN algorithm. As alluded to by the lengthy name, the DBSCAN approach expands upon basic distance metric evaluation by also incorporating the notion of density. If there are clumps of data points that all exist in the same area as one another, they can be seen as members of the same cluster:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B15923_03_01.jpg" alt="Figure 3.1: Neighbors have a direct connection to clusters"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1: Neighbors have a direct connection to clusters</p>
			<p>In the preceding figure, we can see four neighborhoods. The density-based approach has a number of benefits when compared to the past approaches we've covered that focus exclusively on distance. If you were just focusing on distance as a clustering threshold, then you may find your clustering makes little sense if faced with a sparse feature space with outliers. Both k-means and hierarchical clustering will automatically group together all data points in the space until no points are left.</p>
			<p>While hierarchical clustering does provide a path around this issue somewhat, since you can dictate where clusters are formed using a dendrogram post-clustering run, k-means is the most susceptible to failure as it is the simplest approach to clustering. These pitfalls are less evident when we begin evaluating neighborhood approaches to clustering. In the following dendrogram, you can see an example of the pitfall where all data points are grouped together. Clearly, as you travel down the dendrogram, there is a lot of potential variation that gets grouped together since every point needs to be a member of a cluster. This is less of an issue with neighborhood-based clustering:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B15923_03_02.jpg" alt="Figure 3.2: Example dendrogram"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2: Example dendrogram</p>
			<p>By incorporating the notion of neighbor density in DBSCAN, we can leave outliers out of clusters if we choose to, based on the hyperparameters we choose at runtime. Only the data points that have close neighbors will be seen as members within the same cluster, and those that are farther away can be left as unclustered outliers.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Introduction to DBSCAN</h1>
			<p>In DBSCAN, density is evaluated as a combination of neighborhood radius and minimum points found in a neighborhood deemed a cluster. This concept can be driven home if we reconsider the scenario where you are tasked with organizing an unlabeled shipment of wine for your store. In the previous example, it was made clear that we can find similar wines based on their features, such as chemical traits. Knowing this information, we can more easily group together similar wines and efficiently have our products organized for sale in no time. In the real world, however, the products that you order to stock your store will reflect real-world purchase patterns. To promote variety in your inventory, but still have sufficient stock of the most popular wines, there is a highly uneven distribution of product types that you have available. Most people love the classic wines, such as white and red; however, you may still carry more exotic wines for your customers who love expensive varieties. This makes clustering more difficult, since there are uneven class distributions (you don't order 10 bottles of every wine available, for example).</p>
			<p>DBSCAN differs from k-means and hierarchical clustering because you can build this intuition into how we evaluate the clusters of customers we are interested in forming. It can cut through the noise in an easier fashion and only point out customers who have the highest potential for remarketing in a campaign.</p>
			<p>By clustering through the concept of a neighborhood, we can separate out the one-off customers who can be seen as random noise, relative to the more valuable customers who come back to our store time and time again. This approach calls into question how we establish the best numbers when it comes to neighborhood radius and minimum points per neighborhood.</p>
			<p>As a high-level heuristic, we want our neighborhood radius to be small, but not too small. At one end of the extreme, you can have the neighborhood radius quite high – this can max out at treating all points in the feature space as one massive cluster. At the opposite end of the extreme, you can have a very small neighborhood radius. Overly small neighborhood radii can result in no points being clustered together and having a large collection of single-member clusters.</p>
			<p>Similar logic applies when it comes to the minimum number of points that can make up a cluster. Minimum points can be seen as a secondary threshold that tunes the neighborhood radius a bit, depending on what data you have available in your space. If all of the data in your feature space is extremely sparse, minimum points become extremely valuable, in tandem with the neighborhood radius, to make sure you don't just have a large number of uncorrelated data points. When you have very dense data, the minimum points threshold becomes less of a driving factor than neighborhood radius.</p>
			<p>As you can see from these two hyperparameter rules, the best options are, as usual, dependent on what your dataset looks like. Oftentimes, you will want to find the perfect "goldilocks" zone of not being too small in your hyperparameters, but also not too large.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor063"/>DBSCAN in Detail</h2>
			<p>To see how DBSCAN works, we can trace the path of a simple toy program as it merges together to form a variety of clusters and noise-labeled data points:</p>
			<ol>
				<li>Out of <em class="italic">n</em> unvisited sample data points, we'll first move through each point in a loop and mark each one as visited.</li>
				<li>From each point, we'll look at the distance to every other point in the dataset.</li>
				<li>All points that fall within the neighborhood radius hyperparameter should be considered as neighbors.</li>
				<li>The number of neighbors should be at least as many as the minimum points required.</li>
				<li>If the minimum point threshold is reached, the points should be grouped together as a cluster, or else marked as noise.</li>
				<li>This process should be repeated until all data points are categorized in clusters or as noise.</li>
			</ol>
			<p>DBSCAN is fairly straightforward in some senses – while there are the new concepts of density through neighborhood radius and minimum points, at its core, it is still just evaluating using a distance metric. </p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/>Walkthrough of the DBSCAN Algorithm</h2>
			<p>The following steps will walk you through this path in slightly more detail:</p>
			<ol>
				<li value="1">Given six sample data points, view each point as its own cluster [ (1,3) ], [ (-8,6) ], [ (-6,4) ] , [ (4,-2) ], ] (2,5) ], [ (-2,0) ]:<div id="_idContainer067" class="IMG---Figure"><img src="image/B15923_03_03.jpg" alt="Figure 3.3: Plot of sample data points"/></div><p class="figure-caption">Figure 3.3: Plot of sample data points</p></li>
				<li>Calculate the pairwise Euclidean distance between each of the points:<div id="_idContainer068" class="IMG---Figure"><img src="image/B15923_03_04.jpg" alt="Figure 3.4: Point distances"/></div><p class="figure-caption">Figure 3.4: Point distances</p></li>
				<li>From each point, expand a neighborhood size outward and form clusters. For the purpose of this example, imagine you pass through a neighborhood radius of five. This means that any two points will be neighbors if the distance between them is less than five units. For example, point (1,3) has points (2,5) and (-2,0) as neighbors.<p>Depending on the number of points in the neighborhood of a given point, the point can be classified into the following three categories:</p><p><strong class="bold">Core Point</strong>: If the point under observation has data points greater than the minimum number of points in its neighborhood that make up a cluster, then that point is called a core point of the cluster. All core points within the neighborhood of other core points are part of the same cluster. However, all the core points that are not in same neighborhood are part of another cluster.</p><p><strong class="bold">Boundary Point</strong>: If the point under observation does not have sufficient neighbors (data points) of its own, but it has at least one core point (in its neighborhood), then that point represents the boundary point of the cluster. Boundary points belong to the same cluster of their nearest core point. </p><p><strong class="bold">Noise Point</strong>: A data point is treated as a noise point if it does not have the required minimum number of data points in its neighborhood and is not associated with a core point. This point is treated as pure noise and is excluded from clustering.</p></li>
				<li>Points that have neighbors are then evaluated to see whether they pass the minimum points threshold. In this example, if we had passed through a minimum points threshold of two, then points (1,3), (2,5), and (-2,0) could formally be grouped together as a cluster. If we had a minimum points threshold of four, then these three data points would be considered superfluous noise.</li>
				<li>Points that have fewer neighbors than the minimum number of neighboring points required and whose neighborhood does not contain a core point are marked as noise and remain unclustered. Thus, points (-6,4), (4,-2), and (-8,6) fall under this category. However, points such as (2,5) and (2,0), though don't satisfy the criteria of the minimum number of points in neighborhood, do contain a core point as their neighbor, and are therefore marked as boundary points. </li>
				<li>The following table summarizes the neighbors of a particular point and classifies them as core, boundary, and noise data points (mentioned in the preceding step) for a neighborhood radius of 5 and a minimum-neighbor criterion of 2.<div id="_idContainer069" class="IMG---Figure"><img src="image/B15923_03_05.jpg" alt="Figure 3.5: Table showing details of neighbors for given points"/></div><p class="figure-caption">Figure 3.5: Table showing details of neighbors for given points</p></li>
				<li>Repeat this process on any remaining unvisited data points.</li>
			</ol>
			<p>At the end of this process, you will have sorted your entire dataset into either clusters or unrelated noise. DBSCAN performance is highly dependent on the threshold hyperparameters you choose. This means that you may have to run DBSCAN a couple of times with different hyperparameter options to get an understanding of how they influence overall performance.</p>
			<p>Note that DBSCAN does not require the centroids that we saw in both k-means and centroid-focused implementation of hierarchical clustering. This feature allows DBSCAN to work better for complex datasets, since most data is not shaped like clean blobs. DBSCAN is also more effective against outliers and noise than k-means or hierarchical clustering.</p>
			<p>Let's now see how the performance of DBSCAN changes with varying neighborhood radius sizes.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Exercise 3.01: Evaluating the Impact of Neighborhood Radius Size</h2>
			<p>For this exercise, we will work in reverse of what we have typically seen in previous examples by first seeing the packaged implementation of DBSCAN in scikit-learn, and then implementing it on our own. This is done on purpose to fully explore how different neighborhood radius sizes drastically impact DBSCAN performance. </p>
			<p>By completing this exercise, you will become familiar with how tuning neighborhood radius size can change how well DBSCAN performs. It is important to understand these facets of DBSCAN, as they can save you time in the future by troubleshooting your clustering algorithms efficiently: </p>
			<ol>
				<li value="1">Import the packages from scikit-learn and matplotlib that are necessary for this exercise:<p class="source-code">from sklearn.cluster import DBSCAN</p><p class="source-code">from sklearn.datasets import make_blobs</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p></li>
				<li>Generate a random cluster dataset to experiment on; X = coordinate points, and y = cluster labels (not needed):<p class="source-code">X, y = make_blobs(n_samples=1000, centers=8, \</p><p class="source-code">                  n_features=2, random_state=800)</p><p class="source-code"># Visualize the data</p><p class="source-code">plt.scatter(X[:,0], X[:,1])</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer070" class="IMG---Figure"><img src="image/B15923_03_06.jpg" alt="Figure 3.6: Visualized toy data example"/></div><p class="figure-caption">Figure 3.6: Visualized toy data example</p></li>
				<li>After plotting the dummy data for this toy problem, you will see that the dataset has two features and approximately seven to eight clusters. To implement DBSCAN using scikit-learn, you will need to instantiate a new scikit-learn class:<p class="source-code">db = DBSCAN(eps=0.5, min_samples=10, metric='euclidean')</p><p>Our example DBSCAN instance is stored in the <strong class="source-inline">db</strong> variable, and our hyperparameters are passed through on creation. For the sake of this example, you can see that the neighborhood radius (<strong class="source-inline">eps</strong>) is set to <strong class="source-inline">0.5</strong>, while the minimum number of points is set to <strong class="source-inline">10</strong>. To keep in line with our past chapters, we will once again be using Euclidean distance as our distance metric.</p><p class="callout-heading">Note</p><p class="callout"><strong class="source-inline">eps</strong> stands for epsilon and is the radius of the neighborhood that your algorithm will look within when searching for neighbors.</p></li>
				<li>Let's set up a loop that allows us to explore potential neighborhood radius size options interactively:<p class="source-code">eps = [0.2,0.7,4]</p><p class="source-code">for ep in eps:</p><p class="source-code">    db = DBSCAN(eps=ep, min_samples=10, metric='euclidean')</p><p class="source-code">    plt.scatter(X[:,0], X[:,1], c=db.fit_predict(X))</p><p class="source-code">    plt.title('Toy Problem with eps: ' + str(ep))</p><p class="source-code">    plt.show()</p><p>The preceding code results in the following plots:</p><div id="_idContainer071" class="IMG---Figure"><img src="image/B15923_03_07.jpg" alt="Figure 3.7: Resulting plots"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.7: Resulting plots</p>
			<p>As you can see from the plots, setting our neighborhood size too small will cause everything to be seen as random noise (purple points). Bumping our neighborhood size up a little bit allows us to form clusters that make more sense. A larger epsilon value would again convert the entire dataset into a single cluster (purple data points). Try recreating the preceding plots and experiment with varying <strong class="source-inline">eps</strong> sizes.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gEijGC">https://packt.live/3gEijGC</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2ZPBfeJ">https://packt.live/2ZPBfeJ</a>.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>DBSCAN Attributes – Neighborhood Radius</h2>
			<p>In the preceding exercise, you saw how impactful setting the proper neighborhood radius is on the performance of your DBSCAN implementation. If your neighborhood is too small, then you will run into issues where all the data will be treated as noise and is left unclustered. If you set your neighborhood too large, then all of the data will similarly be grouped together into one cluster and not provide any value. If you explored the preceding exercise further with your own <strong class="source-inline">eps</strong> sizes, you may have noticed that it is very difficult to perform effective clustering using only the neighborhood size. This is where a minimum points threshold comes in handy. We will visit that topic later.</p>
			<p>To go deeper into the neighborhood concept of DBSCAN, let's take a deeper look at the <strong class="source-inline">eps</strong> hyperparameter you pass at instantiation time. This epsilon value is converted to a radius that sweeps around any given data point in a circular manner to serve as a neighborhood:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B15923_03_08.jpg" alt="Figure 3.8: Visualization of the neighborhood radius; the red circle is the neighborhood"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8: Visualization of the neighborhood radius; the red circle is the neighborhood</p>
			<p>In this instance, there will be four neighbors of the center point, as can be seen in the preceding plot.</p>
			<p>One key aspect to observe here is that the shape formed by your neighborhood search is a circle in two dimensions, and a sphere in three dimensions. This may impact the performance of your model simply based on how the data is structured. Once again, blobs may seem like an intuitive structure to find – this may not always be the case. Fortunately, DBSCAN is well equipped to handle this dilemma of clusters that you may be interested in, but that do not fit the explicit blob structure:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B15923_03_09.jpg" alt="Figure 3.9: Impact of varying neighborhood radius size"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9: Impact of varying neighborhood radius size</p>
			<p>On the left, the data point will be classified as random noise. On the right, the data point has multiple neighbors and could be its own cluster.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Activity 3.01: Implementing DBSCAN from Scratch</h2>
			<p>During an interview, you are asked to create the DBSCAN algorithm from scratch using a generated two-dimensional dataset. To do this, you will need to convert the theory behind neighborhood searching into production code, with a recursive call that adds neighbors. As explained in the previous section, you will use a distance scan in space surrounding a specified point to add these neighbors.</p>
			<p>Given what you've learned about DBSCAN and distance metrics from prior chapters, build an implementation of DBSCAN from scratch in Python. You are free to use NumPy and SciPy to evaluate distances here.</p>
			<p>These steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Generate a random cluster dataset.</li>
				<li>Visualize the data.</li>
				<li>Create functions from scratch that allow you to call DBSCAN on a dataset.</li>
				<li>Use your created DBSCAN implementation to find clusters in the generated dataset. Feel free to use hyperparameters as you see fit, tuning them based on their performance.</li>
				<li>Visualize the clustering performance of your DBSCAN implementation from scratch.</li>
			</ol>
			<p>The desired outcome of this exercise is for you to implement how DBSCAN works from the ground up before you use the fully packaged implementation in scikit-learn. Taking this approach to any machine learning algorithm from scratch is important, as it helps you "earn" the ability to use easier implementations, while still being able to discuss DBSCAN in depth in the future:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B15923_03_10.jpg" alt="Figure 3.10: Expected outcome"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10: Expected outcome</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 428.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/>DBSCAN Attributes – Minimum Points</h2>
			<p>The other core component to a successful implementation of DBSCAN beyond the neighborhood radius is the minimum number of points required to justify membership within a cluster. As mentioned earlier, it is more obvious that this lower bound benefits your algorithm when it comes to sparser datasets. That's not to say that it is a useless parameter when you have very dense data; however, while having single data points randomly interspersed through your feature space can be easily bucketed as noise, it becomes more of a gray area when we have random patches of two to three, for example. Should these data points be their own cluster, or should they also be categorized as noise? Minimum points thresholding helps to solve this problem.</p>
			<p>In the scikit-learn implementation of DBSCAN, this hyperparameter is seen in the <strong class="source-inline">min_samples</strong> field passed on DBSCAN instance creation. This field is very valuable in tandem with the neighborhood radius size hyperparameter to fully round out your density-based clustering approach:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B15923_03_11.jpg" alt="Figure 3.11: Minimum points threshold deciding whether a group of data points is noise or a cluster"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11: Minimum points threshold deciding whether a group of data points is noise or a cluster</p>
			<p>On the right, if the minimum points threshold is 10 points, it will classify data in this neighborhood as noise.</p>
			<p>In real-world scenarios, you can see minimum points being highly impactful when you have truly large amounts of data. Going back to the wine-clustering example, if your store was actually a large wine warehouse, you could have thousands of individual wines with only one or two bottles that could easily be viewed as their own cluster. This may be helpful depending on your use case; however, it is important to keep in mind the subjective magnitudes that come with your data. If you have millions of data points, then random noise can easily be seen as hundreds or even thousands of random one-off sales. However, if your data is on the scale of hundreds or thousands, single data points can be seen as random noise.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Exercise 3.02: Evaluating the Impact of the Minimum Points Threshold</h2>
			<p>Similar to <em class="italic">Exercise 3.01</em>, <em class="italic">Evaluating the Impact of Neighborhood Radius Size</em>, where we explored the value of setting a proper neighborhood radius size, we will repeat the exercise, but instead will change the minimum points threshold on a variety of datasets.</p>
			<p>Using our current implementation of DBSCAN, we can easily tune the minimum points threshold. Tune this hyperparameter and see how it performs on generated data.</p>
			<p>By tuning the minimum points threshold for DBSCAN, you will understand how it can affect the quality of your clustering predictions.</p>
			<p>Once again, let's start with randomly generated data: </p>
			<ol>
				<li value="1">Generate a random cluster dataset, as follows:<p class="source-code">from sklearn.cluster import DBSCAN</p><p class="source-code">from sklearn.datasets import make_blobs</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">X, y = make_blobs(n_samples=1000, centers=8,\</p><p class="source-code">                  n_features=2, random_state=800)</p></li>
				<li>Visualize the data as follows:<p class="source-code"># Visualize the data</p><p class="source-code">plt.scatter(X[:,0], X[:,1])</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer076" class="IMG---Figure"><img src="image/B15923_03_12.jpg" alt="Figure 3.12: Plot of the data generated&#13;&#10;"/></div><p class="figure-caption">Figure 3.12: Plot of the data generated</p></li>
				<li>With the same plotted data as before, let's grab one of the better-performing neighborhood radius sizes from <em class="italic">Exercise 3.01</em>, <em class="italic">Evaluating the Impact of Neighborhood Radius Size</em> – <strong class="source-inline">eps = 0.7</strong>:<p class="source-code">db = DBSCAN(eps=0.7, min_samples=10, metric='euclidean')</p><p class="callout-heading">Note</p><p class="callout"><strong class="source-inline">eps</strong> is a tunable hyperparameter. Earlier in <em class="italic">Step 3</em> of the previous exercise, we used a value of <strong class="source-inline">0.5</strong>. In this step, we are using <strong class="source-inline">eps = 0.7</strong> based on our experimentation with this parameter.</p></li>
				<li>After instantiating the DBSCAN clustering algorithm, let's treat the <strong class="source-inline">min_samples</strong> hyperparameters as the variable we wish to tune. We can cycle through a loop to find which minimum number of points works best for our use case:<p class="source-code">num_samples = [10,19,20]</p><p class="source-code">for min_num in num_samples:</p><p class="source-code">    db = DBSCAN(eps=0.7, min_samples=min_num, metric='euclidean')</p><p class="source-code">    plt.scatter(X[:,0], X[:,1], c=db.fit_predict(X))</p><p class="source-code">    plt.title('Toy Problem with Minimum Points: ' + str(min_num))</p><p class="source-code">    plt.show()</p><p>Looking at the first plot generated, we can see where we ended if you followed <em class="italic">Exercise 3.01</em>,<em class="italic"> Evaluating the Impact of Neighborhood Radius Size</em> exactly, using 10 minimum points to mark the threshold for cluster membership:</p><div id="_idContainer077" class="IMG---Figure"><img src="image/B15923_03_13.jpg" alt="Figure 3.13: Plot of the toy problem with a minimum of 10 points"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.13: Plot of the toy problem with a minimum of 10 points</p>
			<p>The remaining two hyperparameter options can be seen to greatly impact the performance of your DBSCAN clustering algorithm, and show how a shift in one number can greatly influence performance:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B15923_03_14.jpg" alt="Figure 3.14: Plots of the toy problem"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14: Plots of the toy problem</p>
			<p>As you can see, simply changing the number of minimum points from 19 to 20 adds an additional (incorrect!) cluster to our feature space. Given what you've learned about minimum points through this exercise, you can now tweak both epsilon and minimum points thresholding in your scikit-learn implementation to achieve the optimal number of clusters.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In our original generation of the data, we created eight clusters. These indicate that small changes in minimum points can add entire new clusters that we know shouldn't be there.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fa4L5F">https://packt.live/3fa4L5F</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/31XUeqi">https://packt.live/31XUeqi</a>.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Activity 3.02: Comparing DBSCAN with k-means and Hierarchical Clustering</h2>
			<p>In the preceding chapter, we attempted to group different wines together using hierarchical clustering. Let's attempt this approach again with DBSCAN and see whether a neighborhood search fares any better. As a reminder, you are managing store inventory and have received a large shipment of wine, but the brand labels fell off the bottles during transit. Fortunately, your supplier provided you with the chemical readings for each bottle along with their respective serial numbers. Unfortunately, you aren't able to open each bottle of wine and taste test the difference – you must find a way to group the unlabeled bottles back together according to their chemical readings! You know from the order list that you ordered three different types of wine and are given only two wine attributes to group the wine types back together.</p>
			<p>In the previous sections, we were able to see how k-means and hierarchical clustering performed on the wine dataset. In our best-case scenario, we were able to achieve a silhouette score of 0.59. Using scikit-learn's implementation of DBSCAN, let's see whether we can get even better clustering.</p>
			<p>These steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Import the necessary packages.</li>
				<li>Load the wine dataset and check what the data looks like.</li>
				<li>Visualize the data.</li>
				<li>Generate clusters using k-means, agglomerative clustering, and DSBSCAN.</li>
				<li>Evaluate a few different options for DSBSCAN hyperparameters and their effect on the silhouette score.</li>
				<li>Generate the final clusters based on the highest silhouette score.</li>
				<li>Visualize clusters generated using each of the three methods.<p class="callout-heading">Note</p><p class="callout">We have sourced this dataset from <a href="https://archive.ics.uci.edu/ml/datasets/wine">https://archive.ics.uci.edu/ml/datasets/wine</a>. [Citation: Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science]. You can also access it at <a href="https://packt.live/3bW8NME">https://packt.live/3bW8NME</a>.</p></li>
			</ol>
			<p>By completing this activity, you will be recreating a full workflow of a clustering problem. You have already made yourself familiar with the data in <em class="italic">Chapter 2</em>, <em class="italic">Hierarchical Clustering</em>, and, by the end of this activity, you will have performed model selection to find the best model and hyperparameters for your dataset. You will have silhouette scores of the wine dataset for each type of clustering.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 431.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor071"/>DBSCAN versus k-means and Hierarchical Clustering</h1>
			<p>Now that you've reached an understanding of how DBSCAN is implemented and how many different hyperparameters you can tweak to drive performance, let's survey how it compares to the clustering methods we have covered previously – k-means clustering and hierarchical clustering.</p>
			<p>You may have noticed in <em class="italic">Activity 3.02</em>, <em class="italic">Comparing DBSCAN with k-means and Hierarchical Clustering,</em> that DBSCAN can be a bit finicky when it comes to finding the optimal clusters via a silhouette score. This is a downside of the neighborhood approach – k-means and hierarchical clustering really excel when you have some idea regarding the number of clusters in your data. In most cases, this number is low enough that you can iteratively try a few different numbers and see how it performs. DBSCAN, instead, takes a more bottom-up approach by working with your hyperparameters and finding the clusters it views as important. In practice, it is helpful to consider DBSCAN when the first two options fail, simply because of the amount of tweaking needed to get it to work properly. That said, when your DBSCAN implementation is working correctly, it will often immensely outperform k-means and hierarchical clustering (in practice, this often happens with highly intertwined, yet still discrete, data, such as a feature space containing two half-moons). </p>
			<p>Compared to k-means and hierarchical clustering, DBSCAN can be seen as being potentially more efficient, since it only has to look at each data point once. Instead of multiple iterations of finding new centroids and evaluating where their nearest neighbors are, once a point has been assigned to a cluster in DBSCAN, it does not change cluster membership. The other key feature that DBSCAN and hierarchical clustering both share, in comparison with k-means, is not needing to explicitly pass a number of clusters expected at the time of creation. This can be extremely helpful when you have no external guidance on how to break your dataset down.</p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor072"/>Summary</h1>
			<p>In this chapter, we discussed hierarchical clustering and DBSCAN, and in what type of situations they are best employed. While hierarchical clustering can, in some respects, be seen as an extension of the nearest-neighbor approach seen in k-means, DBSCAN approaches the problem of finding neighbors by applying a notion of density. This can prove extremely beneficial when it comes to highly complex data that is intertwined in a complex fashion. While DBSCAN is very powerful, it is not infallible and can even be overkill, depending on what your original data looks like. </p>
			<p>Combined with k-means and hierarchical clustering, however, DBSCAN completes a strong toolbox when it comes to the unsupervised learning task of clustering your data. When faced with any problem in this space, it is worthwhile comparing the performance of each method and seeing which performs best.</p>
			<p>With clusterin<a id="_idTextAnchor073"/>g explored, we will now move onto another key piece of rounding out your skills in unsupervised learning: dimensionality reduction. Through the smart reduction of dimensions, we can make clustering easier to understand and communicate to stakeholders. Dimensionality reduction is also key to creating all types of machine learning models in the most efficient manner possible. In the next chapter, we will dive deeper into topic models and see how the aspects of clustering learned in these chapters apply to NLP-type problems. </p>
		</div>
		<div>
			<div id="_idContainer080" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer081" class="Content">
			</div>
		</div>
	</body></html>
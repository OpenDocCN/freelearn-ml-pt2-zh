<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Bitcoin Prices</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Bitcoin and other cryptocurrencies have attracted the attention of many parties over the years, mainly due to their explosion in price levels, as well as the business opportunities that blockchain technologies offer. In this chapter, we will attempt to predict the next day's Bitcoin (BTC) price using historical data. There are many sources that offer cryptocurrency's historical price data. We will use Yahoo finance data, available at <a href="https://finance.yahoo.com/quote/BTC-USD/history/">https://finance.yahoo.com/quote/BTC-USD/history/</a>. In this chapter, we will focus on predicting future prices and leveraging that knowledge to invest in bitcoin.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Time series data</li>
<li>Voting</li>
<li>Stacking</li>
<li>Bagging</li>
<li>Boosting</li>
<li>Random forests</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter10">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter10</a></p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/2JOsR7d">http://bit.ly/2JOsR7d</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Time series data</h1>
                </header>
            
            <article>
                
<p>Time series data is concerned with data instances in which each instance relates to a specific point in <span>time </span>or interval. How often we measure the variable of choice defines the time series' sampling frequency. For example, atmospheric temperature differs throughout the day and throughout the year. We can choose to measure the temperature every hour, so we have an hourly frequency, or we can choose to measure it each day, so we have a daily frequency. In finance, it is not unusual to have frequencies that are between major time intervals; this could be every 10 minutes (10m frequency) or every 4 hours (4h frequency). Another interesting characteristic of time series is that there is usually a correlation between instances that refer to proximal time points.</p>
<p>This is called <strong>autocorrelation</strong>. For example, the atmospheric temperature cannot vary by a great magnitude between consecutive minutes. Furthermore, this enables us to utilize earlier data points to predict future data points. An example of temperatures (an average of 3 hours) for Athens and Greece for the years 2016–2019 is provided in f<span>igure</span><span>. Notice how most temperatures are relatively close to the previous day's temperature, even though there are variations. Furthermore, we see a repeating pattern of hot and cold months (seasons), which is called seasonality:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-663 image-border" src="assets/b95a1076-0053-49d1-93bb-3973b7a8adfc.png" style="width:35.92em;height:26.83em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Temperatures for Athens, Greece 2016–2019</div>
<p>To examine the level of correlation between different points in time, we utilize the <strong>autocorrelation function</strong> (<strong>ACF</strong>). ACF measures the linear correlation between a data point and previous points (called <strong>lags</strong>). In the following figure, the ACF for the temperature data (resampled as the month's average) is provided. It indicates a strong positive correlation with the first lag. This means that a month's temperature cannot deviate much from the previous month, which is logical. For example, December and January are cold months, and usually, their average temperatures are closer than December and March, for example. Furthermore, there is a strong negative correlation between lags 5 and 6, indicating that a cold winter results in a hot summer and vice versa:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-664 image-border" src="assets/4c446b11-ddeb-42b5-8471-c3ce092915e5.png" style="width:38.50em;height:28.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">ACF for the temperature data</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bitcoin data analysis</h1>
                </header>
            
            <article>
                
<p><span>Bitcoin data is very different from temperature data. Temperatures have more or less the same value for the same month of each year. This indicates that the distribution of temperatures does not change over time. Time series that exhibit this behavior are called <strong>stationary</strong>. This allows for relatively easy modeling with time series analysis tools, such as <strong>auto regressive</strong> (<strong>AR</strong>), <strong>moving average</strong> (<strong>MA</strong>), and <strong>auto regressive integrated moving average</strong> (<strong>ARIMA</strong>) models.</span> <span>Financial data is usually non-stationary, as seen in the daily Bitcoin close</span><span> data, depicted in figure. This means that the data does not exhibit the same behavior throughout its entire history, but instead its behavior varies.</span></p>
<div class="packt_infobox"><span>Financial data usually provides open (the first price for the day), high (the highest price for the day), low (the lowest price for the day), and close (the last price for the day) values.</span></div>
<p>There are clear trends in the data (time intervals where the price, on average, increases or decreases), as well as heteroskedasticity (variable variance over time). One way to identify stationarity is to study the ACF function.If there is a very strong correlation between lags of a very high order that do not decay, the time series is most probably non-stationary. The ACF for the BTC data is also provided, showing weakly decaying correlations:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-665 image-border" src="assets/129b0c46-e882-4541-a670-1d541ddf4316.png" style="width:34.50em;height:25.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">BTC/USD prices for mid-2014 to present</div>
<p>The following figure depicts the <span>ACF for BTC. We can clearly see that the correlations do not drop for very high lag values:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-666 image-border" src="assets/6922b6d5-ebf1-4adb-baaf-c350a321c8ea.png" style="width:37.08em;height:27.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">ACF for BTC data</div>
<p>Take a look at the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/11504a16-70f1-401c-a372-b973ab8ac221.png" style="width:8.33em;height:3.25em;"/></p>
<p>Where <em>p</em> is the percentage change, <em>t<sub>n </sub></em>is the price at time<span> </span><em>n</em>,<span> </span>and <em>tn-1</em> is the price at time<span> </span><em>n-1</em>. By applying the transformation to the data, we get a time series that is stationary, but less correlated.</p>
<p>The following figure shows the plots for the data, and the ACF and the average 30-day standard deviation are provided:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-667 image-border" src="assets/3d5ca1aa-f104-46e5-b543-34423bdd6d24.png" style="width:31.33em;height:23.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Transformed data</div>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-668 image-border" src="assets/95c24465-fecb-4957-a51e-cdffdd6030cd.png" style="width:31.42em;height:23.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Rolling 30-day standard deviation and ACF for transformed data</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Establishing a baseline</h1>
                </header>
            
            <article>
                
<p>In order to establish a baseline, we will try to model the data using linear regression. Although it is a time series, we will not directly take time into account. Instead, we will utilize sliding windows of size <em>S</em> to generate features at each time point and use those features to predict the next point. Next, we will move the window one step forward in time to include the true value of the data point we predicted and discard the oldest data point inside the window. We will continue this process until all data points have been predicted. This is called walk-forward validation. One drawback is that we cannot predict the first <em>S</em> data points, as we do not have enough data to generate features for them. Another point of concern is that we need to re-train the model <em>L</em>-<em>S</em> times, in which <em>L</em> is the total number of points in the time series. A graphical representation of the first two steps is provided in the following figure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-669 image-border" src="assets/fc41a330-90f5-468a-8c7e-79c5d9d43514.png" style="width:25.50em;height:36.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Walk-forward validation procedure, first two steps. The procedure continues for the whole time series.</div>
<p>First, we load the required libraries and data from the <kbd>BTC-USD.csv</kbd> file. We also set the seed for a NumPy random number generator:</p>
<pre>import numpy as np<br/>import pandas as pd<br/>from simulator import simulate<br/>from sklearn import metrics<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.model_selection import train_test_split<br/>np.random.seed(123456)<br/>lr = LinearRegression()<br/>data = pd.read_csv('BTC-USD.csv')</pre>
<p>We then clean the data by removing entries that contain NaN values, using <kbd>data.dropna()</kbd>, parse the dates using <kbd>pd.to_datetime</kbd>, and set the dates as an index. Finally, we calculate the percentage differences of <kbd>Close</kbd> values (and discard the first value, as it is a NaN) and save the Pandas series' length:</p>
<pre>data = data.dropna()<br/>data.Date = pd.to_datetime(data.Date)<br/>data.set_index('Date', drop=True, inplace=True)<br/>diffs = (data.Close.diff()/data.Close).values[1:]<br/><br/>diff_len = len(diffs)</pre>
<p>We have created a function that generates the features at each data point. Features are essentially the different percentages at previous lags. Thus, to fill a dataset's feature with values, we only have to shift the data forward by as many points as the lags indicate. Any features that do not have available data to calculate lags, will have a value of zero. The following figure<em> </em>shows a toy example of a time series containing the numbers 1, 2, 3, and 4:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-670 image-border" src="assets/425b19c2-be6f-4814-814d-a7089a444894.png" style="width:24.83em;height:16.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">How lag features are filled</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The actual function, to fill lag <em>t</em>, selects all of the data from the time series except for the last <em>t</em> and places it in the corresponding feature, starting from index <em>t</em>. We chose to use the past 20 days as there does not seem to be any significant linear correlations after that point. Furthermore, we scale the features and targets by a factor of 100 and round them to 8 decimal points. This is important, as it allows the reproducibility of results. If the data is not rounded, overflow errors introduce stochasticity to the results, as shown in the following:</p>
<pre>def create_x_data(lags=1):<br/> diff_data = np.zeros((diff_len, lags))<br/><br/>for lag in range(1, lags+1):<br/> this_data = diffs[:-lag]<br/> diff_data[lag:, lag-1] = this_data<br/><br/>return diff_data<br/><br/># REPRODUCIBILITY<br/>x_data = create_x_data(lags=20)*100<br/>y_data = diffs*100</pre>
<p>Finally, we execute the walk-forward validation. We chose a training window of 150 points, which equates to roughly 5 months. Given the data's nature and volatility, it provides a good trade-off between having a large enough train set and capturing recent market behaviors. A larger window would include market conditions that no longer reflect reality. A shorter window would provide too little data and would be prone to overfitting. We measure our model's predictive quality by utilizing the mean squared error between our predictions and the original percentage differences:</p>
<pre class="mce-root">window = 150<br/>preds = np.zeros(diff_len-window)<br/>for i in range(diff_len-window-1):<br/> x_train = x_data[i:i+window, :]<br/> y_train = y_data[i:i+window]<br/> lr.fit(x_train, y_train)<br/> preds[i] = lr.predict(x_data[i+window+1, :].reshape(1, -1))<br/><br/>print('Percentages MSE: %.2f'%metrics.mean_absolute_error(y_data[window:], preds))</pre>
<p class="mce-root"/>
<p>Simple linear regression might produce an MSE of 18.41. We could also attempt to reconstruct the time series by multiplying each data point by (1 + prediction) to get the next predicted point. Furthermore, we could attempt to take advantage of the dataset's nature and simulate trading activity. Each time the prediction is greater than +0.5% change, we invest 100 USD in buying Bitcoins. If we have Bitcoins in our possession and the prediction is lower than -0.5%, we sell the Bitcoins at the current market close. To assess the quality of our model as a trading strategy, we utilize a simplified <strong>Sharpe</strong> ratio, which is calculated as the ratio of mean returns (percentage profits) over the standard deviation of the returns. Higher Sharpe values indicate a better trading strategy. The formula utilized here is calculated as follows. Usually, an alternative <strong>safe</strong> return percentage is subtracted from the expected return, but as we only want to compare the models we will generate with each other, we'll omit it:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/292cc638-9c04-4f53-85ce-75ca3ccaf8e2.png" style="width:9.17em;height:2.42em;"/></p>
<p>When utilized as a trading strategy, linear regression is able to produce a Sharpe value of 0.19. The following figure indicates the trades and profits generated by our model. The blue triangles indicate time points at which the strategy bought Bitcoins worth 100 USD and the red triangles indicate the time points at which it sold the previously bought Bitcoins:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-671 image-border" src="assets/2be506f8-c88d-449b-b6ce-7802d6e07a5f.png" style="width:33.83em;height:25.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Profits and entry/exit points of our model</div>
<p>In the rest of this chapter, we will try to improve the MSE and Sharpe values by utilizing the ensemble methods we presented in the previous chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The simulator</h1>
                </header>
            
            <article>
                
<p>Here, we'll provide a brief explanation of how the simulator works. It is implemented as a function that accepts our standard Pandas DataFrame data and the model's predictions as inputs. First, we'll define the buying threshold and the stake size (how much money we invest in each buy), as well as placeholder variables. The variables will be used to store the true and predicted time series, as well as the profits of our model (<kbd>balances</kbd>). Furthermore, we define the <kbd>buy_price</kbd> <span>variable, </span><span>which stores the price at which we bought the Bitcoins. If the price is <kbd>0</kbd>, we assume that we do not hold any Bitcoins. The </span><kbd>buy_points</kbd> <span>and</span> <kbd>sell_points</kbd><span> </span><span>lists </span><span>indicate the points in time when we bought or sold the Bitcoins and are used only for plotting. Furthermore, we store the starting index, which is equivalent to the sliding window's size as shown in the following example:</span></p>
<pre>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd<br/><br/>from sklearn import metrics<br/><br/>def simulate(data, preds):<br/> # Constants and placeholders<br/> buy_threshold = 0.5<br/> stake = 100<br/><br/>true, pred, balances = [], [], []<br/><br/>buy_price = 0<br/> buy_points, sell_points = [], []<br/> balance = 0<br/><br/>start_index = len(data)-len(preds)-1</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Next, for each point, we store the actual and predicted values. If the predicted value is greater than 0.5 and we do not hold any Bitcoins, we buy 100 USD worth of Bitcoins. If the predicted value is less than -0.5 and we have already bought Bitcoins, we sell them at the current close value. We add the current profit (or loss) to our balances, cast the true and predicted values as NumPy arrays, and produce the plots:</p>
<pre># Calculate predicted values<br/> for i in range(len(preds)):<br/><br/>last_close = data.Close[i+start_index-1]<br/> current_close = data.Close[i+start_index]<br/><br/># Save predicted values and true values<br/> true.append(current_close)<br/> pred.append(last_close*(1+preds[i]/100))<br/><br/> # Buy/Sell according to signal<br/> if preds[i] &gt; buy_threshold and buy_price == 0:<br/> buy_price = true[-1]<br/> buy_points.append(i)<br/><br/>elif preds[i] &lt; -buy_threshold and not buy_price == 0:<br/> profit = (current_close - buy_price) * stake/buy_price<br/> balance += profit<br/> buy_price = 0<br/> sell_points.append(i)<br/><br/>balances.append(balance)<br/> true = np.array(true)<br/> pred = np.array(pred)<br/><br/># Create plots<br/> plt.figure()<br/><br/>plt.subplot(2, 1, 1)<br/> plt.plot(true, label='True')<br/> plt.plot(pred, label='pred')<br/> plt.scatter(buy_points, true[buy_points]+500, marker='v',<br/> c='blue', s=5, zorder=10)<br/> plt.scatter(sell_points, true[sell_points]-500, marker='^'<br/> , c='red', s=5, zorder=10)<br/> plt.title('Trades')<br/><br/>plt.subplot(2, 1, 2)<br/> plt.plot(balances)<br/> plt.title('Profit')<br/> print('MSE: %.2f'%metrics.mean_squared_error(true, pred))<br/> balance_df = pd.DataFrame(balances)<br/><br/>pct_returns = balance_df.diff()/stake<br/> pct_returns = pct_returns[pct_returns != 0].dropna()<br/><br/> print('Sharpe: %.2f'%(np.mean(pct_returns)/np.std(pct_returns)))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Voting</h1>
                </header>
            
            <article>
                
<p>We will try to combine three basic regression algorithms by voting to improve the MSE of the simple regression. To combine the algorithms, we will utilize the average of their predictions. Thus, we code a simple class that creates a dictionary of base learners and handles their training and prediction averaging. The main logic is the same as with the custom voting classifier we implemented in <span class="cdp-organizer-chapter-number"><a href="ad9aa66b-7b30-4779-8914-0ff58140b3e8.xhtml">Chapter 3</a></span><span class="cdp-organizer-chapter-number">,</span><span> <em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Voting</span></span></em>:</span></p>
<pre>import numpy as np<br/>from copy import deepcopy<br/><br/>class VotingRegressor():<br/><br/># Accepts a list of (name, classifier) tuples<br/> def __init__(self, base_learners):<br/> self.base_learners = {}<br/> for name, learner in base_learners:<br/> self.base_learners[name] = deepcopy(learner)<br/><br/> # Fits each individual base learner<br/> def fit(self, x_data, y_data):<br/> for name in self.base_learners:<br/> learner = self.base_learners[name]<br/> learner.fit(x_data, y_data)</pre>
<p>The predictions are stored in a NumPy matrix, in which each row corresponds to a single instance and each column corresponds to a single base learner. The row-averaged values are the ensemble's output, as shown here:</p>
<pre class="mce-root"># Generates the predictions<br/> def predict(self, x_data):<br/><br/># Create the predictions matrix<br/> predictions = np.zeros((len(x_data), len(self.base_learners)))<br/><br/>names = list(self.base_learners.keys())<br/><br/># For each base learner<br/> for i in range(len(self.base_learners)):<br/> name = names[i]<br/> learner = self.base_learners[name]<br/><br/># Store the predictions in a column<br/> preds = learner.predict(x_data)<br/> predictions[:,i] = preds<br/><br/># Take the row-average<br/> predictions = np.mean(predictions, axis=1)<br/> return predictions</pre>
<p>We chose to utilize a support vector machine, a K-Nearest Neighbors Regressor, and a linear regression as a base learners, as they provide diverse learning paradigms. To utilize the ensemble, we first import the required modules:</p>
<pre>import numpy as np<br/>import pandas as pd<br/><br/>from simulator import simulate<br/>from sklearn import metrics<br/>from sklearn.neighbors import KNeighborsRegressor<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.svm import SVR<br/>from voting_regressor import VotingRegressor</pre>
<p>Next, in the code we presented earlier, we replace the <kbd>lr = LinearRegression()</kbd> <span>line </span><span>with the following:</span></p>
<pre>base_learners = [('SVR', SVR()),<br/> ('LR', LinearRegression()),<br/> ('KNN', KNeighborsRegressor())]<br/><br/>lr = VotingRegressor(base_learners)</pre>
<p>By adding the two additional regressors, we are able to reduce the MSE to 16.22 and produce a Sharpe value of 0.22.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving voting</h1>
                </header>
            
            <article>
                
<p>Although our results are better than linear regression, we can further improve them by removing the linear regression, thus, leaving the base learners as follows:</p>
<pre>base_learners = [('SVR', SVR()), ('KNN', KNeighborsRegressor())]</pre>
<p>This further improves the MSE, reducing it to 15.71. If we utilize this model as a trading strategy, we can achieve a Sharpe value of 0.21; considerably better than simple linear regression. The following table summarizes our results:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Metric</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>SVR-KNN</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>SVR-LR-KNN</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>MSE</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>15.71</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>16.22</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sharpe</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.21</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.22</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Voting ensemble results</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacking</h1>
                </header>
            
            <article>
                
<p>Moving on to more complex ensembles, we will utilize stacking to <span>combine basic regressors </span><span>more efficiently. Using</span> <kbd>StackingRegressor</kbd> <span>from <span class="cdp-organizer-chapter-number"><a href="49a05219-d6cb-4893-aaac-49280842b647.xhtml">Chapter 4</a>, </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Stacking</span></span></em>, we will try to combine the same algorithms as we did with voting. First, we modify the <kbd>predict</kbd> function of our ensemble (to allow for single-instance prediction) as follows:</span></p>
<pre> # Generates the predictions<br/> def predict(self, x_data):<br/><br/># Create the predictions matrix<br/> predictions = np.zeros((len(x_data), len(self.base_learners)))<br/><br/>names = list(self.base_learners.keys())<br/><br/># For each base learner<br/> for i in range(len(self.base_learners)):<br/> name = names[i]<br/> learner = self.base_learners[name]<br/><br/># Store the predictions in a column<br/> preds = learner.predict(x_data)<br/> predictions[:,i] = preds<br/><br/># Take the row-average<br/> predictions = np.mean(predictions, axis=1)<br/> return predictions</pre>
<p>Again, we modify the code to use the stacking regressor, as follows:</p>
<pre>base_learners = [[SVR(), LinearRegression(), KNeighborsRegressor()],<br/>                [LinearRegression()]]<br/>lr = StackingRegressor(base_learners)</pre>
<p>In this setup, the ensemble yields a model with an MSE of 16.17 and a Sharpe value of 0.21.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving stacking</h1>
                </header>
            
            <article>
                
<p>Our results are slightly worse than the final Voting ensemble, so we will attempt to improve them by removing the linear regression, as we did with the voting ensemble. By doing so, we can slightly improve our model, achieving an MSE of 16.16 and a Sharpe value of 0.22. Comparing it to voting, stacking is slightly better as part of an investing strategy (the same Sharpe value and a slightly better MSE), although it is unable to achieve the same level of predictive accuracy. Its results are summarized in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Metric</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>SVR-KNN</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>SVR-LR-KNN</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>MSE</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>16.17</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>16.16</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sharpe</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.21</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.22</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Stacking results</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bagging</h1>
                </header>
            
            <article>
                
<p>Usually, when fitting predictive models onto financial data, variance is our main problem. Bagging is a very useful tool to counter variance; thus, we hope that it will be able to produce better performing models compared to simple voting and stacking. To utilize bagging, we will use scikit's <kbd>BaggingRegressor</kbd>, presented in <span class="cdp-organizer-chapter-number"><a href="a0e9eea5-bc95-4d15-9679-fafce5718525.xhtml">Chapter 5</a>, </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Bagging</span></span></em>. To implement it in our experiment, we simply call it using <kbd>lr = BaggingRegressor()</kbd> instead of the previous regressors. This results in an MSE of 19.45 and a Sharpe of 0.09. The following figure depicts the profits and trades that our model generates:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-672 image-border" src="assets/c0e89628-fedc-4663-935e-94dc46540cde.png" style="width:34.00em;height:25.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Bagging profits and trades</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving bagging</h1>
                </header>
            
            <article>
                
<p>We can further improve bagging as its performance is worse than any previous model. First, we can experiment with shallow trees, which will further reduce variance in the ensemble. By utilizing trees with a maximum depth of <kbd>3</kbd>, using <kbd>lr = BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=3))</kbd>, we can improve the model's performance, generating an MSE of 17.59 and a Sharpe value of 0.15. Further restricting the trees' growth to <kbd>max_depth=1</kbd>, allows the model to achieve an MSE of 16.7 and a Sharpe value of 0.27. If we examine the model's trading plots, we observe a reduction in the number of trades, as well as a considerable improvement in performance during periods in which Bitcoin's price significantly drops. This indicates that the model can <span>filter noise from actual signals </span><span>more efficiently.</span></p>
<p><span>The reduction in variance has indeed helped our model to improve its performance: </span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-673 image-border" src="assets/acf5e8a5-4203-4c40-a5d2-ce2033618a61.png" style="width:33.00em;height:24.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Final Bagging profits and trades</div>
<p><span>The following table summarizes the results for the various bagging models we tested:</span></p>
<div>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Metric</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>DT_max_depth=1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>DT_max_depth=3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>DT</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>MSE</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>16.70</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>17.59</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>19.45</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sharpe</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.27</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.15</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.09</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="packt_figref CDPAlignCenter CDPAlign">Table 3: Bagging results</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boosting</h1>
                </header>
            
            <article>
                
<p>One of the most powerful ensemble learning techniques is boosting. It allows complicated models to be generated. In this section, we will utilize XGBoost to model our time series data. As there are many degrees of freedom (hyperparameters) when modeling with XGBoost, we expect some level of fine-tuning to be needed to achieve satisfactory results. By replacing our example's regressor with <kbd>lr = XGBRegressor()</kbd>, we can utilize XGBoost and fit it onto our data. This results in an MSE of 19.20 and a Sharpe value of 0.13.</p>
<p>Figure depicts the profits and trades generated by the model. Although the Sharpe value is lower than for other models, we can see that it continues to generate profit, even during periods in which the Bitcoin price drops:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-674 image-border" src="assets/27b520e4-28f1-4251-b82c-50d9b794fc52.png" style="width:36.83em;height:28.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Trades generated by the Boosting model</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving boosting</h1>
                </header>
            
            <article>
                
<p>Due to the out-of-sample performance and the frequency at which boosting is bought and sold, we can assume it is overfitting the training data. Therefore, we'll <span>will try to regularize its learning. The first step is to limit the maximum depth of individual trees. We start by imposing an upper limit of 2, using</span> <kbd>max_depth=2</kbd><span>. This slightly improves our model, yielding an MSE of 19.14 and a Sharpe value of 0.17. Further limiting the overfitting capabilities of the model by using only 10 base learners (</span><kbd>n_estimators=10</kbd><span>), the model achieves additional improvement.</span></p>
<p class="mce-root"/>
<p><span>The MSE of the model is reduced to 16.39 and the Sharpe value is increased to 0.21. Adding an L1 regularization term of 0.5 (</span><kbd>reg_alpha=0.5</kbd><span>) only reduces the MSE to 16.37. We have come to a point where further fine-tuning will not contribute much performance to our model. At this point, our regressor looks like this:</span></p>
<pre class="CodePACKT">lr = XGBRegressor(max_depth=2, n_estimators=10, reg_alpha=0.5)</pre>
<p class="NormalPACKT">Given the capabilities of XGBoost, we will try to increase the amount of information available to the model. We will increase the available feature lags to 30 and add a rolling mean of the previous 15 lags to the features. To do this, we modify the feature creation section of the code as follows:</p>
<pre>def create_x_data(lags=1):<br/> diff_data = np.zeros((diff_len, lags))<br/> ma_data = np.zeros((diff_len, lags))<br/><br/> diff_ma = (data.Close.diff()/data.Close).rolling(15).mean().fillna(0).values[1:]<br/> for lag in range(1, lags+1):<br/> this_data = diffs[:-lag]<br/> diff_data[lag:, lag-1] = this_data<br/><br/> this_data = diff_ma[:-lag]<br/> ma_data[lag:, lag-1] = this_data<br/> return np.concatenate((diff_data, ma_data), axis=1)<br/><br/>x_data = create_x_data(lags=30)*100<br/>y_data = diffs*100</pre>
<p>This increases the trading performance of our model, achieving a Sharpe value of 0.32—the highest of all of the models, while it also increases its MSE to 16.78. The trades generated by this model are depicted in figure and in the table that follows. It is interesting to note that the number of buys has greatly reduced, a behavior that bagging also exhibited when we managed to improve its performance as an investment strategy:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-675 image-border" src="assets/0e1ed186-97e3-4188-bddb-daf4bd6207c1.png" style="width:33.25em;height:24.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Final boosting model performance</div>
<div class="packt_figref"/>
<div>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Metric</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>md=2/ne=10/reg=0.5+data</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>md=2/ne=10/reg=0.5</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>md=2/ne=10</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>md=2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>xgb</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>MSE</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>16.78</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>16.37</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>16.39</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>19.14</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>19.20</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sharpe</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.32</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.21</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.21</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.17</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.13</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Metrics for all boosting models</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forests</h1>
                </header>
            
            <article>
                
<p>Finally, we will utilize random forests to model our data. Although we expect that the ensemble to be able to utilize the information from additional lags and the rolling average, we will start with only 20 lags and the return percentages as inputs. Thus, our initial regressor is simply <kbd>RandomForestRegressor()</kbd>. This results in a model that does not perform very well. Its MSE is 19.02 and its Sharpe value is 0.11.</p>
<p>The following figure depicts the trades that the model generates:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-676 image-border" src="assets/7b9348f6-e878-4eb9-befc-ce907de295e9.png" style="width:35.42em;height:26.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Trades of random forest model</div>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving random forest</h1>
                </header>
            
            <article>
                
<p>In an attempt to improve our model, we try to restrict its overfitting capabilities, imposing a maximum depth of <kbd>3</kbd> for each tree. This results in considerable performance improvement as the model achieves an MSE of 17.42 and a Sharpe value of 0.17. Further restricting the maximum depth to <kbd>2</kbd> improves the MSE score slightly more to 17.13, but reduces its Sharpe value to 0.16. Finally, increasing the ensemble's size to 50, using <kbd>n_estimators=50</kbd>, produces a considerably better model, with an MSE of 16.88 and a Sharpe value of 0.23. As we have only used the original feature set (20 lags of return percentages), we wish to also experiment with the expanded dataset we utilized in the boosting section. By adding the 15-day rolling average, as well as increasing the number of available lags to 30, the model can increase its Sharpe value to 0.24, although its MSE also increases to 18.31. The trades generated by the model are depicted in figure: </p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-677 image-border" src="assets/8b6ca443-a5e6-4393-abba-2496fe75180d.png" style="width:31.92em;height:23.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Random forest's results with the expanded dataset</div>
<p><span>The model's results are summarized in the following table:</span></p>
<div class="packt_figref"/>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Metric</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>md=2/ne=50+data</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>md=2/ne=50</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>md=2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>md=3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>RF</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>MSE</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>18.31</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>16.88</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>17.13</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>17.42</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>19.02</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sharpe</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.24</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.23</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.16</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.17</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.11</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Metrics for all random forest models</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we tried to model <span>historical</span><span> Bitcoin prices using all of the ensemble methods presented in earlier chapters of this book. As with most datasets, there are many decisions that affect a model's quality. Data preprocessing and feature engineering are some of the most important factors, especially when the dataset's nature does not allow direct modeling of the data. Time series datasets fall into this category, in which the construction of appropriate features and targets is required. By transforming our non-stationary time series to stationary, we improved the algorithm's ability to model the data.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>To assess the quality of our models, we used the MSE of return percentages, as well as the Sharpe ratio (in which we assumed that the model was utilized as a trading strategy). When MSE is concerned, the best performing ensemble proved to be the simple voting ensemble. The ensemble consisted of an SVM and KNN regressor, without any hyperparameter fine-tuning, achieving an MSE of 15.71. As a trading strategy, XGBoost proved to be the best ensemble, achieving a Sharpe value of 0.32. Although not exhaustive, this chapter has explored the possibilities and techniques used in time series modeling using ensemble</span> learning methods.</p>
<p>In the next chapter, we will leverage the capabilities of ensemble learning methods, in order to predict the sentiment of various tweets.</p>


            </article>

            
        </section>
    </body></html>
<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Categories with K-Nearest Neighbors</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <strong>k-Nearest Neighbors</strong> (<strong>k-NN</strong>) algorithm is a form of supervised machine learning that is used to predict categories. In this chapter, you will learn about the following:</p>
<ul>
<li>Preparing a dataset for machine learning with scikit-learn</li>
<li>How the k-NN algorithm works <em>under the hood</em></li>
<li>Implementing your first <span>k-NN</span> algorithm to predict a fraudulent transaction</li>
<li>Fine-tuning the parameters of the k-NN algorithm</li>
<li>Scaling your data for optimized performance</li>
</ul>
<p>The <span>k-NN</span> algorithm has a wide range of applications in the field of classification and supervised machine learning. Some of the real-world applications for this algorithm include predicting loan defaults and credit-based fraud in the financial industry and predicting whether a patient has cancer in the healthcare industry.</p>
<p class="mce-root">This book's design facilitates the implementation of a robust machine learning pipeline for each and every algorithm mentioned in the book, and a Jupyter Notebook will be required. </p>
<p>The Jupyter Notebook can be installed on your local machine by following the instructions provided at:<a href="https://jupyter.org/install.html"> https://jupyter.org/install.html</a>.</p>
<p>Alternatively, you can also work with the Jupyter Notebook in the browser by using: <a href="https://jupyter.org/try">https://jupyter.org/try</a>.</p>
<div class="packt_tip">Each chapter in this book comes with a pipeline that is implemented in a Jupyter Notebook on the official GitHub repository of this book, and as such, it is highly recommended that you install Jupyter Notebook on your local machine.</div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You will be required to have </span><span><span class="fontstyle2">Python 3.6 or greater, P</span><span class="fontstyle2">andas ≥ 0.23.4, </span><span class="fontstyle2">Scikit-learn ≥ 0.20.0, </span><span class="fontstyle2">NumPy ≥ 1.15.1, and </span><span class="fontstyle2">Matplotlib ≥ 3.0.0</span> </span><span class="fontstyle0">installed on your system.</span></p>
<p class="mce-root">The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_02.ipynb">https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_02.ipynb</a></p>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2Q2DGop">http://bit.ly/2Q2DGop</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing a dataset for machine learning with scikit-learn</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first step to implementing any machine learning algorithm with scikit-learn is data preparation. Scikit-learn comes with a set of constraints to implementation that will be discussed later in this section. The dataset that we will be using is based on mobile payments and is found on the world's most popular competitive machine learning website – Kaggle.</p>
<p>You can download the dataset from: <a href="https://www.kaggle.com/ntnu-testimon/paysim1">https://www.kaggle.com/ntnu-testimon/paysim1</a>.<a href="https://www.kaggle.com/ntnu-testimon/paysim1"/></p>
<p>Once downloaded, open a new Jupyter Notebook by using the following code in Terminal (macOS/Linux) or Anaconda Prompt/PowerShell (Windows):</p>
<pre><strong>Jupyter Notebook</strong></pre>
<p>The fundamental goal of this dataset is to predict whether a mobile transaction is fraudulent. In order to do this, we need to first have a brief understanding of the contents of our data. In order to explore the dataset, we will use the <kbd>pandas</kbd><em> </em>package in Python. You can install pandas by using the following code in Terminal (macOS/Linux) or PowerShell (Windows):</p>
<pre><strong>pip3 install pandas</strong></pre>
<p>Pandas can be installed on Windows machines in an Anaconda Prompt by using the following code:</p>
<pre><strong>conda install pandas</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>We can now read in the dataset into our Jupyter Notebook by using the following code: </p>
<pre>#Package Imports<br/><br/>import pandas as pd<br/><br/>#Reading in the dataset<br/><br/>df = pd.read_csv('PS_20174392719_1491204439457_log.csv')<br/><br/>#Viewing the first 5 rows of the dataset<br/><br/>df.head()</pre>
<p>This produces an output as illustrated in the <span>following </span>screenshot: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/29cad5dd-77a7-4546-bbdd-5cf4e997053b.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dropping features that are redundant</h1>
                </header>
            
            <article>
                
<p>From the dataset seen previously, there are a few columns that are redundant to the machine learning process:</p>
<ul>
<li><kbd>nameOrig</kbd>:<em> </em>This column is a unique identifier that belongs to each customer. Since each identifier is unique with every row of the dataset, the machine learning algorithm will not be able to discern any patterns from this feature. </li>
<li><kbd>nameDest</kbd>:<em> </em>This column is also a unique identifier that belongs to each customer and as such provides no value to the machine learning algorithm. </li>
<li><kbd>isFlaggedFraud</kbd>: This column flags a transaction as fraudulent if a person tries to transfer more than 200,000 in a single transaction. Since we already have a feature called <kbd>isFraud</kbd><em> </em>that flags a transaction as fraud, this feature becomes redundant. </li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can drop these features from the dataset by using the following code: </p>
<pre>#Dropping the redundant features<br/><br/>df = df.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reducing the size of the data</h1>
                </header>
            
            <article>
                
<p class="mce-root">The dataset that we are working with contains over 6 million rows of data. Most machine learning algorithms will take a large amount of time to work with a dataset of this size. In order to make our execution time quicker, we will reduce the size of the dataset to 20,000 rows. We can do this by using the <span>following </span>code:</p>
<pre>#Storing the fraudulent data into a dataframe<br/><br/>df_fraud = df[df['isFraud'] == 1]<br/><br/>#Storing the non-fraudulent data into a dataframe <br/><br/>df_nofraud = df[df['isFraud'] == 0]<br/><br/>#Storing 12,000 rows of non-fraudulent data<br/><br/>df_nofraud = df_nofraud.head(12000)<br/><br/>#Joining both datasets together <br/><br/>df = pd.concat([df_fraud, df_nofraud], axis = 0)</pre>
<p>In the preceding code, the fraudulent rows are stored in one dataframe. This dataframe contains a little over 8,000 rows. The 12,000 non-fraudulent rows are stored in another dataframe, and the two dataframes are joined together using the <kbd>concat</kbd><em> </em>method from pandas.</p>
<p>This results in a dataframe with a little over 20,000 rows, over which we can now execute our algorithms relatively quickly. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding the categorical variables</h1>
                </header>
            
            <article>
                
<p>One of the main constraints of scikit-learn is that you cannot implement the machine learning algorithms on columns that are categorical in nature. For example, the <kbd>type</kbd><em> </em>column in our dataset has five categories:</p>
<p class="mce-root"/>
<ul>
<li><kbd>CASH-IN</kbd></li>
<li><kbd>CASH-OUT</kbd></li>
<li><kbd>DEBIT</kbd></li>
<li><kbd>PAYMENT</kbd></li>
<li><kbd>TRANSFER</kbd></li>
</ul>
<p>These categories will have to be encoded into numbers that scikit-learn can make sense of. In order to do this, we have to implement a two-step process. </p>
<p>The first step is to convert each category into a number: <kbd>CASH-IN = 0</kbd>, <kbd>CASH-OUT = 1</kbd>, <kbd>DEBIT = 2</kbd>, <kbd>PAYMENT = 3</kbd>, <kbd>TRANSFER = 4</kbd>. We can do this by using the following code:</p>
<pre>#Package Imports<br/><br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.preprocessing import OneHotEncoder<br/><br/>#Converting the type column to categorical<br/><br/>df['type'] = df['type'].astype('category')<br/><br/>#Integer Encoding the 'type' column<br/><br/>type_encode = LabelEncoder()<br/><br/>#Integer encoding the 'type' column<br/><br/>df['type'] = type_encode.fit_transform(df.type)</pre>
<p>The code first coverts the <kbd>type</kbd> column to a categorical feature. We then use <kbd>LabelEncoder()</kbd> in order to initialize an integer encoder object that is called <kbd>type_encode</kbd><em>. </em>Finally, we apply the <kbd>fit_transform</kbd><em> </em>method on the <kbd>type</kbd><em> </em>column in order to convert each category into a number.</p>
<p>Broadly speaking, there are two types of categorical variables:</p>
<ul>
<li>Nominal </li>
<li>Ordinal</li>
</ul>
<p>Nominal categorical variables have no inherent order to them. An example of the nominal type of categorical variable is the <kbd>type</kbd><em> </em>column. </p>
<p>Ordinal categorical variables have an inherent order to them. An example of the ordinal type of categorical variable is Education Level, in which people with a Master's degree will have a higher order/value compared to people with a Undergraduate degree only. </p>
<p class="mce-root"><span>In the case of ordinal categorical variables, integer encoding, as illustrated previously, is sufficient and we do not need to one-hot encode them. Since the </span><kbd>type</kbd><em> </em><span>column is a nominal categorical variable, we have to one-hot encode it after integer encoding it. This is done by using the following code: </span></p>
<pre>#One hot encoding the 'type' column<br/><br/>type_one_hot = OneHotEncoder()<br/><br/>type_one_hot_encode = type_one_hot.fit_transform(df.type.values.reshape(-1,1)).toarray()<br/><br/>#Adding the one hot encoded variables to the dataset <br/><br/>ohe_variable = pd.DataFrame(type_one_hot_encode, columns = ["type_"+str(int(i)) for i in range(type_one_hot_encode.shape[1])])<br/><br/>df = pd.concat([df, ohe_variable], axis=1)<br/><br/>#Dropping the original type variable <br/><br/>df = df.drop('type', axis = 1)<br/><br/>#Viewing the new dataframe after one-hot-encoding <br/><br/>df.head()</pre>
<p>In the code, we first create a one-hot encoding object called <kbd>type_one_hot</kbd><em>. </em>We then transform the <kbd>type</kbd><em> </em>column into one-hot encoded columns by using the <br/>
<kbd>fit_transform</kbd><em> </em>method. </p>
<p class="mce-root">We have five categories that are represented by integers 0 to 4. Each of these five categories will now get its own column. Therefore, we create five columns called <kbd>type_0</kbd>, <kbd>type_1</kbd>, <kbd>type_2</kbd>, <kbd>type_3</kbd>, and <kbd>type_4</kbd>. Each of these five columns is represented by two values: <kbd>1</kbd>, which indicates the presence of that category, and <kbd>0</kbd>, which indicates the absence of that category. </p>
<p>This information is stored in the <kbd>ohe_variable</kbd><em>. </em>Since this variable holds the five columns, we will join this to the original dataframe by using the <kbd>concat</kbd><em> </em>method from <kbd>pandas</kbd><em>. </em></p>
<p class="mce-root">The ordinal <kbd>type</kbd> column is then dropped from the dataframe as this column is now redundant post one hot encoding. The final dataframe now looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d1f08c7a-918a-4852-9ec1-485138e2c010.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Missing values</h1>
                </header>
            
            <article>
                
<p><span>Another constraint with scikit-learn is that it cannot handle data with missing values. Therefore, we must check whether our dataset has any missing values in any of the columns to begin with. We can do this by using the following code: </span></p>
<pre>#Checking every column for missing values<br/><br/>df.isnull().any()</pre>
<p>This produces this output: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/539429b2-889d-474b-9403-e591e58f082b.png" style=""/></div>
<p>Here we note that every column has some amount of missing values. </p>
<p>Missing values can be handled in a variety of ways, such as the following:</p>
<ul>
<li>Median imputation</li>
<li>Mean imputation</li>
<li>Filling them with the majority value</li>
</ul>
<p>The amount of techniques is quite large and varies depending on the nature of your dataset. This process of handling features with missing values is called <strong>feature engineering</strong>.</p>
<p>Feature engineering can be done for both categorical and numerical columns and would require an entire book to explain the various methodologies that comprise the topic. </p>
<p>Since this book provides you with a deep focus on the art of applying the various machine learning algorithms that scikit-learn offers, feature engineering will not be dealt with. </p>
<p>So, for the purpose of aligning with the goals that this book intends to achieve, we will impute all the missing values with a zero.</p>
<p>We can do this by using the following code: </p>
<pre>#Imputing the missing values with a 0<br/><br/>df = df.fillna(0)</pre>
<p>We now have a dataset that is ready for machine learning with scikit-learn. We will use this dataset for all the other chapters that we will go through in the future. To make it easy for us, then, we will export this dataset as a <kbd>.csv</kbd> file and store it in the same directory that you are working in with the Jupyter Notebook.</p>
<p>We can do this by using the following code: </p>
<pre>df.to_csv('fraud_prediction.csv')</pre>
<p>This will create a <kbd>.csv</kbd> file of this dataset in the directory that you are working in, which you can load into the notebook again using pandas. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The k-NN algorithm</h1>
                </header>
            
            <article>
                
<p>Mathematically speaking, the k-NN algorithm is one of the most simple machine learning algorithms out there. See the following diagram for a visual overview of how it works: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ce91b1e6-4ccd-4b65-a34a-19b913cb1651.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">How k-NN works under the hood</div>
<p>The stars in the preceding diagram represent new data points. If we built a k-NN algorithm with three neighbors, then the stars would search for the three data points that are closest to it. </p>
<p>In the lower-left case, the star sees two triangles and one circle. Therefore, the algorithm would classify the star as a triangle since the number of triangles was greater than the number of circles. </p>
<p>In the upper-right case, the star sees two circles and one circle. Therefore, the algorithm will classify the star as a circle since the number of circles was greater than the number of triangles. </p>
<p>The real algorithm does this in a very probabilistic manner and picks the category/shape with the highest probability. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the k-NN algorithm using scikit-learn</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following section, we will implement the first version of the k-NN algorithm and assess its initial accuracy. When implementing machine learning algorithms using scikit-learn, it is always a good practice to implement algorithms without fine-tuning or optimizing any of the associated parameters first in order to evaluate how well it performs.</p>
<p>In the following section, you will learn how to do the following:</p>
<ul>
<li>Split your data into training and test sets</li>
<li>Implement the first version of the algorithm on the data</li>
<li>Evaluate the accuracy of your model using a k-NN score</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Splitting the data into training and test sets</h1>
                </header>
            
            <article>
                
<p>The idea of training and test sets is fundamental to every machine learning problem. When thinking about this concept, it is easy to understand why the concept was introduced. Think of machine learning as the direct equivalent to the process of human learning; when learning a concept in mathematics, we first learn how to solve a set of problems with solutions attached to them so that we can understand the exact methodology involved in solving these problems. We then take a test at school or university and attempt to solve problems that we have not encountered or seen before in order to test our understanding. </p>
<p>The training set is a part of the dataset that a machine learning algorithm uses to learn from. The test set is a part of the dataset that the machine learning algorithm has not seen before and is used to assess the performance of the machine learning algorithm. </p>
<p>The first step to this process is to save all our features into one variable and the target variable, which contains the labels into another variable. </p>
<p>In our dataset, the target variable is called <kbd>isFraud</kbd><em> </em>and contains two labels: 0 if the transaction is not a fraud and 1 if the transaction is a fraud. The features are the remaining variables. We can store these into two separate variables by using the following code: </p>
<pre class="mce-root">#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values</pre>
<p>In the preceding code, .<em>values </em>is used to convert the values in the features and target variables into NumPy arrays.</p>
<p><span>Next, we will split the features and target into training and test sets by using the following code:</span></p>
<pre>from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)</pre>
<p>We use the <kbd>train_test_split</kbd> from <kbd>sklearn.model_selection</kbd><em> </em>in order to perform this task. In the preceding code, we have four variables. <kbd>X_train</kbd> and <kbd>X_test</kbd> correspond to the training and test sets for the features, while <kbd>y_train</kbd> and <kbd>y_test</kbd> correspond to training and test sets for the target variable. </p>
<p class="mce-root"/>
<p>The <kbd>train_test_split()</kbd><em> </em>function takes in four arguments. The first argument is the array containing the features, the second argument is the array containing the target variable. The <kbd> test_size</kbd> argument<em> </em>is used to specify the amount of data that will be split and stored into the test set. Since we specified <kbd>0.3</kbd>, 30% of the original data will be stored in the test set, while 70% of the original data will be used for training. </p>
<p><span><span>There are two primary ways in which the <kbd>train_test_split()</kbd> function shuffles data into training and test sets for the target variable:</span></span></p>
<ul>
<li><strong>Random sampling</strong>: Randomly puts target labels into training and test sets (<kbd>y_train</kbd> and <kbd>y_test</kbd> in the preceding case).</li>
<li><strong>Stratified sampling</strong>:<strong> </strong>Ensures that the target labels are represented adequately in the training and test sets. In the preceding code, the <em>stratify</em> argument has been set to the target labels to ensure that this happens. <br/>
 </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation and evaluation of your model</h1>
                </header>
            
            <article>
                
<p>Now that we have the training and test splits, we can implement the k-NN algorithm on the training sets and evaluate its score on the test sets. We can do this by using the following code:</p>
<pre>from sklearn.neighbors import KNeighborsClassifier<br/><br/>#Initializing the kNN classifier with 3 neighbors <br/><br/>knn_classifier = KNeighborsClassifier(n_neighbors=3)<br/><br/>#Fitting the classifier on the training data <br/><br/>knn_classifier.fit(X_train, y_train)<br/><br/>#Extracting the accuracy score from the test sets<br/><br/>knn_classifier.score(X_test, y_test)</pre>
<p>In the preceding code, we first initialize a k-NN classifier with three neighbors. The number of neighbors is chosen arbitrarily, and three is a good starting number. Next, we use the <kbd>.fit()</kbd><em> </em>method to fit this classifier onto our training data. Finally, by using the <kbd>.score()</kbd><em> </em>method on the test data, we obtain a value between 0 and 1 that indicates how accurate the classifier is. </p>
<p>In our case, we obtained an accuracy score of <kbd>0.98</kbd>, which is very good! </p>
<p class="mce-root"/>
<p>There are many ways of assessing and evaluating the performance of the classifier, and the accuracy score should not be the only way you evaluate the performance of your classifier. Further methods of evaluation will be discussed at a later stage in the chapter. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fine-tuning the parameters of the k-NN algorithm</h1>
                </header>
            
            <article>
                
<p>In the previous section, we arbitrarily set the number of neighbors to three while initializing the k-NN classifier. However, is this the optimal value? Well, it could be, since we obtained a relatively high accuracy score in the test set. </p>
<p>Our goal is to create a machine learning model that does not overfit or underfit the data. Overfitting the data means that the model has been trained very specifically to the training examples provided and will not generalize well to cases/examples of data that it has not encountered before. For instance, we might have fit the model very specifically to the training data, with the test cases being also very similar to the training data. Thus, the model would have been able to perform very well and produce a very high value of accuracy. </p>
<p>Underfitting is another extreme case, in which the model fits the data in a very generic way and does not perform well in predicting the correct class labels in the test set. This is the exact opposite of overfitting. </p>
<p class="mce-root"><span>Both these cases can be avoided by visualizing how well the model performs in the training and test sets by using a different number of neighbors. To do this, we fi</span><span>rst find the optimal number of neighbors by using the <kbd>GridSearchCV</kbd> algorithm. </span></p>
<p><kbd>GridSearchCV</kbd> creates an empty grid and fills it with possible values of the number of neighbors or any other machine learning parameter that we want to optimize. It then uses each value in the grid and tests its performance and determines the  optimal value of the parameter. We can implement the <kbd>GridSearchCV</kbd> algorithm to find the optimal number of neighbors by using the following code:</p>
<pre>import numpy as np<br/>from sklearn.model_selection import GridSearchCV<br/><br/>#Initializing a grid with possible number of neighbors from 1 to 24<br/><br/>grid = {'n_neighbors' : np.arange(1, 25)}<br/><br/>#Initializing a k-NN classifier <br/><br/>knn_classifier = KNeighborsClassifier()<br/><br/>#Using cross validation to find optimal number of neighbors <br/><br/>knn = GridSearchCV(knn_classifier, grid, cv = 10)<br/><br/>knn.fit(X_train, y_train)<br/><br/>#Extracting the optimal number of neighbors <br/><br/>knn.best_<em>params_<br/><br/>#Extracting the accuracy score for optimal number of neighbors<br/><br/>knn.best_score_<br/></em></pre>
<p>In this code, we first initialize a number array with values between 1 and 24. This range was chosen arbitrarily and you can increase or decrease the range. However, increasing the range will mean that it will take more computational time to compute and find the optimal number of neighbors, especially when your dataset is large. </p>
<p>Next, we initialize a k-NN classifier and use the <kbd>GridSearchCV()</kbd><em> </em>function on the classifier along with the grid. We set the <kbd>cv</kbd><em> </em>argument to 10, indicating that we want to use 10-fold cross validation while doing this. Cross validation is a technique in which the classifier first divides the data into 10 parts. The first nine parts are used as the training set while the <span>10</span><sup>th</sup> part is used as the test set. In the second iteration, we use the first eight parts and the 10<sup>th</sup> part as the training set, while the ninth part is used as the test set. This process is repeated until every part of the data is used for testing. This creates a very robust classifier, since we have used the entire dataset for training and testing and have not left out any part of the data. </p>
<p>Cross-validation is illustrated for you in the following diagram: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/21251247-757b-4866-bc0d-8cbbb882ec41.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Cross-validation in action</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the preceding diagram, the black boxes illustrate the training data while the white box illustrates the test data. </p>
<p>Finally, we use the <kbd>.best_params_</kbd><em> </em>to extract the optimal number of neighbors. In our case, the optimal number of neighbors was 1, which resulted in an accuracy score of <kbd>0.985</kbd>. This is an improvement of 0.002 from the original classifier that we built, which had a score of <kbd>0.983</kbd> with three neighbors. </p>
<p>Using cross-validation ensures that we do not overfit or underfit the data as we have used the entire dataset for training and testing. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling for optimized performance</h1>
                </header>
            
            <article>
                
<p>The k-NN algorithm is an algorithm that works based on distance. When a new data point is thrown into the dataset and the algorithm is given the task of classifying this new data point, it uses distance to check the points that are closest to it. </p>
<p>If we have features that have different ranges of values – for example, feature one has a range between 0 to 800 while feature two has a range between one to five – this distance metric does not make sense anymore. We want all the features to have the same range of values so that the distance metric is on level terms across all features. </p>
<p>One way to do this is to subtract each value of each feature by the mean of that feature and divide by the variance of that feature. This is called <strong>standardization</strong>:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/4c23e62d-4280-4704-8ecb-1c1acc45a6ba.png" style="width:70.25em;height:1.83em;"/></div>
<p>We can do this for our dataset by using the following code: </p>
<pre>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import Pipeline<br/><br/>#Setting up the scaling pipeline <br/><br/>pipeline_order = [('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors = 1))]<br/><br/>pipeline = Pipeline(pipeline_order)<br/><br/>#Fitting the classfier to the scaled dataset <br/><br/>knn_classifier_scaled = pipeline.fit(X_train, y_train)<br/><br/>#Extracting the score <br/><br/>knn_classifier_scaled.score(X_test, y_test)</pre>
<p>In this code, we specify the order in which the pipeline has to be executed. We store this order in a variable called <kbd>pipeline_order</kbd><em> </em>by specifying that we want to scale our data first by using the <kbd>StandardScaler()</kbd><em> </em>function and then build a k-NN classifier with one neighbor. </p>
<p>Next, we use the <kbd>Pipeline()</kbd><em> </em>function and pass in the order of the pipeline as the only argument. We then fit this pipeline to the training set and extract the accuracy scores from the test set. </p>
<p>The <kbd>Pipeline</kbd> function, as the name implies, is used to fit multiple functions into a pipeline and execute them in a specified order that we think is apt for the process. This function helps us streamline and automate common machine learning tasks. </p>
<p>This resulted in an accuracy score of <kbd>0.997</kbd>, which is a substantial improvement from the score of <kbd>0.985</kbd>. Thus, we see how scaling the data results in improved performance. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter was fundamental in helping you prepare a dataset for machine learning with scikit-learn. You have learned about the constraints that are imposed when you do machine learning with scikit-learn and how to create a dataset that is perfect for scikit-learn. </p>
<p>You have also learned how the k-NN algorithm works behind the scenes and have implemented a version of it using scikit-learn to predict whether a transaction was fraudulent. You then learned how to optimize the parameters of the algorithm using the popular <kbd>GridSearchCV</kbd> algorithm. Finally, you have learnt how to standardize and scale your data in order to optimize the performance of your model. </p>
<p class="mce-root"><span>In the next chapter, you will learn how to classify fraudulent transactions yet again with a new algorithm – t</span><span>he logistic regression algorithm!</span></p>


            </article>

            
        </section>
    </body></html>
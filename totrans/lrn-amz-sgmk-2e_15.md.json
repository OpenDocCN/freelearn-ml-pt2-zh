["```py\n    $ tar xvfz model.tar.gz\n    x model_algo-1\n    $ unzip model_algo-1\n    archive:  model_algo-1\n    extracting: additional-params.json\n    extracting: manifest.json\n    extracting: mx-mod-symbol.json\n    extracting: mx-mod-0000.params\n    ```", "```py\n    import json\n    sym_json = json.load(open('mx-mod-symbol.json'))\n    sym_json_string = json.dumps(sym_json)\n    ```", "```py\n    import mxnet as mx\n    from mxnet import gluon\n    net = gluon.nn.SymbolBlock(\n        outputs=mx.sym.load_json(sym_json_string),\n        inputs=mx.sym.var('data')) \n    ```", "```py\n    mx.viz.plot_network(\n        net(mx.sym.var('data'))[0],   \n        node_attrs={'shape':'oval','fixedsize':'false'})\n    ```", "```py\n    net.load_parameters('mx-mod-0000.params', \n                        allow_missing=True)\n    net.collect_params().initialize()\n    ```", "```py\n    test_sample = mx.nd.array(\n    [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98])\n    ```", "```py\n    response = net(test_sample)\n    print(response)\n    ```", "```py\n    array([[30.173424]], dtype=float32)\n    ```", "```py\n    $ tar xvfz model.tar.gz\n    x image-classification-0010.params\n    x model-shapes.json\n    x image-classification-symbol.json\n    ```", "```py\n    import mxnet, json\n    from mxnet import gluon\n    sym_json = json.load(\n               open('image-classification-symbol.json'))\n    sym_json_string = json.dumps(sym_json)\n    net = gluon.nn.SymbolBlock(\n        outputs=mx.sym.load_json(sym_json_string),\n        inputs=mx.sym.var('data'))\n    net.load_parameters(\n        'image-classification-0010.params',  \n        allow_missing=True)\n    net.collect_params().initialize()\n    ```", "```py\n    test_sample = mx.ndarray.random.normal(\n        shape=(1,3,300,300))\n    response = net(test_sample)\n    print(response)\n    ```", "```py\n    array([[0.99126923, 0.00873081]], dtype=float32)\n    ```", "```py\n$ tar xvfz model.tar.gz\nx xgboost-model\n$ python\n>>> import pickle\n>>> model = pickle.load(open('xgboost-model', 'rb'))\n>>> type(model)\n<class 'xgboost.core.Booster'>\n```", "```py\n$ tar xvfz model.tar.gz\nx xgb.model\n$ python\n>>> import xgboost as xgb\n>>> bst = xgb.Booster({'nthread': 4})\n>>> model = bst.load_model('xgb.model')\n>>> type(bst)\n<class 'xgboost.core.Booster'>\n```", "```py\n$ tar xvfz model.tar.gz\nx model.joblib\n$ python\n>>> import joblib\n>>> model = joblib.load('model.joblib')\n>>> type(model)\n<class 'sklearn.linear_model._base.LinearRegression'>\n```", "```py\n$ mkdir /tmp/models\n$ tar xvfz model.tar.gz -C /tmp/models\nx 1/\nx 1/saved_model.pb\nx 1/assets/\nx 1/variables/\nx 1/variables/variables.index\nx 1/variables/variables.data-00000-of-00002\nx 1/variables/variables.data-00001-of-00002\n```", "```py\n$ docker run -t --rm -p 8501:8501\n  -v \"/tmp/models:/models/fmnist\"\n  -e MODEL_NAME=fmnist\n  tensorflow/serving\n```", "```py\n    $ tar xvfz model.tar.gz\n    training_args.bin\n    config.json\n    pytorch_model.bin\n    ```", "```py\n    from transformers import AutoConfig, DistilBertForSequenceClassification\n    config = AutoConfig.from_pretrained(\n             './model/config.json')\n    model = DistilBertForSequenceClassification\n           .from_pretrained('./model/pytorch_model.bin',  \n                            config=config)\n    ```", "```py\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n                'distilbert-base-uncased') \n    ```", "```py\n    import torch\n    def probs(logits):\n        softmax = torch.nn.Softmax(dim=1)\n        pred = softmax(logits).detach().numpy()\n        return pred\n    ```", "```py\n    inputs = tokenizer(\"The Phantom Menace was a really bad movie. What a waste of my life.\", return_tensors='pt')\n    outputs = model(**inputs)\n    print(probs(outputs.logits))\n    ```", "```py\n    [[0.22012234 0.7798777 ]]\n    ```", "```py\n    $ tar cvfz model-xgb.tar.gz xgboost-model\n    ```", "```py\n    import sagemaker\n    sess = sagemaker.Session()\n    prefix = 'export-xgboost'\n    model_path = sess.upload_data(\n        path=model-xgb.tar.gz', \n        key_prefix=prefix)\n    ```", "```py\n    from sagemaker.xgboost.model import XGBoostModel\n    xgb_model = XGBoostModel(\n        model_data=model_path,\n        entry_point='xgb-script.py',\n        framework_version='1.3-1',\n        role=sagemaker.get_execution_role())\n    ```", "```py\n    import os\n    import xgboost as xgb\n    def model_fn(model_dir):\n        model = xgb.Booster()\n        model.load_model(\n            os.path.join(model_dir,'xgboost-model'))\n        return model\n    ```", "```py\n    xgb_predictor = xgb_model.deploy(. . .)\n    xgb_predictor.predict(. . .)\n    ```", "```py\n    $ tar tvfz model.tar.gz\n    1/\n    1/saved_model.pb\n    1/assets/\n    1/variables/\n    1/variables/variables.index\n    1/variables/variables.data-00000-of-00002\n    1/variables/variables.data-00001-of-00002\n    ```", "```py\n    import sagemaker\n    sess = sagemaker.Session()\n    prefix = 'byo-tf'\n    model_path = sess.upload_data(\n       path='model.tar.gz', \n       key_prefix=prefix)\n    ```", "```py\n    from sagemaker.tensorflow.model import TensorFlowModel\n    tf_model = TensorFlowModel(\n        model_data=model_path,\n        framework_version='2.3.1',\n        role=sagemaker.get_execution_role())\n    ```", "```py\n    tokenizer = AutoTokenizer.from_pretrained(\n                'distilbert-base-uncased')\n    def model_fn(model_dir):\n      config_path='{}/config.json'.format(model_dir)\n      model_path='{}/pytorch_model.bin'.format(model_dir)\n      config=AutoConfig.from_pretrained(config_path)\n      model= DistilBertForSequenceClassification\n             .from_pretrained(model_path, config=config)\n      return model\n    ```", "```py\n    def input_fn(serialized_input_data, \n                 content_type=JSON_CONTENT_TYPE):  \n      if content_type == JSON_CONTENT_TYPE:\n        input_data = json.loads(serialized_input_data)\n        return input_data\n      else:\n        raise Exception('Unsupported input type: ' \n                        + content_type)\n    def output_fn(prediction_output, \n                  accept=JSON_CONTENT_TYPE):\n      if accept == JSON_CONTENT_TYPE:\n        return json.dumps(prediction_output), accept\n      else:\n        raise Exception('Unsupported output type: '\n                        + accept)\n    ```", "```py\n    CLASS_NAMES = ['negative', 'positive']\n    def predict_fn(input_data, model):\n        inputs = tokenizer(input_data['text'], \n                           return_tensors='pt')\n        outputs = model(**inputs)\n        logits = outputs.logits\n        _, prediction = torch.max(logits, dim=1)\n        return CLASS_NAMES[prediction]\n    ```", "```py\n    from sagemaker.pytorch import PyTorchModel\n    model = PyTorchModel(\n        model_data=model_data_uri,\n        role=sagemaker.get_execution_role(), \n        entry_point='torchserve-predictor.py',\n        source_dir='src',\n        framework_version='1.6.0',\n        py_version='py36')\n    ```", "```py\n    positive_data = {'text': \"This is a very nice camera, I'm super happy with it.\"}\n    negative_data = {'text': \"Terrible purchase, I want my money back!\"}\n    prediction = predictor.predict(positive_data)\n    print(prediction)\n    prediction = predictor.predict(negative_data)\n    print(prediction)\n    ```", "```py\n    from sagemaker.tensorflow.model import TensorFlowPredictor\n    another_predictor = TensorFlowPredictor(\n        endpoint_name=tf_endpoint_name,\n        serializer=sagemaker.serializers.JSONSerializer()\n    )\n    ```", "```py\n    another_predictor.predict(…)\n    ```", "```py\n    another_predictor.update_endpoint(\n        initial_instance_count=2,\n        instance_type='ml.t2.medium')\n    ```", "```py\n    production_variants = [\n      { 'VariantName': 'variant-1',\n        'ModelName': model_name_1,\n        'InitialInstanceCount': 1,\n        'InitialVariantWeight': 9,\n        'InstanceType': 'ml.t2.medium'},\n      { 'VariantName': 'variant-2',\n        'ModelName': model_name_2,\n        'InitialInstanceCount': 1,\n        'InitialVariantWeight': 1,\n        'InstanceType': 'ml.t2.medium'}]\n    ```", "```py\n    import boto3\n    sm = boto3.client('sagemaker')\n    endpoint_config_name = 'xgboost-two-models-epc'\n    response = sm.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=production_variants,\n        Tags=[{'Key': 'Name', \n               'Value': endpoint_config_name},\n              {'Key': 'Algorithm', 'Value': 'xgboost'}])\n    ```", "```py\n    endpoint_name = 'xgboost-two-models-ep'\n    response = sm.create_endpoint(\n        EndpointName=endpoint_name,\n        EndpointConfigName=endpoint_config_name,\n        Tags=[{'Key': 'Name','Value': endpoint_name},\n              {'Key': 'Algorithm','Value': 'xgboost'},\n              {'Key': 'Environment',\n               'Value': 'development'}])\n    ```", "```py\n    waiter = sm.get_waiter('endpoint_in_service')\n    waiter.wait(EndpointName=endpoint_name)\n    ```", "```py\n    smrt = boto3.Session().client(\n        service_name='runtime.sagemaker') \n    response = smrt.invoke_endpoint(\n       EndpointName=endpoint_name,\n       ContentType='text/csv',\n       Body=test_sample)\n    ```", "```py\n    variants = ['variant-1', 'variant-2']\n    for v in variants:\n      response = smrt.invoke_endpoint(\n                     EndpointName=endpoint_name, \n                     ContentType='text/csv',\n                     Body=test_sample,\n                     TargetVariant=v)\n      print(response['Body'].read())\n    ```", "```py\n    b'[0.0013231043703854084]'\n    b'[0.001262241625227034]'\n    ```", "```py\n    response = sm.update_endpoint_weights_and_capacities(\n        EndpointName=endpoint_name,\n        DesiredWeightsAndCapacities=[\n            { 'VariantName': 'variant-1', \n              'DesiredWeight': 5},\n            { 'VariantName': 'variant-2', \n              'DesiredWeight': 5}])\n    ```", "```py\n    production_variants_2 = [\n      {'VariantName': 'variant-2',\n       'ModelName': model_name_2,\n       'InitialInstanceCount': 1,\n       'InitialVariantWeight': 1,\n       'InstanceType': 'ml.t2.medium'}]\n    endpoint_config_name_2 = 'xgboost-one-model-epc'\n    response = sm.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name_2,\n        ProductionVariants=production_variants_2,\n        Tags=[{'Key': 'Name',\n               'Value': endpoint_config_name_2},\n              {'Key': 'Algorithm','Value': 'xgboost'}])\n    response = sm.update_endpoint(\n        EndpointName=endpoint_name,\n        EndpointConfigName=endpoint_config_name_2)\n    ```", "```py\n    sm.delete_endpoint(EndpointName=endpoint_name)\n    sm.delete_endpoint_config(\n      EndpointConfigName=endpoint_config_name)\n    sm.delete_endpoint_config(\n      EndpointConfigName=endpoint_config_name_2)\n    ```", "```py\n    from sagemaker.sklearn import SKLearn\n    sk = SKLearn(entry_point='sklearn-boston-housing.py',\n       role=sagemaker.get_execution_role(),\n       instance_count=1,\n       instance_type='ml.m5.large',\n       output_path=output,\n       hyperparameters=\n           {'normalize': True, 'test-size': 0.1})\n    sk.fit({'training':training})\n    ```", "```py\n    import pandas as pd\n    data = pd.read_csv('housing.csv')\n    data.drop(['medv'], axis=1, inplace=True)\n    data.to_csv('data.csv', header=False, index=False)\n    batch_input = sess.upload_data(\n        path='data.csv', \n        key_prefix=prefix + '/batch')\n    ```", "```py\n    sk_transformer = sk.transformer(\n        instance_count=1, \n        instance_type='ml.m5.large')\n    sk_transformer.transform(\n        batch_input, \n        content_type='text/csv', \n        wait=True, logs=True)\n    ```", "```py\n    print(sk_transformer.output_path)\n    s3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2020-06-12-08-28-30-978\n    ```", "```py\n    %%bash -s \"$sk_transformer.output_path\"\n    aws s3 cp $1/data.csv.out .\n    head -1 data.csv.out\n    [[29.73828574177013], [24.920634119498292], …\n    ```", "```py\n    response = sagemaker.create_model(\n        ModelName='pca-linearlearner-pipeline',\n            Containers=[\n                {\n                 'Image': pca_container,\n                 'ModelDataUrl': pca_model_artifact,\n                  . . .\n                },\n                {\n                 'Image': ll_container,\n                 'ModelDataUrl': ll_model_artifact,\n                  . . .\n                }\n            ],\n            ExecutionRoleArn=role\n    )\n    ```", "```py\n    from sagemaker.model_monitor.data_capture_config import DataCaptureConfig\n    capture_path = 's3://{}/{}/capture/'.format(bucket, prefix)\n    ll_predictor = ll.deploy(\n        initial_instance_count=1,\n        instance_type='ml.t2.medium',\n        data_capture_config = DataCaptureConfig(     \n             enable_capture = True,                    \n             sampling_percentage = 100,                \n             capture_options = ['REQUEST', 'RESPONSE'],\n             destination_s3_uri = capture_path))\n    ```", "```py\n    %%bash -s \"$capture_path\"\n    aws s3 ls --recursive $1\n    aws s3 cp --recursive $1 .\n    ```", "```py\n    {\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"30.4133586884\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"8f45e35c-fa44-40d2-8ed3-1bcab3a596f3\",\"inferenceTime\":\"2020-07-30T13:36:30Z\"},\"eventVersion\":\"0\"}\n    ```", "```py\n    data.to_csv('housing.csv', sep=',', index=False)\n    training = sess.upload_data(\n       path='housing.csv', \n       key_prefix=prefix + \"/baseline\")\n    ```", "```py\n    from sagemaker.model_monitor import DefaultModelMonitor\n    from sagemaker.model_monitor.dataset_format import DatasetFormat\n    ll_monitor = DefaultModelMonitor(role=role,\n        instance_count=1, instance_type='ml.m5.large')\n    ll_monitor.suggest_baseline(baseline_dataset=training,\n        dataset_format=DatasetFormat.csv(header=True))\n    ```", "```py\n    baseline = ll_monitor.latest_baselining_job\n    constraints = pd.io.json.json_normalize(\n        baseline.suggested_constraints()\n        .body_dict[\"features\"])\n    schema = pd.io.json.json_normalize(\n        baseline.baseline_statistics().body_dict[\"features\"])\n    ```", "```py\nfrom sagemaker.model_monitor import CronExpressionGenerator\nll_monitor.create_monitoring_schedule(\n    monitor_schedule_name='ll-housing-schedule',\n    endpoint_input=ll_predictor.endpoint,\n    statistics=ll_monitor.baseline_statistics(),\n    constraints=ll_monitor.suggested_constraints(),\n    schedule_cron_expression=CronExpressionGenerator.hourly())\n```", "```py\n    test_sample = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'\n    ll_predictor.serializer =    \n        sagemaker.serializers.CSVSerializer()\n    ll_predictor.deserializer =  \n        sagemaker.deserializers.CSVDeserializer()\n    response = ll_predictor.predict(test_sample)\n    print(response)\n    ```", "```py\n    [['30.1734218597']]\n    ```", "```py\n    bad_sample_1 = '632.0,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'\n    response = ll_predictor.predict(bad_sample_1)\n    print(response)\n    ```", "```py\n    [['-35.7245635986']]\n    ```", "```py\n    bad_sample_2 = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,-4.98'\n    response = ll_predictor.predict(bad_sample_2)\n    print(response)\n    ```", "```py\n    [['34.4245414734']]\n    ```", "```py\n    ll_executions = ll_monitor.list_executions()\n    print(ll_executions)\n    ```", "```py\n    [<sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdd1d55a6d8>,\n    <sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdd1d581630>,\n    <sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdce4b1c860>]\n    ```", "```py\n    violations = ll_monitor.latest_monitoring_constraint_violations()\n    violations = pd.io.json.json_normalize(\n        violations.body_dict[\"violations\"])\n    violations\n    ```", "```py\n    %%bash -s \"$report_path\"\n    echo $1\n    aws s3 ls --recursive $1\n    aws s3 cp --recursive $1 .\n    ```", "```py\n    {\n        \"feature_name\" : \"chas\",\n        \"constraint_check_type\" : \"data_type_check\",\n        \"description\" : \"Data type match requirement is not met.\n            Expected data type: Integral, Expected match: 100.0%.  \n            Observed: Only 0.0% of data is Integral.\"\n    }\n    ```", "```py\n    response = ll_monitor.delete_monitoring_schedule()\n    ll_predictor.delete_endpoint()\n    ```", "```py\n    %env model_data {tf_estimator.model_data}\n    ```", "```py\n    %%sh\n    aws s3 cp ${model_data} .\n    mkdir test-models\n    tar xvfz model.tar.gz -C test-models\n    ```", "```py\n    <initialize git repository>\n    $ cd test-models\n    $ git add model\n    $ git commit -m \"New model\"\n    $ git push\n    ```", "```py\n    %%sh\n    sudo curl -o /usr/local/bin/ecs-cli https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest\n    sudo chmod 755 /usr/local/bin/ecs-cli\n    ```", "```py\n    %%sh \n    aws ecs create-cluster --cluster-name fargate-demo\n    ecs-cli configure --cluster fargate-demo --region eu-west-1\n    ```", "```py\n    %%sh\n    aws logs create-log-group --log-group-name awslogs-tf-ecs\n    ```", "```py\n    {\n      \"requiresCompatibilities\": [\"FARGATE\"],\n      \"family\": \"inference-fargate-tf-230\",\n      \"memory\": \"8192\",\n      \"cpu\": \"4096\",\n    ```", "```py\n      \"containerDefinitions\": [{\n        \"name\": \"dlc-tf-inference\",\n        \"image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.2-cpu-py37-ubuntu18.04\",\n        \"essential\": true,\n    ```", "```py\n        \"command\": [\n           \"mkdir -p /test && cd /test && git clone https://gitlab.com/juliensimon/test-models.git && tensorflow_model_server --port=8500 \n    --rest_api_port=8501 --model_name=1 \n    --model_base_path=/test/test-models/model\"\n        ],\n        \"entryPoint\": [\"sh\",\"-c\"],\n    ```", "```py\n        \"portMappings\": [\n            {\n              \"hostPort\": 8500,\n              \"protocol\": \"tcp\",\n              \"containerPort\": 8500\n            },\n            {\n              \"hostPort\": 8501,\n              \"protocol\": \"tcp\",\n              \"containerPort\": 8501\n            }\n        ],\n    ```", "```py\n        \"logConfiguration\": {\n          \"logDriver\": \"awslogs\",\n            \"options\": {\n              \"awslogs-group\": \"awslogs-tf-ecs\",\n              \"awslogs-region\": \"eu-west-1\",\n              \"awslogs-stream-prefix\": \"inference\"\n            }\n        }\n      }],\n    ```", "```py\n      \"networkMode\": \"awsvpc\"\n    ```", "```py\n      \"executionRoleArn\":  \n      \"arn:aws:iam::123456789012:role/ecsTaskExecutionRole\"\n    }\n    ```", "```py\n    %%sh\n    aws ecs run-task\n      --cluster fargate-demo \n      --task-definition inference-fargate-tf-230:1 \n      --count 1\n      --launch-type FARGATE\n      --network-configuration \n        \"awsvpcConfiguration={subnets=[$SUBNET_ID],\n         securityGroups=[$SECURITY_GROUP_ID],\n         assignPublicIp=ENABLED}\"\n    ```", "```py\n    %%sh\n    ecs-cli ps --desired-status RUNNING\n    a9c9a3a8-8b7c-4dbb-9ec4-d20686ba5aec/dlc-tf-inference  \n    RUNNING  \n    52.49.238.243:8500->8500/tcp, \n    52.49.238.243:8501->8501/tcp                         inference-fargate-tf230:1\n    ```", "```py\n    import random, json, requests\n    inference_task_ip = '52.49.238.243'\n    inference_url = 'http://' +   \n                    inference_task_ip +  \n                    ':8501/v1/models/1:predict'\n    indices = random.sample(range(x_val.shape[0] - 1), 10)\n    images = x_val[indices]/255\n    labels = y_val[indices]\n    data = images.reshape(num_samples, 28, 28, 1)\n    data = json.dumps(\n        {\"signature_name\": \"serving_default\", \n         \"instances\": data.tolist()})\n    headers = {\"content-type\": \"application/json\"}\n    json_response = requests.post(\n        inference_url, \n        data=data, \n        headers=headers)\n    predictions = json.loads(\n        json_response.text)['predictions']\n    predictions = np.array(predictions).argmax(axis=1)\n    print(\"Labels     : \", labels)\n    print(\"Predictions: \", predictions)\n    Labels     :  [9 8 8 8 0 8 9 7 1 1]\n    Predictions:  [9 8 8 8 0 8 9 7 1 1]\n    ```", "```py\n    %%sh\n    aws ecs stop-task --cluster fargate-demo \\\n                      --task $TASK_ARN\n    ecs-cli down --force --cluster fargate-demo\n    ```"]
- en: Chapter 3. Feature Extraction and Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The examples discussed in the previous chapter used simple numeric explanatory
    variables, such as the diameter of a pizza. Many machine learning problems require
    learning from observations of categorical variables, text, or images. In this
    chapter, you will learn basic techniques for preprocessing data and creating feature
    representations of these observations. These techniques can be used with the regression
    models discussed in [Chapter 2](ch02.html "Chapter 2. Linear Regression"), *Linear
    Regression*, as well as the models we will discuss in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning problems have **categorical**, or **nominal**, rather
    than continuous features. For example, an application that predicts a job's salary
    based on its description might use categorical features such as the job's location.
    Categorical variables are commonly encoded using **one-of-K** or **one-hot** encoding,
    in which the explanatory variable is encoded using one binary feature for each
    of the variable's possible values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s assume that our model has a `city` explanatory variable
    that can take one of three values: `New York`, `San Francisco`, or `Chapel Hill`.
    One-hot encoding represents this explanatory variable using one binary feature
    for each of the three possible cities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, the `DictVectorizer` class can be used to one-hot encode categorical
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that resulting features will not necessarily be ordered in the feature
    vector as they were encountered. In the first training example, the `city` feature's
    value is `New York`. The second element in the feature vectors corresponds to
    the `New York` value and is set to `1` for the first instance. It may seem intuitive
    to represent the values of a categorical explanatory variable with a single integer
    feature, but this would encode artificial information. For example, the feature
    vectors for the previous example would have only one dimension. `New York` could
    be represented by `0`, `San Francisco` by `1`, and `Chapel Hill` by `2`. This
    representation would encode an order for the values of the variable that does
    not exist in the real world; there is no natural order of cities.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many machine learning problems use text as an explanatory variable. Text must
    be transformed to a different representation that encodes as much of its meaning
    as possible in a feature vector. In the following sections we will review variations
    of the most common representation of text that is used in machine learning: the
    bag-of-words model.'
  prefs: []
  type: TYPE_NORMAL
- en: The bag-of-words representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common representation of text is the **bag-of-words** model. This representation
    uses a multiset, or bag, that encodes the words that appear in a text; the bag-of-words
    does not encode any of the text's syntax, ignores the order of words, and disregards
    all grammar. Bag-of-words can be thought of as an extension to one-hot encoding.
    It creates one feature for each word of interest in the text. The bag-of-words
    model is motivated by the intuition that documents containing similar words often
    have similar meanings. The bag-of-words model can be used effectively for document
    classification and retrieval despite the limited information that it encodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A collection of documents is called a **corpus**. Let''s use a corpus with
    the following two documents to examine the bag-of-words model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This corpus contains eight unique words: `UNC`, `played`, `Duke`, `in`, `basketball`,
    `lost`, `the`, and `game`. The corpus''s unique words comprise its **vocabulary**.
    The bag-of-words model uses a feature vector with an element for each of the words
    in the corpus''s vocabulary to represent each document. Our corpus has eight unique
    words, so each document will be represented by a vector with eight elements. The
    number of elements that comprise a feature vector is called the vector''s **dimension**.
    A **dictionary** maps the vocabulary to indices in the feature vector.'
  prefs: []
  type: TYPE_NORMAL
- en: In the most basic bag-of-words representation, each element in the feature vector
    is a binary value that represents whether or not the corresponding word appeared
    in the document. For example, the first word in the first document is `UNC`. The
    first word in the dictionary is `UNC`, so the first element in the vector is equal
    to one. The last word in the dictionary is `game`. The first document does not
    contain the word `game`, so the eighth element in its vector is set to `0`. The
    `CountVectorizer` class can produce a bag-of-words representation from a string
    or file. By default, `CountVectorizer` converts the characters in the documents
    to lowercase, and **tokenizes** the documents. Tokenization is the process of
    splitting a string into **tokens**,or meaningful sequences of characters. Tokens
    frequently are words, but they may also be shorter sequences including punctuation
    characters and affixes. The `CountVectorizer` class tokenizes using a regular
    expression that splits strings on whitespace and extracts sequences of characters
    that are two or more characters in length.
  prefs: []
  type: TYPE_NORMAL
- en: 'The documents in our corpus are represented by the following feature vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s add a third document to our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Our corpus''s dictionary now contains the following ten unique words. Note
    that `I` and `a` were not extracted as they do not match the default regular expression
    that CountVectorizer uses to tokenize strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our feature vectors are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The meanings of the first two documents are more similar to each other than
    they are to the third document, and their corresponding feature vectors are more
    similar to each other than they are to the third document''s feature vector when
    using a metric such as **Euclidean distance**. The Euclidean distance between
    two vectors is equal to the **Euclidean norm**, or L2 norm, of the difference
    between the two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The bag-of-words representation](img/8365OS_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall that the Euclidean norm of a vector is equal to the vector''s magnitude,
    which is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The bag-of-words representation](img/8365OS_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'scikit-learn''s `euclidean_distances` function can be used to calculate the
    distance between two or more vectors, and it confirms that the most semantically
    similar documents are also the closest to each other in space. In the following
    example, we will use the `euclidean_distances` function to compare the feature
    vectors for our documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now let's assume that we are using a corpus of news articles instead of our
    toy corpus. Our dictionary may now have hundreds of thousands of unique words
    instead of only twelve. The feature vectors representing the articles will each
    have hundreds of thousands of elements, and many of the elements will be zero.
    Most sports articles will not have any of the words particular to finance articles
    and most culture articles will not have any of the words particular to articles
    about finance. High-dimensional feature vectors that have many zero-valued elements
    are called **sparse vectors**.
  prefs: []
  type: TYPE_NORMAL
- en: Using high-dimensional data creates several problems for all machine learning
    tasks, including those that do not involve text. The first problem is that high-dimensional
    vectors require more memory than smaller vectors. NumPy provides some data types
    that mitigate this problem by efficiently representing only the nonzero elements
    of sparse vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The second problem is known as the **curse of dimensionality**, or the **Hughes
    effect**. As the feature space's dimensionality increases, more training data
    is required to ensure that there are enough training instances with each combination
    of the feature's values. If there are insufficient training instances for a feature,
    the algorithm may overfit noise in the training data and fail to generalize. In
    the following sections, we will review several strategies to reduce the dimensionality
    of text features. In [Chapter 7](ch07.html "Chapter 7. Dimensionality Reduction
    with PCA"), *Dimensionality Reduction with PCA*, we will review techniques for
    numerical dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Stop-word filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A basic strategy to reduce the dimensions of the feature space is to convert
    all of the text to lowercase. This is motivated by the insight that the letter
    case does not contribute to the meanings of most words; `sandwich` and `Sandwich`
    have the same meaning in most contexts. Capitalization may indicate that a word
    is at the beginning of a sentence, but the bag-of-words model has already discarded
    all information from word order and grammar.
  prefs: []
  type: TYPE_NORMAL
- en: 'A second strategy is to remove words that are common to most of the documents
    in the corpus. These words, called **stop** **words**, include determiners such
    as `the`, `a`, and `an`; auxiliary verbs such as `do`, `be`, and `will`; and prepositions
    such as `on`, `around`, and `beneath`. Stop words are often functional words that
    contribute to the document''s meaning through grammar rather than their denotations.
    The `CountVectorizer` class can filter stop words provided as the `stop_words`
    keyword argument and also includes a basic English stop list. Let''s recreate
    the feature vectors for our documents using stop filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The feature vectors have now fewer dimensions, and the first two document vectors
    are still more similar to each other than they are to the third document.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and lemmatization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While stop filtering is an easy strategy for dimensionality reduction, most
    stop lists contain only a few hundred words. A large corpus may still have hundreds
    of thousands of unique words after filtering. Two similar strategies for further
    reducing dimensionality are called **stemming** and **lemmatization**.
  prefs: []
  type: TYPE_NORMAL
- en: A high-dimensional document vector may separately encode several derived or
    inflected forms of the same word. For example, `jumping` and `jumps` are both
    forms of the word `jump`; a document vector in a corpus of long-jumping articles
    may encode each inflected form with a separate element in the feature vector.
    Stemming and lemmatization are two strategies to condense inflected and derived
    forms of a word into a single feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider another toy corpus with two documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The documents have similar meanings, but their feature vectors have no elements
    in common. Both documents contain a conjugation of `ate` and an inflected form
    of `sandwich`. Ideally, these similarities should be reflected in the feature
    vectors. Lemmatization is the process of determining the **lemma**, or the morphological
    root, of an inflected word based on its context. Lemmas are the base forms of
    words that are used to key the word in a dictionary. **Stemming** has a similar
    goal to lemmatization, but it does not attempt to produce the morphological roots
    of words. Instead, stemming removes all patterns of characters that appear to
    be affixes, resulting in a token that is not necessarily a valid word. Lemmatization
    frequently requires a lexical resource, like WordNet, and the word's part of speech.
    Stemming algorithms frequently use rules instead of lexical resources to produce
    stems and can operate on any token, even without its context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider lemmatization of the word `gathering` in two documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first sentence `gathering` is a verb, and its lemma is `gather`. In
    the second sentence `gathering` is a noun, and its lemma is `gathering`. We will
    use the **Natural Language Tool Kit** (**NTLK**) to stem and lemmatize the corpus.
    NLTK can be installed using the instructions at [http://www.nltk.org/install.html](http://www.nltk.org/install.html).
    After installation, execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then follow the instructions to download the corpora for NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the parts of speech of `gathering`, NLTK''s `WordNetLemmatizer` correctly
    lemmatizes the words in both documents as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compare lemmatization with stemming. The Porter stemmer cannot consider
    the inflected form''s part of speech and returns `gather` for both documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s lemmatize our toy corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Through stemming and lemmatization, we reduced the dimensionality of our feature
    space. We produced feature representations that more effectively encode the meanings
    of the documents despite the fact that the words in the corpus's dictionary are
    inflected differently in the sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Extending bag-of-words with TF-IDF weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section we used the bag-of-words representation to create feature
    vectors that encode whether or not a word from the corpus's dictionary appears
    in a document. These feature vectors do not encode grammar, word order, or the
    frequencies of words. It is intuitive that the frequency with which a word appears
    in a document could indicate the extent to which a document pertains to that word.
    A long document that contains one occurrence of a word may discuss an entirely
    different topic than a document that contains many occurrences of the same word.
    In this section, we will create feature vectors that encode the frequencies of
    words, and discuss strategies to mitigate two problems caused by encoding term
    frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a binary value for each element in the feature vector, we will
    now use an integer that represents the number of times that the words appeared
    in the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following corpus. With stop word filtering, the corpus is represented
    by the following feature vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The element for `dog` is now set to `1` and the element for `sandwich` is set
    to `2` to indicate that the corresponding words occurred once and twice, respectively.
    Note that the `binary` keyword argument of `CountVectorizer` is omitted; its default
    value is `False`, which causes it to return raw term frequencies rather than binary
    frequencies. Encoding the terms' raw frequencies in the feature vector provides
    additional information about the meanings of the documents, but assumes that all
    of the documents are of similar lengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many words might appear with the same frequency in two documents, but the documents
    could still be dissimilar if one document is many times larger than the other.
    scikit-learn''s `TfdfTransformer` object can mitigate this problem by transforming
    a matrix of term frequency vectors into a matrix of normalized term frequency
    weights. By default, `TfdfTransformer` smoothes the raw counts and applies L2
    normalization. The smoothed, normalized term frequencies are given by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_10.jpg) is the
    frequency of term ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_16.jpg)
    in document ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_13.jpg)
    and ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_11.jpg) is the
    L2 norm of the count vector. In addition to normalizing raw term counts, we can
    improve our feature vectors by calculating **logarithmically scaled term frequencies**,
    which scale the counts to a more limited range,or **augmented term frequencies**,
    which further mitigates the bias for longer documents. Logarithmically scaled
    term frequencies are given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `TfdfTransformer` object calculates logarithmically scaled term frequencies
    when its `sublinear_tf` keyword argument is set to `True`. Augmented frequencies
    are given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_12.jpg) is the
    greatest frequency of all of the words in document ![Extending bag-of-words with
    TF-IDF weights](img/8365OS_03_13.jpg). scikit-learn 0.15.2 does not implement
    augmented term frequencies, but the output of `CountVectorizer` can be easily
    transformed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalization, logarithmically scaled term frequencies, and augmented term
    frequencies can represent the frequencies of terms in a document while mitigating
    the effects of different document sizes. However, another problem remains with
    these representations. The feature vectors contain large weights for terms that
    occur frequently in a document, even if those terms occur frequently in most documents
    in the corpus. These terms do not help to represent the meaning of a particular
    document relative to the rest of the corpus. For example, most of the documents
    in a corpus of articles about Duke''s basketball team could include the words
    `basketball`, `Coach K`, and `flop`. These words can be thought of as corpus-specific
    stop words and may not be useful to calculate the similarity of documents. The
    **inverse document frequency** (**IDF**) is a measure of how rare or common a
    word is in a corpus. The inverse document frequency is given by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_14.jpg) is
    the total number of documents in the corpus and ![Extending bag-of-words with
    TF-IDF weights](img/8365OS_03_15.jpg) is the number of documents in the corpus
    that contain the term ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_16.jpg).
    A term''s **TF-IDF** value is the product of its term frequency and inverse document
    frequency. `TfidfTransformer` returns TF-IDF''s weight when its `use_idf` keyword
    argument is set to its default value, `True`. Since TF-IDF weighted feature vectors
    are commonly used to represent text, scikit-learn provides a `TfidfVectorizer`
    class that wraps `CountVectorizer` and `TfidfTransformer`. Let''s use `TfidfVectorizer`
    to create TF-IDF weighted feature vectors for our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: By comparing the TF-IDF weights to the raw term frequencies, we can see that
    words that are common to many of the documents in the corpus, such as `sandwich`,
    have been penalized.
  prefs: []
  type: TYPE_NORMAL
- en: Space-efficient feature vectorizing with the hashing trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter''s previous examples, a dictionary containing all of the corpus''s
    unique tokens is used to map a document''s tokens to the elements of a feature
    vector. Creating this dictionary has two drawbacks. First, two passes are required
    over the corpus: the first pass is used to create the dictionary and the second
    pass is used to create feature vectors for the documents. Second, the dictionary
    must be stored in memory, which could be prohibitive for large corpora. It is
    possible to avoid creating this dictionary through applying a hash function to
    the token to determine its index in the feature vector directly. This shortcut
    is called the **hashing trick**. The following example uses `HashingVectorizer`
    to demonstrate the hashing trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The hashing trick is stateless. It can be used to create feature vectors in
    both parallel and online, or streaming, applications because it does not require
    an initial pass over the corpus.. Note that `n_features` is an optional keyword
    argument. Its default value, ![Space-efficient feature vectorizing with the hashing
    trick](img/8365OS_03_17.jpg), is adequate for most problems; it is set to `6`
    here so that the matrix will be small enough to print and still display all of
    the nonzero features. Also, note that some of the term frequencies are negative.
    Since hash collisions are possible, `HashingVectorizer` uses a signed hash function.
    The value of a feature takes the same sign as its token's hash; if the term `cats`
    appears twice in a document and is hashed to `-3`, the fourth element of the document's
    feature vector will be decremented by two. If the term `dogs` also appears twice
    and is hashed to `3`, the fourth element of the feature vector will be incremented
    by two. Using a signed hash function creates the possibility that errors from
    hash collisions will cancel each other out rather than accumulate; a loss of information
    is preferable to a loss of information and the addition of spurious information.
    Another disadvantage of the hashing trick is that the resulting model is more
    difficult to inspect, as the hashing function cannot recall what input token is
    mapped to each element of the feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer vision is the study and design of computational artifacts that process
    and understand images. These artifacts sometimes employ machine learning. An overview
    of computer vision is far beyond the scope of this book, but in this section we
    will review some basic techniques used in computer vision to represent images
    in machine learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from pixel intensities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A digital image is usually a raster, or pixmap, that maps colors to coordinates
    on a grid. An image can be viewed as a matrix in which each element represents
    a color. A basic feature representation for an image can be constructed by reshaping
    the matrix into a vector by concatenating its rows together. **Optical character
    recognition** (**OCR**) is a canonical machine learning problem. Let's use this
    technique to create basic feature representations that could be used in an OCR
    application for recognizing hand-written digits in character-delimited forms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `digits` dataset included with scikit-learn contains grayscale images of
    more than 1,700 hand-written digits between zero and nine. Each image has eight
    pixels on a side. Each pixel is represented by an intensity value between zero
    and 16; white is the most intense and is indicated by zero, and black is the least
    intense and is indicated by 16\. The following figure is an image of a hand-written
    digit taken from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting features from pixel intensities](img/8365OS_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s create a feature vector for the image by reshaping its 8 x 8 matrix
    into a 64-dimensional vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This representation can be effective for some basic tasks, like recognizing
    printed characters. However, recording the intensity of every pixel in the image
    produces prohibitively large feature vectors. A tiny 100 x 100 grayscale image
    would require a 10,000-dimensional vector, and a 1920 x 1080 grayscale image would
    require a 2,073,600-dimensional vector. Unlike the TF-IDF feature vectors we created,
    in most problems these vectors are not sparse. Space-complexity is not the only
    disadvantage of this representation; learning from the intensities of pixels at
    particular locations results in models that are sensitive to changes in the scale,
    rotation, and translation of images. A model trained on our basic feature representations
    might not be able to recognize the same zero if it were shifted a few pixels in
    any direction, enlarged, or rotated a few degrees. Furthermore, learning from
    pixel intensities is itself problematic, as the model can become sensitive to
    changes in illumination. For these reasons, this representation is ineffective
    for tasks that involve photographs or other natural images. Modern computer vision
    applications frequently use either hand-engineered feature extraction methods
    that are applicable to many different problems, or automatically learn features
    without supervision problem using techniques such as deep learning. We will focus
    on the former in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting points of interest as features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The feature vector we created previously represents every pixel in the image;
    all of the informative attributes of the image are represented and all of the
    noisy attributes are represented too. After inspecting the training data, we can
    see that all of the images have a perimeter of white pixels; these pixels are
    not useful features. Humans can quickly recognize many objects without observing
    every attribute of the object. We can recognize a car from the contours of the
    hood without observing the rear-view mirrors, and we can recognize an image of
    a human face from a nose or mouth. This intuition is motivation to create representations
    of only the most informative attributes of an image. These informative attributes,
    or **points of interest**, are points that are surrounded by rich textures and
    can be reproduced despite perturbing the image. **Edges** and **corners** are
    two common types of points of interest. An edge is a boundary at which pixel intensity
    rapidly changes, and a corner is an intersection of two edges. Let''s use scikit-image
    to extract points of interest from the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting points of interest as features](img/8365OS_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The code to extract the points of interest is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The following figure plots the extracted points of interest. Of the image's
    230400 pixels, 466 were extracted as points of interest. This representation is
    much more compact; ideally, there is enough variation proximal to the points of
    interest to reproduce them despite changes in the image's illumination.
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting points of interest as features](img/8365OS_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: SIFT and SURF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scale-Invariant Feature Transform** (**SIFT**) is a method for extracting
    features from an image that is less sensitive to the scale, rotation, and illumination
    of the image than the extraction methods we have previously discussed. Each SIFT
    feature, or descriptor, is a vector that describes edges and corners in a region
    of an image. Unlike the points of interest in our previous example, SIFT also
    captures information about the composition of each point of interest and its surroundings.
    **Speeded-Up Robust Features** (**SURF**) is another method of extracting interesting
    points of an image and creating descriptions that are invariant of the image''s
    scale, orientation, and illumination.SURF can be computed more quickly than SIFT,
    and it is more effective at recognizing features across images that have been
    transformed in certain ways.'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining how SIFT and SURF extraction are implemented is beyond the scope
    of this book. However, with an intuition for how they work, we can still effectively
    use libraries that implement them.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will extract SURF from the following image using the `mahotas`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '![SIFT and SURF](img/8365OS_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Like the extracted points of interest, the extracted SURF are only the first
    step in creating a feature representation that could be used in a machine learning
    task. Different SURF will be extracted for each instance in the training set.
    In [Chapter 6](ch06.html "Chapter 6. Clustering with K-Means"), *Clustering with
    K-Means*, we will cluster extracted SURF to learn features that can be used by
    an image classifier. In the following example we will use `mahotas` to extract
    SURF descriptors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Data standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many estimators perform better when they are trained on standardized data sets.
    Standardized data has **zero mean** and **unit variance**. An explanatory variable
    with zero mean is centered about the origin; its average value is zero. A feature
    vector has unit variance when the variances of its features are all of the same
    order of magnitude. For example, assume that a feature vector encodes two explanatory
    variables. The first values of the first variable range from zero to one. The
    values of the second explanatory variable range from zero to 100,000\. The second
    feature must be scaled to a range closer to {0,1} for the data to have unit variance.
    If a feature''s variance is orders of magnitude greater than the variances of
    the other features, that feature may dominate the learning algorithm and prevent
    it from learning from the other variables. Some learning algorithms also converge
    to the optimal parameter values more slowly when data is not standardized. The
    value of an explanatory variable can be standardized by subtracting the variable''s
    mean and dividing the difference by the variable''s standard deviation. Data can
    be easily standardized using scikit-learn''s `scale` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed feature extraction and developed an understanding
    about the basic techniques for transforming arbitrary data into feature representations
    that can be used by machine learning algorithms. First, we created features from
    categorical explanatory variables using one-hot encoding and scikit-learn''s `DictVectorizer`.Then,
    we discussed the creation of feature vectors for one of the most common types
    of data used in machine learning problems: text. We worked through several variations
    of the bag-of-words model, which discards all syntax and encodes only the frequencies
    of the tokens in a document. We began by creating basic binary term frequencies
    with `CountVectorizer`. You learned to preprocess text by filtering stop words
    and stemming tokens, and you also replaced the term counts in our feature vectors
    with TF-IDF weights that penalize common words and normalize for documents of
    different lengths. Next, we created feature vectors for images. We began with
    an optical character recognition problem in which we represented images of hand-written
    digits with flattened matrices of pixel intensities. This is a computationally
    costly approach. We improved our representations of images by extracting only
    their most interesting points as SURF descriptors.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned to standardize data to ensure that our estimators can learn
    from all of the explanatory variables and can converge as quickly as possible.
    We will use these feature extraction techniques in the subsequent chapters' examples.
    In the next chapter, we will combine the bag-of-words representation with a generalization
    of multiple linear regressions to classify documents.
  prefs: []
  type: TYPE_NORMAL

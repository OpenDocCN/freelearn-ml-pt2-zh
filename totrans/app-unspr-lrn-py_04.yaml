- en: '*Chapter 4*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimension Reduction and PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply dimension reduction techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the concepts behind principal components and dimensionality reduction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply principal component analysis (PCA) when solving problems using scikit-learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare manual PCA versus scikit-learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will look at dimension reduction and different dimension
    reduction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter is the first of a series of three chapters that investigate the
    use of different feature sets (or spaces) in our unsupervised learning algorithms,
    and we will start with a discussion around dimensionality reduction, specifically,
    PCA. We will then extend upon our understanding of the benefits of the different
    feature spaces through an exploration of two independently powerful machine learning
    architectures in neural network-based auto-encoders. Neural networks certainly
    have a well-deserved reputation for being powerful models in supervised learning
    problems, and, through the use of an autoencoder stage, have been shown to be
    sufficiently flexible for their application to unsupervised learning problems.
    Finally, we will build on our neural network implementation and dimensionality
    reduction as we cover t-distributed nearest neighbors in the final chapter of
    this micro-series.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Dimensionality Reduction?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dimensionality reduction is an important tool in any data scientists'' toolkit,
    and, due to its wide variety of use cases, is essentially assumed knowledge within
    the field. So, before we can consider reducing the dimensionality and why we would
    want to reduce it, we must first have a good understanding of what dimensionality
    is. To put it simply, dimensionality is the number of dimensions, features, or
    variables associated with a sample of data. Often, this can be thought of as a
    number of columns in a spreadsheet, where each sample is on a new row, and each
    column describes some attribute of the sample. The following table is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Two samples of data with three different features'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In *Figure 4.1*, we have two samples of data, each with three independent features
    or dimensions. Depending upon the problem being solved, or the origin of this
    dataset, we may want to reduce the number of dimensions per sample without losing
    the provided information. This is where dimensionality reduction can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how exactly can dimensionality reduction help us in solving problems? We
    will cover the applications in more detail in the following section; but let''s
    say that we had a very large dataset of time series data, such as echo-cardiogram
    or ECG (also known as an EKG in some countries) signals as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Electrocardiogram (ECG or EKG)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'These signals were captured from your company''s new model of watch, and we
    need to look for signs of a heart attack or stroke. In looking through the dataset,
    we can make a few observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the individual heartbeats are very similar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is some noise in the data from the recording system or from the patient
    moving during the recording.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the noise, the heartbeat signals are still visible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a lot of data – too much to be able to process using the hardware available
    on the watch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is in such a situation that dimensionality reduction really shines! By using
    dimensionality reduction, we are able to remove much of the noise from the signal,
    which, in turn, will assist with the performance of the algorithms that are applied
    to the data as well as reduce the size of the dataset to allow for reduced hardware
    requirements. The techniques that we are going to discuss in this chapter, in
    particular, PCA and autoencoders, have been well applied in research and industry
    to effectively process, cluster, and classify such datasets. By the end of this
    chapter, you will be able to apply these techniques to your own data and hopefully
    see an increase in the performance of your own machine learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Dimensionality Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we start a detailed investigation of dimensionality reduction and PCA,
    we will discuss some of the common applications for these techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-processing/feature engineering**: One of the most common implementations
    is in the pre-processing or feature engineering stages of developing a machine
    learning solution. The quality of the information provided during the algorithm
    development, as well as the correlation between the input data and the desired
    result, is critical in order for a high-performing solution to be designed. In
    this situation, PCA can provide assistance, as we are able to isolate the most
    important components of information from the data and provide this to the model
    so that only the most relevant information is being provided. This can also have
    a secondary benefit in that we have reduced the number of features being provided
    to the model, so there can be a corresponding reduction in the number of calculations
    to be completed. This can reduce the overall training time for the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise reduction**: Dimensionality reduction can also be used as an effective
    noise reduction/filtering technique. It is expected that the noise within a signal
    or dataset does not comprise a large component of the variation within the data.
    Thus, we can remove some of the noise from the signal by removing the smaller
    components of variation and then restoring the data back to the original dataspace.
    In the following example, the image on the left has been filtered to the first
    20 most significant sources of data, which gives us the image on the right. We
    can see that the quality of the image has reduced, but the critical information
    is still there:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.3: An image filtered with dimensionality reduction. Left: The original
    image (Photo by Arthur Brognoli from Pexels), Right: The filtered image](img/C12626_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: An image filtered with dimensionality reduction. Left: The original
    image (Photo by Arthur Brognoli from Pexels), Right: The filtered image'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This photograph was taken by Arthur Brognoli from Pexels and is available for
    free use under [https://www.pexels.com/photo-license/](https://www.pexels.com/photo-license/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating plausible artificial datasets**: As PCA divides the dataset into
    the components of information (or variation), we can investigate the effects of
    each components or generate new dataset samples by adjusting the ratios between
    the eigenvalues. We can scale these components, which, in effect, increases or
    decreases the importance of that specific component. This is also referred to
    as **statistical shape modelling**, as one common method is to use it to create
    plausible variants of shapes. It is also used detect facial landmarks in images
    in the process of **active shape modelling**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial modelling/risk analysis**: Dimensionality reduction provides a
    useful toolkit for the finance industry, as being able to consolidate a large
    number of individual market metrics or signals into a smaller number of components
    allows for faster, and more efficient computations. Similarly, the components
    can be used to highlight those higher-risk products/companies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Curse of Dimensionality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we can understand the benefits to using dimensionality reduction techniques,
    we must first understand why the dimensionality of feature sets need to be reduced
    at all. The **curse of dimensionality** is a phrase commonly used to describe
    issues that arise when working with data that has a high number of dimensions
    in the feature space; for example, the number of attributes that are collected
    for each sample. Consider a dataset of point locations within a game of *Pac-Man*.
    Your character, Pac-Man, occupies a position within the virtual world defined
    by two dimensions or coordinates (*x*, *y*). Let''s say that we are creating a
    new computer enemy: an AI-driven ghost to play against, and that it requires some
    information regarding our character to make its own game logic decisions. For
    the bot to be effective, we require the player''s position (*x*, *y*) and their
    velocity in each of the directions (*vx*, *vy*) in addition to the players last
    five (*x*, *y*) positions, the number of remaining hearts, and the number of remaining
    power-pellets in the maze (power-pellets temporarily allow Pac-Man to eat ghosts).
    Now, for each moment in time, our bot requires 16 individual features (or dimensions)
    to make its decisions. This is clearly a lot more than just the two dimensions
    as provided by the position.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Dimensions in a PacMan game'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To explain the concept of dimensionality reduction, we will consider a fictional
    dataset (see [*Figure 4.5*](C12626_04_ePub_Final_SZ.xhtml#_idTextAnchor082)) of
    *x* and *y* coordinates as features, giving two dimensions in the feature space.
    It should be noted that this example is by no means a mathematical proof, but
    is rather intended to provide a means of visualizing the effect of increased dimensionality.
    In this dataset, we have six individual samples (or points) and we can visualize
    the currently occupied volume within the feature space of approximately (3 – 1)
    x (4 – 2) = 2 x 2 = 4 squared units.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Data in a 2D feature space'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Suppose, the dataset comprises the same number of points, but with an additional
    feature (the *z* coordinate) to each sample. The occupied data volume is now approximately
    2 x 2 x 2 = 8 cubed units. So, we now have the same number of samples, but the
    space enclosing the dataset is now larger. As such, the data takes up less relative
    volume in the available space and is now sparser. This is the curse of dimensionality;
    as we increase the number of available features, we increase the sparsity of the
    data, and, in turn, make statistically valid correlations more difficult. Looking
    back to our example of creating a video game bot to play against a human player,
    we have 12 features that are a mix of different feature types: speed, velocity,
    acceleration, skill level, selected weapon, and available ammunition. Depending
    on the range of possible values for each of these features and the variance to
    the dataset provided by each feature, the data could be extremely sparse. Even
    within the constrained world of Pac-Man, the potential variance of each of the
    features could be quite large, some much larger than others.'
  prefs: []
  type: TYPE_NORMAL
- en: So, without dealing with the sparsity of the dataset, we have more information
    with the additional feature(s), but may not be able to improve the performance
    of our machine learning model, as the statistical correlations are more difficult.
    What we would like to do is keep the useful information provided by the extra
    features but minimize the negative effect of sparsity. This is exactly what dimensionality
    reduction techniques are designed to do and these can be extremely powerful in
    increasing the performance of your machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will discuss a number of different dimensionality
    reduction techniques and will cover one of the most important and useful methods,
    PCA, in greater detail with a worked example.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Dimensionality Reduction Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed in the Introduction section, the goal of any dimensionality reduction
    technique is to manage the sparsity of the dataset while keeping the useful information
    that is provided, so dimensionality reduction is typically an important pre-processing
    step used before a classification stage. Most dimensionality reduction techniques
    aim to complete this task using a process of **feature projection**, which adjusts
    the data from the higher dimensional space into a space with fewer dimensions
    to remove the sparsity from the data. Again, as a means of visualizing the projection
    process, consider a sphere in a 3D space. We can project the sphere into lower
    2D space into a circle with some information loss (the value for the *z* coordinate)
    but retaining much of the information that describes its original shape. We still
    know the origin, radius, and manifold (outline) of the shape, and it is still
    very clear that it is a circle. So, if we were given just the 2D projection, it
    would also be possible to re-create the original 3D shape with this information.
    So, depending upon the problem that we are trying to solve, we may have reduced
    the dimensionality while retaining the important information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: A projection of a 3D sphere into a 2D space](img/C12626_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: A projection of a 3D sphere into a 2D space'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The secondary benefit that can be obtained by pre-processing the dataset with
    a dimensionality reduction stage is the improved computational performance that
    can be achieved. As the data has been projected into a lower dimensional space,
    it will contain fewer, but potentially more powerful, features. The fact that
    there are fewer features means that, during later classification or regression
    stages, the size of the dataset being processed is significantly smaller. This
    will potentially reduce the required system resources and processing time for
    classification/regression, and, in some cases, the dimensionality reduction technique
    can also be used directly to complete the analysis..
  prefs: []
  type: TYPE_NORMAL
- en: This analogy also introduces one of the important considerations of dimensionality
    reduction. We are always trying to balance the information loss resulting from
    the projection into lower dimensional space with reducing the sparsity of the
    data. Depending upon the nature of the problem and the dataset being used, the
    correct balance could present itself and be relatively straightforward. In some
    applications, this decision may rely on the outcome of additional validation methods,
    such as cross-validation (particularly in supervised learning problems) or the
    assessment of experts in your problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: One way we like to think about this trade-off in dimensionality reduction is
    to consider compressing a file or image on a computer for transfer. Dimensionality
    reduction techniques, such as PCA, are essentially methods of compressing information
    into a smaller size for transfer, and, in many compression methods, some losses
    occur as a result of the compression process. Sometimes, these losses are acceptable;
    if we are transferring a 50 MB image and need to shrink it to 5 MB for transfer,
    we can expect to still be able to see the main subject of the image, but perhaps
    some smaller background features will become too blurry to see. We would also
    not expect to be able to restore the original image to a pixel-perfect representation
    from the compressed version, but we could expect to restore it with some additional
    artefacts, such as blurring.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction and Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dimensionality reduction techniques have many uses in machine learning, as
    the ability to extract the useful information of a dataset can provide performance
    boosts in many machine learning problems. They are particularly useful in unsupervised
    learning as opposed to supervised learning methods, as the dataset does not contain
    any ground truth labels or targets to achieve. In unsupervised learning, the training
    environment is being used to organize the data in a way that is appropriate for
    the problem being solved (for example, clustering in a classification problem),
    which is typically based on the most important information in the dataset. Dimensionality
    reduction provides an effective means of extracting the important information,
    and, as there are a number of different methods that we could use, it is beneficial
    to review some of the available options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Discriminant Analysis** (**LDA**): This is a particularly handy technique
    that can be used for both classification as well as dimensionality reduction.
    LDA will be covered in more detail in *Chapter 7*: *Topic Modeling*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-negative matrix factorization** (**NNMF**): Like many of the dimensionality
    reduction techniques, this relies upon the properties of linear algebra to reduce
    the number of features in the dataset. NNMF will also be covered in more detail
    in *Chapter 7*, *Topic Modeling*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**): This is somewhat related to PCA
    (which is covered in more detail in this chapter) and is also a matrix decomposition
    process not too dissimilar to NNMF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independant Component Analysis** (**ICA**): This also shares some similarities
    to SVD and PCA, but relaxing the assumption of the data being a Gaussian distribution
    allows for non-Gaussian data to be separated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the methods described so far all use linear separation to reduce the
    sparsity of the data in their original implementation. Some of these methods also
    have variants that use non-linear kernel functions in the separation process,
    providing the ability to reduce the sparsity in a non-linear fashion. Depending
    on the dataset being used, a non-linear kernel may be more effective at extracting
    the most useful information from the signal.
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we described previously, PCA is a commonly used and very effective dimensionality
    reduction technique, which often forms a pre-processing stage for a number of
    machine learning models and techniques. For this reason, we will dedicate this
    section of the book to looking at PCA in more detail than any of the other methods.
    PCA reduces the sparsity in the dataset by separating the data into a series of
    components where each component represents a source of information within the
    data. As its name suggests, the first component produced in PCA, **the principal
    component** comprises the majority of information or variance within the data.
    The principal component can often be thought of as contributing the most amount
    of interesting information in addition to the mean. With each subsequent component,
    less information, but more subtlety, is contributed to the compressed data. If
    we consider all of these components together, there will be no benefit from using
    PCA, as the original dataset will be returned. To clarify this process and the
    information returned by PCA, we will use a worked example, completing the PCA
    calculations by hand. But first, we must review some foundational statistical
    concepts, which are required to execute the PCA calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The mean, or the average value, is simply the addition of all values divided
    by the number of values in the set.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Deviation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often referred to as the spread of the data and related to the variance, the
    standard deviation is a measure of how much of the data lies within proximity
    to the mean. In a normally distributed dataset, approximately 68% of the dataset
    lies within one standard deviation of the mean.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between the variance and standard deviation is quite a simple
    one – the variance is the standard deviation squared.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Where standard deviation or variance is the spread of the data calculated on
    a single dimension, the covariance is the variance of one dimension (or feature)
    against another. When the covariance of a dimension is computed against itself,
    the result is the same as simply calculating the variance for the dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A covariance matrix is a matrix representation of the possible covariance values
    that can be computed for a dataset. Other than being particularly useful in data
    exploration, covariance matrices are also required for executing the PCA of a
    dataset. To determine the variance of one feature with respect to another, we
    simply look up the corresponding value in the covariance matrix. Referring to
    *Figure 4.7* we can see that, in column 1, row 2, the value is the variance of
    feature or dataset *Y* with respect to *X* (*cov(Y, X))*. We can also see that
    there is a diagonal column of covariance values computed against the same feature
    or dataset; for example, *cov(X, X)*. In this situation, the value is simply the
    variance of *X*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: The covariance matrix](img/C12626_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: The covariance matrix'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Typically, the exact values of each of the covariances are not as interesting
    as looking at the magnitude and relative size of each of the covariances within
    the matrix. A large value of the covariance of one feature against another would
    suggest that one feature changes significantly with respect to the other, while
    a value close to zero would signify very little change. The other interesting
    aspect of the covariance to look for is the sign associated with the covariance;
    a positive value indicates that as one feature increases or decreases then so
    does the other, while a negative covariance indicates that the two features diverge
    from each other with one increasing as the other decreases or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, `numpy` and `scipy` provide functions to efficiently make these
    calculations for you. In the next exercise, we will compute these values in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11: Understanding the Foundational Concepts of Statistics'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will briefly review how to compute some of the foundational
    statistical concepts using both the `numpy` and `pandas` Python packages. In this
    exercise, we will use dataset of measurements of different Iris flower species,
    created in 1936 by the British biologist and statistician Sir Ronald Fisher. The
    dataset, which can be found in the accompanying source code, comprises four individual
    measurements (sepal width and length, and petal width and length) of three different
    Iris flower varieties: Iris setosa, Iris versicolor, and Iris virginica.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise11](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise11).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `numpy`, and `matplotlib` packages for use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset and preview the first five lines of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8: The head of the data](img/C12626_04_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.8: The head of the data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We only require the `Sepal Length` and `Sepal Width` features, so remove the
    other columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9: The head after cleaning the data](img/C12626_04_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.9: The head after cleaning the data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Visualize the dataset by plotting the `Sepal Length` versus `Sepal Width` values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10: Plot of the data](img/C12626_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.10: Plot of the data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compute the mean value using the `pandas` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the mean value using the `numpy` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the standard deviation value using the `pandas` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the standard deviation value using the `numpy` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the variance values using the `pandas` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the variance values using the `numpy` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the covariance matrix using the `pandas` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.11: Covariance matrix using the Pandas method](img/C12626_04_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.11: Covariance matrix using the Pandas method'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compute the covariance matrix using the `numpy` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.12: The covariance matrix using the NumPy method](img/C12626_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: The covariance matrix using the NumPy method'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we know how to compute the foundational statistic values, we will turn
    our attention to the remaining components of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalues and Eigenvectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The mathematical concept of eigenvalues and eigenvectors is a very important
    one in the fields of physics and engineering, and they also form the final steps
    in computing the principal components of a dataset. The exact mathematical definition
    of eigenvalues and eigenvectors is outside the scope of this book, as it is quite
    involved and requires a reasonable understanding of linear algebra. The linear
    algebra equation to decompose a dataset (*a)* into eigenvalues (*S*) and eigenvectors
    (*U*) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: An eigenvector/eigenvalue decomposition'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In *Figure 4.13*, *U* and *V* are related as the left and right values of dataset
    *a*. If *a* has the shape *m x n*, then *U* will contain values in the shape *m
    x m* and *V* in the shape *n x n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Put simply, in the context of PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eigenvectors** (*U*) are the components contributing information to the dataset
    as described in the first paragraph of this section on principal components. Each
    eigenvector describes some amount of variability within the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eigenvalues** (*S*) are the individual values that describe how much contribution
    each eigenvector provides to the dataset. As we described previously, the signal
    eigenvector that describes the largest contribution is referred to as the principal
    component, and as such, will have the largest eigenvalue. Accordingly, the eigenvector
    with the smallest eigenvalue contributes the least amount of variance or information
    to the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 12: Computing Eigenvalues and Eigenvectors'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed previously, deriving and computing the eigenvalues and eigenvectors
    manually is a little involved and is not in the scope of this book. Thankfully,
    `numpy` provides all the functionality for us to compute these values. Again,
    we will use the Iris dataset for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  prefs: []
  type: TYPE_NORMAL
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise12](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise12).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` and `numpy` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14: The first five rows of the dataset](img/C12626_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.14: The first five rows of the dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Again, we only require the `Sepal Length` and `Sepal Width` features, so remove
    the other columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.15: The Sepal Length and Sepal Width feature](img/C12626_04_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.15: The Sepal Length and Sepal Width feature'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From NumPy''s linear algebra module, use the single value decomposition function
    to compute the `eigenvalues` and `eigenvectors`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The use of the `full_matrices=False` function argument is a flag for the function
    to return the eigenvectors in the shape we need; that is, # Samples x # Features.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Look at the eigenvalues; we can see that the first value is the largest, so
    the first eigenvector contributes the most information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is handy to look at eigenvalues as a percentage of the total variance within
    the dataset. We will use a cumulative sum function to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Divide by the last or maximum value to convert to a percentage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see here that the first (or principal) component comprises 92% of the
    variation within the data, and thus, most of the information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s look at the `eigenvectors`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.16: Eigenvectors'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Confirm that the shape of the eigenvector matrix is in the for `# Samples x
    # Features`; that is, `150` x `2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, from the eigenvalues, we saw that the principal component was the first
    eigenvector. Look at the values for the first eigenvector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have decomposed the dataset down into the principal components, and, using
    the eigenvectors, we can further reduce the dimensionality of the available data.
    In the later examples, we will consider PCA and apply this technique to an example
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Process of PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have all of the pieces ready to complete PCA to reduce the number of
    dimensions in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall algorithm for completing PCA is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required Python packages (`numpy` and `pandas`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the entire dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the available data, select the features that you wish to use in the dimensionality
    reduction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: If there is a significant difference in the scale between the features of the
    dataset; for example, one feature ranges in values between 0 and 1, and another
    between 100 and 1,000, you may need to normalize one of the features, as such
    differences in magnitude can eliminate the effect of the smaller features. In
    such a situation, you may need to divide the larger feature into its maximum value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As an example, have a look at this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`x1 = [0.1, 0.23, 0.54, 0.76, 0.78]`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`x2 = [121, 125, 167, 104, 192]`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`x2 = x2 / np.max(x2) # Normalise x2 to be between 0 and 1`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the `covariance` matrix of the selected (and possibly normalized) data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the eigenvalues and eigenvectors of the `covariance` matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues (and corresponding eigenvectors) from highest to lowest
    value eigenvalue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the eigenvalues as a percentage of the total variance within the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the number of eigenvalues (and corresponding eigenvectors) required to
    comprise a pre-determined value of a minimum composition variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: At this stage, the sorted eigenvalues represent a percentage of the total variance
    within the dataset. As such, we can use these values to select the number of eigenvectors
    required, either for the problem being solved or to sufficiently reduce the size
    of the dataset being applied in the model. For example, say that we required at
    least 90% of the variance to be accounted for within the output of PCA. We would
    then select the number of eigenvalues (and corresponding eigenvectors) that comprise
    at least 90% of the variance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Multiply the dataset by the selected eigenvectors and you have completed a PCA,
    reducing the number of features representing the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before moving on to the next exercise, note that **transpose** is a term from
    linear algebra that means to swap the rows with the columns and vice versa. Say
    we had a matrix of ![](img/C12626_04_Formula_01.png), then the transpose of *X*
    would be ![](img/C12626_04_Formula_02.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13: Manually Executing PCA'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this exercise, we will be completing PCA manually, again using the Iris
    dataset. For this example, we want to sufficiently reduce the number of dimensions
    within the dataset to comprise at least 75% of the available variance:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise13](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise13).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` and `numpy` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.17: The first five rows of the dataset](img/C12626_04_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.17: The first five rows of the dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Again, we only require the `Sepal Length` and `Sepal Width` features, so remove
    the other columns. In this example, we are not normalizing the selected dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.18: The sepal length and sepal width feature](img/C12626_04_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.18: The sepal length and sepal width feature'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compute the `covariance` matrix for the selected data. Note that we need to
    take the transpose of the `covariance` matrix to ensure that it is based on the
    number of features (2) and not samples (150):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_04_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.19: The covariance matrix for the selected data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compute the eigenvectors and eigenvalues for the covariance matrix, Again,
    use the `full_matrices` function argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'What are the eigenvalues? These are returned sorted from the highest to lowest
    value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What are the corresponding eigenvectors?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_04_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.20: Eigenvectors'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compute the eigenvalues as a percentage of the variance within the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As per the introduction to the exercise, we need to describe the data with at
    least 75% of the available variance. As per *Step* [*7*](C12626_04_ePub_Final_SZ.xhtml#_idTextAnchor097),
    the principal component comprises 78% of the available variance. As such, we require
    only the principal component from the dataset. What are the principal components?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we can apply the dimensionality reduction process. Execute a matrix multiplication
    of the principal component with the transpose of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The dimensionality reduction process is a matrix multiplication of the selected
    eigenvectors and the data to be transformed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Without taking the transpose of the `df.values` matrix, multiplication could
    not occur:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_04_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.21: The result of matrix multiplication'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The transpose of the dataset is required to execute matrix multiplication, as
    the **inner dimensions of the matrix must be the same** for matrix multiplication
    to occur. For **A.dot(B)** to be valid, **A** must have the shape *m x n* and
    **B** must have the shape *n x p*. In this example, the inner dimensions of **A**
    and **B** are both *n*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the following example, the output of the PCA is a single-column, 150-sample
    dataset. As such, we have just reduced the size of the initial dataset by half,
    comprising approximately 79% of the variance within the data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.22: The output of PCA](img/C12626_04_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.22: The output of PCA'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the values of the principal component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.23: The Iris dataset transformed by using a manual PCA](img/C12626_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: The Iris dataset transformed by using a manual PCA'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this exercise, we simply computed the covariance matrix of the dataset without
    applying any transformations to the dataset beforehand. If the two features have
    roughly the same mean and standard deviation, this is perfectly fine. However,
    if one feature is much larger in value (and has a somewhat different mean) than
    the other, then this feature may dominate the other when decomposing into components.
    This could have the effect of removing the information provided by the smaller
    feature altogether. One simple normalization technique before computing the covariance
    matrix would be to subtract the respective means from the features, thus centering
    the dataset around zero. We will demonstrate this in *Exercise 15*, *Visualizing
    Variance Reduction with Manual PCA*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14: Scikit-Learn PCA'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Typically, we will not complete PCA manually, especially when scikit-learn
    provides an optimized API with convenient methods that will allow us to easily
    transform the data to and from the reduced-dimensional space. In this exercise,
    we will look at using a scikit-learn PCA on the Iris dataset in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  prefs: []
  type: TYPE_NORMAL
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise14](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise14).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `numpy`, and `PCA` modules from the `sklearn` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.24: The first five rows of the dataset](img/C12626_04_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.24: The first five rows of the dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Again, we only require the `Sepal Length` and `Sepal Width` features, so remove
    the other columns. In this example, we are not normalizing the selected dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.25: The Sepal Length and Sepal Width features](img/C12626_04_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.25: The Sepal Length and Sepal Width features'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fit the data to a scikit-learn PCA model of the covariance data. Using the
    default values, as we have here, produces the maximum number of eigenvalues and
    eigenvectors possible for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.26: Fitting data to a PCA model](img/C12626_04_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.26: Fitting data to a PCA model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, `copy` indicates that the data fit within the model is copied before any
    calculations are applied. `iterated_power` shows that the `Sepal Length` and `Sepal
    Width` featuresis the number of principal components to keep. The default value
    is `None`, which selects the number of components as one less than the minimum
    of either the number of samples or number of features. `random_state` allows the
    user to specify a seed for the random number generator used by the SVD solver.
    `svd_solver` specifies the SVD solver to be used during PCA. `tol` is the tolerance
    values used by the SVD solver. With `whiten`, the component vectors are multiplied
    by the square root of the number of samples. This will remove some information,
    but can improve the performance of some downstream estimators.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The percentage of variance described by the components (eigenvalues) is contained
    within the `explained_variance_ratio_` property. Display the values for `explained_variance_ratio_`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the eigenvectors via the `components_` property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.27: Eigenvectors](img/C12626_04_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.27: Eigenvectors'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this exercise, we will again only use the primary component, so we will
    create a new `PCA` model, this time specifying the number of components (eigenvectors/eigenvalues)
    to be `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `fit` method to fit the `covariance` matrix to the `PCA` model and
    generate the corresponding eigenvalues/eigenvectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.28: The maximum number of eigenvalues and eigenvectors](img/C12626_04_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.28: The maximum number of eigenvalues and eigenvectors'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The model is fitted using a number of default parameters, as listed in the preceding
    output. `copy = True` is the data provided to the `fit` method, which is copied
    before PCA is applied. `iterated_power='auto'` is used to define the number of
    iterations by the internal SVD solver. `n_components=1` specifies that the PCA
    model is to return only the principal component. `random_state=None` specifies
    the random number generator to be used by the internal SVD solver if required.
    `svd_solver='auto'` is the type of SVD solver used. `tol=0.0` is the tolerance
    value for the SVD solver to deem converged. `whiten=False` specifies that the
    eigenvectors are not to be modified. If set to `True`, whitening further modifies
    the components by multiplying by the square root of the number of samples and
    dividing by the singular values. This can help to improve the performance of later
    algorithm steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Typically, you will not need to worry about adjusting any of these parameters,
    other than the number of components (`n_components`), which you can pass to the
    `fit` method, for example, `model.fit(data, n_components=2)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the eigenvectors using the `components_` property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Transform the Iris dataset into the lower space by using the `fit_transform`
    method of the model on the dataset. Assign the transformed values to the `data_t`
    variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the transformed values to visualize the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.29: The Iris dataset transformed using the scikit-learn PCA](img/C12626_04_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.29: The Iris dataset transformed using the scikit-learn PCA'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You have just reduced the dimensionality of the Iris dataset
    using manual PCA, as well as the scikit-learn API. But before we celebrate too
    early, compare Figure 4.23 and Figure 4.29; these plots should be identical, shouldn't
    they? We used two separate methods to complete a PCA on the same dataset and selected
    the principal component for both. In the next activity, we will investigate why
    there are differences between the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Manual PCA versus scikit-learn'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose that you have been asked to port some legacy code from an older application
    executing PCA manually, to a newer application that uses scikit-learn. During
    the porting process, you notice some differences between the output of the manual
    PCA versus your port. Why is there a difference between the output of our manual
    PCA and scikit-learn? Compare the results of the two approaches on the Iris dataset.
    What are the differences between them?
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity06](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity06).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `pandas`, `numpy`, and `matplotlib` plotting libraries and the scikit-learn
    `PCA` model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the dataset and select only the sepal features as per the previous exercises.
    Display the first five rows of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the `covariance` matrix for the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the data using the scikit-learn API and only the first principal component.
    Store the transformed data in the `sklearn_pca` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the data using the manual PCA and only the first principal component.
    Store the transformed data in the `manual_pca` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the `sklearn_pca` and `manual_pca` values on the same plot to visualize
    the difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice that the two plots look almost identical, but with some key differences.
    What are these differences?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See whether you can modify the output of the manual PCA process to bring it
    in line with the scikit-learn version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: As a hint, the scikit-learn API subtracts the mean of the data prior to the
    transform.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Expected output: By the end of this activity, you will have transformed the
    dataset using both the manual and scikit-learn PCA methods. You will have produced
    a plot demonstrating that the two reduced datasets are, in fact, identical, and
    you should have an understanding of why they initially looked quite different.
    The final plot should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.30: The expected final plot](img/C12626_04_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.30: The expected final plot'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This plot will demonstrate that the dimensionality reduction completed by the
    two methods are, in fact, the same.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 324.
  prefs: []
  type: TYPE_NORMAL
- en: Restoring the Compressed Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have covered a few different examples of transforming a dataset
    into a lower-dimensional space, we should consider what practical effect this
    transformation has had on the data. Using PCA as a pre-processing step to condense
    the number of features in the data will result in some of the variance being discarded.
    The following exercise will walk us through this process so that we can see how
    much information has been discarded by the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15: Visualizing Variance Reduction with Manual PCA'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most important aspects of dimensionality reduction is understanding
    how much information has been removed from the dataset as a result of the dimensionality
    reduction process. Removing too much information will add additional challenges
    to later processing, while not removing enough defeats the purpose of PCA or other
    techniques. In this exercise, we will visualize the amount of information that
    has been removed from the Iris dataset as a result of PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  prefs: []
  type: TYPE_NORMAL
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise15](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise15).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `numpy`, and `matplotlib` plotting libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the `Sepal` features from the Iris dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.31: Sepal features](img/C12626_04_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.31: Sepal features'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Centre the dataset around zero by subtracting the respective means:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To calculate the data and print the results, use the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.32: Section of the output](img/C12626_04_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.32: Section of the output'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use manual PCA to transform the data on the basis of the first principal component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the data into the lower-dimensional space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the principal components for later use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To compute the inverse transform of the reduced dataset, we need to restore
    the selected eigenvectors into the higher-dimensional space. To do this, we will
    invert the matrix. Matrix inversion is another linear algebra technique that we
    will only cover very briefly. A square matrix, *A*, is said to be invertible if
    there exists another square matrix, *B*, and if *AB=BA=I*, where *I* is a special
    matrix known as an identity matrix, consisting of values of `1` only through the
    center diagonal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the transformed data for use in the matrix multiplication:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the inverse transform of the reduced data and plot the result to visualize
    the effect of removing the variance from the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.33: The inverse transform of the reduced data](img/C12626_04_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.33: The inverse transform of the reduced data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Add the `means` back to the transformed data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the result by plotting the original dataset and the transformed dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.34: The inverse transform after removing variance](img/C12626_04_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.34: The inverse transform after removing variance'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are only two components of variation in this dataset. If we do not remove
    any of the components, what will be the result of the inverse transform? Again,
    transform the data into the lower-dimensional space, but this time, use all of
    the eigenvectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transpose `data_transformed` to put it into the correct shape for matrix multiplication:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, restore the data back to the higher-dimensional space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.35: The restored data](img/C12626_04_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.35: The restored data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Add the means back to the restored data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the restored data in the context of the original dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_04_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.36: The inverse transform after removing the variance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we compare the two plots produced in this exercise, we can see that the PCA
    reduced, and the restored dataset is essentially a negative linear trend line
    between the two feature sets. We can compare this to the dataset restored from
    all available components, where we have recreated the original dataset as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 16: Visualizing Variance Reduction with'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will again visualize the effect of reducing the dimensionality
    of the dataset; however, this time, we will be using the scikit-learn API. This
    is this method that you will commonly use in practical applications, due to the
    power and simplicity of the scikit-learn model:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise16](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise16).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `numpy`, and `matplotlib` plotting libraries and the `PCA`
    model from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the `Sepal` features from the Iris dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.37: The Sepal features from the Iris dataset](img/C12626_04_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.37: The Sepal features from the Iris dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use the scitkit-learn API to transform the data on the basis of the first principal
    component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the inverse transform of the reduced data and plot the result to visualize
    the effect of removing the variance from the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.38: The inverse transform after removing the variance](img/C12626_04_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.38: The inverse transform after removing the variance'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: There are only two components of variation in this dataset. If we do not remove
    any of the components, what will the result of the inverse transform be?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.39: The inverse transform after removing the variance](img/C12626_04_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.39: The inverse transform after removing the variance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Again, we have demonstrated the effect of removing information from the dataset
    and the ability to recreate the original data using all the available eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: The previous exercises specified the reduction of the dimensionality using PCA
    to two dimensions, partly to allow the results to be easily visualized. We can,
    however, use PCA to reduce the dimensions to any value less than that of the original
    set. The following example demonstrates how PCA can be used to reduce a dataset
    to three dimensions, allowing visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 17: Plotting 3D Plots in Matplotlib'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating 3D scatter plots in matplotlib are unfortunately not as simple as
    providing a series of (*x*, *y*, *z*)coordinates to a scatter plot. In this exercise
    we will work through a simple 3D plotting example, using the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  prefs: []
  type: TYPE_NORMAL
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise17](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Exercise17).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and `matplotlib`. To enable 3D plotting, you will also need
    to import `Axes3D`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Read in the dataset and select the `Sepal Length`, `Sepal Width`, and `Petal
    Width` columns
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.40: The first five rows of the data](img/C12626_04_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.40: The first five rows of the data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the data in three dimensions and use the `projection=''3d''` argument
    with the `add_subplot` method to create the 3D plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot will look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.41: The expanded Iris dataset](img/C12626_04_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.41: The expanded Iris dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While the Axes3D was imported but not directly used, it is required for configuring
    the plot window in three dimensions. If the import of Axes3D was omitted, the
    `projection='3d'` argument would return an `AttributeError`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7: PCA Using the Expanded Iris Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we are going to use the complete Iris dataset to look at
    the effect of selecting a differing number of components in the PCA decomposition.
    This activity aims to simulate the process that is typically completed in a real-world
    problem as we try to determine the optimum number of components to select, attempting
    to balance the extent of dimensionality reduction and information loss. Henceforth,
    we will be using the scikit-learn PCA model:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  prefs: []
  type: TYPE_NORMAL
- en: It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity07](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson04/Activity07).
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pandas` and `matplotlib`. To enable 3D plotting, you will also need
    to import `Axes3D`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read in the dataset and select the `Sepal Length`, `Sepal Width`, and `Petal
    Width` columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the data in three dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `PCA` model without specifying the number of components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the eigenvalues or `explained_variance_ratio_`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We want to reduce the dimensionality of the dataset but still keep at least
    90% of the variance. What are the minimum number of components required to keep
    90% of the variance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new `PCA` model, this time specifying the number of components required
    to keep at least 90% of the variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the data using the new model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the transformed data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore the transformed data to the original dataspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Plot the restored data in three dimensions in one subplot and the original
    data in a second subplot to visualize the effect of removing some of the variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expected Output: The final plot will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.42: Expected plots](img/C12626_04_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.42: Expected plots'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 328.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we covered the process of dimensionality reduction and PCA.
    We completed a number of exercises and developed the skills to reduce the size
    of a dataset by extracting only the most important components of variance within
    the data, using both a manual PCA process and the model provided by scikit-learn.
    During this chapter, we also returned the reduced datasets back to the original
    dataspace and observed the effect of removing the variance on the original data.
    Finally, we discussed a number of potential applications for PCA and other dimensionality
    reduction processes. In our next chapter, we will introduce neural network-based
    autoencoders and use the Keras package to implement them.
  prefs: []
  type: TYPE_NORMAL

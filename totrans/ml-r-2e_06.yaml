- en: Chapter 6. Forecasting Numeric Data – Regression Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mathematical relationships help us to understand many aspects of everyday life.
    For example, body weight is a function of one's calorie intake, income is often
    related to education and job experience, and poll numbers help us estimate a presidential
    candidate's odds of being re-elected.
  prefs: []
  type: TYPE_NORMAL
- en: When such relationships are expressed with exact numbers, we gain additional
    clarity. For example, an additional 250 kilocalories consumed daily may result
    in nearly a kilogram of weight gain per month; each year of job experience may
    be worth an additional $1,000 in yearly salary; and a president is more likely
    to be re-elected when the economy is strong. Obviously, these equations do not
    perfectly fit every situation, but we expect that they are reasonably correct,
    on average.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter extends our machine learning toolkit by going beyond the classification
    methods covered previously and introducing techniques for estimating relationships
    among numeric data. While examining several real-world numeric prediction tasks,
    you will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic statistical principles used in regression, a technique that models
    the size and the strength of numeric relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to prepare data for regression analysis, and estimate and interpret a regression
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pair of hybrid techniques known as regression trees and model trees, which
    adapt decision tree classifiers for numeric prediction tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on a large body of work in the field of statistics, the methods used in
    this chapter are a bit heavier on math than those covered previously, but don't
    worry! Even if your algebra skills are a bit rusty, R takes care of the heavy
    lifting.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is concerned with specifying the relationship between a single numeric
    **dependent variable** (the value to be predicted) and one or more numeric **independent
    variables** (the predictors). As the name implies, the dependent variable depends
    upon the value of the independent variable or variables. The simplest forms of
    regression assume that the relationship between the independent and dependent
    variables follows a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The origin of the term "regression" to describe the process of fitting lines
    to data is rooted in a study of genetics by Sir Francis Galton in the late 19th
    century. He discovered that fathers who were extremely short or extremely tall
    tended to have sons whose heights were closer to the average height. He called
    this phenomenon "regression to the mean".
  prefs: []
  type: TYPE_NORMAL
- en: You might recall from basic algebra that lines can be defined in a **slope-intercept
    form** similar to *y = a + bx*. In this form, the letter *y* indicates the dependent
    variable and *x* indicates the independent variable. The **slope** term *b* specifies
    how much the line rises for each increase in *x*. Positive values define lines
    that slope upward while negative values define lines that slope downward. The
    term *a* is known as the **intercept** because it specifies the point where the
    line crosses, or intercepts, the vertical *y* axis. It indicates the value of
    *y* when *x = 0*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding regression](img/B03905_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Regression equations model data using a similar slope-intercept format. The
    machine's job is to identify values of *a* and *b* so that the specified line
    is best able to relate the supplied *x* values to the values of *y*. There may
    not always be a single function that perfectly relates the values, so the machine
    must also have some way to quantify the margin of error. We'll discuss this in
    depth shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regression analysis is commonly used for modeling complex relationships among
    data elements, estimating the impact of a treatment on an outcome, and extrapolating
    into the future. Although it can be applied to nearly any task, some specific
    use cases include:'
  prefs: []
  type: TYPE_NORMAL
- en: Examining how populations and individuals vary by their measured characteristics,
    for use in scientific research across fields as diverse as economics, sociology,
    psychology, physics, and ecology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantifying the causal relationship between an event and the response, such
    as those in clinical drug trials, engineering safety tests, or marketing research
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying patterns that can be used to forecast future behavior given known
    criteria, such as predicting insurance claims, natural disaster damage, election
    results, and crime rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression methods are also used for **statistical hypothesis testing**, which
    determines whether a premise is likely to be true or false in light of the observed
    data. The regression model's estimates of the strength and consistency of a relationship
    provide information that can be used to assess whether the observations are due
    to chance alone.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hypothesis testing is extremely nuanced and falls outside the scope of machine
    learning. If you are interested in this topic, an introductory statistics textbook
    is a good place to get started.
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis is not synonymous with a single algorithm. Rather, it is
    an umbrella for a large number of methods that can be adapted to nearly any machine
    learning task. If you were limited to choosing only a single method, regression
    would be a good choice. One could devote an entire career to nothing else and
    perhaps still have much to learn.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll focus only on the most basic **linear regression** models—those
    that use straight lines. When there is only a single independent variable it is
    known as **simple linear regression**. In the case of two or more independent
    variables, this is known as **multiple linear regression**, or simply "multiple
    regression". Both of these techniques assume that the dependent variable is measured
    on a continuous scale.
  prefs: []
  type: TYPE_NORMAL
- en: Regression can also be used for other types of dependent variables and even
    for some classification tasks. For instance, **logistic regression** is used to
    model a binary categorical outcome, while **Poisson regression**—named after the
    French mathematician Siméon Poisson—models integer count data. The method known
    as **multinomial logistic regression** models a categorical outcome; thus, it
    can be used for classification. The same basic principles apply across all the
    regression methods, so after understanding the linear case, it is fairly simple
    to learn the others.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many of the specialized regression methods fall into a class of **Generalized
    Linear Models** (**GLM**). Using a GLM, linear models can be generalized to other
    patterns via the use of a **link function**, which specifies more complex forms
    for the relationship between *x* and *y*. This allows regression to be applied
    to almost any type of data.
  prefs: []
  type: TYPE_NORMAL
- en: We'll begin with the basic case of simple linear regression. Despite the name,
    this method is not too simple to address complex problems. In the next section,
    we'll see how the use of a simple linear regression model might have averted a
    tragic engineering disaster.
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On January 28, 1986, seven crew members of the United States space shuttle *Challenger*
    were killed when a rocket booster failed, causing a catastrophic disintegration.
    In the aftermath, experts focused on the launch temperature as a potential culprit.
    The rubber O-rings responsible for sealing the rocket joints had never been tested
    below 40ºF (4ºC) and the weather on the launch day was unusually cold and below
    freezing.
  prefs: []
  type: TYPE_NORMAL
- en: With the benefit of hindsight, the accident has been a case study for the importance
    of data analysis and visualization. Although it is unclear what information was
    available to the rocket engineers and decision makers leading up to the launch,
    it is undeniable that better data, utilized carefully, might very well have averted
    this disaster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section''s analysis is based on data presented in Dalal SR, Fowlkes EB,
    Hoadley B. *Risk analysis of the space shuttle: pre-Challenger prediction of failure*.
    Journal of the American Statistical Association. 1989; 84:945-957\. For one perspective
    on how data may have changed the result, see Tufte ER. *Visual Explanations: Images
    and Quantities, Evidence and Narrative*. Graphics Press; 1997\. For a counterpoint,
    see Robison W, Boisioly R, Hoeker D, Young, S. *Representation and misrepresentation:
    Tufte and the Morton Thiokol engineers on the Challenger*. Science and Engineering
    Ethics. 2002; 8:59-81.'
  prefs: []
  type: TYPE_NORMAL
- en: The rocket engineers almost certainly knew that cold temperatures could make
    the components more brittle and less able to seal properly, which would result
    in a higher chance of a dangerous fuel leak. However, given the political pressure
    to continue with the launch, they needed data to support this hypothesis. A regression
    model that demonstrated a link between temperature and O-ring failure, and could
    forecast the chance of failure given the expected temperature at launch, might
    have been very helpful.
  prefs: []
  type: TYPE_NORMAL
- en: To build the regression model, scientists might have used the data on launch
    temperature and component distresses from 23 previous successful shuttle launches.
    A component distress indicates one of the two types of problems. The first problem,
    called erosion, occurs when excessive heat burns up the O-ring. The second problem,
    called blowby, occurs when hot gases leak through or "blow by" a poorly sealed
    O-ring. Since the shuttle has a total of six primary O-rings, up to six distresses
    can occur per flight. Though the rocket can survive one or more distress events,
    or fail with as few as one, each additional distress increases the probability
    of a catastrophic failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following scatterplot shows a plot of primary O-ring distresses detected
    for the previous 23 launches, as compared to the temperature at launch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple linear regression](img/B03905_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Examining the plot, there is an apparent trend. Launches occurring at higher
    temperatures tend to have fewer O-ring distress events.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the coldest launch (53º F) had two distress events, a level which
    had only been reached in one other launch. With this information at hand, the
    fact that the Challenger was scheduled to launch at a temperature over 20 degrees
    colder seems concerning. But exactly how concerned should we be? To answer this
    question, we can turn to simple linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple linear regression model defines the relationship between a dependent
    variable and a single independent predictor variable using a line defined by an
    equation in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple linear regression](img/B03905_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Don't be alarmed by the Greek characters, this equation can still be understood
    using the slope-intercept form described previously. The intercept, *α* (alpha),
    describes where the line crosses the *y* axis, while the slope, *β* (beta), describes
    the change in *y* given an increase of *x*. For the shuttle launch data, the slope
    would tell us the expected reduction in the number of O-ring failures for each
    degree the launch temperature increases.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Greek characters are often used in the field of statistics to indicate variables
    that are parameters of a statistical function. Therefore, performing a regression
    analysis involves finding **parameter estimates** for *α* and *β*. The parameter
    estimates for alpha and beta are often denoted using *a* and *b*, although you
    may find that some of this terminology and notation is used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we know that the estimated regression parameters in the equation for
    the shuttle launch data are: *a = 3.70* and *b = -0.048*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the full linear equation is *y = 3.70 – 0.048x*. Ignoring for a moment
    how these numbers were obtained, we can plot the line on the scatterplot like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple linear regression](img/B03905_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As the line shows, at 60 degrees Fahrenheit, we predict just under one O-ring
    distress. At 70 degrees Fahrenheit, we expect around 0.3 failures. If we extrapolate
    our model, all the way to 31 degrees—the forecasted temperature for the Challenger
    launch—we would expect about *3.70 - 0.048 * 31 = 2.21* O-ring distress events.
    Assuming that each O-ring failure is equally likely to cause a catastrophic fuel
    leak means that the Challenger launch at 31 degrees was nearly three times more
    risky than the typical launch at 60 degrees, and over eight times more risky than
    a launch at 70 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the line doesn't pass through each data point exactly. Instead,
    it cuts through the data somewhat evenly, with some predictions lower or higher
    than the line. In the next section, we will learn about why this particular line
    was chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinary least squares estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to determine the optimal estimates of *α* and *β*, an estimation method
    known as **Ordinary Least Squares** (**OLS**) was used. In OLS regression, the
    slope and intercept are chosen so that they minimize the sum of the squared errors,
    that is, the vertical distance between the predicted *y* value and the actual
    *y* value. These errors are known as **residuals**, and are illustrated for several
    points in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares estimation](img/B03905_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In mathematical terms, the goal of OLS regression can be expressed as the task
    of minimizing the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares estimation](img/B03905_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In plain language, this equation defines *e* (the error) as the difference between
    the actual *y* value and the predicted *y* value. The error values are squared
    and summed across all the points in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The caret character (`^`) above the *y* term is a commonly used feature of statistical
    notation. It indicates that the term is an estimate for the true *y* value. This
    is referred to as the *y*-hat, and is pronounced exactly like the hat you'd wear
    on your head.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution for *a* depends on the value of *b*. It can be obtained using
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares estimation](img/B03905_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand these equations, you'll need to know another bit of statistical
    notation. The horizontal bar appearing over the *x* and *y* terms indicates the
    mean value of *x* or *y*. This is referred to as the *x*-bar or *y*-bar, and is
    pronounced just like the establishment you'd go to for an alcoholic drink.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the proof is beyond the scope of this book, it can be shown using calculus
    that the value of *b* that results in the minimum squared error is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares estimation](img/B03905_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we break this equation apart into its component pieces, we can simplify
    it a bit. The denominator for *b* should look familiar; it is very similar to
    the variance of *x*, which is denoted as *Var(x)*. As we learned in [Chapter 2](ch02.html
    "Chapter 2. Managing and Understanding Data"), *Managing and Understanding Data*,
    the variance involves finding the average squared deviation from the mean of *x*.
    This can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares estimation](img/B03905_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The numerator involves taking the sum of each data point''s deviation from
    the mean *x* value multiplied by that point''s deviation away from the mean *y*
    value. This is similar to the **covariance** function for *x* and *y*, denoted
    as *Cov(x, y)*. The covariance formula is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares estimation](img/B03905_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we divide the covariance function by the variance function, the *n* terms
    get cancelled and we can rewrite the formula for *b* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares estimation](img/B03905_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Given this restatement, it is easy to calculate the value of *b* using built-in
    R functions. Let's apply it to the rocket launch data to estimate the regression
    line.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you would like to follow along with these examples, download the `challenger.csv`
    file from the Packt Publishing website and load to a data frame using the `launch
    <- read.csv("challenger.csv")` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that our shuttle launch data is stored in a data frame named `launch`,
    the independent variable *x* is temperature, and the dependent variable *y* is
    `distress_ct`. We can then use R''s `cov()` and `var()` functions to estimate
    *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'From here we can estimate *a* using the `mean()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Estimating the regression equation by hand is not ideal, so R provides functions
    for performing this calculation automatically. We will use such methods shortly.
    First, we will expand our understanding of regression by learning a method for
    measuring the strength of a linear relationship, and then we will see how linear
    regression can be applied to data having more than one independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Correlations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **correlation** between two variables is a number that indicates how closely
    their relationship follows a straight line. Without additional qualification,
    correlation typically refers to **Pearson's correlation coefficient**, which was
    developed by the 20th century mathematician Karl Pearson. The correlation ranges
    between -1 and +1\. The extreme values indicate a perfectly linear relationship,
    while a correlation close to zero indicates the absence of a linear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following formula defines Pearson''s correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlations](img/B03905_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More Greek notation has been introduced here. The first symbol (which looks
    like a lowercase p) is *rho*, and it is used to denote the Pearson correlation
    statistic. The characters that look like q turned sideways are the Greek letter
    *sigma*, and they indicate the standard deviation of *x* or *y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this formula, we can calculate the correlation between the launch temperature
    and the number of O-ring distress events. Recall that the covariance function
    is `cov()` and the standard deviation function is `sd()`. We''ll store the result
    in `r`, a letter that is commonly used to indicate the estimated correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can use R''s correlation function, `cor()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The correlation between the temperature and the number of distressed O-rings
    is -0.51\. The negative correlation implies that increases in temperature are
    related to decreases in the number of distressed O-rings. To the NASA engineers
    studying the O-ring data, this would have been a very clear indicator that a low
    temperature launch could be problematic. The correlation also tells us about the
    relative strength of the relationship between temperature and O-ring distress.
    Because -0.51 is halfway to the maximum negative correlation of -1, this implies
    that there is a moderately strong negative linear association.
  prefs: []
  type: TYPE_NORMAL
- en: There are various rules of thumb used to interpret correlation strength. One
    method assigns a status of "weak" to values between 0.1 and 0.3, "moderate" to
    the range of 0.3 to 0.5, and "strong" to values above 0.5 (these also apply to
    similar ranges of negative correlations). However, these thresholds may be too
    lax for some purposes. Often, the correlation must be interpreted in context.
    For data involving human beings, a correlation of 0.5 may be considered extremely
    high, while for data generated by mechanical processes, a correlation of 0.5 may
    be weak.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You have probably heard the expression "correlation does not imply causation."
    This is rooted in the fact that a correlation only describes the association between
    a pair of variables, yet there could be other unmeasured explanations. For example,
    there may be a strong association between mortality and time per day spent matching
    movies, but before doctors should start recommending that we all watch more movies,
    we need to rule out another explanation—younger people watch more movies and are
    less likely to die.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the correlation between two variables gives us a way to quickly gauge
    the relationships among the independent and dependent variables. This will be
    increasingly important as we start defining the regression models with a larger
    number of predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most real-world analyses have more than one independent variable. Therefore,
    it is likely that you will be using **multiple linear regression** for most numeric
    prediction tasks. The strengths and weaknesses of multiple linear regression are
    shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: By far the most common approach for modeling numeric data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be adapted to model almost any modeling task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides estimates of both the strength and size of the relationships among
    features and the outcome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Makes strong assumptions about the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model's form must be specified by the user in advance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not handle missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only works with numeric features, so categorical data requires extra processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires some knowledge of statistics to understand the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: We can understand multiple regression as an extension of simple linear regression.
    The goal in both cases is similar—find values of beta coefficients that minimize
    the prediction error of a linear equation. The key difference is that there are
    additional terms for additional independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple regression equations generally follow the form of the following equation.
    The dependent variable *y* is specified as the sum of an intercept term *α* plus
    the product of the estimated *β* value and the *x* values for each of the *i*
    features. An error term (denoted by the Greek letter *epsilon*) has been added
    here as a reminder that the predictions are not perfect. This represents the **residual**
    term noted previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/B03905_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's consider for a moment the interpretation of the estimated regression parameters.
    You will note that in the preceding equation, a coefficient is provided for each
    feature. This allows each feature to have a separate estimated effect on the value
    of *y*. In other words, *y* changes by the amount *β[i]* for each unit increase
    in *x[i]*. The intercept *α* is then the expected value of *y* when the independent
    variables are all zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the intercept term *α* is really no different than any other regression
    parameter, it is also sometimes denoted as *β[0]* (pronounced beta-naught), as
    shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/B03905_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Just like before, the intercept is unrelated to any of the independent *x*
    variables. However, for reasons that will become clear shortly, it helps to imagine
    *β[0]* as if it were being multiplied by a term *x[0]*, which is a constant with
    the value 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/B03905_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to estimate the values of the regression parameters, each observed
    value of the dependent variable *y* must be related to the observed values of
    the independent *x* variables using the regression equation in the previous form.
    The following figure illustrates this structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/B03905_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The many rows and columns of data illustrated in the preceding figure can be
    described in a condensed formulation using bold font **matrix notation** to indicate
    that each of the terms represents multiple values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/B03905_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The dependent variable is now a vector, **Y**, with a row for every example.
    The independent variables have been combined into a matrix, **X**, with a column
    for each feature plus an additional column of '1' values for the intercept term.
    Each column has a row for every example. The regression coefficients **β** and
    residual errors **ε** are also now vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is now to solve for **β**, the vector of regression coefficients that
    minimizes the sum of the squared errors between the predicted and actual **Y**
    values. Finding the optimal solution requires the use of matrix algebra; therefore,
    the derivation deserves more careful attention than can be provided in this text.
    However, if you''re willing to trust the work of others, the best estimate of
    the vector **β** can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple linear regression](img/B03905_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This solution uses a pair of matrix operations—the **T** indicates the **transpose**
    of matrix **X**, while the negative exponent indicates the **matrix inverse**.
    Using R's built-in matrix operations, we can thus implement a simple multiple
    regression learner. Let's apply this formula to the Challenger launch data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are unfamiliar with the preceding matrix operations, the Wikipedia pages
    for transpose and matrix inverse provide a thorough introduction and are quite
    understandable, even without a strong mathematics background.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following code, we can create a basic regression function named `reg()`,
    which takes a parameter `y` and a parameter `x` and returns a vector of estimated
    beta coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reg()` function created here uses several R commands that we have not
    used previously. First, since we will be using the function with sets of columns
    from a data frame, the `as.matrix()` function is used to convert the data frame
    into matrix form. Next, the `cbind()` function is used to bind an additional column
    onto the `x` matrix; the command `Intercept = 1` instructs R to name the new column
    `Intercept` and to fill the column with repeating 1 values. Then, a number of
    matrix operations are performed on the `x` and `y` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`solve()` takes the inverse of a matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t()` is used to transpose a matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%*%` multiplies two matrices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining these as shown, our function will return a vector `b`, which contains
    the estimated parameters for the linear model relating `x` to `y`. The final two
    lines in the function give the `b` vector a name and print the result on screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply our function to the shuttle launch data. As shown in the following
    code, the dataset includes three features and the distress count (`distress_ct`),
    which is the outcome of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can confirm that our function is working correctly by comparing its result
    to the simple linear regression model of O-ring failures versus temperature, which
    we found earlier to have parameters *a = 3.70* and *b = -0.048*. Since temperature
    is in the third column of the launch data, we can run the `reg()` function as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'These values exactly match our prior result, so let''s use the function to
    build a multiple regression model. We''ll apply it just as before, but this time
    specifying three columns of data instead of just one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This model predicts the number of O-ring distress events versus temperature,
    field check pressure, and the launch ID number. As with the simple linear regression
    model, the coefficient for the temperature variable is negative, which suggests
    that as temperature increases, the number of expected O-ring events decreases.
    The field check pressure refers to the amount of pressure applied to the O-ring
    to test it prior to launch. Although the check pressure had originally been 50
    psi, it was raised to 100 and 200 psi for some launches, which led some to believe
    that it may be responsible for O-ring erosion. The coefficient is positive, but
    small. The flight number is included to account for the shuttle's age. As it gets
    older, its parts may be more brittle or prone to fail. The small positive association
    between flight number and distress count may reflect this fact.
  prefs: []
  type: TYPE_NORMAL
- en: So far we've only scratched the surface of linear regression modeling. Although
    our work was useful to help us understand exactly how regression models are built,
    R's functions also include some additional functionality necessary for the more
    complex modeling tasks and diagnostic output that are needed to aid model interpretation
    and assess fit. Let's apply our knowledge of regression to a more challenging
    learning task.
  prefs: []
  type: TYPE_NORMAL
- en: Example – predicting medical expenses using linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order for a health insurance company to make money, it needs to collect more
    in yearly premiums than it spends on medical care to its beneficiaries. As a result,
    insurers invest a great deal of time and money in developing models that accurately
    forecast medical expenses for the insured population.
  prefs: []
  type: TYPE_NORMAL
- en: Medical expenses are difficult to estimate because the most costly conditions
    are rare and seemingly random. Still, some conditions are more prevalent for certain
    segments of the population. For instance, lung cancer is more likely among smokers
    than non-smokers, and heart disease may be more likely among the obese.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this analysis is to use patient data to estimate the average medical
    care expenses for such population segments. These estimates can be used to create
    actuarial tables that set the price of yearly premiums higher or lower, depending
    on the expected treatment costs.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this analysis, we will use a simulated dataset containing hypothetical medical
    expenses for patients in the United States. This data was created for this book
    using demographic statistics from the US Census Bureau, and thus, approximately
    reflect real-world conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you would like to follow along interactively, download the `insurance.csv`
    file from the Packt Publishing website and save it to your R working folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `insurance.csv` file includes 1,338 examples of beneficiaries currently
    enrolled in the insurance plan, with features indicating characteristics of the
    patient as well as the total medical expenses charged to the plan for the calendar
    year. The features are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`age`: An integer indicating the age of the primary beneficiary (excluding
    those above 64 years, since they are generally covered by the government).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sex`: The policy holder''s gender, either male or female.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bmi`: The body mass index (BMI), which provides a sense of how over- or under-weight
    a person is relative to their height. BMI is equal to weight (in kilograms) divided
    by height (in meters) squared. An ideal BMI is within the range of 18.5 to 24.9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`children`: An integer indicating the number of children/dependents covered
    by the insurance plan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`smoker`: A yes or no categorical variable that indicates whether the insured
    regularly smokes tobacco.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`region`: The beneficiary''s place of residence in the US, divided into four
    geographic regions: northeast, southeast, southwest, or northwest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to give some thought to how these variables may be related to
    billed medical expenses. For instance, we might expect that older people and smokers
    are at higher risk of large medical expenses. Unlike many other machine learning
    methods, in regression analysis, the relationships among the features are typically
    specified by the user rather than being detected automatically. We'll explore
    some of these potential relationships in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have done before, we will use the `read.csv()` function to load the data
    for analysis. We can safely use `stringsAsFactors = TRUE` because it is appropriate
    to convert the three nominal variables to factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `str()` function confirms that the data is formatted as we had expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model''s dependent variable is `expenses`, which measures the medical costs
    each person charged to the insurance plan for the year. Prior to building a regression
    model, it is often helpful to check for normality. Although linear regression
    does not strictly require a normally distributed dependent variable, the model
    often fits better when this is true. Let''s take a look at the summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the mean value is greater than the median, this implies that the distribution
    of insurance expenses is right-skewed. We can confirm this visually using a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 2 – exploring and preparing the data](img/B03905_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the figure shows a right-skewed distribution. It also shows that
    the majority of people in our data have yearly medical expenses between zero and
    $15,000, in spite of the fact that the tail of the distribution extends far past
    these peaks. Although this distribution is not ideal for a linear regression,
    knowing this weakness ahead of time may help us design a better-fitting model
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we address that issue, another problem is at hand. Regression models
    require that every feature is numeric, yet we have three factor-type features
    in our data frame. For instance, the sex variable is divided into male and female
    levels, while smoker is divided into yes and no. From the `summary()` output,
    we know that the `region` variable has four levels, but we need to take a closer
    look to see how they are distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see that the data has been divided nearly evenly among four geographic
    regions. We will see how R's linear regression function handles these factor variables
    shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring relationships among features – the correlation matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before fitting a regression model to data, it can be useful to determine how
    the independent variables are related to the dependent variable and each other.
    A **correlation matrix** provides a quick overview of these relationships. Given
    a set of variables, it provides a correlation for each pairwise relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a correlation matrix for the four numeric variables in the insurance
    data frame, use the `cor()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: At the intersection of each row and column pair, the correlation is listed for
    the variables indicated by that row and column. The diagonal is always `1.0000000`
    since there is always a perfect correlation between a variable and itself. The
    values above and below the diagonal are identical since correlations are symmetrical.
    In other words, `cor(x, y)` is equal to `cor(y, x)`.
  prefs: []
  type: TYPE_NORMAL
- en: None of the correlations in the matrix are considered strong, but there are
    some notable associations. For instance, `age` and `bmi` appear to have a weak
    positive correlation, meaning that as someone ages, their body mass tends to increase.
    There is also a moderate positive correlation between `age` and `expenses`, `bmi`
    and `expenses`, and `children` and `expenses`. These associations imply that as
    age, body mass, and number of children increase, the expected cost of insurance
    goes up. We'll try to tease out these relationships more clearly when we build
    our final regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing relationships among features – the scatterplot matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It can also be helpful to visualize the relationships among numeric features
    by using a scatterplot. Although we could create a scatterplot for each possible
    relationship, doing so for a large number of features might become tedious.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to create a **scatterplot matrix** (sometimes abbreviated
    as **SPLOM**), which is simply a collection of scatterplots arranged in a grid.
    It is used to detect patterns among three or more variables. The scatterplot matrix
    is not a true multidimensional visualization because only two features are examined
    at a time. Still, it provides a general sense of how the data may be interrelated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use R''s graphical capabilities to create a scatterplot matrix for the
    four numeric features: `age`, `bmi`, `children`, and `expenses`. The `pairs()`
    function is provided in a default R installation and provides basic functionality
    for producing scatterplot matrices. To invoke the function, simply provide it
    the data frame to present. Here, we''ll limit the `insurance` data frame to the
    four numeric variables of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing relationships among features – the scatterplot matrix](img/B03905_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the scatterplot matrix, the intersection of each row and column holds the
    scatterplot of the variables indicated by the row and column pair. The diagrams
    above and below the diagonal are transpositions since the *x* axis and *y* axis
    have been swapped.
  prefs: []
  type: TYPE_NORMAL
- en: Do you notice any patterns in these plots? Although some look like random clouds
    of points, a few seem to display some trends. The relationship between `age` and
    `expenses` displays several relatively straight lines, while the `bmi` versus
    `expenses` plot has two distinct groups of points. It is difficult to detect trends
    in any of the other plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we add more information to the plot, it can be even more useful. An enhanced
    scatterplot matrix can be created with the `pairs.panels()` function in the `psych`
    package. If you do not have this package installed, type `install.packages("psych")`
    to install it on your system and load it using the `library(psych)` command. Then,
    we can create a scatterplot matrix as we had done previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a slightly more informative scatterplot matrix, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing relationships among features – the scatterplot matrix](img/B03905_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Above the diagonal, the scatterplots have been replaced with a correlation matrix.
    On the diagonal, a histogram depicting the distribution of values for each feature
    is shown. Finally, the scatterplots below the diagonal are now presented with
    additional visual information.
  prefs: []
  type: TYPE_NORMAL
- en: The oval-shaped object on each scatterplot is a **correlation ellipse**. It
    provides a visualization of correlation strength. The dot at the center of the
    ellipse indicates the point at the mean values for the *x* and *y* axis variables.
    The correlation between the two variables is indicated by the shape of the ellipse;
    the more it is stretched, the stronger the correlation. An almost perfectly round
    oval, as with `bmi` and `children`, indicates a very weak correlation (in this
    case, it is 0.01).
  prefs: []
  type: TYPE_NORMAL
- en: The curve drawn on the scatterplot is called a **loess curve**. It indicates
    the general relationship between the *x* and *y* axis variables. It is best understood
    by example. The curve for `age` and `children` is an upside-down U, peaking around
    middle age. This means that the oldest and youngest people in the sample have
    fewer children on the insurance plan than those around middle age. Because this
    trend is non-linear, this finding could not have been inferred from the correlations
    alone. On the other hand, the loess curve for `age` and `bmi` is a line sloping
    gradually up, implying that body mass increases with age, but we had already inferred
    this from the correlation matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To fit a linear regression model to data with R, the `lm()` function can be
    used. This is included in the `stats` package, which should be included and loaded
    by default with your R installation. The `lm()` syntax is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 3 – training a model on the data](img/B03905_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command fits a linear regression model relating the six independent
    variables to the total medical expenses. The R formula syntax uses the tilde character
    `~` to describe the model; the dependent variable `expenses` goes to the left
    of the tilde while the independent variables go to the right, separated by `+`
    signs. There is no need to specify the regression model''s intercept term as it
    is assumed by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the `.` character can be used to specify all the features (excluding
    those already specified in the formula), the following command is equivalent to
    the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After building the model, simply type the name of the model object to see the
    estimated beta coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the regression coefficients is fairly straightforward. The intercept
    is the predicted value of `expenses` when the independent variables are equal
    to zero. As is the case here, quite often the intercept is of little value alone
    because it is impossible to have values of zero for all features. For example,
    since no person exists with age zero and BMI zero, the intercept has no real-world
    interpretation. For this reason, in practice, the intercept is often ignored.
  prefs: []
  type: TYPE_NORMAL
- en: The beta coefficients indicate the estimated increase in expenses for an increase
    of one in each of the features, assuming all other values are held constant. For
    instance, for each additional year of age, we would expect $256.80 higher medical
    expenses on average, assuming everything else is equal. Similarly, each additional
    child results in an average of $475.70 in additional medical expenses each year,
    and each unit increase in BMI is associated with an average increase of $339.30
    in yearly medical expenses, all else equal.
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that although we only specified six features in our model formula,
    there are eight coefficients reported in addition to the intercept. This happened
    because the `lm()` function automatically applied a technique known as **dummy
    coding** to each of the factor-type variables we included in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dummy coding allows a nominal feature to be treated as numeric by creating
    a binary variable, often called a **dummy** **variable**, for each category of
    the feature. The dummy variable is set to `1` if the observation falls into the
    specified category or `0` otherwise. For instance, the `sex` feature has two categories:
    `male` and `female`. This will be split into two binary variables, which R names
    `sexmale` and `sexfemale`. For observations where `sex = male`, then `sexmale
    = 1` and `sexfemale = 0`; conversely, if `sex = female`, then `sexmale = 0` and
    `sexfemale = 1`. The same coding applies to variables with three or more categories.
    For example, R split the four-category feature `region` into four dummy variables:
    `regionnorthwest`, `regionsoutheast`, `regionsouthwest`, and `regionnortheast`.'
  prefs: []
  type: TYPE_NORMAL
- en: When adding a dummy variable to a regression model, one category is always left
    out to serve as the reference category. The estimates are then interpreted relative
    to the reference. In our model, R automatically held out the `sexfemale`, `smokerno`,
    and `regionnortheast` variables, making female non-smokers in the northeast region
    the reference group. Thus, males have $131.40 less medical expenses each year
    relative to females and smokers cost an average of $23,847.50 more than non-smokers
    per year. The coefficient for each of the three regions in the model is negative,
    which implies that the reference group, the northeast region, tends to have the
    highest average expenses.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, R uses the first level of the factor variable as the reference.
    If you would prefer to use another level, the `relevel()` function can be used
    to specify the reference group manually. Use the `?relevel` command in R for more
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the linear regression model make logical sense: old age, smoking,
    and obesity tend to be linked to additional health issues, while additional family
    member dependents may result in an increase in physician visits and preventive
    care such as vaccinations and yearly physical exams. However, we currently have
    no sense of how well the model is fitting the data. We''ll answer this question
    in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The parameter estimates we obtained by typing `ins_model` tell us about how
    the independent variables are related to the dependent variable, but they tell
    us nothing about how well the model fits our data. To evaluate the model performance,
    we can use the `summary()` command on the stored model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output. Note that the output has been labeled for
    illustrative purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 4 – evaluating model performance](img/B03905_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `summary()` output may seem confusing at first, but the basics are easy
    to pick up. As indicated by the numbered labels in the preceding output, the output
    provides three key ways to evaluate the performance, or fit, of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: The **residuals** section provides summary statistics for the errors in our
    predictions, some of which are apparently quite substantial. Since a residual
    is equal to the true value minus the predicted value, the maximum error of 29981.7
    suggests that the model under-predicted expenses by nearly $30,000 for at least
    one observation. On the other hand, 50 percent of errors fall within the 1Q and
    3Q values (the first and third quartile), so the majority of predictions were
    between $2,850.90 over the true value and $1,383.90 under the true value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each estimated regression coefficient, the **p-value**, denoted `Pr(>|t|)`,
    provides an estimate of the probability that the true coefficient is zero given
    the value of the estimate. Small p-values suggest that the true coefficient is
    very unlikely to be zero, which means that the feature is extremely unlikely to
    have no relationship with the dependent variable. Note that some of the p-values
    have stars (`***`), which correspond to the footnotes to indicate the **significance
    level** met by the estimate. This level is a threshold, chosen prior to building
    the model, which will be used to indicate "real" findings, as opposed to those
    due to chance alone; p-values less than the significance level are considered
    **statistically significant**. If the model had few such terms, it may be cause
    for concern, since this would indicate that the features used are not very predictive
    of the outcome. Here, our model has several highly significant variables, and
    they seem to be related to the outcome in logical ways.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **multiple R-squared value** (also called the coefficient of determination)
    provides a measure of how well our model as a whole explains the values of the
    dependent variable. It is similar to the correlation coefficient, in that the
    closer the value is to 1.0, the better the model perfectly explains the data.
    Since the R-squared value is 0.7494, we know that the model explains nearly 75
    percent of the variation in the dependent variable. Because models with more features
    always explain more variation, the **adjusted R-squared value** corrects R-squared
    by penalizing models with a large number of independent variables. It is useful
    for comparing the performance of models with different numbers of explanatory
    variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the preceding three performance indicators, our model is performing fairly
    well. It is not uncommon for regression models of real-world data to have fairly
    low R-squared values; a value of 0.75 is actually quite good. The size of some
    of the errors is a bit concerning, but not surprising given the nature of medical
    expense data. However, as shown in the next section, we may be able to improve
    the model's performance by specifying the model in a slightly different way.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, a key difference between the regression modeling and
    other machine learning approaches is that regression typically leaves feature
    selection and model specification to the user. Consequently, if we have subject
    matter knowledge about how a feature is related to the outcome, we can use this
    information to inform the model specification and potentially improve the model's
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Model specification – adding non-linear relationships
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In linear regression, the relationship between an independent variable and the
    dependent variable is assumed to be linear, yet this may not necessarily be true.
    For example, the effect of age on medical expenditure may not be constant throughout
    all the age values; the treatment may become disproportionately expensive for
    oldest populations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, a typical regression equation follows a form similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model specification – adding non-linear relationships](img/B03905_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To account for a non-linear relationship, we can add a higher order term to
    the regression model, treating the model as a polynomial. In effect, we will be
    modeling a relationship like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model specification – adding non-linear relationships](img/B03905_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The difference between these two models is that an additional beta will be estimated,
    which is intended to capture the effect of the *x*-squared term. This allows the
    impact of age to be measured as a function of age squared.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add the non-linear age to the model, we simply need to create a new variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Then, when we produce our improved model, we'll add both `age` and `age2` to
    the `lm()` formula using the `expenses ~ age + age2` form. This will allow the
    model to separate the linear and non-linear impact of age on medical expenses.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation – converting a numeric variable to a binary indicator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we have a hunch that the effect of a feature is not cumulative, rather
    it has an effect only after a specific threshold has been reached. For instance,
    BMI may have zero impact on medical expenditures for individuals in the normal
    weight range, but it may be strongly related to higher costs for the obese (that
    is, BMI of 30 or above).
  prefs: []
  type: TYPE_NORMAL
- en: We can model this relationship by creating a binary obesity indicator variable
    that is 1 if the BMI is at least 30, and 0 if less. The estimated beta for this
    binary feature would then indicate the average net impact on medical expenses
    for individuals with BMI of 30 or above, relative to those with BMI less than
    30.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the feature, we can use the `ifelse()` function, which for each element
    in a vector tests a specified condition and returns a value depending on whether
    the condition is true or false. For BMI greater than or equal to 30, we will return
    `1`, otherwise `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can then include the `bmi30` variable in our improved model, either replacing
    the original `bmi` variable or in addition, depending on whether or not we think
    the effect of obesity occurs in addition to a separate linear BMI effect. Without
    good reason to do otherwise, we'll include both in our final model.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have trouble deciding whether or not to include a variable, a common
    practice is to include it and examine the p-value. If the variable is not statistically
    significant, you have evidence to support excluding it in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Model specification – adding interaction effects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have only considered each feature's individual contribution to the
    outcome. What if certain features have a combined impact on the dependent variable?
    For instance, smoking and obesity may have harmful effects separately, but it
    is reasonable to assume that their combined effect may be worse than the sum of
    each one alone.
  prefs: []
  type: TYPE_NORMAL
- en: When two features have a combined effect, this is known as an **interaction**.
    If we suspect that two variables interact, we can test this hypothesis by adding
    their interaction to the model. Interaction effects are specified using the R
    formula syntax. To have the obesity indicator (`bmi30`) and the smoking indicator
    (`smoker`) interact, we would write a formula in the form `expenses ~ bmi30*smoker`.
  prefs: []
  type: TYPE_NORMAL
- en: The `*` operator is shorthand that instructs R to model `expenses ~ bmi30 +
    smokeryes + bmi30:smokeryes`. The `:` (colon) operator in the expanded form indicates
    that `bmi30:smokeryes` is the interaction between the two variables. Note that
    the expanded form also automatically included the `bmi30` and `smoker` variables
    as well as the interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interactions should never be included in a model without also adding each of
    the interacting variables. If you always create interactions using the `*` operator,
    this will not be a problem since R will add the required components automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together – an improved regression model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on a bit of subject matter knowledge of how medical costs may be related
    to patient characteristics, we developed what we think is a more accurately specified
    regression formula. To summarize the improvements, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Added a non-linear term for age
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Created an indicator for obesity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specified an interaction between obesity and smoking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll train the model using the `lm()` function as before, but this time we''ll
    add the newly constructed variables and the interaction term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we summarize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Putting it all together – an improved regression model](img/B03905_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The model fit statistics help to determine whether our changes improved the
    performance of the regression model. Relative to our first model, the R-squared
    value has improved from 0.75 to about 0.87\. Similarly, the adjusted R-squared
    value, which takes into account the fact that the model grew in complexity, also
    improved from 0.75 to 0.87\. Our model is now explaining 87 percent of the variation
    in medical treatment costs. Additionally, our theories about the model's functional
    form seem to be validated. The higher-order `age2` term is statistically significant,
    as is the obesity indicator, `bmi30`. The interaction between obesity and smoking
    suggests a massive effect; in addition to the increased costs of over $13,404
    for smoking alone, obese smokers spend another $19,810 per year. This may suggest
    that smoking exacerbates diseases associated with obesity.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Strictly speaking, regression modeling makes some strong assumptions about
    the data. These assumptions are not as important for numeric forecasting, as the
    model''s worth is not based upon whether it truly captures the underlying process—we
    simply care about the accuracy of its predictions. However, if you would like
    to make firm inferences from the regression model coefficients, it is necessary
    to run diagnostic tests to ensure that the regression assumptions have not been
    violated. For an excellent introduction to this topic, see Allison PD*. Multiple
    regression: A primer*. Pine Forge Press; 1998.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regression trees and model trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you recall from [Chapter 5](ch05.html "Chapter 5. Divide and Conquer – Classification
    Using Decision Trees and Rules"), *Divide and Conquer – Classification Using Decision
    Trees and Rules*, a decision tree builds a model much like a flowchart in which
    decision nodes, leaf nodes, and branches define a series of decisions that are
    used to classify examples. Such trees can also be used for numeric prediction
    by making only small adjustments to the tree-growing algorithm. In this section,
    we will consider only the ways in which trees for numeric prediction differ from
    trees used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Trees for numeric prediction fall into two categories. The first, known as **regression
    trees**, were introduced in the 1980s as part of the seminal **Classification
    and Regression Tree** (**CART**) algorithm. Despite the name, regression trees
    do not use linear regression methods as described earlier in this chapter, rather
    they make predictions based on the average value of examples that reach a leaf.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The CART algorithm is described in detail in Breiman L, Friedman JH, Stone
    CJ, Olshen RA. *Classification and Regression Trees*. Belmont, CA: Chapman and
    Hall; 1984.'
  prefs: []
  type: TYPE_NORMAL
- en: The second type of trees for numeric prediction are known as **model trees**.
    Introduced several years later than regression trees, they are lesser-known, but
    perhaps more powerful. Model trees are grown in much the same way as regression
    trees, but at each leaf, a multiple linear regression model is built from the
    examples reaching that node. Depending on the number of leaf nodes, a model tree
    may build tens or even hundreds of such models. This may make model trees more
    difficult to understand than the equivalent regression tree, with the benefit
    that they may result in a more accurate model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The earliest model tree algorithm, **M5**, is described in Quinlan JR. *Learning
    with continuous classes*. Proceedings of the 5th Australian Joint Conference on
    Artificial Intelligence. 1992:343-348.
  prefs: []
  type: TYPE_NORMAL
- en: Adding regression to trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Trees that can perform numeric prediction offer a compelling yet often overlooked
    alternative to regression modeling. The strengths and weaknesses of regression
    trees and model trees relative to the more common regression methods are listed
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Combines the strengths of decision trees with the ability to model numeric data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not require the user to specify the model in advance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses automatic feature selection, which allows the approach to be used with
    a very large number of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May fit some types of data much better than linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not require knowledge of statistics to interpret the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Not as well-known as linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires a large amount of training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to determine the overall net effect of individual features on the
    outcome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large trees can become more difficult to interpret than a regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Though traditional regression methods are typically the first choice for numeric
    prediction tasks, in some cases, numeric decision trees offer distinct advantages.
    For instance, decision trees may be better suited for tasks with many features
    or many complex, non-linear relationships among features and outcome. These situations
    present challenges for regression. Regression modeling also makes assumptions
    about how numeric data is distributed that are often violated in real-world data.
    This is not the case for trees.
  prefs: []
  type: TYPE_NORMAL
- en: Trees for numeric prediction are built in much the same way as they are for
    classification. Beginning at the root node, the data is partitioned using a divide-and-conquer
    strategy according to the feature that will result in the greatest increase in
    homogeneity in the outcome after a split is performed. In classification trees,
    you will recall that homogeneity is measured by entropy, which is undefined for
    numeric data. Instead, for numeric decision trees, homogeneity is measured by
    statistics such as variance, standard deviation, or absolute deviation from the
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'One common splitting criterion is called the **Standard Deviation Reduction**
    (**SDR**). It is defined by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding regression to trees](img/B03905_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, the *sd(T)* function refers to the standard deviation of the
    values in set *T*, while *T[1]*, *T[2]*, ..., *T[n]* are the sets of values resulting
    from a split on a feature. The *|T|* term refers to the number of observations
    in set *T*. Essentially, the formula measures the reduction in standard deviation
    by comparing the standard deviation pre-split to the weighted standard deviation
    post-split.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following case in which a tree is deciding whether
    or not to perform a split on binary feature A or B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding regression to trees](img/B03905_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the groups that would result from the proposed splits, we can compute
    the SDR for A and B as follows. The `length()` function used here returns the
    number of elements in a vector. Note that the overall group T is named `tee` to
    avoid overwriting R''s built-in `T()` and `t()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compare the SDR of A against the SDR of B:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The SDR for the split on feature A was about 1.2 versus 1.4 for the split on
    feature B. Since the standard deviation was reduced more for the split on B, the
    decision tree would use B first. It results in slightly more homogeneous sets
    than with A.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the tree stopped growing here using this one and only split. A
    regression tree's work is done. It can make predictions for new examples depending
    on whether the example's value on feature B places the example into group *T[1]*
    or *T[2]*. If the example ends up in *T[1]*, the model would predict *mean(bt1)
    = 2*, otherwise it would predict *mean(bt2) = 6.25*.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, a model tree would go one step further. Using the seven training
    examples falling in group *T[1]* and the eight in *T[2]*, the model tree could
    build a linear regression model of the outcome versus feature A. Note that Feature
    B is of no help in building the regression model because all examples at the leaf
    have the same value of B—they were placed into *T[1]* or *T[2]* according to their
    value of B. The model tree can then make predictions for new examples using either
    of the two linear models.
  prefs: []
  type: TYPE_NORMAL
- en: To further illustrate the differences between these two approaches, let's work
    through a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: Example – estimating the quality of wines with regression trees and model trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Winemaking is a challenging and competitive business that offers the potential
    for great profit. However, there are numerous factors that contribute to the profitability
    of a winery. As an agricultural product, variables as diverse as the weather and
    the growing environment impact the quality of a varietal. The bottling and manufacturing
    can also affect the flavor for better or worse. Even the way the product is marketed,
    from the bottle design to the price point, can affect the customer's perception
    of taste.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, the winemaking industry has heavily invested in data collection
    and machine learning methods that may assist with the decision science of winemaking.
    For example, machine learning has been used to discover key differences in the
    chemical composition of wines from different regions, or to identify the chemical
    factors that lead a wine to taste sweeter.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, machine learning has been employed to assist with rating the
    quality of wine—a notoriously difficult task. A review written by a renowned wine
    critic often determines whether the product ends up on the top or bottom shelf,
    in spite of the fact that even the expert judges are inconsistent when rating
    a wine in a blinded test.
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we will use regression trees and model trees to create a
    system capable of mimicking expert ratings of wine. Because trees result in a
    model that is readily understood, this can allow the winemakers to identify the
    key factors that contribute to better-rated wines. Perhaps more importantly, the
    system does not suffer from the human elements of tasting, such as the rater's
    mood or palate fatigue. Computer-aided wine testing may therefore result in a
    better product as well as more objective, consistent, and fair ratings.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To develop the wine rating model, we will use data donated to the UCI Machine
    Learning Data Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))
    by P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. The data include
    examples of red and white Vinho Verde wines from Portugal—one of the world's leading
    wine-producing countries. Because the factors that contribute to a highly rated
    wine may differ between the red and white varieties, for this analysis we will
    examine only the more popular white wines.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To follow along with this example, download the `whitewines.csv` file from the
    Packt Publishing website and save it to your R working directory. The `redwines.csv`
    file is also available in case you would like to explore this data on your own.
  prefs: []
  type: TYPE_NORMAL
- en: The white wine data includes information on 11 chemical properties of 4,898
    wine samples. For each wine, a laboratory analysis measured characteristics such
    as acidity, sugar content, chlorides, sulfur, alcohol, pH, and density. The samples
    were then rated in a blind tasting by panels of no less than three judges on a
    quality scale ranging from zero (very bad) to 10 (excellent). In the case of judges
    disagreeing on the rating, the median value was used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The study by Cortez evaluated the ability of three machine learning approaches
    to model the wine data: multiple regression, artificial neural networks, and support
    vector machines. We covered multiple regression earlier in this chapter, and we
    will learn about neural networks and support vector machines in [Chapter 7](ch07.html
    "Chapter 7. Black Box Methods – Neural Networks and Support Vector Machines"),
    *Black Box Methods – Neural Networks and Support Vector Machines*. The study found
    that the support vector machine offered significantly better results than the
    linear regression model. However, unlike regression, the support vector machine
    model is difficult to interpret. Using regression trees and model trees, we may
    be able to improve the regression results while still having a model that is easy
    to understand.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To read more about the wine study described here, please refer to Cortez P,
    Cerdeira A, Almeida F, Matos T, Reis J. *Modeling wine preferences by data mining
    from physicochemical properties*. Decision Support Systems*.* 2009; 47:547-553.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we will use the `read.csv()` function to load the data into R. Since
    all of the features are numeric, we can safely ignore the `stringsAsFactors` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The wine data includes 11 features and the quality outcome, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Compared with other types of machine learning models, one of the advantages
    of trees is that they can handle many types of data without preprocessing. This
    means we do not need to normalize or standardize the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a bit of effort to examine the distribution of the outcome variable
    is needed to inform our evaluation of the model''s performance. For instance,
    suppose that there was a very little variation in quality from wine-to-wine, or
    that wines fell into a bimodal distribution: either very good or very bad. To
    check for such extremes, we can examine the distribution of quality using a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 2 – exploring and preparing the data](img/B03905_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The wine quality values appear to follow a fairly normal, bell-shaped distribution,
    centered around a value of six. This makes sense intuitively because most wines
    are of average quality; few are particularly bad or good. Although the results
    are not shown here, it is also useful to examine the `summary(wine)` output for
    outliers or other potential data problems. Even though trees are fairly robust
    with messy data, it is always prudent to check for severe problems. For now, we'll
    assume that the data is reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last step then is to divide into training and testing datasets. Since the
    `wine` data set was already sorted into random order, we can partition into two
    sets of contiguous rows as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In order to mirror the conditions used by Cortez, we used sets of 75 percent
    and 25 percent for training and testing, respectively. We'll evaluate the performance
    of our tree-based models on the testing data to see if we can obtain results comparable
    to the prior research study.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will begin by training a regression tree model. Although almost any implementation
    of decision trees can be used to perform regression tree modeling, the `rpart`
    (recursive partitioning) package offers the most faithful implementation of regression
    trees as they were described by the CART team. As the classic R implementation
    of CART, the `rpart` package is also well-documented and supported with functions
    for visualizing and evaluating the `rpart` models.
  prefs: []
  type: TYPE_NORMAL
- en: Install the `rpart` package using the `install.packages("rpart")` command. It
    can then be loaded into your R session using the `library(rpart)` command. The
    following syntax will train a tree using the default settings, which typically
    work fairly well. If you need more finely-tuned settings, refer to the documentation
    for the control parameters using the `?rpart.control` command.
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 3 – training a model on the data](img/B03905_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the R formula interface, we can specify `quality` as the outcome variable
    and use the dot notation to allow all the other columns in the `wine_train` data
    frame to be used as predictors. The resulting regression tree model object is
    named `m.rpart` to distinguish it from the model tree that we will train later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'For basic information about the tree, simply type the name of the model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: For each node in the tree, the number of examples reaching the decision point
    is listed. For instance, all 3,750 examples begin at the root node, of which 2,372
    have `alcohol < 10.85` and 1,378 have `alcohol >= 10.85`. Because alcohol was
    used first in the tree, it is the single most important predictor of wine quality.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes indicated by `*` are terminal or leaf nodes, which means that they result
    in a prediction (listed here as `yval`). For example, node 5 has a `yval` of 5.971091\.
    When the tree is used for predictions, any wine samples with `alcohol < 10.85`
    and `volatile.acidity < 0.2275` would therefore be predicted to have a quality
    value of 5.97.
  prefs: []
  type: TYPE_NORMAL
- en: A more detailed summary of the tree's fit, including the mean squared error
    for each of the nodes and an overall measure of feature importance, can be obtained
    using the `summary(m.rpart)` command.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing decision trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the tree can be understood using only the preceding output, it is often
    more readily understood using visualization. The `rpart.plot` package by Stephen
    Milborrow provides an easy-to-use function that produces publication-quality decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on `rpart.plot`, including additional examples of the types
    of decision tree diagrams that the function can produce, refer to the author's
    website at [http://www.milbo.org/rpart-plot/](http://www.milbo.org/rpart-plot/).
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the package using the `install.packages("rpart.plot")` command,
    the `rpart.plot()` function produces a tree diagram from any `rpart` model object.
    The following commands plot the regression tree we built earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting tree diagram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing decision trees](img/B03905_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition to the `digits` parameter that controls the number of numeric digits
    to include in the diagram, many other aspects of the visualization can be adjusted.
    The following command shows just a few of the useful options: The `fallen.leaves`
    parameter forces the leaf nodes to be aligned at the bottom of the plot, while
    the `type` and `extra` parameters affect the way the decisions and nodes are labeled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of these changes is a very different looking tree diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing decision trees](img/B03905_06_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Visualizations like these may assist with the dissemination of regression tree
    results, as they are readily understood even without a mathematics background.
    In both cases, the numbers shown in the leaf nodes are the predicted values for
    the examples reaching that node. Showing the diagram to the wine producers may
    thus help to identify the key factors that predict the higher rated wines.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use the regression tree model to make predictions on the test data, we use
    the `predict()` function. By default, this returns the estimated numeric value
    for the outcome variable, which we''ll save in a vector named `p.rpart`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick look at the summary statistics of our predictions suggests a potential
    problem; the predictions fall on a much narrower range than the true values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This finding suggests that the model is not correctly identifying the extreme
    cases, in particular the best and worst wines. On the other hand, between the
    first and third quartile, we may be doing well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correlation between the predicted and actual quality values provides a
    simple way to gauge the model''s performance. Recall that the `cor()` function
    can be used to measure the relationship between two equal-length vectors. We''ll
    use this to compare how well the predicted values correspond to the true values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: A correlation of 0.54 is certainly acceptable. However, the correlation only
    measures how strongly the predictions are related to the true value; it is not
    a measure of how far off the predictions were from the true values.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance with the mean absolute error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another way to think about the model''s performance is to consider how far,
    on average, its prediction was from the true value. This measurement is called
    the **mean absolute error** (**MAE**). The equation for MAE is as follows, where
    *n* indicates the number of predictions and *e[i]* indicates the error for prediction
    *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring performance with the mean absolute error](img/B03905_06_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As the name implies, this equation takes the mean of the absolute value of
    the errors. Since the error is just the difference between the predicted and actual
    values, we can create a simple `MAE()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The MAE for our predictions is then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This implies that, on average, the difference between our model's predictions
    and the true quality score was about 0.59\. On a quality scale from zero to 10,
    this seems to suggest that our model is doing fairly well.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, recall that most wines were neither very good nor very bad;
    the typical quality score was around five to six. Therefore, a classifier that
    did nothing but predict the mean value may still do fairly well according to this
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean quality rating in the training data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If we predicted the value 5.87 for every wine sample, we would have a mean
    absolute error of only about 0.67:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Our regression tree (*MAE = 0.59*) comes closer on average to the true quality
    score than the imputed mean (*MAE = 0.67*), but not by much. In comparison, Cortez
    reported an MAE of 0.58 for the neural network model and an MAE of 0.45 for the
    support vector machine. This suggests that there is room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To improve the performance of our learner, let's try to build a model tree.
    Recall that a model tree improves on regression trees by replacing the leaf nodes
    with regression models. This often results in more accurate results than regression
    trees, which use only a single value for prediction at the leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The current state-of-the-art in model trees is the **M5' algorithm** (**M5-prime**)
    by Y. Wang and I.H. Witten, which is a variant of the original M5 model tree algorithm
    proposed by J.R. Quinlan in 1992.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the M5' algorithm, see Wang Y, Witten IH. *Induction
    of model trees for predicting continuous classes*. Proceedings of the Poster Papers
    of the European Conference on Machine Learning. 1997.
  prefs: []
  type: TYPE_NORMAL
- en: The M5 algorithm is available in R via the `RWeka` package and the `M5P()` function.
    The syntax of this function is shown in the following table. Be sure to install
    the `RWeka` package if you haven't already. Because of its dependence on Java,
    the installation instructions are included in [Chapter 1](ch01.html "Chapter 1. Introducing
    Machine Learning"), *Introducing Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 5 – improving model performance](img/B03905_06_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll fit the model tree using essentially the same syntax as we used for
    the regression tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The tree itself can be examined by typing its name. In this case, the tree
    is very large and only the first few lines of output are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You will note that the splits are very similar to the regression tree that we
    built earlier. Alcohol is the most important variable, followed by volatile acidity
    and free sulfur dioxide. A key difference, however, is that the nodes terminate
    not in a numeric prediction, but a linear model (shown here as `LM1` and `LM2`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear models themselves are shown later in the output. For instance, the
    model for `LM1` is shown in the forthcoming output. The values can be interpreted
    exactly the same as the multiple regression models we built earlier in this chapter.
    Each number is the net effect of the associated feature on the predicted wine
    quality. The coefficient of `0.266` for fixed acidity implies that for an increase
    of 1 unit of acidity, the wine quality is expected to increase by `0.266`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that the effects estimated by `LM1` apply only to wine
    samples reaching this node; a total of 36 linear models were built in this model
    tree, each with different estimates of the impact of fixed acidity and the other
    10 features.
  prefs: []
  type: TYPE_NORMAL
- en: 'For statistics on how well the model fits the training data, the `summary()`
    function can be applied to the M5P model. However, note that since these statistics
    are based on the training data, they should be used only as a rough diagnostic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, we''ll look at how well the model performs on the unseen test data.
    The `predict()` function gets us a vector of predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The model tree appears to be predicting a wider range of values than the regression
    tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The correlation also seems to be substantially higher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, the model has slightly reduced the mean absolute error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Although we did not improve a great deal beyond the regression tree, we surpassed
    the performance of the neural network model published by Cortez, and we are getting
    closer to the published mean absolute error value of 0.45 for the support vector
    machine model, all by using a much simpler learning method.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not surprisingly, we have confirmed that predicting the quality of wines is
    a difficult problem; wine tasting, after all, is inherently subjective. If you
    would like additional practice, you may try revisiting this problem after reading
    [Chapter 11](ch11.html "Chapter 11. Improving Model Performance"), *Improving
    Model Performance*, which covers additional techniques that may lead to better
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we studied two methods for modeling numeric data. The first
    method, linear regression, involves fitting straight lines to data. The second
    method uses decision trees for numeric prediction. The latter comes in two forms:
    regression trees, which use the average value of examples at leaf nodes to make
    numeric predictions; and model trees, which build a regression model at each leaf
    node in a hybrid approach that is, in some ways, the best of both worlds.'
  prefs: []
  type: TYPE_NORMAL
- en: We used linear regression modeling to calculate the expected medical costs for
    various segments of the population. Because the relationship between the features
    and the target variable are well-described by the estimated regression model,
    we were able to identify certain demographics, such as smokers and the obese,
    who may need to be charged higher insurance rates to cover the higher-than-average
    medical expenses.
  prefs: []
  type: TYPE_NORMAL
- en: Regression trees and model trees were used to model the subjective quality of
    wines from measureable characteristics. In doing so, we learned how regression
    trees offer a simple way to explain the relationship between features and a numeric
    outcome, but the more complex model trees may be more accurate. Along the way,
    we learned several methods for evaluating the performance of numeric models.
  prefs: []
  type: TYPE_NORMAL
- en: In stark contrast to this chapter, which covered machine learning methods that
    result in a clear understanding of the relationships between the input and the
    output, the next chapter covers methods that result in nearly-incomprehensible
    models. The upside is that they are extremely powerful techniques—among the most
    powerful stock classifiers—that can be applied to both classification and numeric
    prediction problems.
  prefs: []
  type: TYPE_NORMAL

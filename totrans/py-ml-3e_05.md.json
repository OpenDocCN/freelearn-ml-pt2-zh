["```py\n>>> import pandas as pd\n>>> df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n...           'machine-learning-databases/wine/wine.data',\n...           header=None) \n```", "```py\ndf = pd.read_csv(\n    'https://archive.ics.uci.edu/ml/'\n    'machine-learning-databases/wine/wine.data',\n    header=None) \n```", "```py\ndf = pd.read_csv(\n    'your/local/path/to/wine.data',\n    header=None) \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n>>> X_train, X_test, y_train, y_test = \\\n...     train_test_split(X, y, test_size=0.3,\n...                      stratify=y,\n...                      random_state=0)\n>>> # standardize the features\n>>> from sklearn.preprocessing import StandardScaler\n>>> sc = StandardScaler()\n>>> X_train_std = sc.fit_transform(X_train)\n>>> X_test_std = sc.transform(X_test) \n```", "```py\n>>> import numpy as np\n>>> cov_mat = np.cov(X_train_std.T)\n>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n>>> print('\\nEigenvalues \\n%s' % eigen_vals)\nEigenvalues\n[ 4.84274532  2.41602459  1.54845825  0.96120438  0.84166161\n  0.6620634   0.51828472  0.34650377  0.3131368   0.10754642\n  0.21357215  0.15362835  0.1808613 ] \n```", "```py\n>>> tot = sum(eigen_vals)\n>>> var_exp = [(i / tot) for i in\n...            sorted(eigen_vals, reverse=True)]\n>>> cum_var_exp = np.cumsum(var_exp)\n>>> import matplotlib.pyplot as plt\n>>> plt.bar(range(1,14), var_exp, alpha=0.5, align='center',\n...         label='Individual explained variance')\n>>> plt.step(range(1,14), cum_var_exp, where='mid',\n...          label='Cumulative explained variance')\n>>> plt.ylabel('Explained variance ratio')\n>>> plt.xlabel('Principal component index')\n>>> plt.legend(loc='best')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> # Make a list of (eigenvalue, eigenvector) tuples\n>>> eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n...                for i in range(len(eigen_vals))]\n>>> # Sort the (eigenvalue, eigenvector) tuples from high to low\n>>> eigen_pairs.sort(key=lambda k: k[0], reverse=True) \n```", "```py\n>>> w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n...                eigen_pairs[1][1][:, np.newaxis]))\n>>> print('Matrix W:\\n', w)\nMatrix W:\n[[-0.13724218   0.50303478]\n [ 0.24724326   0.16487119]\n [-0.02545159   0.24456476]\n [ 0.20694508  -0.11352904]\n [-0.15436582   0.28974518]\n [-0.39376952   0.05080104]\n [-0.41735106  -0.02287338]\n [ 0.30572896   0.09048885]\n [-0.30668347   0.00835233]\n [ 0.07554066   0.54977581]\n [-0.32613263  -0.20716433]\n [-0.36861022  -0.24902536]\n [-0.29669651   0.38022942]] \n```", "```py\n>>> X_train_std[0].dot(w)\narray([ 2.38299011,  0.45458499]) \n```", "```py\n>>> X_train_pca = X_train_std.dot(w) \n```", "```py\n>>> colors = ['r', 'b', 'g']\n>>> markers = ['s', 'x', 'o']\n>>> for l, c, m in zip(np.unique(y_train), colors, markers):\n...     plt.scatter(X_train_pca[y_train==l, 0],\n...                 X_train_pca[y_train==l, 1],\n...                 c=c, label=l, marker=m)\n>>> plt.xlabel('PC 1')\n>>> plt.ylabel('PC 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nfrom matplotlib.colors import ListedColormap\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot examples by class\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0],\n                    y=X[y == cl, 1],\n                    alpha=0.6,\n                    color=cmap(idx),\n                    edgecolor='black',\n                    marker=markers[idx],\n                    label=cl) \n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.decomposition import PCA\n>>> # initializing the PCA transformer and\n>>> # logistic regression estimator:\n>>> pca = PCA(n_components=2)\n>>> lr = LogisticRegression(multi_class='ovr',\n...                         random_state=1,\n...                         solver='lbfgs')\n>>> # dimensionality reduction:\n>>> X_train_pca = pca.fit_transform(X_train_std)\n>>> X_test_pca = pca.transform(X_test_std)\n>>> # fitting the logistic regression model on the reduced dataset:\n>>> lr.fit(X_train_pca, y_train)\n>>> plot_decision_regions(X_train_pca, y_train, classifier=lr)\n>>> plt.xlabel('PC 1')\n>>> plt.ylabel('PC 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> plot_decision_regions(X_test_pca, y_test, classifier=lr)\n>>> plt.xlabel('PC1')\n>>> plt.ylabel('PC2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> pca = PCA(n_components=None)\n>>> X_train_pca = pca.fit_transform(X_train_std)\n>>> pca.explained_variance_ratio_\narray([ 0.36951469, 0.18434927, 0.11815159, 0.07334252,\n        0.06422108, 0.05051724, 0.03954654, 0.02643918,\n        0.02389319, 0.01629614, 0.01380021, 0.01172226,\n        0.00820609]) \n```", "```py\n>>> np.set_printoptions(precision=4)\n>>> mean_vecs = []\n>>> for label in range(1,4):\n...     mean_vecs.append(np.mean(\n...                X_train_std[y_train==label], axis=0))\n...     print('MV %s: %s\\n' %(label, mean_vecs[label-1]))\nMV 1: [ 0.9066  -0.3497  0.3201  -0.7189  0.5056  0.8807  0.9589  -0.5516\n0.5416  0.2338  0.5897  0.6563  1.2075]\nMV 2: [-0.8749  -0.2848  -0.3735  0.3157  -0.3848  -0.0433  0.0635  -0.0946\n0.0703  -0.8286  0.3144  0.3608  -0.7253]\nMV 3: [ 0.1992  0.866  0.1682  0.4148  -0.0451  -1.0286  -1.2876  0.8287\n-0.7795  0.9649  -1.209  -1.3622  -0.4013] \n```", "```py\n>>> d = 13 # number of features\n>>> S_W = np.zeros((d, d))\n>>> for label, mv in zip(range(1, 4), mean_vecs):\n...     class_scatter = np.zeros((d, d))\n>>> for row in X_train_std[y_train == label]:\n...     row, mv = row.reshape(d, 1), mv.reshape(d, 1)\n...     class_scatter += (row - mv).dot((row - mv).T)\n...     S_W += class_scatter\n>>> print('Within-class scatter matrix: %sx%s' % (\n...       S_W.shape[0], S_W.shape[1]))\nWithin-class scatter matrix: 13x13 \n```", "```py\n>>> print('Class label distribution: %s'\n...       % np.bincount(y_train)[1:])\nClass label distribution: [41 50 33] \n```", "```py\n>>> d = 13 # number of features\n>>> S_W = np.zeros((d, d))\n>>> for label,mv in zip(range(1, 4), mean_vecs):\n...     class_scatter = np.cov(X_train_std[y_train==label].T)\n...     S_W += class_scatter\n>>> print('Scaled within-class scatter matrix: %sx%s'\n...      % (S_W.shape[0], S_W.shape[1]))\nScaled within-class scatter matrix: 13x13 \n```", "```py\n>>> mean_overall = np.mean(X_train_std, axis=0)\n>>> d = 13 # number of features\n>>> S_B = np.zeros((d, d))\n>>> for i, mean_vec in enumerate(mean_vecs):\n...     n = X_train_std[y_train == i + 1, :].shape[0]\n...     mean_vec = mean_vec.reshape(d, 1) # make column vector\n...     mean_overall = mean_overall.reshape(d, 1)\n...     S_B += n * (mean_vec - mean_overall).dot(\n...     (mean_vec - mean_overall).T)\n>>> print('Between-class scatter matrix: %sx%s' % (\n...                S_B.shape[0], S_B.shape[1]))\nBetween-class scatter matrix: 13x13 \n```", "```py\n>>> eigen_vals, eigen_vecs =\\\n...     np.linalg.eig(np.linalg.inv(S_W).dot(S_B)) \n```", "```py\n>>> eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i])\n...                for i in range(len(eigen_vals))]\n>>> eigen_pairs = sorted(eigen_pairs,\n...               key=lambda k: k[0], reverse=True)\n>>> print('Eigenvalues in descending order:\\n')\n>>> for eigen_val in eigen_pairs:\n...     print(eigen_val[0])\nEigenvalues in descending order:\n349.617808906\n172.76152219\n3.78531345125e-14\n2.11739844822e-14\n1.51646188942e-14\n1.51646188942e-14\n1.35795671405e-14\n1.35795671405e-14\n7.58776037165e-15\n5.90603998447e-15\n5.90603998447e-15\n2.25644197857e-15\n0.0 \n```", "```py\n>>> tot = sum(eigen_vals.real)\n>>> discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]\n>>> cum_discr = np.cumsum(discr)\n>>> plt.bar(range(1, 14), discr, alpha=0.5, align='center',\n...         label='Individual \"discriminability\"')\n>>> plt.step(range(1, 14), cum_discr, where='mid',\n...          label='Cumulative \"discriminability\"')\n>>> plt.ylabel('\"Discriminability\" ratio')\n>>> plt.xlabel('Linear Discriminants')\n>>> plt.ylim([-0.1, 1.1])\n>>> plt.legend(loc='best')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n...                eigen_pairs[1][1][:, np.newaxis].real))\n>>> print('Matrix W:\\n', w)\nMatrix W:\n [[-0.1481  -0.4092]\n  [ 0.0908  -0.1577]\n  [-0.0168  -0.3537]\n  [ 0.1484   0.3223]\n  [-0.0163  -0.0817]\n  [ 0.1913   0.0842]\n  [-0.7338   0.2823]\n  [-0.075   -0.0102]\n  [ 0.0018   0.0907]\n  [ 0.294   -0.2152]\n  [-0.0328   0.2747]\n  [-0.3547  -0.0124]\n  [-0.3915  -0.5958]] \n```", "```py\n>>> X_train_lda = X_train_std.dot(w)\n>>> colors = ['r', 'b', 'g']\n>>> markers = ['s', 'x', 'o']\n>>> for l, c, m in zip(np.unique(y_train), colors, markers):\n...     plt.scatter(X_train_lda[y_train==l, 0],\n...                 X_train_lda[y_train==l, 1] * (-1),\n...                 c=c, label=l, marker=m)\n>>> plt.xlabel('LD 1')\n>>> plt.ylabel('LD 2')\n>>> plt.legend(loc='lower right')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> # the following import statement is one line\n>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n>>> lda = LDA(n_components=2)\n>>> X_train_lda = lda.fit_transform(X_train_std, y_train) \n```", "```py\n>>> lr = LogisticRegression(multi_class='ovr', random_state=1,\n...                         solver='lbfgs')\n>>> lr = lr.fit(X_train_lda, y_train)\n>>> plot_decision_regions(X_train_lda, y_train, classifier=lr)\n>>> plt.xlabel('LD 1')\n>>> plt.ylabel('LD 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> X_test_lda = lda.transform(X_test_std)\n>>> plot_decision_regions(X_test_lda, y_test, classifier=lr)\n>>> plt.xlabel('LD 1')\n>>> plt.ylabel('LD 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy import exp\nfrom scipy.linalg import eigh\nimport numpy as np\ndef rbf_kernel_pca(X, gamma, n_components):\n    \"\"\"\n    RBF kernel PCA implementation.\n\n    Parameters\n    ------------\n    X: {NumPy ndarray}, shape = [n_examples, n_features]\n\n    gamma: float\n        Tuning parameter of the RBF kernel\n\n    n_components: int\n        Number of principal components to return\n\n    Returns\n    ------------\n    X_pc: {NumPy ndarray}, shape = [n_examples, k_features]\n        Projected dataset\n\n    \"\"\"\n    # Calculate pairwise squared Euclidean distances\n    # in the MxN dimensional dataset.\n    sq_dists = pdist(X, 'sqeuclidean')\n\n    # Convert pairwise distances into a square matrix.\n    mat_sq_dists = squareform(sq_dists)\n\n    # Compute the symmetric kernel matrix.\n    K = exp(-gamma * mat_sq_dists)\n\n    # Center the kernel matrix.\n    N = K.shape[0]\n    one_n = np.ones((N,N)) / N\n    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n\n    # Obtaining eigenpairs from the centered kernel matrix\n    # scipy.linalg.eigh returns them in ascending order\n    eigvals, eigvecs = eigh(K)\n    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n\n    # Collect the top k eigenvectors (projected examples)\n    X_pc = np.column_stack([eigvecs[:, i]\n                           for i in range(n_components)])\n\n    return X_pc \n```", "```py\n>>> from sklearn.datasets import make_moons\n>>> X, y = make_moons(n_samples=100, random_state=123)\n>>> plt.scatter(X[y==0, 0], X[y==0, 1],\n...             color='red', marker='^', alpha=0.5)\n>>> plt.scatter(X[y==1, 0], X[y==1, 1],\n...             color='blue', marker='o', alpha=0.5)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.decomposition import PCA\n>>> scikit_pca = PCA(n_components=2)\n>>> X_spca = scikit_pca.fit_transform(X)\n>>> fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7,3))\n>>> ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1],\n...               color='red', marker='^', alpha=0.5)\n>>> ax[0].scatter(X_spca[y==1, 0], X_spca[y==1, 1],\n...               color='blue', marker='o', alpha=0.5)\n>>> ax[1].scatter(X_spca[y==0, 0], np.zeros((50,1))+0.02,\n...               color='red', marker='^', alpha=0.5)\n>>> ax[1].scatter(X_spca[y==1, 0], np.zeros((50,1))-0.02,\n...               color='blue', marker='o', alpha=0.5)\n>>> ax[0].set_xlabel('PC1')\n>>> ax[0].set_ylabel('PC2')\n>>> ax[1].set_ylim([-1, 1])\n>>> ax[1].set_yticks([])\n>>> ax[1].set_xlabel('PC1')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n>>> fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n>>> ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1],\n...               color='red', marker='^', alpha=0.5)\n>>> ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],\n...               color='blue', marker='o', alpha=0.5)\n>>> ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1))+0.02,\n...               color='red', marker='^', alpha=0.5)\n>>> ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1))-0.02,\n...               color='blue', marker='o', alpha=0.5)\n>>> ax[0].set_xlabel('PC1')\n>>> ax[0].set_ylabel('PC2')\n>>> ax[1].set_ylim([-1, 1])\n>>> ax[1].set_yticks([])\n>>> ax[1].set_xlabel('PC1')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.datasets import make_circles\n>>> X, y = make_circles(n_samples=1000,\n...                     random_state=123, noise=0.1,\n...                     factor=0.2)\n>>> plt.scatter(X[y == 0, 0], X[y == 0, 1],\n...             color='red', marker='^', alpha=0.5)\n>>> plt.scatter(X[y == 1, 0], X[y == 1, 1],\n...             color='blue', marker='o', alpha=0.5)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> scikit_pca = PCA(n_components=2)\n>>> X_spca = scikit_pca.fit_transform(X)\n>>> fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7,3))\n>>> ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1],\n...               color='red', marker='^', alpha=0.5)\n>>> ax[0].scatter(X_spca[y==1, 0], X_spca[y==1, 1],\n...               color='blue', marker='o', alpha=0.5)\n>>> ax[1].scatter(X_spca[y==0, 0], np.zeros((500,1))+0.02,\n...               color='red', marker='^', alpha=0.5)\n>>> ax[1].scatter(X_spca[y==1, 0], np.zeros((500,1))-0.02,\n...               color='blue', marker='o', alpha=0.5)\n>>> ax[0].set_xlabel('PC1')\n>>> ax[0].set_ylabel('PC2')\n>>> ax[1].set_ylim([-1, 1])\n>>> ax[1].set_yticks([])\n>>> ax[1].set_xlabel('PC1')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n>>> fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3))\n>>> ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1],\n...               color='red', marker='^', alpha=0.5)\n>>> ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],\n...               color='blue', marker='o', alpha=0.5)\n>>> ax[1].scatter(X_kpca[y==0, 0], np.zeros((500,1))+0.02,\n...               color='red', marker='^', alpha=0.5)\n>>> ax[1].scatter(X_kpca[y==1, 0], np.zeros((500,1))-0.02,\n...               color='blue', marker='o', alpha=0.5)\n>>> ax[0].set_xlabel('PC1')\n>>> ax[0].set_ylabel('PC2')\n>>> ax[1].set_ylim([-1, 1])\n>>> ax[1].set_yticks([])\n>>> ax[1].set_xlabel('PC1')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy import exp\nfrom scipy.linalg import eigh\nimport numpy as np\ndef rbf_kernel_pca(X, gamma, n_components):\n    \"\"\"\n    RBF kernel PCA implementation.\n\n    Parameters\n    ------------\n    X: {NumPy ndarray}, shape = [n_examples, n_features]\n\n    gamma: float\n        Tuning parameter of the RBF kernel\n\n    n_components: int\n        Number of principal components to return\n\n    Returns\n    ------------\n    alphas {NumPy ndarray}, shape = [n_examples, k_features]\n        Projected dataset\n\n    lambdas: list\n        Eigenvalues\n\n    \"\"\"\n    # Calculate pairwise squared Euclidean distances\n    # in the MxN dimensional dataset.\n    sq_dists = pdist(X, 'sqeuclidean')\n\n    # Convert pairwise distances into a square matrix.\n    mat_sq_dists = squareform(sq_dists)\n\n    # Compute the symmetric kernel matrix.\n    K = exp(-gamma * mat_sq_dists)\n\n    # Center the kernel matrix.\n    N = K.shape[0]\n    one_n = np.ones((N,N)) / N\n    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n\n    # Obtaining eigenpairs from the centered kernel matrix\n    # scipy.linalg.eigh returns them in ascending order\n    eigvals, eigvecs = eigh(K)\n    eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n\n    # Collect the top k eigenvectors (projected examples)\n    alphas = np.column_stack([eigvecs[:, i]\n                             for i in range(n_components)])\n\n    # Collect the corresponding eigenvalues\n    lambdas = [eigvals[i] for i in range(n_components)]\n    return alphas, lambdas \n```", "```py\n>>> X, y = make_moons(n_samples=100, random_state=123)\n>>> alphas, lambdas = rbf_kernel_pca(X, gamma=15, n_components=1) \n```", "```py\n>>> x_new = X[25]\n>>> x_new\narray([ 1.8713187 ,  0.00928245])\n>>> x_proj = alphas[25] # original projection\n>>> x_proj\narray([ 0.07877284])\n>>> def project_x(x_new, X, gamma, alphas, lambdas):\n...     pair_dist = np.array([np.sum(\n...                 (x_new-row)**2) for row in X])\n...     k = np.exp(-gamma * pair_dist)\n...     return k.dot(alphas / lambdas) \n```", "```py\n>>> x_reproj = project_x(x_new, X,\n...            gamma=15, alphas=alphas,\n...            lambdas=lambdas)\n>>> x_reproj\narray([ 0.07877284]) \n```", "```py\n>>> plt.scatter(alphas[y==0, 0], np.zeros((50)),\n...             color='red', marker='^',alpha=0.5)\n>>> plt.scatter(alphas[y==1, 0], np.zeros((50)),\n...             color='blue', marker='o', alpha=0.5)\n>>> plt.scatter(x_proj, 0, color='black',\n...             label='Original projection of point X[25]',\n...             marker='^', s=100)\n>>> plt.scatter(x_reproj, 0, color='green',\n...             label='Remapped point X[25]',\n...             marker='x', s=500)\n>>> plt.yticks([], [])\n>>> plt.legend(scatterpoints=1)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.decomposition import KernelPCA\n>>> X, y = make_moons(n_samples=100, random_state=123)\n>>> scikit_kpca = KernelPCA(n_components=2,\n...               kernel='rbf', gamma=15)\n>>> X_skernpca = scikit_kpca.fit_transform(X) \n```", "```py\n>>> plt.scatter(X_skernpca[y==0, 0], X_skernpca[y==0, 1],\n...             color='red', marker='^', alpha=0.5)\n>>> plt.scatter(X_skernpca[y==1, 0], X_skernpca[y==1, 1],\n...             color='blue', marker='o', alpha=0.5)\n>>> plt.xlabel('PC1')\n>>> plt.ylabel('PC2')\n>>> plt.tight_layout()\n>>> plt.show() \n```"]
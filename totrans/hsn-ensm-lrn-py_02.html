<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">A Machine Learning Refresher</h1>
                </header>
            
            <article>
                
<p class="mce-root">Machine learning is a sub field of <strong>artificial intelligence</strong> (<strong>AI</strong>) focused on the aim of developing algorithms and techniques that enable computers to learn from massive amounts of data. Given the increasing rate at which data is produced, machine learning has played a critical role in solving difficult problems in recent years. This success was the main driving force behind the funding and development of many great machine learning libraries that make use of data in order to build predictive models. Furthermore, businesses have started to realize the potential of machine learning, driving the demand for data scientists and machine learning engineers to new heights, in order to design better-performing predictive models.</p>
<p class="mce-root">This chapter serves as a refresher on the main concepts and terminology, as well as an introduction to the frameworks that will be used throughout the book, in order to approach ensemble learning with a solid foundation.</p>
<p class="mce-root">The main topics covered in this chapter are the following:</p>
<ul>
<li>The various machine learning problems and datasets</li>
<li>How to evaluate the performance of a predictive model</li>
<li>Machine learning algorithms</li>
<li>Python environment setup and the required libraries</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p> <a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter01">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter01</a></p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/30u8sv8">http://bit.ly/30u8sv8</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning from data</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">Data is the raw ingredient of machine learning. Processing data can produce information; for example, measuring the height of a portion of a school's students (data) and calculating their average (processing) can give us an idea of the whole school's height (information). If we process the data further, for example, by grouping males and females and calculating two averages – one for each group, we will gain more information, as we will have an idea about the average height of the school's males and females. Machine learning strives to produce the most information possible from any given data. In this example, we produced a very basic predictive model. By calculating the two averages, we can predict the average height of any student just by knowing whether the student is male or female.</p>
<p class="NormalPACKT">The set of data that a machine learning algorithm is tasked with processing is called the problem's <span class="KeyWordPACKT">dataset</span>. In our example, the dataset consists of height measurements (in centimeters) and the child's sex (male/female). In machine learning, input variables are called <span class="KeyWordPACKT">features</span> and output variables are called <span class="KeyWordPACKT">targets</span>. In this dataset, the features of our predictive model consist solely of the students' sex, while our target is the students' height in centimeters. The predictive model that is produced and maps features to targets will be referred to as simply the <span class="KeyWordPACKT">model <span>from now on</span></span>, unless otherwise specified. Each data point is called an <span class="KeyWordPACKT">instance</span>. In this problem, each student is an instance of the dataset.</p>
<p class="NormalPACKT">When the target is a continuous variable (a number), it presents a <span class="KeyWordPACKT">regression</span> problem, as the aim is to regress the target on the features. When the target is a set of categories, it presents a <span class="KeyWordPACKT">classification</span> problem, as we try to assign each instance to a category or <span class="KeyWordPACKT">class</span>.</p>
<p class="NormalPACKT">Note that, in classification problems, the target class can be represented by a number; this does not mean that it is a regression problem. The most useful way to determine whether it is a regression problem is to think about whether the instances can be ordered by their targets. In our example, the target is height, so we can order the students from tallest to shortest, as 100 cm is less than 110 cm. As a counter example, if the target was their favorite color, we could represent each color by a number, but we could not order them. Even if we represented red as one and blue as two, we could not say that red is "before" or "less than" blue. Thus, this counter example is a classification problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Popular machine learning datasets</h1>
                </header>
            
            <article>
                
<p>Machine learning relies on data in order to produce high-performing models. Without data, it's not even possible to create models. In this section, we'll present some popular machine learning datasets, which we will utilize throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Diabetes</h1>
                </header>
            
            <article>
                
<p>The diabetes dataset concerns 442 individual diabetes patients and the progression of the disease one year after a baseline measurement. The dataset consists of 10 features, which are the patient's age, sex, <strong>body mass index</strong> (<strong>bmi</strong>), average <strong>blood pressure</strong> (<strong>bp</strong>), and six measurements of their blood serum. The dataset target is the progression of the disease one year after the baseline measurement. This is a regression dataset, as the target is a number.</p>
<p> In this book, the dataset features are mean-centered and scaled such that the dataset sum of squares for each feature equals one. The following table depicts a sample of the diabetes dataset:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 46px">
<p class="CDPAlignCenter CDPAlign"><strong>age</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p><strong>sex</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p><strong>bmi</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p><strong>bp</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p><strong>s1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p><strong>s2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p><strong>s3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p><strong>s4</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p><strong>s5</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p><strong>s6</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 48px">
<p><strong>target</strong></p>
</td>
</tr>
<tr>
<td style="width: 46px">
<p class="CDPAlignCenter CDPAlign">0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>0.05</p>
</td>
<td style="width: 45px">
<p class="CDPAlignCenter CDPAlign">0.06</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>0.02</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>-0.03</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>0.00</p>
</td>
<td style="width: 45px">
<p class="CDPAlignCenter CDPAlign">0.02</p>
</td>
<td style="width: 46px">
<p>-0.02</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 48px">
<p>151</p>
</td>
</tr>
<tr>
<td style="width: 46px">
<p class="CDPAlignCenter CDPAlign">0.00</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>-0.05</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.03</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.01</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>-0.02</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>0.07</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>-0.07</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.09</p>
</td>
<td style="width: 48px">
<p class="CDPAlignCenter CDPAlign">75</p>
</td>
</tr>
<tr>
<td style="width: 46px">
<p class="CDPAlignCenter CDPAlign">0.09</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>0.05</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.01</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.05</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>-0.03</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.03</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>0.00</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>0.00</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.03</p>
</td>
<td style="width: 48px">
<p class="CDPAlignCenter CDPAlign">141</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.09</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>-0.01</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>0.01</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>0.02</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>-0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p>0.03</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 45px">
<p>0.02</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 46px">
<p class="CDPAlignCenter CDPAlign">-0.01</p>
</td>
<td style="width: 48px">
<p class="CDPAlignCenter CDPAlign">206</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breast cancer</h1>
                </header>
            
            <article>
                
<p>The breast cancer dataset concerns 569 biopsies of malignant and benign tumors. The dataset provides 30 features extracted from images of fine-needle aspiration biopsies that describe cell nuclei. The images provide information about the shape, size, and texture of each cell nucleus. Furthermore, for each characteristic, three distinct values are provided. The mean, the standard error, and the worst or largest value. This ensures that, for each image, the cell population is adequately described.</p>
<p>The dataset target concerns the diagnosis, that is, whether a tumor is malignant or benign. Thus, this is a classification dataset. The available features are listed as follows:</p>
<ul>
<li>Mean radius</li>
<li>Mean texture</li>
<li>Mean perimeter</li>
<li>Mean area</li>
<li>Mean smoothness</li>
<li>Mean compactness</li>
<li>Mean concavity</li>
<li>Mean concave points</li>
<li>Mean symmetry</li>
<li>Mean fractal dimension</li>
<li>Radius error</li>
<li>Texture error</li>
<li>Perimeter error</li>
<li>Area error</li>
<li>Smoothness error</li>
<li>Compactness error</li>
<li>Concavity error</li>
<li>Concave points error</li>
<li>Symmetry error</li>
<li>Fractal dimension error</li>
<li>Worst radius</li>
<li>Worst texture</li>
<li>Worst perimeter</li>
<li>Worst area</li>
<li>Worst smoothness</li>
<li>Worst compactness</li>
<li>Worst concavity</li>
<li>Worst concave points</li>
<li>Worst symmetry</li>
<li>Worst fractal dimension</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handwritten digits</h1>
                </header>
            
            <article>
                
<p>The MNIST handwritten digit dataset is one of the most famous image recognition datasets. It consists of square images, 8 x 8 pixels, each containing a single handwritten digit. Thus, the dataset features are an 8 by 8 matrix, containing each pixel's color in grayscale. The target consists of 10 classes, one for each digit from 0 to 9. This is a classification dataset. The following figure is a <span>sample from the handwritten digit dataset</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-567 image-border" src="assets/67c53ed8-f82c-4835-9794-93a597511b7a.png" style="width:37.50em;height:21.42em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span>Sample of the handwritten digit dataset</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised and unsupervised learning</h1>
                </header>
            
            <article>
                
<p>Machine learning can be divided into many subcategories; two broad categories are supervised and unsupervised learning. These categories contain some of the most popular and widely used machine learning methods. In this section, we present them, as well as some toy example uses of supervised and unsupervised learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In examples such as those in the previous section, the data consisted of some features and a target; no matter whether the target was quantitative (regression) or categorical (classification). Under these circumstances, we call the dataset a labeled dataset. When we try to produce a model from a labeled dataset in order to make predictions about unseen or future data (for example, to diagnose a new tumor case), we make use of supervised learning. In simple cases, supervised learning models can be visualized as a line. This line's purpose is to either separate the data based on the target (in classification) or to closely follow the data (in regression).</p>
<p class="mce-root">The following figure illustrates a simple regression example. Here, <em>y</em> is the target and <em>x</em> is the dataset feature. Our model consists of the simple equation <em>y</em>=2<em>x</em>-5. As is evident, the line closely follows the data. In order to estimate the <em>y</em> value of a new unseen point, we calculate its value using the preceding formula. The following f<span>igure</span> shows a simple regression with <em>y</em>=2<em>x</em>-5 as the predictive model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-568 image-border" src="assets/b1680c76-c8ad-4c1f-92f7-7091d33a1870.png" style="width:28.58em;height:22.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Simple regression with <span>y=2x-5</span> as the predictive model</div>
<p class="mce-root">In the following figure, a simple classification problem is depicted. Here, the dataset features are <em>x</em> and <em>y</em>, while the target is the instance color. Again, the dotted line is <em>y</em>=2<em>x</em>-5, but this time we test whether the point is above or below the line. If the point's <em>y</em> value is lower than expected (smaller), then we expect it to be orange. If it is higher (greater), we expect it to be blue. The following <span>figure</span> is a simple classification with <em>y</em>=2<em>x</em>-5 as the boundary:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-569 image-border" src="assets/210bb1bf-db70-4be2-ba9c-a344b16a1bf7.png" style="width:25.67em;height:20.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Simple classification with y=2x-5 as boundary</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In both regression and classification, we have a clear understanding of how the data is structured or how it behaves. Our goal is to simply model that structure or behavior. In some cases, we do not know how the data is structured. In those cases, we can utilize unsupervised learning in order to discover the structure, and thus information, within the data. The simplest form of unsupervised learning is clustering. As the name implies, clustering techniques attempt to group (or cluster) data instances. Thus, instances that belong to the same cluster share many similarities in their features, while they are dissimilar to instances that belong in separate clusters. A simple example with three clusters is depicted in the following figure. Here, the dataset features are <em>x</em> and <em>y</em>, while there is no target.</p>
<p class="mce-root">The clustering algorithm discovered three distinct groups, centered around the points (0, 0), (1, 1), and (2, 2):</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img class="aligncenter size-full wp-image-570 image-border" src="assets/c0ab1932-b0fd-4959-b37a-583d2257a464.png" style="width:25.25em;height:19.58em;"/></div>
<div class="CDPAlignCenter packt_figref CDPAlign">Clustering with three distinct groups</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction</h1>
                </header>
            
            <article>
                
<p class="mce-root">Another form of unsupervised learning is dimensionality reduction. The number of features present in a dataset equals the dataset's dimensions. Often, many features can be correlated, noisy, or simply not provide much information. Nonetheless, the cost of storing and processing data is correlated with a dataset's dimensionality. Thus, by reducing the dataset's dimensions, we can help the algorithms to better model the data.</p>
<p class="mce-root">Another use of dimensionality reduction is for the visualization of high-dimensional datasets. For example, using the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm, we can reduce the breast cancer dataset to two dimensions or components. Although it is not easy to visualize 30 dimensions, it is quite easy to visualize two.</p>
<p class="mce-root">Furthermore, we can visually test whether the information contained within the dataset can be utilized to separate the dataset's classes or not. The next figure depicts the two components on the <em>y</em> and <em>x</em> axis, while the color represents the instance's class. Although we cannot plot all of the dimensions, by plotting the two components, we can conclude that a degree of separability between the classes exists:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-571 image-border" src="assets/a007fc1c-de3b-4a3f-9f7f-afeea4c6546e.png" style="width:27.92em;height:21.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Using t-SNE to reduce the dimensionality of the breast cancer dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance measures</h1>
                </header>
            
            <article>
                
<p class="mce-root">Machine learning is a highly quantitative field. Although we can gauge the performance of a model by plotting how it separates classes and how closely it follows data, more quantitative performance measures are needed in order to evaluate models. In this section, we present cost functions and metrics. Both of them are used in order to assess a model's performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cost functions</h1>
                </header>
            
            <article>
                
<p>A machine learning model's objective is to model our dataset. In order to assess each model's performance, we define an objective function. These functions usually express a cost, or how far from perfect a model is. These cost functions usually utilize a loss function to assess how well the model performed on each individual dataset instance.</p>
<p>Some of the most widely used cost functions are described in the following sections, assuming that the dataset has<span> </span><em>n</em><span> </span>instances, the target's true value for instance<span> </span><em>i</em><span> </span>is<span> </span><em>t<sub>i</sub></em><span> </span>and the model's output is<span> </span><em>y<sub>i .</sub></em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean absolute error</h1>
                </header>
            
            <article>
                
<p><strong>Mean absolute error</strong> (<strong>MAE</strong>) or L1 loss is the mean absolute distance between the target's real values and the model's outputs. It is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e08a7ef6-713b-408c-af9f-ae0a55486bc5.png" style="width:11.58em;height:3.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean squared error</h1>
                </header>
            
            <article>
                
<p><strong>Mean squared error</strong> (<strong>MSE</strong>) or L2 loss is the mean squared distance between the target's real values and the model's output. It is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3233ee2b-4852-4c38-baeb-1c1482ac59b7.png" style="width:13.08em;height:3.92em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cross entropy loss</h1>
                </header>
            
            <article>
                
<p>Cross entropy loss is used in models that output probabilities between 0 and 1, usually to express the probability that an instance is a member of a specific class. As the output probability diverges from the actual label, the loss increases. For a simple case where the dataset consists of two classes, it is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ca72266c-12e1-4d41-828d-049d20b0a341.png" style="width:21.08em;height:1.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Metrics</h1>
                </header>
            
            <article>
                
<p>Cost functions are useful when we try to numerically optimize our models. But as humans, we need metrics that are useful and intuitive to understand and report. As such, there are a number of metrics available that give insight into a model's performance. The most common metrics are presented in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification accuracy</h1>
                </header>
            
            <article>
                
<p>The simplest and easiest to grasp of all, classification accuracy refers to the percentage of correct predictions. In order to calculate accuracy, we divide the number of correct predictions by the total number of instances:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/be71cf1e-4842-4b5c-888f-7226840eb631.png" style="width:15.58em;height:2.42em;"/></p>
<p>In order for accuracy to hold any substantial value, the dataset must contain an equal number of instances belonging to each class. If the dataset is unbalanced, accuracy will be affected. For example, if a dataset consists of 90% class A and 10% class B, a model that predicts each instance as class A will have 90% accuracy, although it will hold zero predictive power.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confusion matrix</h1>
                </header>
            
            <article>
                
<p>In order to tackle the preceding problem, it is possible to utilize a confusion matrix. Confusion matrices present the number of instances correctly or incorrectly predicted as each possible class. In a dataset with only two classes (Yes and No), a confusion matrix has the following form:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 74px">
<p class="CDPAlignCenter CDPAlign"><strong>n = 200</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 93px">
<p><strong>Predicted: Yes</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 90px">
<p><strong>Predicted: No</strong></p>
</td>
</tr>
<tr>
<td style="width: 74px">
<p class="CDPAlignCenter CDPAlign"><strong>Target: Yes</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 93px">
<p>80</p>
</td>
<td style="width: 90px">
<p class="CDPAlignCenter CDPAlign">70</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 74px">
<p><strong>Target: No</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 93px">
<p>20</p>
</td>
<td style="width: 90px">
<p class="CDPAlignCenter CDPAlign">30</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>There are four cells, each corresponding to one of the following:</p>
<ul>
<li><strong>True Positives</strong> (<strong>TP</strong>): When the target belongs to the Yes class and the model predicted Yes</li>
<li><strong>True Negatives</strong> (<strong>TN</strong>): When the target belongs to the No class and the model predicted No</li>
<li><strong>False Positives</strong> (<strong>FP</strong>): When the target belongs to the No class and the model predicted Yes</li>
<li><strong>False Negatives</strong> (<strong>FN</strong>): When the target belongs to the Yes class and the model predicted No</li>
</ul>
<p>Confusion matrices provide information about the balance of the true and predicted classes. In order to calculate the accuracy from a confusion matrix, we divide the sum of TP and TN by the total number of instances:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/56ea3bcb-3940-47ff-bae2-6699fa045bf1.png" style="width:10.25em;height:2.25em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sensitivity, specificity, and area under the curve</h1>
                </header>
            
            <article>
                
<p>Area under the curve (AUC) concerns binary classification datasets, and it depicts the probability that the model will rank any given instance correctly. In order to define it, we must first define sensitivity and specificity:</p>
<ul>
<li><strong>Sensitivity</strong> (<strong>True Positive Rate</strong>): Sensitivity is the percentage of positive instances correctly predicted as positive, relative to all positive instances. It is calculated as follows:</li>
</ul>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/44038c3e-a8c4-466f-8c94-001702317038.png" style="width:12.00em;height:2.50em;"/></p>
<ul>
<li><strong>Specificity</strong> (<strong>False Positive Rate</strong>): Specificity is the percentage of negative instances incorrectly predicted as positive, relative to all negative instances. It is calculated as follows:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f1bd2469-3805-4932-86c7-e075947d82ce.png" style="width:11.58em;height:2.42em;"/></p>
<p>By iteratively computing (1-specificity) and sensitivity at specific intervals (for example, in 0.05 increments), we can see how the model behaves. The intervals concern the model's output probability for each instance; for example, we first compute them for all instances with an estimated probability of belonging to the Yes class of less than 0.05. Then, we re-compute for all instances with an estimated probability of less than 0.1 and so on. The result is depicted here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-572 image-border" src="assets/289e210c-25ec-47c6-a2be-a45556c5018e.png" style="width:33.00em;height:25.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Receiver operator characteristic curve</div>
<p>The straight line represents an equal probability of ranking an instance correctly or incorrectly: a random model. The orange line (ROC curve) depicts the model's probability. If the ROC curve is below the straight line, it means that the model performs worse than a random, uninformed model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Precision, recall, and the F1 score</h1>
                </header>
            
            <article>
                
<p>Precision gauges how a model behaves by quantifying the percentage of instances correctly classified as a specific class, relative to all instances predicted as the same class. It is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/996f726a-e333-4ba7-b981-460abc3ff77a.png" style="width:10.75em;height:2.42em;"/></p>
<p>Recall is another name for sensitivity. The harmonic mean of precision and recall is called the F1 score and is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/89a6f1fc-1249-429a-802a-e77de45ad214.png" style="width:12.42em;height:2.42em;"/></p>
<p>The reason to use the harmonic mean instead of a simple average is that the harmonic mean is greatly affected by imbalances between the two values (precision and recall). Thus, if either precision or recall is significantly smaller than the other, the F1 score will reflect this imbalance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating models</h1>
                </header>
            
            <article>
                
<p>Although there are various metrics that indicate a model's performance, it is important to carefully set the testing environment. One of the most important things is to split the dataset into two parts. One part of the dataset will be utilized by the algorithm in order to generate a model; the second part will be utilized to assess the model. These are usually called the train and test set.</p>
<p>The train set is available to the algorithm to generate and optimize a model, using any cost function. After the algorithm is finished, the produced model is tested on the test set, in order to assess its predictive ability on unseen data. While the algorithm may produce a model that performs well on the train set (in-sample performance), it may not be able to generalize and perform as well on the test set (out-of-sample performance). This can be attributed to many factors – covered in the next chapter. Some of the problems that arise can be tackled with the use of ensembles. Nonetheless, if the algorithm is presented with low-quality data, there is little that can be done to improve out-of-sample performance.</p>
<p>In order to obtain a fair estimate, we sometimes iteratively split different parts of a dataset into fixed-size train and test sets, say, 90% train and 10% test, until we have tested the whole dataset. This is called K-fold cross validation. In the case of a 90% to 10% split, it is called 10-fold cross validation, because we need to perform it 10 times in order to get an estimate for the whole dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning algorithms</h1>
                </header>
            
            <article>
                
<p><span>There are a number of machine learning algorithms, for both supervised and unsupervised learning. In this book, we will cover some of the most popular algorithms that can be utilized within ensembles. In this chapter, we will go over the key concepts behind each algorithm, the basic algorithms, and the libraries that implement them in Python.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Python packages</h1>
                </header>
            
            <article>
                
<p class="mce-root">In order to leverage the power of any programming language, libraries are essential. They provide a convenient and tested implementation of many algorithms. In this book, we will be using Python 3.6 along with the following libraries: NumPy, for its excellent implementation of numerical operators and matrices; Pandas, for its convenient data manipulation methods; Matplotlib, to visualize our data; scikit-learn, for its excellent implementations of various machine learning algorithms, and Keras to build neural networks, utilizing its Pythonic, intuitive interface. Keras is an interface for other frameworks, such as TensorFlow, PyTorch, and Theano. The specific versions of each library used in this book are listed as follows:</p>
<ul>
<li>numpy==1.15.1</li>
<li>pandas==0.23.4</li>
<li>scikit-learn==0.19.1</li>
<li>matplotlib==2.2.2</li>
<li>Keras==2.2.4</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning algorithms</h1>
                </header>
            
            <article>
                
<p>The most common class of machine learning algorithm is supervised learning algorithms. These concern problems where data has a known structure. This means that each data point has a specific value related to it that we wish to model or predict.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression</h1>
                </header>
            
            <article>
                
<p>Regression is one of the simplest machine learning algorithms. The <strong>Ordinary Least Squares</strong> (<strong>OLS</strong>) regression of the form <em>y=ax+b </em>attempts to optimize the <em>a</em> and <em>b</em> parameters in order to fit the data. It uses MSE as its cost function. As the name implies, it is able to solve regression problems.</p>
<p>We can use the scikit-learn implementation of OLS to try and model the diabetes dataset (the dataset is provided with the library):</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn import metrics<br/>diabetes = load_diabetes()</pre>
<p class="mceNonEditable"/>
<p class="mce-root">The first section deals with <span><span>importing libraries</span></span> and loading data. We use the <kbd>LinearRegression</kbd> implementation that exists in the <kbd>linear_model</kbd> package:</p>
<pre class="mce-root"># --- SECTION 2 ---<br/># Split the data into train and test set<br/>train_x, train_y = diabetes.data[:400], diabetes.target[:400]<br/>test_x, test_y = diabetes.data[400:], diabetes.target[400:]</pre>
<p class="mce-root">The second section splits the data into a train and a test set. For this example, we used the first 400 instances as the train set and the other 42 as the test set:</p>
<pre class="mce-root"># --- SECTION 3 ---<br/># Instantiate, train and evaluate the model<br/>ols = LinearRegression()<br/>ols.fit(train_x, train_y)<br/>err = metrics.mean_squared_error(test_y, ols.predict(test_x))<br/>r2 = metrics.r2_score(test_y, ols.predict(test_x))</pre>
<p class="mce-root">The next section instantiates a linear regression object with <kbd>ols = LinearRegression()</kbd>. It then optimizes the parameters, or fits the model with our training instances, using <kbd>ols.fit(train_x, train_y)</kbd>. Finally, by using the <kbd>metrics</kbd> package, we calculate the MSE and <em>R<sup>2</sup></em> of our model, using the test data in Section 4:</p>
<pre class="mce-root"># --- SECTION 4 ---<br/># Print the model<br/>print('---OLS on diabetes dataset.---')<br/>print('Coefficients:')<br/>print('Intercept (b): %.2f'%ols.intercept_)<br/>for i in range(len(diabetes.feature_names)):<br/> print(diabetes.feature_names[i]+': %.2f'%ols.coef_[i])<br/>print('-'*30)<br/>print('R-squared: %.2f'%r2, ' MSE: %.2f \n'%err)</pre>
<p class="mce-root">The code's output is the following:</p>
<pre class="mce-root">---OLS on diabetes dataset.---<br/>Coefficients:<br/>Intercept (b): 152.73<br/>age: 5.03<br/>sex: -238.41<br/>bmi: 521.63<br/>bp: 299.94<br/>s1: -752.12<br/>s2: 445.15<br/>s3: 83.51<br/>s4: 185.58<br/>s5: 706.47<br/>s6: 88.68<br/>------------------------------<br/>R-squared: 0.70 MSE: 1668.75</pre>
<p class="mce-root">Another form of regression, logistic regression, attempts to model the probability that an instance belongs to one of two classes. Again, it attempts to optimize the <em>a</em> and <em>b</em> <span>parameters </span>in order to model <em>p=1/(1+e<sup>-(ax+b)</sup>)</em> . Once again, using scikit-learn and the breast cancer dataset, we can create and evaluate a simple logistic regression. The following code sections are similar to the preceding ones, but this time we'll use classification accuracy and a confusion matrix rather than <em>R<sup>2</sup></em> as a metric:</p>
<pre class="mce-root"># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn import metrics<br/>bc = load_breast_cancer()<br/><br/># --- SECTION 2 ---<br/># Split the data into train and test set<br/>train_x, train_y = bc.data[:400], bc.target[:400]<br/>test_x, test_y = bc.data[400:], bc.target[400:]<br/><br/># --- SECTION 3 ---<br/># Instantiate, train and evaluate the model<br/>logit = LogisticRegression()<br/>logit.fit(train_x, train_y)<br/>acc = metrics.accuracy_score(test_y, logit.predict(test_x))<br/><br/># --- SECTION 4 ---<br/># Print the model<br/>print('---Logistic Regression on breast cancer dataset.---')<br/>print('Coefficients:')<br/>print('Intercept (b): %.2f'%logit.intercept_)<br/>for i in range(len(bc.feature_names)):<br/> print(bc.feature_names[i]+': %.2f'%logit.coef_[0][i])<br/>print('-'*30)<br/>print('Accuracy: %.2f \n'%acc)<br/>print(metrics.confusion_matrix(test_y, logit.predict(test_x)))</pre>
<p class="mce-root"/>
<p class="mce-root">The test classification accuracy achieved with this model is 95%, which is quite good. Furthermore, the confusion matrix that follows here indicates that the model does not try to take advantage of class imbalances. Later in this book, we will learn how to further increase the classification accuracy with the use of ensemble methods. The following table shows the logit model confusion matrix:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>n = 169</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Predicted: Malignant</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Predicted: Benign</strong></p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Target: Malignant</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>38</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">1</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Target: Benign</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>8</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">122</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machines</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT"><span class="KeyWordPACKT">Support vector machines</span> or <span class="KeyWordPACKT">SVMs</span> use a subset of training data, specifically data points near the edge of each class, in order to define a separating hyperplane (in two dimensions, a line). These edge cases are called <span class="KeyWordPACKT">support vectors</span>. The goal of an SVM is to find the hyperplane that maximizes the margin (distance) between the support vectors (depicted in the following figure). In order to classify nonlinear separable classes, SVMs use the <span class="KeyWordPACKT">kernel trick</span> to map data in a higher dimensional space, where it can become linearly separable:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-573 image-border" src="assets/d9689d3f-38d0-478f-abdb-313232de0725.png" style="width:25.42em;height:19.83em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span>SVM margins and support vectors</span></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_tip">
<p>If you want to learn more about the kernel trick, this is a good starting point: <a href="https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick">https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick</a>.</p>
</div>
<p>In scikit-learn, an SVM is implemented under <kbd>sklearn.svm</kbd>, both for regression with <kbd>sklearn.svm.SVR</kbd> and classification with <kbd>sklearn.svm.SVC</kbd>. Once again, we'll test the algorithm's potential using scikit-learn and the code utilized in the regression examples. Using an SVM with a linear kernel on the breast cancer dataset results in 95% accuracy and the following confusion matrix:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>n = 169</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Predicted: Malignant</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Predicted: Benign</strong></p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Target: Malignant</strong></p>
</td>
<td>
<p class="CDPAlignCenter">39</p>
</td>
<td>
<p class="CDPAlignCenter">0</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Target: Benign</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">9</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">121</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>On the diabetes dataset, by fine-tuning the <em>C</em> parameter to 1,000 during the <kbd>(svr = SVR(kernel='linear', C=1e3))</kbd> <span>object instantiation, </span>we are able to achieve an R2 of 0.71 and an MSE of 1622.36, marginally better than the logit model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks</h1>
                </header>
            
            <article>
                
<p>Neural networks, inspired by the way biological brains are connected, consist of many neurons, or computational modules, organized in layers. Data is provided at the input layer and predictions are produced at the output layer. All intermediate layers are called hidden layers. Neurons that belong to the same layer are not connected to each other, only to neurons that belong in other layers. Each neuron can have multiple inputs, where each input is multiplied by a specific weight and the sum of multiplied inputs is passed to an activation function that defines the neuron's output. Common activation functions include the following:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 25%"><strong>Sigmoid</strong></td>
<td class="CDPAlignCenter CDPAlign" style="width: 24.4088%"><strong>Tanh</strong></td>
<td class="CDPAlignCenter CDPAlign" style="width: 23.5912%"><strong>ReLU</strong></td>
<td class="CDPAlignCenter CDPAlign" style="width: 21%"><strong>Linear</strong></td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign" style="width: 25%">    <img src="assets/15d85d5f-1a56-4f98-a267-9bec857704fa.png" style="width:8.75em;height:2.75em;"/></td>
<td style="width: 24.4088%">       <img class="alignnone size-full wp-image-914 image-border" src="assets/3b36b9f5-f2db-4490-b19a-977905bd2c45.png" style="width:8.50em;height:3.08em;"/></td>
<td style="width: 23.5912%">       <img class="alignnone size-full wp-image-915 image-border" src="assets/263f0f9f-6ebc-4155-a347-2f46288986d6.png" style="width:8.08em;height:1.83em;"/></td>
<td style="width: 21%">          <img class="alignnone size-full wp-image-916 image-border" src="assets/1c60ae33-f144-43ff-890a-0739f7e46fc1.png" style="width:4.75em;height:1.42em;"/></td>
</tr>
</tbody>
</table>
<p> </p>
<p>The network's goal is to optimize each neuron's weights, such that the cost function is minimized. Neural networks can be either used for regression, where the output layer consists of a single neuron, or classification, where it consists of many neurons, usually equal to the number of classes. There are a number of optimizing algorithms or optimizers available for neural networks. The most common is stochastic gradient descent or SGD. The main idea is that the weights are updated based on the direction and magnitude (first derivative) of the error's gradient, multiplied by a factor called the learning rate.</p>
<p>Variations and extensions have been proposed that take into account the second derivative, adapt the learning rate, or use the momentum of previous weight changes to update the weights.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Although the concept of neural networks has existed for a long time, recently their popularity has greatly increased with the advent of deep learning. Modern architectures consist of convolutional layers, where each layer's weights consist of matrices, and the output is calculated by sliding the weight matrix onto the input. Another type of layers, max pooling layers, calculates the output as the maximum input element again by sliding a fixed-size window onto the input. Recurrent layers retain information about their previous<br/>
states. Finally, fully connected layers are traditional neurons, as described previously.</p>
<p>Scikit-learn implements traditional neural networks, under the <kbd>sklearn.neural_network</kbd> package. Once again, using the preceding examples, we'll try to model the diabetes and breast cancer datasets. On the diabetes dataset, we'll use <kbd>MLPRegressor</kbd> with <strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>) as the optimizer, with <kbd>mlpr = MLPRegressor(solver='sgd')</kbd>. Without any further fine-tuning, we achieve an R<sup>2</sup> of 0.64 and an MSE of 1977. On the breast cancer dataset, using the <strong>Limited-memory Broyden–Fletcher–Goldfarb–Shanno</strong> (<strong>LBFGS</strong>) optimizer, with <kbd>mlpc = MLPClassifier(solver='lbfgs')</kbd>, we get a classification accuracy of 93% and a competent confusion matrix. The following table shows the neural network confusion matrix for the breast cancer dataset:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 120px">
<p class="CDPAlignCenter CDPAlign"><strong>n = 169</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 139px">
<p><strong>Predicted: Malignant</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 117px">
<p><strong>Predicted: Benign</strong></p>
</td>
</tr>
<tr>
<td style="width: 120px">
<p class="CDPAlignCenter CDPAlign"><strong>Target: Malignant</strong></p>
</td>
<td style="width: 139px">
<p class="CDPAlignCenter CDPAlign">35</p>
</td>
<td style="width: 117px">
<p class="CDPAlignCenter CDPAlign">4</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 120px">
<p><strong>Target: Benign</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 139px">
<p>8</p>
</td>
<td style="width: 117px">
<p class="CDPAlignCenter CDPAlign">122</p>
</td>
</tr>
</tbody>
</table>
<div class="mce-root packt_tip">A very important note on neural networks: the initial weights of a network are randomly initialized. Thus, the same code can perform differently if it is executed several times. In order to ensure non-random (non-stochastic) execution, the initial random state of the network must be fixed. The two scikit-learn classes implement this feature through the <kbd>random_state</kbd> parameter in the object constructor. In order to set the random state to a specific seed value, the constructor must be called as follows: <kbd>mlpc = MLPClassifier(solver='lbfgs', random_state=12418)</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision trees</h1>
                </header>
            
            <article>
                
<p>Decision trees are less of a black box than other machine learning algorithms. They can easily explain how they produce a prediction, which is called <strong>interpretability</strong>. The main concept is that they produce rules by splitting the training set using the provided features. By iteratively splitting the data, a tree form is produced, thus this is where their name derives from. Let's consider a dataset where the instances are individual persons deciding on their vacations.</p>
<p class="mce-root"/>
<p>The dataset features consist of the person's age and available money, while the target is their preferred destination, one of either <strong>Summer Camp</strong>, <strong>Lake</strong>, or <strong>Bahamas</strong>. A possible decision tree model is depicted in the following figure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-576 image-border" src="assets/f80c05fa-627e-4833-b9ae-d3ae34c8aa1e.png" style="width:32.17em;height:26.83em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Decision tree model for the vacation destination problem</div>
<p>As is evident, the model can explain how it produces any predictions. The way that the model itself is built is by trying to select the feature and threshold that maximize the information produced. Roughly, this means that the model will try to iteratively split the dataset in a way that separates the greatest number of remaining instances.</p>
<p>Although intuitive to understand, decision trees can produce unreasonable models, with the extreme being the generation of so many rules that, eventually, each rule combination leads to a single instance. In order to avoid such models, we can restrict the model by requiring that it does not exceed a specific depth (maximum number of consecutive rules), or that each node has at least a minimum number of instances before it can be further split.</p>
<p class="mce-root"/>
<p>In scikit-learn, decision trees are implemented under the <kbd>sklearn.tree</kbd> package, with <kbd>DecisionTreeClassifier</kbd> and <kbd>DecisionTreeRegressor</kbd>. In our examples, using <kbd>DecisionTreeRegressor</kbd> with <kbd>dtr = DecisionTreeRegressor(max_depth=2)</kbd>, we achieve an R<sup>2</sup> of 0.52 and an MSE of 2655. On the breast cancer dataset, using <kbd>dtc = DecisionTreeClassifier(max_depth=2)</kbd>, we achieve 89% accuracy and the following confusion matrix:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>n = 169</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Predicted: Malignant</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Predicted: Benign</strong></p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Target: Malignant</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">37</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">2</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Target: Benign</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>17</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">113</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Although not the best-performing algorithm so far, we can clearly see how each individual was classified, by exporting the tree to the <kbd>graphviz</kbd> format with <kbd>export_graphviz(dtc, feature_names=bc.feature_names, class_names=bc.target_names, impurity=False)</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9da5bb06-3901-4bb1-9903-c3e921d77f90.png" style="width:41.25em;height:23.42em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">The decision tree generated for the breast cancer dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-Nearest Neighbors</h1>
                </header>
            
            <article>
                
<p><strong>k-Nearest Neighbors</strong> (<strong>k-NN</strong>) is a relatively simple machine learning algorithm. Each instance is classified by comparing it to its K-nearest examples as the majority class. In regression, the average value of neighbors is used. Scikit-learn's implementation lies within the <kbd>sklearn.neighbors</kbd> package of the library. As it is the naming convention of the library, <kbd>KNeighborsClassifier</kbd> implements the classification and <kbd>KNeighborsRegressor</kbd> implements the regression version of the algorithm. Using them in our examples, the regressor generates an R<sup>2</sup> of 0.58 with an MSE of 2342, while the classifier achieves 93% accuracy. The following table shows the k-NN confusion matrix for the breast cancer dataset:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>n = 169</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Predicted: Malignant</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Predicted: Benign</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Target: Malignant</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>37</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Target: Benign</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>9</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>121</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means</h1>
                </header>
            
            <article>
                
<p>K-means is a clustering algorithm that presents similarities to k-NN. A number of cluster centers are produced, and each instance is assigned to its nearest cluster. After all instances are assigned to a cluster, the centroid of the cluster becomes the new center, until the algorithm converges to a stable solution. In scikit-learn, this algorithm is implemented in <kbd>sklearn.cluster.KMeans</kbd>. We can try to cluster the first two features of the breast cancer dataset: the mean radius and the texture of the FNA imaging.</p>
<p>First, we load the required data and libraries, while retaining only the first two features of the dataset:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.cluster import KMeans<br/>bc = load_breast_cancer()<br/>bc.data=bc.data[:,:2]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Then, we fit the cluster on the data. Note that we don't have to split the data into train and test sets:</p>
<pre class="mce-root"># --- SECTION 2 ---<br/># Instantiate and train<br/>km = KMeans(n_clusters=3)<br/>km.fit(bc.data)</pre>
<p class="mce-root">Following that, we create a two-dimensional mesh and cluster every point, in order to plot the cluster areas and boundaries:</p>
<pre class="mce-root"># --- SECTION 3 ---<br/># Create a point mesh to plot cluster areas<br/><span># Step size of the mesh. </span><br/><span>h = .02<br/></span># Plot the decision boundary. For that, we will assign a color to each<br/>x_min, x_max = bc.data[:, 0].min() - 1, bc.data[:, 0].max() + 1<br/>y_min, y_max = bc.data[:, 1].min() - 1, bc.data[:, 1].max() + 1<br/># Create the actual mesh and cluster it<br/>xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))<br/>Z = km.predict(np.c_[xx.ravel(), yy.ravel()])<br/># Put the result into a color plot<br/>Z = Z.reshape(xx.shape)<br/>plt.figure(1)<br/>plt.clf()<br/>plt.imshow(Z, interpolation='nearest',<br/> extent=(xx.min(), xx.max(), yy.min(), yy.max()),<br/> aspect='auto', origin='lower',)</pre>
<p class="mce-root">Finally, we plot the actual data, color-mapped to its respective clusters:</p>
<pre class="mce-root"> --- SECTION 4 ---<br/># Plot the actual data<br/>c = km.predict(bc.data)<br/>r = c == 0<br/>b = c == 1<br/>g = c == 2<br/>plt.scatter(bc.data[r, 0], bc.data[r, 1], label='cluster 1')<br/>plt.scatter(bc.data[b, 0], bc.data[b, 1], label='cluster 2')<br/>plt.scatter(bc.data[g, 0], bc.data[g, 1], label='cluster 3')<br/>plt.title('K-means')<br/>plt.xlim(x_min, x_max)<br/>plt.ylim(y_min, y_max)<br/>plt.xticks(())<br/>plt.yticks(())<br/>plt.xlabel(bc.feature_names[0])<br/>plt.ylabel(bc.feature_names[1])<br/>`()<br/>plt.show()</pre>
<p class="mce-root">The result is a two-dimensional image with color-coded boundaries of each cluster, as well as the instances:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-574 image-border" src="assets/5590354e-8202-4dda-9414-be4d73bfa4ec.png" style="width:30.08em;height:23.83em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">K-means clustering of the first two features of the breast cancer dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented the basic datasets, algorithms, and metrics that we will use throughout the book. We talked about regression and classification problems, where datasets have not only features but also targets. We called these labeled datasets. We also talked about unsupervised learning, in the form of clustering and dimensionality reduction. We introduced cost functions and model metrics that we will use to evaluate the models that we generate. Furthermore, we presented the basic learning algorithms and Python libraries that we will utilize in the majority of our examples.</p>
<p>In the next chapter, we will introduce the concepts of bias and variance, as well as the concept of ensemble learning. Some key points to remember are as follows:</p>
<ul>
<li>We try to solve a regression problem when the target variable is a continuous number and its values have a meaning in terms of magnitude, such as speed, cost, blood pressure, and so on. Classification problems can have their targets coded as numbers, but we cannot treat them as such. There is no meaning in trying to sort colors or foods based on the number they are assigned during a problem's encoding.</li>
<li>Cost functions are a way to quantify how far away a predictive model is from modelling data perfectly. Metrics<strong> </strong>provide information that is easier for humans to understand and report.</li>
<li>All of the algorithms presented in this chapter have implementations for both classification and regression problems in scikit-learn. Some are better suited to particular tasks, at least without tuning their hyper parameters. Decision trees produce models that are easily interpreted by humans.</li>
</ul>


            </article>

            
        </section>
    </body></html>
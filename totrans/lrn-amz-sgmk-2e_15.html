<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer173">
			<h1 id="_idParaDest-238"><a id="_idTextAnchor237"/>Chapter 11: Deploying Machine Learning Models</h1>
			<p>In previous chapters, we've deployed models in the simplest way possible: by configuring an estimator, calling the <strong class="source-inline">fit()</strong> <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) to train the model, and calling the <strong class="source-inline">deploy()</strong> API to create a real-time endpoint. This is the simplest scenario for development and testing, but it's not the only one.</p>
			<p>Models can be imported. For example, you could take an existing model that you trained on your local machine, import it into SageMaker, and deploy it as if you had trained it on SageMaker.</p>
			<p>In addition, models can be deployed in different configurations, as follows:</p>
			<ul>
				<li>A single model on a real-time endpoint, which is what we've done so far, as well as several model variants in the same endpoint.</li>
				<li>A <a id="_idIndexMarker1216"/>sequence of up to five models, called an <strong class="bold">inference pipeline</strong>.</li>
				<li>An arbitrary number of related models that are loaded on <a id="_idIndexMarker1217"/>demand on the same endpoint, known as a <strong class="bold">multi-model endpoint</strong>. We'll examine this configuration in <a href="B17705_13_Final_JM_ePub.xhtml#_idTextAnchor290"><em class="italic">Chapter 13</em></a>, <em class="italic">Optimizing Cost and Performance</em>.</li>
				<li>A single model or an inference pipeline that predicts <a id="_idIndexMarker1218"/>data in batch mode through a feature known as <strong class="bold">batch transform</strong>.</li>
			</ul>
			<p>Of course, models <a id="_idIndexMarker1219"/>can also be exported. You can grab a training artifact in <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>), extract the model, and deploy it anywhere you like.</p>
			<p>In this chapter, we'll cover the following topics:</p>
			<ul>
				<li>Examining model artifacts and exporting models</li>
				<li>Deploying models on real-time endpoints</li>
				<li>Deploying models on batch transformers</li>
				<li>Deploying models on inference pipelines</li>
				<li>Monitoring prediction quality with Amazon SageMaker Model Monitor</li>
				<li>Deploying models on container services</li>
				<li>Let's get started!</li>
			</ul>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor238"/>Technical requirements</h1>
			<p>You will need an <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) account to run the examples included in this chapter. If you haven't got one already, please browse to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create one. You should also familiarize yourself with the AWS Free Tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the AWS <strong class="bold">Command Line Interface</strong> (<strong class="bold">CLI</strong>) for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).  </p>
			<p>You will need a working Python 3.x environment. Installing the Anaconda distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory but strongly encouraged as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>The code examples included in this book are available on GitHub at https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor239"/>Examining model artifacts and exporting models</h1>
			<p>A model artifact contains one or several files that are produced by a training job and that are required for model <a id="_idIndexMarker1220"/>deployment. The number and nature <a id="_idIndexMarker1221"/>of these files depend on the algorithm that was trained. As we've seen many times, the model artifact is stored as a <strong class="source-inline">model.tar.gz</strong> file, at the S3 output location defined in the estimator.</p>
			<p>Let's look at different examples, where we reuse artifacts from the jobs we previously trained. </p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor240"/>Examining and exporting built-in models </h2>
			<p>Almost all <a id="_idIndexMarker1222"/>built-in algorithms are implemented <a id="_idIndexMarker1223"/>with <strong class="bold">Apache MXNet</strong>, and their artifacts reflect this. For more information <a id="_idIndexMarker1224"/>on MXNet, please visit <a href="https://mxnet.apache.org/">https://mxnet.apache.org/</a>. </p>
			<p>Let's see <a id="_idIndexMarker1225"/>how we can load these models directly. Another option would be to use <strong class="bold">Multi Model Server</strong> (<strong class="bold">MMS</strong>) (<a href="https://github.com/awslabs/multi-model-server">https://github.com/awslabs/multi-model-server</a>), but we'll proceed as follows:</p>
			<ol>
				<li>Let's start from the artifact for the <strong class="bold">Linear Learner</strong> model we trained in <a href="B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Machine Learning Models</em>, as illustrated in the following code snippet:<p class="source-code"><strong class="bold">$ tar xvfz model.tar.gz</strong></p><p class="source-code"><strong class="bold">x model_algo-1</strong></p><p class="source-code"><strong class="bold">$ unzip model_algo-1</strong></p><p class="source-code"><strong class="bold">archive:  model_algo-1</strong></p><p class="source-code"><strong class="bold">extracting: additional-params.json</strong></p><p class="source-code"><strong class="bold">extracting: manifest.json</strong></p><p class="source-code"><strong class="bold">extracting: mx-mod-symbol.json</strong></p><p class="source-code"><strong class="bold">extracting: mx-mod-0000.params</strong></p></li>
				<li>We load <a id="_idIndexMarker1226"/>the symbol file, which contains a <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) definition of the model, as follows:<p class="source-code">import json</p><p class="source-code">sym_json = json.load(open('mx-mod-symbol.json'))</p><p class="source-code">sym_json_string = json.dumps(sym_json)</p></li>
				<li>We use this JSON definition to instantiate a new Gluon model. We also define the name of its input symbol (<strong class="source-inline">data</strong>), as follows:<p class="source-code">import mxnet as mx</p><p class="source-code">from mxnet import gluon</p><p class="source-code">net = gluon.nn.SymbolBlock(</p><p class="source-code">    outputs=mx.sym.load_json(sym_json_string),</p><p class="source-code">    inputs=mx.sym.var('data')) </p></li>
				<li>Now, we <a id="_idIndexMarker1227"/>can easily plot the model, like this:<p class="source-code">mx.viz.plot_network(</p><p class="source-code">    net(mx.sym.var('data'))[0],   </p><p class="source-code">    node_attrs={'shape':'oval','fixedsize':'false'})</p><p>This <a id="_idIndexMarker1228"/>creates the following output:</p><div id="_idContainer165" class="IMG---Figure"><img src="Images/B17705_11_1.jpg" alt="Figure 11.1 – Linear Learner model&#13;&#10;" width="722" height="841"/></div><p class="figure-caption">Figure 11.1 – Linear Learner model</p></li>
				<li>Then, we load the model parameters learned during training, as follows:<p class="source-code">net.load_parameters('mx-mod-0000.params', </p><p class="source-code">                    allow_missing=True)</p><p class="source-code">net.collect_params().initialize()</p></li>
				<li>We define <a id="_idIndexMarker1229"/>a test sample stored in an MXNet <strong class="source-inline">NDArray</strong> (https://mxnet.apache.org/versions/1.6/api/python/docs/api/ndarray/index.html), as follows:<p class="source-code">test_sample = mx.nd.array(</p><p class="source-code">[0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98])</p></li>
				<li>Finally, we <a id="_idIndexMarker1230"/>forward it through the model and read the output, as follows:<p class="source-code">response = net(test_sample)</p><p class="source-code">print(response)</p><p>The predicted price of this house is <strong class="bold">US Dollars</strong> (<strong class="bold">USD</strong>) 30,173, as illustrated here:</p><p class="source-code"><strong class="bold">array([[30.173424]], dtype=float32)</strong></p></li>
			</ol>
			<p>This technique should work with all MXNet-based algorithms. Now, let's take a look at the built-in algorithms for <strong class="bold">Computer Vision</strong> (<strong class="bold">CV</strong>).</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor241"/>Examining and exporting built-in CV models </h2>
			<p>The three <a id="_idIndexMarker1231"/>built-in algorithms for CV are <a id="_idIndexMarker1232"/>also based on Apache MXNet. The process is exactly the same, as outlined here:</p>
			<ol>
				<li value="1">The following is the artifact for the <strong class="bold">image classification</strong> model we trained on dogs and cats in <a href="B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 5</em></a>, <em class="italic">Training Computer Vision Models</em>:<p class="source-code"><strong class="bold">$ tar xvfz model.tar.gz</strong></p><p class="source-code"><strong class="bold">x image-classification-0010.params</strong></p><p class="source-code"><strong class="bold">x model-shapes.json</strong></p><p class="source-code"><strong class="bold">x image-classification-symbol.json</strong></p></li>
				<li>Load the <a id="_idIndexMarker1233"/>model and its parameters, as follows:<p class="source-code">import mxnet, json</p><p class="source-code">from mxnet import gluon</p><p class="source-code">sym_json = json.load(</p><p class="source-code">           open('image-classification-symbol.json'))</p><p class="source-code">sym_json_string = json.dumps(sym_json)</p><p class="source-code">net = gluon.nn.SymbolBlock(</p><p class="source-code">    outputs=mx.sym.load_json(sym_json_string),</p><p class="source-code">    inputs=mx.sym.var('data'))</p><p class="source-code">net.load_parameters(</p><p class="source-code">    'image-classification-0010.params',  </p><p class="source-code">    allow_missing=True)</p><p class="source-code">net.collect_params().initialize()</p></li>
				<li>The input <a id="_idIndexMarker1234"/>shape is a 300x300 color image with three channels (<strong class="bold">red, green, and blue</strong>, or <strong class="bold">RGB</strong>). Accordingly, we create a fake image using random values. We forward it through the model and read the results, as follows:<p class="source-code">test_sample = mx.ndarray.random.normal(</p><p class="source-code">    shape=(1,3,300,300))</p><p class="source-code">response = net(test_sample)</p><p class="source-code">print(response)</p><p>Funnily enough, this random image is classified as a cat, as defined in the following code snippet:</p><p class="source-code"><strong class="bold">array([[0.99126923, 0.00873081]], dtype=float32)</strong></p></li>
			</ol>
			<p>Reusing <strong class="bold">Object Detection</strong> is more complicated as the training network needs to be modified for prediction. You can find an example at <a href="https://github.com/aws-samples/amazon-sagemaker-aws-greengrass-custom-object-detection-model/">https://github.com/aws-samples/amazon-sagemaker-aws-greengrass-custom-object-detection-model/</a>. </p>
			<p>Now, let's look at <strong class="bold">Extreme Gradient Boosting</strong> (<strong class="bold">XGBoost</strong>) artifacts.</p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor242"/>Examining and exporting XGBoost models</h2>
			<p>An XGBoost <a id="_idIndexMarker1235"/>artifact contains a single file—the model itself. However, the format of the model depends on how you're using XGBoost.</p>
			<p>With the <a id="_idIndexMarker1236"/>built-in algorithm, the model is a pickled file that stores a <strong class="source-inline">Booster</strong> object. Once the artifact has been extracted, we simply unpickle the model and load it, as follows:</p>
			<p class="source-code">$ tar xvfz model.tar.gz</p>
			<p class="source-code">x xgboost-model</p>
			<p class="source-code">$ python</p>
			<p class="source-code">&gt;&gt;&gt; import pickle</p>
			<p class="source-code">&gt;&gt;&gt; model = pickle.load(open('xgboost-model', 'rb'))</p>
			<p class="source-code">&gt;&gt;&gt; type(model)</p>
			<p class="source-code">&lt;class 'xgboost.core.Booster'&gt;</p>
			<p>With the built-in framework, the model is just a saved model. Once the artifact has been extracted, we load the model directly, as follows:</p>
			<p class="source-code">$ tar xvfz model.tar.gz</p>
			<p class="source-code">x xgb.model</p>
			<p class="source-code">$ python</p>
			<p class="source-code">&gt;&gt;&gt; import xgboost as xgb</p>
			<p class="source-code">&gt;&gt;&gt; bst = xgb.Booster({'nthread': 4})</p>
			<p class="source-code">&gt;&gt;&gt; model = bst.load_model('xgb.model')</p>
			<p class="source-code">&gt;&gt;&gt; type(bst)</p>
			<p class="source-code">&lt;class 'xgboost.core.Booster'&gt;</p>
			<p>Now, let's look at <strong class="bold">scikit-learn</strong> artifacts.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor243"/>Examining and exporting scikit-learn models</h2>
			<p>Scikit-learn models are saved <a id="_idIndexMarker1237"/>and loaded with <strong class="source-inline">joblib</strong> (https://joblib.readthedocs.io), as illustrated in the following code snippet. This library contains <a id="_idIndexMarker1238"/>a set of tools that provide <a id="_idIndexMarker1239"/>lightweight pipelining, but we'll only use it to save models:</p>
			<p class="source-code">$ tar xvfz model.tar.gz</p>
			<p class="source-code">x model.joblib</p>
			<p class="source-code">$ python</p>
			<p class="source-code">&gt;&gt;&gt; import joblib</p>
			<p class="source-code">&gt;&gt;&gt; model = joblib.load('model.joblib')</p>
			<p class="source-code">&gt;&gt;&gt; type(model)</p>
			<p class="source-code">&lt;class 'sklearn.linear_model._base.LinearRegression'&gt;</p>
			<p>Finally, let's look at <strong class="bold">TensorFlow</strong> artifacts.</p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor244"/>Examining and exporting TensorFlow models</h2>
			<p>TensorFlow <a id="_idIndexMarker1240"/>and <strong class="bold">Keras</strong> models are <a id="_idIndexMarker1241"/>saved in <strong class="bold">TensorFlow Serving</strong> format, as illustrated in the following code snippet:</p>
			<p class="source-code">$ mkdir /tmp/models</p>
			<p class="source-code">$ tar xvfz model.tar.gz -C /tmp/models</p>
			<p class="source-code">x 1/</p>
			<p class="source-code">x 1/saved_model.pb</p>
			<p class="source-code">x 1/assets/</p>
			<p class="source-code">x 1/variables/</p>
			<p class="source-code">x 1/variables/variables.index</p>
			<p class="source-code">x 1/variables/variables.data-00000-of-00002</p>
			<p class="source-code">x 1/variables/variables.data-00001-of-00002</p>
			<p>The easiest <a id="_idIndexMarker1242"/>way to serve such a model is <a id="_idIndexMarker1243"/>to run the <strong class="bold">Docker</strong> image for TensorFlow Serving, as illustrated in the following code snippet. You can find more details at <a href="https://www.tensorflow.org/tfx/serving/serving_basic">https://www.tensorflow.org/tfx/serving/serving_basic</a>:</p>
			<p class="source-code">$ docker run -t --rm -p 8501:8501</p>
			<p class="source-code">  -v "/tmp/models:/models/fmnist"</p>
			<p class="source-code">  -e MODEL_NAME=fmnist</p>
			<p class="source-code">  tensorflow/serving</p>
			<p>Let's look at a final example where we export a Hugging Face model.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor245"/>Examining and exporting Hugging Face models</h2>
			<p>Hugging Face <a id="_idIndexMarker1244"/>models can be trained <a id="_idIndexMarker1245"/>on either TensorFlow or PyTorch. Let's reuse our Hugging Face example from <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services with Built-in Frameworks</em>, where we trained a sentiment analysis model with PyTorch, and proceed as follows:</p>
			<ol>
				<li value="1">We copy the model artifact from S3 and extract it, like this:<p class="source-code"><strong class="bold">$ tar xvfz model.tar.gz</strong></p><p class="source-code"><strong class="bold">training_args.bin</strong></p><p class="source-code"><strong class="bold">config.json</strong></p><p class="source-code"><strong class="bold">pytorch_model.bin</strong></p></li>
				<li>In a Jupyter notebook, we use the Hugging Face API to load the model configuration. We then build the model using a <strong class="source-inline">DistilBertForSequenceClassification</strong> object, which corresponds to the model that we trained on SageMaker. Here's the code to accomplish this:<p class="source-code">from transformers import AutoConfig, DistilBertForSequenceClassification</p><p class="source-code">config = AutoConfig.from_pretrained(</p><p class="source-code">         './model/config.json')</p><p class="source-code">model = DistilBertForSequenceClassification</p><p class="source-code">       .from_pretrained('./model/pytorch_model.bin',  </p><p class="source-code">                        config=config)</p></li>
				<li>Next, we <a id="_idIndexMarker1246"/>fetch the tokenizer <a id="_idIndexMarker1247"/>associated with the model, as follows:<p class="source-code">from transformers import AutoTokenizer</p><p class="source-code">tokenizer = AutoTokenizer.from_pretrained(</p><p class="source-code">            'distilbert-base-uncased') </p></li>
				<li>We write a short function that will apply <strong class="source-inline">softmax</strong> to the activation values returned by the output layer of the model, as follows:<p class="source-code">import torch</p><p class="source-code">def probs(logits):</p><p class="source-code">    softmax = torch.nn.Softmax(dim=1)</p><p class="source-code">    pred = softmax(logits).detach().numpy()</p><p class="source-code">    return pred</p></li>
				<li>Finally, we define a sample and predict it with our model, as follows:<p class="source-code">inputs = tokenizer("The Phantom Menace was a really bad movie. What a waste of my life.", return_tensors='pt')</p><p class="source-code">outputs = model(**inputs)</p><p class="source-code">print(probs(outputs.logits))</p><p>As expected, the sentiment is strongly negative, as we can see here:</p><p class="source-code"><strong class="bold">[[0.22012234 0.7798777 ]]</strong></p></li>
			</ol>
			<p>This concludes <a id="_idIndexMarker1248"/>the section on exporting <a id="_idIndexMarker1249"/>models from SageMaker. As you can see, it's really not difficult at all. </p>
			<p>Now, let's learn how to deploy models on real-time endpoints.</p>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor246"/>Deploying models on real-time endpoints</h1>
			<p>SageMaker endpoints serve real-time predictions using models hosted on fully managed infrastructure. They <a id="_idIndexMarker1250"/>can be created and managed <a id="_idIndexMarker1251"/>with either the SageMaker <strong class="bold">software development kit</strong> (<strong class="bold">SDK</strong>) or with an AWS SDK such as <strong class="source-inline">boto3</strong>. </p>
			<p>You can find <a id="_idIndexMarker1252"/>information on your endpoints in SageMaker Studio, under <strong class="bold">SageMaker resources</strong>/<strong class="bold">Endpoints</strong>.</p>
			<p>Now, let's look at the SageMaker SDK in greater detail.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor247"/>Managing endpoints with the SageMaker SDK</h2>
			<p>The SageMaker SDK lets you work with <a id="_idIndexMarker1253"/>endpoints in several ways, as outlined here:</p>
			<ul>
				<li>Configuring an estimator, training it with <strong class="source-inline">fit()</strong>, deploying an endpoint with <strong class="source-inline">deploy()</strong>, and invoking it with <strong class="source-inline">predict()</strong></li>
				<li>Importing and deploying a model</li>
				<li>Invoking an existing endpoint</li>
				<li>Updating an existing endpoint</li>
			</ul>
			<p>We've used the first scenario in many examples so far. Let's look at the other ones.</p>
			<h3>Importing and deploying an XGBoost model</h3>
			<p>This is useful when <a id="_idIndexMarker1254"/>you want to import a model that wasn't trained on SageMaker, or when you want to redeploy a SageMaker model. In the previous section, we <a id="_idIndexMarker1255"/>saw what model artifacts look like, and how we should use them to package models. We'll now proceed as follows: </p>
			<ol>
				<li value="1">Starting from an XGBoost model that we trained and saved locally with <strong class="source-inline">save_model()</strong>, we first create a model artifact by running the following code:<p class="source-code"><strong class="bold">$ tar cvfz model-xgb.tar.gz xgboost-model</strong></p></li>
				<li>In a Jupyter notebook, we upload the model artifact to our default bucket, like this:<p class="source-code">import sagemaker</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">prefix = 'export-xgboost'</p><p class="source-code">model_path = sess.upload_data(</p><p class="source-code">    path=model-xgb.tar.gz', </p><p class="source-code">    key_prefix=prefix)</p></li>
				<li>Then, we create an <strong class="source-inline">XGBoostModel</strong> object, passing the location of the artifact and an inference script (more on this in a second). We also select a framework version, and it should match the one we use to train the model. The code is illustrated in the following snippet:<p class="source-code">from sagemaker.xgboost.model import XGBoostModel</p><p class="source-code">xgb_model = XGBoostModel(</p><p class="source-code">    model_data=model_path,</p><p class="source-code">    entry_point='xgb-script.py',</p><p class="source-code">    framework_version='1.3-1',</p><p class="source-code">    role=sagemaker.get_execution_role())</p></li>
				<li>The <a id="_idIndexMarker1256"/>inference script is very simple. It only needs to contain a model-loading function, as explained when we discussed deploying framework models in <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services with Built-in Frameworks</em>. The code is illustrated in the following snippet:<p class="source-code">import os</p><p class="source-code">import xgboost as xgb</p><p class="source-code">def model_fn(model_dir):</p><p class="source-code">    model = xgb.Booster()</p><p class="source-code">    model.load_model(</p><p class="source-code">        os.path.join(model_dir,'xgboost-model'))</p><p class="source-code">    return model</p></li>
				<li>Back <a id="_idIndexMarker1257"/>in the notebook, we then deploy and predict as usual, as follows:<p class="source-code">xgb_predictor = xgb_model.deploy(. . .)</p><p class="source-code">xgb_predictor.predict(. . .)</p></li>
			</ol>
			<p>Now, let's do the same with a TensorFlow model.</p>
			<h3>Importing and deploying a TensorFlow model</h3>
			<p>The process is very similar, as we will see next:</p>
			<ol>
				<li value="1">We first use <strong class="source-inline">tar</strong> to package a TensorFlow model that we trained and saved in TensorFlow <a id="_idIndexMarker1258"/>Serving format. Our artifact should look <a id="_idIndexMarker1259"/>like this (please don't forget to create the top-level directory!):<p class="source-code"><strong class="bold">$ tar tvfz model.tar.gz</strong></p><p class="source-code"><strong class="bold">1/</strong></p><p class="source-code"><strong class="bold">1/saved_model.pb</strong></p><p class="source-code"><strong class="bold">1/assets/</strong></p><p class="source-code"><strong class="bold">1/variables/</strong></p><p class="source-code"><strong class="bold">1/variables/variables.index</strong></p><p class="source-code"><strong class="bold">1/variables/variables.data-00000-of-00002</strong></p><p class="source-code"><strong class="bold">1/variables/variables.data-00001-of-00002</strong></p></li>
				<li>Then, we upload the artifact to S3, as follows:<p class="source-code">import sagemaker</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">prefix = 'byo-tf'</p><p class="source-code">model_path = sess.upload_data(</p><p class="source-code">   path='model.tar.gz', </p><p class="source-code">   key_prefix=prefix)</p></li>
				<li>Next, we create a SageMaker model from the artifact. By default, we don't have to provide an inference script. We would pass if we needed custom preprocessing and postprocessing handlers for feature engineering, exotic serialization, and so on. You'll find more information at https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#deploying-from-an-estimator. The code is illustrated in the following snippet:<p class="source-code">from sagemaker.tensorflow.model import TensorFlowModel</p><p class="source-code">tf_model = TensorFlowModel(</p><p class="source-code">    model_data=model_path,</p><p class="source-code">    framework_version='2.3.1',</p><p class="source-code">    role=sagemaker.get_execution_role())</p></li>
				<li>We then <a id="_idIndexMarker1260"/>deploy and predict as usual, thanks to the <strong class="bold">Deep Learning Container</strong> (<strong class="bold">DLC</strong>) for <a id="_idIndexMarker1261"/>TensorFlow.</li>
			</ol>
			<p>Now, let's <a id="_idIndexMarker1262"/>do a final example, where we import and deploy a Hugging Face model with the DLC for PyTorch and an inference script for model loading and custom processing.</p>
			<h3>Importing and deploying a Hugging Face model with PyTorch</h3>
			<p>Let's reuse <a id="_idIndexMarker1263"/>our Hugging Face example, and first focus on the inference script. It contains four functions: model loading, preprocessing, prediction, and <a id="_idIndexMarker1264"/>postprocessing. We'll proceed as follows:</p>
			<ol>
				<li value="1">The model-loading function uses the same code that we used when we exported the model. The only difference is that we load the file from <strong class="source-inline">model_dir</strong>, which is passed by SageMaker to the PyTorch container. We also load the tokenizer once. The code is illustrated in the following snippet:<p class="source-code">tokenizer = AutoTokenizer.from_pretrained(</p><p class="source-code">            'distilbert-base-uncased')</p><p class="source-code">def model_fn(model_dir):</p><p class="source-code">  config_path='{}/config.json'.format(model_dir)</p><p class="source-code">  model_path='{}/pytorch_model.bin'.format(model_dir)</p><p class="source-code">  config=AutoConfig.from_pretrained(config_path)</p><p class="source-code">  model= DistilBertForSequenceClassification</p><p class="source-code">         .from_pretrained(model_path, config=config)</p><p class="source-code">  return model</p></li>
				<li>The <a id="_idIndexMarker1265"/>preprocessing and postprocessing functions are <a id="_idIndexMarker1266"/>simple. They only check for the correct content and accept types. You can see these in the following code snippet:<p class="source-code">def input_fn(serialized_input_data, </p><p class="source-code">             content_type=JSON_CONTENT_TYPE):  </p><p class="source-code">  if content_type == JSON_CONTENT_TYPE:</p><p class="source-code">    input_data = json.loads(serialized_input_data)</p><p class="source-code">    return input_data</p><p class="source-code">  else:</p><p class="source-code">    raise Exception('Unsupported input type: ' </p><p class="source-code">                    + content_type)</p><p class="source-code">def output_fn(prediction_output, </p><p class="source-code">              accept=JSON_CONTENT_TYPE):</p><p class="source-code">  if accept == JSON_CONTENT_TYPE:</p><p class="source-code">    return json.dumps(prediction_output), accept</p><p class="source-code">  else:</p><p class="source-code">    raise Exception('Unsupported output type: '</p><p class="source-code">                    + accept)</p></li>
				<li>Finally, the prediction function tokenizes input data, predicts it, and returns the name of the most probable class, as follows:<p class="source-code">CLASS_NAMES = ['negative', 'positive']</p><p class="source-code">def predict_fn(input_data, model):</p><p class="source-code">    inputs = tokenizer(input_data['text'], </p><p class="source-code">                       return_tensors='pt')</p><p class="source-code">    outputs = model(**inputs)</p><p class="source-code">    logits = outputs.logits</p><p class="source-code">    _, prediction = torch.max(logits, dim=1)</p><p class="source-code">    return CLASS_NAMES[prediction]</p></li>
			</ol>
			<p>Now our <a id="_idIndexMarker1267"/>inference script is ready, let's move to a notebook, import the model, and deploy it, as follows:</p>
			<ol>
				<li value="1">We <a id="_idIndexMarker1268"/>create a <strong class="source-inline">PyTorchModel</strong> object, passing the location of the model artifact in S3 and the location of our inference script, as follows:<p class="source-code">from sagemaker.pytorch import PyTorchModel</p><p class="source-code">model = PyTorchModel(</p><p class="source-code">    model_data=model_data_uri,</p><p class="source-code">    role=sagemaker.get_execution_role(), </p><p class="source-code">    entry_point='torchserve-predictor.py',</p><p class="source-code">    source_dir='src',</p><p class="source-code">    framework_version='1.6.0',</p><p class="source-code">    py_version='py36')</p></li>
				<li>We deploy with <strong class="source-inline">model.deploy()</strong>. Then, we create two samples and send them to our endpoint, as follows:<p class="source-code">positive_data = {'text': "This is a very nice camera, I'm super happy with it."}</p><p class="source-code">negative_data = {'text': "Terrible purchase, I want my money back!"}</p><p class="source-code">prediction = predictor.predict(positive_data)</p><p class="source-code">print(prediction)</p><p class="source-code">prediction = predictor.predict(negative_data)</p><p class="source-code">print(prediction)</p><p>As expected, the outputs are <strong class="source-inline">positive</strong> and <strong class="source-inline">negative</strong>.</p></li>
			</ol>
			<p>This concludes <a id="_idIndexMarker1269"/>the section on importing and deploying models. Now, let's <a id="_idIndexMarker1270"/>learn how to invoke an endpoint that has already been deployed.</p>
			<h3>Invoking an existing endpoint</h3>
			<p>This is useful when <a id="_idIndexMarker1271"/>you want to work with a live endpoint but don't have access to the predictor. All we need to know is the endpoint's name. Proceed as follows:</p>
			<ol>
				<li value="1">Build a <strong class="source-inline">TensorFlowPredictor</strong> predictor for the endpoint we deployed in a previous example. Again, the object is framework-specific. The code is illustrated in the following snippet:<p class="source-code">from sagemaker.tensorflow.model import TensorFlowPredictor</p><p class="source-code">another_predictor = TensorFlowPredictor(</p><p class="source-code">    endpoint_name=tf_endpoint_name,</p><p class="source-code">    serializer=sagemaker.serializers.JSONSerializer()</p><p class="source-code">)</p></li>
				<li>Then, predict it as usual, as follows:<p class="source-code">another_predictor.predict(…)</p></li>
			</ol>
			<p>Now, let's learn how to update endpoints.</p>
			<h3>Updating an existing endpoint</h3>
			<p>The <strong class="source-inline">update_endpoint()</strong> API lets you update <a id="_idIndexMarker1272"/>the configuration of an endpoint in a non-disruptive fashion. The endpoint stays in service, and you can keep predicting with it. </p>
			<p>Let's try this on our TensorFlow endpoint, as follows:</p>
			<ol>
				<li value="1">We set the instance count to <strong class="source-inline">2</strong> and update the endpoint, as follows:<p class="source-code">another_predictor.update_endpoint(</p><p class="source-code">    initial_instance_count=2,</p><p class="source-code">    instance_type='ml.t2.medium')</p></li>
				<li>The endpoint is immediately updated, as shown in the following screenshot.<p class="figure-caption"> </p><div id="_idContainer166" class="IMG---Figure"><img src="Images/B17705_11_2.jpg" alt="Figure 11.2 – Endpoint being updated&#13;&#10;" width="387" height="144"/></div><p class="figure-caption">Figure 11.2 – Endpoint being updated</p></li>
				<li>Once the update is complete, the endpoint is now backed by two instances, as shown in the following screenshot:</li>
			</ol>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="Images/B17705_11_3.jpg" alt="Figure 11.3 – Endpoint backed by two instances&#13;&#10;" width="957" height="92"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – Endpoint backed by two instances</p>
			<p>As you can see, it's very easy to import, deploy, redeploy, and update models with the SageMaker <a id="_idIndexMarker1273"/>SDK. However, some operations require that we work with lower-level APIs. They're available in the AWS language SDKs, and we'll use our good friend <strong class="source-inline">boto3</strong> to demonstrate them.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor248"/>Managing endpoints with the boto3 SDK</h2>
			<p><strong class="source-inline">boto3</strong> is the <a id="_idIndexMarker1274"/>AWS SDK for Python (<a href="https://aws.amazon.com/sdk-for-python/">https://aws.amazon.com/sdk-for-python/</a>). It includes <a id="_idIndexMarker1275"/>APIs for all AWS services (unless they don't have APIs!). The SageMaker API is available at https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html.</p>
			<p><strong class="source-inline">boto3</strong> APIs are service-level APIs, and they give us full control over all service operations. Let's see how they can help us deploy and manage endpoints in ways that the SageMaker SDK doesn't allow.</p>
			<h3>Deploying endpoints with the boto3 SDK</h3>
			<p>Deploying an endpoint with <strong class="source-inline">boto3</strong> is a four-step operation, outlined as follows:</p>
			<ol>
				<li value="1">Create one <a id="_idIndexMarker1276"/>or more models with the <strong class="source-inline">create_model()</strong> API. Alternatively, we could use existing models that <a id="_idIndexMarker1277"/>have been trained or imported with the SageMaker SDK. For the sake of brevity, we'll do this here.</li>
				<li>Define one or more <strong class="bold">production variants</strong>, listing the infrastructure requirements for each model.</li>
				<li>Create an <strong class="bold">endpoint configuration</strong> with the <strong class="source-inline">create_endpoint_config()</strong> API, passing the production variants defined previously and assigning each one a weight.</li>
				<li>Create an endpoint with the <strong class="source-inline">create_endpoint()</strong> API.</li>
			</ol>
			<p>Let's put <a id="_idIndexMarker1278"/>these APIs to work and deploy <a id="_idIndexMarker1279"/>an endpoint running two variants of the XGBoost model we trained on the Boston Housing dataset, as follows:</p>
			<ol>
				<li value="1">We define two variants; both are backed by a single instance. However, they will receive nine-tenths and one-tenth of incoming requests, respectively—that is to say, "variant weight/sum of weights". We could use this setup if we wanted to introduce a new model in production and make sure it worked fine before sending it traffic. The code is illustrated in the following snippet:<p class="source-code">production_variants = [</p><p class="source-code">  { 'VariantName': 'variant-1',</p><p class="source-code">    'ModelName': model_name_1,</p><p class="source-code">    'InitialInstanceCount': 1,</p><p class="source-code">    'InitialVariantWeight': 9,</p><p class="source-code">    'InstanceType': 'ml.t2.medium'},</p><p class="source-code">  { 'VariantName': 'variant-2',</p><p class="source-code">    'ModelName': model_name_2,</p><p class="source-code">    'InitialInstanceCount': 1,</p><p class="source-code">    'InitialVariantWeight': 1,</p><p class="source-code">    'InstanceType': 'ml.t2.medium'}]</p></li>
				<li>We create an endpoint configuration by passing our two variants and setting optional tags, as follows:<p class="source-code">import boto3</p><p class="source-code">sm = boto3.client('sagemaker')</p><p class="source-code">endpoint_config_name = 'xgboost-two-models-epc'</p><p class="source-code">response = sm.create_endpoint_config(</p><p class="source-code">    EndpointConfigName=endpoint_config_name,</p><p class="source-code">    ProductionVariants=production_variants,</p><p class="source-code">    Tags=[{'Key': 'Name', </p><p class="source-code">           'Value': endpoint_config_name},</p><p class="source-code">          {'Key': 'Algorithm', 'Value': 'xgboost'}])</p><p>We <a id="_idIndexMarker1280"/>can list all <a id="_idIndexMarker1281"/>endpoint configurations with <strong class="source-inline">list_endpoint_configs()</strong> and describe a particular one with the <strong class="source-inline">describe_endpoint_config()</strong> <strong class="source-inline">boto3</strong> APIs.</p></li>
				<li>We create an endpoint based on this configuration:<p class="source-code">endpoint_name = 'xgboost-two-models-ep'</p><p class="source-code">response = sm.create_endpoint(</p><p class="source-code">    EndpointName=endpoint_name,</p><p class="source-code">    EndpointConfigName=endpoint_config_name,</p><p class="source-code">    Tags=[{'Key': 'Name','Value': endpoint_name},</p><p class="source-code">          {'Key': 'Algorithm','Value': 'xgboost'},</p><p class="source-code">          {'Key': 'Environment',</p><p class="source-code">           'Value': 'development'}])</p><p>We can list all the endpoints with <strong class="source-inline">list_endpoints()</strong> and describe a particular one with the <strong class="source-inline">describe_endpoint()</strong> <strong class="source-inline">boto3</strong> APIs.</p></li>
				<li>Creating a <strong class="source-inline">boto3</strong> waiter is a handy way to wait for the endpoint to be in service. You can see one being created here:<p class="source-code">waiter = sm.get_waiter('endpoint_in_service')</p><p class="source-code">waiter.wait(EndpointName=endpoint_name)</p></li>
				<li>After a few minutes, the <a id="_idIndexMarker1282"/>endpoint is in service. As shown in the following screenshot, it now uses two production variants:<div id="_idContainer168" class="IMG---Figure"><img src="Images/B17705_11_4.jpg" alt="Figure 11.4 – Viewing production variants&#13;&#10;" width="898" height="118"/></div><p class="figure-caption">Figure 11.4 – Viewing production variants</p></li>
				<li>Then, we <a id="_idIndexMarker1283"/>invoke the endpoint, as shown in the following code snippet. By default, prediction requests are forwarded to variants according to their weights:<p class="source-code">smrt = boto3.Session().client(</p><p class="source-code">    service_name='runtime.sagemaker') </p><p class="source-code">response = smrt.invoke_endpoint(</p><p class="source-code">   EndpointName=endpoint_name,</p><p class="source-code">   ContentType='text/csv',</p><p class="source-code">   Body=test_sample)</p></li>
				<li>We can also select the variant that receives the prediction request. This is useful for A/B testing, where we need to stick users to a given model. The following code snippet shows you how to do this:<p class="source-code">variants = ['variant-1', 'variant-2']</p><p class="source-code">for v in variants:</p><p class="source-code">  response = smrt.invoke_endpoint(</p><p class="source-code">                 EndpointName=endpoint_name, </p><p class="source-code">                 ContentType='text/csv',</p><p class="source-code">                 Body=test_sample,</p><p class="source-code">                 TargetVariant=v)</p><p class="source-code">  print(response['Body'].read())</p><p>This results in the following output:</p><p class="source-code"><strong class="bold">b'[0.0013231043703854084]'</strong></p><p class="source-code"><strong class="bold">b'[0.001262241625227034]'</strong></p></li>
				<li>We can also update <a id="_idIndexMarker1284"/>weights—for example, give equal weights to both variants so that they receive the same share of incoming traffic—as follows:<p class="source-code">response = sm.update_endpoint_weights_and_capacities(</p><p class="source-code">    EndpointName=endpoint_name,</p><p class="source-code">    DesiredWeightsAndCapacities=[</p><p class="source-code">        { 'VariantName': 'variant-1', </p><p class="source-code">          'DesiredWeight': 5},</p><p class="source-code">        { 'VariantName': 'variant-2', </p><p class="source-code">          'DesiredWeight': 5}])</p></li>
				<li>We can remove <a id="_idIndexMarker1285"/>one variant entirely and send all traffic to the remaining one. Here too, the endpoint stays in service the whole time, and no traffic is lost. The code is illustrated in the following snippet:<p class="source-code">production_variants_2 = [</p><p class="source-code">  {'VariantName': 'variant-2',</p><p class="source-code">   'ModelName': model_name_2,</p><p class="source-code">   'InitialInstanceCount': 1,</p><p class="source-code">   'InitialVariantWeight': 1,</p><p class="source-code">   'InstanceType': 'ml.t2.medium'}]</p><p class="source-code">endpoint_config_name_2 = 'xgboost-one-model-epc'</p><p class="source-code">response = sm.create_endpoint_config(</p><p class="source-code">    EndpointConfigName=endpoint_config_name_2,</p><p class="source-code">    ProductionVariants=production_variants_2,</p><p class="source-code">    Tags=[{'Key': 'Name',</p><p class="source-code">           'Value': endpoint_config_name_2},</p><p class="source-code">          {'Key': 'Algorithm','Value': 'xgboost'}])</p><p class="source-code">response = sm.update_endpoint(</p><p class="source-code">    EndpointName=endpoint_name,</p><p class="source-code">    EndpointConfigName=endpoint_config_name_2)</p></li>
				<li>Finally, we clean up by deleting the endpoint and the two endpoint configurations, as follows:<p class="source-code">sm.delete_endpoint(EndpointName=endpoint_name)</p><p class="source-code">sm.delete_endpoint_config(</p><p class="source-code">  EndpointConfigName=endpoint_config_name)</p><p class="source-code">sm.delete_endpoint_config(</p><p class="source-code">  EndpointConfigName=endpoint_config_name_2)</p></li>
			</ol>
			<p>As you <a id="_idIndexMarker1286"/>can see, the <strong class="source-inline">boto3</strong> API is more verbose, but it <a id="_idIndexMarker1287"/>also gives us the flexibility we need for <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) operations. In the next chapter, we'll learn how to automate these.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor249"/>Deploying models on batch transformers</h1>
			<p>Some use <a id="_idIndexMarker1288"/>cases don't require a real-time <a id="_idIndexMarker1289"/>endpoint. For example, you may want to predict 10 <strong class="bold">gigabytes</strong> (<strong class="bold">GB</strong>) of data once a week in one go, get the results, and feed them to a downstream application. Batch transformers are a very simple way to get this done.</p>
			<p>In this example, we will use the scikit-learn script that we trained on the Boston Housing dataset in <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services with Built-in Frameworks</em>. Let's get started, as follows:</p>
			<ol>
				<li value="1">Configure the estimator as usual, by running the following code:<p class="source-code">from sagemaker.sklearn import SKLearn</p><p class="source-code">sk = SKLearn(entry_point='sklearn-boston-housing.py',</p><p class="source-code">   role=sagemaker.get_execution_role(),</p><p class="source-code">   instance_count=1,</p><p class="source-code">   instance_type='ml.m5.large',</p><p class="source-code">   output_path=output,</p><p class="source-code">   hyperparameters=</p><p class="source-code">       {'normalize': True, 'test-size': 0.1})</p><p class="source-code">sk.fit({'training':training})</p></li>
				<li>Let's <a id="_idIndexMarker1290"/>predict the training set in batch <a id="_idIndexMarker1291"/>mode. We remove the target value, save the dataset to a <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) file, and upload it to S3, as follows:<p class="source-code">import pandas as pd</p><p class="source-code">data = pd.read_csv('housing.csv')</p><p class="source-code">data.drop(['medv'], axis=1, inplace=True)</p><p class="source-code">data.to_csv('data.csv', header=False, index=False)</p><p class="source-code">batch_input = sess.upload_data(</p><p class="source-code">    path='data.csv', </p><p class="source-code">    key_prefix=prefix + '/batch')</p></li>
				<li>Create a transformer object and launch batch processing, as follows:<p class="source-code">sk_transformer = sk.transformer(</p><p class="source-code">    instance_count=1, </p><p class="source-code">    instance_type='ml.m5.large')</p><p class="source-code">sk_transformer.transform(</p><p class="source-code">    batch_input, </p><p class="source-code">    content_type='text/csv', </p><p class="source-code">    wait=True, logs=True)</p></li>
				<li>In the training log, we can see that SageMaker creates a temporary endpoint and uses it to predict data. For large-scale jobs, we could optimize throughput by mini-batching <a id="_idIndexMarker1292"/>samples for prediction (using the <strong class="source-inline">strategy</strong> parameter), increase the level of prediction concurrency (<strong class="source-inline">max_concurrent_transforms</strong>), and increase the maximum payload size (<strong class="source-inline">max_payload</strong>). </li>
				<li>Once <a id="_idIndexMarker1293"/>the job is complete, predictions are available in S3, as indicated here:<p class="source-code">print(sk_transformer.output_path)</p><p class="source-code"><strong class="bold">s3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2020-06-12-08-28-30-978</strong></p></li>
				<li>Using the AWS CLI, we can easily retrieve these predictions by running the following code:<p class="source-code">%%bash -s "$sk_transformer.output_path"</p><p class="source-code">aws s3 cp $1/data.csv.out .</p><p class="source-code">head -1 data.csv.out</p><p class="source-code"><strong class="bold">[[29.73828574177013], [24.920634119498292], …</strong></p></li>
				<li>Just as for training, the infrastructure used by the transformer is shut down as soon as the job completes, so there's nothing to clean up.</li>
			</ol>
			<p>In the next section, we will look at inference pipelines and how to use them to deploy a sequence of related models.</p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor250"/>Deploying models on inference pipelines</h1>
			<p>Real-life ML scenarios often involve more than one model; for example, you may need to run <a id="_idIndexMarker1294"/>preprocessing steps on incoming data or reduce <a id="_idIndexMarker1295"/>its dimensionality with the <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) algorithm.</p>
			<p>Of course, you <a id="_idIndexMarker1296"/>could deploy each model to a dedicated endpoint. However, orchestration code would be required to pass prediction requests to each model in sequence. Multiplying endpoints would also introduce additional costs.</p>
			<p>Instead, <strong class="bold">inference pipelines</strong> let you deploy up to five models on the same endpoint or for batch transform and automatically handle the prediction sequence.</p>
			<p>Let's say that we wanted to run PCA and then Linear Learner. Building the inference pipeline would look like this:</p>
			<ol>
				<li value="1">Train the PCA model on the input dataset.</li>
				<li>Process the training and validation sets with PCA and store the results in S3. batch transform is a good way to do this.</li>
				<li>Train the Linear Learner model using the datasets processed by PCA as input.</li>
				<li>Use the <strong class="source-inline">create_model()</strong> API to create an inference pipeline, as follows:<p class="source-code">response = sagemaker.create_model(</p><p class="source-code">    ModelName='pca-linearlearner-pipeline',</p><p class="source-code">        Containers=[</p><p class="source-code">            {</p><p class="source-code">             'Image': pca_container,</p><p class="source-code">             'ModelDataUrl': pca_model_artifact,</p><p class="source-code">              . . .</p><p class="source-code">            },</p><p class="source-code">            {</p><p class="source-code">             'Image': ll_container,</p><p class="source-code">             'ModelDataUrl': ll_model_artifact,</p><p class="source-code">              . . .</p><p class="source-code">            }</p><p class="source-code">        ],</p><p class="source-code">        ExecutionRoleArn=role</p><p class="source-code">)</p></li>
				<li>Create an endpoint configuration and an endpoint in the usual way. We could also use the pipeline with a batch transformer.</li>
			</ol>
			<p>You can <a id="_idIndexMarker1297"/>find a complete example that uses scikit-learn and Linear Learner at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_inference_pipeline</a>. </p>
			<p><strong class="bold">Spark</strong> is a very <a id="_idIndexMarker1298"/>popular choice for data processing, and SageMaker lets you deploy Spark models with the <strong class="bold">SparkML Serving</strong> built-in container (https://github.com/aws/sagemaker-sparkml-serving-container), which uses the <strong class="bold">mleap</strong> library (<a href="https://github.com/combust/mleap">https://github.com/combust/mleap</a>). Of course, these models can be part of an <strong class="bold">inference pipeline</strong>. You can find several examples at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality</a>.</p>
			<p>This concludes our discussion on model deployment. In the next section, we'll introduce a SageMaker capability that helps us detect data issues that impact prediction quality: <strong class="bold">SageMaker Model Monitor</strong>.</p>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor251"/>Monitoring prediction quality with Amazon SageMaker Model Monitor</h1>
			<p>SageMaker Model Monitor <a id="_idIndexMarker1299"/>has two main features, outlined here:</p>
			<ul>
				<li>Capturing data sent to an endpoint, as well as predictions returned by the endpoint. This is useful for further analysis, or to replay real-life traffic during the development and testing of new models.</li>
				<li>Comparing incoming traffic to a baseline built from the training set, as well as sending alerts about data quality issues, such as missing features, mistyped features, and differences in <a id="_idIndexMarker1300"/>statistical properties (also known as "data drift").</li>
			</ul>
			<p>We'll use the <strong class="bold">Linear Learner</strong> example from <a href="B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Machine Learning Models</em>, where we trained a model on the Boston Housing dataset. First, we'll add data capture to the endpoint. Then, we'll <a id="_idIndexMarker1301"/>build a <strong class="bold">baseline</strong> and set up a <strong class="bold">monitoring schedule</strong> to periodically compare the <a id="_idIndexMarker1302"/>incoming data to that baseline.</p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor252"/>Capturing data</h2>
			<p>We can set up the data-<a id="_idIndexMarker1303"/>capture process when we deploy an endpoint. We can also enable it on an existing endpoint with the <strong class="source-inline">update_endpoint()</strong> API that we just used with production variants. </p>
			<p>At the time of writing, there are certain caveats that you should be aware of, as outlined here:</p>
			<ul>
				<li>You can only send <strong class="bold">one sample at a time</strong> if you want to perform model monitoring. Mini-batch predictions will be captured, but they will cause the monitoring job to fail.</li>
				<li>Likewise, data samples and predictions must be <strong class="bold">flat, tabular data</strong>. Structured data (such as lists of lists and nested JSON) will be captured, but the model-monitoring job will fail to process it. Optionally, you can add a preprocessing script and a postprocessing script to flatten it. You can find more information at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html</a>. </li>
				<li>The content type and the accept type must be <strong class="bold">identical</strong>. You can use either CSV or JSON, but you can't mix them.</li>
				<li>You cannot delete an endpoint if it has a monitoring schedule attached to it. You have to <strong class="bold">delete the monitoring schedule first</strong>, then delete the endpoint.</li>
			</ul>
			<p>Knowing <a id="_idIndexMarker1304"/>that, let's capture some data! Here we go:</p>
			<ol>
				<li value="1">Training takes place as usual. You can find the code in the GitHub repository.</li>
				<li>We create a data-capture configuration for 100% of the prediction requests and responses, storing everything in S3, as follows:<p class="source-code">from sagemaker.model_monitor.data_capture_config import DataCaptureConfig</p><p class="source-code">capture_path = 's3://{}/{}/capture/'.format(bucket, prefix)</p><p class="source-code">ll_predictor = ll.deploy(</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.t2.medium',</p><p class="source-code">    data_capture_config = DataCaptureConfig(     </p><p class="source-code">         enable_capture = True,                    </p><p class="source-code">         sampling_percentage = 100,                </p><p class="source-code">         capture_options = ['REQUEST', 'RESPONSE'],</p><p class="source-code">         destination_s3_uri = capture_path))</p></li>
				<li>Once the endpoint is in service, we send data for prediction. Within a minute or two, we see <a id="_idIndexMarker1305"/>captured data in S3 and then copy it locally, as follows:<p class="source-code">%%bash -s "$capture_path"</p><p class="source-code">aws s3 ls --recursive $1</p><p class="source-code">aws s3 cp --recursive $1 .</p></li>
				<li>Opening one of the files, we see samples and predictions, as follows:<p class="source-code"><strong class="bold">{"captureData":{"endpointInput":{"observedContentType":"text/csv","mode":"INPUT","data":"0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98","encoding":"CSV"},"endpointOutput":{"observedContentType":"text/csv; charset=utf-8","mode":"OUTPUT","data":"30.4133586884","encoding":"CSV"}},"eventMetadata":{"eventId":"8f45e35c-fa44-40d2-8ed3-1bcab3a596f3","inferenceTime":"2020-07-30T13:36:30Z"},"eventVersion":"0"}</strong></p></li>
			</ol>
			<p>If this <a id="_idIndexMarker1306"/>were live data, we could use it to test new models later on in order to compare their performance to existing models. </p>
			<p>Now, let's learn how to create a baseline from the training set.</p>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor253"/>Creating a baseline</h2>
			<p>SageMaker Model Monitor <a id="_idIndexMarker1307"/>includes a built-in container we can use to build the baseline, and we can use it directly with the <strong class="source-inline">DefaultModelMonitor</strong> object. You can also bring your own container, in which case you would use the <strong class="source-inline">ModelMonitor</strong> object instead. Let's get started, as follows:</p>
			<ol>
				<li value="1">A baseline can only be built on CSV datasets and JSON datasets. Our dataset is space-separated and needs to be converted into a CSV file, as follows. We can then upload it to S3:<p class="source-code">data.to_csv('housing.csv', sep=',', index=False)</p><p class="source-code">training = sess.upload_data(</p><p class="source-code">   path='housing.csv', </p><p class="source-code">   key_prefix=prefix + "/baseline")</p><p class="callout-heading">Note</p><p class="callout">There is a small caveat here: the baselining job is a Spark job running in <strong class="bold">SageMaker Processing</strong>. Hence, column names need to be Spark-compliant, or your job will fail in cryptic ways. In particular, dots are not allowed in column names. We don't have that problem here, but please keep this in mind.</p></li>
				<li>Define <a id="_idIndexMarker1308"/>the infrastructure requirements, the location of the training set, and its format, as follows:<p class="source-code">from sagemaker.model_monitor import DefaultModelMonitor</p><p class="source-code">from sagemaker.model_monitor.dataset_format import DatasetFormat</p><p class="source-code">ll_monitor = DefaultModelMonitor(role=role,</p><p class="source-code">    instance_count=1, instance_type='ml.m5.large')</p><p class="source-code">ll_monitor.suggest_baseline(baseline_dataset=training,</p><p class="source-code">    dataset_format=DatasetFormat.csv(header=True))</p></li>
				<li>As you can guess, this is running as a SageMaker Processing job, and you can find its log in <strong class="bold">CloudWatch Logs</strong> under the <strong class="source-inline">/aws/sagemaker/ProcessingJobs</strong> prefix. <p>Two JSON artifacts are available at its output location: <strong class="source-inline">statistics.json</strong> and <strong class="source-inline">constraints.json</strong>. We can view their content with <strong class="source-inline">pandas</strong> by running the following code:</p><p class="source-code">baseline = ll_monitor.latest_baselining_job</p><p class="source-code">constraints = pd.io.json.json_normalize(</p><p class="source-code">    baseline.suggested_constraints()</p><p class="source-code">    .body_dict["features"])</p><p class="source-code">schema = pd.io.json.json_normalize(</p><p class="source-code">    baseline.baseline_statistics().body_dict["features"])</p></li>
				<li>As shown <a id="_idIndexMarker1309"/>in the following screenshot, the <strong class="source-inline">constraints</strong> file gives us the inferred type of each feature, its completeness in the dataset, and whether it contains negative values or not:<div id="_idContainer169" class="IMG---Figure"><img src="Images/B17705_11_5.jpg" alt="Figure 11.5 – Viewing the inferred schema&#13;&#10;" width="1523" height="378"/></div><p class="figure-caption">Figure 11.5 – Viewing the inferred schema</p></li>
				<li>The <strong class="source-inline">statistics</strong> file adds basic statistics, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="Images/B17705_11_6.jpg" alt="Figure 11.6 – Viewing data statistics&#13;&#10;" width="1650" height="392"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – Viewing data statistics</p>
			<p>It also includes distribution information based on KLL sketches (https://arxiv.org/abs/1603.05346v2), a compact way to define quantiles.</p>
			<p>Once a <a id="_idIndexMarker1310"/>baseline has been created, we can set up a monitoring schedule in order to compare incoming traffic to the baseline.</p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor254"/>Setting up a monitoring schedule</h2>
			<p>We simply pass the <a id="_idIndexMarker1311"/>name of the endpoint, the statistics, the constraints, and the frequency at which the analysis should run. We will go for hourly, which is the shortest frequency allowed. The code is illustrated in the following snippet:</p>
			<p class="source-code">from sagemaker.model_monitor import CronExpressionGenerator</p>
			<p class="source-code">ll_monitor.create_monitoring_schedule(</p>
			<p class="source-code">    monitor_schedule_name='ll-housing-schedule',</p>
			<p class="source-code">    endpoint_input=ll_predictor.endpoint,</p>
			<p class="source-code">    statistics=ll_monitor.baseline_statistics(),</p>
			<p class="source-code">    constraints=ll_monitor.suggested_constraints(),</p>
			<p class="source-code">    schedule_cron_expression=CronExpressionGenerator.hourly())</p>
			<p>Here, the analysis will be performed by a built-in container. Optionally, we could provide our own container with bespoke analysis code. You can find more information at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-containers.html">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-containers.html</a>. </p>
			<p>Now, let's send some nasty data to the endpoint and see if SageMaker Model Monitor picks it up.</p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor255"/>Sending bad data</h2>
			<p>Unfortunately, a model may receive incorrect data at times. Maybe it's been corrupted at the source, maybe <a id="_idIndexMarker1312"/>the application in charge of invoking the endpoint is buggy, and so on. Let's simulate this and see how much impact this has on the quality of the prediction, as follows:</p>
			<ol>
				<li value="1">Starting from a valid sample, we get a correct prediction, as illustrated here:<p class="source-code">test_sample = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'</p><p class="source-code">ll_predictor.serializer =    </p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">ll_predictor.deserializer =  </p><p class="source-code">    sagemaker.deserializers.CSVDeserializer()</p><p class="source-code">response = ll_predictor.predict(test_sample)</p><p class="source-code">print(response)</p><p>The price of this house is USD 30,173:</p><p class="source-code"><strong class="bold">[['30.1734218597']]</strong></p></li>
				<li>Now, let's multiply the first feature by 10,000, as shown in the following code snippet. Scaling and unit errors are quite frequent in application code:<p class="source-code">bad_sample_1 = '632.0,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'</p><p class="source-code">response = ll_predictor.predict(bad_sample_1)</p><p class="source-code">print(response)</p><p>Ouch! The price is negative, as we can see here. Clearly, this is a bogus prediction:</p><p class="source-code"><strong class="bold">[['-35.7245635986']]</strong></p></li>
				<li>Let's try <a id="_idIndexMarker1313"/>negating the last feature by running the following code:<p class="source-code">bad_sample_2 = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,-4.98'</p><p class="source-code">response = ll_predictor.predict(bad_sample_2)</p><p class="source-code">print(response)</p><p>The prediction is much higher than what it should be, as we can see in the following snippet. This is a sneakier issue, which means it is harder to detect and could have serious business consequences:</p><p class="source-code"><strong class="bold">[['34.4245414734']]</strong></p></li>
			</ol>
			<p>You should try experimenting with bad data and see which features are the most brittle. All this traffic will be captured by SageMaker Model Monitor. Once the monitoring job has run, you should see entries in its <strong class="bold">violation report</strong>.</p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor256"/>Examining violation reports</h2>
			<p>Previously, we created <a id="_idIndexMarker1314"/>an hourly monitoring job. Don't worry if it takes a little more than 1 hour to see results; job execution is load-balanced by the backend, and short delays are likely:</p>
			<ol>
				<li value="1">We can find more information about our monitoring job in the SageMaker console, in the <strong class="bold">Processing jobs</strong> section. We can also call the <strong class="source-inline">describe_schedule()</strong> API and list executions with the <strong class="source-inline">list_executions()</strong> API, as follows:<p class="source-code">ll_executions = ll_monitor.list_executions()</p><p class="source-code">print(ll_executions)</p><p>Here, we can see three executions:</p><p class="source-code"><strong class="bold">[&lt;sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdd1d55a6d8&gt;,</strong></p><p class="source-code"><strong class="bold">&lt;sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdd1d581630&gt;,</strong></p><p class="source-code"><strong class="bold">&lt;sagemaker.model_monitor.model_monitoring.MonitoringExecution at 0x7fdce4b1c860&gt;]</strong></p></li>
				<li>The violations report is <a id="_idIndexMarker1315"/>stored as a JSON file in S3. We can read it and display it with <strong class="source-inline">pandas</strong>, as follows:<p class="source-code">violations = ll_monitor.latest_monitoring_constraint_violations()</p><p class="source-code">violations = pd.io.json.json_normalize(</p><p class="source-code">    violations.body_dict["violations"])</p><p class="source-code">violations</p><p>This prints out the violations that were detected by the last monitoring job, as shown in the following screenshot:</p><div id="_idContainer171" class="IMG---Figure"><img src="Images/B17705_11_7.jpg" alt="Figure 11.7 – Viewing violations&#13;&#10;" width="1567" height="326"/></div><p class="figure-caption">Figure 11.7 – Viewing violations</p></li>
				<li>Of course, we can also fetch the file in S3 and display its contents, as follows:<p class="source-code">%%bash -s "$report_path"</p><p class="source-code">echo $1</p><p class="source-code">aws s3 ls --recursive $1</p><p class="source-code">aws s3 cp --recursive $1 .</p><p>Here's a sample entry, warning <a id="_idIndexMarker1316"/>us that the model received a fractional value for the <strong class="source-inline">chas</strong> feature, although it's defined as an integer in the schema:</p><p class="source-code"><strong class="bold">{</strong></p><p class="source-code"><strong class="bold">    "feature_name" : "chas",</strong></p><p class="source-code"><strong class="bold">    "constraint_check_type" : "data_type_check",</strong></p><p class="source-code"><strong class="bold">    "description" : "Data type match requirement is not met.</strong></p><p class="source-code"><strong class="bold">        Expected data type: Integral, Expected match: 100.0%.  </strong></p><p class="source-code"><strong class="bold">        Observed: Only 0.0% of data is Integral."</strong></p><p class="source-code"><strong class="bold">}</strong></p><p>We could also emit these violations to CloudWatch metrics and trigger alarms to notify developers of potential data-quality issues. You can find more information at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html</a>. </p></li>
				<li>When you're done, don't forget to delete the monitoring schedule and the endpoint itself, as follows:<p class="source-code">response = ll_monitor.delete_monitoring_schedule()</p><p class="source-code">ll_predictor.delete_endpoint()</p></li>
			</ol>
			<p>As you can see, SageMaker Model Monitor helps you capture both incoming data and predictions, a useful feature for model testing. In addition, you can also perform data-quality analysis using a built-in container or your own.</p>
			<p>In the next section, we're going to move away from endpoints and learn how to deploy models to container services.</p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor257"/>Deploying models to container services</h1>
			<p>Previously, we saw <a id="_idIndexMarker1317"/>how to fetch a model artifact <a id="_idIndexMarker1318"/>in S3 and how to extract the actual <a id="_idIndexMarker1319"/>model from it. Knowing <a id="_idIndexMarker1320"/>this, it's pretty easy to deploy it on a <a id="_idIndexMarker1321"/>container service, such as <strong class="bold">Amazon Elastic Container Service</strong> (<strong class="bold">ECS</strong>), <strong class="bold">Amazon Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>), or <strong class="bold">Amazon Fargate</strong>. </p>
			<p>Maybe it's company policy to deploy everything in containers, maybe you just like them, or maybe both! Whatever the reason is, you can definitely do it. There's nothing specific to SageMaker here, and the AWS documentation for these services will tell you everything you need to know.</p>
			<p> A sample high-level process could look like this:</p>
			<ol>
				<li value="1">Train a model on SageMaker.</li>
				<li>When training is complete, grab the artifact and extract the model.</li>
				<li>Push the model to a Git repository.</li>
				<li>Write a task definition (for ECS and Fargate) or a pod definition (for EKS). It could use one of the built-in containers or your own. Then, it could run a model server or your own code to clone the model from your Git repository, load it, and serve predictions.</li>
				<li>Using this definition, run a container on your cluster.</li>
			</ol>
			<p>Let's apply this to Amazon Fargate.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor258"/>Training on SageMaker and deploying on Amazon Fargate</h2>
			<p><strong class="bold">Amazon Fargate</strong> lets you run containers on fully <a id="_idIndexMarker1322"/>managed infrastructure (<a href="https://aws.amazon.com/fargate">https://aws.amazon.com/fargate</a>). There's no need to create and manage <a id="_idIndexMarker1323"/>clusters, which makes it ideal for users who don't want <a id="_idIndexMarker1324"/>to get involved with infrastructure details. However, please note that, at the time of writing, Fargate doesn't support <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) containers.</p>
			<h3>Preparing a model</h3>
			<p>We prepare the model using the following steps:</p>
			<ol>
				<li value="1">First, we train <a id="_idIndexMarker1325"/>a TensorFlow model on Fashion-MNIST. Business as usual.</li>
				<li>We find the location of the model artifact in S3 and set it as an environment variable, as follows:<p class="source-code">%env model_data {tf_estimator.model_data}</p></li>
				<li>We download the artifact from S3 and extract it to a local directory, like this:<p class="source-code">%%sh</p><p class="source-code">aws s3 cp ${model_data} .</p><p class="source-code">mkdir test-models</p><p class="source-code">tar xvfz model.tar.gz -C test-models</p></li>
				<li>We open a terminal and commit the model to a public Git repository, as illustrated in the following code snippet. I'm using one of mine here (<a href="https://gitlab.com/juliensimon/test-models">https://gitlab.com/juliensimon/test-models</a>); you should replace it with yours:<p class="source-code"><strong class="bold">&lt;initialize git repository&gt;</strong></p><p class="source-code"><strong class="bold">$ cd test-models</strong></p><p class="source-code"><strong class="bold">$ git add model</strong></p><p class="source-code"><strong class="bold">$ git commit -m "New model"</strong></p><p class="source-code"><strong class="bold">$ git push</strong></p></li>
			</ol>
			<h3>Configuring Fargate</h3>
			<p>Now that the model <a id="_idIndexMarker1326"/>is available in a repository, we need to configure Fargate. We'll use the command line this time. You could do the same with <strong class="source-inline">boto3</strong> or any other language SDK. We'll proceed as follows:</p>
			<ol>
				<li value="1"><strong class="source-inline">ecs-cli</strong> is a convenient CLI tool used to manage clusters. Let's install it by running the following code:<p class="source-code">%%sh</p><p class="source-code">sudo curl -o /usr/local/bin/ecs-cli https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest</p><p class="source-code">sudo chmod 755 /usr/local/bin/ecs-cli</p></li>
				<li>We use it to "create" a Fargate cluster. In practice, this isn't creating any infrastructure; we're only defining a cluster name that we'll use to run tasks. Please make sure that your <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) role includes the required permission for <strong class="source-inline">ecs:CreateCluster</strong>. If not, please add it before continuing. The code is illustrated in the following snippet:<p class="source-code">%%sh </p><p class="source-code">aws ecs create-cluster --cluster-name fargate-demo</p><p class="source-code">ecs-cli configure --cluster fargate-demo --region eu-west-1</p></li>
				<li>We create <a id="_idIndexMarker1327"/>a log group in <strong class="bold">CloudWatch</strong> where our container will write its output. We only need to do this once. Here's the code to accomplish this:<p class="source-code">%%sh</p><p class="source-code">aws logs create-log-group --log-group-name awslogs-tf-ecs</p></li>
				<li>We will need a <strong class="bold">security group</strong> for our task that opens the two inbound TensorFlow Serving ports (<strong class="source-inline">8500</strong> for Google remote procedure call (<strong class="bold">gRPC</strong>); <strong class="source-inline">8501</strong> for the <strong class="bold">REpresentational State Transfer</strong> (<strong class="bold">REST</strong>) API). If you don't have one already, you can easily create one in the <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) console. Here, I created one in my default <strong class="bold">virtual private cloud</strong> (<strong class="bold">VPC</strong>). It looks like this: </li>
			</ol>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="Images/B17705_11_8.jpg" alt="Figure 11.8 – Viewing the security group&#13;&#10;" width="1650" height="839"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Viewing the security group</p>
			<h3>Defining a task</h3>
			<p>Now, we need to write a <strong class="bold">JSON</strong> file containing a <strong class="bold">task definition</strong>: the container image to use, its <a id="_idIndexMarker1328"/>entry point, and its system and network properties. Let's get started, as follows:</p>
			<ol>
				<li value="1">First, we define the amount of <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) and memory that the task is allowed to consume. Unlike ECS and EKS, Fargate only allows a limited set of values, available at <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html</a>. We will go for 4 <strong class="bold">virtual CPUs</strong> (<strong class="bold">vCPUs</strong>) and 8 GB of <strong class="bold">random-access memory</strong> (<strong class="bold">RAM</strong>), as illustrated in the following code snippet:<p class="source-code">{</p><p class="source-code">  "requiresCompatibilities": ["FARGATE"],</p><p class="source-code">  "family": "inference-fargate-tf-230",</p><p class="source-code">  "memory": "8192",</p><p class="source-code">  "cpu": "4096",</p></li>
				<li>Next, we define a container that will load our model and run predictions. We will use the DLC for TensorFlow 2.3.0. You can find a full list at <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md">https://github.com/aws/deep-learning-containers/blob/master/available_images.md</a>. The code is illustrated in the following snippet:<p class="source-code">  "containerDefinitions": [{</p><p class="source-code">    "name": "dlc-tf-inference",</p><p class="source-code">    "image": "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.2-cpu-py37-ubuntu18.04",</p><p class="source-code">    "essential": true,</p></li>
				<li>Its entry point creates a directory, clones the repository where we pushed the model, and <a id="_idIndexMarker1329"/>launches TensorFlow Serving, as follows:<p class="source-code">    "command": [</p><p class="source-code">       "mkdir -p /test &amp;&amp; cd /test &amp;&amp; git clone https://gitlab.com/juliensimon/test-models.git &amp;&amp; tensorflow_model_server --port=8500 </p><p class="source-code">--rest_api_port=8501 --model_name=1 </p><p class="source-code">--model_base_path=/test/test-models/model"</p><p class="source-code">    ],</p><p class="source-code">    "entryPoint": ["sh","-c"],</p></li>
				<li>Accordingly, we map the two TensorFlow Serving ports, like this: <p class="source-code">    "portMappings": [</p><p class="source-code">        {</p><p class="source-code">          "hostPort": 8500,</p><p class="source-code">          "protocol": "tcp",</p><p class="source-code">          "containerPort": 8500</p><p class="source-code">        },</p><p class="source-code">        {</p><p class="source-code">          "hostPort": 8501,</p><p class="source-code">          "protocol": "tcp",</p><p class="source-code">          "containerPort": 8501</p><p class="source-code">        }</p><p class="source-code">    ],</p></li>
				<li>We define the log configuration that's pointing at the CloudWatch log group we created earlier, as follows:<p class="source-code">    "logConfiguration": {</p><p class="source-code">      "logDriver": "awslogs",</p><p class="source-code">        "options": {</p><p class="source-code">          "awslogs-group": "awslogs-tf-ecs",</p><p class="source-code">          "awslogs-region": "eu-west-1",</p><p class="source-code">          "awslogs-stream-prefix": "inference"</p><p class="source-code">        }</p><p class="source-code">    }</p><p class="source-code">  }],</p></li>
				<li>We set the networking <a id="_idIndexMarker1330"/>mode for the container, as illustrated in the following code snippet. <strong class="source-inline">awsvpc</strong> is the most flexible option, and it will allow our container to be publicly accessible, as explained at <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html</a>. It will create an <strong class="bold">elastic network interface</strong> in the subnet of our choice:<p class="source-code">  "networkMode": "awsvpc"</p></li>
				<li>Finally, we define an IAM role for the task. If this is the first time you're working with <a id="_idIndexMarker1331"/>ECS, you should create this role in the IAM console. You can find instructions for this at https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html. The code is illustrated in the following snippet:<p class="source-code">  "executionRoleArn":  </p><p class="source-code">  "arn:aws:iam::123456789012:role/ecsTaskExecutionRole"</p><p class="source-code">}</p></li>
			</ol>
			<h3>Running a task</h3>
			<p>We're now ready to run our task using the security group we created earlier and one of the subnets in our default VPC. Let's get started, as follows:</p>
			<ol>
				<li value="1">We launch the <a id="_idIndexMarker1332"/>task with the <strong class="source-inline">run-task</strong> API, passing the family name of the task definition (not the filename!). Please pay attention to the version number as well as it will automatically increase every time you register a new version of the task definition, so make sure you're using the latest one. The code is illustrated in the following snippet:<p class="source-code">%%sh</p><p class="source-code">aws ecs run-task</p><p class="source-code">  --cluster fargate-demo </p><p class="source-code">  --task-definition inference-fargate-tf-230:1 </p><p class="source-code">  --count 1</p><p class="source-code">  --launch-type FARGATE</p><p class="source-code">  --network-configuration </p><p class="source-code">    "awsvpcConfiguration={subnets=[$SUBNET_ID],</p><p class="source-code">     securityGroups=[$SECURITY_GROUP_ID],</p><p class="source-code">     assignPublicIp=ENABLED}"</p></li>
				<li>A few seconds later, we can see our prediction container running (showing the task <strong class="bold">identifier</strong> (<strong class="bold">ID</strong>), state, ports, and task definition), as follows:<p class="source-code">%%sh</p><p class="source-code">ecs-cli ps --desired-status RUNNING</p><p class="source-code"><strong class="bold">a9c9a3a8-8b7c-4dbb-9ec4-d20686ba5aec/dlc-tf-inference  </strong></p><p class="source-code"><strong class="bold">RUNNING  </strong></p><p class="source-code"><strong class="bold">52.49.238.243:8500-&gt;8500/tcp, </strong></p><p class="source-code"><strong class="bold">52.49.238.243:8501-&gt;8501/tcp                         inference-fargate-tf230:1</strong></p></li>
				<li>Using <a id="_idIndexMarker1333"/>the public <strong class="bold">Internet Protocol</strong> (<strong class="bold">IP</strong>) address of the container, we build a TensorFlow Serving prediction request with 10 sample images and send it to our container, as follows:<p class="source-code">import random, json, requests</p><p class="source-code">inference_task_ip = '52.49.238.243'</p><p class="source-code">inference_url = 'http://' +   </p><p class="source-code">                inference_task_ip +  </p><p class="source-code">                ':8501/v1/models/1:predict'</p><p class="source-code">indices = random.sample(range(x_val.shape[0] - 1), 10)</p><p class="source-code">images = x_val[indices]/255</p><p class="source-code">labels = y_val[indices]</p><p class="source-code">data = images.reshape(num_samples, 28, 28, 1)</p><p class="source-code">data = json.dumps(</p><p class="source-code">    {"signature_name": "serving_default", </p><p class="source-code">     "instances": data.tolist()})</p><p class="source-code">headers = {"content-type": "application/json"}</p><p class="source-code">json_response = requests.post(</p><p class="source-code">    inference_url, </p><p class="source-code">    data=data, </p><p class="source-code">    headers=headers)</p><p class="source-code">predictions = json.loads(</p><p class="source-code">    json_response.text)['predictions']</p><p class="source-code">predictions = np.array(predictions).argmax(axis=1)</p><p class="source-code">print("Labels     : ", labels)</p><p class="source-code">print("Predictions: ", predictions)</p><p class="source-code"><strong class="bold">Labels     :  [9 8 8 8 0 8 9 7 1 1]</strong></p><p class="source-code"><strong class="bold">Predictions:  [9 8 8 8 0 8 9 7 1 1]</strong></p></li>
				<li>When we're done, we <a id="_idIndexMarker1334"/>stop the task using the task <strong class="bold">Amazon Resource Name</strong> (<strong class="bold">ARN</strong>) returned by the <strong class="source-inline">run-task</strong> API and delete the cluster, as illustrated in the following code snippet. Of course, you can also use the ECS console:<p class="source-code">%%sh</p><p class="source-code">aws ecs stop-task --cluster fargate-demo \</p><p class="source-code">                  --task $TASK_ARN</p><p class="source-code">ecs-cli down --force --cluster fargate-demo</p></li>
			</ol>
			<p>The processes for ECS and EKS are extremely similar. You can find simple examples at <a href="https://gitlab.com/juliensimon/dlcontainers">https://gitlab.com/juliensimon/dlcontainers</a>. They should be a good starting point if you wish to build your own workflow.</p>
			<p>Kubernetes fans <a id="_idIndexMarker1335"/>can also use <strong class="bold">SageMaker Operators for Kubernetes</strong> and use native tools such as <strong class="source-inline">kubectl</strong> to train and deploy models. A detailed tutorial is available at <a href="https://sagemaker.readthedocs.io/en/stable/workflows/kubernetes/index.html">https://sagemaker.readthedocs.io/en/stable/workflows/kubernetes/index.html</a>.</p>
			<h1 id="_idParaDest-260"><a id="_idTextAnchor259"/>Summary</h1>
			<p>In this chapter, you learned about model artifacts, what they contain, and how to use them to export models outside of SageMaker. You also learned how to import and deploy existing models, as well as how to manage endpoints in detail, both with the SageMaker SDK and the <strong class="source-inline">boto3</strong> SDK. </p>
			<p>Then, we discussed alternative deployment scenarios with SageMaker, using either batch transform or inference pipelines, as well as outside of SageMaker with container services.</p>
			<p>Finally, you learned how to use SageMaker Model Monitor to capture endpoint data and monitor data quality.</p>
			<p>In the next chapter, we'll discuss automating ML workflows with three different AWS services: <strong class="bold">AWS CloudFormation</strong>, the <strong class="bold">AWS Cloud Development Kit</strong> (<strong class="bold">AWS CDK</strong>), and <strong class="bold">Amazon SageMaker Pipelines</strong>.</p>
		</div>
	</div></body></html>
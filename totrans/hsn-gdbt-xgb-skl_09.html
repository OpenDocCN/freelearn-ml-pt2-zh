<html><head></head><body>
		<div id="_idContainer141">
			<h1 id="_idParaDest-153"><em class="italic"><a id="_idTextAnchor161"/>Chapter 7</em>: Discovering Exoplanets with XGBoost</h1>
			<p>In this chapter, you will journey through the stars in an attempt to discover exoplanets with <strong class="source-inline">XGBClassifier</strong> as your guide.</p>
			<p>The reason for this chapter is twofold. The first is that it's important to gain practice in a top-to-bottom study using XGBoost since for all practical purposes, that is what you will normally do with XGBoost. Although you may not discover exoplanets with XGBoost on your own, the strategies that you implement here, which include choosing the correct scoring metric and carefully fine-tuning hyperparameters with that scoring metric in mind, apply to any practical use of XGBoost. The second reason for this particular case study is that it's essential for all machine learning practitioners to be proficient at competently handling imbalanced datasets, which is the key theme of this particular chapter.</p>
			<p>Specifically, you will gain new skills in using the <strong class="bold">confusion matrix</strong> and the <strong class="bold">classification report</strong>, understanding <strong class="bold">precision versus recall</strong>, resampling data, applying <strong class="source-inline">scale_pos_weight</strong>, and more. Getting the best results from <strong class="source-inline">XGBClassifier</strong> will require careful analysis of the imbalanced data and clear expectations of the goal at hand. In this chapter, <strong class="source-inline">XGBClassifier</strong> is the centerpiece of a top-to-bottom study analyzing light data to predict exoplanets in the universe.</p>
			<p>In this chapter, we cover the following main topics:</p>
			<ul>
				<li><p>Searching for exoplanets</p></li>
				<li><p>Analyzing the confusion matrix</p></li>
				<li><p>Resampling imbalanced data </p></li>
				<li><p>Tuning and scaling XGBClassifier</p></li>
			</ul>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor162"/>Technical requirements</h1>
			<p>The code for this chapter may be found at <a href="https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter07">https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter07</a>.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor163"/>Searching for exoplanets</h1>
			<p>In this section, we'll begin the search for exoplanets by analyzing the Exoplanets dataset. We'll provide <a id="_idIndexMarker433"/>historical context for the discovery of exoplanets before attempting to detect them via plotting and observing light graphs. Plotting time series is a valuable machine learning skill that may be used to gain insights into any time series datasets. Finally, we'll make initial predictions using machine learning before revealing a glaring shortcoming.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor164"/>Historical background</h2>
			<p>Astronomers have been gathering information from light since antiquity. With the advent of the <a id="_idIndexMarker434"/>telescope, astronomical knowledge surged in the 17<span class="superscript">th</span> century. The combination of telescopes and mathematical models empowered 18<span class="superscript">th</span>-century astronomers to predict planetary locations and eclipses within our own solar system with great precision.</p>
			<p>In the 20th century, astronomical research continued with more advanced technology and more complex mathematics. Planets revolving around other stars, called exoplanets, were <a id="_idIndexMarker435"/>discovered in the habitable zone. A planet in the habitable zone means that the exoplanet's location and size are comparable to Earth, and therefore it's a candidate for harboring liquid water and life.</p>
			<p>These exoplanets are not viewed directly via telescopes, rather they are inferred through periodic changes in starlight. An object that periodically revolves around a star that is large enough to block a detectable fraction of starlight is by definition a planet. Discovering exoplanets from starlight requires measuring light fluctuations over extended intervals of time. Since the change in light is often very minute, it's not easy to determine whether an exoplanet is actually present.</p>
			<p>In this chapter, we are going to predict whether stars have exoplanets with XGBoost.</p>
			<h2 id="_idParaDest-157">The Exoplanet datas<a id="_idTextAnchor165"/>et</h2>
			<p>You previewed the Exoplanet dataset in <a href="B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a>, <em class="italic">From Gradient Boosting to XGBoost</em>, to uncover the time advantage that <a id="_idIndexMarker436"/>XGBoost has over comparable ensemble methods for large datasets. In this chapter, we will take a deeper look at the Exoplanet dataset.</p>
			<p>This Exoplanet dataset is taken from <em class="italic">NASA Kepler Space Telescope</em>, <em class="italic">Campaign 3</em>, <em class="italic">Summer 2016</em>. Information about the <a id="_idIndexMarker437"/>data source is available on Kaggle at <a href="https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data">https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data</a>. Of all the stars in the dataset, 5,050 do not have exoplanets, while 37 have exoplanets.</p>
			<p>The 300+ columns and 5,000+ rows equal 1.5 million plus entries. When multiplied by 100 XGBoost trees, this is 150 million plus data points. To expedite matters, we start with a subset of the data. Starting with a subset is a common practice when dealing with large datasets, to save time.</p>
			<p><strong class="source-inline">pd.read_csv</strong> contains an <strong class="source-inline">nrows</strong> parameter, used to limit the number of rows. Note that <strong class="source-inline">nrows=n</strong> selects the first <em class="italic">n</em> rows of the dataset. Depending on the data structure, additional code may be required to ensure that the subset is representative of the whole. Let's get started.</p>
			<p>Import <strong class="source-inline">pandas</strong>, then load <strong class="source-inline">exoplanets.csv</strong> with <strong class="source-inline">nrows=400</strong>. Then view the data:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">df = pd.read_csv('exoplanets.csv', nrows=400)</p>
			<p class="source-code">df.head()</p>
			<p>The output should appear as follows:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B15551_07_01.jpg" alt="Figure 7.1 – Exoplanet DataFrame"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Exoplanet DataFrame</p>
			<p>The large number of columns (<strong class="bold">3198</strong>) listed underneath the DataFrame makes sense. When looking for periodic changes in light, you need enough data points to find periodicity. The revolutions of planets within our own solar system range from 88 days (Mercury) to 165 years (Neptune). If exoplanets are to be detected, data points must be examined frequently enough so as not to miss the transit of the planet when the planet orbits in front of the star.</p>
			<p>Since there are only 37 exoplanet stars, it's important to know how many exoplanet stars are contained in the subset.</p>
			<p>The <strong class="source-inline">.value_counts()</strong> method determines the number of each value in a particular column. Since <a id="_idIndexMarker438"/>we are interested in the <strong class="source-inline">LABEL</strong> column, the number of exoplanet stars may be found using the following code:</p>
			<p class="source-code">df['LABEL'].value_counts()</p>
			<p>The output is as follows:</p>
			<p class="source-code">1    363 2     37 Name: LABEL, dtype: int64</p>
			<p>All exoplanet stars are included in our subset. As <strong class="source-inline">.head()</strong> reveals, the exoplanet stars are at the beginning.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor166"/>Graphing the data</h2>
			<p>The expectation is <a id="_idIndexMarker439"/>that when an exoplanet blocks light from a star, the light flux goes down. If drops in flux occur periodically, an exoplanet is likely the reason since, by definition, a planet is a large object orbiting a star.</p>
			<p>Let's visualize the data by graphing:</p>
			<ol>
				<li><p>Import <strong class="source-inline">matplotlib</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">seaborn</strong>, then set <strong class="source-inline">seaborn</strong> to the dark grid as follows:</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import seaborn as sns</p><p class="source-code">sns.set()</p><p>When plotting light fluctuations, the <strong class="source-inline">LABEL</strong> column is not of interest. The <strong class="source-inline">LABEL</strong> column will be our target column for machine learning. </p><p class="callout-heading">Tip</p><p class="callout"><strong class="source-inline">seaborn</strong> is recommended to improve your <strong class="source-inline">matplotlib</strong> graphs. The <strong class="source-inline">sns.set()</strong> default provides a nice light-gray background with a white grid. Furthermore, many standard graphs, such as <strong class="source-inline">plt.hist()</strong>, look more aesthetically pleasing with this Seaborn default in place. For more <a id="_idIndexMarker440"/>information on Seaborn, check out <a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a>.</p></li>
				<li><p>Now, let's split the data into <strong class="source-inline">X</strong>, the predictor columns (which we will graph), and <strong class="source-inline">y</strong>, the target column. Note that for the Exoplanet dataset, the target column is the first column, not the last:</p><p class="source-code">X = df.iloc[:,1:]</p><p class="source-code">y = df.iloc[:,0]</p></li>
				<li><p>Now write a <a id="_idIndexMarker441"/>function called <strong class="source-inline">light_plot</strong>, which takes as input the index of the data (the row) that plots all data points as <em class="italic">y</em> coordinates (the light flux), and the number of observations as <em class="italic">x</em> coordinates. Use appropriate labels for the graph as follows:</p><p class="source-code">def light_plot(index):</p><p class="source-code">    y_vals = X.iloc[index]</p><p class="source-code">    x_vals = np.arange(len(y_vals))</p><p class="source-code">    plt.figure(figsize=(15,8))</p><p class="source-code">    plt.xlabel('Number of Observations')</p><p class="source-code">    plt.ylabel('Light Flux')</p><p class="source-code">    plt.title('Light Plot ' + str(index), size=15)</p><p class="source-code">    plt.plot(x_vals, y_vals)</p><p class="source-code">    plt.show()</p></li>
				<li><p>Now, call the function to plot the first index. This star has been classified as an exoplanet star:</p><p class="source-code">light_plot(0)</p><p>Here is the expected graph for our first light plot:</p><div id="_idContainer132" class="IMG---Figure"><img src="image/B15551_07_02.jpg" alt="Figure 7.2 – Light plot 0. Periodic drops in light are present"/></div><p class="figure-caption">Figure 7.2 – Light plot 0. Periodic drops in light are present</p><p>There are clear <a id="_idIndexMarker442"/>drops in the data that occur periodically. However, concluding that an exoplanet is present is not obvious from this graph alone.</p></li>
				<li><p>By comparison, contrast this plot with the 37<span class="superscript">th</span> index, the first non-exoplanet star in the dataset:</p><p class="source-code">light_plot(37)</p><p>Here is the expected graph for the 37<span class="superscript">th</span> index:</p><div id="_idContainer133" class="IMG---Figure"><img src="image/B15551_07_03.jpg" alt="Figure 7.3 – Light plot 37"/></div><p class="figure-caption">Figure 7.3 – Light plot 37</p><p>Increases and <a id="_idIndexMarker443"/>decreases in light are present, but not over the entire range.</p><p>There are clear drops in the data, but they are not periodic throughout the graph. The frequency of the drops does not recur consistently. Based on this evidence alone, it's not enough to determine the presence of an exoplanet.</p></li>
				<li><p>Here is the second light plot of an exoplanet star:</p><p class="source-code">light_plot(1)</p><p>Here is the expected graph for the first index:</p></li>
			</ol>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B15551_07_04.jpg" alt="Figure 7.4 – Clear periodic drops indicate the presence of an exoplanet"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Clear periodic drops indicate the presence of an exoplanet</p>
			<p>The plot shows clear periodicity with large drops in light flux making an exoplanet extremely likely! If all the <a id="_idIndexMarker444"/>plots were this clear, machine learning would be unnecessary. As the other plots reveal, concluding that an exoplanet is present is usually not this clear.</p>
			<p>The purpose here is to highlight the data and the difficulty of classifying exoplanets based on visual graphs alone. Astronomers use different methods to classify exoplanets, and machine learning is one such method. </p>
			<p>Although this dataset is a time series, the goal is not to predict light flux for the next unit of time, but rather to classify the star based on all the data. In this respect, machine learning classifiers may be used to predict whether a given star hosts an exoplanet. The idea is to train the classifier on the provided data, which may in turn be used to predict exoplanets on new data. In this chapter, we attempt to classify the exoplanets within the data using <strong class="source-inline">XGBClassifier</strong>. Before we move on to classify the data, we must first prepare the data.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor167"/>Preparing data</h2>
			<p>We witnessed in the <a id="_idIndexMarker445"/>previous section that not all graphs are clear enough to determine the existence of an exoplanet. This is where machine learning may be of great benefit. To begin, let's prepare the data for machine learning:</p>
			<ol>
				<li value="1"><p>First, we need the dataset to be numerical with no null values. Check the data types and null values using <strong class="source-inline">df.info()</strong>:</p><p class="source-code">df.info()</p><p>Here is the expected output:</p><p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p><p class="source-code">RangeIndex: 400 entries, 0 to 399</p><p class="source-code">Columns: 3198 entries, LABEL to FLUX.3197</p><p class="source-code">dtypes: float64(3197), int64(1)</p><p class="source-code">memory usage: 9.8 MB</p><p>The subset contains 3,197 floats, and 1 int, so all columns are numerical. No information is provided about null values due to the large number of columns. </p></li>
				<li><p>We can use the <strong class="source-inline">.sum()</strong> method twice on <strong class="source-inline">.null()</strong> to sum all null values, once to sum the null values in each column, and the second time to sum all columns:</p><p class="source-code">df.isnull().sum().sum()</p><p>The expected output is as follows:</p><p class="source-code">0</p></li>
			</ol>
			<p>Since there are no null values and the data is numerical, we will proceed with machine learning.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor168"/>Initial XGBClassifier</h2>
			<p>To start <a id="_idIndexMarker446"/>building an initial XGBClassifier, take the following steps:</p>
			<ol>
				<li value="1"><p>Import <strong class="source-inline">XGBClassifier</strong> and <strong class="source-inline">accuracy_score</strong>:</p><p class="source-code">from xgboost import XGBClassifier from sklearn.metrics import accuracy_score</p></li>
				<li><p>Split the model into a training and test set:</p><p class="source-code">from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</p></li>
				<li><p>Build and score the model using <strong class="source-inline">booster='gbtree'</strong>, <strong class="source-inline">objective='binary:logistic'</strong>, and <strong class="source-inline">random_state=2</strong> as parameters:</p><p class="source-code">model = XGBClassifier(booster='gbtree', objective='binary:logistic', random_state=2)model.fit(X_train, y_train)y_pred = model.predict(X_test)score = accuracy_score(y_pred, y_test)print('Score: ' + str(score))</p><p>The score is as follows:</p><p class="source-code">Score: 0.89</p></li>
			</ol>
			<p>Correctly classifying 89% of stars seems like a good starting point, but there is one glaring issue.</p>
			<p>Can you figure it out?</p>
			<p>Imagine that you present your model to your astronomy professor. Assuming your professor is well-trained in data analysis, your professor would respond, "I see that you obtained 89% accuracy, but exoplanets represent 10% of the data, so how do you know your results aren't better than a model that predicts no exoplanets 100% of the time?"</p>
			<p>Therein lies the issue. If the model determines that no stars contain exoplanets, its accuracy will be approximately 90% since 9 out of 10 stars do not contain exoplanets.</p>
			<p><em class="italic">With imbalanced data, accuracy isn't enough.</em></p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor169"/>Analyzing the confusion matrix</h1>
			<p>A confusion matrix is a table that summarizes the correct and incorrect predictions of a classification model. The <a id="_idIndexMarker447"/>confusion matrix is ideal for analyzing imbalanced data because it provides more information on which predictions are correct, and which predictions are wrong.</p>
			<p>For the Exoplanet subset, here is the expected output for a perfect confusion matrix:</p>
			<p class="source-code">array([[88, 0],</p>
			<p class="source-code">       [ 0,  12]])</p>
			<p>When all positive entries are on the left diagonal, the model has 100% accuracy. A perfect confusion matrix here predicts 88 non-exoplanet stars and 12 exoplanet stars. Notice that the confusion matrix does not provide labels, but in this case, labels may be inferred based on the size.</p>
			<p>Before getting into further detail, let's see the actual confusion matrix using scikit-learn.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor170"/>confusion_matrix </h2>
			<p>Import <strong class="source-inline">confusion_matrix</strong> from <strong class="source-inline">sklearn.metrics</strong> as <a id="_idIndexMarker448"/>follows:</p>
			<p class="source-code">from sklearn.metrics import confusion_matrix</p>
			<p>Run <strong class="source-inline">confusion_matrix</strong> with <strong class="source-inline">y_test</strong> and <strong class="source-inline">y_pred</strong> as inputs (variables obtained in the previous section), making sure to put <strong class="source-inline">y_test</strong> first:</p>
			<p class="source-code">confusion_matrix(y_test, y_pred)</p>
			<p>The output is as follows:</p>
			<p class="source-code">array([[86, 2],</p>
			<p class="source-code">       [9,  3]])</p>
			<p>The numbers on the diagonals of the confusion matrix reveal <strong class="source-inline">86</strong> correct non-exoplanet-star predictions and only <strong class="source-inline">3</strong> correct exoplanet star predictions.</p>
			<p>In the upper-right corner of the matrix, the number <strong class="source-inline">2</strong> reveals that two non-exoplanet-stars were misclassified as exoplanet stars. Similarly, in the bottom-left corner of the matrix, the number <strong class="source-inline">9</strong> reveals that <strong class="source-inline">9</strong> exoplanet stars were misclassified as non-exoplanet-stars.</p>
			<p>When analyzed horizontally, 86 of 88 non-exoplanet stars were correctly classified, while only 3 of 12 exoplanet stars were correctly classified.</p>
			<p>As you can see, the <a id="_idIndexMarker449"/>confusion matrix reveals important details of the model's predictions that an accuracy score is unable to pick up on.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor171"/>classification_report</h2>
			<p>The various <a id="_idIndexMarker450"/>percentages from the numbers revealed in the confusion matrix in the previous section are contained within a classification report. Let's view the classification report:</p>
			<ol>
				<li value="1"><p>Import <strong class="source-inline">classification_report</strong> from <strong class="source-inline">sklearn.metrics</strong>:</p><p class="source-code">from sklearn.metrics import classification_report</p></li>
				<li><p>Place <strong class="source-inline">y_test</strong> and <strong class="source-inline">y_pred</strong> inside <strong class="source-inline">clasification_report</strong>, making sure to put <strong class="source-inline">y_test</strong> first. Then place <strong class="source-inline">classification_report</strong> inside the global print function to keep the output aligned and easy to read:</p><p class="source-code">print(classification_report(y_test, y_pred))</p><p>Here is the expected output: </p><p class="source-code">              precision    recall  f1-score   support</p><p class="source-code">           1       0.91      0.98      0.94        88</p><p class="source-code">           2       0.60      0.25      0.35        12</p><p class="source-code">    accuracy                           0.89       100</p><p class="source-code">   macro avg       0.75      0.61      0.65       100</p><p class="source-code">weighted avg       0.87      0.89      0.87       100</p></li>
			</ol>
			<p>It's important to understand what the preceding scores mean, so let's review them one at a time.</p>
			<h3>Precision</h3>
			<p>Precision gives the <a id="_idIndexMarker451"/>predictions of the positive cases (2s) that are actually correct. It's technically defined in terms of true positives and false positives.</p>
			<h4>True positives </h4>
			<p>Here are a <a id="_idIndexMarker452"/>definition and example of true positives:</p>
			<ul>
				<li><p>Definition – Number of labels correctly predicted as positive. </p></li>
				<li><p>Example – 2s are correctly predicted as 2s.</p></li>
			</ul>
			<h4>False positives</h4>
			<p>Here are a definition <a id="_idIndexMarker453"/>and example of false positives:</p>
			<ul>
				<li><p>Definition – Number of positive labels incorrectly predicted as negative.</p></li>
				<li><p>Example – For exoplanet stars, 2s are incorrectly predicted as 1s.</p></li>
			</ul>
			<p>The definition of precision is most often referred to in its <a id="_idIndexMarker454"/>math<a id="_idTextAnchor172"/>ematical form as follows:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/Formula_07_001.jpg" alt=""/>
				</div>
			</div>
			<p>Here TP stands for true positive and FP stands for false positive.</p>
			<p>In the E<a id="_idTextAnchor173"/>xoplanet dataset, we have the following two mathematical forms:</p>
			<p class="figure-caption"><img src="image/Formula_07_002.png" alt=""/></p>
			<p>and</p>
			<p class="figure-caption"><img src="image/Formula_07_003.png" alt=""/></p>
			<p>Precision gives the <a id="_idIndexMarker455"/>percentage of correct predictions for each target class. Now let's review other key scoring metrics that the classification report reveals.</p>
			<h3>Recall</h3>
			<p>Recall gives you the <a id="_idIndexMarker456"/>percentage of positive cases that your predictions uncovered. Recall is the number of true positives divided by the true positives plus false negatives. </p>
			<h4>False negatives</h4>
			<p>Here are a definition <a id="_idIndexMarker457"/>and example of false negatives:</p>
			<ul>
				<li><p>Definition – Number of labels incorrectly predicted as negative.</p></li>
				<li><p>Example – For exoplanet star predictions, 2s are incorrectly predicted as 1s.</p></li>
			</ul>
			<p>I<a id="_idTextAnchor174"/>n mathematical form, this looks as follows:</p>
			<p class="figure-caption"><img src="image/Formula_07_004.png" alt=""/></p>
			<p>Here TP stands for true positive and FN stands for false negative.</p>
			<p>In th<a id="_idTextAnchor175"/>e Exoplanet dataset, we have the following:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/Formula_07_005.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption"><a id="_idTextAnchor176"/></p>
			<p>and </p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/Formula_07_006.jpg" alt=""/>
				</div>
			</div>
			<p>Recall tells you how many of the positive cases were found. In the exoplanet case, only 25% of exoplanets have been found.</p>
			<h3>F1 score</h3>
			<p>The F1 score is the harmonic mean between precision and recall. The harmonic mean is used because <a id="_idIndexMarker458"/>precision and recall are based on different denominators and the harmonic mean evens them out. When precision and recall are equally important, the F1 score is best. Note that the F1 score ranges from 0 to 1 with 1 being the highest.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor177"/>Alternative scoring methods</h2>
			<p>Precision, recall, and the F1 score are alternative scoring methods provided by scikit-learn. A list of standard scoring methods may be found in the official documentation at <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a>.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Accuracy is often not the best choice for classification datasets. Another popular scoring method is <strong class="source-inline">roc_auc_score</strong>, the area under the curve of the receiving operator characteristic. As with most classification scoring methods, the closer to 1, the better the results. See <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score</a> for more information.</p>
			<p>When choosing a scoring method, it's critical to understand the goal. The goal in the Exoplanet dataset is to find exoplanets. This is obvious. What is not obvious is how to select the best scoring method to achieve the desired results.</p>
			<p>Imagine two different scenarios:</p>
			<ul>
				<li><p>Scenario 1: Of the 4 exoplanet stars the machine learning model predicts, 3 are actually exoplanet stars: 3/4 = 75% precision.</p></li>
				<li><p>Scenario 2: Of the 12 exoplanet stars, the model correctly predicts 8 exoplanet stars (8/12 = 66% recall).</p></li>
			</ul>
			<p>Which is more desirable? </p>
			<p>The answer is that it depends. Recall is ideal for flagging potential positive cases (exoplanets) with the goal of finding them all. Precision is ideal for ensuring that the predictions (exoplanets) are indeed positive.</p>
			<p>Astronomers are unlikely to announce that an exoplanet has been discovered just because a machine learning model says so. They are more likely to carefully examine potential exoplanet stars before confirming or refuting the claim based on additional evidence.</p>
			<p>Assuming that the goal of the machine learning model is to find as many exoplanets as possible, recall is an excellent choice. Why? Recall tells us how many of the 12 exoplanet stars have been found (2/12, 5/12, 12/12). Let's try to find them all.</p>
			<p class="callout-heading">Precision note</p>
			<p class="callout">A higher percentage of precision does not indicate more exoplanet stars. For instance, a recall of 1/1 is 100%, but it only finds one exoplanet.</p>
			<h3>recall_score</h3>
			<p>As indicated in the previous <a id="_idIndexMarker459"/>section, we will proceed with recall as the scoring method for the Exoplanet dataset to find as many exoplanets as possible. Let's begin:</p>
			<ol>
				<li value="1"><p>Import <strong class="source-inline">recall_score</strong> from <strong class="source-inline">sklearn.metrics</strong>:</p><p class="source-code">from sklearn.metrics import recall_score</p><p>By default, <strong class="source-inline">recall_score</strong> reports the recall score of the positive class, typically labeled <strong class="source-inline">1</strong>. It is unusual for the positive class to be labeled <strong class="source-inline">2</strong> and for the negative class to be labeled <strong class="source-inline">1</strong> as is the case with the Exoplanet dataset.</p></li>
				<li><p>To obtain the <strong class="source-inline">recall_score</strong> value of exoplanet stars, input <strong class="source-inline">y_test</strong> and <strong class="source-inline">y_pred</strong> as parameters for <strong class="source-inline">recall_score</strong> along with <strong class="source-inline">pos_label=2</strong>:</p><p class="source-code">recall_score(y_test, y_pred, pos_label=2)</p><p>The score of exoplanet stars is as follows:</p><p class="source-code">0.25</p></li>
			</ol>
			<p>This is the same percentage given by the classification report under the recall score of <strong class="source-inline">2</strong>, which is the exoplanet stars. Going forward, instead of using <strong class="source-inline">accuracy_score</strong>, we will use <strong class="source-inline">recall_score</strong> with <a id="_idIndexMarker460"/>the preceding parameters as our scoring metric.</p>
			<p>Next, let's learn about resampling, an important strategy for improving the scores of imbalanced datasets.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor178"/>Resampling imbalanced data</h1>
			<p>Now that we have an <a id="_idIndexMarker461"/>appropriate scoring method to discover exoplanets, it's time to explore strategies such as resampling, undersampling, and oversampling for correcting the imbalanced data causing the low recall score.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor179"/>Resampling</h2>
			<p>One strategy to <a id="_idIndexMarker462"/>counteract imbalanced data is to resample the data. It's possible to undersample the data by reducing rows of the majority class and to oversample the data by repeating rows of the minority class.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor180"/>Undersampling</h2>
			<p>Our exploration <a id="_idIndexMarker463"/>began by selecting 400 rows from 5,087. This is an example of undersampling since the subset contains fewer rows than the original.</p>
			<p>Let's write a function that allows us to undersample the data by any number of rows. This function should return the recall score so that we can see how undersampling changes the results. We will begin with the scoring function.</p>
			<h3>The scoring function</h3>
			<p>The following function <a id="_idIndexMarker464"/>takes XGBClassifier and the number of rows as input and produces the confusion matrix, classification report, and recall score of exoplanet stars as output.</p>
			<p>Here are the steps:</p>
			<ol>
				<li value="1"><p>Define a function, <strong class="source-inline">xgb_clf</strong>, that takes <strong class="source-inline">model</strong>, the machine learning model, and <strong class="source-inline">nrows</strong>, the number of rows, as input:</p><p class="source-code">def xgb_clf(model, nrows):</p></li>
				<li><p>Load the DataFrame with <strong class="source-inline">nrows</strong>, then split the data into <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> and training and test sets:  </p><p class="source-code">    df = pd.read_csv('exoplanets.csv', nrows=nrows)</p><p class="source-code">    X = df.iloc[:,1:]</p><p class="source-code">    y = df.iloc[:,0]</p><p class="source-code">    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</p></li>
				<li><p>Initialize the model, fit the model to the training set, and score it with the test set using <strong class="source-inline">y_test</strong>, <strong class="source-inline">y_pred</strong>, and <strong class="source-inline">pos_label=2</strong> for <strong class="source-inline">recall_score</strong> as input:</p><p class="source-code">    model.fit(X_train, y_train)</p><p class="source-code">    y_pred = xg_clf.predict(X_test)</p><p class="source-code">    score = recall_score(y_test, y_pred, pos_label=2)</p></li>
				<li><p>Print the confusion matrix and classification report, and return the score:</p><p class="source-code">    print(confusion_matrix(y_test, y_pred))</p><p class="source-code">    print(classification_report(y_test, y_pred))</p><p class="source-code">    return score</p></li>
			</ol>
			<p>Now, we can undersample the number of rows and see how the scores change.</p>
			<h3>Undersampling nrows</h3>
			<p>Let's start <a id="_idIndexMarker465"/>by doubling <strong class="source-inline">nrows</strong> to <strong class="source-inline">800</strong>. This is still undersampling since the original dataset has <strong class="source-inline">5087</strong> rows:</p>
			<p class="source-code">xgb_clf(XGBClassifier(random_state=2), nrows=800)</p>
			<p>This is the expected output:</p>
			<p class="source-code">[[189   1]</p>
			<p class="source-code"> [  9   1]]</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           1       0.95      0.99      0.97       190</p>
			<p class="source-code">           2       0.50      0.10      0.17        10</p>
			<p class="source-code">    accuracy                           0.95       200</p>
			<p class="source-code">   macro avg       0.73      0.55      0.57       200</p>
			<p class="source-code">weighted avg       0.93      0.95      0.93       200</p>
			<p class="source-code">0.1</p>
			<p>Despite the near-perfect recall for non-exoplanet stars, the confusion matrix reveals that only 1 of 10 exoplanet stars have been recalled. </p>
			<p>Next, decrease <strong class="source-inline">nrows</strong> from <strong class="source-inline">400</strong> to <strong class="source-inline">200</strong>:</p>
			<p class="source-code">xgb_clf(XGBClassifier(random_state=2), nrows=200)</p>
			<p>This is the expected output:</p>
			<p class="source-code">[[37  0]</p>
			<p class="source-code"> [ 8  5]]</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           1       0.82      1.00      0.90        37</p>
			<p class="source-code">           2       1.00      0.38      0.56        13</p>
			<p class="source-code">    accuracy                           0.84        50</p>
			<p class="source-code">   macro avg       0.91      0.69      0.73        50</p>
			<p class="source-code">weighted avg       0.87      0.84      0.81        50</p>
			<p>This is a little better. By decreasing <strong class="source-inline">n_rows</strong> the recall has gone up.</p>
			<p>Let's see what <a id="_idIndexMarker466"/>happens if we balance the classes precisely. Since there are 37 exoplanet-stars, 37 non-exoplanet stars balance the data.</p>
			<p>Run the <strong class="source-inline">xgb_clf</strong> function with <strong class="source-inline">nrows=74</strong>:</p>
			<p class="source-code">xgb_clf(XGBClassifier(random_state=2), nrows=74)</p>
			<p>This is the expected output:</p>
			<p class="source-code">[[6 2]</p>
			<p class="source-code"> [5 6]]</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           1       0.55      0.75      0.63         8</p>
			<p class="source-code">           2       0.75      0.55      0.63        11</p>
			<p class="source-code">    accuracy                           0.63        19</p>
			<p class="source-code">   macro avg       0.65      0.65      0.63        19</p>
			<p class="source-code">weighted avg       0.66      0.63      0.63        19</p>
			<p class="source-code">0.5454545454545454</p>
			<p>These results are respectable, even though the subset is much smaller.</p>
			<p>Next, let's see what <a id="_idIndexMarker467"/>happens when we apply the strategy of oversampling.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor181"/>Oversampling</h2>
			<p>Another resampling technique is oversampling. Instead of eliminating rows, oversampling adds rows by <a id="_idIndexMarker468"/>copying and redistributing the positive cases.</p>
			<p>Although the original dataset has over 5,000 rows, we continue to use <strong class="source-inline">nrows=400</strong> as our starting point to expedite the process.</p>
			<p>When <strong class="source-inline">nrows=400</strong>, the ratio of positive to negative cases is 10 to 1. We need 10 times as many positive cases to obtain a balance.</p>
			<p>Our strategy is as follows:</p>
			<ul>
				<li><p>Create a new DataFrame that copies the positive cases nine times. </p></li>
				<li><p>Concatenate a new DataFrame with the original to obtain a 10-10 ratio.</p></li>
			</ul>
			<p>Before proceeding, a warning is in order. If the data is resampled before splitting it into training and test sets, the recall score will be inflated. Can you see why?</p>
			<p>When resampling, nine copies will be made of the positive cases. After splitting this data into training and test sets, copies are likely contained in both sets. So, the test set will contain most of the same data points as the training set.</p>
			<p>The appropriate strategy is to split the data into a training and test set first and then to resample the data. As done previously, we can use <strong class="source-inline">X_train</strong>, <strong class="source-inline">X_test</strong>, <strong class="source-inline">y_train</strong>, and <strong class="source-inline">y_test</strong>. Let's start:</p>
			<ol>
				<li value="1"><p>Merge <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong> on the left and right index with <strong class="source-inline">pd.merge</strong> as follows:</p><p class="source-code">df_train = pd.merge(y_train, X_train, left_index=True, right_index=True)</p></li>
				<li><p>Create a DataFrame, <strong class="source-inline">new_df</strong>, using <strong class="source-inline">np.repeat</strong> that includes the following:</p><p>a) The values of the positive cases: <strong class="source-inline">df_train[df_train['LABEL']==2.values</strong>.</p><p>b) The number of copies – in this case, <strong class="source-inline">9</strong></p><p>c) The <strong class="source-inline">axis=0</strong> parameter to specify that we are working with columns:</p><p class="source-code">new_df = pd.DataFrame(np.repeat(df_train[df_train['LABEL']==2].values,9,axis=0))</p></li>
				<li><p>Copy the column names:</p><p class="source-code">new_df.columns = df_train.columns</p></li>
				<li><p>Concatenate <a id="_idIndexMarker469"/>the DataFrames:</p><p class="source-code">df_train_resample = pd.concat([df_train, new_df])</p></li>
				<li><p>Verify that <strong class="source-inline">value_counts</strong> is as expected:</p><p class="source-code">df_train_resample['LABEL'].value_counts()</p><p>The expected output is as follows:</p><p class="source-code">1.0    275</p><p class="source-code">2.0    250</p><p class="source-code">Name: LABEL, dtype: int64</p></li>
				<li><p>Split <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> using the resampled DataFrame:</p><p class="source-code">X_train_resample = df_train_resample.iloc[:,1:]</p><p class="source-code">y_train_resample = df_train_resample.iloc[:,0]</p></li>
				<li><p>Fit the model on the resampled training set:</p><p class="source-code">model = XGBClassifier(random_state=2)</p><p class="source-code">model.fit(X_train_resample, y_train_resample)</p></li>
				<li><p>Score the model with <strong class="source-inline">X_test</strong> and <strong class="source-inline">y_test</strong>. Include the confusion matrix and classification report in your result:</p><p class="source-code">y_pred = model.predict(X_test)</p><p class="source-code">score = recall_score(y_test, y_pred, pos_label=2)</p><p class="source-code">print(confusion_matrix(y_test, y_pred))</p><p class="source-code">print(classification_report(y_test, y_pred))</p><p class="source-code">print(score)</p><p>The <a id="_idIndexMarker470"/>score is as follows:</p><p class="source-code">[[86  2]</p><p class="source-code"> [ 8  4]]</p><p class="source-code">              precision    recall  f1-score   support</p><p class="source-code">           1       0.91      0.98      0.95        88</p><p class="source-code">           2       0.67      0.33      0.44        12</p><p class="source-code">    accuracy                           0.90       100</p><p class="source-code">   macro avg       0.79      0.66      0.69       100</p><p class="source-code">weighted avg       0.89      0.90      0.88       100</p><p class="source-code">0.3333333333333333</p></li>
			</ol>
			<p>By appropriately holding out a test set to begin with, oversampling achieves 33.3% recall, a score that is twice as strong as the 17% obtained earlier, although still much too low.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout"><strong class="bold">SMOTE</strong> is a popular resampling library that may be imported from <strong class="source-inline">imblearn</strong>, which must be downloaded to use. I achieved the same results as SMOTE using the preceding resampling code. </p>
			<p>Since <a id="_idIndexMarker471"/>resampling has produced modest gains at best, it's time to adjust the hyperparameters of XGBoost.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor182"/>Tuning and scaling XGBClassifier</h1>
			<p>In this section, we will <a id="_idIndexMarker472"/>fine-tune and scale XGBClassifier to obtain the <a id="_idIndexMarker473"/>best possible <strong class="source-inline">recall_score</strong> value for the Exoplanets dataset. First, you will adjust weights using <strong class="source-inline">scale_pos_weight</strong>, then you will run grid searches to find the best combination of hyperparameters. In addition, you will score models for different subsets of the data before consolidating and analyzing the results.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor183"/>Adjusting weights</h2>
			<p>In <a href="B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117"><em class="italic">Chapter 5</em></a>, <em class="italic">XGBoost Unveiled</em>, you used the <strong class="source-inline">scale_pos_weight</strong> hyperparameter to counteract imbalances in the Higgs boson dataset. <strong class="source-inline">Scale_pos_weight</strong> is a <a id="_idIndexMarker474"/>hyperparameter used to scale the <em class="italic">positive</em> weight. The emphasis here on <em class="italic">positive</em> is important because XGBoost assumes that a target value of <strong class="source-inline">1</strong> is <em class="italic">positive</em> and a target value of <strong class="source-inline">0</strong> is <em class="italic">negative</em>.</p>
			<p>In the Exoplanet dataset, we have been using the default <strong class="source-inline">1</strong> as negative and <strong class="source-inline">2</strong> as positive as provided by the dataset. We will now switch to <strong class="source-inline">0</strong> as negative and <strong class="source-inline">1</strong> as positive using the <strong class="source-inline">.replace()</strong> method.</p>
			<h3>replace</h3>
			<p>The <strong class="source-inline">.replace()</strong> method may be used <a id="_idIndexMarker475"/>to reassign values. The following code replaces <strong class="source-inline">1</strong> with <strong class="source-inline">0</strong> and <strong class="source-inline">2</strong> with <strong class="source-inline">1</strong> in the <strong class="source-inline">LABEL</strong> column:</p>
			<p class="source-code">df['LABEL'] = df['LABEL'].replace(1, 0)</p>
			<p class="source-code">df['LABEL'] = df['LABEL'].replace(2, 1)</p>
			<p>If the two lines of code were reversed, all column values would end up as 0 since all 2s would become 1s, and then all 1s would become 0s. In programming, order matters!</p>
			<p>Verify the counts using the <strong class="source-inline">value_counts</strong> method:</p>
			<p class="source-code">df['LABEL'].value_counts()</p>
			<p>Here is the expected output:</p>
			<p class="source-code">0    363</p>
			<p class="source-code">1     37</p>
			<p class="source-code">Name: LABEL, dtype: int64</p>
			<p>The positive cases are now labeled <strong class="source-inline">1</strong> and the negative cases are labeled <strong class="source-inline">0</strong>. </p>
			<h3>scale_pos_weight</h3>
			<p>It's time <a id="_idIndexMarker476"/>to build a new <strong class="source-inline">XGBClassifier</strong> with <strong class="source-inline">scale_pos_weight=10</strong> to account for the imbalance in the data: </p>
			<ol>
				<li value="1"><p>Split the new DataFrame into <strong class="source-inline">X</strong>, the predictor columns, and <strong class="source-inline">y</strong>, the target columns:</p><p class="source-code">X = df.iloc[:,1:]</p><p class="source-code">y = df.iloc[:,0]</p></li>
				<li><p>Split the data into training and test sets:</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</p></li>
				<li><p>Build, fit, predict, and score <strong class="source-inline">XGBClassifier</strong> with <strong class="source-inline">scale_pos_weight=10</strong>. Print out the confusion matrix and the classification report to view the complete results:</p><p class="source-code">model = XGBClassifier(scale_pos_weight=10, random_state=2)</p><p class="source-code">model.fit(X_train, y_train)</p><p class="source-code">y_pred = model.predict(X_test)</p><p class="source-code">score = recall_score(y_test, y_pred)</p><p class="source-code">print(confusion_matrix(y_test, y_pred))</p><p class="source-code">print(classification_report(y_test, y_pred))</p><p class="source-code">print(score)</p><p>Here is the expected output:</p><p class="source-code">[[86  2]</p><p class="source-code"> [ 8  4]]</p><p class="source-code">              precision    recall  f1-score   support</p><p class="source-code">           0       0.91      0.98      0.95        88</p><p class="source-code">           1       0.67      0.33      0.44        12</p><p class="source-code">    accuracy                           0.90       100</p><p class="source-code">   macro avg       0.79      0.66      0.69       100</p><p class="source-code">weighted avg       0.89      0.90      0.88       100</p><p class="source-code">0.3333333333333333</p></li>
			</ol>
			<p>The results are the same as our resampling method from the previous section.</p>
			<p>The <a id="_idIndexMarker477"/>oversampling method that we implemented from scratch gives the same predictions as <strong class="source-inline">XGBClassifier</strong> with <strong class="source-inline">scale_pos_weight</strong>.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor184"/>Tuning XGBClassifier</h2>
			<p>It's time to see <a id="_idIndexMarker478"/>whether hyperparameter fine-tuning can increase precision.</p>
			<p>It's standard to use <strong class="source-inline">GridSearchCV</strong> and <strong class="source-inline">RandomizedSearchCV</strong> when fine-tuning hyperparameters. Both require cross-validation of two or more folds. We have yet to implement cross-validation since our initial models did not perform well and it's computationally expensive to test multiple folds on large datasets.</p>
			<p>A balanced approach is to use <strong class="source-inline">GridSearchCV</strong> and <strong class="source-inline">RandomizedSearchCV</strong> with two folds to save time. To ensure consistent results, <strong class="source-inline">StratifiedKFold</strong> (<a href="B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136"><em class="italic">Chapter 6</em></a>,<em class="italic"> XGBoost Hyperparameters</em>) is <a id="_idIndexMarker479"/>recommended. We will begin with the baseline model.</p>
			<h3>The baseline model</h3>
			<p>Here are the steps to build a <a id="_idIndexMarker480"/>baseline model that implements the same k-fold cross-validation as grid searches: </p>
			<ol>
				<li value="1"><p>Import <strong class="source-inline">GridSearchCV</strong>, <strong class="source-inline">RandomizedSearchCV</strong>, <strong class="source-inline">StratifiedKFold</strong>, and <strong class="source-inline">cross_val_score</strong>: </p><p class="source-code">from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score</p></li>
				<li><p>Intialize <strong class="source-inline">StratifiedKFold</strong> as <strong class="source-inline">kfold</strong> with <strong class="source-inline">n_splits=2</strong> and <strong class="source-inline">shuffle=True</strong>:</p><p class="source-code">kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=2)</p></li>
				<li><p>Initialize <strong class="source-inline">XGBClassifier</strong> with <strong class="source-inline">scale_pos_weight=10</strong> since there are 10 times as many negative cases as positive cases:</p><p class="source-code">model = XGBClassifier(scale_pos_weight=10, random_state=2)</p></li>
				<li><p>Score the model using <strong class="source-inline">cross_val_score</strong> with <strong class="source-inline">cv=kfold</strong> and <strong class="source-inline">score='recall'</strong> as parameters, then display the scores:</p><p class="source-code">scores = cross_val_score(model, X, y, cv=kfold, scoring='recall')</p><p class="source-code">print('Recall: ', scores)</p><p class="source-code">print('Recall mean: ', scores.mean())</p><p>The scores are as follows:</p><p class="source-code">Recall:  [0.10526316 0.27777778]</p><p class="source-code">Recall mean:  0.1915204678362573</p></li>
			</ol>
			<p>The scores are a little worse with cross-validation. When there are very few positive cases, it makes a <a id="_idIndexMarker481"/>difference which rows end up in the training and test sets. Different implementations of <strong class="source-inline">StratifiedKFold</strong> and <strong class="source-inline">train_test_split</strong> may lead to different results.</p>
			<h3>grid_search</h3>
			<p>We'll now <a id="_idIndexMarker482"/>implement a variation of the <strong class="source-inline">grid_search</strong> function from <a href="B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136"><em class="italic">Chapter 6</em></a>, <em class="italic">XGBoost Hyperparameters</em>, to fine-tune hyperparameters: </p>
			<ol>
				<li value="1"><p>The new function takes the same dictionary of parameters as input, along with a random option that uses <strong class="source-inline">RandomizedSearchCV</strong>. In addition, <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> are provided as default parameters for use with other subsets and the scoring method is recall as follows:</p><p class="source-code">def grid_search(params, random=False, X=X, y=y, model=XGBClassifier(random_state=2)): </p><p class="source-code">    xgb = model</p><p class="source-code">    if random:</p><p class="source-code">        grid = RandomizedSearchCV(xgb, params, cv=kfold, n_jobs=-1, random_state=2, scoring='recall')</p><p class="source-code">    else:</p><p class="source-code">        grid = GridSearchCV(xgb, params, cv=kfold, n_jobs=-1, scoring='recall')</p><p class="source-code">    grid.fit(X, y)</p><p class="source-code">    best_params = grid.best_params_</p><p class="source-code">    print("Best params:", best_params)</p><p class="source-code">    best_score = grid.best_score_</p><p class="source-code">    print("Best score: {:.5f}".format(best_score))</p></li>
				<li><p>Let's run the grid searches excluding defaults to try and improve scores. Here are some initial grid searches along with their results:</p><p>a) Grid search 1:</p><p class="source-code">grid_search(params={'n_estimators':[50, 200, 400, 800]})</p><p>Results:</p><p class="source-code">Best params: {'n_estimators': 50}Best score: 0.19152</p><p>b) Grid <a id="_idIndexMarker483"/>search 2:</p><p class="source-code">grid_search(params={'learning_rate':[0.01, 0.05, 0.2, 0.3]})</p><p>Results:</p><p class="source-code">Best params: {'learning_rate': 0.01}</p><p class="source-code">Best score: 0.40351</p><p>c) Grid search 3:</p><p class="source-code">grid_search(params={'max_depth':[1, 2, 4, 8]})</p><p>Results:</p><p class="source-code">Best params: {'max_depth': 2}</p><p class="source-code">Best score: 0.24415</p><p>d) Grid search 4:</p><p class="source-code">grid_search(params={'subsample':[0.3, 0.5, 0.7, 0.9]})</p><p>Results:</p><p class="source-code">Best params: {'subsample': 0.5}</p><p class="source-code">Best score: 0.21637</p><p>e) Grid search 5:</p><p class="source-code">grid_search(params={'gamma':[0.05, 0.1, 0.5, 1]})</p><p>Results:</p><p class="source-code">Best params: {'gamma': 0.05}</p><p class="source-code">Best score: 0.24415</p></li>
				<li><p>Changing <strong class="source-inline">learning_rate</strong> , <strong class="source-inline">max_depth</strong>, and <strong class="source-inline">gamma</strong> has resulted in gains. Let's try to combine them by narrowing the range:</p><p class="source-code">grid_search(params={'learning_rate':[0.001, 0.01, 0.03], 'max_depth':[1, 2], 'gamma':[0.025, 0.05, 0.5]})</p><p>The score is as follows:</p><p class="source-code">Best params: {'gamma': 0.025, 'learning_rate': 0.001, 'max_depth': 2}</p><p class="source-code">Best score: 0.53509</p></li>
				<li><p>It's also worth <a id="_idIndexMarker484"/>trying <strong class="source-inline">max_delta_step</strong>, which XGBoost only recommends for imbalanced datasets. The default is 0 and increasing the steps results in a more conservative model:</p><p class="source-code">grid_search(params={'max_delta_step':[1, 3, 5, 7]})</p><p>The score is as follows:</p><p class="source-code">Best params: {'max_delta_step': 1}</p><p class="source-code">Best score: 0.24415</p></li>
				<li><p>As a final strategy, we combine <strong class="source-inline">subsample</strong> with all the column samples in a random search:</p><p class="source-code">grid_search(params={'subsample':[0.3, 0.5, 0.7, 0.9, 1], </p><p class="source-code">'colsample_bylevel':[0.3, 0.5, 0.7, 0.9, 1], </p><p class="source-code">'colsample_bynode':[0.3, 0.5, 0.7, 0.9, 1], </p><p class="source-code">'colsample_bytree':[0.3, 0.5, 0.7, 0.9, 1]}, random=True)</p><p>The score is as follows:</p><p class="source-code">Best params: {'subsample': 0.3, 'colsample_bytree': 0.7, 'colsample_bynode': 0.7, 'colsample_bylevel': 1}</p><p class="source-code">Best score: 0.35380</p></li>
			</ol>
			<p>Instead of <a id="_idIndexMarker485"/>continuing with this subset of data that contains <strong class="source-inline">400</strong> rows, let's switch to the balanced subset (undersampled) that contains <strong class="source-inline">74</strong> rows to compare results.</p>
			<h3>The balanced subset</h3>
			<p>The balanced subset of <strong class="source-inline">74</strong> rows has the least amount of data points. It's also the fastest to test.</p>
			<p><strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> need to be <a id="_idIndexMarker486"/>explicitly defined since they were last used for the balanced subset inside a function. The new definitions for <strong class="source-inline">X_short</strong> and <strong class="source-inline">y_short</strong> are given as follows:</p>
			<p class="source-code">X_short = X.iloc[:74, :]</p>
			<p class="source-code">y_short = y.iloc[:74]</p>
			<p>After a few grid searches, combining <strong class="source-inline">max_depth</strong> and <strong class="source-inline">colsample_bynode</strong> gave the following results:</p>
			<p class="source-code">grid_search(params={'max_depth':[1, 2, 3], 'colsample_bynode':[0.5, 0.75, 1]}, X=X_short, y=y_short, model=XGBClassifier(random_state=2)) </p>
			<p>The score is as follows:</p>
			<p class="source-code">Best params: {'colsample_bynode': 0.5, 'max_depth': 2}</p>
			<p class="source-code">Best score: 0.65058</p>
			<p>This is an improvement.</p>
			<p>It's time to try hyperparameter fine-tuning on all the data.</p>
			<h3>Fine-tuning all the data</h3>
			<p>The issue with <a id="_idIndexMarker487"/>implementing the <strong class="source-inline">grid_search</strong> function on all the data is time. Now that we are at the end, it's time to run the code and take breaks as the computer sweats:</p>
			<ol>
				<li value="1"><p>Read all the data into a new DataFrame, <strong class="source-inline">df_all</strong>:</p><p class="source-code">df_all = pd.read_csv('exoplanets.csv')</p></li>
				<li><p>Replace the 1s with 0s and the 2s with 1s:</p><p class="source-code">df_all['LABEL'] = df_all['LABEL'].replace(1, 0)df_all['LABEL'] = df_all['LABEL'].replace(2, 1)</p></li>
				<li><p>Split the data into <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong>:</p><p class="source-code">X_all = df_all.iloc[:,1:]y_all = df_all.iloc[:,0]</p></li>
				<li><p>Verify <strong class="source-inline">value_counts</strong> of the <strong class="source-inline">'LABEL'</strong> column:</p><p class="source-code">df_all['LABEL'].value_counts()</p><p>The output is as follows:</p><p class="source-code">0    5050 1      37 Name: LABEL, dtype: int64</p></li>
				<li><p>Scale the weights by dividing the negative class by the positive class:</p><p class="source-code">weight = int(5050/37)</p></li>
				<li><p>Score a baseline model for all the data with <strong class="source-inline">XGBClassifier</strong> and <strong class="source-inline">scale_pos_weight=weight</strong>:</p><p class="source-code">model = XGBClassifier(scale_pos_weight=weight, random_state=2)</p><p class="source-code">scores = cross_val_score(model, X_all, y_all, cv=kfold, scoring='recall')</p><p class="source-code">print('Recall:', scores)</p><p class="source-code">print('Recall mean:', scores.mean())</p><p>The output is as follows:</p><p class="source-code">Recall: [0.10526316 0.        ]</p><p class="source-code">Recall mean: 0.05263157894736842</p><p>This score is awful. Presumably, the classifier is scoring a high percentage of accuracy, despite the low recall. </p></li>
				<li><p>Let's try optimizing hyperparameters based on the most successful results thus far: </p><p class="source-code">grid_search(params={'learning_rate':[0.001, 0.01]}, X=X_all, y=y_all, model=XGBClassifier(scale_pos_weight=weight, random_state=2)) </p><p>The <a id="_idIndexMarker488"/>score is as follows:</p><p class="source-code">Best params: {'learning_rate': 0.001}</p><p class="source-code">Best score: 0.26316</p><p>This is much better than the initial score with all the data.</p><p>Let's try combining hyperparameters:</p><p class="source-code">grid_search(params={'max_depth':[1, 2],'learning_rate':[0.001]}, X=X_all, y=y_all, model=XGBClassifier(scale_pos_weight=weight, random_state=2)) </p><p>The score is as follows:</p><p class="source-code">Best params: {'learning_rate': 0.001, 'max_depth': 2}</p><p class="source-code">Best score: 0.53509</p></li>
			</ol>
			<p>This is better, though not as strong as the undersampled dataset scored earlier. </p>
			<p>With the score on all the data starting lower and taking more time, a question naturally arises. Are the <a id="_idIndexMarker489"/>machine learning models better on the smaller subsets for the Exoplanet dataset?</p>
			<p>Let's find out.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor185"/>Consolidating results</h2>
			<p>It's tricky to <a id="_idIndexMarker490"/>consolidate results with different datasets. We have been working with the following subsets:</p>
			<ul>
				<li><p>5,050 rows – approx. 54% recall</p></li>
				<li><p>400 rows – approx. 54% recall</p></li>
				<li><p>74 rows – approx. 68% recall</p></li>
			</ul>
			<p>The best results obtained have included <strong class="source-inline">learning_rate=0.001</strong>,  <strong class="source-inline">max_depth=2</strong>, and <strong class="source-inline">colsample_bynode=0.5</strong>.</p>
			<p>Let's train a model on <em class="italic">all 37 exoplanet stars</em>. This means the test results will come from data points that the model has already trained on. Normally, this is not a good idea. In this case, however, the positive cases are very few and it may be instructive to see how the smaller subsets test on the positive cases it has not seen before.</p>
			<p>The following function takes <strong class="source-inline">X</strong>, <strong class="source-inline">y</strong>, and the machine learning model as input. The model is fit on the data provided, then predictions are made on the entire dataset. Finally, <strong class="source-inline">recall_score</strong>, <strong class="source-inline">confusion matrix</strong>, and <strong class="source-inline">classification report</strong> are all printed:</p>
			<p class="source-code">def final_model(X, y, model):</p>
			<p class="source-code">    model.fit(X, y)</p>
			<p class="source-code">    y_pred = model.predict(X_all)</p>
			<p class="source-code">    score = recall_score(y_all, y_pred,)</p>
			<p class="source-code">    print(score)</p>
			<p class="source-code">    print(confusion_matrix(y_all, y_pred,))</p>
			<p class="source-code">    print(classification_report(y_all, y_pred))</p>
			<p>Let's run the function for each of our three subsets. Of the three strongest hyperparameters, it turns out that <strong class="source-inline">colsample_bynode</strong> and <strong class="source-inline">max_depth</strong> give the best results.</p>
			<p>Let's start with the <a id="_idIndexMarker491"/>smallest number of rows, where the number of exoplanet stars and non-exoplanet stars match.</p>
			<h3>74 rows</h3>
			<p>Let's <a id="_idIndexMarker492"/>begin with 74 rows:</p>
			<p class="source-code">final_model(X_short, y_short, XGBClassifier(max_depth=2, colsample_by_node=0.5, random_state=2))</p>
			<p>The output is as follows:</p>
			<p class="source-code">1.0</p>
			<p class="source-code">[[3588 1462]</p>
			<p class="source-code"> [   0   37]]</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       1.00      0.71      0.83      5050</p>
			<p class="source-code">           1       0.02      1.00      0.05        37</p>
			<p class="source-code">    accuracy                           0.71      5087</p>
			<p class="source-code">   macro avg       0.51      0.86      0.44      5087</p>
			<p class="source-code">weighted avg       0.99      0.71      0.83      5087</p>
			<p>All 37 exoplanet stars were correctly identified, but 1,462 non-exoplanet stars were misclassified! Despite 100% recall, the precision is 2%, and the F1 score is 5%. Low precision and a low F1 score are a risk when tuning for recall only. In practice, an astronomer would have to sort through 1,462 potential exoplanet stars to find 37. This is unacceptable.</p>
			<p>Now let's see what happens when we train on 400 rows.</p>
			<h3>400 rows</h3>
			<p>In the case of 400 <a id="_idIndexMarker493"/>rows, we use the <strong class="source-inline">scale_pos_weight=10</strong> hyperparameter to balance the data:</p>
			<p class="source-code">final_model(X, y, XGBClassifier(max_depth=2, colsample_bynode=0.5, scale_pos_weight=10, random_state=2))</p>
			<p>The output is as follows:</p>
			<p class="source-code">1.0</p>
			<p class="source-code">[[4901  149]</p>
			<p class="source-code"> [   0   37]]</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       1.00      0.97      0.99      5050</p>
			<p class="source-code">           1       0.20      1.00      0.33        37</p>
			<p class="source-code">    accuracy                           0.97      5087</p>
			<p class="source-code">   macro avg       0.60      0.99      0.66      5087</p>
			<p class="source-code">weighted avg       0.99      0.97      0.98      5087</p>
			<p>Again, all 37 exoplanet stars were correctly classified for 100% recall, but 149 non-exoplanet stars were incorrectly classified, for a precision of 20%. In this case, an astronomer would need to sort through 186 stars to find the 37 exoplanet stars. </p>
			<p>Finally, let's train on all the data.</p>
			<h3>5,050 rows</h3>
			<p>In the case of all <a id="_idIndexMarker494"/>the data, set <strong class="source-inline">scale_pos_weight</strong> equal to the <strong class="source-inline">weight</strong> variable, as previously defined:</p>
			<p class="source-code">final_model(X_all, y_all, XGBClassifier(max_depth=2, colsample_bynode=0.5, scale_pos_weight=weight, random_state=2))</p>
			<p>The output is as follows:</p>
			<p class="source-code">1.0</p>
			<p class="source-code">[[5050    0]</p>
			<p class="source-code"> [   0   37]]</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       1.00      1.00      1.00      5050</p>
			<p class="source-code">           1       1.00      1.00      1.00        37</p>
			<p class="source-code">    accuracy                           1.00      5087</p>
			<p class="source-code">   macro avg       1.00      1.00      1.00      5087</p>
			<p class="source-code">weighted avg       1.00      1.00      1.00      5087</p>
			<p>Amazing. All predictions, recall and precision, are 100% perfect. In this highly desirable case, an astronomer would find all of the exoplanet stars without having to sift through any bad data.</p>
			<p>Keep in mind, however, that these scores are based on the training data, not on unseen test data, which is <a id="_idIndexMarker495"/>mandatory to build a strong model. In other words, although the model fits the training data perfectly, it's unlikely to generalize this well to new data. These numbers, however, are valuable.</p>
			<p>Based on this result, since the machine learning model performs impressively on the training set and modestly at best on the test set, the variance is likely too high. Additionally, more trees and more rounds of fine-tuning may be required to pick up on nuanced patterns within the data.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor186"/>Analyzing results</h2>
			<p>When scored on the <a id="_idIndexMarker496"/>training set, the tuned models delivered perfect recall but varied considerably on the precision. Here are the takeaways: </p>
			<ul>
				<li><p>Using precision without recall or the F1 score can result in suboptimal models. By using the classification report, more details are revealed.</p></li>
				<li><p>Over-emphasizing high scores from small subsets is not advised.</p></li>
				<li><p>When test scores are low, but training scores are high on imbalanced datasets, deeper models with extensive hyperparameter fine-tuning is advised.</p></li>
			</ul>
			<p>A survey of kernels, publicly displayed notebooks put forward by Kaggle users, at <a href="https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data/kernels">https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data/kernels</a> for the Exoplanet dataset reveals the following:</p>
			<ul>
				<li><p>Many users fail to understand that a high accuracy score is easy to obtain and virtually meaningless with highly imbalanced data.</p></li>
				<li><p>Users posting <a id="_idIndexMarker497"/>precision are generally posting from 50 to 70 percent, and users posting recall are posting 60 to 100 percent (a user with 100% recall has 55% precision), indicating the challenges and limitations of this dataset. </p></li>
			</ul>
			<p>When you present your results to your astronomy professor, wiser to the limitations of imbalanced data, you conclude that your model performs with 70% recall at best, and that 37 exoplanet stars are not enough to build a robust machine learning model to find life on other planets. Your XGBClassifier, however, will allow astronomers and others trained in data analysis to use machine learning to decide which stars to focus on in the universe to discover the next exoplanets in orbit.</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor187"/>Summary</h1>
			<p>In this chapter, you surveyed the universe with the Exoplanet dataset to discover new planets, and potentially new life. You built multiple XGBClassifiers to predict when exoplanet stars are the result of periodic changes in light. With only 37 exoplanet stars and 5,050 non-exoplanet stars, you corrected the imbalanced data by undersampling, oversampling, and tuning XGBoost hyperparameters including <strong class="source-inline">scale_pos_weight</strong>.</p>
			<p>You analyzed results using the confusion matrix and the classification report. You learned key differences between various classification scoring metrics, and why for the Exoplanet dataset accuracy is virtually worthless, while a high recall is ideal, especially when combined with high precision for a good F1 score. Finally, you realized the limitations of machine learning models when the data is extremely varied and imbalanced.</p>
			<p>After this case study, you have the necessary background and skills to fully analyze imbalanced datasets with XGBoost using <strong class="source-inline">scale_pos_weight</strong>, hyperparameter fine-tuning, and alternative classification scoring metrics.</p>
			<p>In the next chapter, you will greatly expand your range of XGBoost by applying alternative XGBoost base learners beyond gradient boosted trees. Although gradient boosted trees are often the best option, XGBoost comes equipped with linear base learners, dart base learners, and even random forests, all coming next!</p>
		</div>
	</body></html>
<html><head></head><body>
		<div class="Content" id="_idContainer044">
			<h1 id="_idParaDest-39"><em class="italics"><a id="_idTextAnchor039"/>Chapter 2</em></h1>
		</div>
		<div class="Content" id="_idContainer045">
			<h1 id="_idParaDest-40"><a id="_idTextAnchor040"/>Hierarchical Clustering</h1>
		</div>
		<div class="Content" id="_idContainer046">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Implement the hierarchical clustering algorithm from scratch by using packages</li>
				<li class="bullets">Perform agglomerative clustering</li>
				<li class="bullets">Compare k-means with hierarchical clustering </li>
			</ul>
			<p>In this chapter, we will use hierarchical clustering to build stronger groupings which make more logical sense.</p>
		</div>
		<div class="Content" id="_idContainer066">
			<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>Introduction</h2>
			<p>In this chapter, we will expand on the basic ideas that we built in <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Clustering</em>, by surrounding clustering with the concept of similarity. Once again, we will be implementing forms of the Euclidean distance to capture the notion of similarity. It is important to bear in mind that the Euclidean distance just happens to be one of the most popular distance metrics and not the only one! Through these distance metrics, we will expand on the simple neighbor calculations that we explored in the previous chapter by introducing the concept of hierarchy. By using hierarchy to convey clustering information, we can build stronger groupings that make more logical sense. </p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>Clustering Refresher</h2>
			<p><em class="italics">Chapter 1</em>, <em class="italics">Introduction to Clustering</em>, covered both the high-level intuition and in-depth details of one of the most basic clustering algorithms: k-means. While it is indeed a simple approach, do not discredit it; it will be a valuable addition to your toolkit as you continue your exploration of the unsupervised learning world. In many real-world use cases, companies experience groundbreaking discoveries through the simplest methods, such as k-means or linear regression (for supervised learning). As a refresher, let's quickly walk through what clusters are and how k-means works to find them:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer047">
					<img alt="Figure 2.1: The attributes that separate supervised and unsupervised problems&#13;&#10;" src="image/C12626_02_01.jpg"/>
				</div>
			</div>
			<h6>Figure 2.1: The attributes that separate supervised and unsupervised problems</h6>
			<p>If you were given a random collection of data without any guidance, you would likely start your exploration using basic statistics – for example, what the mean, median, and mode values are of each of the features. Remember that, from a high-level data model that simply exists, knowing whether it is supervised or unsupervised learning is ascribed by the data goals that you have set for yourself or that were set by your manager. If you were to determine that one of the features was actually a label and you wanted to see how the remaining features in the dataset influence it, this would become a supervised learning problem. However, if after initial exploration you realize that the data you have is simply a collection of features without a target in mind (such as a collection of health metrics, purchase invoices from a web store, and so on), then you could analyze it through unsupervised methods.</p>
			<p>A classic example of unsupervised learning is finding clusters of similar customers in a collection of invoices from a web store. Your hypothesis is that by understanding which people are most similar, you can create more granular marketing campaigns that appeal to each cluster's interests. One way to achieve these clusters of similar users is through k-means.</p>
			<h3 id="_idParaDest-43"><a id="_idTextAnchor043"/>k-means Refresher</h3>
			<p>k-means clustering works by finding "k" number clusters in your data through pairwise Euclidean distance calculations. "K" points (also called centroids) are randomly initialized in your data and the distance is calculated from each data point to each of the centroids. The minimum of these distances designates which cluster a data point belongs to. Once every point has been assigned to a cluster, the mean intra-cluster data point is calculated as the new centroid. This process is repeated until the newly-calculated cluster centroid no longer changes position.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>The Organization of Hierarchy</h2>
			<p>Both the natural and human-made world contain many examples of organizing systems into hierarchies and why, for the most part, it makes a lot of sense. A common representation that is developed from these hierarchies can be seen in tree-based data structures. Imagine that you had a parent node with any number of child nodes that could subsequently be parent nodes themselves. By organizing concepts into a tree structure, you can build an information-dense diagram that clearly shows how things are related to their peers and their larger abstract concepts. </p>
			<p>An example from the natural world to help illustrate this concept can be seen in how we view the hierarchy of animals, which goes from parent classes to individual species:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer048">
					<img alt="Figure 2.2: Navigating the relationships of animal species in a hierarchical tree structure&#13;&#10;" src="image/C12626_02_02.jpg"/>
				</div>
			</div>
			<h6>Figure 2.2: Navigating the relationships of animal species in a hierarchical tree structure</h6>
			<p>In Figure 2.2, you can see an example of how relational information between varieties of animals can be easily mapped out in a way that both saves space and still transmits a large amount of information. This example can be seen as both a tree of its own (showing how cats and dogs are different but both domesticated animals), and as a potential piece of a larger tree that shows a breakdown of domesticated versus non-domesticated animals.</p>
			<p>In the event that most of you are not biologists, let's move back toward the concept of a web store selling products. If you sold a large variety of products, then you would likely want to create a hierarchical system of navigation for your customers. By withholding all of the information in your product catalog, customers will only be exposed to the path down the tree that matches their interests. An example of the hierarchical benefits of navigation can be seen in Figure 2.3:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer049">
					<img alt="Figure 2.3: Navigating product categories in a hierarchical tree structure&#13;&#10;" src="image/C12626_02_03.jpg"/>
				</div>
			</div>
			<h6>Figure 2.3: Navigating product categories in a hierarchical tree structure</h6>
			<p>Clearly, the benefits of a hierarchical system of navigation cannot be overstated in terms of improving your customer experience. By organizing information into a hierarchical structure, you can build an intuitive structure out of your data that demonstrates explicit nested relationships. If this sounds like another approach to finding clusters in your data, then you're definitely on the right track! Through the use of similar distance metrics such as the Euclidean distance from k-means, we can develop a tree that shows the many cuts of data that allow a user to subjectively create clusters at their discretion.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>Introduction to Hierarchical Clustering</h2>
			<p>Until this point, we have shown that hierarchies can be excellent structures in which to organize information that clearly show nested relationships among data points. While this is helpful in gaining an understanding of the parent/child relationships between items, it can also be very handy when forming clusters. Expanding on the animal example of the prior section, imagine that you were simply presented with two features of animals: their height (measured from the tip of the nose to the end of the tail) and their weight. Using this information, you then have to recreate the same structure in order to identify which records in your dataset correspond to dogs or cats, as well as their relative subspecies.</p>
			<p>Since you are only given animal heights and weights, you won't be able to extrapolate the specific names of each species. However, by analyzing the features that you have been provided, you can develop a structure within the data that serves as an approximation of what animal species exist in your data. This perfectly sets the stage for an unsupervised learning problem that is well solved with hierarchical clustering. In the following plot, you will see the two features that we created on the left: with animal height in the left-hand column and animal weight in the right-hand column. This is then charted on a two-axis plot with the height on the x axis and the weight on the y axis:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer050">
					<img alt="Figure 2.4: An example of a two-feature dataset comprising animal height and animal weight&#13;&#10;" src="image/C12626_02_04.jpg"/>
				</div>
			</div>
			<h6>Figure 2.4: An example of a two-feature dataset comprising animal height and animal weight</h6>
			<p>One way to approach hierarchical clustering is by starting with each data point serving as its own cluster and recursively joining the similar points together to form clusters – this is known as <strong class="keyword">agglomerative</strong> hierarchical clustering. We will go into more detail about the different ways of approaching hierarchical clustering in a later section. </p>
			<p>In the agglomerative hierarchical clustering approach, the concept of data point similarity can be thought of in the paradigm that we saw during k-means. In k-means, we used the Euclidean distance to calculate the distance from the individual points to the centroids of the expected "k" clusters. For this approach to hierarchical clustering, we will reuse the same distance metric to determine the similarity between the records in our dataset.</p>
			<p>Eventually, by grouping individual records from the data with their most similar records recursively, you end up building a hierarchy from the bottom up. The individual single-member clusters join together into one single cluster at the top of our hierarchy.</p>
			<h3 id="_idParaDest-46"><a id="_idTextAnchor046"/>Steps to Perform Hierarchical Clustering</h3>
			<p>To understand how agglomerative hierarchical clustering works, we can trace the path of a simple toy program as it merges together to form a hierarchy:</p>
			<ol>
				<li>Given n sample data points, view each point as an individual "cluster" with just that one point as a member.</li>
				<li>Calculate the pairwise Euclidean distance between the centroids of all the clusters in your data.</li>
				<li>Group the closest point pairs together.</li>
				<li>Repeat <em class="italics">Step 2</em> and <em class="italics">Step 3</em> until you reach a single cluster containing all the data in your set.</li>
				<li>Plot a dendrogram to show how your data has come together in a hierarchical structure. A dendrogram is simply a diagram that is used to represent a tree structure, showing an arrangement of clusters from top to bottom. </li>
				<li>Decide what level you want to create the clusters at.</li>
			</ol>
			<h3 id="_idParaDest-47"><a id="_idTextAnchor047"/>An Example Walk-Through of Hierarchical Clustering</h3>
			<p>While slightly more complex than k-means, hierarchical clustering does not change too much from a logistical perspective. Here is a simple example that walks through the preceding steps in slightly more detail:</p>
			<ol>
				<li value="1">Given a list of four sample data points, view each point as a centroid that is also its own cluster with the point indices from 0 to 3:<p>Clusters (4): [ (1,7) ], [ (-5,9) ], [ (-9,4) ] , [ (4, -2) ]</p><p>Centroids (4): [ (1,7) ], [ (-5,9) ], [ (-9,4) ] , [ (4, -2) ]</p></li>
				<li>Calculate the pairwise Euclidean distance between the centroids of all the clusters. In the matrix displayed in the following diagram, the point indices are between 0 and 3 both horizontally and vertically, showing the distance between the respective points. Along the diagonal are extremely high values to ensure that we do not select a point as its own neighbor (since it technically is the "closest" point). Notice that the values are mirrored across the diagonal:<div class="IMG---Figure" id="_idContainer051"><img alt="Figure 2.5: An array of distances&#13;&#10;" src="image/C12626_02_05.jpg"/></div><h6>Figure 2.5: An array of distances</h6></li>
				<li>Group the closest point pairs together.<p>In this case, points [1,7] and [-5,9] join into a cluster since they are closest, with the remaining two points left as single-member clusters:</p><div class="IMG---Figure" id="_idContainer052"><img alt="Figure 2.6: An array of distances&#13;&#10;" src="image/C12626_02_06.jpg"/></div><h6>Figure 2.6: An array of distances</h6><p>Here are the resulting three clusters: </p><p class="snippet">[ [1,7], [-5,9] ]</p><p class="snippet">[-9,4]</p><p class="snippet">[4,-2] </p></li>
				<li>Calculate the centroid of the two-member cluster, as follows: <p class="snippet">mean([ [1,7], [-5,9] ]) = [-2,8]</p></li>
				<li>Add the centroid to the two single-member centroids and recalculate the distances.<p>Clusters (3): </p><p class="snippet">[ [1,7], [-5,9] ]</p><p class="snippet">[-9,4]</p><p class="snippet">[4,-2] </p><p>Centroids (3): </p><p class="snippet">[-2,8]</p><p class="snippet">[-9,4]</p><p class="snippet">[4,-2]</p><p>The output will be similar to the following diagram, with the shortest distance called using a red arrow:</p><div class="IMG---Figure" id="_idContainer053"><img alt="Figure 2.7: An array of distances&#13;&#10;" src="image/C12626_02_07.jpg"/></div><h6>Figure 2.7: An array of distances</h6></li>
				<li>Since it has the shortest distance, point [-9,4] is added to cluster 1:<p>Clusters (2): </p><p class="snippet">[ [1,7], [-5,9], [-9,4] ]</p><p class="snippet">[4,-2] </p></li>
				<li>With only point (4,-2) left as the furthest distance away from its neighbors, you can just add it to cluster 1 to unify all the clusters:<p> Clusters (1): </p><p class="snippet">[ [ [1,7], [-5,9], [-9,4], [4,-2] ] ]</p></li>
				<li>Plot a dendrogram to show the relationship between the points and the clusters:</li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer054">
					<img alt="Figure 2.8: A dendrogram showing the relationship between the points and the clusters&#13;&#10;" src="image/C12626_02_08.jpg"/>
				</div>
			</div>
			<h6>Figure 2.8: A dendrogram showing the relationship between the points and the clusters</h6>
			<p>At the end of this process you can visualize the hierarchical structure that you created through a dendrogram. This plot shows how data points are similar and will look familiar to the hierarchical tree structures that we discussed earlier. Once you have this dendrogram structure, you can interpret how the data points relate to each other and subjectively decide at which "level" the clusters should exist.</p>
			<p>Revisiting the previous animal taxonomy example from that involved dog and cat species, imagine that you were presented with the following dendrogram:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer055">
					<img alt="Figure 2.9: An animal taxonomy dendrogram&#13;&#10;" src="image/C12626_02_09.jpg"/>
				</div>
			</div>
			<h6>Figure 2.9: An animal taxonomy dendrogram</h6>
			<p>The great thing about hierarchical clustering and dendrograms is that you can see the entire breakdown of potential clusters to choose from. If you were just interested in grouping your species dataset into dogs and cats, you could stop clustering at the first level of the grouping. However, if you wanted to group all species into domesticated or non-domesticated animals, you could stop clustering at level two. </p>
			<h3 id="_idParaDest-48"><a id="_idTextAnchor048"/>Exercise 7: Building a Hierarchy</h3>
			<p>Let's try implementing the preceding hierarchical clustering approach in Python. With the framework for the intuition laid out, we can now explore the process of building a hierarchical cluster with some helper functions provided in <strong class="inline">SciPy</strong>. This exercise uses <strong class="inline">SciPy</strong>, an open source library that packages functions that are helpful in scientific and technical computing; examples of this include easy implementations of linear algebra and calculus-related methods. In addition to <strong class="inline">SciPy</strong>, we will be using Matplotlib to complete this exercise:</p>
			<ol>
				<li value="1">Generate some dummy data as follows:<p class="snippet">from scipy.cluster.hierarchy import linkage, dendrogram, fcluster</p><p class="snippet">from sklearn.datasets import make_blobs</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">%matplotlib inline</p><p class="snippet"># Generate a random cluster dataset to experiment on. X = coordinate points, y = cluster labels (not needed)</p><p class="snippet">X, y = make_blobs(n_samples=1000, centers=8, n_features=2, random_state=800)</p></li>
				<li>Visualize the data as follows:<p class="snippet">plt.scatter(X[:,0], X[:,1])</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer056"><img alt="Figure 2.10: A plot of the dummy data&#13;&#10;" src="image/C12626_02_10.jpg"/></div><h6>Figure 2.10: A plot of the dummy data</h6><p>After plotting this simple toy example, it should be pretty clear that our dummy data is comprised of eight clusters.</p></li>
				<li>We can easily generate the distance matrix using the built-in <strong class="inline">SciPy</strong> package, '<strong class="inline">linkage</strong>':<p class="snippet"># Generate distance matrix with 'linkage' function</p><p class="snippet">distances = linkage(X, method="centroid", metric="euclidean")</p><p class="snippet">print(distances)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer057"><img alt="Figure 2.11: A matrix of the distances&#13;&#10;" src="image/C12626_02_11.jpg"/></div><h6>Figure 2.11: A matrix of the distances</h6><p>In the first situation, you can see that customizing the hyperparameters really drives the performance when finding the ideal linkage matrix. If you recall our previous steps, linkage works by simply calculating the distances between each of the data points. In the <strong class="inline">linkage</strong> function, we have the option to select both the metric and the method (we will cover more on this later). </p></li>
				<li>After we determine the linkage matrix, we can easily pass it through the dendrogram function provided by <strong class="inline">SciPy</strong>:<p class="snippet">dn = dendrogram(distances)</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer058"><img alt="Figure 2.12: A dendrogram of the distances &#13;&#10;" src="image/C12626_02_12.jpg"/></div><h6>Figure 2.12: A dendrogram of the distances </h6><p>This plot will give us some perspective on the potential breakouts of our data.</p></li>
				<li>Using this information, we can wrap up our exercise on hierarchical clustering by using the <strong class="inline">fcluster</strong> function from <strong class="inline">SciPy</strong>. The number <strong class="inline">3</strong> in the </li>
				<li>following example represents the maximum inter-cluster distance threshold hyperparameter that you will set. This hyperparameter is tunable based on the dataset that you are looking at; however, it is supplied for you as <strong class="inline">3</strong> for this exercise:<p class="snippet">scipy_clusters = fcluster(distances, 3, criterion="distance")</p><p class="snippet">plt.scatter(X[:,0], X[:,1], c=scipy_clusters)</p><p class="snippet">plt.show()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer059">
					<img alt="Figure 2.13: A scatter plot of the distances&#13;&#10;" src="image/C12626_02_13.jpg"/>
				</div>
			</div>
			<h6>Figure 2.13: A scatter plot of the distances</h6>
			<p>By simply calling a few helper functions provided by SciPy, you can easily implement agglomerative clustering in just a few lines of code. While SciPy does help with many of the intermediate steps, this is still an example that is a bit more verbose than what you will probably see in your regular work. We will cover more streamlined implementations later. </p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Linkage</h2>
			<p>In <em class="italics">Exercise 7</em>, <em class="italics">Building a Hierarchy</em>, you implemented hierarchical clustering using what is known as <strong class="keyword">Centroid Linkage</strong>. Linkage is the concept of determining how you can calculate the distances between clusters and is dependent on the type of problem you are facing. Centroid linkage was chosen for the first activity as it essentially mirrors the new centroid search that we used in k-means. However, this is not the only option when it comes to clustering data points together. Two other popular choices for determining distances between clusters are single linkage and complete linkage.</p>
			<p><strong class="keyword">Single Linkage</strong> works by finding the minimum distance between a pair of points between two clusters as its criteria for linkage. Put simply, it essentially works by combining clusters based on the closest points between the two clusters. This is expressed mathematically as follows:</p>
			<p>dist(a,b) = min( dist( a[i]), b[j] ) )</p>
			<p><strong class="keyword">Complete Linkage</strong> is the opposite of single linkage and it works by finding the maximum distance between a pair of points between two clusters as its criteria for linkage. Put simply, it works by combining clusters based on the furthest points between the two clusters. This is mathematically expressed as follows:</p>
			<p>dist(a,b) = max( dist( a[i]), b[j] ) )</p>
			<p>Determining what linkage criteria is best for your problem is as much about art as it is about science and it is heavily dependent on your particular dataset. One reason to choose single linkage is that your data is similar in a nearest-neighbor sense, therefore, when there are differences, then the data is extremely dissimilar. Since single linkage works by finding the closest points, it will not be affected by these distant outliers. Conversely, complete linkage may be a better option if your data is distant in terms of inter-cluster, however, it is quite dense intra-cluster. Centroid linkage has similar benefits but falls apart if the data is very noisy and there are less clearly defined "centers" of clusters. Typically, the best approach is to try a few different linkage criteria options and to see which fits your data in a way that's most relevant to your goals.</p>
			<h3 id="_idParaDest-50">Activity 2: A<a id="_idTextAnchor050"/>pplying Linkage Criteria</h3>
			<p>Recall the dummy data of the eight clusters that we generated in the previous exercise. In the real world, you may be given real data that resembles discrete Gaussian blobs in the same way. Imagine that the dummy data represents different groups of shoppers in a particular store. The store manager has asked you to analyze the shopper data in order to classify the customers into different groups, so that they can tailor marketing materials to each group. </p>
			<p>Using the data already generated in the previous exercise, or by generating new data, you are going to analyze which linkage types do the best job of grouping the customers into distinct clusters.</p>
			<p>Once you have generated the data, view the documents supplied using SciPy to understand what linkage types are available in the <strong class="inline">linkage</strong> function. Then, evaluate the linkage types by applying them to your data. The linkage types you should test are shown in the following list:</p>
			<p class="snippet">['centroid', 'single', 'complete', 'average', 'weighted']</p>
			<p>By completing this activity, you will gain an understanding of the linkage criteria – which is important to understand how effective your hierarchical clustering is. The aim is to gain an understanding of how linkage criteria play a role in different datasets and how it can make a useless clustering into a valid one.</p>
			<p>You may realize that we have not covered all of the previously mentioned linkage types – a key part of this activity is to learn how to parse the docstrings provided using packages to explore all of their capabilities.</p>
			<p>Here are the steps required to complete this activity:</p>
			<ol>
				<li value="1">Visualize the dataset that we created in <em class="italics">Exercise 7</em>, <em class="italics">Building a Hierarchy</em>.</li>
				<li>Create a list with all the possible linkage method hyperparameters.</li>
				<li>Loop through each of the methods in the list that you just created and display the effect they have on the same dataset.</li>
			</ol>
			<p>You should generate a plot for each linkage type and use the plots to comment on which linkage types are most suitable for this data. </p>
			<p>The plots that you will generate should look similar to the ones in the following diagram:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer060">
					<img alt="Figure 2.14: The expected scatter plots for all methods&#13;&#10;" src="image/C12626_02_14.jpg"/>
				</div>
			</div>
			<h6>Figure 2.14: The expected scatter plots for all methods</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity is on page 310.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>Agglomerative versus Divisive Clustering</h2>
			<p>Our instances of hierarchical clustering so far have all been agglomerative – that is, they have been built from the bottom up. While this is typically the most common approach for this type of clustering, it is important to know that it is not the only way a hierarchy can be created. The opposite hierarchical approach, that is, built from the top up, can also be used to create your taxonomy. This approach is called <strong class="keyword">Divisive</strong> Hierarchical Clustering and works by having all the data points in your dataset in one massive cluster. Many of the internal mechanics of the divisive approach will prove to be quite similar to the agglomerative approach:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer061">
					<img alt="Figure 2.15: Agglomerative versus divisive hierarchical clustering&#13;&#10;" src="image/C12626_02_15.jpg"/>
				</div>
			</div>
			<h6>Figure 2.15: Agglomerative versus divisive hierarchical clustering</h6>
			<p>As with most problems in unsupervised learning, deciding the best approach is often highly dependent on the problem you are faced with solving. </p>
			<p>Imagine that you are an entrepreneur who has just bought a new grocery store and needs to stock it with goods. You receive a large shipment of food and drink in a container, but you've lost track of all the shipment information! In order to most effectively sell your products, you must group similar products together (your store will be a huge mess if you just put everything on the shelves in a random order). Setting out on this organizational goal, you can take either a bottom-up or top-down approach. On the bottom-up side, you will go through the shipping container and think of everything as disorganized – you will then pick up a random object and find its most similar product. For example, you may pick up apple juice and realize that it makes sense to group it together with orange juice. With the top-down approach, you will view everything as organized in one large group. Then, you will move through your inventory and split the groups based on the largest differences in similarity. For example, you may originally think that apple juice and tofu go together, but on second thoughts, they are really different. Therefore, you will break them into smaller, dissimilar groups.</p>
			<p>In general, it helps to think of agglomerative as the bottom-up approach and divisive as the top-down approach – but how do they trade off in performance? Due to the greedy nature of Agglomerative, it has the potential to be fooled by local neighbors and not see the larger implications of clusters it forms at any given time. On the flip side, the divisive approach has the benefit of seeing the entire data distribution as one from the beginning and choosing the best way to break down clusters. This insight into what the entire dataset looks like is helpful for potentially creating more accurate clusters and should not be overlooked. Unfortunately, a top-down approach, typically, trades off greater accuracy with deeper complexity. In practice, an agglomerative approach works most of the time and should be the preferred starting point when it comes to hierarchical clustering. If, after reviewing the hierarchies, you are unhappy with the results, it may help to take a divisive approach.</p>
			<h3 id="_idParaDest-52"><a id="_idTextAnchor052"/>Exercise 8: Implementing Agglomerative Clustering with scikit-learn</h3>
			<p>In most real-world use cases, you will likely find yourself implementing hierarchical clustering with a package that abstracts everything away, such as scikit-learn. Scikit-learn is a free package that is indispensable when it comes to machine learning in Python. It conveniently provides highly optimized forms of the most popular algorithms, such as regression, classification, and, of book, clustering. By using an optimized package such as scikit-learn, your work becomes much easier. However, you should only use it when you fully understand how hierarchical clustering works from the prior sections. The following exercise will compare two potential routes that you can take when forming clusters – using SciPy and scikit-learn. By completing the exercise, you will learn what the pros and cons are of each, and which suits you best from a user perspective:</p>
			<ol>
				<li value="1">Scikit-learn makes implementation as easy as just a few lines of code:<p class="snippet">from sklearn.cluster import AgglomerativeClustering</p><p class="snippet">from sklearn.datasets import make_blobs</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from scipy.cluster.hierarchy import linkage, dendrogram, fcluster</p><p class="snippet">ac = AgglomerativeClustering(n_clusters = 8, affinity="euclidean", linkage="average")</p><p class="snippet">X, y = make_blobs(n_samples=1000, centers=8, n_features=2, random_state=800)</p><p class="snippet">distances = linkage(X, method="centroid", metric="euclidean")</p><p class="snippet">sklearn_clusters = ac.fit_predict(X)</p><p class="snippet">scipy_clusters = fcluster(distances, 3, criterion="distance")</p><p>First, we assign the model to the <strong class="inline">ac</strong> variable, by passing in parameters that we are familiar with, such as <strong class="inline">affinity</strong> (the distance function) and <strong class="inline">linkage</strong> (explore your options as we did in <em class="italics">Activity 2</em>, <em class="italics">Implementing Linkage Criteria</em>). </p></li>
				<li>After instantiating our model into a variable, we can simply pass through the dataset we are interested in in order to determine where the cluster memberships lie using <strong class="inline">.fit_predict()</strong> and assigning it to an additional variable.</li>
				<li>We can then compare how each of the approaches work by comparing the final cluster results through plotting. Let's take a look at the clusters from the scikit-learn approach:<p class="snippet">plt.figure(figsize=(6,4))</p><p class="snippet">plt.title("Clusters from Sci-Kit Learn Approach")</p><p class="snippet">plt.scatter(X[:, 0], X[:, 1], c = sklearn_clusters ,s=50, cmap='tab20b')</p><p class="snippet">plt.show()</p><p>Here is the output for the clusters from the scikit-learn approach:</p><div class="IMG---Figure" id="_idContainer062"><img alt="Figure 2.16: A plot of the Scikit-Learn approach&#13;&#10;" src="image/C12626_02_16.jpg"/></div></li>
			</ol>
			<h6> </h6>
			<h6>Figure 2.16: A plot of the Scikit-Learn approach</h6>
			<p>Take a look at the clusters from the SciPy Learn approach:</p>
			<p class="snippet">plt.figure(figsize=(6,4))</p>
			<p class="snippet">plt.title("Clusters from SciPy Approach")</p>
			<p class="snippet">plt.scatter(X[:, 0], X[:, 1], c = scipy_clusters ,s=50, cmap='tab20b')</p>
			<p class="snippet">plt.show()</p>
			<p>The output is as follows:</p>
			<h6> </h6>
			<div>
				<div class="IMG---Figure" id="_idContainer063">
					<img alt="Figure 2.17: A plot of the SciPy approach&#13;&#10;" src="image/C12626_02_17.jpg"/>
				</div>
			</div>
			<h6>Figure 2.17: A plot of the SciPy approach</h6>
			<p>As you can see in our example problem, the two converge to basically the same clusters. While this is great from a toy-problem perspective, you will soon learn, in the next activity, that small changes to the input parameters can lead to wildly different results!</p>
			<h3 id="_idParaDest-53">Activity 3: Compa<a id="_idTextAnchor053"/>ring k-means with Hierarchical Clustering</h3>
			<p>You are managing a store's inventory and receive a large shipment of wine, but the brand labels have fallen off the bottles during transit. Fortunately, your supplier has provided you with the chemical readings for each bottle, along with their respective serial numbers. Unfortunately, you aren't able to open each bottle of wine and taste test the difference – you must find a way to group the unlabeled bottles back together according to their chemical readings! You know from the order list that you ordered three different types of wine and are given only two wine attributes to group the wine types back together. In this activity, we will be using the wine dataset.</p>
			<h4>Note</h4>
			<p class="callout">The wine dataset can be downloaded from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/"><span class="Hyperlink">https://archive.ics.uci.edu/ml/machine-learning-databases/wine/</span></a>. It can be accessed at <a href="https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson02/Activity03"><span class="Hyperlink">https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson02/Activ</span></a><span class="Hyperlink">ity03</span>.</p>
			<p class="callout">UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml"><span class="Hyperlink">http://archive.ics.uci.edu/ml</span></a>]. Irvine, CA: University of California, School of Information and Computer Science</p>
			<p>The aim of this activity is to implement k-means and hierarchical clustering on the wine dataset and to explore which approach ends up being more accurate or easier for you to use. You can try different combinations of scikit-learn implementations and use helper functions in SciPy and NumPy. You can use the silhouette score to compare the different clustering methods and visualize the clusters on a graph.</p>
			<p>Expected Outcome: </p>
			<p>After completing this activity, you will have gained an understanding of how k-means and hierarchical clustering work on similar datasets. You will likely notice that one method performs better than the other depending on how the data is shaped. Another key outcome from this activity is gaining an understanding of how important hyperparameters are in any given use case.</p>
			<p>Here are the steps to complete this activity:</p>
			<ol>
				<li value="1">Import the necessary packages from scikit-learn (<strong class="inline">KMeans</strong>, <strong class="inline">AgglomerativeClustering</strong>, and <strong class="inline">silhouette_score</strong>).</li>
				<li>Read the wine dataset into the pandas DataFrame and print a small sample.</li>
				<li>Visualize the wine dataset to understand its data structure.</li>
				<li>Use the sklearn implementation of k-means on the wine dataset, knowing that there are three wine types.</li>
				<li>Use the sklearn implementation of hierarchical clustering on the wine dataset.</li>
				<li>Plot the predicted clusters from k-means.</li>
				<li>Plot the predicted clusters from hierarchical clustering.</li>
				<li>Compare the silhouette score of each clustering method.</li>
			</ol>
			<p>Plot the predicted clusters from the k-means clustering method as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer064">
					<img alt="Figure 2.18: The expected clusters from the k-means method&#13;&#10;" src="image/C12626_02_18.jpg"/>
				</div>
			</div>
			<h6>Figure 2.18: The expected clusters from the k-means method</h6>
			<p>Plot the predicted clusters from the agglomerative clustering method, as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer065">
					<img alt="Figure 2.19: The expected clusters from the agglomerative method&#13;&#10;" src="image/C12626_02_19.jpg"/>
				</div>
			</div>
			<h6>Figure 2.19: The <a id="_idTextAnchor054"/>expected clusters from the agglomerative method</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity is on page 312.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor055"/>k-means versus Hierarchical Clustering</h2>
			<p>Now that we have expanded our understanding of how k-means clustering works, it is important to explore where hierarchical clustering fits into the picture. As mentioned in the linkage criteria section, there is some potential direct overlap when it comes to grouping data points together using centroids. Universal to all of the approaches mentioned so far, is also the use of a distance function to determine similarity. Due to our in-depth exploration in the previous chapter, we have kept using the Euclidean distance, but we understand that any distance function can be used to determine similarity.</p>
			<p>In practice, here are some quick highlights for choosing one clustering method over another:</p>
			<ul>
				<li>Hierarchical clustering benefits from not needing to pass in an explicit "k" number of clusters apriori. This means that you can find all the potential clusters and decide which clusters make the most sense after the algorithm has completed.</li>
				<li>k-means clustering benefits from a simplicity perspective – oftentimes, in business use cases, there is a challenge to find methods that can be explained to non-technical audiences but still be accurate enough to generate quality results. k-means can easily fill this niche. </li>
				<li>Hierarchical clustering has more parameters to tweak than k-means clustering when it comes to dealing with abnormally shaped data. While k-means is great at finding discrete clusters, it can falter when it comes to mixed clusters. By tweaking the parameters in hierarchical clustering, you may find better results.</li>
				<li>Vanilla k-means clustering works by instantiating random centroids and finding the closest points to those centroids. If they are randomly instantiated in areas of the feature space that are far away from your data, then it can end up taking quite some time to converge, or it may never even get to that point. Hierarchical clustering is less prone to falling prey to this weakness.</li>
			</ul>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor056"/>Summary</h2>
			<p>In this chapter, we discussed how hierarchical clustering works and where it may be best employed. In particular, we discussed various aspects of how clusters can be subjectively chosen through the evaluation of a dendrogram plot. This is a huge advantage compared to k-means clustering if you have absolutely no idea of what you're looking for in the data. Two key parameters that drive the success of hierarchical clustering were also discussed: the agglomerative versus divisive approach and linkage criteria. Agglomerative clustering takes a bottom-up approach by recursively grouping nearby data together until it results in one large cluster. Divisive clustering takes a top-down approach by starting with the one large cluster and recursively breaking it down until each data point falls into its own cluster. Divisive clustering has the potential to be more accurate since it has a complete view of the data from the start; however, it adds a layer of complexity that can decrease the stability and increase the runtime. </p>
			<p>Linkage criteria grapples with the concept of how distance is calculated between candidate clusters. We have explored how centroids can make an appearance again beyond k-means clustering, as well as single and complete linkage criteria. Single linkage finds cluster distances by comparing the closest points in each cluster, while complete linkage finds cluster distances by comparing more distant points in each cluster. From the understanding that you have gained in this chapter, you are now able to evaluate how both k-means and hierarchical clustering can best fit the challenge that you are working on. In the next chapter, we will cover a clustering approach that will serve us best in the highly complex data: <strong class="bold">DBSCAN</strong> (Density-Based Spatial Clustering of Applications with Noise).</p>
		</div>
	</body></html>
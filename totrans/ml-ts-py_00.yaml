- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Time-Series with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is about machine learning for time-series with Python, and you can
    see this chapter as a 101 class for time-series. In this chapter, we'll introduce
    time-series, the history of research into time-series, and how to use Python for
    time-series.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with what a time-series is and its main properties. We'll then look
    at the history of the study of time-series in different scientific disciplines
    foundational to the field, such as demography, astronomy, medicine, and economics.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we'll go over the capabilities of Python for time-series and why Python
    is the go-to language for doing machine learning with time-series. Finally, I
    will describe how to install the most prominent libraries in Python for time-series
    analysis and machine learning, and we'll cover the basics of Python as relevant
    to time-series and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Time-Series?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characteristics of Time-Series
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Time-Series and Forecasting – Past and Present
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demography
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Genetics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Astronomy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Economics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Meteorology
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Medicine
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Applied Statistics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Python for Time-Series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But what is a time-series? Let's start with a definition!
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Time-Series?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since this is a book about time-series data, we should start with a clarification
    of what we are talking about. In this section, we'll introduce time-series and
    their characteristics, and we'll go through different kinds of problems and types
    of analyses relevant to machine learning and statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many disciplines, such as finance, public administration, energy, retail, and
    healthcare, are dominated by time-series data. Large areas of micro- and macro-economics
    rely on applied statistics with an emphasis on time-series analyses and modeling.
    The following are examples of time-series data:'
  prefs: []
  type: TYPE_NORMAL
- en: Daily closing values of a stock index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of weekly infections of a disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weekly series of train accidents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rainfall per day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensor data such as temperature measurements per hour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Population growth per year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quarterly earnings of a company over a number of years
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is only to name but a few. Any data that deals with changes over time is
    a time-series.
  prefs: []
  type: TYPE_NORMAL
- en: It might be worth defining briefly what is considered a time-series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition: Time-Series are datasets where observations are arranged in chronological
    order.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a very broad definition. Alternatively, we could have said that a time-series
    is a sequence of data points taken sequentially over time, or that a time-series
    is the result of a stochastic process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, we can define a time-series in two ways. The first one is as a mapping
    from the time domain to the domain of real numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_01_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B17577_01_002.png) and ![](img/B17577_01_003.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to define a time-series is as a stochastic process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_01_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17577_01_005.png) or ![](img/B17577_01_006.png) denotes the value
    of the random variable X at time point t.
  prefs: []
  type: TYPE_NORMAL
- en: If T is a set of real numbers, it's a continuous-time stochastic process. If
    T is a set of integers, we call it a stochastic process in discrete time. The
    convention in the latter case is to write ![](img/B17577_01_007.png).
  prefs: []
  type: TYPE_NORMAL
- en: Since time is the primary index of the dataset, by implication, time-series
    datasets describe how the world changes over time. They often deal with the question
    of how the past influences the presence or future.
  prefs: []
  type: TYPE_NORMAL
- en: The increase of monitoring and data collection brings with it the need for both
    statistical and machine learning techniques applied to time-series to predict
    and characterize the behavior of complex systems or components within a system.
    An important part of working with time-series is the question of how the future
    can be predicted based on the past. This is called forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Some methods allow adding business cycles as additional features. These additional
    features are called **exogenous** features - they are time-dependent, explanatory
    variables. We'll go through examples of feature generation in *chapter 3*, *Preprocessing
    Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of Time-Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s an extract of a time-series dataset as an example, exported from Google
    Trends, on searches for Python, R, and Julia:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 2)/Screenshot 2021-03-11 at 17.13.02.png](img/B17577_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Extract of a time-series dataset'
  prefs: []
  type: TYPE_NORMAL
- en: This is a **multivariate** time-series, with columns for Python, R, and Julia.
    The first column is the index, a date column, and its period is the month. In
    cases, where we have only a single variable, we speak of a **univariate** series.
    This dataset would be univariate if we had only one programming language instead
    of three.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time-Series mostly come as discrete-time, where the time difference between
    each point is the same. The most important characteristics of time-series are
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Long-term movements of the values (**trend**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seasonal variations (**seasonality**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irregular or cyclic components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A trend is the general direction in which something is developing or changing,
    such as a long-term increase or decrease in a sequence. An example of where a
    trend can be observed would be global warming, the process by which the temperatures
    on our planet have been rising over the last half-century.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a plot of global surface temperature changes over the last 100 years
    from the GISS Surface Temperature Analysis dataset released by NASA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![temperatures.png](img/B17577_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: GISS surface temperature analysis from 1880 to 2019'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 1.2*, temperature changes have been varying around
    0 until the mid-20^(th) century; however, since then, there's been a clearly visible
    trend of an overall rise in the yearly temperature.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonality is a variation that occurs at specific regular intervals of less
    than a year. Seasonality can occur on different time spans, such as daily, weekly,
    monthly, or yearly. An example of weekly seasonality would be sales of ice cream
    picking up each weekend. Also, depending on where you live, ice cream might only
    be sold in spring and summer. This is a yearly variation.
  prefs: []
  type: TYPE_NORMAL
- en: Other than seasonal changes and trends, there is variability that's not of a
    fixed frequency or that rises and falls in a way that's not based on seasonal
    frequency. Some of these we might be able to explain based on the knowledge we
    have.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of cyclic variability that's irregular, bank holidays can fall
    on different calendar days each year, and promotional campaigns could depend on
    business decisions, such as the introduction of a new product. As an example of
    cyclic changes that are not seasonal, changes at the scale of milliseconds or
    that take place over time periods longer than a year would not be called seasonal
    effects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stationarity** is the property of a time-series not to change its distribution
    over time as described by its summary statistics. If a time-series is stationary,
    it means that it has no trend and no deterministic seasonal variability, although
    other cyclical variability is permitted. This is an important feature for the
    algorithms that we''ll discuss in *Chapter 5*, *Forecasting with Moving Averages
    and Autoregressive Models*. To apply them, we''ll need to transform non-stationary
    data into stationary data by removing seasonality and trend.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss these and other concepts in more detail in *Chapter 2*, *Time-Series
    Analysis with Python*, and *Chapter 3, Preprocessing Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: The task of identifying, quantifying, and decomposing these and other characteristics
    is called **time-series** **analysis**. Exploratory time-series analysis is often
    the first step before any feature transformation and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Time-Series and Forecasting – Past and Present
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time-Series have been studied since antiquity, and since then, time-series analysis
    and forecasting have come a long way. A variety of disciplines contributed to
    the development of techniques applied to time-series, including mathematics, astronomy,
    demographics, and statistics. Many innovations came initially from mathematics,
    later statistics, and finally machine learning. Many innovations in applied statistics
    had their origins in demography (used in public administration), economics, or
    other fields.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I'll sketch the development path from simpler methods leading
    up to the machine learning methods available today. I'll try to chart the development
    of concepts relevant to time-series from the time of the Industrial Revolution
    to modernity. We'll deal with the more technical and up-to-date side of things
    in *Chapter 4*, *Introduction to Machine Learning with Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: There's still much more to come for time-series. The development of wearable
    sensors and the Internet of Things means that big data is available to be analyzed
    and used for forecasting. The availability of large datasets for benchmarks and
    competitions has been helping create new methods in recent years as we'll discuss
    in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Demography
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much of the early work that went into establishing the theory and practice of
    time-series analysis came from demography as used in public administration. Many
    of the people mentioned in this section either worked as public servants or contributed
    in a private capacity out of interest in abstract problems.
  prefs: []
  type: TYPE_NORMAL
- en: John Graunt, originally a haberdasher by profession, became interested in death
    records as recorded by London parishes. In 1662, he published public health statistics
    in his book "*Natural and Political Observations Made upon the Bills of Mortality.*"
    Among statistics about epidemiology, it included the first life table. A **life
    table** (also called a mortality table or actuarial table) is a table that shows,
    for each age, what the probability is that a person of that age will die before
    their next birthday. Graunt made his inferences from bills of mortality, generated
    by parish clerks who recorded burials in Church of England churchyards in the
    City of London and areas outside the city.
  prefs: []
  type: TYPE_NORMAL
- en: Graunt's book was highly influential, and he is widely regarded as the founder
    of demography. Graunt was elected as a fellow of the Royal Society; however, he
    suffered bankruptcy after his house burned down during the Great Fire of London
    in 1666, and he died of jaundice and liver disease at the age of 53.
  prefs: []
  type: TYPE_NORMAL
- en: Among other things, he inspired the work of Swiss mathematician Jakob Bernoulli,
    "*Ars Conjectandi*," written between 1684 and 1689 and published posthumously
    in 1713, a landmark publication in combinatorics and probability theory that included
    – among many other things – a first version of the law of large numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The law of large numbers describes what happens when an experiment is repeated
    a large number of times. Bernoulli proved that in a game of chance with two outcomes
    (such as a coin toss), a win or a loss, if it's repeated many times, the fraction
    of times that the game would be won approaches the true, expected probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another major milestone in the history of demography, the statistical study
    of human populations, came in 1689 as an article written by Caspar Neumann, a
    German professor and clergyman – "*Reflexionen über Leben und Tod bei denen in
    Breslau Geborenen und Gestorbenen*" (translated: Reflections about the Life and
    Death of People Who Were Born and Died in Breslau). Neumann sent this treatise
    to Gottfried Leibniz, the eminent philosopher and mathematician, and later made
    his data available to the Royal Society in London.'
  prefs: []
  type: TYPE_NORMAL
- en: Many subsequent works were based on the data and statistics in this article.
    In 1693, in an article on life annuities ("*An Estimate of the Degrees of the
    Mortality of Mankind*") published in the *Philosophical Transactions of the Royal
    Society*, Edmond Halley prepared mortality tables based on Neumann's data. **Annuities**
    are payments made at equal intervals, such as mortgage, insurance, and pension
    payments. Halley's article guided the development of actuarial science and informed
    the British government when it came to selling retirement income insurance at
    an appropriate price based on the age of the purchaser. We'll encounter Halley
    again in the astronomy section.
  prefs: []
  type: TYPE_NORMAL
- en: Abraham de Moivre was a Frenchman who moved to England at a young age due to
    the religious persecution of the Huguenots in France. Today, he is best known
    for his work on the normal distribution and probability theory. In 1724, he published
    a book called "*Annuities upon Lives*," the cover of which you can see below,
    about mortality statistics and the foundation of the theory of annuities.
  prefs: []
  type: TYPE_NORMAL
- en: '![../../Desktop/Screenshot%202021-04-15%20at%2022.55.21.pn](img/B17577_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Annuities upon Lives'
  prefs: []
  type: TYPE_NORMAL
- en: De Moivre is also remembered for an approximation to the binomial distribution
    and for his work on the Poisson distribution (later named after Siméon Denis Poisson).
  prefs: []
  type: TYPE_NORMAL
- en: With some statistics foundations in place, we are now getting into projections
    into the future, and this is where time-series forecasts come in. In 1751, Benjamin
    Franklin examined population growth and its limits in his essay "*Observations
    Concerning the Increase of Mankind, Peopling of Countries, etc.*," projecting
    exponential growth in the British colonies. He projected a doubling of the population
    in the British Crown Colonies every 25 years, with the potential, he argued, to
    spread liberal political tradition and increase the power of England. His projection
    proved correct, and the exponential growth held up until the 1850s when the population
    of the United States surpassed England's.
  prefs: []
  type: TYPE_NORMAL
- en: Influenced by Franklin was the English cleric Thomas Robert Malthus, who feared
    that population growth would outstrip growth in food production. In his scenario,
    while population growth is exponential, the growth of food supply and other resources
    is linear, which would eventually lead to a collapse of society and massive population
    death. Writing at the end of the 18th century, he described ever-increasing famine
    and poverty (referred to after him as the "*Malthusian catastrophe*").
  prefs: []
  type: TYPE_NORMAL
- en: Many other statistical and mathematical concepts were worked out based on demographics
    data. Adolphe Quetelet, an astronomer, mathematician, and sociologist from Ghent,
    in today's Belgium, introduced statistical methods to social sciences to describe
    relationships underlying crime rates, marriage rates, and suicide rates. He called
    for a "social physics" that would find the laws underlying social phenomena, thus
    revealing the work of God. Among other things, he developed the body mass index,
    originally called the Quetelet index. In his 1835 book, called "*Treatise on Man*"
    in the English translation, he describes the concept of the average man based
    on normal distribution. One of Quetelet's students, Pierre Verhulst, developed
    the logistic function as a model of population growth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Siméon Denis Poisson published "*Recherches sur la probabilité des jugements
    en matière criminelle et en matière civile*" (translated: Studies on the Probability
    of Judgments in Criminal and Civil Matters) in 1837, where he elaborated on probability
    theory for discrete occurrences that take place within a given interval. The Poisson
    distribution was named after him.'
  prefs: []
  type: TYPE_NORMAL
- en: Wilhelm Lexis, a pioneer of the analysis of demographic time-series, published
    a paper called "*On the Theory of the Stability of Statistical Series*" (1879),
    which introduced the quantity now called the Lexis ratio. The ratio distinguishes
    between stable series, where underlying probability distributions giving rise
    to the observed rates remain constant, and non-stable series. These stable time-series
    would not be influenced by forces other than random noise. In today's terminology,
    a stable time-series would be referred to as a white noise process or a zero-order
    moving average.
  prefs: []
  type: TYPE_NORMAL
- en: In order to distinguish between stable and non-stable time-series, Lexis created
    a test statistic equal to the ratio between the dispersion of the observed rates
    and the dispersion that would be expected if the underlying probabilities for
    each of the observed rates were all equal across all of the observations. If this
    ratio, Q, was more than 1.41, he argued, this means the time-series is non-stable
    or – in his words – influenced by physical forces. Lexis later became a member
    of the Insurance Advisory Council for Germany's Federal Insurance Supervisory
    Office.
  prefs: []
  type: TYPE_NORMAL
- en: Genetics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Francis Galton, a Victorian-era English scientist, was born into an illustrious
    family of bankers and gun manufacturers that included several members of the Royal
    Society. Galton was a highly prolific writer and researcher. Today, he is mostly
    remembered for coining the word eugenics, the study of changes to the racial quality
    of future generations with a focus on desirable human qualities. Eugenics is associated
    with racism and white supremacy.
  prefs: []
  type: TYPE_NORMAL
- en: Galton was interested in many scientific disciplines, such as psychology, statistics,
    psychophysics, photography, and others, and for his contributions, he was knighted
    in 1909\. Among other things, he contributed to anthropometry, the systematic
    measurement and description of human bodies. For this work, he rediscovered the
    concept of correlation (first developed by French physicist Auguste Bravais in
    1846) and described correlations between forearm length and height, head width
    and head breadth, and head length and height (1888).
  prefs: []
  type: TYPE_NORMAL
- en: One of his protégés (and biographers) was Karl Pearson, born in Islington, London,
    into a Quaker family to a father who was Queen's Counsel (a lawyer). After studying
    mathematics at King's College, Cambridge, physics and philosophy at the University
    of Heidelberg, and physiology at the University of Berlin, he returned to London
    to study law. In London, he was introduced to Galton, and the two stayed in contact.
  prefs: []
  type: TYPE_NORMAL
- en: After Galton's death, Pearson was the first to hold the Chair in Eugenics endowed
    by Galton in his will. Pearson's main interest was in applying biometrics in the
    context of inheritance. He is credited with the invention of the standard deviation,
    a measure of the variability of the normal distribution, which replaced Carl Friedrich
    Gauss' concept of the mean error. He also developed contributions to statistics,
    including the chi-squared test, the p-value for statistical significance, correlation
    as it's used today, principal component analysis, and the histogram.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson was succeeded as the Galton Professor of Eugenics (later renamed the
    Galton Chair of Genetics) by Ronald Fisher. Fisher made many innovations in evolutionary
    theory about mimicry, parental investment, and the Fisher principle behind the
    1:1 sex ratio. In statistics, he described the linear discriminant analysis, Fisher
    information, the F-distribution, and the Student's t-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: His contributions to statistics laid the groundwork for statistical testing
    in time-series analysis and some of the classical models. Fisher was made a Knight
    Bachelor by Queen Elizabeth II in 1952\. However, his connection to racist views
    – for example, he endorsed the German Nazi party's policy of extermination with
    the goal of improving the genetic stock – has led to a recent reappraisal of his
    work.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, the Ronald Fisher Centre at **University College London**
    (**UCL**) was renamed to the Centre for Computational Biology, and UCL released
    a public apology for its role in propagating eugenics.
  prefs: []
  type: TYPE_NORMAL
- en: Astronomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Observations of comets and asteroids, and the and the movements of the sun and
    the planets have been recorded for a long time, and people have been studying
    these records to understand the regularities and relationships of these movements
    and our place in the universe. Edmond Halley, English astronomer and geophysicist,
    who we mentioned in the section on demography, applied Isaac Newton's laws of
    motion (from 1687) to comet sightings throughout history. A comet visible to the
    naked eye from Earth, it has been seen around the world and inscribed by astronomers
    and philosophers for at least about 2,000 years, appearing in Ancient Greek writings
    and Babylonian tables.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, its appearance in 12 BCE, close to the assigned date of the birth
    of Jesus Christ, has led to suggestions that it might be behind the biblical story
    of the Star of Bethlehem. In 1066, the comet was seen in England and thought to
    be a divine message, a bad omen foretelling Harold II's fate when he died the
    same year at the Battle of Hastings fighting against Norman invaders led by William
    the Conqueror.
  prefs: []
  type: TYPE_NORMAL
- en: Halley connected many of these sightings and concluded that it was the same
    comet each time and calculated a periodicity of 75-76 years. Today it is named
    after him in his honor, **Halley's Comet**. This was published in the "*Synopsis
    of the Astronomy of Comets* (1705)". Halley's Comet will re-appear in 2061.
  prefs: []
  type: TYPE_NORMAL
- en: 'This figure shows the orbit of Halley''s Comet (source: Wikimedia Commons):'
  prefs: []
  type: TYPE_NORMAL
- en: '![ile:Halley''s Comet animation.gif - Wikimedia Commons](img/B17577_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Halley''s Comet orbit'
  prefs: []
  type: TYPE_NORMAL
- en: German polymath Carl Friedrich Gauss devised a method for determining the orbit
    of the dwarf planet Ceres in 1801\. Ceres orbits in the asteroid belt between
    Mars and Jupiter. Gauss did this based on observations of a Catholic priest and
    astronomer, Giuseppe Piazzi, who traced an object between January and February
    of the same year, before losing sight of it.
  prefs: []
  type: TYPE_NORMAL
- en: Later, line fitting was applied to the movements of celestial bodies, most prominently,
    the **least squares method**. It was first described by Adrien-Marie Legendre
    in 1805 ("*méthode des moindres carrés*"), but today it is co-credited to Gauss.
    Gauss published about the method later, in 1809; however, he expanded significantly
    beyond Legendre's work, among other things inventing the distribution named after
    him, the Gaussian distribution (also called the normal or Bell distribution).
  prefs: []
  type: TYPE_NORMAL
- en: The **least squares** method is the underpinning of linear regression methods,
    where the parameters in sets of equations are estimated. It's a statistical procedure
    to find the best fit for a set of data points by minimizing the sum of the squared
    residuals from the plotted curve.
  prefs: []
  type: TYPE_NORMAL
- en: Only a year later, Pierre-Simon Laplace proved the **central limit theorem**,
    which roughly states that the sum of independent variables, even if they are not
    from the normal distribution, tends toward a normal distribution. This gave an
    important justification for the method of least squares and the normal distribution
    in the case of large datasets. The normal distribution has been highly influential
    in the field of statistics ever since, and measures such as the mean and the standard
    deviation are used to describe it.
  prefs: []
  type: TYPE_NORMAL
- en: Laplace was highly interested in planetary motion, but he also came up with
    a dynamic systems theory of tidal movements and probability theory. A fun fact
    to know about Laplace is that he was Napoleon Bonapart's examiner in 1784 when
    the latter attended the École Militaire in Paris.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of Laplace''s most famous contributions is the rule of succession, which
    describes the probability that an event will occur given past events. The sunrise
    example that he came up with for illustration of the rule of succession is the
    probability of the sun rising tomorrow given the number of days it has risen in
    the past:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_01_008.png)'
  prefs: []
  type: TYPE_IMG
- en: Laplace's assumption for the sunrise problem was that we have no knowledge of
    the matter other than the number of days of observations used in the formula.
    He actually cautioned that the application in this context is a misapplication
    of the rule given that we know much more about the movements of the sun and Earth.
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by astronomic calculations, Augustin-Louis Cauchy invented the gradient
    descent optimization algorithm in 1847 (in the journal of the French Academy of
    Sciences, *Comptes rendus de l'Académie des Sciences*), where repeated steps in
    the opposite direction of the gradient led to finding a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Many other optimization and curve-fitting innovations followed. First published
    in 1944 by Kenneth Levenberg and rediscovered in 1963 by Donald Marquardt, the
    Levenberg–Marquardt algorithm (also called the damped least-squares method) can
    be used for curve fitting in problems, where the dependent variables are a non-linear
    combination of the model parameters (non-linear problems). It combines the Gauss-Newton
    algorithm, a variation of the Newton algorithm published by Gauss in 1809, and
    the method of gradient descent invented about a hundred years earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Economics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: William Playfair was born in Scotland in 1759, the fourth son of a reverend's
    family. He took an apprenticeship with Andrew Meikle, the inventor of the threshing
    machine, and went on to become the personal assistant to James Watt at the Boulton and
    Watt steam engine manufactory.
  prefs: []
  type: TYPE_NORMAL
- en: His life was so eventful that several novels could be written about it. In 1789,
    he took part in the storming of the Bastille in Paris. After that he was involved
    as an agent of William Duer, a speculator, and the Scioto Company, in what could
    have been an embezzlement scheme, selling worthless deeds for land in Ohio to
    Frenchmen willing to emigrate. Back in London, he opened a bank that went bankrupt.
    Later, he was imprisoned for a few years in a debtor's prison, Fleet Prison, for
    being indebted. Then he went on to become a British secret agent, counterfeiting
    the French currency from 1789 to 1796, the assignat, to undermine the French government.
    The assignat soon became worthless, and inflation undermined the French government.
    He also patented several inventions for metalworking machinery and ships.
  prefs: []
  type: TYPE_NORMAL
- en: One of Playfair's principal achievements, however, was his popularization of
    several kinds of visualizations, such as the pie chart, the bar chart, and the
    time-series chart. He is sometimes credited with the invention of the bar chart,
    although Nicole Oresme showed a bar chart in a publication several hundred years
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two plots, the bar chart and the time-series plot, both from his "*Commercial
    and Political Atlas*" in 1786 (image source: Wikipedia):'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://upload.wikimedia.org/wikipedia/commons/e/e0/1786_Playfair_-_Exports_and_Imports_of_Scotland_to_and_from_different_parts_for_one_Year_from_Christmas_1780_to_Christmas_1781.jpg](img/B17577_01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Playfair''s visualizations from Commercial and Political Atlas'
  prefs: []
  type: TYPE_NORMAL
- en: On the left, you can see the bar chart Playfair used for a quantitative comparison
    of import and export data in Scotland. On the right is the time-series chart,
    to show the British trade balance over time.
  prefs: []
  type: TYPE_NORMAL
- en: Meteorology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Greek philosopher Aristotle was the first to write about weather and its
    measurement; however, it took much longer for the first weather predictions to
    be made. Vice Admiral Robert FitzRoy founded the United Kingdom's national weather
    service, the Meteorological Office, in 1854\. FitzRoy had already reserved his
    place in history as the captain of the HMS Beagle, the ship that carried a recently
    graduated naturalist by the name of Charles Darwin around the world, playing a
    pivotal role in the formation of theories on evolution and natural selection.
  prefs: []
  type: TYPE_NORMAL
- en: Supported by the telegraph and the barograph, a form of barometer, the Met Office
    collected weather data from many different locations in London. In 1859, the steam
    clipper Royal Charter, while returning to Liverpool from Melbourne, Australia,
    shipwrecked on rocks off the Welsh coast in a storm, leading to the loss of about
    450 lives. This disaster led to the development of a storm warning system that
    was later extended to general weather predictions. It was FitzRoy, in fact, who
    coined the word *forecast*, although at the time, many contemporaries referred
    to them as "quack weather prognostications". It is unclear whether his forecasts
    followed any system. He was much ridiculed by the scientific establishments for
    his work. He was prominently criticized by Sir Francis Galton, who had published
    a book called "*Meteorographica*" and later published the first weather maps.
    FitzRoy took his own life by cutting his throat in 1865\. The storm warnings were
    temporarily discontinued, only to be resumed a few years later to continue to
    this day.
  prefs: []
  type: TYPE_NORMAL
- en: The first weather models that used atmosphere and oceans were attempted in the
    1920s by Lewis Fry Richardson based on work by Norwegian Vilhelm Bjerknes, lecturer
    at the University of Stockholm on differential equations of fluid dynamics and
    thermodynamics. These models were impractical before the advent of computers,
    however—Richardson worked for about six weeks on a weather forecast of a limited
    area. His forecast turned out to be inaccurate because of numerical instability,
    even though his methodology was essentially correct. He abandoned his work when
    it became clear that his work could be of value to chemical weapons designers.
  prefs: []
  type: TYPE_NORMAL
- en: The first computerized weather models were programmed on the **Electronic Numerical
    Integrator and Computer** (**ENIAC**). The ENIAC, designed by John Mauchly and
    J. Presper Eckert, could run arbitrary sequences of operations; however, it didn't
    read the programs from tapes but from plugboard switches. The giant 15x9-meter
    machine is exhibited today at the Smithsonian Institute in Washington, D.C. Consisting
    of 17,500 vacuum tubes, it first produced calculations for the construction of
    a hydrogen bomb and was then exploited to extend forecasting past one or two days
    using new methods of numerical weather prediction. The computer was programmed
    by Klara von Neumann.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a photo of the ENIAC (source: Wikimedia Commons):'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Eniac.jpg/1024px-Eniac.jpg](img/B17577_01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Electronic Numerical Integrator and Computer (ENIAC)'
  prefs: []
  type: TYPE_NORMAL
- en: You can see Betty Snyder, one of the earliest programmers of the ENIAC, standing
    in front of the ENIAC.
  prefs: []
  type: TYPE_NORMAL
- en: Later, Joseph Smagorinsky and Douglas Lilly developed a mathematical model for
    turbulence used in computational fluid dynamics. This model, the Smagorinsky-Lilly
    model, which is still in use today, used data about the wind, cloud cover, precipitation,
    atmospheric pressure, and radiation emanating from the earth and sun as input.
    Smagorinsky continued to lead research on global warming, investigating the climate's
    sensitivity to increasing carbon dioxide levels.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of mobile sensor arrays and computerized models has greatly
    increased the accuracy of predictions. Valuable temperature and wind data is collected
    by sensors deployed by meteorology offices or other sources, most importantly
    by commercial aircraft as they fly. Today, within a seven-day window, a forecast
    is accurate about 80% of the time. The grounding of commercial flights during
    the COVID pandemic, where there were about 75% fewer flights for some periods,
    has led to less accurate forecasts recently.
  prefs: []
  type: TYPE_NORMAL
- en: Medicine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 1901, Willem Einthoven applied the string galvanometer used in the telegraph
    receiver to physiology. Working in Leiden, in the Netherlands, he improved upon
    previous designs, producing the first practical electrocardiogram (ECG). The ECG
    is important for monitoring and screening the function of the heart and can detect
    cardiac rhythm disturbances, inadequate coronary artery blood flow, and electrolyte
    disturbances. For the importance of this innovation, Einthoven was awarded the
    1924 Nobel prize in Physiology or Medicine.
  prefs: []
  type: TYPE_NORMAL
- en: Hans Berger recorded the first human electroencephalography (EEG) recording
    in 1924\. EEG measures the electrical activity of the brain with electrodes placed
    on the scalp. An EEG recording shows the brain's spontaneous electrical activity
    over a period of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a graph of an EEG signal (from the EEG Eye State dataset uploaded by
    Oliver Roesler from DHBW, Germany):'
  prefs: []
  type: TYPE_NORMAL
- en: '![eeg_signal.png](img/B17577_01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: EEG signal'
  prefs: []
  type: TYPE_NORMAL
- en: 'In EEG, the electrical activity of the brain is recorded through electrodes
    placed on the scalp. Its signal typically shows strong oscillations (also referred
    to as brain waves) at a variety of frequency ranges, most prominently these:'
  prefs: []
  type: TYPE_NORMAL
- en: Alpha (8-12 Hertz) would occur in a relaxed state, especially when closing the
    eyes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beta (16-31 Hertz) signals more active thinking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamma (more than 32 Hertz) indicates cross-modal sensory processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The medical uses of EEG are broad – among other things, EEG can be used to diagnose
    epilepsy, sleep disorders, tumors, stroke, depth of anesthesia, coma, and brain
    death.
  prefs: []
  type: TYPE_NORMAL
- en: Applied Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applied statistics and mathematics also provided inspiration and a foundation
    for work with time-series. Reverend Thomas Bayes (pronounced /be![](img/B17577_01_009.png)z/)
    proves a theorem that describes the probability of an event based on prior knowledge.
    Bayes' theorem is considered the foundation of Bayesian inference, a statistical
    inference approach.
  prefs: []
  type: TYPE_NORMAL
- en: We'll come back to this in *Chapter 9*, *Probabilistic Models for Time-Series*.
    Bayes' manuscripts were read to the Royal Society by his friend Richard Price
    within two years of his death (in 1761) in a heavily edited form.
  prefs: []
  type: TYPE_NORMAL
- en: The Fourier transform, which is important for filtering, converts a signal from
    its time domain to a representation in the frequency domain. The trigonometric
    decomposition of functions was discovered by Joseph Fourier in 1807, but a fast
    algorithm was first invented by Gauss around 1805 (although published only after
    his death and in Latin), and then rediscovered 160 years later by J. W. Cooley
    and John Tukey.
  prefs: []
  type: TYPE_NORMAL
- en: Classical time-series modeling approaches were introduced by George Box and
    Gwilym Jenkins in 1970 in their book "*Time-Series Analysis Forecasting and Control*."
    Most importantly, they formalized the ARIMA and ARMAX models and described how
    to apply them to time-series forecasting. We'll talk about these types of models
    in *Chapter 5*, *Forecasting with Moving Averages and Autoregressive Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Python for Time-Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For time-series, there are two main languages, R and Python, and it's worth
    briefly comparing the two and describing what makes Python special. Python is
    one of the top programming languages by popularity. According to the TIOBE from
    February 2021, it is only surpassed in popularity by C and Java.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Rank** | **Language** | **Ratings** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | C | 16.34% |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Java | 11.29% |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Python | 10.86% |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | C++ | 6.88% |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | R | 1.56% |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | Julia | 0.52% |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 1.8: TIOBE language usage statistics'
  prefs: []
  type: TYPE_NORMAL
- en: I've included R and Julia, two other languages used for data science, in order
    to support the point that Python is the most popular data science language. When
    comparing search volumes for Python, R, and Julia, the three foremost languages
    for data science, we can see that Python is much more popular than R, with Julia
    being the distant third. In fact, Python is ranked similar to languages such as
    C, Java, and C++. R is at a similar level to Assembly language and Groovy, and
    Julia is at the level of specialist languages such as Prolog.
  prefs: []
  type: TYPE_NORMAL
- en: R's community consists of statisticians and mathematicians, and R's strengths
    lie in statistics and plotting (ggplot). The weakness of R is its tooling and
    the virtual absence of consistent code style conventions.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, Python has been catching up in statistics and scientific
    computing with libraries such as NumPy, SciPy, and pandas, and it has overtaken
    R in both usage and usability for data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python stands out in terms of machine learning libraries. The following libraries
    are written entirely or mainly in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn is written in Python and Cython (a Python dialect similar to the
    C programming language). It provides implementations of a very large set of algorithms
    for training and evaluating machine learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statsmodels provides statistical tests, and models such as the generalized linear
    model (GLM), ARMA, and many more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras is an abstraction for training neural networks in Python that interact
    with TensorFlow and other libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the most popular machine learning frameworks – ones that see lots of
    use for development and have a large range of scalable algorithms, such as TensorFlow,
    PyTorch, and XGBoost – are also mainly written in Python or provide first-class
    interfaces for Python.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, being a general-purpose language, Python is ideal if you want to
    go beyond just data analysis. With Python, you can implement the full data flow
    necessary for building an end-to-end machine learning system that you can deploy
    and integrate with the backend platforms of your company.
  prefs: []
  type: TYPE_NORMAL
- en: The following time-series plot shows the popularity of Python and R according
    to Google Trends. Julia is omitted because it hardly registered at the bottom
    of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, COVID has dented the popularity of Python, but other programming languages
    have gone down in terms of search volumes as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![python_timeline.png](img/B17577_01_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Python versus R usage over time'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python has been clearly winning out over R for the last few years, although
    it has to be admitted that the comparison is not completely fair since Python
    finds much broader application than R. However, Python is also one of the best-supported
    languages for data science in general and time-series in particular. As of February
    2021, if we search GitHub for time-series, we find about five times the number
    of repositories (including repositories with Jupyter notebooks). For Julia, I
    found about 104 repositories. Please see the following table for the exact numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Language** | **Repositories for time-series** |'
  prefs: []
  type: TYPE_TB
- en: '| Jupyter Notebook | 11,297 |'
  prefs: []
  type: TYPE_TB
- en: '| Python | 4,891 |'
  prefs: []
  type: TYPE_TB
- en: '| R | 3,656 |'
  prefs: []
  type: TYPE_TB
- en: '| Julia | 104 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 1.10: TIOBE language usage statistics'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to just give a flavor of Python machine learning projects specializing
    in time-series, here''s a short list of prominent libraries on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: prophet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sktime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gluon-ts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tslearn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pyts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: seglearn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: darts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cesium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pmdarima
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These screenshots (taken from gitcompare.com) summarize some of the statistics
    around these libraries, such as the number of stars (how many times someone liked
    the library), forks (how many times someone copied the library in order to study
    it or make changes), age (how long has the repository existed), and others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/Users/ben/Dropbox/vimwiki/_html/assets/time-series-libraries1.png](img/B17577_01_09.png)![/Users/ben/Dropbox/vimwiki/_html/assets/time-series-libraries2.png](img/B17577_01_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11: Library statistics for prominent Python libraries'
  prefs: []
  type: TYPE_NORMAL
- en: We'll go through many of these time-series libraries in this book. We'll deal
    with a few Python data science libraries in the following sections, but if you
    want a full introduction to any of these libraries, you should go through a book
    specific to data science in Python, or even NumPy and pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Installing libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two main tools for maintaining and installing the Python libraries you'll
    need for this book are conda and pip.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the commands within the next two subsections should be executed
    from the system terminal or – in the case of conda – using the Anaconda navigator.
    For Windows and Mac users, there are graphical user interfaces available, where
    libraries can be searched and installed, instead of relying on the terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '**conda** works with Python, R, and other languages for the management of dependencies,
    and for environment encapsulation. conda helps with the installation of system
    libraries as well by maintaining lists of libraries associated with Python libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to get started with conda is to install anaconda by following
    the instructions from this link: [https://docs.continuum.io/anaconda/install/](https://docs.continuum.io/anaconda/install/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s also a graphical interface to conda that comes with a slick design,
    as this screenshot shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![anaconda%20navigator.png](img/B17577_01_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.12: Anaconda navigator'
  prefs: []
  type: TYPE_NORMAL
- en: The Anaconda navigator can be installed on macOS and Microsoft Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can rely completely on the terminal. For example, here''s
    how to install the NumPy library from your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As a side note – if you want to work with the R programming language, you can
    use conda, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: See the conda documentation for an in-depth introduction and tutorials. Conda
    also installs versions of Python and pip, so you can use either pip or conda to
    install Python libraries, while having your environment managed with conda.
  prefs: []
  type: TYPE_NORMAL
- en: Terminal commands can be executed either from your system terminal or from within
    the Jupyter environment, the notebook or JupyterLab, by prefixing an exclamation
    mark.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a command from within the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Can be written within the Jupyter environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The exclamation mark from within Jupyter tells the interpreter that this is
    a shell command. In recent versions of Jupyter, the exclamation mark is not necessary
    anymore with the pip command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at how a simple session of starting Python and installing
    a library could appear on the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![terminal.png](img/B17577_01_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13: Terminal window'
  prefs: []
  type: TYPE_NORMAL
- en: 'pip is a package manager for Python libraries. Here are some useful commands
    from your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can install different versions of Python and pip and different versions
    of libraries. These can be maintained as environments that you can switch between.
    Virtualenv is a tool to maintain environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `activate` command will change your `$PATH` variable to point to the `virtualenv bin/` directory,
    which contains versions of Python and pip executables, among other things. This
    means you have all of those available to use as options. You should usually see
    the prompt reflect this change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that for the activation of the environment, you can use a complete
    or relative path. In Windows, the activation command is slightly different – you''d
    run a shell script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Jupyter Notebook and JupyterLab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jupyter stands for Julia, Python, R. It's a platform to run scripts in these
    and other supported languages, such as Scala and C, in an interactive environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start up a notebook server on your computer from the terminal like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see your browser opening a new tab with the Jupyter notebook. The
    beginning of my notebook for loading the data science language time-series looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 3)/Screenshot 2021-03-11 at 17.29.18.png](img/B17577_01_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14: Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can also use JupyterLab, the next-generation notebook server
    that brings significant improvements in usability.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start up a JupyterLab notebook server from the terminal like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'JupyerLab looks a bit different from the default Jupyter server, as you can
    see in the screenshot below (from the JupyterLab GitHub repo):'
  prefs: []
  type: TYPE_NORMAL
- en: '![i_glow_up](img/B17577_01_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.15: JupyterLab'
  prefs: []
  type: TYPE_NORMAL
- en: Either one of these two, the Jupyter notebook or JupyterLab, will give you an
    **integrated development environment** (**IDE**) to work on the code that we'll
    be introducing in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it''s very handy to know how to get help from within Jupyter. This
    is where the question mark comes in. The question mark, ?, is used to provide
    in-notebook help like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_jyFfFP/Screenshot
    2021-06-26 at 22.06.11.png](img/B17577_01_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.16: In-notebook help'
  prefs: []
  type: TYPE_NORMAL
- en: You can also use single or double question marks at the end of a function if
    you want to access the signature or the complete source code listing of the function.
    This functionality can save a lot of time – instead of searching Google for the
    code or the definition of classes or functions, you can get to the information
    in milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NumPy is a foundational library for scientific computing in Python because so
    many libraries depend on it. Libraries such as PyTorch and TensorFlow provide
    an interface with NumPy so that data import/export is a breeze. pandas is basically
    a high-level interface around NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: SciPy also builds on top of NumPy. SciPy stands for *scientific Python* and
    contains functionality ranging from mathematical constants to integration, optimization,
    interpolation, signal processing, and more.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy allows you to work with matrices in different dimensions and perform computations
    on them. You might work mainly with pandas or other libraries and never come much
    in contact with NumPy; however, for a deeper understanding and for high performance,
    it's definitely important to know NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few basic commands in NumPy are below. This is supposed to be executed in
    the Python interpreter. We''ll create one-dimensional and two-dimensional arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy has very handy functions for documentation; for example, to retrieve
    the documentation for the `optimize.fmin` function, use this (I''ve omitted a
    few lines for conciseness):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: pandas is a library that allows accessing matrices or arrays as tables, by indexes
    such as column names – this is called a **DataFrame**. A single column or single
    row can be accessed as a series, another datatype in pandas. These series are
    NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pandas library includes functions and classes for importing and exporting
    data from/to CSV, Excel, and many other formats; for selecting and slicing data;
    and for merge, join, groupby, and aggregation functions reminiscent of Structured
    Query Language (SQL). You can also plot directly from pandas, because pandas is
    integrated with matplotlib, but it also works with other plotting libraries such
    as bokeh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the last command should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Best practice in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, I want to talk about good coding. Whole books have been written
    about this, and this one section cannot do justice to this matter; however, I
    aim to at least give some essentials and pointers. For coding beyond the beginner's
    level and within a corporate environment, or any organization for that matter,
    good practice takes on importance.
  prefs: []
  type: TYPE_NORMAL
- en: It's not easy and takes experience to write generalizable code that lends itself
    to maintenance and enhancements. Only code that expresses ideas in a way that
    is readable to other people will be useful for a team. One of the most important
    principles is DRY (don't repeat yourself), where repetition is reduced and each
    functionality finds its unique representation within the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not a full list, but some other principles include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependency management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing (in particular, unit testing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these have entire books written about them, and it's outside our scope
    here to go into detail. Each of these is crucially important once you gear up
    to production so your code can be relied on.
  prefs: []
  type: TYPE_NORMAL
- en: It still happens to me that I feel like an idiot whenever I return to one of
    my projects, be it in my job or private life, and realize I didn't write enough
    documentation. When that happens, I have to expend energy rebuilding the correct
    mental representation of my code. If done properly, writing documentation can
    help you in your flow. Other people reading your code, and your future self in
    a few months, will be glad you wrote documentation, especially for functions,
    classes, and modules (docstrings).
  prefs: []
  type: TYPE_NORMAL
- en: Encapsulation of dependencies means that your code is isolated, portable, and
    reproducible. Two main tools have emerged during the last few years for the management
    of dependencies and environments in Python, conda and pip, which we've mentioned
    in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: A mishmash of styles and conventions render any project a mess that's not only
    hard to read but hard to maintain. One of the most important coding styles for
    Python is Python Enhancement Proposal 8, or PEP 8 for short. You can find the
    style guide for PEP 8 at [http://bit.ly/3evsgIW](http://bit.ly/3evsgIW).
  prefs: []
  type: TYPE_NORMAL
- en: 'A few tools have been developed to check Python code for adherence to PEP 8
    (and a few additional conventions). These tools can help you make your code more
    legible and maintainable while saving time and mental energy; for example, Flake8,
    Black, mypy, or Pylint. Flake8 and Pylint not only check for coding style but
    also for common coding mistakes and potential bugs. If you want to run a Flake8
    test on a Python script, you can type this, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Black can nag you about formatting or automatically fix formats in a file, module,
    or even a whole project. pydocstyle checks for the existence of documentation
    and the compliance of the documentation with documentation style guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: Further, more in-depth development and coding styles have been created by developers
    from several high-level projects and can be very instructive. The guide for the
    scikit-learn project can be found at [http://bitly.com/3etFrtz](http://bitly.com/3etFrtz).
    For pandas, you can compare the styles at [http://bit.ly/2OlpCKZ](http://bit.ly/2OlpCKZ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit testing is a method by which you set up tests for modules, classes, and
    other units of code. One of the most popular libraries for unit testing in Python
    is pytest. You can find out more about pytest in the pytest documentation: [https://docs.pytest.org/en/stable/](https://docs.pytest.org/en/stable/)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've introduced time-series, the history of research into
    time-series, and Python for time-series.
  prefs: []
  type: TYPE_NORMAL
- en: We started with a definition of time-series and its main properties. We then
    looked at the history of the study of time-series in different scientific disciplines,
    such as demography and genetics, astronomy, economics, meteorology, medicine,
    and applied statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we went over the capabilities of Python for time-series and why Python
    is the go-to language for doing machine learning with time-series. Finally, I
    described how to install and use Python for time-series analysis and machine learning,
    and we covered some of the basics of Python as relevant to time-series and machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at time-series analysis with Python.
  prefs: []
  type: TYPE_NORMAL

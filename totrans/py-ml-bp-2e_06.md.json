["```py\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline \n\ndfc = pd.read_json('viral_dataset.json') \ndfc.reset_index(drop=True, inplace=True) \ndfc \n```", "```py\ndfc.columns \n```", "```py\ndef get_word_count(x): \n    if not x is None: \n        return len(x.split(' ')) \n    else: \n        return None \n\ndfc['word_count'] = dfc['text'].map(get_word_count) \ndfc \n```", "```py\nimport matplotlib.colors as mpc \n\ndef get_rgb(x): \n    try: \n        if x.get('images'): \n            main_color = x.get('images')[0].get('colors')[0].get('color') \n            return main_color \n    except: \n        return None \n\ndef get_hex(x): \n    try: \n        if x.get('images'): \n            main_color = x.get('images')[0].get('colors')[0].get('color') \n            return mpc.rgb2hex([(x/255) for x in main_color]) \n    except: \n        return None \n dfc['main_hex'] = dfc['json_data'].map(get_hex) \ndfc['main_rgb'] = dfc['json_data'].map(get_rgb) \n\ndfc \n```", "```py\ndfc['img_count'].value_counts().to_frame('count') \n```", "```py\nfig, ax = plt.subplots(figsize=(8,6)) \ny = dfc['img_count'].value_counts().sort_index() \nx = y.sort_index().index \nplt.bar(x, y, color='k', align='center') \nplt.title('Image Count Frequency', fontsize=16, y=1.01) \nax.set_xlim(-.5,5.5) \nax.set_ylabel('Count') \nax.set_xlabel('Number of Images') \n```", "```py\nmci = dfc['main_hex'].value_counts().to_frame('count') \n\nmci \n```", "```py\nmci['color'] = ' ' \n\ndef color_cells(x): \n    return 'background-color: ' + x.index \n\nmci.style.apply(color_cells, subset=['color'], axis=0) \n\nmci \n```", "```py\ndef get_csplit(x): \n    try: \n        return x[0], x[1], x[2] \n    except: \n        return None, None, None \n\ndfc['reds'], dfc['greens'], dfc['blues'] = zip(*dfc['main_rgb'].map(get_csplit)) \n```", "```py\nfrom sklearn.cluster import KMeans \n\nclf = KMeans(n_clusters=16) \nclf.fit(dfc[['reds', 'greens', 'blues']].dropna()) \n\nclusters = pd.DataFrame(clf.cluster_centers_, columns=['r', 'g', 'b']) \n\nclusters \n```", "```py\ndef hexify(x): \n    rgb = [round(x['r']), round(x['g']), round(x['b'])] \n    hxc = mpc.rgb2hex([(x/255) for x in rgb]) \n    return hxc \n\nclusters.index = clusters.apply(hexify, axis=1) \n\nclusters['color'] = ' ' \n\nclusters.style.apply(color_cells, subset=['color'], axis=0) \n```", "```py\nfrom nltk.util import ngrams \nfrom nltk.corpus import stopwords \nimport re \n\ndef get_word_stats(txt_series, n, rem_stops=False): \n    txt_words = [] \n    txt_len = [] \n    for w in txt_series: \n        if w is not None: \n            if rem_stops == False: \n                word_list = [x for x in ngrams(re.findall('[a-z0-9\\']+', w.lower()), n)] \n            else: \n                word_list = [y for y in ngrams([x for x in re.findall('[a-z0-9\\']+', w.lower())\\ \n                                                if x not in stopwords.words('english')], n)] \n            word_list_len = len(list(word_list)) \n            txt_words.extend(word_list) \n            txt_len.append(word_list_len) \n    return pd.Series(txt_words).value_counts().to_frame('count'), pd.DataFrame(txt_len, columns=['count']) \n```", "```py\nhw,hl = get_word_stats(dfc['title'], 1, 0) \n\nhl \n```", "```py\nhl.describe() \n```", "```py\nhw,hl = get_word_stats(dfc['title'], 2, 0) \n\nhw \n```", "```py\nhw,hl = get_word_stats(dfc['title'], 2, 1) \n\nhw \n```", "```py\nhw,hl = get_word_stats(dfc['title'], 3, 0) \n```", "```py\ndfc['site'].value_counts().to_frame() \n```", "```py\nhw,hl = get_word_stats(dfc['text'], 2, 1) \n\nhw \n```", "```py\nhw,hl = get_word_stats(dfc['text'], 3, 1) \n\nhw \n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor \n\nall_data = dfc.dropna(subset=['img_count', 'word_count']) \nall_data.reset_index(inplace=True, drop=True) \n\ntrain_index = [] \ntest_index = [] \nfor i in all_data.index: \n    result = np.random.choice(2, p=[.65,.35]) \n    if result == 1: \n        test_index.append(i) \n    else: \n        train_index.append(i) \n```", "```py\nprint('test length:', len(test_index), '\\ntrain length:', len(train_index)) \n```", "```py\nsites = pd.get_dummies(all_data['site']) \n\nsites \n```", "```py\ny_train = all_data.iloc[train_index]['fb'].astype(int) \nX_train_nosite = all_data.iloc[train_index][['img_count', 'word_count']] \n\nX_train = pd.merge(X_train_nosite, sites.iloc[train_index], left_index=True, right_index=True) \n\ny_test = all_data.iloc[test_index]['fb'].astype(int) \nX_test_nosite = all_data.iloc[test_index][['img_count', 'word_count']] \n\nX_test = pd.merge(X_test_nosite, sites.iloc[test_index], left_index=True, right_index=True) \n```", "```py\nclf = RandomForestRegressor(n_estimators=1000) \nclf.fit(X_train, y_train) \n```", "```py\ny_actual = y_test \ndeltas = pd.DataFrame(list(zip(y_pred, y_actual, (y_pred - y_actual)/(y_actual))), columns=['predicted', 'actual', 'delta']) \n\ndeltas \n```", "```py\ndeltas['delta'].describe() \n```", "```py\na = pd.Series([10,10,10,10]) \nb = pd.Series([12,8,8,12]) \n\nnp.sqrt(np.mean((b-a)**2))/np.mean(a) \n```", "```py\n(b-a).mean() \n```", "```py\nnp.sqrt(np.mean((y_pred-y_actual)**2))/np.mean(y_actual) \n```", "```py\ndeltas[['predicted','actual']].iloc[:30,:].plot(kind='bar', figsize=(16,8)) \n```", "```py\nall_data.loc[test_index[:30],['title', 'fb']].reset_index(drop=True) \n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer \n\nvect = CountVectorizer(ngram_range=(1,3)) \nX_titles_all = vect.fit_transform(all_data['title']) \n\nX_titles_train = X_titles_all[train_index] \nX_titles_test = X_titles_all[test_index] \n\nX_test = pd.merge(X_test, pd.DataFrame(X_titles_test.toarray(), index=X_test.index), left_index=True, right_index=True) \n\nX_train = pd.merge(X_train, pd.DataFrame(X_titles_train.toarray(), index=X_train.index), left_index=True, right_index=True) \n```", "```py\nclf.fit(X_train, y_train) \n\ny_pred = clf.predict(X_test) \n\ndeltas = pd.DataFrame(list(zip(y_pred, y_actual, (y_pred - y_actual)/(y_actual))), columns=['predicted', 'actual', 'delta']) \n\ndeltas \n```", "```py\nnp.sqrt(np.mean((y_pred-y_actual)**2))/np.mean(y_actual) \n```", "```py\nall_data = all_data.assign(title_wc = all_data['title'].map(lambda x: len(x.split(' ')))) \n\nX_train = pd.merge(X_train, all_data[['title_wc']], left_index=True, right_index=True) \n\nX_test = pd.merge(X_test, all_data[['title_wc']], left_index=True, right_index=True) \n\nclf.fit(X_train, y_train) \n\ny_pred = clf.predict(X_test) \n\nnp.sqrt(np.mean((y_pred-y_actual)**2))/np.mean(y_actual) \n```"]
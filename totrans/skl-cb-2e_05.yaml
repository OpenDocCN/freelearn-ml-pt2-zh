- en: Linear Models – Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from the UCI repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing the Pima Indians diabetes dataset with pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the UCI Pima Indians dataset web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning with logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining logistic regression errors with a confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varying the classification threshold in logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Receiver operating characteristic – ROC analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting an ROC curve without context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together – UCI breast cancer dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is a very old method and part of traditional statistics. *Machine
    learning linear regression* involves a training and testing set. This way, it
    can be compared by utilizing *cross-validation* with other models and algorithms.
    *Traditional linear regression* trains and tests on the whole dataset. This is
    still a common practice, possibly because linear regression tends to underfit
    rather than overfit.
  prefs: []
  type: TYPE_NORMAL
- en: Using linear methods for classification – logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in [Chapter 1](9a5af114-e518-47ef-ac63-edf9ae69384c.xhtml), *High-Performance
    Machine Learning – NumPy*, logistic regression is a classification method. In
    some contexts, it is a regressor as it computes the real number probability of
    a class before assigning a categorical classification prediction. With this in
    mind, let's explore the Pima Indians diabetes dataset provided by the **University
    of California, Irvine** (**UCI**).
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from the UCI repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first dataset we will load is the Pima Indians diabetes dataset. This will
    require access to the internet. The dataset is available thanks to Sigillito V.
    (1990), UCI machine learning repository ([https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data](https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data)),
    Laurel, MD at Johns Hopkins University, applied physics laboratory.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing in your mind if you are an open source veteran is, what is the
    license/permission to this database? This is a very important issue. The UCI repository
    has a use policy that requires citation of the database whenever we are using
    it. We are allowed to use it but we must give them proper credit for their great
    help and provide a citation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to IPython and import `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Type the web location of the Pima Indians diabetes dataset as a string as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Type the column names of the data in a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the feature names as a list. Exclude the `target` column, the last column
    name in `column_names`, because it is not a feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a pandas dataframe to store the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Viewing the Pima Indians diabetes dataset with pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can view the data in various ways. View the top of the dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/85474218-fbe0-41de-9f83-1595cc9f4845.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Nothing seems amiss here, except possibly an insulin level of zero. Is this
    possible? What about the `skin_mm` variable? Can that be zero? Make a note about
    it as a comment in your IPython:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Get a rough overview of the dataframe with the `describe()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/225b593a-a7f2-463b-a074-b2e258260e52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Make a note again in your notebook about additional zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw a histogram of the `pregnancy_x` variable. Set the `hist()` method variable
    bins equal to 50 for more bins and a higher resolution in the image; otherwise,
    the image is hard to read:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3cd07e63-b559-4998-9f18-3b7f3c682de7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Make histograms for all columns of the dataframe. Change `figsize` within the
    method to the tuple `(15,9)` and bins to `50` again; otherwise, it is hard to
    read the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5aaa8355-0f01-49a8-aa4a-df447d30b05c.png)`blood_pressure` and `bmi`
    look like normal distributions aside from the anomalous zeros.'
  prefs: []
  type: TYPE_NORMAL
- en: The `pedigree_func` and `plasma_con` variables are skewed-normal (possibly log-normal).
    The `age` and `pregnancy_x` variables are decaying in some way. The insulin and
    `skin_mm` variables look like they could be normally distributed except for the
    many values of zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, note the class imbalance in the `target` variable. Reexamine that
    imbalance with the `value_counts()` pandas series method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There are more cases where the person is described by category zero instead
    of category one.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the UCI Pima Indians dataset web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We did some exploratory analysis to get a rough understanding of the data. Now
    we will read the UCI Pima Indians dataset documentation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: View the citation policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to [https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is all the information about the UCI Pima Indians diabetes dataset. First,
    scroll down to the bottom of the page and look at their citation policy. The diabetes
    dataset has the general UCI citation policy available at; [https://archive.ics.uci.edu/ml/citation_policy.html](https://archive.ics.uci.edu/ml/citation_policy.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The general policy says that to publish material using the dataset, please cite
    the UCI repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read about missing values and context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The top of the page has important links and an abstract of the dataset. The
    abstract mentions there are missing values in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d6af13eb-bcc7-471d-b8ad-f10be76a4503.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Below the abstract, there is a description of the attributes (this is how I
    came up with the names for the columns at the beginning):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/20fa3278-8e3b-4085-ae7b-0c97cb891847.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What do the class variables mean anyway? What does the zero or one mean in
    the target? To figure this out, click on the Data Set Description link above the
    abstract. Scroll to point nine on the page, which yields the desired information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/67a2dc7c-d858-437a-815d-dcaa29a1f815.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that a 1 refers to a positive for diabetes. This is important information
    and with regard to this data analysis, provides a context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there is a disclaimer noting that: As pointed out by a repository
    user, this cannot be true: there are zeros in places where they are biologically
    impossible, such as the blood pressure attribute. It seems very likely that zero
    values encode missing data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, we were right in suspecting some of the impossible zeros in the data exploration
    phase. Many datasets have corrupt or missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning with logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You are familiar with the steps of training and testing a classifier. With
    logistic regression, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load data into feature and target arrays, `X` and `y`, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the data into training and testing sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the logistic regression classifier on the training set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the performance of the classifier on the test set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Define X, y – the feature and target arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start predicting with scikit-learn''s logistic regression. Perform the
    necessary imports and set the input variables X and the target variable `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Provide training and testing sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import `train_test_split` to create testing and training sets for both `X`
    and `y`: the inputs and target. Note the `stratify=y`, which stratifies the categorical
    variable `y`. This means that there are the same proportions of zeros and ones
    in both `y_train` and `y_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Train the logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now import the `LogisticRegression` and fit it to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a prediction on the test set and store it as `y_pred`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Score the logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check the accuracy of the prediction with `accuracy_score`, the percentage
    of classifications that are correct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: So, we have obtained a score, but is this score the best measure we can use
    under these circumstances? Can we do better? Perhaps yes. Look at a confusion
    matrix of the results as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Examining logistic regression errors with a confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import and view the confusion matrix for the logistic regression we constructed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'I passed three arguments to the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '`y_test`: The test target set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_pred`: Our logistic regression predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`: References to a positive class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `labels = [1,0]` means that the positive class is `1` and the negative class
    is `0`. In the medical context, we found while exploring the Pima Indians diabetes
    dataset that class `1` tested positive for diabetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the confusion matrix, again in pandas dataframe form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e53803a3-fc60-4560-93a4-ac3b7e6cfbad.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reading the confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The small array of numbers has the following meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f6802ef-d273-48e3-8bd3-1503d8a707db.png)'
  prefs: []
  type: TYPE_IMG
- en: The confusion matrix tells us a bit more about what occurred during classification,
    not only the accuracy score. The diagonal elements from upper-left to lower-right
    are correct classifications. There were 27 + 88 = 115 correct classifications.
    Off that diagonal, 27 + 12 = 39 mistakes were made in classification. Note that
    115 / (115 + 39) yields the classifier accuracy again, of about 0.75.
  prefs: []
  type: TYPE_NORMAL
- en: Let us focus on the errors again. In the confusion matrix, 27 people were labelled
    as not having diabetes although they do. In a real-life context, this is a worse
    error than those who were thought to have diabetes but did not. The first set
    might go home in real-life and forget while the second set might be retested.
  prefs: []
  type: TYPE_NORMAL
- en: General confusion matrix in context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A general confusion matrix where the positive class refers to identifying a
    condition (diabetes in this case) thereby having a medical diagnosis context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f51675e3-8550-44cc-a45b-398e68a110ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Varying the classification threshold in logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the fact that underlying the logistic regression classification,
    there is regression to minimize the number of times people were sent home for
    not having diabetes although they do. Do so by calling the `predict_proba()` method
    of the estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields an array of probabilities. View the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the first row, a probability of about 0.87 is assigned to class `0` and a
    probability of 0.13 is assigned to `1`. Note that, as probabilities, these numbers
    add up to 1\. Because there are only two categories, this result can be viewed
    as a regressor, a real number that talks about the probability of the class being
    `1` (or `0`). Visualize the probabilities of the class being `1` with a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the second column of the array, turn it into a pandas series, and draw
    a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/910ab5af-85bb-40e2-8268-f2e3af798570.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the probability histogram, high probabilities in regards to selecting 1
    are fewer than low probabilities. For example, often the probability of selecting
    1 is from 0.1 to 0.2\. Within logistic regression, the algorithm will pick 1 by
    default only if the probability is greater than 0.5 or half. Now, contrast this
    with the target histogram at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5d263257-e2ab-46a9-86e9-bd06f35db1ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following recipe, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Call the class method `y_pred_proba()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `binarize` function with a specific threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look at the confusion matrix that is generated by the threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To select the classification class based on a threshold, use `binarize` from
    the `preprocessing` module. First import it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the first two columns of `y_pred_proba`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then try `binarize` function on `y_pred_proba` with a threshold of `0.5`. View
    the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `binarize` function fills the array with `1` if values in `y_pred_proba`
    are greater than 0.5; otherwise it places a `0`. In the first row, 0.87 is greater
    than 0.5 while 0.13 is not. So to binarize, replace the 0.87 with a `1` and the
    0.13 with a `0`. Now, take the first column of `y_pred_default`. View it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This recovers the decisions made by the default logistic regression classifier
    with threshold `0.5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trying the confusion matrix function on the NumPy array yields the first confusion
    matrix we encountered (note that the labels are chosen to be `[1,0]` again):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Try a different threshold so that class `1` has a better chance of being selected.
    View its confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: By changing the threshold, we increased the likelihood of predicting class `1`—increasing
    the size of the numbers in the first column of the confusion matrix. The first
    column now adds up to 50 + 48 = 98\. Before, the first column was 27 + 12 = 39,
    a much lower number. Now only four people were classified to not have diabetes
    although they do. Note that this is a good thing in some contexts.
  prefs: []
  type: TYPE_NORMAL
- en: When the algorithm predicts zero, it is likely to be correct. When it predicts
    one, it tends to not work. Suppose you run a hospital. You might like this test
    because you rarely send someone home believing they do not have diabetes although
    they do. Whenever you send people home who do have diabetes, they cannot be treated
    earlier and incur greater costs to the hospital, insurance companies, and themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can measure the accuracy of the test when it predicts zero. Observe the
    second column of the confusion matrix, [4, 52]. In this situation, it is 52 /
    (52 + 4) or about 0.93 accurate. This is called the **negative predictive value**
    (**NPV**). You can write a function to calculate NPV-based on the threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then plot it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/be4025da-47f3-4060-9590-8b888bb16644.png)'
  prefs: []
  type: TYPE_IMG
- en: Receiver operating characteristic – ROC analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Along the same lines of examining NPV, there are standard measures that examine
    cells within a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sensitivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sensitivity, like NPV in the previous section, is a mathematical function of
    the confusion matrix cells. Sensitivity is the proportion of people who took the
    test with a condition and were correctly labeled as having the condition, diabetes
    in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29bbe2b5-5c8d-4b78-bc12-bbc736ff415d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Mathematically, it is the ratio of patients correctly labeled as having a condition
    (TP) divided by the total number of people who actually have the condition (TP
    + FN). First, recall the confusion matrix cells. Focus on the **Truth** row, which
    corresponds to *all people who have diabetes*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43d62022-e905-4a75-94aa-8f392ffee870.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consequently:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65760563-8d44-4d1b-8d30-ef81a9bf88f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Another name for sensitivity is **true positive rate** (**TPR**).
  prefs: []
  type: TYPE_NORMAL
- en: A visual perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another perspective on the confusion matrix is very visual. Let us visualize
    both the positive class having diabetes and the negative class having no diabetes
    with histograms (on the left column in the next diagram). Each class roughly looks
    like a normal distribution. With SciPy, I can find the best fit normal distributions.
    Note on the bottom right that the threshold is set to 0.5, the default setting
    in the logistic regression. Observe how the threshold causes us to select, imperfectly,
    **false negatives** (**FN**) and **false positives** (**FP**).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6f1aa07-1abf-470c-b7ac-36d7e76b09e0.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calculating TPR in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn has convenient functions for calculating the sensitivity or TPR
    for the logistic regression given a vector of probabilities of the positive class,
    `y_pred_proba[:,1]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, given the positive class vector, the `roc_curve` function in scikit-learn
    yielded a tuple of three arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: The TPR array (denoted by `tpr`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FPR array (denoted by `fpr`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A custom set of thresholds to calculate TPR and FPR (denoted by `ths`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To elaborate on the **false positive rate** (**FPR**), it describes the rate
    of false alarms. It is the number of people incorrectly thought to have diabetes
    although they do not:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fe1f564-fd47-475b-bf6a-95d64b006750.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is a statement of people who do not have diabetes. Mathematically, with
    the cells of the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a8a70d8-c8ea-4270-a7b2-c61494829ef9.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting sensitivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Plot sensitivity in the *y* axis and the thresholds in the *x* axis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a026c031-1967-402d-81a1-4c8e7d51c7be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the lower the threshold, the better the sensitivity. Looking at the confusion
    matrix for the threshold at `0.1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, no one went home believing they had diabetes when they did not.
    Yet, like our computation with NPV, when the test predicts that someone has diabetes,
    it is very inaccurate. The best scenario of this type is when the threshold is
    `0.146`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Even then, the test does not work when the person is predicted to have diabetes.
    It works 33 / (33 + 121) = 0.21, or 21% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The confusion matrix in a non-medical context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you are a banker and want to determine whether a customer deserves a
    mortgage loan to buy a house. Up next is a possible confusion matrix showing whether
    to give a person a mortgage loan based on customer data available to the bank.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task is to classify people and determine whether they should receive a
    mortgage loan or not. In this context, numbers can be assigned to every scenario.
    When every cell in the confusion matrix has a clear cost, it is easier to find
    the best classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc33a826-1ae3-4fcd-b6f0-fe46c11304ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting an ROC curve without context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An ROC curve is a diagnostic tool for any classifier without any context. No
    context means that we do not know yet which error type (FP or FN) is less desirable
    yet. Let us plot it right away using a vector of probabilities, `y_pred_proba[:,1]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/aad4b839-f44c-4922-ac95-c0edb0c51ab1.png)'
  prefs: []
  type: TYPE_IMG
- en: The ROC is a plot of the FPR (false alarms) in the *x* axis and TPR (finding
    everyone with the condition who really has it) in the *y* axis. Without context,
    it is a tool to measure classifier performance.
  prefs: []
  type: TYPE_NORMAL
- en: Perfect classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A perfect classifier would have a TPR of 1 regardless of the **false alarm
    rate** (**FAR**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f83c56a-c8ed-4bc7-b11b-5897cb3d9be6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding graph, FN is very small; so the TPR, TP / (TP + FN), is close
    to 1\. Its ROC curve has an L-shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f7fffa4-8f06-451e-a5a7-0b59e9b1a72f.png)'
  prefs: []
  type: TYPE_IMG
- en: Imperfect classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following images, the distributions overlap and the categories cannot
    be distinguished from one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/843b64d1-18f8-4b71-b71a-9056f62473bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the imperfect classifier, FN and TN are nearly equal and so are FP and TP.
    Thus, by substitution, the TPR TP/ (TP + FP) is nearly equal to the **false negative
    rate** (**FNR**) FP/ (FP + TN). This is true even if you vary the threshold. Consequently,
    we obtain an ROC curve that is a straight line with a slope of about 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1a7e274-4ccc-4c4d-aa07-4fed11a65f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: AUC – the area under the ROC curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The area of the L-shaped perfect classifier is 1 x 1 = 1\. The area of the
    bad classifier is 0.5\. To measure classifier performance, scikit-learn has a
    handy **area under the ROC curve** (**AUC**) calculating function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Putting it all together – UCI breast cancer dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset is provided thanks to Street, N (1990), UCI machine learning repository
    ([https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)),
    Madison, WI: University of Wisconsin, computer sciences department:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After reading the citation/license information, load the dataset from UCI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the data types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: It turns out that the feature compactness has characters like `?`. For now,
    we do not use this feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, reading the documentation, the target variable is set to `2` (not having
    cancer) and `4` (having cancer). Change the variables to `0` for not having cancer
    and `1` for having cancer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Define `X` and `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Split `X` and `y` into training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw the ROC curve and compute the AUC score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d754055f-d6fa-4430-acbc-b5c4eaaa6831.png)'
  prefs: []
  type: TYPE_IMG
- en: This classifier performs fairly well.
  prefs: []
  type: TYPE_NORMAL
- en: Outline for future projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Overall, for future classification projects, you can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the best data you can find for a particular problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Determine whether there is a classification context: is FP better or worse
    than FN?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform training and testing of the data without context using ROC-AUC scores.
    If several algorithms perform poorly at this step, you might want to go back to
    step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the context is important, explore it with confusion matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Logistic regression is particularly well suited for all these steps, though
    any algorithm with a `predict_proba()` method will work very similarly. As an
    exercise, you can generalize this process for other algorithms or even general
    regression if you are ambitious. The main point here is that not all errors are
    the same, a point easily emphasized with health datasets, wherein it is very important
    to treat all patients that have a condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'A final note on the breast cancer dataset: observe that the data consists of
    cell measurements. You can gather these measurements by automating looking at
    the pictures with computers and finding the measurements with a traditional program
    or machine learning.'
  prefs: []
  type: TYPE_NORMAL

["```py\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\nX = iris.data\ny = iris.target\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()     #Instantiate tree class\ndtc.fit(X_train, y_train)\n```", "```py\nfrom sklearn.metrics import accuracy_score\n\ny_pred = dtc.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.91111111111111109\n```", "```py\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.externals.six import StringIO\nimport pydot\nfrom IPython.display import Image\n\ndot_iris = StringIO() \ntree.export_graphviz(dtc, out_file = dot_iris, feature_names = iris.feature_names) \ngraph = pydot.graph_from_dot_data(dot_iris.getvalue())\nImage(graph.create_png())\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(criterion='entropy')\ndtc.fit(X_train, y_train)\n```", "```py\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data[:,:2]\ny = iris.target\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n```", "```py\nimport pandas as pd\npd.DataFrame(X,columns=iris.feature_names[:2])\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()     #Instantiate tree with default parameters\ndtc.fit(X_train, y_train)\n```", "```py\nfrom sklearn.metrics import accuracy_score\n\ny_pred = dtc.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n0.66666666666666663\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=((12,6)))\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\n\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n```", "```py\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nparam_grid = {'criterion':['gini','entropy'], 'max_depth' : [3,5,7,20]}\n\ngs_inst = GridSearchCV(dtc,param_grid=param_grid,cv=5)\ngs_inst.fit(X_train, y_train)\n```", "```py\nfrom sklearn.metrics import accuracy_score\n\ny_pred_gs = gs_inst.predict(X_test)\naccuracy_score(y_test, y_pred_gs)\n\n0.68888888888888888\n```", "```py\ngs_inst.grid_scores_\n\n[mean: 0.78095, std: 0.09331, params: {'criterion': 'gini', 'max_depth': 3},\n mean: 0.68571, std: 0.08832, params: {'criterion': 'gini', 'max_depth': 5},\n mean: 0.70476, std: 0.08193, params: {'criterion': 'gini', 'max_depth': 7},\n mean: 0.66667, std: 0.09035, params: {'criterion': 'gini', 'max_depth': 20},\n mean: 0.78095, std: 0.09331, params: {'criterion': 'entropy', 'max_depth': 3},\n mean: 0.69524, std: 0.11508, params: {'criterion': 'entropy', 'max_depth': 5},\n mean: 0.72381, std: 0.09712, params: {'criterion': 'entropy', 'max_depth': 7},\n mean: 0.67619, std: 0.09712, params: {'criterion': 'entropy', 'max_depth': 20}]\n```", "```py\ngs_inst.best_estimator_\n\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n max_features=None, max_leaf_nodes=None,\n min_impurity_split=1e-07, min_samples_leaf=1,\n min_samples_split=2, min_weight_fraction_leaf=0.0,\n presort=False, random_state=None, splitter='best')\n```", "```py\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.externals.six import StringIO\n\nimport pydot\nfrom IPython.display import Image\n\ndot_iris = StringIO()\ntree.export_graphviz(gs_inst.best_estimator_, out_file = dot_iris, feature_names = iris.feature_names[:2])\ngraph = pydot.graph_from_dot_data(dot_iris.getvalue())\n\nImage(graph.create_png())\n```", "```py\ngrid_interval = 0.02\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nxmin, xmax = np.percentile(X[:, 0], [0, 100])\nymin, ymax = np.percentile(X[:, 1], [0, 100])\n\nxmin_plot, xmax_plot = xmin - .5, xmax + .5\nymin_plot, ymax_plot = ymin - .5, ymax + .5\n\nxx, yy = np.meshgrid(np.arange(xmin_plot, xmax_plot, grid_interval),\nnp.arange(ymin_plot, ymax_plot, grid_interval))\n```", "```py\ntest_preds = gs_inst.best_estimator_.predict(np.array(zip(xx.ravel(), yy.ravel())))\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nX_0 = X[y == 0]\nX_1 = X[y == 1]\nX_2 = X[y == 2]\n\nplt.figure(figsize=(15,8)) #change figure-size for easier viewing\nplt.scatter(X_0[:,0],X_0[:,1], color = 'red')\nplt.scatter(X_1[:,0],X_1[:,1], color = 'blue')\nplt.scatter(X_2[:,0],X_2[:,1], color = 'green')\n\ncolors = np.array(['r', 'b','g'])\nplt.scatter(xx.ravel(), yy.ravel(), color=colors[test_preds], alpha=0.15)\nplt.scatter(X[:, 0], X[:, 1], color=colors[y])\nplt.title(\"Decision Tree Visualization\")\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\n```", "```py\nplt.axvline(x = 5.45, color='black')\nplt.axvline(x = 6.2, color='black')\nplt.plot((xmin_plot, 5.45), (2.8, 2.8), color='black')\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nmax_depths = range(2,51)\nparam_grid = {'max_depth' : max_depths}\n\ngs_inst = GridSearchCV(dtc, param_grid=param_grid,cv=5)\ngs_inst.fit(X_train, y_train)\n\nplt.plot(max_depths,gs_inst.cv_results_['mean_test_score'])\nplt.xlabel('Max Depth')\nplt.ylabel(\"Cross-validation Score\")\n```", "```py\n#Use within an Jupyter notebook\n%matplotlib inline \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nX_feature_names = ['age', 'gender', 'body mass index', 'average blood pressure','bl_0','bl_1','bl_2','bl_3','bl_4','bl_5']\n```", "```py\npd.Series(y).hist(bins=50)\n```", "```py\nbins = 50*np.arange(8)\nbins\n\narray([ 0, 50, 100, 150, 200, 250, 300, 350])\n```", "```py\nbinned_y = np.digitize(y, bins)\n```", "```py\npd.Series(binned_y).hist(bins=50)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=binned_y)\n```", "```py\nfrom sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\n```", "```py\ny_pred = dtr.predict(X_test)\n```", "```py\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_pred)\n\n58.49438202247191\n```", "```py\n(np.abs(y_test - y_pred)/(y_test)).mean()\n\n0.4665997687095611\n```", "```py\npd.Series((y_test - y_pred)).hist(bins=50)\n```", "```py\npd.Series((y_test - y_pred)/(y_test)).hist(bins=50)\n```", "```py\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nX_feature_names = ['age', 'gender', 'body mass index', 'average blood pressure','bl_0','bl_1','bl_2','bl_3','bl_4','bl_5']\n\nbins = 50*np.arange(8)\nbinned_y = np.digitize(y, bins)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=binned_y)\n```", "```py\nfrom sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor()\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n\ngs_inst = GridSearchCV(dtr, param_grid = {'max_depth': [3,5,7,9,20]},cv=10)\ngs_inst.fit(X_train, y_train)\n```", "```py\ngs_inst.best_estimator_\n\nDecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n max_leaf_nodes=None, min_impurity_split=1e-07,\n min_samples_leaf=1, min_samples_split=2,\n min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n splitter='best')\n```", "```py\ny_pred = gs_inst.predict(X_test)\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_pred)\n\n54.299263338774338\n```", "```py\n(np.abs(y_test - y_pred)/(y_test)).mean()\n\n0.4672742120960478\n```", "```py\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.externals.six import StringIO\n\nimport pydot\nfrom IPython.display import Image\n\ndot_diabetes = StringIO()\ntree.export_graphviz(gs_inst.best_estimator_, out_file = dot_diabetes, feature_names = X_feature_names)\ngraph = pydot.graph_from_dot_data(dot_diabetes.getvalue())\n\nImage(graph.create_png())\n```", "```py\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nX_feature_names = ['age', 'gender', 'body mass index', 'average blood pressure','bl_0','bl_1','bl_2','bl_3','bl_4','bl_5']\n\n#bin target variable for better sampling\nbins = 50*np.arange(8)\nbinned_y = np.digitize(y, bins)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=binned_y)\n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\n\nrft = RandomForestRegressor()\nrft.fit(X_train, y_train)\n```", "```py\ny_pred = rft.predict(X_test)\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, y_pred)\n\n48.539325842696627\n\n(np.abs(y_test - y_pred)/(y_test)).mean()\n\n0.42821508503434541\n```", "```py\nrft.estimators_\n\n[DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n max_leaf_nodes=None, min_impurity_split=1e-07,\n min_samples_leaf=1, min_samples_split=2,\n min_weight_fraction_leaf=0.0, presort=False,\n random_state=492413116, splitter='best')\n...\n```", "```py\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.externals.six import StringIO\n\nimport pydot\nfrom IPython.display import Image\n\ndot_diabetes = StringIO()\ntree.export_graphviz(rft.estimators_[0], out_file = dot_diabetes, feature_names = X_feature_names)\ngraph = pydot.graph_from_dot_data(dot_diabetes.getvalue())\n\nImage(graph.create_png()) \n```", "```py\nrft.feature_importances_\n\narray([ 0.06103037, 0.00969354, 0.34865274, 0.09091215, 0.04331388,\n\n 0.04376602, 0.04827391, 0.02430837, 0.23251334, 0.09753567])\n```", "```py\nfig, ax = plt.subplots(figsize=(10,5))\n\nbar_rects = ax.bar(np.arange(10), rft.feature_importances_,color='r',align='center')\nax.xaxis.set_ticks(np.arange(10))\nax.set_xticklabels(X_feature_names, rotation='vertical')\n```", "```py\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_diabetes\ndiabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nX_feature_names = ['age', 'gender', 'body mass index', 'average blood pressure','bl_0','bl_1','bl_2','bl_3','bl_4','bl_5']\n\n#bin target variable for better sampling\nbins = 50*np.arange(8)\nbinned_y = np.digitize(y, bins)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=binned_y)\n```", "```py\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n```", "```py\nparam_dist = {\n 'max_samples': [0.5,1.0],\n 'max_features' : [0.5,1.0],\n 'oob_score' : [True, False],\n 'base_estimator__n_neighbors': [3,5],\n 'n_estimators': [100]\n }\n```", "```py\nsingle_estimator = KNeighborsRegressor()\nensemble_estimator = BaggingRegressor(base_estimator = single_estimator)\n```", "```py\npre_gs_inst_bag = RandomizedSearchCV(ensemble_estimator,\n param_distributions = param_dist,\n cv=3,\n n_iter = 5,\n n_jobs=-1)\n\npre_gs_inst_bag.fit(X_train, y_train)\n```", "```py\npre_gs_inst_bag.best_params_\n\n{'base_estimator__n_neighbors': 5,\n 'max_features': 1.0,\n 'max_samples': 0.5,\n 'n_estimators': 100,\n 'oob_score': True}\n```", "```py\nrs_bag = BaggingRegressor(**{'max_features': 1.0,\n 'max_samples': 0.5,\n 'n_estimators': 1000,\n 'oob_score': True,\n 'base_estimator': KNeighborsRegressor(n_neighbors=5)})\n\nrs_bag.fit(X_train, y_train)\n```", "```py\ny_pred = rs_bag.predict(X_test)\n\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint \"R-squared\",r2_score(y_test, y_pred)\nprint \"MAE : \",mean_absolute_error(y_test, y_pred)\nprint \"MAPE : \",(np.abs(y_test - y_pred)/y_test).mean()\n\nR-squared 0.498096653258\nMAE :  44.3642741573\nMAPE :  0.419361955306\n```", "```py\n%matplotlib inline \n\nfrom __future__ import division #Load within Python 2.7 for regular division\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\n\ncali_housing = fetch_california_housing()\n\nX = cali_housing.data\ny = cali_housing.target\n\n#bin output variable to split training and testing sets into two similar sets\nbins = np.arange(6)\nbinned_y = np.digitize(y, bins)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=binned_y)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n```", "```py\nparam_dist = {'max_features' : ['log2',1.0],\n 'max_depth' : [3, 5, 7, 10],\n 'min_samples_leaf' : [2, 3, 5, 10],\n 'n_estimators': [50, 100],\n 'learning_rate' : [0.0001,0.001,0.01,0.05,0.1,0.3],\n 'loss' : ['ls','huber']\n }\n```", "```py\npre_gs_inst = RandomizedSearchCV(GradientBoostingRegressor(warm_start=True),\n param_distributions = param_dist,\n cv=3,\n n_iter = 30, n_jobs=-1)\npre_gs_inst.fit(X_train, y_train)\n```", "```py\nimport numpy as np\nimport pandas as pd\n\ndef get_grid_df(fitted_gs_estimator):\n res_dict = fitted_gs_estimator.cv_results_\n\n results_df = pd.DataFrame()\n for key in res_dict.keys():\n results_df[key] = res_dict[key]\n\n return results_df\n\ndef group_report(results_df):\n param_cols = [x for x in results_df.columns if 'param' in x and x is not 'params']\n focus_cols = param_cols + ['mean_test_score']\n\n print \"Grid CV Report \\n\"\n\n output_df = pd.DataFrame(columns = ['param_type','param_set',\n 'mean_score','mean_std'])\n cc = 0\n for param in param_cols:\n for key,group in results_df.groupby(param):\n output_df.loc[cc] = (param, key, group['mean_test_score'].mean(), group['mean_test_score'].std())\n cc += 1\n return output_df\n```", "```py\nresults_df = get_grid_df(pre_gs_inst)\ngroup_report(results_df)\n```", "```py\nparam_dist = {'max_features' : ['sqrt',0.5,1.0],\n 'max_depth' : [2,3,4],\n 'min_samples_leaf' : [3, 4],\n 'n_estimators': [50, 100],\n 'learning_rate' : [0.2,0.25, 0.3, 0.4],\n 'loss' : ['ls','huber']\n }\n pre_gs_inst = RandomizedSearchCV(GradientBoostingRegressor(warm_start=True),\n param_distributions = param_dist,\n cv=3,\n n_iter = 30, n_jobs=-1)\n pre_gs_inst.fit(X_train, y_train)\n```", "```py\nresults_df = get_grid_df(pre_gs_inst)\ngroup_report(results_df)\n```", "```py\nparam_dist = {'max_features' : [0.4, 0.5, 0.6],\n 'max_depth' : [5,6],\n 'min_samples_leaf' : [4,5],\n 'n_estimators': [300],\n 'learning_rate' : [0.3],\n 'loss' : ['ls','huber']\n }\n```", "```py\nrs_gbt = GradientBoostingRegressor(warm_start=True,\n max_features = 0.5,\n min_samples_leaf = 4,\n learning_rate=0.3,\n max_depth = 6,\n n_estimators = 4000,loss = 'huber')\n\nrs_gbt.fit(X_train, y_train)\n```", "```py\ny_pred = rs_gbt.predict(X_test)\n\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint \"R-squared\",r2_score(y_test, y_pred)\nprint \"MAE : \",mean_absolute_error(y_test, y_pred)\nprint \"MAPE : \",(np.abs(y_test - y_pred)/y_test).mean()\n\nR-squared 0.84490423214\nMAE : 0.302125381378\nMAPE : 0.169831775387\n```", "```py\npd.Series(y).hist(bins=50)\n```", "```py\nbins = np.arange(6)\nbinned_y = np.digitize(y, bins)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=binned_y)\n```", "```py\ny_binary = np.where(y >= 5, 1,0)\n```", "```py\ntrain_shape = X_train.shape[0]\n\ny_train_binned = y_binary[:train_shape]\ny_test_binned = y_binary[train_shape:]\n```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_dist = {'max_features' : ['log2',0.5,1.0],\n 'max_depth' : [2,3,6],\n 'min_samples_leaf' : [1,2,3,10],\n 'n_estimators': [100],\n 'learning_rate' : [0.1,0.2,0.3,1],\n 'loss' : ['deviance']\n }\npre_gs_inst = RandomizedSearchCV(GradientBoostingClassifier(warm_start=True),\n param_distributions = param_dist,\n cv=3,\n n_iter = 10, n_jobs=-1)\n\npre_gs_inst.fit(X_train, y_train_binned)\n```", "```py\npre_gs_inst.best_params_\n\n{'learning_rate': 0.2,\n 'loss': 'deviance',\n 'max_depth': 2,\n 'max_features': 1.0,\n 'min_samples_leaf': 2,\n 'n_estimators': 50}\n```", "```py\ngbc = GradientBoostingClassifier(**{'learning_rate': 0.2,\n 'loss': 'deviance',\n 'max_depth': 2,\n 'max_features': 1.0,\n 'min_samples_leaf': 2,\n 'n_estimators': 1000, 'warm_start':True}).fit(X_train, y_train_binned)\n```", "```py\ny_pred = gbc.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test_binned, y_pred)\n\n0.93580426356589153\n```", "```py\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n 'n_estimators': [50, 100],\n 'learning_rate' : [0.01,0.05,0.1,0.3,1],\n 'loss' : ['linear', 'square', 'exponential']\n }\n\npre_gs_inst = RandomizedSearchCV(AdaBoostRegressor(),\n param_distributions = param_dist,\n cv=3,\n n_iter = 10,\n n_jobs=-1)\n\npre_gs_inst.fit(X_train, y_train)\n```", "```py\npre_gs_inst.best_params_\n\n{'learning_rate': 0.05, 'loss': 'linear', 'n_estimators': 100}\n```", "```py\nparam_dist = {\n 'n_estimators': [100],\n 'learning_rate' : [0.04,0.045,0.05,0.055,0.06],\n 'loss' : ['linear']\n }\n```", "```py\nimport copy\nada_best = copy.deepcopy(pre_gs_inst.best_params_)\nada_best['n_estimators'] = 3000\n```", "```py\nrs_ada = AdaBoostRegressor(**ada_best)\nrs_ada.fit(X_train, y_train)\n```", "```py\ny_pred = rs_ada.predict(X_test)\n\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint \"R-squared\",r2_score(y_test, y_pred)\nprint \"MAE : \",mean_absolute_error(y_test, y_pred)\nprint \"MAPE : \",(np.abs(y_test - y_pred)/y_test).mean()\n\nR-squared 0.485619387823\nMAE : 0.708716094846\nMAPE : 0.524923208329\n```", "```py\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\n\ncali_housing = fetch_california_housing()\n\nX = cali_housing.data\ny = cali_housing.target\n\nbins = np.arange(6)\n\nfrom __future__ import division\n\nfrom sklearn.model_selection import train_test_split\n\nbinned_y = np.digitize(y, bins)\n\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n\nfrom sklearn.model_selection import GridSearchCV\n```", "```py\nX_train_prin, X_test_prin, y_train_prin, y_test_prin = train_test_split(X, y,\n test_size=0.2,\n stratify=binned_y)\n\nbinned_y_train_prin = np.digitize(y_train_prin, bins)\n\nX_1, X_stack, y_1, y_stack = train_test_split(X_train_prin, \n y_train_prin,\n test_size=0.33,\n stratify=binned_y_train_prin )\n```", "```py\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n 'max_samples': [0.5,1.0],\n 'max_features' : [0.5,1.0],\n 'oob_score' : [True, False],\n 'base_estimator__n_neighbors': [3,5],\n 'n_estimators': [100]\n }\n\nsingle_estimator = KNeighborsRegressor()\nensemble_estimator = BaggingRegressor(base_estimator = single_estimator)\n\npre_gs_inst_bag = RandomizedSearchCV(ensemble_estimator,\n param_distributions = param_dist,\n cv=3,\n n_iter = 5,\n n_jobs=-1)\n\npre_gs_inst_bag.fit(X_1, y_1)\n```", "```py\nrs_bag = BaggingRegressor(**{'max_features': 0.5,\n 'max_samples': 0.5,\n 'n_estimators': 3000,\n 'oob_score': False, \n 'base_estimator': KNeighborsRegressor(n_neighbors=3)})\n\nrs_bag.fit(X_1, y_1)\n```", "```py\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {'max_features' : ['log2',0.4,0.5,0.6,1.0],\n 'max_depth' : [2,3, 4, 5,6, 7, 10],\n 'min_samples_leaf' : [1,2, 3, 4, 5, 10],\n 'n_estimators': [50, 100],\n 'learning_rate' : [0.01,0.05,0.1,0.25,0.275,0.3,0.325],\n 'loss' : ['ls','huber']\n }\npre_gs_inst = RandomizedSearchCV(GradientBoostingRegressor(warm_start=True),\nparam_distributions = param_dist,\n cv=3,\n n_iter = 30, n_jobs=-1)\n pre_gs_inst.fit(X_1, y_1)\n```", "```py\ngbt_inst = GradientBoostingRegressor(**{'learning_rate': 0.05,\n 'loss': 'huber',\n 'max_depth': 10,\n 'max_features': 0.4,\n 'min_samples_leaf': 5,\n 'n_estimators': 3000,\n 'warm_start': True}).fit(X_1, y_1)\n```", "```py\ny_pred_bag = rs_bag.predict(X_stack)\ny_pred_gbt = gbt_inst.predict(X_stack)\n```", "```py\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint \"R-squared\",r2_score(y_stack, y_pred_bag)\nprint \"MAE : \",mean_absolute_error(y_stack, y_pred_bag)\nprint \"MAPE : \",(np.abs(y_stack- y_pred_bag)/y_stack).mean()\n\nR-squared 0.527045729567\nMAE : 0.605868386902\nMAPE : 0.397345752723\n```", "```py\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint \"R-squared\",r2_score(y_stack, y_pred_gbt)\nprint \"MAE : \",mean_absolute_error(y_stack, y_pred_gbt)\nprint \"MAPE : \",(np.abs(y_stack - y_pred_gbt)/y_stack).mean()\n\nR-squared 0.841011059404\nMAE : 0.297099247278\nMAPE : 0.163956322255\n```", "```py\ny_pred_bag = rs_bag.predict(X_stack)\ny_pred_gbt = gbt_inst.predict(X_stack)\n\npreds_df = pd.DataFrame(columns = ['bag', 'gbt'])\n\npreds_df['bag'] = y_pred_bag\npreds_df['gbt'] = y_pred_gbt\n```", "```py\npreds_df\n```", "```py\npreds_df.corr()\n```", "```py\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {'max_features' : ['sqrt','log2',1.0],\n 'min_samples_leaf' : [1, 2, 3, 7, 11],\n 'n_estimators': [50, 100],\n 'oob_score': [True, False]}\n\npre_gs_inst = RandomizedSearchCV(ExtraTreesRegressor(warm_start=True,bootstrap=True),\n param_distributions = param_dist,\n cv=3,\n n_iter = 15)\n\npre_gs_inst.fit(preds_df.values, y_stack)\n```", "```py\nimport copy\n\n param_dict = copy.deepcopy(pre_gs_inst.best_params_)\n\n param_dict['n_estimators'] = 2000\n param_dict['warm_start'] = True\n param_dict['bootstrap'] = True\n param_dict['n_jobs'] = -1\n\n param_dict\n\n{'bootstrap': True,\n 'max_features': 1.0,\n 'min_samples_leaf': 11,\n 'n_estimators': 2000,\n 'n_jobs': -1,\n 'oob_score': False,\n 'warm_start': True}\n```", "```py\nfinal_etr = ExtraTreesRegressor(**param_dict)\nfinal_etr.fit(preds_df.values, y_stack)\n```", "```py\ndef handle_X_set(X_train_set):\n y_pred_bag = rs_bag.predict(X_train_set)\n y_pred_gbt = gbt_inst.predict(X_train_set)\n preds_df = pd.DataFrame(columns = ['bag', 'gbt'])\n\n preds_df['bag'] = y_pred_bag\n preds_df['gbt'] = y_pred_gbt\n\n return preds_df.values\n\ndef predict_from_X_set(X_train_set):\n return final_etr.predict(handle_X_set(X_train_set)) \n```", "```py\ny_pred = predict_from_X_set(X_test_prin)\n```", "```py\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint \"R-squared\",r2_score(y_test_prin, y_pred)\nprint \"MAE : \",mean_absolute_error(y_test_prin, y_pred)\nprint \"MAPE : \",(np.abs(y_test_prin- y_pred)/y_test_prin).mean()\n\nR-squared 0.844114615094\nMAE : 0.298422222752\nMAPE : 0.173901911714\n```"]
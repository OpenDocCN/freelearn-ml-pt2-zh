- en: Chapter 4. Topic Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we grouped text documents using clustering. This is
    a very useful tool, but it is not always the best. Clustering results in each
    text belonging to exactly one cluster. This book is about machine learning and
    Python. Should it be grouped with other Python-related works or with machine-related
    works? In a physical bookstore, we will need a single place to stock the book.
    In an Internet store, however, the answer is *this book is about both machine
    learning and Python* and the book should be listed in both the sections in an
    online bookstore. This does not mean that the book will be listed in all the sections,
    of course. We will not list this book with other baking books.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn methods that do not cluster documents into completely
    separate groups but allow each document to refer to several **topics**. These
    topics will be identified automatically from a collection of text documents. These
    documents may be whole books or shorter pieces of text such as a blogpost, a news
    story, or an e-mail.
  prefs: []
  type: TYPE_NORMAL
- en: We would also like to be able to infer the fact that these documents may have
    topics that are central to them, while referring to other topics only in passing.
    This book mentions plotting every so often, but it is not a central topic as machine
    learning is. This means that documents have topics that are central to them and
    others that are more peripheral. The subfield of machine learning that deals with
    these problems is called **topic modeling** and is the subject of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LDA and LDA—**unfortunately, there are two methods in machine learning with
    the initials LDA: latent Dirichlet allocation, which is a topic modeling method
    and linear discriminant analysis, which is a classification method. They are completely
    unrelated, except for the fact that the initials LDA can refer to either. In certain
    situations, this can be confusing. The scikit-learn tool has a submodule, `sklearn.lda`,
    which implements linear discriminant analysis. At the moment, scikit-learn does
    not implement latent Dirichlet allocation.'
  prefs: []
  type: TYPE_NORMAL
- en: The topic model we will look at is **latent Dirichlet allocation** (**LDA**).
    The mathematical ideas behind LDA are fairly complex, and we will not go into
    the details here.
  prefs: []
  type: TYPE_NORMAL
- en: 'For those who are interested, and adventurous enough, Wikipedia will provide
    all the equations behind these algorithms: [http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).'
  prefs: []
  type: TYPE_NORMAL
- en: However, we can understand the ideas behind LDA intuitively at a high-level.
    LDA belongs to a class of models that are called generative models as they have
    a sort of fable, which explains how the data was generated. This generative story
    is a simplification of reality, of course, to make machine learning easier. In
    the LDA fable, we first create topics by assigning probability weights to words.
    Each topic will assign different weights to different words. For example, a Python
    topic will assign high probability to the word "variable" and a low probability
    to the word "inebriated". When we wish to generate a new document, we first choose
    the topics it will use and then mix words from these topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say we have only three topics that books discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each topic, we have a list of words associated with it. This book will be
    a mixture of the first two topics, perhaps 50 percent each. The mixture does not
    need to be equal, it can also be a 70/30 split. When we are generating the actual
    text, we generate word by word; first we decide which topic this word will come
    from. This is a random decision based on the topic weights. Once a topic is chosen,
    we generate a word from that topic's list of words. To be precise, we choose a
    word in English with the probability given by the topic.
  prefs: []
  type: TYPE_NORMAL
- en: In this model, the order of words does not matter. This is a *bag of words*
    model as we have already seen in the previous chapter. It is a crude simplification
    of language, but it often works well enough, because just knowing which words
    were used in a document and their frequencies are enough to make machine learning
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, we do not know what the topics are. Our task is to take a
    collection of text and to reverse engineer this fable in order to discover what
    topics are out there and simultaneously figure out which topics each document
    uses.
  prefs: []
  type: TYPE_NORMAL
- en: Building a topic model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unfortunately, scikit-learn does not support latent Dirichlet allocation. Therefore,
    we are going to use the gensim package in Python. Gensim is developed by Radim
    Řehůřek who is a machine learning researcher and consultant in the United Kingdom.
    We must start by installing it. We can achieve this by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As input data, we are going to use a collection of news reports from the **Associated
    Press** (**AP**). This is a standard dataset for text modeling research, which
    was used in some of the initial works on topic models. After downloading the data,
    we can load it by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `corpus` variable holds all of the text documents and has loaded them in
    a format that makes for easy processing. We can now build a topic model using
    this object as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This single constructor call will statistically infer which topics are present
    in the corpus. We can explore the resulting model in many ways. We can see the
    list of topics a document refers to using the `model[doc]` syntax, as shown in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The result will almost surely look different on our computer! The learning algorithm
    uses some random numbers and every time you learn a new topic model on the same
    input data, the result is different. Some of the qualitative properties of the
    model will be stable across different runs if your data is well behaved. For example,
    if you are using the topics to compare documents, as we do here, then the similarities
    should be robust and change only slightly. On the other hand, the order of the
    different topics will be completely different.
  prefs: []
  type: TYPE_NORMAL
- en: 'The format of the result is a list of pairs: `(topic_index, topic_weight)`.
    We can see that only a few topics are used for each document (in the preceding
    example, there is no weight for topics 0, 1, and 2; the weight for those topics
    is 0). The topic model is a sparse model, as although there are many possible
    topics; for each document, only a few of them are used. This is not strictly true
    as all the topics have a nonzero probability in the LDA model, but some of them
    have such a small probability that we can round it to zero as a good approximation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can explore this further by plotting a histogram of the number of topics
    that each document refers to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a topic model](img/2772OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Sparsity** means that while you may have large matrices and vectors, in principle,
    most of the values are zero (or so small that we can round them to zero as a good
    approximation). Therefore, only a few things are relevant at any given time.'
  prefs: []
  type: TYPE_NORMAL
- en: Often problems that seem too big to solve are actually feasible because the
    data is sparse. For example, even though any web page can link to any other web
    page, the graph of links is actually very sparse as each web page will link to
    a very tiny fraction of all other web pages.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding graph, we can see that about 150 documents have 5 topics, while
    the majority deals with around 10 to 12 of them. No document talks about more
    than 20 different topics.
  prefs: []
  type: TYPE_NORMAL
- en: To a large extent, this is due to the value of the parameters that were used,
    namely, the `alpha` parameter. The exact meaning of alpha is a bit abstract, but
    bigger values for alpha will result in more topics per document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alpha needs to be a value greater than zero, but is typically set to a lesser
    value, usually, less than one. The smaller the value of `alpha`, the fewer topics
    each document will be expected to discuss. By default, gensim will set `alpha`
    to `1/num_topics`, but you can set it explicitly by passing it as an argument
    in the `LdaModel` constructor as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, this is a larger alpha than the default, which should lead to
    more topics per document. As we can see in the combined histogram given next,
    gensim behaves as we expected and assigns more topics to each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a topic model](img/2772OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, we can see in the preceding histogram that many documents touch upon 20
    to 25 different topics. If you set the value lower, you will observe the opposite
    (downloading the code from the online repository will allow you to play around
    with these values).
  prefs: []
  type: TYPE_NORMAL
- en: What are these topics? Technically, as we discussed earlier, they are multinomial
    distributions over words, which means that they assign a probability to each word
    in the vocabulary. Words with high probability are more associated with that topic
    than words with lower probability.
  prefs: []
  type: TYPE_NORMAL
- en: Our brains are not very good at reasoning with probability distributions, but
    we can readily make sense of a list of words. Therefore, it is typical to summarize
    topics by the list of the most highly weighted words.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we display the first ten topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Topic no. | Topic |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | dress military soviet president new state capt carlucci states leader
    stance government |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | koch zambia lusaka oneparty orange kochs party i government mayor new
    political |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | human turkey rights abuses royal thompson threats new state wrote garden
    president |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | bill employees experiments levin taxation federal measure legislation
    senate president whistleblowers sponsor |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | ohio july drought jesus disaster percent hartford mississippi crops northern
    valley virginia |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | united percent billion year president world years states people i bush
    news |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | b hughes affidavit states united ounces squarefoot care delaying charged
    unrealistic bush |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | yeutter dukakis bush convention farm subsidies uruguay percent secretary
    general i told |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | kashmir government people srinagar india dumps city two jammukashmir
    group moslem pakistan |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | workers vietnamese irish wage immigrants percent bargaining last island
    police hutton I |'
  prefs: []
  type: TYPE_TB
- en: 'Although daunting at first glance, when reading through the list of words,
    we can clearly see that the topics are not just random words, but instead these
    are logical groups. We can also see that these topics refer to older news items,
    from when the Soviet Union still existed and Gorbachev was its Secretary General.
    We can also represent the topics as word clouds, making more likely words larger.
    For example, this is the visualization of a topic which deals with the Middle
    East and politics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a topic model](img/2772OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can also see that some of the words should perhaps be removed (for example,
    the word "I") as they are not so informative, they are stop words. When building
    topic modeling, it can be useful to filter out stop words, as otherwise, you might
    end up with a topic consisting entirely of stop words. We may also wish to preprocess
    the text to stems in order to normalize plurals and verb forms. This process was
    covered in the previous chapter and you can refer to it for details. If you are
    interested, you can download the code from the companion website of the book and
    try all these variations to draw different pictures.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building a word cloud like the previous one can be done with several different
    pieces of software. For the graphics in this chapter, we used a Python-based tool
    called pytagcloud. This package requires a few dependencies to install and is
    not central to machine learning, so we won't consider it in the main text; however,
    we have all of the code available in the online code repository to generate the
    figures in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing documents by topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topics can be useful on their own to build the sort of small vignettes with
    words that are shown in the previous screenshot. These visualizations can be used
    to navigate a large collection of documents. For example, a website can display
    the different topics as different word clouds, allowing a user to click through
    to the documents. In fact, they have been used in just this way to analyze large
    collections of documents.
  prefs: []
  type: TYPE_NORMAL
- en: However, topics are often just an intermediate tool to another end. Now that
    we have an estimate for each document of how much of that document comes from
    each topic, we can compare the documents in topic space. This simply means that
    instead of comparing word to word, we say that two documents are similar if they
    talk about the same topics.
  prefs: []
  type: TYPE_NORMAL
- en: This can be very powerful as two text documents that share few words may actually
    refer to the same topic! They may just refer to it using different constructions
    (for example, one document may read "the President of the United States" while
    the other will use the name "Barack Obama").
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Topic models are good on their own to build visualizations and explore data.
    They are also very useful as an intermediate step in many other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can redo the exercise we performed in the last chapter and
    look for the most similar post to an input query, by using the topics to define
    similarity. Whereas, earlier we compared two documents by comparing their word
    vectors directly, we can now compare two documents by comparing their topic vectors.
  prefs: []
  type: TYPE_NORMAL
- en: For this, we are going to project the documents to the topic space. That is,
    we want to have a vector of topics that summarize the document. How to perform
    these types of **dimensionality reduction** in general is an important task in
    itself and we have a chapter entirely devoted to this task. For the moment, we
    just show how topic models can be used for exactly this purpose; once topics have
    been computed for each document, we can perform operations on its topic vector
    and forget about the original words. If the topics are meaningful, they will be
    potentially more informative than the raw words. Additionally, this may bring
    computational advantages, as it is much faster to compare 100 vectors of topic
    weights than vectors of the size of the vocabulary (which will contain thousands
    of terms).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using gensim, we have seen earlier how to compute the topics corresponding
    to all the documents in the corpus. We will now compute these for all the documents
    and store it in a NumPy arrays and compute all pairwise distances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, `topics` is a matrix of topics. We can use the `pdist` function in SciPy
    to compute all pairwise distances. That is, with a single function call, we compute
    all the values of `sum((topics[ti] – topics[tj])**2)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will employ one last little trick; we will set the diagonal elements
    of the `distance` matrix to a high value (it just needs to be larger than the
    other values in the matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And we are done! For each document, we can look up the closest element easily
    (this is a type of nearest neighbor classifier):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note that this will not work if we had not set the diagonal elements to a large
    value: the function will always return the same element as it is the one most
    similar to itself (except in the weird case where two elements had exactly the
    same topic distribution, which is very rare unless they are exactly the same).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is one possible query document (it is the second document
    in our collection):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If we ask for the most similar document to `closest_to(1)`, we receive the
    following document as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The system returns a post by the same author discussing medications.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the whole of Wikipedia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the initial LDA implementations can be slow, which limited their use to
    small document collections, modern algorithms work well with very large collections
    of data. Following the documentation of gensim, we are going to build a topic
    model for the whole of the English-language Wikipedia. This takes hours, but can
    be done even with just a laptop! With a cluster of machines, we can make it go
    much faster, but we will look at that sort of processing environment in a later
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we download the whole Wikipedia dump from [http://dumps.wikimedia.org](http://dumps.wikimedia.org).
    This is a large file (currently over 10 GB), so it may take a while, unless your
    Internet connection is very fast. Then, we will index it with a gensim tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the previous line on the command shell, not on the Python shell. After
    a few hours, the index will be saved in the same directory. At this point, we
    can build the final topic model. This process looks exactly like what we did for
    the small AP dataset. We first import a few packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we set up logging, using the standard Python logging module (which gensim
    uses to print out status messages). This step is not strictly necessary, but it
    is nice to have a little more output to know what is happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we load the preprocessed data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we build the LDA model as we did earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This will again take a couple of hours. You will see the progress on your console,
    which can give you an indication of how long you still have to wait.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it is done, we can save the topic model to a file, so we don''t have to
    redo it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If you exit your session and come back later, you can load the model again
    using the following command (after the appropriate imports, naturally):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `model` object can be used to explore the collection of documents, and build
    the `topics` matrix as we did earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that this is still a sparse model even if we have many more documents
    than we had earlier (over 4 million as we are writing this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: So, the average document mentions 6.4 topics and 94 percent of them mention
    10 or fewer topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can ask what the most talked about topic in Wikipedia is. We will first
    compute the total weight for each topic (by summing up the weights from all the
    documents) and then retrieve the words corresponding to the most highly weighted
    topic. This is performed using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the same tools as we did earlier to build up a visualization, we can
    see that the most talked about topic is related to music and is a very coherent
    topic. A full 18 percent of Wikipedia pages are partially related to this topic
    (5.5 percent of all the words in Wikipedia are assigned to this topic). Take a
    look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling the whole of Wikipedia](img/2772OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These plots and numbers were obtained when the book was being written. As Wikipedia
    keeps changing, your results will be different. We expect that the trends will
    be similar, but the details may vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can look at the least talked about topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Modeling the whole of Wikipedia](img/2772OS_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The least talked about topic is harder to interpret, but many of its top words
    refer to airports in eastern countries. Just 1.6 percent of documents touch upon
    it, and it represents just 0.1 percent of the words.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the number of topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in the chapter, we have used a fixed number of topics for our analyses,
    namely 100\. This was a purely arbitrary number, we could have just as well used
    either 20 or 200 topics. Fortunately, for many uses, this number does not really
    matter. If you are going to only use the topics as an intermediate step, as we
    did previously when finding similar posts, the final behavior of the system is
    rarely very sensitive to the exact number of topics used in the model. This means
    that as long as you use enough topics, whether you use 100 topics or 200, the
    recommendations that result from the process will not be very different; 100 is
    often a good enough number (while 20 is too few for a general collection of text
    documents). The same is true of setting the `alpha` value. While playing around
    with it can change the topics, the final results are again robust against this
    change.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Topic modeling is often an end towards a goal. In that case, it is not always
    very important exactly which parameter values are used. A different number of
    topics or values for parameters such as `alpha` will result in systems whose end
    results are almost identical in their final results.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you are going to explore the topics directly, or build
    a visualization tool that exposes them, you should probably try a few values and
    see which gives you the most useful or most appealing results.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, there are a few methods that will automatically determine the
    number of topics for you, depending on the dataset. One popular model is called
    the **hierarchical Dirichlet process**. Again, the full mathematical model behind
    it is complex and beyond the scope of this book. However, the fable we can tell
    is that instead of having the topics fixed first as in the LDA generative story,
    the topics themselves were generated along with the data, one at a time. Whenever
    the writer starts a new document, they have the option of using the topics that
    already exist or to create a completely new one. When more topics have already
    been created, the probability of creating a new one, instead of reusing what exists
    goes down, but the possibility always exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means *that the more documents we have, the more topics we will end up
    with*. This is one of those statements that is unintuitive at first but makes
    perfect sense upon reflection. We are grouping documents and the more examples
    we have, the more we can break them up. If we only have a few examples of news
    articles, then "Sports" will be a topic. However, as we have more, we start to
    break it up into the individual modalities: "Hockey", "Soccer", and so on. As
    we have even more data, we can start to tell nuances apart, articles about individual
    teams and even individual players. The same is true for people. In a group of
    many different backgrounds, with a few "computer people", you might put them together;
    in a slightly larger group, you will have separate gatherings for programmers
    and systems administrators; and in the real-world, we even have different gatherings
    for Python and Ruby programmers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **hierarchical Dirichlet process** (**HDP**) is available in gensim. Using
    it is trivial. To adapt the code we wrote for LDA, we just need to replace the
    call to `gensim.models.ldamodel.LdaModel` with a call to the `HdpModel` constructor
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That's it (except that it takes a bit longer to compute—there are no free lunches).
    Now, we can use this model in much the same way as we used the LDA model, except
    that we did not need to specify the number of topics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed topic modeling. Topic modeling is more flexible
    than clustering as these methods allow each document to be partially present in
    more than one group. To explore these methods, we used a new package, gensim.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling was first developed and is easier to understand in the case of
    text, but in the computer vision chapter we will see how some of these techniques
    may be applied to images as well. Topic models are very important in modern computer
    vision research. In fact, unlike the previous chapters, this chapter was very
    close to the cutting edge of research in machine learning algorithms. The original
    LDA algorithm was published in a scientific journal in 2003, but the method that
    gensim uses to be able to handle Wikipedia was only developed in 2010 and the
    HDP algorithm is from 2011\. The research continues and you can find many variations
    and models with wonderful names such as *the Indian buffet process* (not to be
    confused with the *Chinese restaurant process*, which is a different model), or
    *Pachinko allocation* (Pachinko being a type of Japanese game, a cross between
    a slot-machine and pinball).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now gone through some of the major machine learning modes: classification,
    clustering, and topic modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we go back to classification, but this time, we will be
    exploring advanced algorithms and approaches.
  prefs: []
  type: TYPE_NORMAL

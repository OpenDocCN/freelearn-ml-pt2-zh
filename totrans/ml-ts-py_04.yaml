- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Forecasting with Moving Averages and Autoregressive Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is about time-series modeling based on moving averages and autoregression.
    This comprises a large set of models that are very popular in different disciplines,
    including econometrics and statistics. We'll discuss autoregression and moving
    averages models, along with others that combine these two, such as ARMA, ARIMA,
    VAR, GARCH, and others.
  prefs: []
  type: TYPE_NORMAL
- en: These models are still held in high esteem and find their applications. However,
    many new models have since sprung up that have been shown to be competitive or
    even outperform these simpler ones. Within their main application, however, in
    univariate forecasting, simple models often provide accurate or accurate enough
    predictions, so that these models constitute a mainstay in time-series modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are classical models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving average and autoregression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Model selection and order
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential smoothing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ARCH and GARCH
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector autoregression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Python libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: statsmodels
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Python practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling in Python
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to start with an introduction to classical models.
  prefs: []
  type: TYPE_NORMAL
- en: What are classical models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll deal with models that could be characterized as having
    a longer tradition, and are rooted in statistics and mathematics. They are used
    heavily in econometrics and statistics.
  prefs: []
  type: TYPE_NORMAL
- en: While there is considerable overlap between statistics and machine learning
    approaches, and each community has been absorbing the work of the other, there
    are still a few key differences. Whereas statistics papers are still overwhelmingly
    formal and deductive, machine learning researchers are more pragmatic, relying
    on the predictive accuracy of models.
  prefs: []
  type: TYPE_NORMAL
- en: We've talked about the very early history of time-series models in *Chapter
    1*, *Introduction to Time-Series with Python*. In this chapter, we'll discuss
    moving averages and autoregressive approaches for forecasting. These were introduced
    in the early 20^(th) century and popularized by George Box and Gwilym Jenkins
    in 1970 in their book "*Time-Series Analysis Forecasting and Control*." Crucially,
    in their book, Box and Jenkins formalized the ARIMA model and described how to
    apply it to time-series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Many time-series exhibit trends and seasonality, while many of the models in
    this chapter assume stationarity. If a time-series is stationary, its mean and
    standard deviation stays constant over time. This implies that the time-series
    has no trend and no cyclic variability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the removal of irregular components, trends, and seasonal fluctuations
    is an intrinsic aspect of applying these models. The models then forecast what''s
    left after removing seasonality and trend: business cycles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to apply classical models, a time-series usually should be decomposed
    into different components. Thus, classical models are usually applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Test for stationarity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Differencing [if stationarity detected]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit method and forecast
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add back the trend and seasonality
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most of the approaches in this chapter are relevant only to univariate time-series.
    Although extensions to multivariate time-series have been proposed, they are not
    as popular as the univariate versions. Univariate time-series consist of a single
    vector or, in other words, one value that changes over time. We'll see **Vector
    Autoregression** (**VAR**) at the end of the chapter though, which is an extension
    to multivariate time-series.
  prefs: []
  type: TYPE_NORMAL
- en: Another important consideration is that most classical models are linear, which
    means they assume linearity in the dependencies between values at the same time
    and between values at different time steps. In practice, the models in this chapter
    work well with a range of time-series that are stationary. This means that the
    distribution is the same over time. Examples of this are temperature changes over
    time. These models are especially valuable in a context where the amount of data
    available is small so that the extra estimation error in non-linear models dominates
    any potential gains in terms of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: However, the stationarity assumption implies that the application of the models
    in this chapter is restricted to time-series that have this property. Alternatively,
    we'd have to preprocess our time-series to enforce stationarity. In contrast,
    the development of statistical methods for nonlinear time-series analysis and
    forecasting has found much less prominence; however, there are some models such
    as the Threshold Autoregressive Model (which we won't cover here).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is left to note that, while a reasonable first approach, many time-series
    such as temperatures can be predicted much more accurately by high-dimensional
    physics-based models of the atmosphere than by statistical models. This illustrates
    the point of complexity: essentially, modeling is condensing a set of hypotheses
    and formalizing them together with parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Real-world time-series come from complicated processes that can be non-linear
    and non-stationary, and there's more than a single way of describing them, each
    of which has its up- and downsides. Thus, we can think of a modeling problem in
    terms of lots of parameters or just as a single or a couple of parameters. In
    a dedicated section below, we'll discuss the issue of selecting a model from a
    set of alternatives based on the number of parameters and the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, nonlinear models come from a different direction of research, either
    neural networks or the broader field of machine learning. We'll see neural networks
    in *Chapter 10*, *Deep Learning for Time-Series*, and we'll discuss and apply
    state-of-the-art machine learning in *Chapter 7*, *Machine Learning Models for
    Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: Moving average and autoregression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classical models can be grouped into families of models – **moving averages**
    (**MA**), **autoregressive** (**AR**) models, ARMA, and ARIMA. These models were
    formalized and popularized over time in books and papers by many mathematicians
    and statisticians, including Peter Whittle (1951) and George Box and Gwilym Jenkins
    (1970). But let's start earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The **moving average** marked the beginning of modern time-series predictions.
    In a moving average, the average (usually, the arithmetic mean) of values is taken
    over a specific number of time points (time frame) in the past.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, the **simple moving average**, the unweighted mean over a period
    of k points, is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where *x*[i] represents the observed time-series.
  prefs: []
  type: TYPE_NORMAL
- en: The moving average can be used to smooth out a time-series, thereby removing
    noise and periodic fluctuations that occur in the short term, effectively working
    as a low-pass filter. Therefore, as mathematician Reginald Hooker pointed out
    in a publication in 1902, the moving average can serve to isolate trend and oscillatory
    components. He conceptualized trend as the direction in which a series is moving
    when oscillations are disregarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The moving average can smooth the trend and cycle over the history of a time-series;
    however, as a model, the moving average can be used to forecast into the future
    as well. The time-series is a linear regression of the current value of the series
    against observed values (error terms). The moving average model of order *q*,
    *MA(q)*, can be denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_002.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B17577_05_003.png) is the average (expectation) of *x*[t] (usually
    assumed to be 0), ![](img/B17577_05_004.png) are parameters, and ![](img/B17577_05_005.png)
    is random noise.
  prefs: []
  type: TYPE_NORMAL
- en: Educated in Cambridge, Hooker worked in the Statistical Branch of the Ministry
    of Agriculture, Fisheries, and Food of the United Kingdom. He was an out-of-hours
    statistician, writing about meteorology and socio-economic topics such as wages,
    marriage rates and trade, and crop forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: The invention of **AR** techniques dates back to a paper by British statistician
    Udny Yule, a personal friend of Hooker's, in 1927 ("*On a Method of Investigating
    Periodicities in Disturbed Time-Series with special reference to Wolfer's Sunspot
    Numbers*"). An **autoregressive model** regresses the variable on its own lagged
    values. In other words, the current value of the value is driven by immediately
    preceding values using a linear combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sunspot variations are highly cyclic as can be seen in this plot of sunspot
    observations over time (loaded through statsmodels data utilities):'
  prefs: []
  type: TYPE_NORMAL
- en: '![../../../../../Downloads/Machine-Learning%20for%20Time-Series%20with%20Python/sunspot_act](img/B17577_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Sunspot observations by year'
  prefs: []
  type: TYPE_NORMAL
- en: Yule formulized a linear model driven by noise as an application to sunspot
    numbers, the count of dark spots on the outer shell of the sun. The spots have
    their origin in giant explosions and indicate magnetic activity of the sun, and
    phenomena such as solar flares.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two images of low and high solar activity according to sunspot numbers
    (from NASA):'
  prefs: []
  type: TYPE_NORMAL
- en: '![hat Will Solar Cycle 25 Look Like? | NASA](img/B17577_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Solar activity'
  prefs: []
  type: TYPE_NORMAL
- en: Today, we know that the solar cycle is a nearly periodic 11-year change in the
    solar magnetic activity between high magnetic activity (solar maximum) and low
    magnetic activity (solar minimum). At the high point, explosions (solar flares)
    can unleash charged particles into space, potentially endangering life on Earth.
  prefs: []
  type: TYPE_NORMAL
- en: Yule studied engineering at **University College London** (**UCL**), went to
    work with Heinrich Hertz in Bonn, returned to UCL to work with Karl Pearson, and
    was later promoted to assistant professor at UCL. After first taking the post
    of statistics at UCL, he moved to Cambridge. He is remembered for his book on
    statistics "*An Introduction to the Theory of Statistics*," which was first published
    in 1911 and went through many editions, as well as for his description of the
    process known today as preferential attachment, where the distribution of new
    quantities to nodes in a network is in accordance with how much the nodes already
    have; this is sometimes noted as "the richer get richer."
  prefs: []
  type: TYPE_NORMAL
- en: 'Andrey Kolmogorov defined the term **stationary process** in 1931, although
    Louis Bachelier had introduced a similar definition earlier (1900) using different
    terminology. **Stationarity** is defined by three characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Finite variation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Constant mean
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Constant variation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Constant variation means that the variation of the time-series in a window
    between two points is constant over time: ![](img/B17577_05_006.png), although
    it can change with the size of the window.'
  prefs: []
  type: TYPE_NORMAL
- en: This is weak stationarity. In the literature, unless otherwise specified, usually
    stationarity means weak stationarity. Strict stationarity means that a time-series
    has a probability density function that is unchanged over time. In other words,
    under strict stationarity, the joint distribution over ![](img/B17577_05_007.png)
    is the same as over ![](img/B17577_05_008.png).
  prefs: []
  type: TYPE_NORMAL
- en: In 1938, Norwegian mathematician Herman Ole Andreas Wold described the decomposition
    of stationary time-series. He observed that stationary time-series can be expressed
    as the sum of a deterministic component (autoregressive) and a stochastic component
    (noise). This decomposition is termed after him today, as **Wold's decomposition**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the formulation of the autoregressive model of order ![](img/B17577_05_009.png),
    *AR(p)*, as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B17577_05_011.png) is a model parameter, *c* is a constant, and
    ![](img/B17577_05_012.png) represents noise. In this equation, *p* is a measure
    of the autocorrelation between successive values of the time-series.
  prefs: []
  type: TYPE_NORMAL
- en: This work was later, in 1951, generalized to multivariate time-series in a Ph.D.
    thesis ("*Hypothesis Testing in Time-Series*") by New Zealander Peter Whittle,
    with Wold as his advisor. Peter Whittle is also credited with the integration
    of the AR and MA models into one, as the **autoregressive moving average** (**ARMA**).
    This was another milestone in the history of time-series modeling, bringing together
    the work of Yule and Hooker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ARMA model consists of two types of lagged values, one for the autoregressive
    component and the other for the moving average component. Therefore, we write
    *ARMA(p, q)*, with the first parameter p indicating the order of the autoregression,
    and the second, *q*, the order of the moving average, as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_013.png)'
  prefs: []
  type: TYPE_IMG
- en: ARMA assumes that the series is stationary. In practice, to ensure stationarity,
    preprocessing has to be applied.
  prefs: []
  type: TYPE_NORMAL
- en: The model parameters were estimated via the least-squares method until George
    Box and Gwilym Jenkins popularized their method of a maximum-likelihood estimation
    of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: George Box was one of the most influential figures of not only classical time-series
    prediction, but also the broader field of statistics. Drafted for World War II
    without having finished his studies in chemistry, he performed poison gas experiments
    for the army, teaching himself statistics for analysis in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Once the war was over, he studied mathematics and statistics at the University
    College London, completing his Ph.D. with Egon Pearson, the son of Karl Pearson,
    as his advisor. He later headed a research group at Princeton, and then founded
    the statistics department at the University of Wisconsin–Madison.
  prefs: []
  type: TYPE_NORMAL
- en: 'Box and Jenkin''s 1970 book "*Time-Series Analysis: Forecasting and Control*"
    outlined many applied examples for time-series forecasting and seasonal adjustment.
    The so-called Box-Jenkins method is one of the most popular forecasting methods.
    Their book also contained a description of the **autoregressive integrated moving
    average** model (**ARIMA**).'
  prefs: []
  type: TYPE_NORMAL
- en: ARIMA(p, d, q) includes a data preprocessing step, called **integration**, to
    make the time-series stationary, which is by replacing values by subtracting the
    immediate past values, a transformation called **differencing**.
  prefs: []
  type: TYPE_NORMAL
- en: The model integration is parametrized by d, which is the number of times differences
    have been taken between current and previous values. As mentioned, the three parameters
    stand for the three parts of the model.
  prefs: []
  type: TYPE_NORMAL
- en: There are some special cases; ARIMA(p,0,0) stands for AR(p), ARIMA(0,d,0) for
    I(d), and ARIMA(0,0,q) is MA(q). I(0) is sometimes used as a convention to refer
    to stationary time-series, which don't require any differencing to be stationary.
  prefs: []
  type: TYPE_NORMAL
- en: While ARIMA type models effectively consider stationary processes, the **Seasonal
    Auto Regressive Integrative Moving Average** models (**SARIMA**), developed as
    an extension of the ARMA model, can describe processes that exhibit non-stationary
    behaviors both within and across seasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seasonal ARIMA models are usually stated as ARIMA(p,d,q)(P,D,Q)m. The parameters
    deserve more explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: m denotes the number of periods in a season
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P, D, Q parametrize the autoregressive, integration, and moving average components
    of the seasonal part
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p, d, q refer to the ARIMA terms, which we've discussed previously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P is a measure of autocorrelation between successive seasonal components of
    the time-series.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write out the seasonal parts to make this clearer. **Seasonal Autoregression**,
    **SAR**, can be stated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_014.png)'
  prefs: []
  type: TYPE_IMG
- en: where *s* is the length of the seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the **seasonal moving average**, **SMA**, can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_015.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that each of these components will use a distinct set of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the model SARIMA(0,1,1)(0,1,1)12 process will contain a non-seasonal
    MA(1) term (with the corresponding parameter ![](img/B17577_05_016.png)) and a
    seasonal MA(1) term (with the corresponding parameter ![](img/B17577_05_017.png)).
  prefs: []
  type: TYPE_NORMAL
- en: Model selection and order
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The parameter q in ARMA would typically be 3 or less, but this is more a reflection
    of computing resources rather than statistics. Today, to set the parameters p
    and q, we would typically look at the autocorrelation and partial autocorrelation
    plots, where we could see peaks in the correlation for each lag.
  prefs: []
  type: TYPE_NORMAL
- en: When we have different models, say models of different p and q, each trained
    on the same dataset, how do we know which one should we use? This is where model
    selection comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection is the methodology for deciding between competing models. One
    of the main ideas in model selection is Occam's razor, named after the English
    Franciscan friar and scholastic philosopher William of Ockham, who lived between
    circa 1287 and 1347\.
  prefs: []
  type: TYPE_NORMAL
- en: According to Occam's razor, when choosing between competing solutions, one should
    prefer the explanation with the fewest assumptions. Ockham argued based on this
    idea that the principle of divine interventions is so simple that miracles are
    a parsimonious explanation. This rule, also called "*lex parsimoniae*" in Latin,
    expresses that a model should be parsimonious, which means that it should be simple
    yet have high explanatory power.
  prefs: []
  type: TYPE_NORMAL
- en: In science, simpler explanations are preferred out of the principle of falsifiability.
    The simpler a scientific explanation, the easier it can be tested, and possibly
    refuted – this lends the model scientific rigor.
  prefs: []
  type: TYPE_NORMAL
- en: ARMA and other models are usually estimated with the **maximum-likelihood estimation**
    (**MLE**). In MLE, this means maximizing a likelihood function so that, given
    the parameters of the model, the observed data is most likely.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most commonly used model selection criteria for the maximum-likelihood
    method is the **Akaike information criterion** (**AIC**), after Hirotugu Akaike,
    who published it first in English in 1973\.
  prefs: []
  type: TYPE_NORMAL
- en: AIC takes the log-likelihood *l* from the maximum-likelihood method and the
    number of parameters *k* in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_018.png)'
  prefs: []
  type: TYPE_IMG
- en: This is saying that the AIC equals two times the number of parameters minus
    two times the log-likelihood. In model selection, we would prefer the model with
    the lowest AIC, which means it has few parameters, but also high log-likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an ARIMA model, we could write more specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_019.png)'
  prefs: []
  type: TYPE_IMG
- en: I've omitted the parameter d since it doesn't introduce additional estimations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Bayesian Information Criterion** (**BIC**) was proposed for model selection
    a few years later (1978), by Gideon Schwarz, and looks very much like AIC. It
    additionally takes *N*, the number of samples in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_020.png)'
  prefs: []
  type: TYPE_IMG
- en: According to BIC, we want a model with few parameters and high log-likelihood,
    but also a small number of training examples.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential smoothing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exponential smoothing, dating back to the work of Siméon Poisson, is a technique
    for smoothing time-series data using an exponential window function, which can
    be used to forecast time-series with seasonality and trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest method, **simple exponential smoothing**, **SES**, *s*[t] of a
    time-series *x*[t] can be denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_021.png)![](img/B17577_05_022.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B17577_05_023.png) is the exponential smoothing factor (a value
    between 0 and 1).
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this is the weighted moving average with weights *α* and ![](img/B17577_05_024.png).
    You can think of the second term, ![](img/B17577_05_025.png) as recursive, where,
    when expanding, ![](img/B17577_05_026.png) gets multiplied by itself over and
    over – this is the exponential term.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter ![](img/B17577_05_027.png) controls how much the smoothed value is
    determined by current versus previous values. The effect of this formula, as with
    moving averages, is that the result becomes smoother.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, John Muth showed in 1960 that SES provided the optimal forecasts
    for a time-series where, at each time step, the values take a random step away
    from its previous value, and steps are independently and identically distributed
    in size plus noise. This kind of time-series is called a random walk, and sometimes,
    the price of a fluctuating stock is assumed to be following such a behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The **Theta method**, another exponential smoothing method, is of particular
    interest to practitioners since it performed well in the M3 competition in 2000\.
    The M3 competition, named after Spyros Makridakis, its organizer, who is a professor
    at the University of Nicosia and Director of the Institute for the Future, was
    a competition for forecasting across 3003 time-series from micro-economics, industry,
    finance, demographic, and other domains. One of its main conclusions was that
    very simple methods can perform well with univariate time-series forecasts. The
    M3 competition proved to be a watershed moment for forecasting, providing benchmarks
    and the **state of the art** (**SOTA**), even though the SOTA has significantly
    changed since then, as we'll see in *Chapter 7*, *Machine Learning Models for
    Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: The Theta method was proposed by Vassilis Assimakopoulos and Konstantinos Nikolopoulos
    in 2000 and re-stated in 2001 by Rob Hyndman and Baki Billah. The Theta model
    can be understood as **simple exponential smoothing** (**SES**) with drift.
  prefs: []
  type: TYPE_NORMAL
- en: This method is based on the decomposition of the de-seasonalized data into two
    lines. The first so-called "theta" line estimates the long-term component, the
    trend, and then takes the weighted average of this trend and the SES.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s state this more formally! The trend component is forecast like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_028.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, *c* is the intercept, ![](img/B17577_05_029.png) is a coefficient
    multiplied by the time step, and ![](img/B17577_05_030.png) is the residual. ![](img/B17577_05_031.png)
    can be fit through ordinary least-squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for Theta is then taking this trend and adding it to the SES as
    a weighted sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_032.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17577_05_033.png) is the forecast for *X* at time step *t*.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular exponential smoothing methods is the **Holtz-Winters
    method**. Charles Holtz, professor at the University of Chicago, first published
    a method for double exponential smoothing (1957) that allowed forecasting based
    on trend and level. His student Peter Winters extended the method to capture seasonality
    in 1960 ("*Forecasting sales by exponentially weighted moving averages*"). Even later,
    Holtz-Winters smoothing was extended to account for multiple seasonalities (n-order
    smoothing).
  prefs: []
  type: TYPE_NORMAL
- en: To apply the Holtz-Winters method, we first remove trend and seasonality. Then
    we forecast the time-series and add back the seasonality and trend.
  prefs: []
  type: TYPE_NORMAL
- en: We can distinguish additive and multiplicative variations of the method. Both
    trend and seasonality can be either additive or multiplicative.
  prefs: []
  type: TYPE_NORMAL
- en: An additive seasonality is seasonality added independently of the values of
    the series. A multiplicative seasonal component is added proportionally, when
    the seasonal effect decreases or increases with the values (or the trend) in the
    time-series. A visual inspection can help in deciding between the two variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Holtz-Winters method is also called triple exponential smoothing because
    it applies exponential smoothing three times as we''ll see. The Holtz-Winters
    method captures three components:'
  prefs: []
  type: TYPE_NORMAL
- en: An estimate of a level for each time point, *L*[t] – this could be an average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A trend component *T*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seasonality *S*[t] with *m* seasons (the number of seasons in a year)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With additive trend and seasonality, in mathematical terms, the Holtz-Winters
    forecast for a value ![](img/B17577_05_034.png) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With **multiplicative seasonality**, we multiply by the seasonality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_036.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The level is updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_037.png)'
  prefs: []
  type: TYPE_IMG
- en: We are updating the current level based on a weighted average of two terms,
    with ![](img/B17577_05_038.png) being the weight between them. These two terms
    are the previous level and the de-seasonalized value of the series.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation, we are de-seasonalizing the series by dividing by the seasonality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the previous trend component gets added up to the previous level like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The trend update is as follows (for additive trend):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the (multiplicative) seasonality update is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_042.png)'
  prefs: []
  type: TYPE_IMG
- en: We can switch these equations to additive variations as required. A more detailed
    treatment is beyond the scope of this book – we'll leave it here.
  prefs: []
  type: TYPE_NORMAL
- en: ARCH and GARCH
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Robert F. Engle, professor of economics at MIT, proposed a model for time-series
    forecasting (1982) that he named **ARCH** (**Auto-Regressive Conditionally Heteroscedastic**).
  prefs: []
  type: TYPE_NORMAL
- en: For financial institutions, value at risk, the level of financial risk over
    a specific time period, is an important concept for risk management. Therefore,
    it is crucial to account for the covariance structure of asset returns. This is
    what ARCH does and explains its importance.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, in recognition of his contributions to the field of time-series econometrics,
    Engle was awarded the 2003 Nobel Prize in Economics (Nobel Memorial Prize in Economic
    Sciences), together with Clive Granger, who we encountered earlier. The citation
    specifically mentioned his groundbreaking work on ARCH.
  prefs: []
  type: TYPE_NORMAL
- en: While, in ARMA-type models, returns are modeled as independent and identically
    distributed over time, ARCH allows for time-varying (heteroscedastic) error terms
    by parametrizing higher-order dependence between returns observed at varying frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In ARCH, the residuals are expressed as consisting of a stochastic, *z*[t],
    and a standard deviation, ![](img/B17577_05_043.png), both of which are time-dependent:
    ![](img/B17577_05_044.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard deviation of the residual at time *t* is modeled depending on
    the residuals of the series at previous points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_045.png)'
  prefs: []
  type: TYPE_IMG
- en: where q is the number of preceding time points the variance depends on.
  prefs: []
  type: TYPE_NORMAL
- en: The model ARCH(q) can be determined using least-squares.
  prefs: []
  type: TYPE_NORMAL
- en: The least-squares algorithm is to solve the linear equations y=X.β for β. It
    consists of finding the parameters that minimize the square of the error, ![](img/B17577_05_046.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**GARCH** (**generalized ARCH**) was born when Tim Bollerslev (1986) and Stephen
    Taylor (1986) both independently extended Engle''s model to make it more general.
    The main difference between GARCH and ARCH is that the residuals come from an ARCH
    model rather than from an autoregressive model, AR.'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, before applying a GARCH or ARCH model, a statistical test for homoscedasticity
    is applied, in other words, whether the variance is constant over time. Commonly
    used is the ARCH-LM test, which works with the null hypothesis that the time-series
    has no ARCH effects.
  prefs: []
  type: TYPE_NORMAL
- en: Vector autoregression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the previous forecasting methods presented in this chapter are for univariate
    time-series, that is, time-series that consist of a single time-dependent variable,
    a single vector. In practice, we usually know more than our single sequence of
    measurements.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if our time-series is about the number of ice cream sales, we might
    know the temperatures or the sales of bathing suits. We could expect that the
    sales of ice cream are highly correlated to temperature, in fact, we might expect
    that ice cream is increasingly consumed when temperatures are high. Equally, we
    could speculate that the sales of bathing suits either coincide, pre-date, or
    post-date the sales of ice cream.
  prefs: []
  type: TYPE_NORMAL
- en: Vector autoregression models can track the relationships between several variables
    as they change over time. They can capture the linear dependence of a time-series
    on a vector of values that precedes the current timestamp, generalizing the AR
    model to multivariate time-series.
  prefs: []
  type: TYPE_NORMAL
- en: 'VAR models are characterized by their order, which refers to the number of
    preceding time points that go into the model. The simplest case, VAR(1), where
    the model takes just one lag of the series, can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_047.png)'
  prefs: []
  type: TYPE_IMG
- en: where *c* is a constant, the intercept of the line, ![](img/B17577_05_048.png)
    are the coefficients of the model, and ![](img/B17577_05_049.png) is the error
    term at point t. *x*[t] and *c* are vectors of length *k*, while ![](img/B17577_05_050.png)
    is a ![](img/B17577_05_051.png) matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'A p-order model, VAR(p), is denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_05_052.png)'
  prefs: []
  type: TYPE_IMG
- en: VAR assumes that the error terms have a mean of 0 and that there is no serial
    correlation of error terms.
  prefs: []
  type: TYPE_NORMAL
- en: Just like vector autoregression is a multivariate generalization of autoregression,
    **vector ARIMA** (**VARIMA**) is an extension of the univariate ARIMA model to
    multivariate time-series. Although it was formalized already as early as 1957,
    available software implementations only appeared much later.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at a few libraries in Python that we can use
    for forecasting with classical models.
  prefs: []
  type: TYPE_NORMAL
- en: Python libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few popular libraries for classical time-series modeling in Python,
    but the most popular by far is statsmodels. The following chart compares the popularity
    of libraries in terms of the number of stars on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![forecasting_libraries.png](img/B17577_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Popularity of Python libraries for classical time-series forecasting'
  prefs: []
  type: TYPE_NORMAL
- en: Statsmodels is clearly the most popular among these libraries. I've only chosen
    to include libraries that are actively maintained and that implement the algorithms
    directly rather than importing them from other libraries. The SkTime or Darts
    libraries, for example, offer traditional forecasting models, but they are not
    implemented there, but in statsmodels.
  prefs: []
  type: TYPE_NORMAL
- en: pmdarima (originally pyramid-arima) contains a parameter search to help fit
    the best ARIMA model to univariate time-series. Anticipy contains a number of
    models, such as exponential decay and step models. Arch implements tools for financial
    econometrics and functionality for **Autoregressive Conditional Heteroscedasticity**
    (**ARCH**).
  prefs: []
  type: TYPE_NORMAL
- en: While not as active as Scikit-Learn and only maintained by a couple of people,
    statsmodels is the go-to library for traditional statistics and econometrics approaches
    to time-series, with a much stronger emphasis on parameter estimation and statistical
    testing than machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Statsmodels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The statsmodels library can help to estimate statistical models and perform
    statistical tests. It's built on SciPy and NumPy and has lots of statistical functions
    and models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table illustrates some of the modeling classes relevant to this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| `ar_model.AutoReg` | Univariate Autoregression Model |'
  prefs: []
  type: TYPE_TB
- en: '| `arima.model.ARIMA` | Autoregressive Integrated Moving Average (ARIMA) model
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ExponentialSmoothing` | Holt Winter''s Exponential Smoothing |'
  prefs: []
  type: TYPE_TB
- en: '| `SimpleExpSmoothing` | Simple Exponential Smoothing |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5.4: A few models implemented in statsmodels'
  prefs: []
  type: TYPE_NORMAL
- en: The ARIMA class also has functionality for SARIMA through a *seasonal_order*
    parameter, ARIMA, with seasonal components. By definition, ARIMA also supports
    MA, AR, and differencing (integration).
  prefs: []
  type: TYPE_NORMAL
- en: There are some other models, such as MarkovAutoregression, but we won't go through
    all of these – we will work through a selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other useful functions are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| `stattools.kpss` | Kwiatkowski-Phillips-Schmidt-Shin test for stationarity
    |'
  prefs: []
  type: TYPE_TB
- en: '| `stattools.adfuller` | Augmented Dickey-Fuller unit root test |'
  prefs: []
  type: TYPE_TB
- en: '| `stattools.ccf` | The cross-correlation function |'
  prefs: []
  type: TYPE_TB
- en: '| `stattools.pacf` | Partial autocorrelation estimate |'
  prefs: []
  type: TYPE_TB
- en: '| `stats.diagnostic.het_arch` | Engle''s Test for Autoregressive Conditional
    Heteroscedasticity (ARCH), also referred to as the ARCH-LM test |'
  prefs: []
  type: TYPE_TB
- en: '| `stattools.q_stat` | Ljung-Box Q Statistic |'
  prefs: []
  type: TYPE_TB
- en: '| `tsa.seasonal.seasonal_decompose` | Seasonal decomposition using moving averages
    |'
  prefs: []
  type: TYPE_TB
- en: '| `tsa.tsatools.detrend` | Detrend a vector |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5.5: Useful functions in statsmodels'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a convention, we import statsmodels like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These statsmodels algorithms are also available through SkTime, which makes
    them available through an interface similar to the Sklearn interface.
  prefs: []
  type: TYPE_NORMAL
- en: This should be enough for a brief overview. Let's get into the modeling itself!
  prefs: []
  type: TYPE_NORMAL
- en: Python practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the introduction to this chapter, we are going to be using the
    statsmodels library for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll use several libraries, which we can quickly install
    from the terminal (or similarly from the anaconda navigator):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We'll execute the commands from the Python (or IPython) terminal, but equally,
    we could execute them from a Jupyter notebook (or a different environment).
  prefs: []
  type: TYPE_NORMAL
- en: Let's get down to modeling!
  prefs: []
  type: TYPE_NORMAL
- en: Modeling in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll work with a stock ticker dataset from Yahoo finance that we'll download
    through the yfinance library. We'll first load the dataset, do some quick exploration,
    and then we'll build several models mentioned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll load a series of Standard & Poor''s depositary receipts (SPDR S&P 500
    ETF Trust):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have to indicate the date range and the ticker symbol. The daily prices come
    for Open, Close, and others. We'll work with the Open prices.
  prefs: []
  type: TYPE_NORMAL
- en: The index column is already a pandas DateTimeIndex so we don't have to convert
    it. Let's plot the series!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ticker_price.png](img/B17577_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Standard & Poor''s depositary receipt prices over time'
  prefs: []
  type: TYPE_NORMAL
- en: Since this is daily data, and there are either 253 or 252 working days in the
    year, I've decided to resample the data to weekly data and make each year consistent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Some years have 53 weeks. We can't handle that, so we'll get rid of the 53rd
    week. We now have weekly data over 52 weeks across 16 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'One final fix: statsmodels can use the frequency information associated with
    the DateTimeIndex; however, this is often not set and `df1.index.freq` is `None`.
    So, we''ll set it ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If we check now, `df1.index.freq` is `<Week: weekday=6>.`'
  prefs: []
  type: TYPE_NORMAL
- en: Setting the frequency can lead to missing values. Therefore, we are carrying
    over from the last valid value for missing values with the `fillna()` operation.
    If we don't do this, some of the models won't converge and give us NaN (not a
    number) values back instead of forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to get some idea of reasonable ranges for the order of the model.
    We''ll look at the autocorrelation and partial autocorrelation functions for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![pcf_acf.png](img/B17577_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Partial autocorrelation and autocorrelation'
  prefs: []
  type: TYPE_NORMAL
- en: These graphs show the correlation of the time-series with itself at lags of
    up to 20 time steps. R or ![](img/B17577_05_053.png) values close to 0 mean that
    consecutive observations at the lags are not correlated with one another. Inversely,
    correlations close to 1 or -1 indicate that there exists a strong positive or
    negative correlation between these observations at the lags.
  prefs: []
  type: TYPE_NORMAL
- en: Both the autocorrelation and the partial autocorrelation return confidence intervals.
    The correlation is significant if it goes beyond the confidence interval (represented
    as shaded regions).
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the partial autocorrelation with lag 1 is very high and much
    lower for higher lags. The autocorrelation is significant and high for all lags,
    but the significance drops as the lag increases.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the autoregressive model. From here on, we'll use the statsmodels
    modeling functionality. The interface is very convenient, as you'll see.
  prefs: []
  type: TYPE_NORMAL
- en: We can't use an autoregressive model straight off because it needs the time-series
    to be stationary, which means the mean and variance is constant over time – no
    seasonality, no trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use statsmodels utilities to look at seasonality and trend from the
    time-series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We set the period to 1 because each data point (row) corresponds to a year.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the components look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![seasonal_decompose.png](img/B17577_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Seasonal decomposition of the time-series'
  prefs: []
  type: TYPE_NORMAL
- en: The first subplot is the original time-series. There are both seasonality and
    trend in this dataset, which you can see separated out in the subplots.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, we need a stationary series for modeling. To establish stationarity,
    we need to remove the seasonal and trend components. We could also take off the
    seasonal or trend components that we estimated just before. Alternatively, we
    can use wrapper functionality in statsmodels or set the d parameter in ARIMA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the Augmented Dickey-Fuller and KPSS tests to check for stationarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We could have used `statsmodels.tsa.stattools.adfuller` or `statsmodels.tsa.stattools.kpss`,
    but we prefer the convenience of the ARCH library versions. We are leaving it
    to the user to check the output of the KPSS test. We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_Y7Tw2W/Screenshot
    2021-08-22 at 15.36.52.png](img/B17577_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Output from the KPSS test for stationarity'
  prefs: []
  type: TYPE_NORMAL
- en: Given the p-value of 0.997, we can reject our null hypothesis of the unit root,
    and we conclude that our process is weakly stationary.
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do we find good values for the differencing? We can use the pmdarima
    library for this, where there is a function for precisely this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We get a value of 1\. We would get the same values for the KPSS and the PP tests.
    This means that we can work off the first difference.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with an autoregressive model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, ARIMA is parametrized with parameters p, d, q, where:'
  prefs: []
  type: TYPE_NORMAL
- en: 'p is for the autoregressive model: AR(p)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: d is for the integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'q is for the moving average: MA(q)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, ARIMA(p, d, 0) is AR(p) with a differencing of order d.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is reassuring to know that statsmodels checks and warns if the stationarity
    assumption is not warranted. Let''s try to run to fit the following AR model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we already know we need a differencing of one degree, we can set d to
    1\. Let''s try again. This time, we''ll use the `STLForecast` wrapper that removes
    seasonality and adds it back in. This is necessary since ARIMA can''t handle seasonality
    out of the box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We get this summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_EjesO5/Screenshot
    2021-08-22 at 17.07.45.png](img/B17577_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Summary of our ARIMA model'
  prefs: []
  type: TYPE_NORMAL
- en: This result summary gives all the key statistics. We see that the model was
    ARIMA(1, 1, 0). The log-likelihood was -1965\. We also see the BIC and AIC values
    that we can use for model selection if we want.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we need to set `trend="t"` here so that the model includes
    a constant. If not, we would get a spurious regression.
  prefs: []
  type: TYPE_NORMAL
- en: How can we use this model? Let's do some forecasting!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This gives us a forecast 20 steps into the future.
  prefs: []
  type: TYPE_NORMAL
- en: Let's visualize this!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![spy_forecast.png](img/B17577_05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Forecast of prices for the SPY ticker symbol'
  prefs: []
  type: TYPE_NORMAL
- en: The solid line is the data that we know. The dotted line represents our forecast
    20 years into the future. The gray area around our forecast is the 95% confidence
    interval.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn't look too bad. It's left as an exercise to the reader to try with
    different parameters. Interesting ones to change are the trend parameter and the
    order of the model.
  prefs: []
  type: TYPE_NORMAL
- en: For the moving average, let's create different models to see the difference
    in their forecasts!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll produce the forecasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the loop, we are iterating over different q parameters, choosing 0, 10,
    and 20\. We estimate moving average models with these values of q and forecast
    20 years ahead. We also print the AIC values corresponding to each q. This is
    the output we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s plot the three forecasts similar to how we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the new plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![forecasts_qs.png](img/B17577_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Forecasts using different q parameters'
  prefs: []
  type: TYPE_NORMAL
- en: So, which one of these models is statistically better?
  prefs: []
  type: TYPE_NORMAL
- en: Let's get back to the AIC. The lower the AIC value, the better the model given
    its log-likelihood and the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the order of 10 gives us the lowest AIC, and according to this
    criterion, we should therefore choose q=10\. Of course, we only tried three different
    values. I'll leave it as an exercise to come up with a more reasonable parameter
    value for q.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that the pmdarima library has functionality for finding the optimal
    parameter values, and the SkTime library provides an implementation for automatic
    discovery of the optimal order of an ARIMA model: AutoARIMA.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on and make a forecast using an exponential smoothing model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the loop, we are iterating over different q parameters, choosing 0, 10,
    and 20\. We estimate moving average models with these values of q and forecast
    20 years ahead. We also print the AIC values corresponding to each q. This is
    what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This fits the model to our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get the forecasts for the next 20 years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s plot the forecasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![exponential_forecast.png](img/B17577_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: Exponential smoothing forecast'
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we've just looked at the graphs of 20 step-ahead forecasts. We still
    haven't been very sophisticated with an analysis of our model performance. Let's
    have a look at the errors!
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we first have to split our dataset into training and testing. We
    can do an n-step ahead forecast and check the error. We''ll just take the time-series
    running up to a certain time point as the training data, and the time points after
    that as test data, where we can compare the predictions to actual data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![theta_forecast.png](img/B17577_05_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: ThetaModel forecast'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dotted line is the prediction. It doesn''t seem to line up well with the
    actual behavior of the time-series. Let''s quantify this using one of the error
    metrics we''ve discussed in the previous chapter, *Introduction to Machine Learning
    for Time-Series*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We get a value of `37.06611385754943`. This is the root mean squared error (we
    are setting the `squared` parameter to `False`).
  prefs: []
  type: TYPE_NORMAL
- en: In forecasting competitions, such as on the Kaggle website, the lowest error
    wins. In real life, parsimony (simplicity) is important as well; however, we usually
    still aim for the lowest error (by whichever chosen metric) that we can get.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot more models to explore and to play around with, but it's time
    to conclude this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've talked about time-series forecasting based on moving
    averages and autoregression. This topic comprises a large set of models that are
    very popular in different disciplines, such as econometrics and statistics. These
    models constitute a mainstay in time-series modeling and provide state-of-the-art
    forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: We've discussed autoregression and moving averages models, and others that combine
    these two, including ARMA, ARIMA, VAR, GARCH, and others. In the practice session,
    we've applied a few models to a dataset of stock ticker prices.
  prefs: []
  type: TYPE_NORMAL

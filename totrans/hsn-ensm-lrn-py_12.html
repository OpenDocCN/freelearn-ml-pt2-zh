<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p><span>One of the most widely used unsupervised learning methods is clustering. Clustering aims to uncover structure in unlabeled data. The aim is to group together data instances, such that there is great similarity between instances of the same cluster, and little similarity between instances of different clusters. As with supervised learning methods, clustering can benefit from combining many base learners. In this chapter, we present k-means; a simple and widely used clustering algorithm. Furthermore, we discuss how ensembles can be used to improve the algorithm's performance. Finally, we use OpenEnsembles, a scikit-learn compatible Python library that implements ensemble clustering. The main topics covered in this chapter are as follows:</span></p>
<ul>
<li>How the K-means algorithm works</li>
<li>Its strengths and weaknesses</li>
<li>How ensembles can improve its performance</li>
<li>Utilizing OpenEnsembles to create clustering ensembles</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter08">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter08</a></p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/2YYzniq">http://bit.ly/2YYzniq</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consensus clustering</h1>
                </header>
            
            <article>
                
<p>Consensus clustering is an alias for ensemble learning when it is applied to clustering methods. In clustering, each base learner assigns a label to each instance, although it is not conditioned on a specific target. Instead, the base learner generates a number of clusters and assigns each instance to a cluster. The label is the cluster itself. As will be demonstrated later, two base learners, produced by the same algorithm, can generate different clusters. Thus, it is not as straightforward to combine their cluster predictions as it is to combine regression or classification predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hierarchical clustering</h1>
                </header>
            
            <article>
                
<p>Hierarchical clustering initially creates as many clusters as there are instances in the dataset. Each cluster contains only a single instance. Following this, it repeatedly finds the two clusters with the minimum distance between them (for example, the Euclidean distance), and merges them together into a new cluster. The process ends when there is only a single cluster. The method's output is a dendrogram, which indicates how instances are hierarchically organized. An example is depicted in the following figure:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e40c693f-709a-4126-ad88-1a66df010ba0.png" style="width:34.83em;height:26.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Dendrogram example</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means clustering</h1>
                </header>
            
            <article>
                
<p>K-means is a relatively simple and effective way to cluster data. The main idea is that by starting with a number of <em>K</em> points as the initial cluster centers, each instance is assigned to the nearest cluster center. Then, the centers are re-calculated as the mean point of their respective members. This process repeats until the cluster centers no longer change. The main steps are as follows:</p>
<ol>
<li>Select the number of clusters, <em>K</em></li>
<li>Select <em>K</em> random instances as the initial cluster centers</li>
<li>Assign each instance to the closest cluster center</li>
<li>Re-calculate the cluster centers as the mean of each cluster's members</li>
<li>If the new centers differ from the previous, go back to <em>Step 3</em></li>
</ol>
<p>A graphical example is depicted as follows. After four iterations, the algorithm converges:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0da7eaa9-db67-4a6a-a122-453ddd902ad6.png" style="width:35.33em;height:26.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The first four iterations on a toy dataset. Stars represent the cluster centers</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strengths and weaknesses</h1>
                </header>
            
            <article>
                
<p>K-means is a simple algorithm, both to understand, as well as to implement. Furthermore, it usually converges relatively fast, requiring small computing power. Nonetheless, it has some disadvantages. The first one is its sensitivity to the initial conditions. Depending on the examples chosen as the first cluster centers, it can require more iterations in order to converge. For example, in the following diagram we present three initial points that put the algorithm at a disadvantage. In fact, in the third iteration, two cluster centers happen to coincide:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c931f880-88ab-426c-ac8b-40fa0b478679.png" style="width:35.17em;height:26.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">An example of unfortunate initial cluster centers</div>
<p>Thus, the algorithm does not produce clusters deterministically. Another major problem is the number of clusters. This is a parameter that the data analyst must choose. There are usually three different solutions to this problem. The first concerns problems where some prior knowledge about the problem exists. Such examples are datasets where there is a need to uncover the structure of something that is known, for example, what is the driving factor behind athletes who improve their performance during a season, given their statistics? In this example, a sports coach could advise that athletes actually either improve drastically, stay the same, or deteriorate. Thus, the analyst could choose 3 as the number of clusters. Another possible solution is to experiment with different values of <em>K</em>, and measure the appropriateness of each value. This approach does not require any prior knowledge about the problem domain, but introduces the problem of measuring the appropriateness of each solution. We will see how we can solve these problems in the rest of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using scikit-learn</h1>
                </header>
            
            <article>
                
<p>The scikit-learn has a number of clustering techniques available for use. Here, we briefly present how to use K-means. The algorithm is implemented in the <kbd>KMeans</kbd> class, which is contained in the <kbd>sklearn.cluster</kbd> package. This package contains all the clustering algorithms that are available in scikit-learn. In this chapter, we will use mainly K-means, as it is one of the most intuitive algorithms. Furthermore, the techniques used in this chapter can be applied to almost any clustering algorithm. For this experiment, we will try to cluster breast cancer data, in order to explore the possibility of distinguishing malignant cases from benign cases. In order to better visualize the results, we will first perform a <strong>t-Distributed Stochastic Neighbor Embedding</strong> (<strong>t-SNE</strong>) decomposition, and use the two-dimensional embeddings as features. In order to proceed, we first load the required data and libraries, as well as set the seed for the NumPy random number generator:</p>
<div class="packt_infobox">You can read more about t-SNE at <a href="https://lvdmaaten.github.io/tsne/">https://lvdmaaten.github.io/tsne/</a>.</div>
<pre>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>from sklearn.cluster import KMeans<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.manifold import TSNE<br/><br/>np.random.seed(123456)<br/><br/>bc = load_breast_cancer()</pre>
<p>Following this, we instantiate t-SNE, and transform our data. We plot the data in order to visually inspect and examine the data structure:</p>
<pre>data = tsne.fit_transform(bc.dataa)<br/>reds = bc.target == 0<br/>blues = bc.target == 1<br/>plt.scatter(data[reds, 0], data[reds, 1], label='malignant')<br/>plt.scatter(data[blues, 0], data[blues, 1], label='benign')<br/>plt.xlabel('1st Component')<br/>plt.ylabel('2nd Component')<br/>plt.title('Breast Cancer dataa')<br/>plt.legend()</pre>
<p>The preceding code generates the following plot. We observe two distinct areas. The area populated by the blue points denotes embedding values that imply a high risk that the tumor is malignant:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0ed097e9-49f9-42ea-a391-f349b0fa5e68.png" style="width:35.67em;height:26.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Plot of the two embeddings (components) of the breast cancer data</div>
<p>As we have identified that there exists some structure in the data, we will try to use K-means clustering in order to model it. By intuition, we assume that two clusters would suffice, as we try to separate two distinct regions, and we know that there are two classes in the dataset. Nonetheless, we will also experiment with four and six clusters, as they might provide more insight on the data. We will measure the percentage of each class assigned to each cluster, in order to gauge their quality. We do this by populating the <kbd>classified</kbd> dictionary. Each key corresponds to a cluster. Each key also points to a second dictionary, where the number of malignant and benign cases are recorded for the specific cluster. Furthermore, we plot the cluster assignments, as we want to see how the data is distributed among the clusters:</p>
<pre>plt.figure()<br/>plt.title('2, 4, and 6 clusters.')<br/>for clusters in [2, 4, 6]:<br/> km = KMeans(n_clusters=clusters)<br/> preds = km.fit_predict(data)<br/> plt.subplot(1, 3, clusters/2)<br/> plt.scatter(*zip(*data), c=preds)<br/><br/>classified = {x: {'m': 0, 'b': 0} for x in range(clusters)}<br/><br/>for i in range(len(data)):<br/> cluster = preds[i]<br/> label = bc.target[i]<br/> label = 'm' if label == 0 else 'b'<br/> classified[cluster][label] = classified[cluster][label]+1<br/><br/>print('-'*40)<br/>for c in classified:<br/> print('Cluster %d. Malignant percentage: ' % c, end=' ')<br/> print(classified[c], end=' ')<br/> print('%.3f' % (classified[c]['m'] /<br/> (classified[c]['m'] + classified[c]['b']))) </pre>
<p><span>The results are depicted on the following table</span> <span>and figure:</span></p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Cluster</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Malignant</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Benign</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Malignant percentage</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>2 clusters</strong></td>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>0</strong></td>
<td class="CDPAlignCenter CDPAlign">206</td>
<td class="CDPAlignCenter CDPAlign">97</td>
<td class="CDPAlignCenter CDPAlign">0.68</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>1</strong></td>
<td class="CDPAlignCenter CDPAlign">6</td>
<td class="CDPAlignCenter CDPAlign">260</td>
<td class="CDPAlignCenter CDPAlign">0.023</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>4 clsuters</strong></td>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>0</strong></td>
<td class="CDPAlignCenter CDPAlign">2</td>
<td class="CDPAlignCenter CDPAlign">124</td>
<td class="CDPAlignCenter CDPAlign">0.016</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>1</strong></td>
<td class="CDPAlignCenter CDPAlign">134</td>
<td class="CDPAlignCenter CDPAlign">1</td>
<td class="CDPAlignCenter CDPAlign">0.993</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>2</strong></td>
<td class="CDPAlignCenter CDPAlign">72</td>
<td class="CDPAlignCenter CDPAlign">96</td>
<td class="CDPAlignCenter CDPAlign">0.429</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>3</strong></td>
<td class="CDPAlignCenter CDPAlign">4</td>
<td class="CDPAlignCenter CDPAlign">136</td>
<td class="CDPAlignCenter CDPAlign">0.029</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong><span>6 clusters</span></strong></td>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>0</strong></td>
<td class="CDPAlignCenter CDPAlign">2</td>
<td class="CDPAlignCenter CDPAlign">94</td>
<td class="CDPAlignCenter CDPAlign">0.021</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>1</strong></td>
<td class="CDPAlignCenter CDPAlign">81</td>
<td class="CDPAlignCenter CDPAlign">10</td>
<td class="CDPAlignCenter CDPAlign">0.89</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>2</strong></td>
<td class="CDPAlignCenter CDPAlign">4</td>
<td class="CDPAlignCenter CDPAlign">88</td>
<td class="CDPAlignCenter CDPAlign">0.043</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>3</strong></td>
<td class="CDPAlignCenter CDPAlign">36</td>
<td class="CDPAlignCenter CDPAlign">87</td>
<td class="CDPAlignCenter CDPAlign">0.0293</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>4</strong></td>
<td class="CDPAlignCenter CDPAlign">0</td>
<td class="CDPAlignCenter CDPAlign">78</td>
<td class="CDPAlignCenter CDPAlign">0</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>5</strong></td>
<td class="CDPAlignCenter CDPAlign">89</td>
<td class="CDPAlignCenter CDPAlign">0</td>
<td class="CDPAlignCenter CDPAlign">1</td>
</tr>
</tbody>
</table>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span><br/>
Distribution of malignant and benign cases among the clusters</span></div>
<p><span>We observe that the algorithm is able to separate the instances belonging to each class quite effectively, even though it has no information about the labels:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/15320348-66ea-44be-b485-b046e535cca0.png" style="width:41.58em;height:13.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Cluster assignment of each instance; 2, 4, and 6 clusters</div>
<p>Furthermore, we see that as we increase the number of clusters, the instances assigned to dominantly malignant or benign clusters does not increase, but the regions are better separated. This enables greater granularity and a more accurate prediction of probability that a selected instance belongs to either class. If we repeat the experiment without transforming the data, we get the following results:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Cluster</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Malignant</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Benign</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Malignant percentage</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>2 clusters</strong></td>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>0</strong></td>
<td class="CDPAlignCenter CDPAlign">82</td>
<td class="CDPAlignCenter CDPAlign">356</td>
<td class="CDPAlignCenter CDPAlign">0.187</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>1</strong></td>
<td class="CDPAlignCenter CDPAlign">130</td>
<td class="CDPAlignCenter CDPAlign">1</td>
<td class="CDPAlignCenter CDPAlign">0.992</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>4 clusters</strong></td>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>0</strong></td>
<td class="CDPAlignCenter CDPAlign">6</td>
<td class="CDPAlignCenter CDPAlign">262</td>
<td class="CDPAlignCenter CDPAlign">0.022</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>1</strong></td>
<td class="CDPAlignCenter CDPAlign">100</td>
<td class="CDPAlignCenter CDPAlign">1</td>
<td class="CDPAlignCenter CDPAlign">0.99</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>2</strong></td>
<td class="CDPAlignCenter CDPAlign">19</td>
<td class="CDPAlignCenter CDPAlign">0</td>
<td class="CDPAlignCenter CDPAlign">1</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>3</strong></td>
<td class="CDPAlignCenter CDPAlign">87</td>
<td class="CDPAlignCenter CDPAlign">94</td>
<td class="CDPAlignCenter CDPAlign">0.481</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>6 clusters</strong></td>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>0</strong></td>
<td class="CDPAlignCenter CDPAlign">37</td>
<td class="CDPAlignCenter CDPAlign">145</td>
<td class="CDPAlignCenter CDPAlign">0.203</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>1</strong></td>
<td class="CDPAlignCenter CDPAlign">37</td>
<td class="CDPAlignCenter CDPAlign">0</td>
<td class="CDPAlignCenter CDPAlign">1</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>2</strong></td>
<td class="CDPAlignCenter CDPAlign">11</td>
<td class="CDPAlignCenter CDPAlign">0</td>
<td class="CDPAlignCenter CDPAlign">1</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>3</strong></td>
<td class="CDPAlignCenter CDPAlign">62</td>
<td class="CDPAlignCenter CDPAlign">9</td>
<td class="CDPAlignCenter CDPAlign">0.873</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>4</strong></td>
<td class="CDPAlignCenter CDPAlign">5</td>
<td class="CDPAlignCenter CDPAlign">203</td>
<td class="CDPAlignCenter CDPAlign">0.024</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>5</strong></td>
<td class="CDPAlignCenter CDPAlign">60</td>
<td class="CDPAlignCenter CDPAlign">0</td>
<td class="CDPAlignCenter CDPAlign">1</td>
</tr>
</tbody>
</table>
<div class="packt_figref"/>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Clustering results on the data without t-sne transform</span></div>
<p>There are also two metrics that can be used in order to determine cluster quality. For data where the ground truth is known (essentially, labeled data), homogeneity measures the rate by which each cluster is dominated by a single class. For data where the ground truth is not known, the silhouette coefficient measures the intra-cluster cohesiveness and the inter-cluster separability. These metrics are implemented in scikit-learn under the <kbd>metrics</kbd> package, by the <kbd>silhouette_score</kbd> and <kbd>homogeneity_score</kbd> functions. The two metrics for each method are depicted in the following table. Homogeneity is higher for the transformed data, but the silhouette score is lower.</p>
<p>This is expected, as the transformed data has only two dimensions, thus making the possible distance between the instances themselves smaller:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Metric</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Clusters</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Raw data</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Transformed data</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Homogeneity</strong></td>
<td class="CDPAlignCenter CDPAlign">2</td>
<td class="CDPAlignCenter CDPAlign">0.422</td>
<td class="CDPAlignCenter CDPAlign">0.418</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">4</td>
<td class="CDPAlignCenter CDPAlign">0.575</td>
<td class="CDPAlignCenter CDPAlign">0.603</td>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">6</td>
<td class="CDPAlignCenter CDPAlign">0.620</td>
<td class="CDPAlignCenter CDPAlign">0.648</td>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Silhouette</strong></td>
<td class="CDPAlignCenter CDPAlign">2</td>
<td class="CDPAlignCenter CDPAlign">0.697</td>
<td class="CDPAlignCenter CDPAlign">0.500</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">4</td>
<td class="CDPAlignCenter CDPAlign">0.533</td>
<td class="CDPAlignCenter CDPAlign">0.577</td>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">6</td>
<td class="CDPAlignCenter CDPAlign">0.481</td>
<td class="CDPAlignCenter CDPAlign">0.555</td>
<td class="CDPAlignCenter CDPAlign"/>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Homogeneity and silhouette scores for clusterings of the raw and transformed data</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using voting</h1>
                </header>
            
            <article>
                
<p>Voting can be utilized in order to combine different clusterings of the same dataset. It is similar to voting for supervised learning, as each model (base learner) contributes to the final result with a vote. Here arises a problem of linking two clusters originating from two different clusterings. As each model will produce different clusters with different centers, we have to link similar clusters originating from different models. This is accomplished by linking together clusters that share the greatest number of instances. For example, assume that the following <span>table and figure</span> clusterings have occurred for a particular dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7cf15245-bc15-4d72-a42f-9c14cc35d94e.png" style="width:43.00em;height:13.75em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Three distinct clustering results</div>
<p>The following table depicts each instance's cluster assignments for the three different clusterings.</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Instance</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>4</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>5</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>6</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>7</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>8</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>9</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>10</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Clustering 1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Clustering 2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Clustering 3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Cluster membership of each instance</span></div>
<p>Using the preceding mapping, we can calculate the co-association matrix for each instance. This matrix indicates how many times a pair of instances has been assigned to the same cluster:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Instances</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>4</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>5</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>6</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>7</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>8</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>9</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>10</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>4</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>5</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>6</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>7</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>8</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>9</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>10</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Co-association matrix for the previous example</span></div>
<p>By dividing each element with the number of base learners in the ensemble, and clustering together samples that have a value greater than 0.5, we get the following cluster assignments:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Instance</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>4</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>5</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>6</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>7</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>8</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>9</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>10</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Voting clustering</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>The voting cluster memberships</span></div>
<p>As it is evident, the clustering is more stable. Furthermore, it is apparent that two clusters are sufficient for this dataset. By plotting the data and their cluster membership, we can see that there are two distinct groups, which is exactly what the voting ensemble was able to model, although each base learner generated three distinct cluster centers:</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img src="assets/fc93b6c8-3fc2-4663-9673-7b5421a6e4c1.png" style="width:45.25em;height:33.75em;"/></div>
<div class="CDPAlignCenter packt_figref CDPAlign">Final cluster memberships for the voting ensemble</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using OpenEnsembles</h1>
                </header>
            
            <article>
                
<p>OpenEnsembles is a Python library that is dedicated to ensemble methods for clustering. In this section, we will present its usage and utilize it in order to cluster some of our example datasets. In order to install the library, the <kbd>pip install openensembles</kbd> command must be executed in the Terminal. Although it leverages scikit-learn, its interface is different. One major difference is that data must be passed as a <kbd>data</kbd> class, implemented by OpenEnsembles. The constructor has two input parameters: a pandas <kbd>DataFrame</kbd> which contains the data, and a list which contains the feature names:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>import openensembles as oe<br/>import pandas as pd<br/>import sklearn.metrics<br/><br/>from sklearn.datasets import load_breast_cancer<br/><br/>bc = load_breast_cancer()<br/><br/># --- SECTION 2 ---<br/># Create the data object<br/>cluster_data = oe.data(pd.DataFrame(bc.data), bc.feature_names)</pre>
<p>In order to create a <kbd>cluster</kbd> ensemble, a <kbd>cluster</kbd> class object is created, passing the data as the parameter:</p>
<pre>ensemble = oe.cluster(cluster_data)</pre>
<p>In this example, we will calculate the homogeneity score for a number of <em>K</em> values and ensemble sizes. In order to add a base learner to the ensemble, the <kbd>cluster</kbd> method of the <kbd>cluster</kbd> class must be called. The method accepts as arguments, <kbd>source_name</kbd>, which denotes the source data matrix name, <kbd>algorithm</kbd>. This dictates what algorithm the base learners will utilize, <kbd>output_name</kbd>, which will be the dictionary key for accessing the results of the specific base learner and <kbd>K</kbd>, the number of clusters for the specific base learner. Finally, in order to compute the final cluster memberships through majority voting, the <kbd>finish_majority_vote</kbd> method must be called. The only parameter that must be specified is the <kbd>threshold</kbd> value:</p>
<pre># --- SECTION 3 ---<br/># Create the ensembles and calculate the homogeneity score<br/>for K in [2, 3, 4, 5, 6, 7]:<br/> for ensemble_size in [3, 4, 5]:<br/> ensemble = oe.cluster(cluster_data)<br/> for i in range(ensemble_size):<br/> name = f'kmeans_{ensemble_size}_{i}'<br/> ensemble.cluster('parent', 'kmeans', name, K)<br/><br/>preds = ensemble.finish_majority_vote(threshold=0.5)<br/>print(f'K: {K}, size {ensemble_size}:', end=' ')<br/>print('%.2f' % sklearn.metrics.homogeneity_score(<br/> bc.target, preds.labels['majority_vote']))</pre>
<p>It is evident that five clusters produce the best results for all three ensemble sizes. <span>The results are summarized in the following table:</span></p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>K</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Size</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Homogeneity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.45</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.47</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.47</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.61</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.35</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.47</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.35</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.27</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.63</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.37</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>OpenEnsembles majority vote cluster homogeneity for the breast cancer dataset</span></div>
<p>If we transform the data into two embeddings with t-SNE, and repeat the experiment, we get the following homogeneity scores:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>K</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Size</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Homogeneity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.59</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.59</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.59</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.61</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.61</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.61</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.61</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.61</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.61</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.65</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.65</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.65</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.66</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.66</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.66</p>
</td>
</tr>
</tbody>
</table>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span>Majority vote cluster homogeneity for the transformed breast cancer dataset</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using graph closure and co-occurrence linkage</h1>
                </header>
            
            <article>
                
<p>Two other methods that can be used to combine cluster results are graph closure and co-occurrence linkage. Here, we demonstrate how to use OpenEnsembles to create both types of ensembles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Graph closure</h1>
                </header>
            
            <article>
                
<p>Graph closure creates a graph from the co-occurrence matrix. Every element (instance pair) is treated as a node. Pairs that have a higher value than the threshold are connected by an edge. Following this, a clique formation occurs, according to a specified size (specified by the number of nodes in the clique). Cliques are subsets of the graph's nodes, such that every two nodes of the clique are neighbors. Finally, the cliques are combined to form unique clusters. In OpenEnsembles, it is implemented by the <kbd>finish_graph_closure</kbd> function, in the <kbd>cluster</kbd> class. The <kbd>clique_size</kbd> parameter determines the number of nodes in each clique. The <kbd>threshold</kbd> parameter determines the minimum co-occurrence that a pair must have in order to be connected by an edge in the graph. Similar to the previous example, we will use graph closure in order to cluster the breast cancer dataset. Notice that the only change in the code will be the usage of <kbd>finish_graph_closure</kbd>, instead of <kbd>finish_majority_vote</kbd>. First, we load the libraries and the dataset, and create the OpenEnsembles data object:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>import openensembles as oe<br/>import pandas as pd<br/>import sklearn.metrics<br/><br/>from sklearn.datasets import load_breast_cancer<br/><br/>bc = load_breast_cancer()<br/><br/># --- SECTION 2 ---<br/># Create the data object<br/>cluster_data = oe.data(pd.DataFrame(bc.data), bc.feature_names)</pre>
<p>Then, we create the ensemble and use <kbd>graph_closure</kbd> in order to combine the cluster results. Notice that the dictionary key also changes to <kbd>'graph_closure'</kbd>:</p>
<pre># --- SECTION 3 ---<br/># Create the ensembles and calculate the homogeneity score<br/>for K in [2, 3, 4, 5, 6, 7]:<br/> for ensemble_size in [3, 4, 5]:<br/> ensemble = oe.cluster(cluster_data)<br/> for i in range(ensemble_size):<br/> name = f'kmeans_{ensemble_size}_{i}'<br/> ensemble.cluster('parent', 'kmeans', name, K)<br/><br/>preds = ensemble.finish_majority_vote(threshold=0.5)<br/>print(f'K: {K}, size {ensemble_size}:', end=' ')<br/>print('%.2f' % sklearn.metrics.homogeneity_score(<br/> bc.target, preds.labels['majority_vote']))</pre>
<p>The effect of <em>K</em> and the ensemble size on the clustering quality is similar to majority voting, although it does not achieve the same level of performance. <span>The results are depicted in the following table:</span></p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>K</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Size</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Homogeneity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.47</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.47</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.5</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.5</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.03</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.62</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.63</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.27</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.27</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign">Homogeneity for graph closure clustering on the raw breast cancer data</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Co-occurrence matrix linkage</h1>
                </header>
            
            <article>
                
<p>Co-occurrence matrix linkage treats the co-occurrence matrix as a distance matrix between instances, and utilizes the distances in order to perform hierarchical clustering. The clustering stops when there is no element on the matrix with a value greater than the threshold. Again, we repeat the example. We use the <kbd>finish_co_occ_linkage</kbd> function to utilize co-occurrence matrix linkage with <kbd>threshold=0.5</kbd>, and use the <kbd>'co_occ_linkage'</kbd> key to access the results:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>import openensembles as oe<br/>import pandas as pd<br/>import sklearn.metrics<br/><br/>from sklearn.datasets import load_breast_cancer<br/><br/>bc = load_breast_cancer()<br/><br/># --- SECTION 2 ---<br/># Create the data object<br/>cluster_data = oe.data(pd.DataFrame(bc.data), bc.feature_names)<br/><br/># --- SECTION 3 ---<br/># Create the ensembles and calculate the homogeneity score<br/>for K in [2, 3, 4, 5, 6, 7]:<br/> for ensemble_size in [3, 4, 5]:<br/>  ensemble = oe.cluster(cluster_data)<br/>  for i in range(ensemble_size):<br/>  name = f'kmeans_{ensemble_size}_{i}'<br/>  ensemble.cluster('parent', 'kmeans', name, K)<br/>  preds = ensemble.finish_co_occ_linkage(threshold=0.5)<br/>  print(f'K: {K}, size {ensemble_size}:', end=' ')<br/>  print('%.2f' % sklearn.metrics.homogeneity_score(<br/>        bc.target, preds.labels['co_occ_linkage']))</pre>
<p>The following table summarizes the results. Notice that it outperforms the other two methods. Furthermore, the results are more stable, and less time is required to execute it than either of the other two methods:</p>
<div class="packt_figref"/>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>K</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Size</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Homogeneity</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.42</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.47</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.47</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.45</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.58</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.6</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.59</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.62</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>6</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.62</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.62</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.63</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.63</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign">Homogeneity results for co-occurrence cluster linkage on the raw breast cancer dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented the K-means clustering algorithm and clustering ensemble methods. We explained how majority voting can be used in order to combine cluster assignments from an ensemble, and how it can outperform the individual base learners. Furthermore, we presented the OpenEnsembles Python library, which is dedicated to clustering ensembles. The chapter can be summarized as follows.</p>
<p><strong>K-means</strong> creates <em>K</em> clusters, and assigns instances to each cluster by iteratively considering the cluster center to be the mean of its members. <span>It can be sensitive to the initial conditions, and the selected number of clusters. </span><span>Majority voting can help to overcome the algorithm's disadvantages. </span><span><strong>Majority voting</strong> clusters together instances that have a high co-occurrence. </span><span><strong>Co-occurrence matrices</strong> show how frequently a pair of instances has been assigned to the same cluster by the same base learner. </span><span><strong>Graph closure</strong> uses co-occurrence matrices in order to create graphs, and clusters the data based on cliques. </span><span><strong>Co-occurrence linkage</strong> uses a specific clustering algorithm, hierarchical (agglomerative) clustering, by treating the co-occurrence matrix as a pairwise distance matrix. In the next chapter, we will try to utilize all the ensemble learning techniques that we have covered in this book, in order to classify fraudulent credit card transactions.</span></p>


            </article>

            
        </section>
    </body></html>
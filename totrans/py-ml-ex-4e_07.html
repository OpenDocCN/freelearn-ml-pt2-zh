<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer262">
    <h1 class="chapterNumber">7</h1>
    <h1 class="chapterTitle" id="_idParaDest-155">Mining the 20 Newsgroups Dataset with Text Analysis Techniques</h1>
    <p class="normal">In previous chapters, we went through a bunch of fundamental machine learning concepts and supervised learning algorithms. Starting from this chapter, as the second step of our learning journey, we will be covering in detail several important unsupervised learning algorithms and techniques related to text analysis. To make our journey more interesting, we will start with a <strong class="keyWord">Natural Language Processing</strong> (<strong class="keyWord">NLP</strong>) problem—exploring the 20 newsgroups data. You will gain hands-on experience and learn how to work with text data, especially how to convert words and phrases into machine-readable values and how to clean up words with little meaning. We will also visualize text data by mapping it into a two-dimensional space in an unsupervised learning manner.</p>
    <p class="normal">We will go into detail on each of the following topics:</p>
    <ul>
      <li class="bulletList">How computers understand language – NLP</li>
      <li class="bulletList">Touring popular NLP libraries and picking up NLP basics</li>
      <li class="bulletList">Getting the newsgroups data</li>
      <li class="bulletList">Exploring the newsgroups data</li>
      <li class="bulletList">Thinking about features for text data</li>
      <li class="bulletList">Visualizing the newsgroups data with t-SNE</li>
      <li class="bulletList">Representing words with dense vectors – word embedding</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-156">How computers understand language – NLP</h1>
    <p class="normal">In <em class="italic">Chapter 1</em>, <em class="italic">Getting Started with Machine Learning and Python</em>, I mentioned that machine learning-driven programs or computers are good at discovering event patterns by processing and working with data. When the data is well structured or well defined, such as in a Microsoft Excel <a id="_idIndexMarker660"/>spreadsheet table or a relational database table, it is intuitively obvious why machine learning is better at dealing with it than humans. Computers read such data the same way as humans—for example, <code class="inlineCode">revenue: 5,000,000</code> as the revenue being 5 million, and <code class="inlineCode">age: 30</code> as the age being 30; then computers crunch assorted data and generate insights in a faster way than humans. However, when the data is unstructured, such as words with which humans communicate, news articles, or someone’s speech in another language, it seems that computers cannot understand words as well as humans do (yet). While computers have made significant progress in understanding words and natural language, they still fall short of human-level understanding in many aspects.</p>
    <h2 class="heading-2" id="_idParaDest-157">What is NLP?</h2>
    <p class="normal">There is a lot of information in the world about words, raw text, or, broadly speaking, <strong class="keyWord">natural language</strong>. This refers to any language that humans use to communicate with each other. Natural language can take<a id="_idIndexMarker661"/> various forms, including, but not limited to, the following:</p>
    <ul>
      <li class="bulletList">Text, such as a web page, SMS, emails, and menus</li>
      <li class="bulletList">Audio, such as speech and commands to Siri</li>
      <li class="bulletList">Signs and gestures</li>
      <li class="bulletList">Many other forms, such as songs, sheet music, and Morse code</li>
    </ul>
    <p class="normal">The list is endless, and we are all surrounded by natural language all of the time (that’s right, right now as you are reading this book). Given the importance of this type of unstructured data (natural language data), we must have methods to get computers to understand and reason with natural language and to extract data from it. Programs equipped with NLP techniques can already do a lot in certain areas, which already seems magical!</p>
    <p class="normal">NLP is a significant subfield of machine learning that deals with the interactions between machines (computers) and human (natural) languages. The data for NLP tasks can be in different forms, for example, text from social media posts, web pages, or even medical prescriptions, or audio from voice mails, commands to control systems, or even a favorite song or movie. Nowadays, NLP is broadly involved in our daily lives: we cannot live without machine translation, weather forecast scripts are automatically generated, we find voice search convenient, we get the answer to a question (such as “What is the population of Canada?”) quickly thanks to intelligent question-answering systems, speech-to-text technology helps people with special needs, and so on.</p>
    <p class="normal">Generative AI and its applications<a id="_idIndexMarker662"/> like ChatGPT are pushing the boundaries of NLP even further. Imagine a world where you can have a conversation with a virtual assistant that can not only answer your questions in a comprehensive way but also generate different <a id="_idIndexMarker663"/>creative text formats, like poems, code, scripts, musical pieces, emails, letters, and so on. By analyzing massive amounts of text data, it can learn the underlying patterns and structures of language, allowing it to generate human-quality text content. For instance, you could ask ChatGPT to write a funny birthday poem for your friend, craft a compelling marketing email for your business, or even brainstorm ideas for a new blog post.</p>
    <h2 class="heading-2" id="_idParaDest-158">The history of NLP</h2>
    <p class="normal">If machines are able to <a id="_idIndexMarker664"/>understand language like humans do, we consider them intelligent. In 1950, the famous mathematician Alan Turing proposed in an article, <em class="italic">Computing Machinery and Intelligence</em>, a test as a criterion of machine intelligence. It’s now called the <strong class="keyWord">Turing test</strong> (<a href="https://plato.stanford.edu/entries/turing-test/"><span class="url">https://plato.stanford.edu/entries/turing-test/</span></a>), and its <a id="_idIndexMarker665"/>goal is to examine whether a computer is able to adequately understand languages so as to fool humans into thinking that the machine is another human. It is probably no surprise to you that no computer has passed the Turing test yet, but the 1950s is considered to be when the history of NLP started.</p>
    <p class="normal">Understanding language might be difficult, but would it be easier to automatically translate texts from one language to another? On my first ever programming course, the lab booklet had the algorithm for coarse-grained machine translation. This type of translation involved looking up words in dictionaries and generating text in a new language. A more practically feasible approach would be to gather texts that are already translated by humans and train a <a id="_idIndexMarker666"/>computer program on these texts. In 1954, in the Georgetown–IBM experiment (<a href="https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment"><span class="url">https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment</span></a>), scientists claimed that machine translation would be solved in three to five years. Unfortunately, a machine translation system that can beat human expert translators does not exist yet. But machine translation has been greatly evolving since the introduction of deep learning and has seen incredible achievements in certain areas, for example, social media (Facebook open <a id="_idIndexMarker667"/>sourced a neural machine translation system, <a href="https://ai.facebook.com/tools/translate/"><span class="url">https://ai.facebook.com/tools/translate/</span></a>), real-time conversation (Microsoft Translator, SwiftKey Keyboard, and Google Pixel Buds), and image-based translation, such as Google Translate.</p>
    <p class="normal">Conversational agents, or chatbots, are another hot topic in NLP. The fact that computers are able to have a conversation with us has reshaped the way businesses are run. In 2016, Microsoft’s AI chatbot, Tay (<a href="https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/"><span class="url">https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/</span></a>), was unleashed to mimic a teenage girl and converse with users on Twitter (now X) in real time. She learned how to speak from all the things users posted and commented on Twitter. However, she was overwhelmed by tweets from trolls and automatically learned their bad behaviors and started to output inappropriate things on her feeds. She ended up being terminated within 24 hours. Generative AI models like ChatGPT are another area of active research, pushing the boundaries of what’s possible. They can be helpful for creative text formats or specific tasks, but achieving true human-level understanding in conversation remains an ongoing pursuit.</p>
    <h2 class="heading-2" id="_idParaDest-159">NLP applications</h2>
    <p class="normal">There are also several <a id="_idIndexMarker668"/>text analysis tasks that attempt to organize knowledge and concepts in such a way that they become easier for computer programs to manipulate.</p>
    <p class="normal">The way we organize and represent <a id="_idIndexMarker669"/>concepts is called <strong class="keyWord">ontology</strong>. An ontology defines concepts and relationships between concepts. For instance, we can have a so-called triple, such as (<code class="inlineCode">"python"</code>, <code class="inlineCode">"language"</code>, <code class="inlineCode">"is-a"</code>) representing the relationship between two concepts, such as <em class="italic">Python is a language</em>.</p>
    <p class="normal">An important use case for NLP at a much lower level, compared to the previous cases, is <strong class="keyWord">part-of-speech</strong> (<strong class="keyWord">PoS</strong>) <strong class="keyWord">tagging</strong>. A PoS is a<a id="_idIndexMarker670"/> grammatical word category such as a noun or verb. PoS tagging tries to determine the appropriate tag for each word in a sentence or a larger document. </p>
    <p class="normal">The following table gives examples of English PoSs:</p>
    <table class="table-container" id="table001-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Part of speech</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Examples</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Noun</p>
          </td>
          <td class="table-cell">
            <p class="normal">David, machine</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Pronoun</p>
          </td>
          <td class="table-cell">
            <p class="normal">They, her</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Adjective</p>
          </td>
          <td class="table-cell">
            <p class="normal">Awesome, amazing</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Verb</p>
          </td>
          <td class="table-cell">
            <p class="normal">Read, write</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Adverb</p>
          </td>
          <td class="table-cell">
            <p class="normal">Very, quite</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Preposition</p>
          </td>
          <td class="table-cell">
            <p class="normal">Out, at</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Conjunction</p>
          </td>
          <td class="table-cell">
            <p class="normal">And, but</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Interjection</p>
          </td>
          <td class="table-cell">
            <p class="normal">Phew, oops</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Article</p>
          </td>
          <td class="table-cell">
            <p class="normal">A, the</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 7.1: PoS examples</p>
    <p class="normal">There are a variety of real-world NLP applications involving supervised learning, such as PoS tagging, mentioned earlier, and <strong class="keyWord">sentiment analysis</strong>. A typical example is identifying news sentiment, which<a id="_idIndexMarker671"/> could be positive or negative in the binary case, or positive, neutral, or negative in <a id="_idIndexMarker672"/>multiclass classification. News sentiment analysis provides a significant signal to trading in the stock market.</p>
    <p class="normal">Another example we can easily think of is news topic classification, where classes may or may not be mutually exclusive. In the newsgroup example that we just discussed, classes are mutually exclusive (despite slight overlapping), such as technology, sports, and religion. It is, however, good to realize that a news article can be occasionally assigned multiple categories (multi-label classification). For example, an article about the Olympic Games may be labeled sports and politics if there is unexpected political involvement.</p>
    <p class="normal">Finally, an interesting application that is perhaps unexpected is <strong class="keyWord">Named Entity Recognition</strong> (<strong class="keyWord">NER</strong>). Named entities are<a id="_idIndexMarker673"/> phrases of definitive categories, such as names of persons, companies, geographic locations, dates and times, quantities, and monetary values. NER is an important subtask of information extraction to seek and identify such entities. For example, we can conduct NER on the following sentence: <code class="inlineCode">SpaceX[Organization]</code>, a <code class="inlineCode">California[Location]</code>-based company founded by a famous tech entrepreneur <code class="inlineCode">Elon Musk[Person]</code>, announced that it would manufacture the next-generation, <code class="inlineCode">9[Quantity]</code>-meter-diameter launch <a id="_idIndexMarker674"/>vehicle and spaceship for the first orbital flight in <code class="inlineCode">2020[Date]</code>.</p>
    <p class="normal">Other key NLP applications include:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Language translation</strong>: NLP powers machine translation systems, enabling automatic translation of text or <a id="_idIndexMarker675"/>speech from one language to another. Platforms like Google Translate and Microsoft Translator utilize NLP to provide real-time translation services.</li>
      <li class="bulletList"><strong class="keyWord">Speech recognition</strong>: NLP is essential in speech recognition systems, converting spoken language into <a id="_idIndexMarker676"/>written text. Virtual assistants like Siri, Alexa, and Google Assistant rely on NLP to understand user commands and respond appropriately.</li>
      <li class="bulletList"><strong class="keyWord">Text summarization</strong>: NLP can automatically generate concise summaries of lengthy texts, providing a quick <a id="_idIndexMarker677"/>overview of the content. Text summarization is useful for information retrieval and content curation.</li>
      <li class="bulletList"><strong class="keyWord">Language generation</strong>: NLP<a id="_idIndexMarker678"/> models, such as <strong class="keyWord">Generative Pre-trained Transformers</strong> (<strong class="keyWord">GPTs</strong>), can generate <a id="_idIndexMarker679"/>human-like text, including creative writing, poetry, and dialogue generation.</li>
      <li class="bulletList"><strong class="keyWord">Information retrieval</strong>: NLP assists in<a id="_idIndexMarker680"/> information retrieval from large volumes of unstructured data, such as web pages, documents, and news articles. Search engines use NLP techniques to understand user queries and retrieve relevant results.</li>
      <li class="bulletList"><strong class="keyWord">Chatbots, question answering, and virtual assistants</strong>: NLP powers chatbots and virtual assistants to <a id="_idIndexMarker681"/>provide interactive and conversational experiences. These systems can answer queries, assist with tasks, and guide users through various processes.</li>
    </ul>
    <p class="normal">In the next chapter, we will discuss how unsupervised learning, including clustering and topic modeling, is applied to text data. We will begin by covering NLP basics in the upcoming sections of this chapter.</p>
    <h1 class="heading-1" id="_idParaDest-160">Touring popular NLP libraries and picking up NLP basics</h1>
    <p class="normal">Now that we have covered a short list of<a id="_idIndexMarker682"/> real-world applications of NLP, we will be touring the essential stack of Python NLP libraries. These packages handle a wide range of NLP tasks, as mentioned previously, including sentiment analysis, text classification, and NER.</p>
    <h2 class="heading-2" id="_idParaDest-161">Installing famous NLP libraries</h2>
    <p class="normal">The most famous NLP libraries in Python<a id="_idIndexMarker683"/> include the <strong class="keyWord">Natural Language Toolkit</strong> (<strong class="keyWord">NLTK</strong>), <strong class="keyWord">spaCy</strong>, <strong class="keyWord">Gensim</strong>, and <strong class="keyWord">TextBlob.</strong> The scikit-learn library also has impressive NLP-related features. Let’s take a look at them in more detail:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">NLTK</strong>: This library (<a href="http://www.nltk.org/"><span class="url">http://www.nltk.org/</span></a>) was originally developed for educational purposes and is now widely used in industry as well. It is said that you can’t talk about NLP without mentioning NLTK. It is one of the most famous and leading <a id="_idIndexMarker684"/>platforms for building Python-based NLP applications. You can install it simply by running the following command line in the terminal:
</li>
    </ul>
        <pre class="programlisting con-one"><code class="hljs-con">sudo pip install -U nltk
</code></pre>
      
    <p class="normal-one">If you’re using <code class="inlineCode">conda</code>, execute the following command line:</p>
    <pre class="programlisting con-one"><code class="hljs-con">conda install nltk
</code></pre>
    <ul>
      <li class="bulletList"><strong class="keyWord">spaCy</strong>: This library (<code class="inlineCode">https://spacy.io/</code>) is a more powerful toolkit in the industry than NLTK. This is mainly <a id="_idIndexMarker685"/>for two reasons: first, <code class="inlineCode">spaCy</code> is written in Cython, which is much more memory-optimized (now you can see where the <code class="inlineCode">Cy</code> in <code class="inlineCode">spaCy</code> comes from) and excels in NLP tasks; second, <code class="inlineCode">spaCy</code> uses state-of-the-art algorithms for core NLP problems, such as <strong class="keyWord">convolutional neural network</strong> (<strong class="keyWord">CNN</strong>) models <a id="_idIndexMarker686"/>for tagging and NER. However, it could seem advanced for beginners. In case you’re interested, here are the installation instructions.</li>
    </ul>
    <p class="normal-one">Run the following command line in the terminal:</p>
    <pre class="programlisting con-one"><code class="hljs-con">pip install -U spacy
</code></pre>
    <p class="normal-one">For <code class="inlineCode">conda</code>, execute the following command line:</p>
    <pre class="programlisting con-one"><code class="hljs-con">conda install -c conda-forge spacy
</code></pre>
    <ul>
      <li class="bulletList"><strong class="keyWord">Gensim</strong>: This library (<a href="https://radimrehurek.com/gensim/"><span class="url">https://radimrehurek.com/gensim/</span></a>), developed by Radim Rehurek, has been gaining <a id="_idIndexMarker687"/>popularity over recent years. It was initially designed in 2008 to generate a list of similar articles given an article, hence the name of this library (<code class="inlineCode">generate similar</code>—&gt; <code class="inlineCode">Gensim</code>). It was later drastically improved by <a id="_idIndexMarker688"/>Radim Rehurek in terms of its efficiency and scalability. Again, you can easily install it via <code class="inlineCode">pip</code> by running the following command line:
</li>
    </ul>
        <pre class="programlisting con-one"><code class="hljs-con">pip install --upgrade gensim
</code></pre>
      
    <p class="normal-one">In the case of <code class="inlineCode">conda</code>, you can execute the following command line in the terminal:</p>
    <pre class="programlisting con-one"><code class="hljs-con">conda install -c conda-forge gensim
</code></pre>
    <div class="note-one">
      <p class="normal">You should make sure that the dependencies, NumPy and SciPy, are already installed before Gensim.</p>
    </div>
    <ul>
      <li class="bulletList"><strong class="keyWord">TextBlob</strong>: This library (<a href="https://textblob.readthedocs.io/en/dev/"><span class="url">https://textblob.readthedocs.io/en/dev/</span></a>) is a relatively new one built on top of NLTK. It simplifies NLP and text analysis with easy-to-use built-in functions <a id="_idIndexMarker689"/>and methods, as well as wrappers around common tasks. We can install <code class="inlineCode">TextBlob</code> by running the following command line in the terminal:
</li>
    </ul>
        <pre class="programlisting con-one"><code class="hljs-con">pip install -U textblob
</code></pre>
      
    <p class="normal-one">Or, for conda:</p>
    <pre class="programlisting con-one"><code class="hljs-con">conda install -c conda-forge textblob
</code></pre>
    <p class="normal-one"><code class="inlineCode">TextBlob</code> has some useful features that are not available in NLTK (currently), such as spell checking and correction, language detection, and translation.</p>
    <h3 class="heading-3" id="_idParaDest-162">Corpora</h3>
    <p class="normal">NLTK comes with over 100 collections of large and well-structured text datasets, which are called <strong class="keyWord">corpora </strong>in NLP. Here are some of the <a id="_idIndexMarker690"/>main corpora that NLTK provides:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Gutenberg Corpus</strong>: A collection of literary works from Project Gutenberg, containing thousands of books in various<a id="_idIndexMarker691"/> languages.</li>
      <li class="bulletList"><strong class="keyWord">Reuters Corpus</strong>: A collection of news<a id="_idIndexMarker692"/> articles from the Reuters newswire service, widely used for text classification and topic modeling tasks.</li>
      <li class="bulletList"><strong class="keyWord">Web and Chat Text</strong>: A collection of web text<a id="_idIndexMarker693"/> and chat conversations, providing a glimpse into informal language and internet slang.</li>
      <li class="bulletList"><strong class="keyWord">Movie Reviews Corpus</strong>: A collection <a id="_idIndexMarker694"/>of movie reviews, often used for sentiment analysis and text classification tasks.</li>
      <li class="bulletList"><strong class="keyWord">Treebank Corpus</strong>: A collection of <a id="_idIndexMarker695"/>parsed and tagged sentences from the Penn Treebank, used for training and evaluating syntactic parsers.</li>
      <li class="bulletList"><strong class="keyWord">WordNet</strong>: A lexical database of<a id="_idIndexMarker696"/> English words, containing synsets (groups of synonymous words) and hypernyms (is-a relationships).</li>
    </ul>
    <p class="normal">Corpora can be used as dictionaries for checking word occurrences and as training pools for model learning and validating. Some more useful and interesting corpora include the Web Text corpus, Twitter (X) samples, the Shakespeare corpus, Sentiment Polarity, the Names corpus (this contains lists of popular names, which we will be exploring very shortly), WordNet, and the Reuters benchmark corpus. The full list can be found at <a href="http://www.nltk.org/nltk_data"><span class="url">http://www.nltk.org/nltk_data</span></a>.</p>
    <p class="normal">Before using any of these corpus resources, we need to first download them by running the following code in the Python interpreter:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> nltk</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nltk.download()</span>
</code></pre>
    <p class="normal">A new window will pop up and ask you which collections (the <strong class="screenText">Collections</strong> tab in the following screenshot) or corpus (the <strong class="screenText">Corpora</strong> tab in the following screenshot) to download, and where to keep the data:</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" src="../Images/B21047_07_01.png"/></figure>
    <p class="packt_figref">Figure 7.1: Collections tab in the NLTK installation</p>
    <p class="normal">Installing the whole popular<a id="_idIndexMarker697"/> package is the quickest solution since it contains all the important corpora needed for your current study and future research. Installing a particular corpus, as shown in the following screenshot, is also fine:</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" src="../Images/B21047_07_02.png"/></figure>
    <p class="packt_figref">Figure 7.2: Corpora tab in the NLTK installation</p>
    <p class="normal">Once the package or corpus you want to explore is installed, you can take a look at the <strong class="keyWord">Names</strong> corpus (make <a id="_idIndexMarker698"/>sure the names corpus is installed for this example).</p>
    <p class="normal">First, import the names corpus:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> nltk.corpus </span><span class="hljs-con-keyword">import</span><span class="language-python"> names</span>
</code></pre>
    <p class="normal">We can check out the first <code class="inlineCode">10</code> names in the list:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(names.words()[:</span><span class="hljs-con-number">10</span><span class="language-python">])</span>
['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie',
'Abby', 'Abigael', 'Abigail', 'Abigale']
</code></pre>
    <p class="normal">There are, in total, <code class="inlineCode">7944</code> names, as shown in the following output derived by executing the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-built_in">len</span><span class="language-python">(names.words()))</span>
7944
</code></pre>
    <p class="normal">Other corpora are also fun to explore.</p>
    <p class="normal">Besides the easy-to-use and abundant corpora pool, more importantly, NLTK is also good at many NLP and text analysis tasks, including tokenization, PoS tagging, NER, word stemming, and lemmatization. We’ll look at these tasks next.</p>
    <h2 class="heading-2" id="_idParaDest-163">Tokenization</h2>
    <p class="normal">Given a text sequence, <strong class="keyWord">tokenization</strong> is the task of breaking it into fragments, which can be words, characters, or sentences. Certain<a id="_idIndexMarker699"/> characters are usually removed, such as punctuation marks, digits, and emoticons. The remaining fragments are the so-called <strong class="keyWord">tokens</strong> used <a id="_idIndexMarker700"/>for further processing.</p>
    <p class="normal">Tokens composed of one word <a id="_idIndexMarker701"/>are also called <strong class="keyWord">unigrams</strong> in computational<a id="_idIndexMarker702"/> linguistics; <strong class="keyWord">bigrams</strong> are composed of two consecutive words; <strong class="keyWord">trigrams</strong> of three<a id="_idIndexMarker703"/> consecutive words; and <strong class="keyWord">n-grams</strong> of <em class="italic">n</em> consecutive words. Here is an<a id="_idIndexMarker704"/> example of tokenization:</p>
    <figure class="mediaobject"><img alt="A picture containing text, font, line, number  Description automatically generated" src="../Images/B21047_07_03.png"/></figure>
    <p class="packt_figref">Figure 7.3: Tokenization example</p>
    <p class="normal">We can implement word-based <a id="_idIndexMarker705"/>tokenization using the <code class="inlineCode">word_tokenize</code> function in NLTK. We will use the input text <code class="inlineCode">'''I am reading a book.</code>, and on the next line, <code class="inlineCode">It is Python Machine Learning By Example,</code>, then <code class="inlineCode">4th edition.'''</code>, as an example, as shown in the following commands:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> nltk.tokenize </span><span class="hljs-con-keyword">import</span><span class="language-python"> word_tokenize</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sent = </span><span class="hljs-con-string">'''I am reading a book.</span>
<span class="hljs-con-meta">...</span> <span class="hljs-con-string">          It is Python Machine Learning By Example,</span>
<span class="hljs-con-meta">...</span> <span class="hljs-con-string">          4th edition.'''</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(word_tokenize(sent))</span>
['I', 'am', 'reading', 'a', 'book', '.', 'It', 'is', 'Python', 'Machine', 'Learning', 'By', 'Example', ',', '3rd', 'edition', '.']
</code></pre>
    <p class="normal">Word tokens are obtained.</p>
    <p class="normal">The <code class="inlineCode">word_tokenize</code> function keeps punctuation marks and digits, and only discards whitespaces and newlines.</p>
    <p class="normal">You might think word tokenization is simply splitting a sentence by space and punctuation. Here’s an interesting example showing that tokenization is more complex than you think:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sent2 = </span><span class="hljs-con-string">'I have been to U.K. and U.S.A.'</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(word_tokenize(sent2))</span>
['I', 'have', 'been', 'to', 'U.K.', 'and', 'U.S.A', '.']
</code></pre>
    <p class="normal">The tokenizer accurately recognizes the words <code class="inlineCode">'U.K.'</code> and <code class="inlineCode">'U.S.A'</code> as tokens instead of <code class="inlineCode">'U'</code> and <code class="inlineCode">'.'</code> followed by <code class="inlineCode">'K'</code>, for example.</p>
    <p class="normal">spaCy also has an outstanding tokenization feature. It uses an accurately trained model that is constantly updated. To install it, we can run the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">python -m spacy download en_core_web_sm
</code></pre>
    <p class="normal">Then, we load the <code class="inlineCode">en_core_web_sm</code> model (if you have not<a id="_idIndexMarker706"/> downloaded the model, you can run <code class="inlineCode">python -m spacy download en_core_web_sm</code> to do so) and parse the sentence using this model:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> spacy</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nlp = spacy.load(</span><span class="hljs-con-string">'en_core_web_sm'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tokens2 = nlp(sent2)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">([token.text </span><span class="hljs-con-keyword">for</span><span class="language-python"> token </span><span class="hljs-con-keyword">in</span><span class="language-python"> tokens2])</span>
['I', 'have', 'been', 'to', 'U.K.', 'and', 'U.S.A.']
</code></pre>
    <p class="normal">We can also segment text based on sentences. For example, in the same input text, using the <code class="inlineCode">sent_tokenize</code> function from NLTK, we have the following commands:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> nltk.tokenize </span><span class="hljs-con-keyword">import</span><span class="language-python"> sent_tokenize</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(sent_tokenize(sent))</span>
['I am reading a book.',
'It's Python Machine Learning By Example,\n          4th edition.']
</code></pre>
    <p class="normal">Two sentence-based tokens are returned, as there are two sentences in the input text.</p>
    <h2 class="heading-2" id="_idParaDest-164">PoS tagging</h2>
    <p class="normal">We can apply an off-the-shelf tagger from NLTK or<a id="_idIndexMarker707"/> combine multiple taggers to customize the tagging process. It is easy to directly use the built-in tagging function, <code class="inlineCode">pos_tag</code>, as in <code class="inlineCode">pos_tag(input_tokens)</code>, for instance, but behind the scenes, it is actually a prediction from a pre-built supervised learning model. The model is trained based on a large corpus composed of words that are correctly tagged.</p>
    <p class="normal">Reusing an earlier example, we can perform PoS tagging as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> nltk</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tokens = word_tokenize(sent)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(nltk.pos_tag(tokens))</span>
[('I', 'PRP'), ('am', 'VBP'), ('reading', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('Python', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('By', 'IN'), ('Example', 'NNP'), (',', ','), ('4th', 'CD'), ('edition', 'NN'), ('.', '.')]
</code></pre>
    <p class="normal">The PoS tag following each token is returned. We can check the meaning of a tag using the <code class="inlineCode">help</code> function. Looking up <code class="inlineCode">PRP</code> and <code class="inlineCode">VBP</code>, for example, gives us the following output:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nltk.</span><span class="hljs-con-built_in">help</span><span class="language-python">.upenn_tagset(</span><span class="hljs-con-string">'PRP'</span><span class="language-python">)</span>
PRP: pronoun, personal
   hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nltk.</span><span class="hljs-con-built_in">help</span><span class="language-python">.upenn_tagset(</span><span class="hljs-con-string">'VBP'</span><span class="language-python">)</span>
VBP: verb, present tense, not 3rd person singular
   predominate wrap resort sue twist spill cure lengthen brush terminate appear tend stray glisten obtain comprise detest tease attract emphasize mold postpone sever return wag ...
</code></pre>
    <p class="normal">In spaCy, getting a PoS tag is also easy. The <code class="inlineCode">token</code> object parsed from an input sentence has an attribute called <code class="inlineCode">pos_</code>, which is<a id="_idIndexMarker708"/> the tag we are looking for. Let’s print <code class="inlineCode">pos_</code> for each token, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">([(token.text, token.pos_) </span><span class="hljs-con-keyword">for</span><span class="language-python"> token </span><span class="hljs-con-keyword">in</span><span class="language-python"> tokens2])</span>
[('I', 'PRON'), ('have', 'VERB'), ('been', 'VERB'), ('to', 'ADP'), ('U.K.', 'PROPN'), ('and', 'CCONJ'), ('U.S.A.', 'PROPN')]
</code></pre>
    <p class="normal">We have just played around with PoS tagging with NLP packages. What about NER? Let’s see in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-165">NER</h2>
    <p class="normal">Given a text sequence, the NER task is to locate and <a id="_idIndexMarker709"/>identify words or phrases that are of definitive categories, such as names of persons, companies, locations, and dates. Let’s take a peep at an example of using spaCy for NER.</p>
    <p class="normal">First, tokenize an input sentence, <code class="inlineCode">The book written by Hayden Liu in 2024 was sold at $30 in America</code>, as usual, as shown in the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tokens3 = nlp(</span><span class="hljs-con-string">'The book written by Hayden Liu in 2024 was sold at $30 in America'</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">The resultant <code class="inlineCode">token</code> object contains an attribute called <code class="inlineCode">ents</code>, which are the named entities. We can extract the tagging for each recognized named entity as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">([(token_ent.text, token_ent.label_) </span><span class="hljs-con-keyword">for</span><span class="language-python"> token_ent </span><span class="hljs-con-keyword">in</span><span class="language-python"> tokens3.ents])</span>
[('Hayden Liu', 'PERSON'), ('2024', 'DATE'), ('30', 'MONEY'), ('America', 'GPE')]
</code></pre>
    <p class="normal">We can see from the results<a id="_idIndexMarker710"/> that <code class="inlineCode">Hayden Liu</code> is <code class="inlineCode">PERSON</code>, <code class="inlineCode">2024</code> is <code class="inlineCode">DATE</code>, <code class="inlineCode">30</code> is <code class="inlineCode">MONEY</code>, and <code class="inlineCode">America</code> is <code class="inlineCode">GPE</code> (country). Please refer to <a href="https://spacy.io/api/annotation#section-named-entities"><span class="url">https://spacy.io/api/annotation#section-named-entities</span></a> for a full list of named entity tags.</p>
    <h2 class="heading-2" id="_idParaDest-166">Stemming and lemmatization</h2>
    <p class="normal">Word <strong class="keyWord">stemming</strong> is a process of<a id="_idIndexMarker711"/> reverting an inflected or derived word to its root form. For instance, <em class="italic">machine</em> is the stem of <em class="italic">machines</em>, and <em class="italic">learning</em> and <em class="italic">learned</em> are generated from <em class="italic">learn</em> as their stem.</p>
    <p class="normal">The word <strong class="keyWord">lemmatization</strong> is a cautious version of stemming. It considers the PoS of a word when conducting stemming. Also, it traces <a id="_idIndexMarker712"/>back to the lemma (base or canonical form) of the word. We will discuss these two text preprocessing techniques, stemming and lemmatization, in further detail shortly. For now, let’s take a quick look at how they’re implemented respectively in NLTK by performing the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Import <code class="inlineCode">porter</code> as one of the three built-in stemming algorithms (<code class="inlineCode">LancasterStemmer</code> and <code class="inlineCode">SnowballStemmer</code> are the other two) and initialize the stemmer as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> nltk.stem.porter </span><span class="hljs-con-keyword">import</span><span class="language-python"> PorterStemmer</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">porter_stemmer = PorterStemmer()</span>
</code></pre>
      </li>
      <li class="numberedList">Stem <code class="inlineCode">machines</code> and <code class="inlineCode">learning</code>, as shown in the following code:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">porter_stemmer.stem(</span><span class="hljs-con-string">'machines'</span><span class="language-python">)</span>
'machin'
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">porter_stemmer.stem(</span><span class="hljs-con-string">'learning'</span><span class="language-python">)</span>
'learn'
</code></pre>
      </li>
    </ol>
    <div class="note-one">
      <p class="normal">Stemming sometimes involves the chopping of letters if necessary, as you can see in <code class="inlineCode">machin</code> in the preceding command output.</p>
    </div>
    <ol>
      <li class="numberedList" value="3">Now, import a lemmatization algorithm based on the built-in WordNet corpus and initialize a lemmatizer:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> nltk.stem </span><span class="hljs-con-keyword">import</span><span class="language-python"> WordNetLemmatizer</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">lemmatizer = WordNetLemmatizer()</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Similar to stemming, we lemmatize <code class="inlineCode">machines</code> and <code class="inlineCode">learning</code>:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">lemmatizer.lemmatize(</span><span class="hljs-con-string">'</span><span class="hljs-con-string">machines'</span><span class="language-python">)</span>
'machine'
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">lemmatizer.lemmatize(</span><span class="hljs-con-string">'learning'</span><span class="language-python">)</span>
'learning'
</code></pre>
    <p class="normal">Why is <code class="inlineCode">learning</code> unchanged? The algorithm defaults to finding the lemma for nouns unless you specify otherwise. If you want to treat <code class="inlineCode">learning</code> as a verb, you can specify it in <code class="inlineCode">lemmatizer.lemmatize('learning', nltk.corpus.wordnet.VERB)</code>, which will return <code class="inlineCode">learn</code>.</p>
    <h2 class="heading-2" id="_idParaDest-167">Semantics and topic modeling</h2>
    <p class="normal">Gensim is famous for its powerful semantic and topic-modeling algorithms. Topic modeling is a typical text-mining<a id="_idIndexMarker713"/> task of discovering the hidden semantic structures in a document. A semantic structure in plain English is the distribution of word occurrences. It is obviously <a id="_idIndexMarker714"/>an unsupervised learning task. What we need to do is to feed in plain text and let the model figure out the abstract topics. For example, we can use topic modeling to group product reviews on an e-commerce site based on the common themes expressed in the reviews. We will study topic modeling in detail in <em class="chapterRef">Chapter 8</em>, <em class="italic">Discovering Underlying Topics in the Newsgroups Dataset with Clustering and Topic Modeling</em>.</p>
    <p class="normal">In addition to robust semantic modeling methods, gensim also provides the following functionalities:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Word embedding</strong>: Also known as <strong class="keyWord">word vectorization</strong>, this is an innovative way to represent words while<a id="_idIndexMarker715"/> preserving words’ co-occurrence<a id="_idIndexMarker716"/> features. Later in this chapter, we will delve into a comprehensive exploration of word embedding.</li>
      <li class="bulletList"><strong class="keyWord">Similarity querying</strong>: This functionality<a id="_idIndexMarker717"/> retrieves objects that are similar to the given query object. It’s a feature built on top of word embedding.</li>
      <li class="bulletList"><strong class="keyWord">Distributed computing</strong>: This<a id="_idIndexMarker718"/> functionality makes it possible to efficiently learn from millions of documents.</li>
    </ul>
    <p class="normal">Last but not least, as mentioned in the first chapter, <em class="italic">Getting Started with Machine Learning and Python</em>, scikit-learn is the main package we have used throughout this entire book. Luckily, it provides all the text processing features we need, such as tokenization, along with comprehensive machine learning functionalities. Plus, it comes with a built-in loader for the 20 newsgroups dataset.</p>
    <p class="normal">Now that the tools are available and properly installed, what about the data?</p>
    <h1 class="heading-1" id="_idParaDest-168">Getting the newsgroups data</h1>
    <p class="normal">The project in this chapter is about<a id="_idIndexMarker719"/> the 20 newsgroups dataset. It’s composed of text taken from newsgroup articles, as its name implies. It was originally collected by Ken Lang and now has been widely used for experiments in text applications of machine learning techniques, specifically NLP techniques.</p>
    <p class="normal">The data contains approximately 20,000 documents across 20 online newsgroups. A newsgroup is a place on the internet where people can ask and answer questions about a certain topic. The data is already cleaned to a certain degree and already split into training and testing sets. The cutoff point is at a certain date.</p>
    <p class="normal">The original data comes from <a href="http://qwone.com/~jason/20Newsgroups/"><span class="url">http://qwone.com/~jason/20Newsgroups/</span></a>, with 20 different topics listed, as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">comp.graphics</code></li>
      <li class="bulletList"><code class="inlineCode">comp.os.ms-windows.misc</code></li>
      <li class="bulletList"><code class="inlineCode">comp.sys.ibm.pc.hardware</code></li>
      <li class="bulletList"><code class="inlineCode">comp.sys.mac.hardware</code></li>
      <li class="bulletList"><code class="inlineCode">comp.windows.x</code></li>
      <li class="bulletList"><code class="inlineCode">rec.autos</code></li>
      <li class="bulletList"><code class="inlineCode">rec.motorcycles</code></li>
      <li class="bulletList"><code class="inlineCode">rec.sport.baseball</code></li>
      <li class="bulletList"><code class="inlineCode">rec.sport.hockey</code></li>
      <li class="bulletList"><code class="inlineCode">sci.crypt</code></li>
      <li class="bulletList"><code class="inlineCode">sci.electronics</code></li>
      <li class="bulletList"><code class="inlineCode">sci.med</code></li>
      <li class="bulletList"><code class="inlineCode">sci.space</code></li>
      <li class="bulletList"><code class="inlineCode">misc.forsale</code></li>
      <li class="bulletList"><code class="inlineCode">talk.politics.misc</code></li>
      <li class="bulletList"><code class="inlineCode">talk.politics.guns</code></li>
      <li class="bulletList"><code class="inlineCode">talk.politics.mideast</code></li>
      <li class="bulletList"><code class="inlineCode">talk.religion.misc</code></li>
      <li class="bulletList"><code class="inlineCode">alt.atheism</code></li>
      <li class="bulletList"><code class="inlineCode">soc.religion.christian</code></li>
    </ul>
    <p class="normal">All of the documents in the dataset are in English. And we can easily deduce the topics from the newsgroups’ names.</p>
    <p class="normal">The dataset is labeled and each document is composed of text data and a group label. This also makes it a perfect fit for supervised learning, such as text classification. At the end of the chapter, feel free to practice classification on this dataset using what you’ve learned so far in this book.</p>
    <p class="normal">Some of the newsgroups are closely related or even overlapping – for instance, those five computer groups (<code class="inlineCode">comp.graphics</code>, <code class="inlineCode">comp.os.ms-windows.misc</code>, <code class="inlineCode">comp.sys.ibm.pc.hardware</code>, <code class="inlineCode">comp.sys.mac.hardware</code>, and <code class="inlineCode">comp.windows.x</code>). Some are not closely related to each other, such <a id="_idIndexMarker720"/>as Christian (<code class="inlineCode">soc.religion.christian</code>) and baseball (<code class="inlineCode">rec.sport.baseball</code>).</p>
    <p class="normal">Hence, it’s a perfect use case for unsupervised learning such as clustering, with which we can see whether similar topics are grouped together and unrelated ones are far apart. Moreover, we can even discover abstract topics beyond the original 20 labels using topic modeling techniques.</p>
    <p class="normal">For now, let’s focus on exploring and analyzing the text data. We will get started with acquiring the data.</p>
    <p class="normal">It is possible to download the dataset manually from the original website or many other online repositories. However, there are also many versions of the dataset—some are cleaned in a certain way and some are in raw form. To avoid confusion, it is best to use a consistent acquisition method. The scikit-learn library provides a utility function that loads the dataset. Once the dataset is downloaded, it’s automatically cached. We don’t need to download the same dataset twice.</p>
    <p class="normal">In most cases, caching the dataset, especially for a relatively small one, is considered a good practice. Other Python libraries also provide data download utilities, but not all of them implement automatic caching. This is another reason why we love scikit-learn.</p>
    <p class="normal">As always, we first import the loader function for the 20 newsgroups data, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.datasets </span><span class="hljs-con-keyword">import</span><span class="language-python"> fetch_20newsgroups</span>
</code></pre>
    <p class="normal">Then, we download the dataset with all the default parameters, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups = fetch_20newsgroups()</span>
Downloading 20news dataset. This may take a few minutes.
Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)
</code></pre>
    <p class="normal">We can also specify one or more certain topic groups and particular sections (training, testing, or both) and just load <a id="_idIndexMarker721"/>such a subset of data in the program. The full list of parameters and options for the loader function is summarized in the following table:</p>
    <table class="table-container" id="table002-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Parameter</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Default value</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Example values</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Description</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">subset</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">'train'</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">'train','test','all'</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">The dataset to load: the training set, the testing set, or both.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">data_home</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">~/scikit_learn_data</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">~/myfolder</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Directory where the files are stored and cached.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">categories</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">['sci.space",alt.atheism']</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">List of newsgroups to load. If <code class="inlineCode">None</code>, all newsgroups will be loaded.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">shuffle</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">True</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">True, False</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Boolean indicating whether to shuffle the data.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">random_state</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">42</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">7, 43</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Random seed integer used to shuffle the data.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">remove</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">0</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">('headers','footers','quotes')</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Tuple indicating the part(s) among “header, footer, and quote” of each newsgroup post to omit. Nothing is removed by default.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">download_if_missing</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">True</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">True, False</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Boolean indicating whether to download the data if it is not found locally.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 7.2: List of parameters of the fetch_20newsgroups() function</p>
    <p class="normal">Remember that <code class="inlineCode">random_state</code> is useful for the purpose of reproducibility. You are able to get the same dataset every<a id="_idIndexMarker722"/> time you run the script. Otherwise, working on datasets shuffled under different orders might bring in unnecessary variations.</p>
    <p class="normal">In this section, we loaded the newsgroups data. Let’s explore it next.</p>
    <h1 class="heading-1" id="_idParaDest-169">Exploring the newsgroups data</h1>
    <p class="normal">After we download the 20 <a id="_idIndexMarker723"/>newsgroups dataset by whatever means we prefer, the <code class="inlineCode">data</code> object of <code class="inlineCode">groups</code> is cached in memory. The <code class="inlineCode">data</code> object is in the form of a key-value dictionary. Its keys are as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups.keys()</span>
dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])
</code></pre>
    <p class="normal">The <code class="inlineCode">target_names</code> key gives the 20 newsgroups names:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups[</span><span class="hljs-con-string">'target_names'</span><span class="language-python">]</span>
   ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']
</code></pre>
    <p class="normal">The <code class="inlineCode">target</code> key corresponds to a newsgroup, but is encoded as an integer:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups.target</span>
array([7, 4, 4, ..., 3, 1, 8])
</code></pre>
    <p class="normal">Then, what are the<a id="_idIndexMarker724"/> distinct values for these integers? We can use the <code class="inlineCode">unique</code> function from NumPy to figure it out:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> numpy </span><span class="hljs-con-keyword">as</span><span class="language-python"> np</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">np.unique(groups.target)</span>
array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])
</code></pre>
    <p class="normal">They range from <code class="inlineCode">0</code> to <code class="inlineCode">19</code>, representing the 1st, 2nd, 3rd, …, and 20th newsgroup topics in <code class="inlineCode">groups['target_names']</code>.</p>
    <p class="normal">In the context of multiple topics or<a id="_idIndexMarker725"/> categories, it is important to know what the distribution of topics is. A balanced class distribution is the easiest to deal with because there are no under-represented or over-represented categories. However, frequently, we have a skewed distribution with one or more categories dominating.</p>
    <p class="normal">We will use<a id="_idIndexMarker726"/> the <code class="inlineCode">seaborn</code> package (<a href="https://seaborn.pydata.org/"><span class="url">https://seaborn.pydata.org/</span></a>) to compute the<a id="_idIndexMarker727"/> histogram of categories and plot it utilizing the <code class="inlineCode">matplotlib</code> package (<a href="https://matplotlib.org/"><span class="url">https://matplotlib.org/</span></a>). We can install both packages via <code class="inlineCode">pip</code> as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">python -m pip install -U matplotlib
pip install seaborn
</code></pre>
    <p class="normal">In the case of <code class="inlineCode">conda</code>, you can execute the following command line:</p>
    <pre class="programlisting con"><code class="hljs-con">conda install -c conda-forge matplotlib
conda install seaborn
</code></pre>
    <p class="normal">Remember to install <code class="inlineCode">matplotlib</code> before <code class="inlineCode">seaborn</code> as <code class="inlineCode">matplotlib</code> is one of the dependencies of the <code class="inlineCode">seaborn</code> package.</p>
    <p class="normal">Now, let’s display the distribution of the classes, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> seaborn </span><span class="hljs-con-keyword">as</span><span class="language-python"> sns</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> matplotlib.pyplot </span><span class="hljs-con-keyword">as</span><span class="language-python"> plt</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sns.histplot(groups.target, bins=</span><span class="hljs-con-number">20</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.xticks(</span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">0</span><span class="language-python">, </span><span class="hljs-con-number">20</span><span class="language-python">, </span><span class="hljs-con-number">1</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">Refer to the following<a id="_idIndexMarker728"/> screenshot for the result:</p>
    <figure class="mediaobject"><img alt="A graph of a number  Description automatically generated" src="../Images/B21047_07_04.png"/></figure>
    <p class="packt_figref">Figure 7.4: Distribution of newsgroup classes</p>
    <p class="normal">As you can see, the distribution is<a id="_idIndexMarker729"/> approximately uniform so that’s one less thing to worry about.</p>
    <div class="packt_tip">
      <p class="normal">It’s good to visualize data to get a general idea of how the data is structured, what possible issues may arise, and whether there are any irregularities that we have to take care of.</p>
    </div>
    <p class="normal">Other keys are quite self-explanatory: <code class="inlineCode">data</code> contains all newsgroup documents and <code class="inlineCode">filenames</code> stores the path where each document is located in your filesystem.</p>
    <p class="normal">Now, let’s have a look at the first document and its topic number and name by executing the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups.data[</span><span class="hljs-con-number">0</span><span class="language-python">]</span>
"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n"
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups.target[</span><span class="hljs-con-number">0</span><span class="language-python">]</span>
7
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups.target_names[groups.target[</span><span class="hljs-con-number">0</span><span class="language-python">]]</span>
'rec.autos'
</code></pre>
    <div class="note">
      <p class="normal">If <code class="inlineCode">random_state</code> isn’t fixed (<code class="inlineCode">42</code> by default), you may get different results running the preceding scripts.</p>
    </div>
    <p class="normal">As you can see, the first <a id="_idIndexMarker730"/>document is from the <code class="inlineCode">rec.autos</code> newsgroup, which was assigned the number <code class="inlineCode">7</code>. Reading this post, we can easily figure out that it’s about cars. The word <code class="inlineCode">car</code> actually occurs a number of times in the document. Words such as <code class="inlineCode">bumper</code> also seem very car-oriented. However, words such as <code class="inlineCode">doors</code> may not necessarily be car-related, as they may also be associated with home improvement or another topic.</p>
    <p class="normal">As a side note, it makes sense to not distinguish between <code class="inlineCode">doors</code> and <code class="inlineCode">door</code>, or the same word with different capitalization, such as <code class="inlineCode">Doors</code>. There are some rare cases where capitalization does matter – for instance, if we’re trying to find out whether a document is about the band called <code class="inlineCode">The Doors</code> or the more common concept, <code class="inlineCode">the doors</code> (made of wood or another material).</p>
    <h1 class="heading-1" id="_idParaDest-170">Thinking about features for text data</h1>
    <p class="normal">From the preceding analysis, we can <a id="_idIndexMarker731"/>safely conclude that if we want to figure out whether a document was from the <code class="inlineCode">rec.autos</code> newsgroup, the presence or absence of words such as <code class="inlineCode">car</code>, <code class="inlineCode">doors</code>, and <code class="inlineCode">bumper</code> can be very useful features. The presence or not of a word is a Boolean variable, and we can also look at the count of certain words. For instance, <code class="inlineCode">car</code> occurs multiple times in the document. Maybe the more times such a word is found in a text, the more likely it is that the document has something to do with cars.</p>
    <h2 class="heading-2" id="_idParaDest-171">Counting the occurrence of each word token</h2>
    <p class="normal">It seems that we are only interested in the occurrence of certain words, their count, or a related measure, and not in the order of the words. We <a id="_idIndexMarker732"/>can therefore view a text as a collection of words. This is called the <strong class="keyWord">Bag of Words</strong> (<strong class="keyWord">BoW</strong>) model. This is a very basic model but it works pretty well in practice. We can optionally define a more complex model that takes into account the order of words and PoS tags. However, such a model is going to be more computationally expensive and more difficult to program. In reality, the basic BoW model, in most cases, suffices. We can give it a shot and see whether the BoW model makes sense.</p>
    <p class="normal">We begin by converting documents into a matrix where each row represents each newsgroup document and each column represents a word token, or specifically, a unigram to begin with. The value of each element in the matrix is the number of times the word (column) occurs in the document (row). We are utilizing the <code class="inlineCode">CountVectorizer</code> class from scikit-learn to do the work:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.feature_extraction.text </span><span class="hljs-con-keyword">import</span><span class="language-python"> CountVectorizer</span>
</code></pre>
    <p class="normal">The important parameters and options for the count conversion function are summarized in the following table:</p>
    <table class="table-container" id="table003-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Constructor parameter</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Default value</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Example values</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Description</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">ngram_range</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(1,1)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(1,2), (2,2)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Lower and upper bound of the n-grams to be extracted in the input text, for example <code class="inlineCode">(1,1)</code> means unigram, <code class="inlineCode">(1,2)</code> means unigram and bigram.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">stop_words</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">'english' or list ['a','the', 'of'] or None</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Which stop word list to use: can be <code class="inlineCode">'english'</code> referring to the built-in list, or a customized input list. If <code class="inlineCode">None</code>, no words will be removed.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">lowercase</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">True</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">True, False</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Whether or not to convert all characters to lowercase.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">max_features</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None, 200, 500</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">The number of top (most frequent) tokens to consider, or all tokens if <code class="inlineCode">None</code>.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">binary</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">False</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">True, False</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">If true, all non-zero counts become 1s.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 7.3: List of parameters of the CountVectorizer() function</p>
    <p class="normal">We first initialize the count vectorizer with the <code class="inlineCode">500</code> top features (500 most frequent tokens):</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python"> count_vector = CountVectorizer(max_features=</span><span class="hljs-con-number">500</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">Use it to fit on the raw text data as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_count = count_vector.fit_transform(groups.data)</span>
</code></pre>
    <p class="normal">Now the count vectorizer<a id="_idIndexMarker733"/> captures the top 500 features and generates a token count matrix out of the original text input:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_count</span>
&lt;11314x500 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
      with 798221 stored elements in Compressed Sparse Row format&gt;
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_count[</span><span class="hljs-con-number">0</span><span class="language-python">]</span>
&lt;1x500 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
      with 53 stored elements in Compressed Sparse Row format&gt;
</code></pre>
    <p class="normal">The resulting count matrix is a sparse matrix where each row only stores non-zero elements (hence, only 798,221 elements instead of <code class="inlineCode">11314 * 500 = 5,657,000</code>). For example, the first document is converted into a sparse vector composed of <code class="inlineCode">53</code> non-zero elements.</p>
    <p class="normal">If you are interested in seeing the whole matrix, feel free to run the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_count.toarray()</span>
</code></pre>
    <p class="normal">If you just want the first row, run the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_count.toarray()[</span><span class="hljs-con-number">0</span><span class="language-python">]</span>
</code></pre>
    <p class="normal">Let’s take a look at the following output derived from the preceding command:</p>
    <figure class="mediaobject"><img alt="A number pattern with numbers  Description automatically generated" src="../Images/B21047_07_05.png"/></figure>
    <p class="packt_figref">Figure 7.5: Output of count vectorization</p>
    <p class="normal">So, what are those 500 top <a id="_idIndexMarker734"/>features? They can be found in the following output:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(count_vector. get_feature_names_out())</span>
['00' '000' '10' '100' '11' '12' '13' '14' '145' '15' '16' '17' '18' '19' '1993' '20' '21' '22' '23' '24' '25' '26' '27' '30' '32' '34' '40' '50' '93' 'a86' 'able' 'about' 'above' 'ac' 'access' 'actually' 'address' 'after'
……
……
……
 'well' 'were' 'what' 'when' 'where' 'whether' 'which' 'while' 'who' 'whole' 'why' 'will' 'win' 'window' 'windows' 'with' 'without' 'won' 'word' 'work' 'works' 'world' 'would' 'writes' 'wrong' 'wrote' 'year' 'years' 'yes' 'yet' 'you' 'your']
</code></pre>
    <p class="normal">Our first trial doesn’t look perfect. Obviously, the most popular tokens are numbers, or letters with numbers such as <code class="inlineCode">a86</code>, which do not convey important information. Moreover, there are many words that have no actual meaning, such as <code class="inlineCode">you</code>, <code class="inlineCode">the</code>, <code class="inlineCode">them</code>, and <code class="inlineCode">then</code>. Also, some words contain<a id="_idIndexMarker735"/> identical information, for example, <code class="inlineCode">tell</code> and <code class="inlineCode">told</code>, <code class="inlineCode">use</code> and <code class="inlineCode">used</code>, and <code class="inlineCode">time</code> and <code class="inlineCode">times</code>. Let’s tackle these issues.</p>
    <h2 class="heading-2" id="_idParaDest-172">Text preprocessing</h2>
    <p class="normal">We begin by retaining letter-only <a id="_idIndexMarker736"/>words so that numbers such as <code class="inlineCode">00</code> and <code class="inlineCode">000</code> and combinations of letters and numbers such as <code class="inlineCode">b8f</code> will be removed. The filter function is defined as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_cleaned = []</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> doc </span><span class="hljs-con-keyword">in</span><span class="language-python"> groups.data:</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    doc_cleaned = </span><span class="hljs-con-string">' '</span><span class="language-python">.join(word </span><span class="hljs-con-keyword">for</span><span class="language-python"> word </span><span class="hljs-con-keyword">in</span><span class="language-python"> doc.split()</span>
                                             if word.isalpha())
<span class="hljs-con-meta">...</span> <span class="language-python">    data_cleaned.append(doc_cleaned)</span>
</code></pre>
    <p class="normal">This will generate a cleaned version of the newsgroups data.</p>
    <h2 class="heading-2" id="_idParaDest-173">Dropping stop words</h2>
    <p class="normal">We didn’t talk about <code class="inlineCode">stop_words</code> as an important parameter in <code class="inlineCode">CountVectorizer</code>. <strong class="keyWord">Stop words</strong> are those common words that <a id="_idIndexMarker737"/>provide little value in helping to differentiate <a id="_idIndexMarker738"/>documents. In general, stop words add noise to the BoW model and can be removed.</p>
    <p class="normal">There’s no universal list of stop words. Hence, depending on the tools or packages you are using, you will remove different sets of stop words. Take scikit-learn as an example—you can check the list that follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.feature_extraction </span><span class="hljs-con-keyword">import</span><span class="language-python"> _stop_words</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(_stop_words.ENGLISH_STOP_WORDS)</span>
frozenset({latter', 'somewhere', 'further', 'full', 'de', 'under', 'beyond', 'than', 'must', 'has', 'him', 'hereafter', 'they', 'third', 'few', 'most', 'con', 'thereby', 'ltd', 'take', 'five', 'alone', 'yours', 'above', 'hereupon', 'seeming', 'least', 'over', 'amongst', 'everyone', 'anywhere', 'yourself', 'these', 'name', 'even', 'in', 'forty', 'part', 'perhaps', 'sometimes', 'seems', 'down', 'among', 'still', 'own', 'wherever', 'same', 'about', 'because', 'four', 'none', 'nothing', 'could'
……
……
'myself', 'except', 'whom', 'up', 'six', 'get', 'sixty', 'those', 'whither', 'once', 'something', 'elsewhere', 'my', 'both', 'another', 'one', 'a', 'hasnt', 'everywhere', 'thin', 'not', 'eg', 'someone', 'seem', 'detail', 'either', 'being'})
</code></pre>
    <p class="normal">To drop stop words from the <a id="_idIndexMarker739"/>newsgroups data, we simply just need to specify the <code class="inlineCode">stop_words</code> parameter:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">count_vector_sw = CountVectorizer(stop_words=</span><span class="hljs-con-string">"english"</span><span class="language-python">, max_features=</span><span class="hljs-con-number">500</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">Besides stop words, you may notice that names are included in the top features, such as <code class="inlineCode">andrew</code>. We can filter names with the <code class="inlineCode">Names</code> corpus from NLTK we just worked with.</p>
    <h2 class="heading-2" id="_idParaDest-174">Reducing inflectional and derivational forms of words</h2>
    <p class="normal">As mentioned earlier, we have two <a id="_idIndexMarker740"/>basic strategies to deal with words from the same root—stemming and lemmatization. Stemming is a quicker approach that involves, if necessary, chopping off letters; for example, <em class="italic">words</em> becomes <em class="italic">word</em> after stemming. The result of stemming doesn’t have to be a valid word. For instance, <em class="italic">trying</em> and <em class="italic">try</em> become <em class="italic">tri</em>. Lemmatizing, on the other hand, is slower but more accurate. It performs a dictionary lookup and guarantees to return a valid word. Recall that we implemented both stemming and lemmatization using NLTK previously.</p>
    <p class="normal">Putting all of these (preprocessing, dropping stop words, lemmatizing, and count vectorizing) together, we obtain the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">all_names = </span><span class="hljs-con-built_in">set</span><span class="language-python">(names.words())</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">get_cleaned_data</span><span class="language-python">(</span><span class="hljs-con-params">groups, lemmatizer, remove_words</span><span class="language-python">):</span>
        data_cleaned = []
        for doc in groups.data:
<span class="hljs-con-meta">...</span> <span class="language-python">        doc = doc.lower()</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        doc_cleaned = </span><span class="hljs-con-string">' '</span><span class="language-python">.join(lemmatizer.lemmatize(word)</span>
                                  for word in doc.split()
                                  if word.isalpha() and
                                  word not in remove_words)
<span class="hljs-con-meta">...</span> <span class="language-python">        data_cleaned.append(doc_cleaned)</span>
        return data_cleaned
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_cleaned = get_cleaned_data(groups, lemmatizer, all_names)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_cleaned_count = count_vector_sw.fit_transform(data_cleaned)</span>
</code></pre>
    <p class="normal">Now the features are much more meaningful:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(count_vector_sw.get_feature_names_out())</span>
['able', 'accept', 'access', 'according', 'act', 'action', 'actually', 'add', 'address', 'ago', 'agree', 'algorithm', 'allow', 'american', 'anonymous', 'answer', 'anybody', 'apple', 'application', 'apr', 'april', 'arab', 'area', 'argument', 'armenian', 'article', 'ask', 'asked',
……
……
'video', 'view', 'wa', 'want', 'wanted', 'war', 'water', 'way', 'weapon', 'week', 'went', 'western', 'white', 'widget', 'win', 'window', 'woman', 'word', 'work', 'working', 'world', 'worth', 'write', 'written', 'wrong', 'year', 'york', 'young']
</code></pre>
    <p class="normal">We have just converted text from each raw newsgroup document into a sparse vector of size <code class="inlineCode">500</code>. For a vector from a<a id="_idIndexMarker741"/> document, each element represents the number of times a word token occurs in this document. Also, these 500 word tokens are selected based on their overall occurrences after text preprocessing, the removal of stop words, and lemmatization. Now, you may ask questions such as, “Is such an occurrence vector representative enough, or does such an occurrence vector convey enough information that can be used to differentiate the document from documents on other topics?” You will see the answer in the next section.</p>
    <h1 class="heading-1" id="_idParaDest-175">Visualizing the newsgroups data with t-SNE</h1>
    <p class="normal">We can answer these questions easily by<a id="_idIndexMarker742"/> visualizing those representation vectors. If we can see that the document vectors from the same topic form a cluster, we did a good job mapping the documents into vectors. But how? They are of 500 dimensions, while we can visualize data of, <strong class="keyWord">at most</strong>, three dimensions. We can resort to t-SNE for dimensionality reduction.</p>
    <h2 class="heading-2" id="_idParaDest-176">What is dimensionality reduction?</h2>
    <p class="normal"><strong class="keyWord">Dimensionality reduction</strong> is an important <a id="_idIndexMarker743"/>machine learning technique that reduces the number of features and, at the same time, retains as much information as possible. It is usually performed by obtaining a set of new principal features.</p>
    <p class="normal">As mentioned before, it is difficult to visualize data of high dimensions. Given a three-dimensional plot, we sometimes don’t find it straightforward to observe any findings, not to mention 10, 100, or 1,000 dimensions. Moreover, some of the features in high-dimensional data may be correlated and, as a result, bring in redundancy. This is why we need dimensionality reduction.</p>
    <p class="normal">Dimensionality reduction is <a id="_idIndexMarker744"/>not simply taking out a pair of two features from the original feature space. It is transforming the original feature space into a new space of fewer dimensions. The<a id="_idIndexMarker745"/> data transformation can be linear, such as the famous one, <strong class="keyWord">Principal Component Analysis</strong> (<strong class="keyWord">PCA</strong>), which maps the data in a higher dimensional space to a lower dimensional space where the variance of the data is maximized, which we will talk about in <em class="chapterRef">Chapter 9</em>, <em class="italic">Recognizing Faces with Support Vector Machine</em>, or nonlinear, such as neural networks and t-SNE, which is coming up shortly. <strong class="keyWord">Non-negative Matrix Factorization</strong> (<strong class="keyWord">NMF</strong>) is another<a id="_idIndexMarker746"/> powerful algorithm, which we will study in detail in <em class="chapterRef">Chapter 8</em>, <em class="italic">Discovering Underlying Topics in the Newsgroups Dataset with Clustering and Topic Modeling</em>.</p>
    <p class="normal">At the end of the day, most dimensionality reduction algorithms are in the family of <strong class="keyWord">unsupervised learning</strong> as the <a id="_idIndexMarker747"/>target or label information (if available) is not used in data transformation.</p>
    <h2 class="heading-2" id="_idParaDest-177">t-SNE for dimensionality reduction</h2>
    <p class="normal"><strong class="keyWord">t-SNE</strong> stands for <strong class="keyWord">t-distributed Stochastic Neighbor Embedding</strong>. It is a popular nonlinear dimensionality reduction technique <a id="_idIndexMarker748"/>developed by Laurens van der Maaten and Geoffrey Hinton (<a href="https://www.cs.toronto.edu/~hinton/absps/tsne.pdf"><span class="url">https://www.cs.toronto.edu/~hinton/absps/tsne.pdf</span></a>). t-SNE has been widely used for data visualization in various domains, including computer vision, NLP, bioinformatics, and computational genomics.</p>
    <p class="normal">As its name implies, t-SNE embeds high-dimensional data into a low-dimensional (usually two-dimensional or three-dimensional) space while preserving the local structure and pairwise similarities of the data as much as possible. It first models a probability distribution over neighbors around data points by assigning a high probability to similar data points and an extremely small probability to dissimilar ones. Note that similarity and neighbor distances are measured by Euclidean distance or other metrics. Then, t-SNE constructs a projection onto a low-dimensional space where the divergence between the input distribution and output distribution is minimized. The original high-dimensional space is modeled as a Gaussian distribution, while the output low-dimensional space is<a id="_idIndexMarker749"/> modeled as a t-distribution.</p>
    <p class="normal">We’ll herein implement t-SNE using the <code class="inlineCode">TSNE</code> class from scikit-learn:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.manifold </span><span class="hljs-con-keyword">import</span><span class="language-python"> TSNE</span>
</code></pre>
    <p class="normal">Now, let’s use t-SNE to verify our count vector representation.</p>
    <p class="normal">We pick three distinct topics, <code class="inlineCode">talk.religion.misc</code>, <code class="inlineCode">comp.graphics</code>, and <code class="inlineCode">sci.space</code>, and visualize document vectors from these three topics.</p>
    <p class="normal">First, just load documents of these three labels, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">categories_3 = [</span><span class="hljs-con-string">'talk.religion.misc'</span><span class="language-python">, </span><span class="hljs-con-string">'comp.graphics'</span><span class="language-python">, </span><span class="hljs-con-string">'sci.space'</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups_3 = fetch_20newsgroups(categories=categories_3)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_cleaned = get_cleaned_data(groups_3, lemmatizer, all_names)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_cleaned_count_3 = count_vector_sw.fit_transform(data_cleaned)</span>
</code></pre>
    <p class="normal">We go through the same process and generate a count matrix, <code class="inlineCode">data_cleaned_count_3</code>, with 500 features from the input, <code class="inlineCode">groups_3</code>. You can refer to the steps in previous sections as you just need to repeat the same code.</p>
    <p class="normal">Next, we apply t-SNE to reduce the 500-dimensional matrix to a two-dimensional matrix:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tsne_model = TSNE(n_components=</span><span class="hljs-con-number">2</span><span class="language-python">, perplexity=</span><span class="hljs-con-number">40</span><span class="language-python">,</span>
                     random_state=42, learning_rate=500)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_tsne = tsne_model.fit_transform(data_cleaned_count_3.toarray())</span>
</code></pre>
    <p class="normal">The parameters we specify in the <code class="inlineCode">TSNE</code> object are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">n_components</code>: The output dimension</li>
      <li class="bulletList"><code class="inlineCode">perplexity</code>: The number of nearest data points considered neighbors in the algorithm with a typical value of between 5 and 50</li>
      <li class="bulletList"><code class="inlineCode">random_state</code>: The random seed for program reproducibility</li>
      <li class="bulletList"><code class="inlineCode">learning_rate</code>: The factor affecting the process of finding the optimal mapping space with a typical value of between 10 and 1,000</li>
    </ul>
    <p class="normal">Note that the <code class="inlineCode">TSNE</code> object only takes in a dense matrix, hence we convert the sparse matrix, <code class="inlineCode">data_cleaned_count_3</code>, into a dense one using <code class="inlineCode">toarray()</code>.</p>
    <p class="normal">We just successfully reduced the input dimension from 500 to 2. Finally, we can easily visualize it in a two-dimensional <a id="_idIndexMarker750"/>scatter plot where the <em class="italic">x</em> axis is the first dimension, the <em class="italic">y</em> axis is the second dimension, and the color, <code class="inlineCode">c</code>, is based on the topic label of each original document:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.scatter(data_tsne[:, </span><span class="hljs-con-number">0</span><span class="language-python">], data_tsne[:, </span><span class="hljs-con-number">1</span><span class="language-python">], c=groups_3.target)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">Refer to the following screenshot for the end result:</p>
    <figure class="mediaobject"><img alt="A colorful dots on a white background  Description automatically generated" src="../Images/B21047_07_06.png"/></figure>
    <p class="packt_figref">Figure 7.6: Applying t-SNE to data from three different topics</p>
    <p class="normal">Data points from the three topics are in different colors – green, purple, and yellow. We can observe three clear clusters. Data points from the same topic are close to each other, while those from different topics are far away. Obviously, count vectors are great representations of original text data as they preserve the distinction between three different topics.</p>
    <p class="normal">You can also play around with the parameters and see whether you can obtain a nicer plot where the three clusters are better separated.</p>
    <p class="normal">Count vectorization does well in keeping document disparity. How about maintaining similarity? We can also check<a id="_idIndexMarker751"/> that using documents from overlapping topics, such as these five topics—<code class="inlineCode">comp.graphics</code>, <code class="inlineCode">comp.os.ms-windows.misc</code>, <code class="inlineCode">comp.sys.ibm.pc.hardware</code>, <code class="inlineCode">comp.sys.mac.hardware</code>, and <code class="inlineCode">comp.windows.x</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">categories_5 = [</span><span class="hljs-con-string">'comp.graphics'</span><span class="language-python">, </span><span class="hljs-con-string">'comp.os.ms-windows.misc'</span><span class="language-python">, </span><span class="hljs-con-string">'comp.sys.ibm.pc.hardware'</span><span class="language-python">, </span><span class="hljs-con-string">'comp.sys.mac.hardware'</span><span class="language-python">, </span><span class="hljs-con-string">'comp.windows.x'</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups_5 = fetch_20newsgroups(categories=categories_5)</span>
</code></pre>
    <p class="normal">Similar processes (including text clean-up, count vectorization, and t-SNE) are repeated and the resulting plot is displayed as follows:</p>
    <figure class="mediaobject"><img alt="A colorful dots on a white background  Description automatically generated" src="../Images/B21047_07_07.png"/></figure>
    <p class="packt_figref">Figure 7.7: Applying t-SNE to data from five similar topics</p>
    <p class="normal">Data points from those five computer-related topics are all over the place, which means they are contextually similar. To conclude, count vectors are simple yet great representations of original text data as they are also good at preserving similarity among related topics. The question now <a id="_idIndexMarker752"/>arises: can we improve upon word (term) count representations? Let’s progress to the next section, where we will explore dense vector representations.</p>
    <h2 class="heading-2" id="_idParaDest-178">Representing words with dense vectors – word embedding</h2>
    <p class="normal">Word count representation results in a high-dimensional, sparse vector where each element represents the frequency of a specific <a id="_idIndexMarker753"/>word. Recall that we only looked at the <code class="inlineCode">500</code> most frequent words previously to avoid this issue. Otherwise, we would have to represent each document with a vector of more than 1 million dimensions (depending on the size of the vocabulary). Also, word count representation lacks the ability to capture the semantics or context of words. It only considers the frequency of words in a document or corpus. On the contrary, <strong class="keyWord">word embedding</strong> represents <a id="_idIndexMarker754"/>words in a <strong class="keyWord">dense</strong> (<strong class="keyWord">continuous</strong>) vector space.</p>
    <h2 class="heading-2" id="_idParaDest-179">Building embedding models using shallow neural networks</h2>
    <p class="normal">Word embedding maps each word to a dense vector of fixed dimensions. Its dimensionality is a lot lower<a id="_idIndexMarker755"/> than the size of the vocabulary and is usually several hundred only. For example, the word <em class="italic">machine</em> can be represented as a vector <code class="inlineCode">[1.4, 2.1, 10.3, 0.2, 6.81]</code>.</p>
    <p class="normal">So, how can we embed a word into a vector? One solution is <strong class="keyWord">word2vec </strong>(see <em class="italic">Efficient Estimation of Word Representations in Vector Space</em>, by Tomas Mikolov, Kai Chen, Greg Corrado, and Jeff Dean, <a href="https://arxiv.org/pdf/1301.3781"><span class="url">https://arxiv.org/pdf/1301.3781</span></a>); this trains a <strong class="keyWord">shallow neural network</strong> to predict a word given the other <a id="_idIndexMarker756"/>words around it, which is called <strong class="keyWord">Continuous Bag of Words</strong> (<strong class="keyWord">CBOW</strong>), or to <a id="_idIndexMarker757"/>predict the other words around a word, which is <a id="_idIndexMarker758"/>called the <strong class="keyWord">skip-gram</strong> approach. The <strong class="keyWord">weights</strong> (<strong class="keyWord">coefficients</strong>) of the<a id="_idIndexMarker759"/> trained neural network are the embedding vectors for the corresponding words. Let’s look at a concrete example.</p>
    <p class="normal">Given the sentence <em class="italic">I love reading python machine learning by example</em> in a corpus and <code class="inlineCode">5</code> as the size of the <strong class="keyWord">word window</strong>, we can<a id="_idIndexMarker760"/> have the following training sets for the CBOW neural network:</p>
    <table class="table-container" id="table004-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Input of neural network</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Output of neural network</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(I, love, python, machine)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(reading)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(love, reading, machine, learning)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(reading, python, learning, by)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(machine)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python, machine, by, example)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(learning)</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 7.4: Input and output of the neural network for CBOW</p>
    <p class="normal">During training, the inputs and outputs of the neural network are one-hot encoding vectors, where values are either 1 for present words or 0 for absent words. And we can have millions of training samples constructed from a corpus, sentence by sentence. After the network is trained, the weights that connect the input layer and hidden layer embed individual input words.</p>
    <p class="normal">A skip-gram-based neural network<a id="_idIndexMarker761"/> embeds words in a similar way. But its input and output are an inverse version of CBOW. Given the same sentence, <em class="italic">I love reading python machine learning by example</em>, and <code class="inlineCode">5</code> as the size of the word window, we can have the following training sets for the skip-gram neural network:</p>
    <table class="table-container" id="table005-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Input of neural network</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Output of neural network</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(reading)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(i)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(reading)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(love)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(reading)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(reading)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(machine)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(love)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(reading)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(machine)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(learning)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(machine)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(reading)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(machine)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(machine)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(learning)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(machine)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(by)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(learning)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(python)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(learning)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(machine)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(learning)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(by)</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(learning)</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">(example)</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 7.5: Input and output of the neural network for skip-gram</p>
    <p class="normal">The embedding vectors are of real values, where each dimension encodes an aspect of meaning for the words in the vocabulary. This helps preserve the semantic information of the words, as opposed to discarding it, as in the dummy one-hot encoding approach using the word count<a id="_idIndexMarker762"/> approach. An interesting phenomenon is that vectors from semantically similar words are proximate to each other in geometric space. For example, both the words <em class="italic">clustering </em>and<em class="italic"> grouping</em> refer to unsupervised clustering in the context of machine learning, hence their embedding vectors are close together. Word embedding is able to capture the meanings of <a id="_idIndexMarker763"/>words and their <strong class="keyWord">context</strong>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">Visualizing word embeddings can be a helpful tool to explore patterns, identify relationships between words, and assess the effectiveness of your embedding model. Here are some best practices for visualizing word embeddings:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Dimensionality reduction</strong>: Word embeddings typically have high-dimensional vectors. To visualize<a id="_idIndexMarker764"/> them, reduce their dimensionality. We can use techniques like PCA or t-SNE to project the high-dimensional data into 2D or 3D space while preserving distances between data points.</li>
        <li class="bulletList"><strong class="keyWord">Clustering</strong>: Cluster similar word<a id="_idIndexMarker765"/> embeddings together to identify groups of words with similar meanings or contexts.</li>
      </ul>
    </div>
    <h2 class="heading-2" id="_idParaDest-180">Utilizing pre-trained embedding models</h2>
    <p class="normal">Training a word embedding neural <a id="_idIndexMarker766"/>network can be time-consuming and computationally expensive. Fortunately, many organizations and research institutions (such as Google, Meta AI Research, OpenAI, Stanford NLP Group, and Hugging Face) have developed pre-trained word embedding models based on different kinds of corpora and made them readily available for developers and researchers to use in various<a id="_idIndexMarker767"/> NLP tasks. We can simply use these <strong class="keyWord">pre-trained</strong> models to map words to vectors. Some popular pre-trained word embedding models are as follows:</p>
    <figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with low confidence" src="../Images/B21047_07_08.png"/></figure>
    <p class="packt_figref">Figure 7.8: Configurations of popular pre-trained word embedding models</p>
    <p class="normal">Once we have embedding vectors <a id="_idIndexMarker768"/>for individual words, we can represent a document sample by averaging all of the vectors of words present in this document.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">Once you have the word embeddings for all words in the document, aggregate them into a single vector representation for the entire document. Common aggregation techniques include averaging and summation. More sophisticated methods include the following:</p>
      <ul>
        <li class="bulletList">Weighted average, where the weights are based on word importance, such as TF-IDF score</li>
        <li class="bulletList">Max/min pooling, where the maximum or minimum value for each dimension across all word embeddings is taken</li>
      </ul>
    </div>
    <p class="normal">The resulting vectors of <a id="_idIndexMarker769"/>document samples are then consumed by downstream predictive tasks, such as classification, similarity ranking in search engines, and clustering.</p>
    <p class="normal">Now let’s play around with <code class="inlineCode">gensim</code>, a popular NLP package with powerful word embedding modules.</p>
    <p class="normal">First, we import the package and load a pre-trained model, <code class="inlineCode">glove-twitter-25</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> gensim.downloader </span><span class="hljs-con-keyword">as</span><span class="language-python"> api</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">model = api.load(</span><span class="hljs-con-string">"glove-twitter-25"</span><span class="language-python">)</span>
[==================================================] 100.0%
104.8/104.8MB downloaded
</code></pre>
    <p class="normal">You will see the process bar if you run this line of code. The <code class="inlineCode">glove-twitter-25</code> model is one of the smallest ones so the download will not take very long.</p>
    <p class="normal">We can obtain the embedding vector for a word (<code class="inlineCode">computer</code>, for example), as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">vector = model[</span><span class="hljs-con-string">'computer'</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">'Word computer is embedded into:\n'</span><span class="language-python">, vector)</span>
Word computer is embedded into:
[ 0.64005 -0.019514 0.70148 -0.66123 1.1723 -0.58859 0.25917
-0.81541 1.1708 1.1413 -0.15405 -0.11369 -3.8414 -0.87233
  0.47489 1.1541 0.97678 1.1107 -0.14572 -0.52013 -0.52234
 -0.92349 0.34651 0.061939 -0.57375 ]
</code></pre>
    <p class="normal">The result is a 25-dimension float vector, as expected.</p>
    <p class="normal">We can also get the top 10 words that are most contextually relevant to <code class="inlineCode">computer</code> using the <code class="inlineCode">most_similar</code> method, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">similar_words = model.most_similar(</span><span class="hljs-con-string">"computer"</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">'Top ten words most contextually relevant to computer:\n'</span><span class="language-python">,</span>
           similar_words)
Top ten words most contextually relevant to computer:
 [('camera', 0.907833456993103), ('cell', 0.891890287399292), ('server', 0.8744666576385498), ('device', 0.869352400302887), ('wifi', 0.8631256818771362), ('screen', 0.8621907234191895), ('app', 0.8615544438362122), ('case', 0.8587921857833862), ('remote', 0.8583616018295288), ('file', 0.8575270771980286)]
</code></pre>
    <p class="normal">The result looks promising.</p>
    <p class="normal">Finally, we demonstrate how to <a id="_idIndexMarker770"/>generate embedding vectors for a document with a simple example, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">doc_sample = [</span><span class="hljs-con-string">'i'</span><span class="language-python">, </span><span class="hljs-con-string">'love'</span><span class="language-python">, </span><span class="hljs-con-string">'reading'</span><span class="language-python">, </span><span class="hljs-con-string">'python'</span><span class="language-python">, </span><span class="hljs-con-string">'machine'</span><span class="language-python">,</span>
                 'learning', 'by', 'example']
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">doc_vector = np.mean([model[word] </span><span class="hljs-con-keyword">for</span><span class="language-python"> word </span><span class="hljs-con-keyword">in</span><span class="language-python"> doc_sample],</span>
                                                           axis=0)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">'The document sample is embedded into:\n'</span><span class="language-python">, doc_vector)</span>
The document sample is embedded into:
 [-0.17100249 0.1388764 0.10616798 0.200275 0.1159925 -0.1515975
  1.1621187 -0.4241785 0.2912 -0.28199488 -0.31453252 0.43692702
 -3.95395 -0.35544625 0.073975 0.1408525 0.20736426 0.17444688
  0.10602863 -0.04121475 -0.34942 -0.2736689 -0.47526264 -0.11842456
 -0.16284864]
</code></pre>
    <p class="normal">The resulting vector is the <strong class="keyWord">average</strong> of embedding vectors of eight input words.</p>
    <p class="normal">In traditional NLP applications, such as text classification and information retrieval tasks, where word frequency plays a significant role, word count representation is still an outstanding solution. In more complicated areas requiring understanding and semantic relationships between words, such as text summarization, machine translation, and question answering, word embedding is used extensively and extracts far better features than the traditional approach.</p>
    <h1 class="heading-1" id="_idParaDest-181">Summary</h1>
    <p class="normal">In this chapter, you learned the fundamental concepts of NLP as an important subfield in machine learning, including tokenization, stemming and lemmatization, and PoS tagging. We also explored three powerful NLP packages and worked on some common tasks using NLTK and spaCy. Then we continued with the main project, exploring the 20 newsgroups data. We began by extracting features with tokenization techniques and went through text preprocessing, stop word removal, and lemmatization. We then performed dimensionality reduction and visualization with t-SNE and proved that count vectorization is a good representation of text data. We proceeded with a more modern representation technique, word embedding, and illustrated how to utilize a pre-trained embedding model.</p>
    <p class="normal">We had some fun mining the 20 newsgroups data using dimensionality reduction as an unsupervised approach. Moving forward, in the next chapter, we’ll be continuing our unsupervised learning journey, specifically looking at topic modeling and clustering.</p>
    <h1 class="heading-1" id="_idParaDest-182">Exercises</h1>
    <ol>
      <li class="numberedList" value="1">Do you think all of the top 500 word tokens contain valuable information? If not, can you impose another list of stop words?</li>
      <li class="numberedList">Can you use stemming instead of lemmatization to process the 20 newsgroups data?</li>
      <li class="numberedList">Can you increase <code class="inlineCode">max_features</code> in <code class="inlineCode">CountVectorizer</code> from <code class="inlineCode">500</code> to <code class="inlineCode">5000</code> and see how the t-SNE visualization will be affected?</li>
      <li class="numberedList">Experiment with representing the data for the three topics discussed in the chapter using the <code class="inlineCode">word2vec-google-news-300</code> model in Gensim and visualize them with t-SNE. Assess whether the visualization appears more improved compared to the result shown in <em class="italic">Figure 7.6</em> using word count representation.</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-183">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/yuxi"><span class="url">https://packt.link/yuxi</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code1878468721786989681.png"/></p>
  </div>
</body></html>
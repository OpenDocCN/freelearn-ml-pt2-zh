<html><head></head><body>
  <div id="_idContainer377">
    <h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-149" class="chapterTitle">Deep Learning for Time-Series</h1>
    <p class="normal">Deep learning is a subfield of machine learning concerned with algorithms relating to neural networks. Neural networks, or, more precisely,<strong class="keyword"> artificial neural networks</strong> (<strong class="keyword">ANNs</strong>) got their name because of the loose association with biological neural networks in the human brain.</p>
    <p class="normal">In recent years, deep learning<a id="_idIndexMarker765"/> has been enhancing the state of the art across the bench in many application domains. This is true for unstructured datasets such as text, images, video, and audio; however, tabular datasets and time-series have so far shown themselves to be less amenable to deep learning.</p>
    <p class="normal">Deep learning brings a very high level of flexibility and can offer advantages of both online learning, as discussed in <em class="chapterRef">Chapter 8</em>, <em class="italic">Online Learning for Time-Series</em>, and probabilistic approaches, as discussed in <em class="chapterRef">Chapter 9</em>, <em class="italic">Probabilistic Models for Time-Series</em>. However, with its highly parameterized models, finding the right model can be a challenge.</p>
    <p class="normal">Among the contributions deep learning has been able to bring to time-series are data augmentation, transfer learning, long sequence time-series forecasts, and data generation <a id="_idIndexMarker766"/>with <strong class="keyword">generative adversarial networks</strong> (<strong class="keyword">GANs</strong>). However, it's only very recently that deep learning approaches have become competitive in relation to forecasting, classification, and regression tasks.</p>
    <p class="normal">In this chapter, we'll discuss deep learning applied to time-series, looking, in particular, at algorithms and approaches designed for time-series. We'll get into current challenges, promising avenues of research, and competitive approaches that bring deep learning to time-series. We'll go into detail about a lot of the recent innovations in deep learning for time-series.</p>
    <p class="normal">We're going to cover the following topics:</p>
    <ul>
      <li class="bullet">Introduction to deep learning</li>
      <li class="bullet">Deep learning for time-series<ul>
          <li class="bullet-l2">Autoencoders</li>
          <li class="bullet-l2">InceptionTime</li>
          <li class="bullet-l2">DeepAR</li>
          <li class="bullet-l2">N-BEATS</li>
          <li class="bullet-l2">Recurrent neural networks</li>
          <li class="bullet-l2">ConvNets</li>
          <li class="bullet-l2">Transformer architectures</li>
          <li class="bullet-l2">Informer</li>
        </ul>
      </li>
      <li class="bullet">Python practice<ul>
          <li class="bullet-l2">Fully connected network</li>
          <li class="bullet-l2">Recurrent neural network</li>
          <li class="bullet-l2">Dilated causal convolutional neural network</li>
        </ul>
      </li>
    </ul>
    <p class="normal">Let's start with an introduction to deep learning and the core concepts.</p>
    <h2 id="_idParaDest-150" class="title">Introduction to deep learning</h2>
    <p class="normal">Deep learning is based on<a id="_idIndexMarker767"/> fundamental concepts that find their roots early in the 20<sup class="Superscript--PACKT-">th</sup> century – the wiring between neurons. Neurons<a id="_idIndexMarker768"/> communicate chemically and electrically through<a id="_idIndexMarker769"/> so-called neurites.</p>
    <p class="normal">This wiring was first described and drawn by Santiago Ramón y Cajal, a Spanish neuroscientist. He charted the anatomy of the brain and the structure of neural networks in the brain. He received the Nobel Prize in Physiology or Medicine in 1906, which he shared with Camillo Golgi, who invented the stains for neurons based on potassium dichromate and silver nitrate that Ramón y Cajal applied in his microscopy studies.</p>
    <p class="normal">The chart below is just one of his elaborate drawings of the arborization of neural connections (called neurites – dendrites and axons) between neurons in the brain (source Wikimedia Commons):</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_01.png" alt="ile:Debuixos Santiago Ramón y Cajal.jpg"/></figure>
    <p class="packt_figref">Figure 10.1: Ramon y Cajal's drawing of networks of neurons in the brain</p>
    <p class="normal">In the schematic, you can appreciate <a id="_idIndexMarker770"/>neurons as gray dots in layers of the brain. Between neurons are dendrites and axons, the wiring of the brain. Each neuron takes up what amounts to information about the environment through gate stations to neurites that are <a id="_idIndexMarker771"/>called synapses.</p>
    <p class="normal">Ramón y Cajal and his pupils brought to life <em class="italic">cable theory</em>, where<a id="_idIndexMarker772"/> the electric current passing through neurites is modeled by mathematical models. The voltage arriving at neural sites through the dendrites<a id="_idIndexMarker773"/> that receive synaptic inputs at different sites and times was recognized as sensory and other information was transmitted between cells. This is the foundation of today's detailed neuron models employed in research to model synaptic and neural responses.</p>
    <p class="normal">The basic function of neurons<a id="_idIndexMarker774"/> was formalized by Frank Rosenblatt in 1958 as the perceptron – a model that contains the essentials of most modern deep learning concepts.</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_02.png" alt="../perceptron.png"/></figure>
    <p class="packt_figref">Figure 10.2: The perceptron model</p>
    <p class="normal">In the <a id="_idIndexMarker775"/>perceptron model, a neuron – illustrated by the oval in the middle – receives input from <a id="_idIndexMarker776"/>other neurons. In a model, these inputs could represent text, images, sounds, or any other type of information. These get integrated by summing them up. In this sum, each input from a neuron, <em class="italic">i</em>, comes with its weight, <em class="italic">w</em><sub class="" style="font-style: italic;">i</sub>, that marks its importance. This integrated input can then lead to a neural activation as given by the neuron's activation function, <em class="italic">g</em>.</p>
    <p class="normal">In the simplest case, the activation function<a id="_idIndexMarker777"/> could just be a threshold function so that the neuron gets activated if the weighted sum of the inputs exceeds a certain value. In modern neural networks, the activation functions are non-linear functions, such as sigmoid functions or the rectified linear function where the output is linear above a threshold and cropped below.</p>
    <p class="normal">When the network is stimulated by data, input neurons are activated and feed second-order neurons, which then feed other neurons in turn until the output layers are activated. This is <a id="_idIndexMarker778"/>called <em class="italic">feedforward propagation</em>.</p>
    <p class="normal">The perceptron<a id="_idIndexMarker779"/> consisted of a single layer of integrating neurons that sum input over their incoming connections. It was demonstrated by Marvin Minsky and Seymour Pappert in their book <em class="italic">Perceptrons</em> (1969) that these neurons, similar to a simple linear model, cannot approximate complex functions that are relevant in the real world.</p>
    <p class="normal">Multilayer neural networks can overcome this limitation, however, and this is where we slowly enter into the realm of deep learning. These<a id="_idIndexMarker780"/> networks can be trained through an algorithm called <em class="italic">backpropagation</em> – often credited to Paul Werbos (1975). In backpropagation, outputs can be compared to targets and the error derivative can be fed back through the network to calculate adjustments to the weights in the connections.</p>
    <p class="normal">Another innovation in neural networks again comes from neuroscience. In the 1950s and 1960s, David Hubel and Torsten Wiesel found that neurons in the cat visual cortex (V1) respond to small regions of the visual field. This region they termed the receptive field (1959, "Receptive fields of single neurons in the cat's striate cortex"). They distinguished between two basic<a id="_idIndexMarker781"/> cell types:</p>
    <ul>
      <li class="bullet">Simple cells – these cells can <a id="_idIndexMarker782"/>be characterized largely by a summation over the inputs</li>
      <li class="bullet">Complex cells – cells that <a id="_idIndexMarker783"/>respond to a variety of stimuli across different locations</li>
    </ul>
    <p class="normal">Complex cells have <a id="_idIndexMarker784"/>inspired computational layers in neural networks that employ convolutions, first by Kunihiko Fukushima in 1980. We've discussed convolutions in <em class="chapterRef">Chapter 3</em>, <em class="italic">Preprocessing Time-Series</em>.</p>
    <p class="normal">Neural networks with convolutional layers are the predominant type of model for applications such as image processing, classification, and segmentation. Yann LeCun and colleagues introduced the LeNet architecture (1989), where convolution kernels are learned through backpropagation for the classification of images of hand-written numbers.</p>
    <p class="normal">Deep learning networks often come not just with layers, where inputs get propagated from one layer to the next (feedforward). The connections can also be recurrent, where they connect to the neurons of the same layer or even back to the same neuron.</p>
    <p class="normal">A recurrent neural network framework, <strong class="keyword">long short-term memory</strong> (<strong class="keyword">LSTM</strong>) was proposed by Jürgen Schmidhuber and Sepp Hochreiter in 1997. LSTMs <a id="_idIndexMarker785"/>can retrieve and learn information for a longer period of time compared to previous models. This model architecture was, for some time, powering industry models such as speech recognition software for Android smartphones, but has since been mostly replaced by convolutional models.</p>
    <p class="normal">In 2012, AlexNet, created by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, made a <a id="_idIndexMarker786"/>breakthrough in the ImageNet Large Scale Visual Recognition Challenge (short ImageNet), where millions of images are to be categorized between 20,000 categories. AlexNet brought down the top-5 error rate from around 25% to about 15%. The model, utilizing massively parallel hardware powered by <strong class="keyword">Graphics Processing Units</strong> (<strong class="keyword">GPUs</strong>), combined <a id="_idIndexMarker787"/>fully connected layers with convolutions and pooling layers.</p>
    <p class="normal">This was only the beginning of a radical performance improvement on different tasks, including images. The AlexNet performance was beaten the following year by ResNet.</p>
    <p class="normal">Since the ResNet paper is highly influential, it's worth taking a short detour to explain how it works. ResNets<a id="_idIndexMarker788"/> were introduced by Kaiming He and others at Microsoft Research in 2015 ("<em class="italic">Deep Residual Learning for Image Recognition</em>"). A common problem with deep neuron networks is that their performance can saturate and degrade with more layers added partly because of the vanishing gradient problem, where the error gradient calculated in the optimization will become too small to be useful.</p>
    <p class="normal">Inspired by pyramidal cells in the brain, residual neural networks employ so-called <em class="italic">skip connections</em>, essentially<a id="_idIndexMarker789"/> shortcuts to jump intermediate layers. A ResNet is a network that contains blocks with skip connections (residual blocks), as indicated in this schema:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_03.png" alt="../resnet%20(2).png"/></figure>
    <p class="packt_figref">Figure 10.3: Residual block with skip connections</p>
    <p class="normal">In the residual block illustrated, the output of layer 2 is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_001.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_10_002.png" alt="" style="height: 1em;"/> and <img src="../Images/B17577_10_003.png" alt="" style="height: 1em;"/> are the activation functions<a id="_idIndexMarker790"/> in layer 2 and the skip connections, respectively. <img src="../Images/B17577_10_004.png" alt="" style="height: 1em;"/> is often the identify function, where activations of layer 1 are unchanged. If the <a id="_idIndexMarker791"/>dimensionality between layer 1 and layer 2 outputs don't match, either padding or convolutions are used.</p>
    <p class="normal">With these skip connections, Kaiming He and others successfully trained networks with as many as 1,000 layers. The original ResNet from 2015 was very successful on images. Among other accolades it collected was winning several top competitions for image classification and object detection: ILSVRC 2015, ILSVRC 2015, COCO 2015 competition in ImageNet Detection, ImageNet localization, Coco detection, and Coco segmentation.</p>
    <p class="normal">I've summarized the early history of ANNs and deep learning in the timeline here:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_04.png" alt="../timeline%20of%20deep%20learning%20(2).png"/></figure>
    <p class="packt_figref">Figure 10.4: Timeline of artificial neural networks and deep learning</p>
    <p class="normal">Please note that this is highly simplified, leaving out many important milestones. I've ended in 2015, when ResNet was presented.</p>
    <p class="normal">There are a host of architectures and approaches in deep learning, and this chart displays a typology<a id="_idIndexMarker792"/> of these methodologies:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_05.png" alt="deep%20learning%20models.png"/></figure>
    <p class="packt_figref">Figure 10.5: Typology of deep learning approaches</p>
    <p class="normal">We've mentioned a few of these approaches in this section, and we'll explain a few of these methods in more detail in the next sections as they are relevant to time-series.</p>
    <p class="normal">The computational complexity of techniques based on deep neural networks is driven in the first instance by the dimension of the input data and depends on the number of hidden layers trained using backpropagation. High-dimensional data tend to require more hidden layers to ensure a higher hierarchy of feature learning, where each layer derives higher-level features based on the previous level. The training time and complexity increase with the number of neurons – the number of hyperparameters can sometimes reach the millions or billions.</p>
    <p class="normal">The representational power of deep learning that constructs a stack of derived features as part of the learning allows modelers to get away from hand-crafted features. Further advantages of using deep learning models include their flexibility in terms of choosing architecture, hyperparameters such as activation functions, regularization, layer sizes, and loss objectives, but this is traded off against their complexity in terms of the number of parameters, and the difficulty of interrogating their inner workings.</p>
    <p class="normal">Deep learning methods offer better representation and, in consequence, prediction on a multitude of time-series datasets compared to other machine learning approaches; however, they haven't found the impact so far that they had in other areas.</p>
    <h1 id="_idParaDest-151" class="title">Deep learning for time-series</h1>
    <p class="normal">Recent years have seen a<a id="_idIndexMarker793"/> proliferation of deep neural networks, with unprecedented improvements across various application domains, in particular images, natural language processing, and sound. The potential advantage of deep learning models is that they can be much more accurate than other types of models, thereby pushing the envelope in <a id="_idIndexMarker794"/>domains such as vision, sound, and <strong class="keyword">natural language processing</strong> (<strong class="keyword">NLP</strong>).</p>
    <p class="normal">In forecasting, especially demand forecasting, data is often highly erratic, discontinuous, or bursty, which violates the core assumptions of classical techniques, such as Gaussian errors, stationarity, or homoscedasticity, as discussed in <em class="chapterRef">Chapter 5</em>, <em class="italic">Forecasting of Time-Series</em>. Deep learning techniques applied to forecasting, classification, or regression tasks could overcome many of the challenges faced by classical approaches, and, most importantly, they could provide a way to model non-linear dynamics usually neglected by traditional methods such as <a id="_idIndexMarker795"/>Box-Jenkins, Exponential Smoothing (ES), or state-space models.</p>
    <p class="normal">Many deep learning algorithms have been applied more recently to time-series, both with univariate and multivariate time-series. The model architectures encompass recurrent neural networks (RNNs), most <a id="_idIndexMarker796"/>prominently long short-term memory (LSTM) models, and transformer and convolutional models, or different types of autoencoders.</p>
    <p class="normal">As regards their application to time-series, however, they haven't been able to challenge the top models in the field. For instance, as pointed out by Spyros Makridakis and others (2020), in the M4 competition, arguably the most important benchmark for univariate time-series forecasting, the best-ranking methods were ensembles of widely used classical statistical techniques rather than pure machine learning methods.</p>
    <p class="normal">This could have been at least partly due to the nature of the competition. As pointed out by Slawek Smyl, de-seasonalization of seasonal series was very important in the M4 competition, given that the series were provided as scalar vectors without timestamps, so there was no way to incorporate calendar features such as the day of the week or the month number.</p>
    <p class="normal">In the M4 competition, out of<a id="_idIndexMarker797"/> 60 competition entries, the first machine learning method ranked at place 23. However, interestingly, the winner of the M4 competition was a hybrid between a dilated LSTM with attention and a Holt-Winters statistical model. Another top contender, developed by the research group around Rob Hyndman, applied a gradient boosted tree ensemble to outputs from traditional models (<em class="italic">FFORMA: Feature-based Forecast Model Averaging</em>, 2020).</p>
    <p class="normal">These rankings led Spyros Makridakis and others to conclude that hybrids or mixtures of classical and machine learning methods are the way forward. The search is ongoing for a deep learning architecture that could provide an inflection point in research and applications similar to that of AlexNet or Inception for the image domain.</p>
    <p class="normal">In <em class="chapterRef">Chapter 4</em>, <em class="italic">Introduction to Machine Learning with Time-Series</em>, we discussed first how difficult it is to beat baseline approaches such as Nearest Neighbor with <strong class="keyword">Dynamic Time Warping</strong> (<strong class="keyword">DTW</strong>) and <a id="_idIndexMarker798"/>then state-of-the-art approaches. The most <a id="_idIndexMarker799"/>competitive model in terms of performance is <strong class="keyword">HIVE-COTE</strong> (<strong class="keyword">Hierarchical Vote Collective of Transformation-Based Ensembles</strong>), which consists of ensembles of machine learning models – very expensive in terms of resources, owing to the number of computations and the long runtime.</p>
    <p class="normal">The sardonic reader might comment that this sounds like deep learning already and ask whether deep learning hasn't already taken over as the state-of-the-art method. Generally speaking, the complexity of deep learning models is much higher than that of traditional models or other machine learning techniques. The case can be made that this is one of the biggest distinguishing characteristics of deep learning models.</p>
    <p class="normal">Is there a deep learning model architecture of similar or lower complexity than HIVE that can achieve competitive results?</p>
    <p class="normal">I've summarized a few libraries that <a id="_idIndexMarker800"/>implement algorithms with deep learning for time-series in this table:</p>
    <table id="table001-7" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Library</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Maintainer</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Algorithms</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Framework</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">dl-4-tsc</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Hassan Ismail Fawaz</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">Multi-Layer Perceptron</strong> (<strong class="keyword">MLP</strong>), <strong class="keyword">Fully Connected Network</strong> (<strong class="keyword">FCN</strong>), ResNet, Encoder (based on CNN), <strong class="keyword">Multi-Scale Convolutional Neural Network</strong> (<strong class="keyword">MCNN</strong>), <strong class="keyword">Time Le-Net</strong> (<strong class="keyword">t-LeNet</strong>), <strong class="keyword">Multi-Channel Deep Convolutional Neural Network</strong> (<strong class="keyword">MCDCNN</strong>), Time-CNN, <strong class="keyword">Time Warping Invariant Echo State Network</strong> (<strong class="keyword">TWIESN</strong>), InceptionTime</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">TensorFlow/Keras</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Sktime-DL</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Students and staff at the University of East Anglia around Tony Bagnell</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">ResNet, CNN, InceptionTime (through an interface with another library)</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">TensorFlow/Keras</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Gluon-TS</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Amazon Web Services – Labs</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Gluon-TS specializes in probabilistic neural network models such as these: <strong class="keyword">Convolutional Neural Network</strong> (<strong class="keyword">CNN</strong>), DeepAR, <strong class="keyword">Recurrent Neural Network</strong> (<strong class="keyword">RNN</strong>), <strong class="keyword">Multi-Layer Perceptron</strong> (<strong class="keyword">MLP</strong>)</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">MXNET</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Pytorch Forecasting</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Daniel Hoyos and others</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Recurrent Networks (GRU, LSTM), Temporal Fusion Transformers, N-Beats, Multilayer Perceptron, DeepAR</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">PyTorch Lightning</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Figure 10.6: Overview of several deep learning libraries for time-series</p>
    <p class="normal">Sktime-DL<a id="_idIndexMarker801"/> is an extension to sktime, maintained by the same research group. As of August 2021, this library is undergoing a rewrite.</p>
    <p class="normal">Gluon-TS<a id="_idIndexMarker802"/> is based on the MXNET deep learning modeling framework, and – apart from the network architectures noted in the table – includes many other features, such as kernels<a id="_idIndexMarker803"/> for <strong class="keyword">Support Vector Machines</strong> (<strong class="keyword">SVMs</strong>) and <strong class="keyword">Gaussian Process</strong> (<strong class="keyword">GP</strong>), and <a id="_idIndexMarker804"/>distributions for probabilistic network models.</p>
    <p class="normal"><em class="italic">dl-4-tsc</em> is the GitHub<a id="_idIndexMarker805"/> companion repository for a review paper of many time-series deep learning algorithms, prepared by Hassan Ismail Fawaz and others (2019). It includes implementations in TensorFlow/Keras of their implementations. It is not a library <em class="italic">per se</em>, as it isn't installed like a library and the models run with datasets; however, since the algorithms are implemented in TensorFlow and Keras, anyone with a knowledge <a id="_idIndexMarker806"/>of these will feel at home.</p>
    <p class="normal">Pytorch-forecasting, sktime-DL, and Gluon-TS come with their own abstractions of datasets that help with the<a id="_idIndexMarker807"/> automation of common tasks. While Sktime-DL builds on the sktime abstractions, Pytorch-Forecasting and Gluon-TS have batteries built-in for deep learning with utilities for common tasks such as the scaling and encoding of variables, normalizing the target variable, and downsampling. These abstractions come at the cost of a learning curve, however, and I should caution the impatient reader that it can take time to get up to speed with this, which is why I am omitting them from the practice part.</p>
    <p class="normal">I've omitted repositories from this table that only implement a single algorithm. In the next visualization, I have included some of these, such as the repositories for the Informer model or the neural prophet. In the following diagram, you can see the popularity of a few repositories for time-series deep learning:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_06.png" alt="deep_learning-star_history.png"/></figure>
    <p class="packt_figref">Figure 10.7: Popularity of deep learning libraries for time-series</p>
    <p class="normal">As always, I've tried to choose the most popular repositories – and repositories that have been updated recently. You can see that Gluon-TS is the most popular repository. Of the repositories implementing several algorithms, Pytorch Forecasting comes closest and has been making inroads recently in terms of popularity.</p>
    <p class="normal">In the next sections, we'll concentrate on recent and competitive approaches with deep learning on time-series. We'll go through a few of the most prominent algorithms in more detail: Autoencoders, InceptionTime, DeepAR, N-BEATS, RNNs (most prominently LSTMs), ConvNets, and Transformers (including the Informer).</p>
    <h2 id="_idParaDest-152" class="title">Autoencoders</h2>
    <p class="normal"><strong class="keyword">Autoencoders</strong> (<strong class="keyword">AEs</strong>) are <a id="_idIndexMarker808"/>artificial neural networks that learn to efficiently compress and encode data and are trained on reconstruction errors. A basic linear AE is essentially functionally equivalent to a <strong class="keyword">Principal Component Analysis</strong> (<strong class="keyword">PCA</strong>), although, in <a id="_idIndexMarker809"/>practice, AEs are often regularized.</p>
    <p class="normal">AEs consist of two parts, the encoder and the decoder, as illustrated below (source: Wikipedia):</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_07.png" alt="https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/220px-Autoencoder_schema.png"/></figure>
    <p class="packt_figref">Figure 10.8: Autoencoder architecture</p>
    <p class="normal">Encoders<a id="_idIndexMarker810"/> and decoders<a id="_idIndexMarker811"/> are often <a id="_idIndexMarker812"/>both of the same architecture, which depends on the domain. For example, as regards images, they often contain convolutions like <code class="Code-In-Text--PACKT-">LeNet</code>. For modeling time dependence, they can include causal convolutions or recurrent layers as a way of modeling time dependence.</p>
    <p class="normal">AEs are a natural way of reducing noise. They are often employed for anomaly detection in time-series.</p>
    <h2 id="_idParaDest-153" class="title">InceptionTime</h2>
    <p class="normal">In a massive test that<a id="_idIndexMarker813"/> took about a month to complete on a cluster of 60 GPUs, Hassan Ismail Fawaz and others at the Université de Haute Alsace ran deep learning algorithms on the univariate UCR/UEA time-series classification archive (85 time-series) and the 13 datasets <a id="_idIndexMarker814"/>of the <strong class="keyword">Multivariate Time-Series</strong> (<strong class="keyword">MTS</strong>) classification archive. They presented this work in their paper "<em class="italic">Deep learning for time-series classification: a review</em>" in 2019.</p>
    <p class="normal">They conducted a systematic <a id="_idIndexMarker815"/>evaluation of 11 models, including LeNet, <strong class="keyword">Fully Connected Networks</strong> (<strong class="keyword">FCNs</strong>), Time-CNN, and ResNet. Only 9 of the algorithms completed all the tests. Compared to deep learning algorithms on the univariate datasets (UCR/UEA), ResNet won on 50 problems out of 85, and it was statistically better than the next best algorithm, the <strong class="keyword">fully convolutional neural network</strong> (<strong class="keyword">FCNN</strong>). At the same time, it was<a id="_idIndexMarker816"/> not statistically worse than HIVE-COTE, the top model in time-series classification. On the multivariate benchmark, the FCNN won, although they couldn't find any statistically significant differences between networks.</p>
    <p class="normal">In another paper, "<em class="italic">InceptionTime: Finding AlexNet for Time-Series Classification,</em>" Hassan Ismail Fawaz and an extended group of researchers, including Geoff Webb and others from Monash University (who we encountered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Preprocessing Time-Series</em>), presented a new model that they called InceptionTime.</p>
    <p class="normal">The name <em class="italic">InceptionTime</em> makes reference to the Inception model ("<em class="italic">Going Deeper with Convolutions</em>", 2014), a network presented by researchers at Google, and the universities of North Carolina and Michigan. The Inception architecture consists of feedforward and convolutional layers, similar to LeNet, which we mentioned earlier in this chapter. A 22-layer variant was therefore also called GoogleLetNet (alternatively: Inception model version 1). Roughly speaking, the inception model consists of modules ("inception modules") that concatenate convolutions of different sizes together.</p>
    <p class="normal">InceptionTime <a id="_idIndexMarker817"/>takes ensembles of inception-type models with different hyperparameters (filters of varying lengths). They experimented with the number of networks in the ensemble and the filter sizes and finally showed that their model significantly outperformed ResNet on the 85 datasets of the UCR archive, while being statistically on a par with HIVE-COTE – with a much-reduced training time compared to HIVE-COTE.</p>
    <h2 id="_idParaDest-154" class="title">DeepAR</h2>
    <p class="normal">DeepAR is a<a id="_idIndexMarker818"/> probabilistic forecasting method coming out of Amazon Research Germany. It is based on training an auto-regressive recurrent network model. In their article "<em class="italic">DeepAR: Probabilistic forecasting with autoregressive recurrent networks</em>" (2019), David Salinas, Valentin Flunkert, and Jan Gasthaus demonstrated through extensive empirical evaluation on several real-world forecasting datasets (parts, electricity, traffic, ec-sub, and ec) accuracy improvements of around 15% compared to state-of-the-art methods.</p>
    <p class="normal">DeepAR was designed for demand forecasting and consists of an RNN architecture and incorporates a negative binomial likelihood for unbounded count data that can stretch across several orders of magnitude. Monte Carlo sampling is used to compute quantile estimates for all sub-ranges in the prediction horizon. For the case when the magnitudes of the time-series vary widely, they also introduced a scaling of the mean and variance parameters of the negative binomial likelihood by factors that depend on the mean of the time-series and the output of the network.</p>
    <h2 id="_idParaDest-155" class="title">N-BEATS</h2>
    <p class="normal">This is a model <a id="_idIndexMarker819"/>architecture for univariate times series point forecasting based on backward and forward residual links and a very deep multilayer network of fully connected ReLU neurons. N-BEATS uses deep learning primitives such as residual blocks instead of any time-series-specific components and is the first architecture to demonstrate that deep learning using no time-series-specific components can outperform well-established statistical approaches.</p>
    <p class="normal">Published in 2019 by a group around Yoshua Bengio ("<em class="italic">N-BEATS: Neural basis expansion analysis for interpretable time-series forecasting</em>"), this network reached state-of-the-art performance for two configurations and outperformed all other methods, including ensembles of traditional statistical methods in benchmarks over the M3, M4, and TOURISM competition datasets.</p>
    <p class="normal">A common criticism of deep learning is the opaque nature of the learning or – inversely – a lack of transparency in terms of what the network does. N-BEATS can be made interpretable with few changes without losing significant accuracy.</p>
    <h2 id="_idParaDest-156" class="title">Recurrent neural networks</h2>
    <p class="normal">RNNs, most <a id="_idIndexMarker820"/>prominently LSTMs, have been applied a lot to multivariate electricity consumption forecasting. Electricity forecasting is a long sequence time-series, where it's necessary to precisely capture the long-range correlation coupling between items of a sequence over time.</p>
    <p class="normal">Earlier works explored combinations of LSTMs with dilation, residual connections, and attention. These served as a basis for the winner of the M4 competition (Slawek Smyl, 2020).</p>
    <p class="normal">Smyl introduced a mixture of a<a id="_idIndexMarker821"/> standard <strong class="keyword">Exponential Smoothing</strong> (<strong class="keyword">ES</strong>) model with LSTM networks. The ES equations enable the method to effectively capture the main components of the individual series, such as seasonality and baseline level, while the LSTM networks can model the nonlinear trend.</p>
    <p class="normal">A problem <a id="_idIndexMarker822"/>with RNNs, including LSTMs, is that they cannot be parallelized easily at the cost of training time and computing resources. It has also been argued that LSTMs cannot capture long-range dependencies since they struggle with sequences longer than about 100 time steps. RNNs encode past hidden states to capture dependencies with previous items, and they show a decrease in performance due to long dependencies.</p>
    <p class="normal">In the next section, we'll look at the transformer architecture, which has been taking over from LSTM models both in terms of performance and, more recently, in popularity.</p>
    <p class="normal">It has been shown that convolutional architectures can outperform recurrent networks on tasks such as audio processing and machine translation, and they have been applied to time-series tasks as well.</p>
    <h2 id="_idParaDest-157" class="title">ConvNets</h2>
    <p class="normal">Researchers from<a id="_idIndexMarker823"/> Carnegie Mellon University and Intel Labs ("<em class="italic">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</em>" 2018) compared generic convolutional and recurrent networks such as LSTM/GRU for sequence modeling across a broad range of tasks. These were large textual datasets and posed sequence problems, such as the problem of addition, the copying of memory tasks, or polyphonic music. Problems and datasets such as these are commonly used to benchmark recurrent networks.</p>
    <p class="normal">They found that a simple convolutional architecture, the <strong class="keyword">Temporal Convolutional Network</strong> (<strong class="keyword">TCN</strong>), performs<a id="_idIndexMarker824"/> better than RNNs on a vast range of tasks' canonical recurrent networks (such as LSTMs) while demonstrating a longer effective memory.</p>
    <p class="normal">The important characteristic of the convolutions in the TCNs is that they are causal. A convolution is causal if its output is the result of only current and past inputs. This is illustrated here (source: keras-tcn, GitHub):</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_08.png" alt="https://github.com/philipperemy/keras-tcn/raw/master/misc/Dilated_Conv.png"/></figure>
    <p class="packt_figref">Figure 10.9: A time-causal convolution</p>
    <p class="normal">An output at time t is convolved only with elements from time t and earlier. This means that information from the future cannot leak to the past. A disadvantage of this basic design is that in order to achieve a long effective history size, we need an extremely deep network or very large filters.</p>
    <p class="normal">Some advantages of convolutions over RNNs are parallelism, the flexible receptive field size (specifying how far the model can see), and stable gradients – backpropagation through time comes with the vanishing gradient problem.</p>
    <p class="normal">The transformer also addresses the perceived shortcomings of RNNs.</p>
    <h2 id="_idParaDest-158" class="title">Transformer architectures</h2>
    <p class="normal">Transformers, introduced in <a id="_idIndexMarker825"/>the article "<em class="italic">Attention is all you need</em>" (2017) by researchers at Google Brain and the University of Toronto, were designed to avoid recursion in order to allow parallel computation.</p>
    <p class="normal">Transformers introduced two building blocks – multi-head attention and positional embeddings. Rather than working sequentially, sequences are processed as a whole rather than item by item. They employ self-attention, where similarity scores between items in a sentence are stored.</p>
    <p class="normal">Transformers were introduced originally for machine translation, where they were shown to outperform Google's Neural Machine Translation models. The central piece is therefore the alignment of two sequences. Instead of recurrence, positional embeddings were introduced where weights encode information related to a specific position of a token in a sequence.</p>
    <p class="normal">Transformers consist of stacked modules, first encoder and then decoder modules. Encoder modules each consist of a self-attention layer and a feed-forward layer, while decoder modules consist of self-attention, encoder-decoder attention, and a feed-forward layer. These modules can be stacked, thereby creating very large models that can learn massive datasets.</p>
    <p class="normal">Transformers have pushed the envelope in NLP, especially in translation and language understanding. Furthermore, OpenAI's powerful GPT-3 model for language generation is a transformer as well, as is DeepMind's AlphaFold 2, a model that predicts protein structure from their genetic sequences.</p>
    <p class="normal">Transformers have been able to maintain performance across longer sequences. However, they can capture only dependencies within the fixed input size used to train them. To work with even longer sentences beyond the fixed input width, architectures such as Transformer-XL reintroduce recursion by storing hidden states of already encoded sentences to leverage them in the subsequent encoding of the next sentences.</p>
    <p class="normal">In the article "<em class="italic">Temporal Fusion Transformers for Interpretable Multi-horizon Time-Series Forecasting,</em>" researchers from the University of Oxford and Google Cloud AI introduced an attention-based architecture, which they <a id="_idIndexMarker826"/>called a <strong class="keyword">Temporal Fusion Transformer</strong> (<strong class="keyword">TFT</strong>). To learn temporal relationships at different scales, TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. Furthermore, a series of gating layers suppress unnecessary components.</p>
    <p class="normal">On a variety of real-world datasets, they demonstrated significant performance improvements of their architecture over a broad benchmark set. Among other improvements, they outperformed Amazon's DeepAR by between 36 and 69%.</p>
    <h2 id="_idParaDest-159" class="title">Informer</h2>
    <p class="normal">A problem with transformers is the<a id="_idIndexMarker827"/> quadratic time complexity and memory usage, along with the limitations of the encoder-decoder architecture. This complicates predictions over longer time periods, for example, 510 time steps of hourly electricity consumption. To address these issues, researchers from Beihang University, UC Berkeley, Rutgers, and SEDD Company designed an efficient transformer-based model for long sequence time-series forecasting, named Informer – "<em class="italic">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</em>". The paper obtained the Outstanding Paper Award at the AAAI conference in 2021.</p>
    <p class="normal">The generative decoder alleviates the time complexity of the encoder-decoder by predicting long time-series sequences in one forward operation rather than step-by-step. They replaced the positional embeddings with a new self-attention mechanism called ProbSparse Self-Attention, which achieves <img src="../Images/B17577_10_005.png" alt="" style="height: 1em;"/>, where <em class="italic">L</em> is the length of the sequence, instead of quadratic time complexity and memory usage, <img src="../Images/B17577_10_006.png" alt="" style="height: 1em;"/>, while maintaining a comparable performance on sequence alignment.</p>
    <p class="normal">Finally, the self-attention distillation halves the cascading layer input, and efficiently handles extreme long input sequences. This reduces the complexity from <img src="../Images/B17577_10_007.png" alt="" style="height: 1em;"/>, where J is the number of transformer layers, to <img src="../Images/B17577_10_008.png" alt="" style="height: 1em;"/>.</p>
    <p class="normal">The Informer architecture is illustrated in this schema (from the official Informer repository):</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_09.png" alt="https://github.com/zhouhaoyi/Informer2020/raw/main/img/informer.png"/></figure>
    <p class="packt_figref">Figure 10.10: The Informer architecture</p>
    <p class="normal">This diagram <a id="_idIndexMarker828"/>shows that the Informer significantly outperforms existing methods on<a id="_idIndexMarker829"/> datasets of long time-series <a id="_idIndexMarker830"/>forecasting such as Electricity Transformer Temperature (ETT), Electricity Consuming Load (ECL), and Weather.</p>
    <p class="normal">On univariate datasets, they achieved superior performance compared with all competitors, except for two cases, where DeepAR was slightly better, as shown here (source: Informer GitHub repository):</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_10.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_WVOQFx/Screenshot 2021-08-01 at 22.05.06.png"/></figure>
    <p class="packt_figref">Figure 10.11: Univariate long sequence time-series forecasting performance</p>
    <p class="normal">Most significantly, they beat competitors such as ARIMA, prophet, LSTMs, and other transformer-based architectures.</p>
    <p class="normal">On a multivariate benchmark, they also beat competitors including other transformer-based models and LSTMs.</p>
    <p class="normal">We'll put some of this into practice now.</p>
    <h1 id="_idParaDest-160" class="title">Python practice</h1>
    <p class="normal">Let's model airplane passengers. We'll forecast the monthly number of passengers.</p>
    <p class="normal">This dataset is considered one of the classic time-series, published by George E.P. Box and Gwilym Jenkins alongside the book "Time-Series Analysis: Forecasting and Control" (1976). I have provided a copy of this dataset in the <code class="Code-In-Text--PACKT-">chapter10</code> folder of the book's GitHub repository. You can download it from there or use the URL directly in <code class="Code-In-Text--PACKT-">pd.read_csv()</code>.</p>
    <p class="normal">We'll first start with a simple FCN and then we'll apply a recurrent network, and finally, we'll apply a very recent architecture, a Dilated Causal Convolutional Neural Network.</p>
    <p class="normal">The FCN is first.</p>
    <h2 id="_idParaDest-161" class="title">Fully connected network</h2>
    <p class="normal">In this first practice<a id="_idIndexMarker831"/> session, we'll use TensorFlow libraries, which we can quickly install from the terminal (or similarly from the anaconda navigator):</p>
    <pre class="programlisting con"><code class="hljs-con">pip install -U tensorflow
</code></pre>
    <p class="normal">We'll execute the commands from the Python (or IPython) terminal, but equally, we could execute them from a Jupyter notebook (or a different environment).</p>
    <p class="normal">The installation could take a while – the TensorFlow library is about 200 MB in size and comes with a few dependencies.</p>
    <p class="normal">Let's load the dataset. Here, I am assuming that you've downloaded it onto your computer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
passengers = pd.read_csv(
  <span class="hljs-string">"passengers.csv"</span>, parse_dates=[<span class="hljs-string">"date"</span>]
).set_index(<span class="hljs-string">"date"</span>)
</code></pre>
    <p class="normal">Let's try naively to just use an FCN, also known as an MLP.</p>
    <p class="normal">Let's set some imports and set a couple of global constants:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow.keras <span class="hljs-keyword">as</span> keras
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense, Input, Dropout
DROPOUT_RATIO = <span class="hljs-number">0.2</span>
HIDDEN_NEURONS = <span class="hljs-number">10</span>
callback = tf.keras.callbacks.EarlyStopping(
  monitor=<span class="hljs-string">'loss'</span>, patience=<span class="hljs-number">3</span>
)
</code></pre>
    <p class="normal">We will use these constants for our model architecture.</p>
    <p class="normal">Dropout (or: Dilution) is a<a id="_idIndexMarker832"/> regularization technique that can help reducing overfitting. Dropout means that during training, a fraction of the connections (in our case 20%) is randomly removed.</p>
    <p class="normal">Early stopping is <a id="_idIndexMarker833"/>another form of regularization, where the training stops as defined by certain conditions. In our case, we've stated it should stop if our loss doesn't improve three times in a row. If the model stops improving, there's no point in continuing to train it, although we may be trapped in a local minimum of the error that we might be able to escape. One of the big advantages of <a id="_idIndexMarker834"/>early stopping is that it can help us quickly see whether a model is working.</p>
    <p class="normal">We can define our model in this function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_model</span><span class="hljs-function">(</span><span class="hljs-params">passengers</span><span class="hljs-function">):</span>
  input_layer = Input(<span class="hljs-built_in">len</span>(passengers.columns))
  hiden_layer = Dropout(DROPOUT_RATIO)(input_layer)
  hiden_layer = Dense(HIDDEN_NEURONS, activation=<span class="hljs-string">'relu'</span>)(hiden_layer)
  output_layer = Dropout(DROPOUT_RATIO)(hiden_layer)
  output_layer = Dense(<span class="hljs-number">1</span>)(output_layer)
  model = keras.models.Model(
    inputs=input_layer, outputs=output_layer
  )
  model.<span class="hljs-built_in">compile</span>(
    loss=<span class="hljs-string">'mse'</span>,
  optimizer=keras.optimizers.Adagrad(),
    metrics=[keras.metrics.RootMeanSquaredError(), keras.metrics.MeanAbsoluteError()])
  <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">With the Keras <a id="_idIndexMarker835"/>functional API, we've defined a two-layer neural network, where the hidden layer of <code class="Code-In-Text--PACKT-">HIDDEN_NEURONS</code> neurons is activated by the Rectified Linear Unit (ReLU) function.</p>
    <p class="normal">Let's split our dataset into training and test sets. We will predict the number of passengers based on passengers in the previous time period (previous month): </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
X_train, X_test, y_train, y_test = train_test_split(
  passengers, passengers.passengers.shift(-<span class="hljs-number">1</span>), shuffle=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">We'll learn based on the first 75% of the dataset – this is the default value for the <code class="Code-In-Text--PACKT-">test_size</code> parameter in the <code class="Code-In-Text--PACKT-">train_test_split</code> function.</p>
    <p class="normal">We can now train our naïve FCN:</p>
    <pre class="programlisting code"><code class="hljs-code">model = create_model(X_train)
model.fit(X_train, y_train, epochs=<span class="hljs-number">1000</span>, callbacks=[callback])
</code></pre>
    <p class="normal">We should get an output of the loss and the metrics at each epoch as they finish:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_11.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_zCMSbK/Screenshot 2021-08-06 at 22.50.40.png"/></figure>
    <p class="packt_figref">Figure 10.12: Model training</p>
    <p class="normal">Ideally, we would see the <a id="_idIndexMarker836"/>error (loss) going down, and we'd see a low error at the end. I haven't included any code to fix the random number generator (<code class="Code-In-Text--PACKT-">tf.random.set_seed</code>), so your output might differ.</p>
    <p class="normal">We can then get the predictions for the test set like this:</p>
    <pre class="programlisting code"><code class="hljs-code">predicted = model.predict(X_test)
</code></pre>
    <p class="normal">Now, it would be good to visualize passenger predictions against the actual passenger values.</p>
    <p class="normal">We can use this function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">show_result</span><span class="hljs-function">(</span><span class="hljs-params">y_test, predicted</span><span class="hljs-function">):</span>
  plt.figure(figsize=(<span class="hljs-number">16</span>, <span class="hljs-number">6</span>))
  plt.plot(y_test.index, predicted, <span class="hljs-string">'o-'</span>, label=<span class="hljs-string">"predicted"</span>)
  plt.plot(y_test.index, y_test, <span class="hljs-string">'.-'</span>, label=<span class="hljs-string">"actual"</span>)
  plt.ylabel(<span class="hljs-string">"Passengers"</span>)
  plt.legend()
</code></pre>
    <p class="normal">Let's visualize our predictions then!</p>
    <pre class="programlisting code"><code class="hljs-code">show_result(y_test, predicted)
</code></pre>
    <p class="normal">Here's the graph:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_12.png" alt="fcn_naive.png"/></figure>
    <p class="packt_figref">Figure 10.13: Predicted against actual airplane passengers: Naïve fully connected network</p>
    <p class="normal">We can see that the <a id="_idIndexMarker837"/>model has learned some of the monthly variability. However, it is systematically under-predicting – it has learned the baseline from the years 1949-1958 of the training set, when far fewer passengers were traveling.</p>
    <p class="normal">Let's make this a bit more sophisticated and better.</p>
    <p class="normal">This first model was trained only on the immediate previous number of travelers.</p>
    <p class="normal">As a first step, we'll include the year and the month as predictor variables. The year can be used to model the trend, while the month is coupled to the monthly variability – so this seems a natural step.</p>
    <p class="normal">This will add month and year columns to the DataFrame based on the DateTimeIndex:</p>
    <pre class="programlisting code"><code class="hljs-code">passengers[<span class="hljs-string">"month"</span>] = passengers.index.month.values
passengers[<span class="hljs-string">"year"</span>] = passengers.index.year.values
</code></pre>
    <p class="normal">Now we can redefine our model – we need to add more input columns:</p>
    <pre class="programlisting code"><code class="hljs-code">model = create_model(passengers)
</code></pre>
    <p class="normal">And we are ready for another training round:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train, X_test, y_train, y_test = train_test_split(
  passengers, passengers.passengers.shift(-<span class="hljs-number">1</span>), shuffle=<span class="hljs-literal">False</span>
)
model.fit(X_train, y_train, epochs=<span class="hljs-number">100</span>, callbacks=[callback])
predicted = model.predict(X_test)
show_result(y_test, predicted)
</code></pre>
    <p class="normal">Let's see how well the model predictions match the test data:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_13.png" alt="fcn_with_year_month.png"/></figure>
    <p class="packt_figref">Figure 10.14: Predicted against actual airplane passengers; Fully connected network with the year and month</p>
    <p class="normal">Please note that, because of the <a id="_idIndexMarker838"/>high number of parameters, and the randomness involved in the learning process, the outcome might differ significantly between runs. This is indeed one of the problems associated with deep learning.</p>
    <p class="normal">This already looks much better. The year feature helped our model learn the baseline. The model has learned something about the monthly variability, but it's not enough to really approximate it.</p>
    <p class="normal">Let's create a less naïve version. We will change a few things in this model:</p>
    <ul>
      <li class="bullet">We'll add an embedding of the month feature</li>
      <li class="bullet">We'll treat the year as a linear predictor</li>
      <li class="bullet">We'll add the previous month's passengers to our predictions</li>
      <li class="bullet">Finally, we'll scale our predictions based on the standard deviation in the training dataset</li>
    </ul>
    <p class="normal">That was quite a mouthful. Let's go through these a bit more slowly.</p>
    <p class="normal">We fed the months as values from 1 to 12 into our previous model. However, we could intuitively guess that January (1) and December (12) are perhaps more similar than November (11) and December. We know that there are lots of travelers in both December and January, but perhaps a much lower volume in November. We can capture these relationships based on the data.</p>
    <p class="normal">This can be done in an embedding layer. An embedding layer is a mapping of distinct categories to real numbers. This mapping is updated as part of the network optimization.</p>
    <p class="normal">The year is closely related to the overall trend. Each year, the number of airline passengers increases. We can model this relationship non-linearly or linearly. Here, I've decided to just model a linear relationship between the year feature and the outcome.</p>
    <p class="normal">The relationship between the previous month's passenger numbers and those in this month is again assumed to be linear.</p>
    <p class="normal">Finally, we can scale our <a id="_idIndexMarker839"/>predictions, similar to the inverse transformation of the standard transformation. You should remember the standard normalization from <em class="chapterRef">Chapter 3</em><em class="italic">, Preprocessing Time-Series</em>, as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_009.png" alt="" style="height: 2.5em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_10_010.png" alt="" style="height: 1em;"/> is the population mean and <img src="../Images/B17577_10_011.png" alt="" style="height: 1em;"/> is the population standard deviation.</p>
    <p class="normal">The inverse of this is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_012.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">Our formula is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_013.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_10_014.png" alt="" style="height: 1em;"/> are the airline passengers at time t and <img src="../Images/B17577_10_015.png" alt="" style="height: 0.8em;"/> is the prediction based on the embedded month and the year.</p>
    <p class="normal">We assume the <a id="_idIndexMarker840"/>network will learn to baseline, but might not learn the scale perfectly – so we'll help out.</p>
    <p class="normal">An illustration might help (from TensorBoard, TensorFlow's visualization toolkit):</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_14.png" alt="../../Desktop/Screenshot%202021-08-06%20at%2022.08.12.pn"/></figure>
    <p class="packt_figref">Figure 10.15: Model architecture: Fully connected network with embedding, scaling, and baseline</p>
    <p class="normal">We can see the three inputs, one of them (the months) going through an embedding layer, and another one through a (linear) projection. They all come together (concatenate) and go through a dense layer, where another math operation is performed on top.</p>
    <p class="normal">We'll need a few more imports first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers.experimental preprocessing
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Embedding, Flatten, Concatenate
<span class="hljs-keyword">from</span> tensorflow.keras.metrics <span class="hljs-keyword">import</span> (
  RootMeanSquaredError, MeanAbsoluteError
)
</code></pre>
    <p class="normal">Now, we redefine our network as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_model</span><span class="hljs-function">(</span><span class="hljs-params">train</span><span class="hljs-function">):</span>
  scale = tf.constant(train.passengers.std())
  cont_layer = Input(shape=<span class="hljs-number">1</span>)
  cat_layer = Input(shape=<span class="hljs-number">1</span>)
  embedded = Embedding(<span class="hljs-number">12</span>, <span class="hljs-number">5</span>)(cat_layer)
  emb_flat = Flatten()(embedded)
  year_input = Input(shape=<span class="hljs-number">1</span>)
  year_layer = Dense(<span class="hljs-number">1</span>)(year_input)
  hidden_output = Concatenate(-<span class="hljs-number">1</span>)([emb_flat, year_layer, cont_layer])
  output_layer = keras.layers.Dense(<span class="hljs-number">1</span>)(hidden_output)
  output = output_layer * scale + cont_layer
  model = keras.models.Model(inputs=[
    cont_layer, cat_layer, year_input
  ], outputs=output)
  model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'mse'</span>, optimizer=keras.optimizers.Adam(),
    metrics=[RootMeanSquaredError(), MeanAbsoluteError()])
  <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">We reinitialize our model:</p>
    <pre class="programlisting code"><code class="hljs-code">model = create_model(X_train)
</code></pre>
    <p class="normal">During training and for<a id="_idIndexMarker841"/> prediction purposes, we need to feed the three types of input separately like this:</p>
    <pre class="programlisting code"><code class="hljs-code">model.fit(
  (X_train[<span class="hljs-string">"passengers"</span>], X_train[<span class="hljs-string">"year"</span>], X_train[<span class="hljs-string">"month"</span>]),
  y_train, epochs=<span class="hljs-number">1000</span>,
  callbacks=[callback]
)
predicted = model.predict((X_test[<span class="hljs-string">"passengers"</span>], X_test[<span class="hljs-string">"year"</span>], X_test[<span class="hljs-string">"month"</span>]))
</code></pre>
    <p class="normal">You might notice that the training carries on for much longer in this configuration.</p>
    <p class="normal">This chart illustrates the fit we are achieving with our new network:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_15.png" alt="fcn_more_sophisticated.png"/></figure>
    <p class="packt_figref">Figure 10.16: Predicted against actual airplane passenger numbers: Fully connected network with embedding, scaling, and baseline</p>
    <p class="normal">This again looks much better than the previous network. We leave it as an exercise to the reader to try to improve this network further.</p>
    <p class="normal">We'll set up an RNN next.</p>
    <h2 id="_idParaDest-162" class="title">Recurrent neural network</h2>
    <p class="normal">We discussed in the<a id="_idIndexMarker842"/> theory section that recurrent neural networks can be very good at modeling long-term relationships between points in a time-series. Let's set up an RNN.</p>
    <p class="normal">We'll use the same dataset as before – the univariate values of airline passengers. In this case, our network is going to need a sequence of points for each training sample. At each training step, the RNN is going to be trained on points (passengers) leading up to the next passenger number.</p>
    <p class="normal">Please note that we can use TensorFlow (or even statsmodels' <code class="Code-In-Text--PACKT-">lagmat()</code>) utility functions for this purpose (and we will use them in <em class="chapterRef">Chapter 12</em>, <em class="italic">Case Studies</em>), but in this instance, we'll write this quickly ourselves.</p>
    <p class="normal">We'll need to resample our passenger numbers thus:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">wrap_data</span><span class="hljs-function">(</span><span class="hljs-params">df, lookback: </span><span class="hljs-built_in">int</span><span class="hljs-function">):</span>
  dataset = []
  <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(lookback, <span class="hljs-built_in">len</span>(df)+<span class="hljs-number">1</span>):
    features = {
        <span class="hljs-string">f"col_</span><span class="hljs-subst">{i}</span><span class="hljs-string">"</span>: <span class="hljs-built_in">float</span>(val) <span class="hljs-keyword">for</span> i, val <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(
          df.iloc[index-lookback:index].values
        )
    }
    row = pd.DataFrame.from_dict([features])
    row.index = [df.index[index-<span class="hljs-number">1</span>]]
    dataset.append(row)
  <span class="hljs-keyword">return</span> pd.concat(dataset, axis=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">This function does the job. It goes over all the points in the dataset and takes a sequence leading up to it. The number of points in the new sequence is defined by the parameter <code class="Code-In-Text--PACKT-">lookback</code>.</p>
    <p class="normal">Let's put it to use:</p>
    <pre class="programlisting code"><code class="hljs-code">LOOKBACK = <span class="hljs-number">10</span>
dataset = wrap_data(passengers, lookback=LOOKBACK)
dataset = dataset.join(passengers.shift(-<span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">We are using a lookback of 10. I've deliberately chosen a value that is not optimal. I'm leaving it as an exercise to the reader to choose one that's better and try it out.</p>
    <p class="normal">The last line in the code above joins the targets (lookahead 1) together with the sequences.</p>
    <p class="normal">We are ready to define <a id="_idIndexMarker843"/>our network, but let's get the imports out of the way:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow.keras <span class="hljs-keyword">as</span> keras
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Input, Bidirectional, LSTM, Dense
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
    <p class="normal">The network is defined by this function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_model</span><span class="hljs-function">(</span><span class="hljs-params">passengers</span><span class="hljs-function">):</span>
  input_layer = Input(shape=(LOOKBACK, <span class="hljs-number">1</span>))
  recurrent = Bidirectional(LSTM(<span class="hljs-number">20</span>, activation=<span class="hljs-string">"tanh"</span>))(input_layer)
  output_layer = Dense(<span class="hljs-number">1</span>)(recurrent)
  model = keras.models.Model(inputs=input_layer, outputs=output_layer)
  model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'mse'</span>, optimizer=keras.optimizers.Adagrad(),
    metrics=[keras.metrics.RootMeanSquaredError(), keras.metrics.MeanAbsoluteError()])
  <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">It's a bidirectional LSTM network. The result of the last layer is projected linearly as our output. I've set the activation function of the LSTM to <code class="Code-In-Text--PACKT-">tanh</code> in case you want to run this on a GPU runtime, so it will benefit from NVIDIA's GPU-accelerated library cuDNN. We are extracting the same metrics as in the previous practice.</p>
    <p class="normal">A few more preliminaries that you should be familiar with from the previous section are as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
callback = tf.keras.callbacks.EarlyStopping(monitor=<span class="hljs-string">'loss'</span>, patience=<span class="hljs-number">3</span>)
model = create_model(passengers)
X_train, X_test, y_train, y_test = train_test_split(
    dataset.drop(columns=<span class="hljs-string">"passengers"</span>),
    dataset[<span class="hljs-string">"passengers"</span>],
    shuffle=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">Let's train it:</p>
    <pre class="programlisting code"><code class="hljs-code">model.fit(X_train, y_train, epochs=<span class="hljs-number">1000</span>, callbacks=[callback])
</code></pre>
    <p class="normal">The result looks quite <a id="_idIndexMarker844"/>good already – even though we made a few sub-optimal choices:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_16.png" alt="rnn_passengers.png"/></figure>
    <p class="packt_figref">Figure 10.17: Recurrent neural network for passenger forecasts</p>
    <p class="normal">Given how easy this was to set up, this already looks very promising.</p>
    <p class="normal">Let's now try a causal ConvNet!</p>
    <h2 id="_idParaDest-163" class="title">Dilated causal convolutional neural network</h2>
    <p class="normal">This example is based on Krist Papadopoulos's SeriesNet implementation of the paper "<em class="italic">Conditional Time-Series Forecasting with Convolutional Neural Networks</em>," by Anastasia Borovykh and others.</p>
    <p class="normal">We'll implement this model <a id="_idIndexMarker845"/>together and we'll apply it to two datasets to see how it does out of the box. I won't be going through tweaking the data and architecture in this example.</p>
    <p class="normal">First, the imports:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv1D, Input, Add, Activation, Dropout
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential, Model
<span class="hljs-keyword">from</span> keras.layers.advanced_activations <span class="hljs-keyword">import</span> LeakyReLU, ELU
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> optimizers
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf 
</code></pre>
    <p class="normal">What is perhaps surprising is how easy it is to do a causal convolution in TensorFlow. Conv1D comes with a parameter, <code class="Code-In-Text--PACKT-">padding</code>, which can be specified as <code class="Code-In-Text--PACKT-">'causal'</code>. This simply pads the layer's input with zeros according to the causal nature, where output at time t only depends on the previous time steps, &lt;t. Please refer to the discussion in the ConvNets section in this chapter.</p>
    <p class="normal">This means that we can predict the values of early time steps in the frame.</p>
    <p class="normal">The main idea of this network is a residual block with causal convolutions. This code segment constructs the corresponding network architecture:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">DC_CNN_Block</span><span class="hljs-function">(</span><span class="hljs-params">nb_filter, filter_length, dilation</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">f</span><span class="hljs-function">(</span><span class="hljs-params">input_</span><span class="hljs-function">):</span>
        residual =    input_
        layer_out =   Conv1D(
            filters=nb_filter, kernel_size=filter_length, 
            dilation_rate=dilation, 
            activation=<span class="hljs-string">'linear'</span>, padding=<span class="hljs-string">'causal'</span>, use_bias=<span class="hljs-literal">False</span>
        )(input_)                    
        layer_out =   Activation(<span class="hljs-string">'selu'</span>)(layer_out)        
        skip_out =    Conv1D(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, activation=<span class="hljs-string">'linear'</span>, use_bias=<span class="hljs-literal">False</span>)(layer_out)        
        network_in =  Conv1D(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, activation=<span class="hljs-string">'linear'</span>, use_bias=<span class="hljs-literal">False</span>)(layer_out)                      
        network_out = Add()([residual, network_in])        
        <span class="hljs-keyword">return</span> network_out, skip_out    
    <span class="hljs-keyword">return</span> f 
</code></pre>
    <p class="normal">I've simplified this a bit to make it easier to read.</p>
    <p class="normal">The network itself just<a id="_idIndexMarker846"/> stacks these layers as a SkipNet follows up with a convolution:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">DC_CNN_Model</span><span class="hljs-function">(</span><span class="hljs-params">length</span><span class="hljs-function">):</span>
    <span class="hljs-built_in">input</span> = Input(shape=(length,<span class="hljs-number">1</span>))
    l1a, l1b = DC_CNN_Block(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)(<span class="hljs-built_in">input</span>)    
    l2a, l2b = DC_CNN_Block(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)(l1a) 
    l3a, l3b = DC_CNN_Block(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>)(l2a)
    l4a, l4b = DC_CNN_Block(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">8</span>)(l3a)
    l5a, l5b = DC_CNN_Block(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">16</span>)(l4a)
    l6a, l6b = DC_CNN_Block(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">32</span>)(l5a)
    l6b = Dropout(<span class="hljs-number">0.8</span>)(l6b)
    l7a, l7b = DC_CNN_Block(<span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">64</span>)(l6a)
    l7b = Dropout(<span class="hljs-number">0.8</span>)(l7b)
    l8 =   Add()([l1b, l2b, l3b, l4b, l5b, l6b, l7b])
    l9 =   Activation(<span class="hljs-string">'relu'</span>)(l8)   
    l21 =  Conv1D(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, activation=<span class="hljs-string">'linear'</span>, use_bias=<span class="hljs-literal">False</span>)(l9)
    model = Model(inputs=<span class="hljs-built_in">input</span>, outputs=l21)
    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'mae'</span>, optimizer=optimizers.Adam(), metrics=[<span class="hljs-string">'mse'</span>])
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">This is for a univariate time-series. For a multivariate time-series, some changes are necessary, which we won't cover here.</p>
    <p class="normal">Let's forecast passenger numbers again. We'll load the DataFrame as in the previous practice sections:</p>
    <pre class="programlisting code"><code class="hljs-code">passengers = pd.read_csv(
  <span class="hljs-string">"passengers.csv"</span>, parse_dates=[<span class="hljs-string">"date "</span>]
).set_index(<span class="hljs-string">"date"</span>)
</code></pre>
    <p class="normal">We'll split this again into test and training sets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    passengers.passengers, passengers.passengers.shift(-<span class="hljs-number">1</span>), shuffle=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">We'll train the model with this function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">fit_model</span><span class="hljs-function">(</span><span class="hljs-params">timeseries</span><span class="hljs-function">):</span>
    length = <span class="hljs-built_in">len</span>(timeseries)-<span class="hljs-number">1</span>
    model = DC_CNN_Model(length)
    model.summary()
    X = timeseries[:-<span class="hljs-number">1</span>].reshape(<span class="hljs-number">1</span>,length, <span class="hljs-number">1</span>)
    y = timeseries[<span class="hljs-number">1</span>:].reshape(<span class="hljs-number">1</span>,length, <span class="hljs-number">1</span>)
    model.fit(X, y, epochs=<span class="hljs-number">3000</span>, callbacks=[callback])
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">This function will do the<a id="_idIndexMarker847"/> forecast for us:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">forecast</span><span class="hljs-function">(</span><span class="hljs-params">model, timeseries, horizon: </span><span class="hljs-built_in">int</span><span class="hljs-function">):</span>
    length = <span class="hljs-built_in">len</span>(timeseries)-<span class="hljs-number">1</span>
    pred_array = np.zeros(horizon).reshape(<span class="hljs-number">1</span>, horizon, <span class="hljs-number">1</span>)
    X_test_initial = timeseries[<span class="hljs-number">1</span>:].reshape(<span class="hljs-number">1</span>,length,<span class="hljs-number">1</span>)
    pred_array[: ,<span class="hljs-number">0</span>, :] = model.predict(X_test_initial)[:, -<span class="hljs-number">1</span>:, :]
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(horizon-<span class="hljs-number">1</span>):
        pred_array[:, i+<span class="hljs-number">1</span>:, :] = model.predict(
            np.append(
                X_test_initial[:, i+<span class="hljs-number">1</span>:, :], 
                pred_array[:, :i+<span class="hljs-number">1</span>, :]
            ).reshape(<span class="hljs-number">1</span>, length, <span class="hljs-number">1</span>))[:, -<span class="hljs-number">1</span>:, :]
    <span class="hljs-keyword">return</span> pred_array.flatten()
</code></pre>
    <p class="normal">The forecast is created by predicting the immediate next future value based on the previous predictions. The parameter <code class="Code-In-Text--PACKT-">horizon</code> is the forecast horizon.</p>
    <p class="normal">We'll put this together as a single function for convenience:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">evaluate_timeseries</span><span class="hljs-function">(</span><span class="hljs-params">series, horizon: </span><span class="hljs-built_in">int</span><span class="hljs-function">):</span>
    model = fit_model(series)
    pred_array = forecast(model, series, horizon)
    <span class="hljs-keyword">return</span> pred_array, model
</code></pre>
    <p class="normal">Now we are ready for training. We'll run everything like this:</p>
    <pre class="programlisting code"><code class="hljs-code">HORIZON = len(y_test)
predictions, model = evaluate_timeseries(
    X_train.values.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), horizon= HORIZON
)
</code></pre>
    <p class="normal">The model is very deep, but is not that big in terms of parameters because of the convolutions. We'll see that there are 865 trainable parameters.</p>
    <p class="normal">The model fit is not that good though, neither in MSE nor does that graph look very impressive either:</p>
    <figure class="mediaobject"><img src="../Images/B17577_10_17.png" alt="convnet_passengers.png"/></figure>
    <p class="packt_figref">Figure 10.18: ConvNet prediction of passenger numbers</p>
    <p class="normal">This graph can be <a id="_idIndexMarker848"/>produced by running <code class="Code-In-Text--PACKT-">show_result(y_test[:HORIZON], predictions[:HORIZON], "Passengers")</code>.</p>
    <p class="normal">This highlights the fact that each model has its strengths and weaknesses and that without adapting a model to our dataset and careful preprocessing, we won't be able to get a good performance. It's left as an exercise to the reader to try and tweak this model.</p>
    <h1 id="_idParaDest-164" class="title">Summary</h1>
    <p class="normal">In this chapter, I've introduced many deep learning concepts relevant to time-series, and we've discussed many architectures and algorithms, such as autoencoders, InceptionTime, DeepAR, N-BEATS, ConvNets, and a few transformer architectures. Deep learning algorithms are indeed coming very close to the state of the art in time-series, and it's an exciting area of research and application.</p>
    <p class="normal">In the practice section, I implemented a fully connected feedforward network and then an RNN before taking a causal ConvNet for a ride.</p>
    <p class="normal">In <em class="chapterRef">Chapter 12</em>, <em class="italic">Multivariate Forecasting</em>, we'll do some more deep learning, including a Transformer model and an LSTM.</p>
  </div>
</body></html>
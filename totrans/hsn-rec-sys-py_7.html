<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Hybrid Recommenders</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this final chapter, we will discuss recommender systems in the context of practicality and industrial use. Until now, we have learned about various types of recommender, including knowledge, content, and collaborative filtering-based engines. However, when used in practice, each recommender usually suffers from one shortcoming or another. </p>
<p>We've discussed these shortcomings in the very first chapter (for instance, the novelty problem of content-based engines and the cold start problem of collaborative filters). We also briefly introduced the concept of the hybrid recommender: a robust system that combines various models to combat the disadvantage of one model with the advantage of another. In this chapter, we will build a simple hybrid recommender that combines the content and the collaborative filters that we've built thus far.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will be required to have Python installed on a system. Finally, to use the Git repository of this book, the user needs to install Git.</p>
<p>The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Hands-On-Recommendation-Systems-with-Python">https://github.com/PacktPublishing/Hands-On-Recommendation-Systems-with-Python</a>.</p>
<p>Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2uOHwcd">http://bit.ly/2uOHwcd</a><a href="http://bit.ly/2uOHwcd">.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>As already mentioned a couple of times, hybrid recommenders are extremely powerful, robust systems that combine various simpler models to give us predictions. There is no single way in which a hybrid model could do this; some hybrids predict using content and collaborative filtering techniques separately to produce results. Some others introduce content-based techniques into collaborative filters and vice versa. </p>
<p>Netflix is a very good example of a hybrid recommender. Netflix employs content-based techniques when it shows you similar movies to a movie you're watching (the <span class="packt_screen">MORE LIKE THIS</span> section), as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/04d435e9-dd63-48ba-ad20-a9897c2d2cb8.png"/></div>
<p>Here, we can see that while watching <em>Ratatouille, </em>Netflix recommends movies to me that are very similar to <em>Ratatouille</em>. All the top five recommended movies are all animated and produced by Disney Pixar<em>.</em></p>
<p>However, animated movies are not the only genre I watch on Netflix. I also like watching drama and comedy. Netflix has a separate row of recommendations for me entitled <span class="packt_screen">Top Picks for Rounak</span><em>, </em>where it uses collaborative filtering to identify users similar to me and recommend movies that they have liked, but that I haven't watched:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/230d060c-e096-41bd-9f01-26053402507b.png"/></div>
<p>In this way, Netflix employs both content- and collaborative-based techniques separately to produce results that are extremely satisfactory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Case study – Building a hybrid model</h1>
                </header>
            
            <article>
                
<p>In this section, let's build a content-based model that incorporates some collaborative filtering techniques into it.</p>
<p>Imagine that you have built a website like Netflix. Every time a user watches a movie, you want to display a list of recommendations in the side pane (like YouTube). At first glance, a content-based recommender seems appropriate for this task. This is because, if the person is currently watching something they find interesting, they will be more inclined to watch something similar to it. </p>
<p>Let's say our user is watching <em>The Dark Knight. </em>Since this is a Batman movie, our content-based recommender is likely to recommend other Batman (or superhero) movies regardless of quality. This may not always lead to the best recommendations. For instance, most people who like <em>The Dark Knight </em>do not rate <em>Batman and Robin </em>very highly, although they feature the same lead character. Therefore, we will introduce a collaborative filter here that predicts the ratings of the movies recommended by our content-based model and return the top few movies with the highest predictions.</p>
<p>In other words, the workflow of our hybrid model will be as follows:</p>
<ol>
<li>Take in a movie title and user as input</li>
<li>Use a content-based model to compute the 25 most similar movies</li>
<li>Compute the predicted ratings that the user might give these 25 movies using a collaborative filter</li>
<li>Return the top 10 movies with the highest predicted rating</li>
</ol>
<p>We will be using different datasets for this task. Go ahead and download the datasets from the following links.</p>
<p class="mce-root">Download the following datasets from Kaggle and Google Drive:</p>
<ul>
<li class="mce-root"> <kbd>ratings_small.csv</kbd>: <a href="https://www.kaggle.com/rounakbanik/the-movies-dataset/downloads/ratings_small.csv/7">https://www.kaggle.com/rounakbanik/the-movies-dataset/downloads/ratings_small.csv/7</a>.</li>
<li class="mce-root"><kbd>movie_ids.csv</kbd>: <a href="https://drive.google.com/drive/folders/1H9pnfVTzP46s7VwOTcC5ZY_VahRTr5Zv?usp=sharing">https://drive.google.com/drive/folders/1H9pnfVTzP46s7VwOTcC5ZY_VahRTr5Zv?usp=sharing</a>.</li>
</ul>
<p>The <kbd>ratings_small.csv</kbd> file contains 100,000 ratings for 9,000 movies from 700 users. We use this file since it contains ratings for more recent movies (the dataset we used for collaborative filtering only contained movies released before 1998).</p>
<p>The<kbd> links_small.csv</kbd><em> </em>file contains the movie IDs of all the movies rated in the<kbd> ratings_small.csv</kbd><em> </em>file and their corresponding titles. We can use these IDs to extract relevant metadata from the <kbd>movies_metadata.csv</kbd><em> </em>file.</p>
<p>With these files in hand, let's proceed to build our model. The first step is to compute the <kbd>cosine_sim</kbd><em> </em>matrix for our movies. In addition, we also need to map every movie to the indices in the <kbd>cosine_sim</kbd><em> </em>matrix. We've already learned how to do this in <a href="336e10ee-05f9-46e4-9b65-26b0a9cff2dc.xhtml" target="_blank">Chapter 3</a>, <em>Building an IMDB Top 250 Clone with Pandas</em>. Computing this matrix and the mapping, therefore, is left as an exercise for the reader. </p>
<div class="packt_infobox">You can download my <kbd>cosine_sim </kbd>and<kbd> cosine_sim_map</kbd><em> </em>files from the following link:<br/>
<a href="https://drive.google.com/drive/folders/1H9pnfVTzP46s7VwOTcC5ZY_VahRTr5Zv?usp=sharing">https://drive.google.com/drive/folders/1H9pnfVTzP46s7VwOTcC5ZY_VahRTr5Zv?usp=sharing</a>. However, be aware that the <kbd>cosine_sim</kbd><em> </em>file is over 1 GB in size, and therefore might take some time to download.</div>
<p>Next, let's use the <kbd>ratings.csv</kbd><em> </em>file to build a collaborative filtering model. We will use the SVD model from the last chapter for this purpose:</p>
<pre>#Build the SVD based Collaborative filter<br/>from surprise import SVD, Reader, Dataset<br/><br/>reader = Reader()<br/>ratings = pd.read_csv('../data/ratings_small.csv')<br/>data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)<br/>data.split(n_folds=5)<br/>svd = SVD()<br/>trainset = data.build_full_trainset()<br/>svd.train(trainset)</pre>
<p>Next, let's load the <kbd>movie_ids.cs</kbd><kbd>v</kbd> file into a DataFrame and construct two mappings: one that returns the movie title for a given movie ID, and the other vice versa:</p>
<pre>#Build title to ID and ID to title mappings<br/>id_map = pd.read_csv('../data/movie_ids.csv')<br/>id_to_title = id_map.set_index('id')<br/>title_to_id = id_map.set_index('title')</pre>
<p>Now, let's import the metadata for our movies so that our recommender can display useful information, such as the IMDB rating and the year of release. This information can be extracted from the main <kbd>movies_metadata.csv </kbd>file, and is again left as an exercise for the reader. </p>
<div class="packt_infobox">You can download the required metadata file from the following link: <a href="https://drive.google.com/drive/folders/1H9pnfVTzP46s7VwOTcC5ZY_VahRTr5Zv?usp=sharing">https://drive.google.com/drive/folders/1H9pnfVTzP46s7VwOTcC5ZY_VahRTr5Zv?usp=sharing</a></div>
<p>We're finally in a position to build the hybrid recommender function according to the workflow described previously:</p>
<pre>def hybrid(userId, title):<br/>    #Extract the cosine_sim index of the movie<br/>    idx = cosine_sim_map[title]<br/>    <br/>    #Extract the TMDB ID of the movie<br/>    tmdbId = title_to_id.loc[title]['id']<br/>    <br/>    #Extract the movie ID internally assigned by the dataset<br/>    movie_id = title_to_id.loc[title]['movieId']<br/>    <br/>    #Extract the similarity scores and their corresponding index for every movie from the cosine_sim matrix<br/>    sim_scores = list(enumerate(cosine_sim[str(int(idx))]))<br/>    <br/>    #Sort the (index, score) tuples in decreasing order of similarity scores<br/>    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)<br/>    <br/>    #Select the top 25 tuples, excluding the first <br/>    #(as it is the similarity score of the movie with itself)<br/>    sim_scores = sim_scores[1:26]<br/>    <br/>    #Store the cosine_sim indices of the top 25 movies in a list<br/>    movie_indices = [i[0] for i in sim_scores]<br/><br/>    #Extract the metadata of the aforementioned movies<br/>    movies = smd.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year', 'id']]<br/>    <br/>    #Compute the predicted ratings using the SVD filter<br/>    movies['est'] = movies['id'].apply(lambda x: svd.predict(userId, id_to_title.loc[x]['movieId']).est)<br/>    <br/>    #Sort the movies in decreasing order of predicted rating<br/>    movies = movies.sort_values('est', ascending=False)<br/>    <br/>    #Return the top 10 movies as recommendations<br/>    return movies.head(10)</pre>
<p>Let's put our hybrid model to the test. Let's imagine that users with the IDs 1 and 2 are both watching the movie <em>Avatar:</em></p>
<pre>hybrid(1, 'Avatar')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b514f6a5-92ae-4cf5-96eb-2eb95cb05659.png" style="width:38.17em;height:20.67em;"/></div>
<pre>hybrid(2, 'Avatar')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/339539a5-61d0-43a8-8009-c42bdae49308.png" style="width:38.33em;height:21.67em;"/></div>
<p>We can see that although both users are currently watching <em>Avatar, </em>the recommendations differ in the content as well as the order. This is influenced by the collaborative filter. However, all the movies listed are similar<em> </em>to  <em>Avatar. </em>This is because of the content-based filtering carried out by the model.</p>
<p>Following this section may have been a little hard, especially if you do not recall the material covered in <a href="https://cdp.packtpub.com/hands_on_recommendation_systems_with_python/wp-admin/post.php?post=30&amp;action=edit#post_26" target="_blank">Chapter 3</a><span>, </span><em>Building an IMDB Top 250 Clone with Pandas</em>. I strongly recommend going back and rereading the chapter if something doesn't make sense. For reference, the entire code for this model can be found in the <kbd>Chapter7 </kbd>folder of the <kbd>RecoSys</kbd><em> </em>repository.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>With this, we come to the end of this chapter, as well as the main part of the book. In this book, we learned the following:</p>
<ul>
<li>We were introduced to the world of recommender systems. We defined the recommendation problem mathematically and discussed the various types of recommendation engines that exist, as well as their advantages and disadvantages.</li>
<li>We then learned to perform data wrangling with the pandas library and familiarized ourselves with two of pandas, most powerful data structures: the series and the DataFrame.</li>
<li>With our newly found data wrangling techniques, we proceeded to build an IMDB Top 250 clone. We then improved on this model to build a knowledge-based recommender that took into account the recommended movies' genre, duration, and year of release.</li>
<li>Next, we learned how to build content-based recommenders using plot lines and subsequently more sophisticated metadata, such as the genre, cast, crew, and keywords. In the process, we familiarized ourselves with vectorizers and the cosine similarity metric.</li>
<li>In the chapter on data mining, we were introduced to the various techniques used in building and improving recommendation systems. We learned about similarity metrics other than the cosine score. We then proceeded to study clustering, with an emphasis on k-means clustering techniques. This was followed by discussions on dimensionality reduction (with an emphasis on PCA) and the various supervised learning techniques. The chapter concluded with a tour of evaluation metrics that are used to gauge the performance of recommender systems.</li>
<li>The chapter on collaborative filtering had us experimenting with a variety of models that used rating data, and also leveraged data mining techniques introduced in the previous chapter. We were also introduced to the <kbd>surprise</kbd> library, which made building recommender systems a breeze.</li>
<li>In this final chapter, we briefly discussed the various kinds of hybrid recommender used in the industry today and built a model that incorporated collaborative filtering into a content-based engine to offer personalized recommendations to a user, while keeping the current movie being watched in mind.</li>
</ul>
<p class="mce-root">What we have covered, of course, only touches the surface of the world of recommender systems. However, I am positive that readers will now be in a very good place to tackle advanced topics in the field. I have listed a few resources in the <kbd>Appendix</kbd> that could serve as a next stop on your journey to becoming a recommendations master.</p>
<p>As mentioned earlier, all the code written as part of this book is available as a GitHub repository to enable you to <span>effortlessly</span><span> tinker and experiment with the code as you journey through this book. I'd like to thank you all for having come this far. If you have any comments, corrections, criticism, or suggestions, feel free to contact me at <kbd>rounakbanik@gmail.com</kbd>.</span></p>


            </article>

            
        </section>
    </body></html>
<html><head></head><body>
		<div id="_idContainer218" class="Content">
			<h1 id="_idParaDest-109"><em class="italics"><a id="_idTextAnchor118"/>Chapter 4</em></h1>
		</div>
		<div id="_idContainer219" class="Content">
			<h1 id="_idParaDest-110"><a id="_idTextAnchor119"/>Classification</h1>
		</div>
		<div id="_idContainer220" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Implement logistic regression and explain how it can be used to classify data into specific groups or classes</li>
				<li class="bullets">Use the K-nearest neighbors clustering algorithm for classification</li>
				<li class="bullets">Use decision trees for data classification, including the ID3 algorithm</li>
				<li class="bullets">Describe the concept of entropy within data</li>
				<li class="bullets">Explain how decision trees such as ID3 aim to reduce entropy</li>
				<li class="bullets">Use decision trees for data classification</li>
			</ul>
			<p>This chapter introduces classification problems, classification using linear and logistic regression, K-nearest neighbors classification, and decision trees.</p>
		</div>
		<div id="_idContainer308" class="Content">
			<h2 id="_idParaDest-111"><a id="_idTextAnchor120"/>Introduction</h2>
			<p>In the previous chapter, we began our supervised machine learning journey using regression techniques, predicting the continuous variable output given a set of input data. We will now turn to the other sub-type of machine learning problems that we previously described: classification problems. Recall that classification tasks aim to predict, given a set of input data, which one of a specified number of groups of classes data belongs to.</p>
			<p>In this chapter, we will extend the concepts learned in <em class="italics">Chapter 3</em>, <em class="italics">Regression Analysis</em>, and will apply them to a dataset labeled with classes, rather than continuous values, as output.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor121"/>Linear Regression as a Classifier</h2>
			<p>We covered linear regression in the context of predicting continuous variable output in the previous chapter, but it can also be used to predict the class that a set of data is a member of. Linear regression classifiers are not as powerful as other types of classifiers that we will cover in this chapter, but they are particularly useful in understanding the process of classification. Let's say we had a fictional dataset containing two separate groups, Xs and Os, as shown in <em class="italics">Figure 4.1</em>. We could construct a linear classifier by first using linear regression to fit the equation of a straight line to the dataset. For any value that lies above the line, the <em class="italics">X</em> class would be predicted, and for any value beneath the line, the <em class="italics">O</em> class would be predicted. Any dataset that can be separated by a straight line is known as linearly separable, which forms an important subset of data types in machine learning problems. While this may not be particularly helpful in the context of a linear regression-based classifier, it often is in the case of other classifiers, such as <strong class="keyword">support vector machines</strong> (<strong class="keyword">SVM</strong>), decision trees, and linear neural network-based classifiers.</p>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<img src="image/C12622_04_01.jpg" alt="Figure 4.1: Linear regression as a classifier"/>
				</div>
			</div>
			<h6>Figure 4.1: Linear regression as a classifier</h6>
			<h3 id="_idParaDest-113"><a id="_idTextAnchor122"/>Exercise 36: Linear Regression as a Classifier</h3>
			<p>This exercise contains a contrived example of using linear regression as a classifier. In this exercise, we will use a completely fictional dataset, and test how linear regression fares as a classifier. The dataset is composed of manually selected <em class="italics">x</em> and <em class="italics">y</em> values for a scatterplot that are approximately divided into two groups. The dataset has been specifically designed for this exercise, to demonstrate how linear regression can be used as a classifier, and this is available in the accompanying code files for this book, as well as on GitHub, at <a href="https://github.com/TrainingByPackt/Supervised-Learning-with-Python">https://github.com/TrainingByPackt/Supervised-Learning-with-Python</a>.</p>
			<ol>
				<li>Load the <strong class="inline">linear_classifier.csv</strong> dataset into a pandas DataFrame:<p class="snippet">df = pd.read_csv('linear_classifier.csv')</p><p class="snippet">df.head()</p><p>The output will be as follows:</p><div id="_idContainer222" class="IMG---Figure"><img src="image/C12622_04_02.jpg" alt="Figure 4.2: First five rows"/></div><h6>Figure 4.2: First five rows</h6><p>Looking through the dataset, each row contains a set of <em class="italics">x, y</em> coordinates, as well as the label corresponding to which class the data belongs to, either a cross (<strong class="bold">x</strong>) or a circle (<strong class="bold">o</strong>).</p></li>
				<li>Produce a scatterplot of the data with the marker for each point as the corresponding class label:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for label, label_class in df.groupby('labels'):</p><p class="snippet">    plt.scatter(label_class.values[:,0], label_class.values[:,1],</p><p class="snippet">                label=f'Class {label}', marker=label, c='k')</p><p class="snippet">plt.legend()</p><p class="snippet">plt.title("Linear Classifier");</p><p>We'll get the following scatterplot:</p><div id="_idContainer223" class="IMG---Figure"><img src="image/C12622_04_03.jpg" alt="Figure 4.3 Scatterplot of a linear classifier"/></div><h6>Figure 4.3 Scatterplot of a linear classifier</h6></li>
				<li>Using the scikit-learn <strong class="inline">LinearRegression</strong> API from the previous chapter, fit a linear model to the <em class="italics">x</em>, <em class="italics">y</em> coordinates of the dataset and print out the linear equation:<p class="snippet"># Fit a linear regression model</p><p class="snippet">model = LinearRegression()</p><p class="snippet">model.fit(df.x.values.reshape((-1, 1)), df.y.values.reshape((-1, 1)))</p><p class="snippet"># Print out the parameters</p><p class="snippet">print(f'y = {model.coef_[0][0]}x + {model.intercept_[0]}')</p><p>The output will be:</p><div id="_idContainer224" class="IMG---Figure"><img src="image/C12622_04_04.jpg" alt="Figure 4.4: Output of model fitting"/></div><h6>Figure 4.4: Output of model fitting</h6></li>
				<li>Plot the fitted trendline over the dataset:<p class="snippet"># Plot the trendline</p><p class="snippet">trend = model.predict(np.linspace(0, 10).reshape((-1, 1)))</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for label, label_class in df.groupby('labels'):</p><p class="snippet">    plt.scatter(label_class.values[:,0], label_class.values[:,1],</p><p class="snippet">                label=f'Class {label}', marker=label, c='k')</p><p class="snippet">plt.plot(np.linspace(0, 10), trend, c='k', label='Trendline')</p><p class="snippet">plt.legend()</p><p class="snippet">plt.title("Linear Classifier");</p><p>The output will be as follows:</p><div id="_idContainer225" class="IMG---Figure"><img src="image/C12622_04_05.jpg" alt="Figure 4.5: Scatterplot with trendline"/></div><h6>Figure 4.5: Scatterplot with trendline</h6></li>
				<li>With the fitted trendline, the classifier can then be applied. For each row in the dataset, determine whether the <em class="italics">x, y</em> point lies above or below the linear model (or trendline). If the point lies below the trendline, the model predicts the <strong class="bold">o</strong> class, if above the line, the <strong class="bold">x</strong> class is predicted. Include these values as a column of predicted labels:<p class="snippet"># Make predictions</p><p class="snippet">y_pred = model.predict(df.x.values.reshape((-1, 1)))</p><p class="snippet">pred_labels = []</p><p class="snippet">for _y, _y_pred in zip(df.y, y_pred):</p><p class="snippet">    if _y &lt; _y_pred:</p><p class="snippet">        pred_labels.append('o')</p><p class="snippet">    else:</p><p class="snippet">        pred_labels.append('x')</p><p class="snippet">df['Pred Labels'] = pred_labels</p><p class="snippet">df.head()</p><p>The output will be as follows:</p><div id="_idContainer226" class="IMG---Figure"><img src="image/C12622_04_06.jpg" alt="Figure 4.6: First five rows"/></div><h6>Figure 4.6: First five rows</h6></li>
				<li>Plot the points with the corresponding ground truth labels. For those points where the labels were correctly predicted, plot the corresponding class. For those incorrect predictions, plot a diamond:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for idx, label_class in df.iterrows():</p><p class="snippet">    if label_class.labels != label_class['Pred Labels']:</p><p class="snippet">        label = 'D'</p><p class="snippet">        s=70</p><p class="snippet">    else:</p><p class="snippet">        label = label_class.labels</p><p class="snippet">        s=50</p><p class="snippet">    plt.scatter(label_class.values[0], label_class.values[1],</p><p class="snippet">                label=f'Class {label}', marker=label, c='k', s=s)</p><p class="snippet">plt.plot(np.linspace(0, 10), trend, c='k', label='Trendline')</p><p class="snippet">plt.title("Linear Classifier");</p><p class="snippet">incorrect_class = mlines.Line2D([], [], color='k', marker='D',</p><p class="snippet">                          markersize=10, label='Incorrect Classification');</p><p class="snippet">plt.legend(handles=[incorrect_class]);</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/C12622_04_07.jpg" alt="Figure 4.7: Scatterplot showing incorrect predictions"/>
				</div>
			</div>
			<h6>Figure 4.7: Scatterplot showing incorrect predictions</h6>
			<p>We can see that, in this plot, the linear classifier made two incorrect predictions in this completely fictional dataset, one at <em class="italics">x = 1</em>, another at <em class="italics">x = 3</em>.</p>
			<p>But what if our dataset is not linearly separable and we cannot classify the data using a straight line model, which is very frequently the case. In this scenario, we turn to other classification methods, many of which use different models, but the process logically flows from our simplified linear classifier model.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor123"/>Logistic Regression</h2>
			<p>The <strong class="keyword">logistic</strong> or <strong class="keyword">logit</strong> model is one such non-linear model that has been effectively used for classification tasks in a number of different domains. In this section, we will use it to classify images of hand-written digits. In understanding the logistic model, we also take an important step in understanding the operation of a particularly powerful machine learning model, <strong class="keyword">artificial neural networks</strong>. So, what exactly is the logistic model? Like the linear model, which is composed of a linear or straight-line function, the logistic model is composed of the standard logistic function, which, in mathematical terms, looks something like this:</p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="image/C12622_04_08.jpg" alt="Figure 4.8: Logistic function"/>
				</div>
			</div>
			<h6>Figure 4.8: Logistic function</h6>
			<p>In practical terms, when trained, this function returns the probability of the input information belonging to a particular class or group.</p>
			<p>Say we would like to predict whether a single entry of data belongs to one of two groups. As in the previous example, in linear regression, this would equate to <em class="italics">y</em> being either zero or one, and <em class="italics">x</em> can take a value between <img src="image/C12622_Formula_04_02.png" alt=""/> and <img src="image/C12622_Formula_04_03.png" alt=""/>:</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/C12622_04_09.jpg" alt="Figure 4.9: Equation for y"/>
				</div>
			</div>
			<h6>Figure 4.9: Equation for y</h6>
			<p>A range of zero to one and <img src="image/C12622_Formula_04_02.png" alt=""/> to <img src="image/C12622_Formula_04_03.png" alt=""/> are significantly different; to improve this, we will calculate the odds ratio, which will then vary from greater than zero to less than <img src="image/C12622_Formula_04_03.png" alt=""/>, which is a step in the right direction:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/C12622_04_10.jpg" alt="Figure 4.10: Odds ratio"/>
				</div>
			</div>
			<h6>Figure 4.10: Odds ratio</h6>
			<p>We can use the mathematical relationships of the natural log to reduce this even further. As the odds ratio approaches zero, <img src="image/C12622_Formula_04_04.png" alt=""/>ss approaches <img src="image/C12622_Formula_04_02.png" alt=""/>; similarly, as the odds ratio approaches one, <img src="image/C12622_Formula_04_04.png" alt=""/> approaches <img src="image/C12622_Formula_04_03.png" alt=""/>. This is exactly what we want; that is, for the two classification options to be as far apart as possible:</p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/C12622_04_11.jpg" alt="Figure 4.11: Natural log of classified points"/>
				</div>
			</div>
			<h6>Figure 4.11: Natural log of classified points</h6>
			<p>With a little bit of equation re-arranging we get the logistic function:</p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/C12622_04_12.jpg" alt="Figure 4.12: Logistic function"/>
				</div>
			</div>
			<h6>Figure 4.12: Logistic function</h6>
			<p>Notice the exponents of <em class="italics">e</em>, that is, <img src="image/C12622_Formula_04_05.png" alt=""/>, and that this relationship is a linear function with two training parameters or <em class="italics">weights</em>, <img src="image/C12622_Formula_04_06.png" alt=""/> and <img src="image/C12622_Formula_04_02.png" alt=""/>, as well as the input feature, <em class="italics">x</em>. If we were to plot the logistic function over the range <em class="italics">(-6, 6)</em>, we would get the following result:</p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/C12622_04_13.jpg" alt="Figure 4.13: Logistic function curve"/>
				</div>
			</div>
			<h6>Figure 4.13: Logistic function curve</h6>
			<p>Examining <em class="italics">Figure 4.13</em>, we can see some features that are important for a classification task. The first thing to note is that, if we look at the probability values on the <em class="italics">y</em> axis at the extremes of the function, the values are almost at zero when <em class="italics">x = -6</em> and at one when <em class="italics">x = 6</em>. While it looks like the values are in fact zero and one, this is not exactly the case. The logistic function approaches zero and one at these extremes and will only equal zero and one when <em class="italics">x</em> is at a positive or negative infinity. In practical terms, what this means is that the logistic function will never return a probability of one or greater or less than or equal to zero, which is perfect for a classification task. We can never have a probability of greater than one, as, by definition, a probability of one is a certainty of an event occurring. Likewise, we cannot have a probability of less than zero, as, by definition, a probability of zero is a certainty of the event not occurring. The fact that the logistic function approaches but never equals one or zero means that there is always some uncertainty in the outcome or the classification. </p>
			<p>The final feature to notice about the logistic function is that at <em class="italics">x = 0</em>, the probability is 0.5, which, if we were to get this result, would indicate that the model is equally uncertain about the outcome of the corresponding class; that is, it really has no idea. Typically, this is the default position at the start of training, and, as the model is exposed to training data, it becomes more confident in its decisions.</p>
			<h4>Note</h4>
			<p class="callout">It is very important to correctly understand and interpret the probability information provided by classification models such as linear regression. Consider this probability score as the chance of the input information belonging to a particular class given the variability in the information provided by the training data. One common mistake is to use this probability score as an objective measure of whether the model can be trusted about its prediction; unfortunately, this isn't necessarily the case. <em class="italics">A model can provide a probability of 99.99% that some data belongs to a particular class and still be 99.99% wrong</em>.</p>
			<p>What we do use the probability value for is selecting the predicted class by the classifier. Say we had a model that was to predict whether some set of data belonged to class A or class B. If the logistic model returned a probability of 0.7 for class A, then we would return class A as the predicted class for the model. If the probability was only 0.2, the predicted class for the model would be class B.</p>
			<h3 id="_idParaDest-115"><a id="_idTextAnchor124"/>Exercise 37: Logistic Regression as a Classifier – Two-Class Classifier</h3>
			<p>For this exercise, we will be using a sample of the famous MNIST dataset (available at <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> or on GitHub at <a href="https://github.com/TrainingByPackt/Supervised-Learning-with-Python">https://github.com/TrainingByPackt/Supervised-Learning-with-Python</a>), which is a sequence of images of handwritten postcode digits, zero through nine, with corresponding labels. The MNIST dataset is comprised of 60,000 training samples and 10,000 test samples, where each sample is a grayscale image with a size of 28 x 28 pixels. In this exercise, we will use logistic regression to build a classifier. The first classifier we will build is a two-class classifier, where we will determine whether the image is a handwritten zero or a one:</p>
			<ol>
				<li value="1">For this exercise, we will need to import a few dependencies. Execute the following import statements:<p class="snippet">import struct</p><p class="snippet">import numpy as np</p><p class="snippet">import gzip</p><p class="snippet">import urllib.request</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from array import array</p><p class="snippet">from sklearn.linear_model import LogisticRegression</p></li>
				<li>We will also need to download the MNIST datasets. You will only need to do this once, so after this step, feel free to comment out or remove these cells. Download the image data, as follows:<p class="snippet">request = urllib.request.urlopen('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz')</p><p class="snippet">with open('train-images-idx3-ubyte.gz', 'wb') as f:</p><p class="snippet">    f.write(request.read())</p><p class="snippet">request = urllib.request.urlopen('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz')</p><p class="snippet">with open('t10k-images-idx3-ubyte.gz', 'wb') as f:</p><p class="snippet">    f.write(request.read())</p></li>
				<li>Download the corresponding labels for the data:<p class="snippet">request = urllib.request.urlopen('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz')</p><p class="snippet">with open('train-labels-idx1-ubyte.gz', 'wb') as f:</p><p class="snippet">    f.write(request.read())</p><p class="snippet">request = urllib.request.urlopen('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz')</p><p class="snippet">with open('t10k-labels-idx1-ubyte.gz', 'wb') as f:</p><p class="snippet">    f.write(request.read())</p></li>
				<li>Once all the files have been successfully downloaded, check out the files in the local directory using the following command for Windows:<p class="snippet">!dir *.gz</p><p>The output will be as follows:</p><div id="_idContainer246" class="IMG---Figure"><img src="image/C12622_04_14.jpg" alt="Figure 4.14: Files in directory"/></div><h6>Figure 4.14: Files in directory</h6><h4>Note</h4><p class="callout">For Linux and macOS, check out the files in the local directory using the <strong class="inline">!ls *.gz</strong> command.</p></li>
				<li>Load the downloaded data. Don't worry too much about the exact details of reading the data, as these are specific to the MNIST dataset:<p class="snippet">with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size, rows, cols = struct.unpack("&gt;IIII", f.read(16))</p><p class="snippet">    img = np.array(array("B", f.read())).reshape((size, rows, cols))</p><p class="snippet">with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size = struct.unpack("&gt;II", f.read(8))</p><p class="snippet">    labels = np.array(array("B", f.read()))</p><p class="snippet">with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size, rows, cols = struct.unpack("&gt;IIII", f.read(16))</p><p class="snippet">    img_test = np.array(array("B", f.read())).reshape((size, rows, cols))</p><p class="snippet">with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size = struct.unpack("&gt;II", f.read(8))</p><p class="snippet">    labels_test = np.array(array("B", f.read()))</p></li>
				<li>As always, having a thorough understanding of the data is key, so create an image plot of the first 10 images in the training sample. Notice the grayscale images and that the corresponding labels are the digits zero through nine:<p class="snippet">for i in range(10):</p><p class="snippet">    plt.subplot(2, 5, i + 1)</p><p class="snippet">    plt.imshow(img[i], cmap='gray');</p><p class="snippet">    plt.title(f'{labels[i]}');</p><p class="snippet">    plt.axis('off')</p><p>The output will be as follows:</p><div id="_idContainer247" class="IMG---Figure"><img src="image/C12622_04_15.jpg" alt="Figure 4.15: Training images"/></div><h6>Figure 4.15: Training images</h6></li>
				<li>As the initial classifier is aiming to classify either images of zeros or images of ones, we must first select these samples from the dataset:<p class="snippet">samples_0_1 = np.where((labels == 0) | (labels == 1))[0]</p><p class="snippet">images_0_1 = img[samples_0_1]</p><p class="snippet">labels_0_1 = labels[samples_0_1]</p><p class="snippet">samples_0_1_test = np.where((labels_test == 0) | (labels_test == 1))</p><p class="snippet">images_0_1_test = img_test[samples_0_1_test].reshape((-1, rows * cols))</p><p class="snippet">labels_0_1_test = labels_test[samples_0_1_test]</p></li>
				<li>Visualize one sample from the zero selection and another from the handwritten one digits to ensure we have correctly allocated the data.<p>Here's the code for zero:</p><p class="snippet">sample_0 = np.where((labels == 0))[0][0]</p><p class="snippet">plt.imshow(img[sample_0], cmap='gray');</p><p>The output will be as follows:</p><div id="_idContainer248" class="IMG---Figure"><img src="image/C12622_04_16.jpg" alt="Figure 4.16: First handwritten image"/></div><h6>Figure 4.16: First handwritten image</h6><p>Here's the code for one:</p><p class="snippet">sample_1 = np.where((labels == 1))[0][0]</p><p class="snippet">plt.imshow(img[sample_1], cmap='gray');</p><p>The output will be as follows:</p><div id="_idContainer249" class="IMG---Figure"><img src="image/C12622_04_17.jpg" alt="Figure 4.17: Second handwritten image"/></div><h6>Figure 4.17: Second handwritten image</h6></li>
				<li>We are almost at the stage where we can start building the model, however, as each sample is an image and has data in a matrix format, we must first re-arrange each of the images. The model needs the images to be provided in vector form, that is, all the information for each image is stored in one row. Do that as follows:<p class="snippet">images_0_1 = images_0_1.reshape((-1, rows * cols))</p><p class="snippet">images_0_1.shape</p></li>
				<li>Now we can build and fit the logistic regression model with the selected images and labels:<p class="snippet">model = LogisticRegression(solver='liblinear')</p><p class="snippet">model.fit(X=images_0_1, y=labels_0_1)</p><p>The output will be:</p><div id="_idContainer250" class="IMG---Figure"><img src="image/C12622_04_18.jpg" alt="Figure 4.18: Logistic Regression Model"/></div><h6>Figure 4.18: Logistic Regression Model</h6><p>Note how the scikit-learn API calls for logistic regression are consistent with that of linear regression. There is an additional argument, <strong class="inline">solver</strong>, which specifies the type of optimization process to be used. We have provided this argument here with the default value to suppress a future warning in this version of scikit-learn that requires <strong class="inline">solver</strong> to be specified. The specifics of the <strong class="inline">solver</strong> argument are out of scope for this chapter and has only been included to suppress the warning message.</p></li>
				<li>Check the performance of this model against the corresponding training data:<p class="snippet">model.score(X=images_0_1, y=labels_0_1)</p><p>We'll get the following output:</p><div id="_idContainer251" class="IMG---Figure"><img src="image/C12622_04_19.jpg" alt="Figure 4.19: Model score"/></div><h6>Figure 4.19: Model score</h6><p>In this example, the model was able to predict the training labels with 100% accuracy.</p></li>
				<li>Display the first two predicted labels for the training data using the model:<p class="snippet">model.predict(images_0_1) [:2]</p><p>The output will be:</p><div id="_idContainer252" class="IMG---Figure"><img src="image/C12622_04_20.jpg" alt="Figure 4.20: The first two labels the model predicted"/></div><h6>Figure 4.20: The first two labels the model predicted</h6></li>
				<li>How is the logistic regression model making the classification decisions? Look at some of the probabilities produced by the model for the training set:<p class="snippet">model.predict_proba(images_0_1)[:2]</p><p>The output will be as follows:</p><div id="_idContainer253" class="IMG---Figure"><img src="image/C12622_04_21.jpg" alt="Figure 4.21: Array of probabilities"/></div><h6>Figure 4.21: Array of probabilities</h6><p>We can see that, for each prediction made, there are two probability values. The first corresponding to the probability of the class being zero, the second the probability of the class being one, both of which add up to one. We can see that, in the first example, the prediction probability is 0.9999999 for class zero and thus the prediction is class zero. Similarly, the inverse is true for the second example.</p></li>
				<li>Compute the performance of the model against the test set to check its performance against data that it has not seen:<p class="snippet">model.score(X=images_0_1_test, y=labels_0_1_test)</p><p>The output will be:</p></li>
			</ol>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<img src="image/C12622_04_22.jpg" alt="Figure 4.22: Model score"/>
				</div>
			</div>
			<h6>Figure 4.22: Model score</h6>
			<h4>Note</h4>
			<p class="callout">Refer to <em class="italics">Chapter 6</em>, <em class="italics">Model Evaluation</em>, for better methods of objectively measuring the model performance.</p>
			<p>We can see here that logistic regression is a powerful classifier that is able to distinguish between hand-written samples of zero and one.</p>
			<h3 id="_idParaDest-116"><a id="_idTextAnchor125"/>Exercise 38: Logistic Regression – Multiclass Classifier</h3>
			<p>In the previous exercise, we examined using logistic regression to classify between one of two groups. Logistic regression, however, can also be used to classify a set of input information to <em class="italics">k</em> different groups and it is this multiclass classifier we will be investigating in this exercise. The process for loading the MNIST training and test data is identical to the previous exercise:</p>
			<ol>
				<li value="1">Load the training/test images and the corresponding labels:<p class="snippet">with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size, rows, cols = struct.unpack("&gt;IIII", f.read(16))</p><p class="snippet">    img = np.array(array("B", f.read())).reshape((size, rows, cols))</p><p class="snippet">with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size = struct.unpack("&gt;II", f.read(8))</p><p class="snippet">    labels = np.array(array("B", f.read()))</p><p class="snippet">with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size, rows, cols = struct.unpack("&gt;IIII", f.read(16))</p><p class="snippet">    img_test = np.array(array("B", f.read())).reshape((size, rows, cols))</p><p class="snippet">with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:</p><p class="snippet">    magic, size = struct.unpack("&gt;II", f.read(8))</p><p class="snippet">    labels_test = np.array(array("B", f.read()))</p></li>
				<li>Given that the training data is so large, we will select a subset of the overall data to reduce the training time as well as the system resources required for the training process:<p class="snippet">np.random.seed(0) # Give consistent random numbers</p><p class="snippet">selection = np.random.choice(len(img), 5000)</p><p class="snippet">selected_images = img[selection]</p><p class="snippet">selected_labels = labels[selection]</p><p>Note that, in this example, we are using data from all 10 classes, not just classes zero and one, so we are making this example a multiclass classification problem.</p></li>
				<li>Again, reshape the input data in vector form for later use:<p class="snippet">selected_images = selected_images.reshape((-1, rows * cols))</p><p class="snippet">selected_images.shape</p><p>The output will be as follows:</p><div id="_idContainer255" class="IMG---Figure"><img src="image/C12622_04_23.jpg" alt="Figure 4.23: Reshaping the data"/></div><h6>Figure 4.23: Reshaping the data</h6></li>
				<li>The next cell is int<a id="_idTextAnchor126"/>entionally commented out. Leave this code commented out for the moment:<p class="snippet"># selected_images = selected_images / 255.0</p><p class="snippet"># img_test = img_test / 255.0</p></li>
				<li>Construct the logistic model. There are a few extra arguments, as follows: the <strong class="inline">lbfgs</strong> value for <strong class="inline">solver</strong> is geared up for multiclass problems, with additional <strong class="inline">max_iter</strong> iterations required for converging on a solution. The <strong class="inline">multi_class</strong> argument is set to <strong class="inline">multinomial</strong> to calculate the loss over the entire probability distribution:<p class="snippet">model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=500, tol=0.1)</p><p class="snippet">model.fit(X=selected_images, y=selected_labels)</p><p>The output will be as follows:</p><div id="_idContainer256" class="IMG---Figure"><img src="image/C12622_04_24.jpg" alt="Figure 4.24: Logistic regression model"/></div><h6>Figure 4.24: Logistic regression model</h6><h4>Note</h4><p class="callout">Refer to the documentation at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a> for more information on the arguments.</p></li>
				<li>Determine the accuracy score against the training set:<p class="snippet">model.score(X=selected_images, y=selected_labels)</p><p>The output will be:</p><div id="_idContainer257" class="IMG---Figure"><img src="image/C12622_04_25.jpg" alt="Figure 4.25: Model score"/></div><h6>Figure 4.25: Model score</h6></li>
				<li>Determine the first two predictions for the training set and plot the images with the corresponding predictions:<p class="snippet">model.predict(selected_images)[:2]</p><div id="_idContainer258" class="IMG---Figure"><img src="image/C12622_04_26.jpg" alt="Figure 4.26: Model score predicted values"/></div><h6>Figure 4.26: Model score predicted values</h6></li>
				<li>Show the images for the first two samples of the training set to see whether we are correct:<p class="snippet">plt.subplot(1, 2, 1)</p><p class="snippet">plt.imshow(selected_images[0].reshape((28, 28)), cmap='gray');</p><p class="snippet">plt.axis('off');</p><p class="snippet">plt.subplot(1, 2, 2)</p><p class="snippet">plt.imshow(selected_images[1].reshape((28, 28)), cmap='gray');</p><p class="snippet">plt.axis('off');</p><p>The output will be as follows:</p><div id="_idContainer259" class="IMG---Figure"><img src="image/C12622_04_27.jpg" alt="Figure 4.27: Images plotted using prediction"/></div><h6>Figure 4.27: Images plotted using prediction</h6></li>
				<li>Again, print out the probability scores provided by the model for the first sample of the training set. Confirm that there are 10 different values for each of the 10 classes in the set:<p class="snippet">model.predict_proba(selected_images)[0]</p><p>The output will be as follows:</p><div id="_idContainer260" class="IMG---Figure"><img src="image/C12622_04_28.jpg" alt="Figure 4.28: Array of predicted values"/></div><h6>Figure 4.28: Array of predicted values</h6><p>Notice that, in the probability array of the first sample, the fifth (index four) sample is the highest probability, thus indicating a prediction of four.</p></li>
				<li>Compute the accuracy of the model against the test set. This will provide a reasonable estimate of the model's <em class="italics">in the wild</em> performance, as it has never seen the data in the test set. It is expected that the accuracy rate of the test set will be slightly lower than the training set, given that the model has not been exposed to this data:<p class="snippet">model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)</p><p>The output will be as follows:</p><div id="_idContainer261" class="IMG---Figure"><img src="image/C12622_04_29.jpg" alt="Figure 4.29: Model score&#13;&#10;"/></div><h6>Figure 4.29: Model score</h6><p>When checked against the test set, the model produced accuracy of 87.8%. When applying a test set, a performance drop is expected, as this is the very first time the model has seen these samples; while, during training, the training set was repeatedly shown to the model.</p></li>
				<li>Find the cell with the commented-out code, as shown in <em class="italics">step four</em>. Uncomment the code in this cell:<p class="snippet">selected_images = selected_images / 255.0</p><p class="snippet">img_test = img_test / 255.0</p><p>This cell simply scales all the image values to between zero and one. Grayscale images are comprised of pixels with values between and including 0 – 255, where 0 is black and 255 is white.</p></li>
				<li>Click <strong class="bold">Restart &amp; Run-All</strong> to rerun the entire notebook.</li>
				<li>Find the training set error:<p class="snippet">model.score(X=selected_images, y=selected_labels)</p><p>We'll get the following score:</p><div id="_idContainer262" class="IMG---Figure"><img src="image/C12622_04_30.jpg" alt="Figure 4.30: Training set model score"/></div><h6>Figure 4.30: Training set model score</h6></li>
				<li>Find the test set error:<p class="snippet">model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)</p><p>We'll get the following score:</p></li>
			</ol>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/C12622_04_31.jpg" alt="Figure 4.31: Test set model score"/>
				</div>
			</div>
			<h6>Figure 4.31: Test set model score</h6>
			<p>What effect did normalizing the images have on the overall performance of the system? The training error is worse! We went from 100% accuracy in the training set to 98.6%. Yes, there was a reduction in the performance of the training set, but an increase in the test set from 87.8% accuracy to 90.02%. The test set performance is of more interest, as the model has not seen this data before, and so it is a better representation of the performance than we could expect once the model is in the field. So, why do we get a better result? Again, review <em class="italics">Figure 4.13</em>, and notice the shape of the curve as it approaches  and . The curve saturates or flattens at almost zero and almost one. So, if we use an image (or <em class="italics">x</em> values) of between 0 and 255, the class probability defined by the logistic function is well within this flat region of the curve. Predictions within this region are highly unlikely to change very much, as they will need to have very large changes in <em class="italics">x</em> values for any meaningful change in <em class="italics">y</em>. Scaling the images to be between zero and one initially puts the predictions closer to <em class="italics">p(x) = 0.5</em>, and so, changes in <em class="italics">x</em> can have a bigger impact on the value for <em class="italics">y</em>. This allows for more sensitive predictions and results in getting a couple of predictions in the training set wrong, but more in the test set right. It is recommended, for your logistic regression models, that you scale the input values to be between either zero and one or negative-one and one prior to training and testing.</p>
			<p>The following function will scale values of a NumPy array between negative-one and one with a mean of approximately zero:</p>
			<p class="snippet">def scale_input(x):</p>
			<p class="snippet">    mean_x = x.mean()</p>
			<p class="snippet">    x = x – mean_x</p>
			<p class="snippet">    max_x = x / no.max(abs(x))</p>
			<p class="snippet">    return x</p>
			<h3 id="_idParaDest-117"><a id="_idTextAnchor127"/>Activity 11: Linear Regression Classifier – Two-Class Classifier</h3>
			<p>In this activity, we will build a two-class linear regression-based classifier using the MNIST dataset to classify between two digits: zero and one.</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Import the required dependencies:<p class="snippet">import struct</p><p class="snippet">import numpy as np</p><p class="snippet">import gzip</p><p class="snippet">import urllib.request</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from array import array</p><p class="snippet">from sklearn.linear_model import LinearRegression</p></li>
				<li>Load the MNIST data into memory.</li>
				<li>Visualize a sample of the data.</li>
				<li>Construct a linear classifier model to classify the digits zero and one. The model we are going to create is to determine whether the samples are either the digits zero or one. To do this, we first need to select only those samples.</li>
				<li>Visualize the selected information with images of one sample of zero and one sample of one.</li>
				<li>In order to provide the image information to the model, we must first flatten the data out so that each image is 1 x 784 pixels in shape.</li>
				<li>Let's construct the model; use the <strong class="inline">LinearRegression</strong> API and call the <strong class="inline">fit</strong> function.</li>
				<li>Determine the R2 score against the training set.</li>
				<li>Determine the label predictions for each of the training samples, using a threshold of 0.5. Values greater than 0.5 classify as one; values less than or equal to 0.5 classify as zero.</li>
				<li>Compute the classification accuracy of the predicted training values versus the ground truth.</li>
				<li>Compare the performance against the test set.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 352.</p></li>
			</ol>
			<h3 id="_idParaDest-118"><a id="_idTextAnchor128"/>Activity 12: Iris Classification Using Logistic Regression</h3>
			<p>In this activity, we will be using the well-known Iris Species dataset (available at <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">https://en.wikipedia.org/wiki/Iris_flower_data_set</a> or on GitHub at <a href="https://github.com/TrainingByPackt/Supervised-Learning-with-Python">https://github.com/TrainingByPackt/Supervised-Learning-with-Python</a>) created in 1936 by botanist, Ronald Fisher. The dataset contains sepal and petal length and width measurements for three different iris flower species: iris setosa, iris versicolor, and iris virginica. In this activity, we will use the measurements provided in the dataset to classify the different flower species.</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Import the required packages. For this activity, we will require the pandas package for loading the data, the Matplotlib package for plotting, and scikit-learn for creating the logistic regression model. Import all the required packages and relevant modules for these tasks:<p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.linear_model import LogisticRegression</p></li>
				<li>Load the iris dataset using pandas and examine the first five rows.</li>
				<li>The next step is feature engineering. We need to select the most appropriate features that will provide the most powerful classification model. Plot a number of different features versus the allocated species classifications, for example, sepal length versus petal length and species. Visually inspect the plots and look for any patterns that could indicate separation between each of the species.</li>
				<li>Select the features by writing the column names in the following list:<p class="snippet">selected_features = [</p><p class="snippet">    '', # List features here</p><p class="snippet">]</p></li>
				<li>Before we can construct the model, we must first convert the <strong class="inline">species</strong> values into labels that can be used within the model. Replace the <strong class="inline">Iris-setosa</strong> species string with the value <strong class="inline">0</strong>, the <strong class="inline">Iris-versicolor</strong> species string with the value <strong class="inline">1</strong>, and the <strong class="inline">Iris-virginica</strong> species string with the value <strong class="inline">2</strong>.</li>
				<li>Create the model using the <strong class="inline">selected_features</strong> and the assigned <strong class="inline">species</strong> labels.</li>
				<li>Compute the accuracy of the model against the training set.</li>
				<li>Construct another model using your second choice <strong class="inline">selected_features</strong> and compare the performance.</li>
				<li>Construct another model using all available information and compare the performance.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 357.</p></li>
			</ol>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor129"/>Classification Using K-Nearest Neighbors</h2>
			<p>Now that we are comfortable with creating multiclass classifiers using logistic regression and are getting reasonable performance with these models, we will turn our attention to another type of classifier: the <strong class="keyword">K-nearest neighbors</strong> (<strong class="keyword">K-NN</strong>) clustering method of classification. This is a handy method, as it can be used in both supervised classification problems as well as in unsupervised problems.</p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/C12622_04_32.jpg" alt="Figure 4.32: Visual representation of K-NN"/>
				</div>
			</div>
			<h6>Figure 4.32: Visual represent<a id="_idTextAnchor130"/>ation of K-NN</h6>
			<p>The solid circle approximately in the center is the test point requiring classification, while the inner circle shows the classification process where <em class="italics">K=3</em> and the outer circle where <em class="italics">K=5</em>.</p>
			<p>K-NN is one of the simplest "learning" algorithms available for data classification. The use of learning in quotation marks is explicit, as K-NN doesn't really learn from the data and encode these learnings in parameters or weights like other methods, such as logistic regression. K-NN uses instance-based or lazy learning in that it simply stores or memorizes all the training samples and the corresponding classes. It derives its name, K-nearest neighbors, from the fact that, when a test sample is provided to the algorithm for class prediction, it uses a majority vote of the <em class="italics">K</em>-nearest points to determine the corresponding class. If we look at <em class="italics">Figure 4.35</em>, the solid circle is the test point to be classified. If we use <em class="italics">K=3</em>, the nearest three points lie within the inner dotted circle, and, in this case, the classification would be a hollow circle. If, however we were to take <em class="italics">K=5</em>, the nearest five points lie within the outer dotted circle and the classification would be a cross (three crosses to two hollow circles). </p>
			<p>This figure highlights a few characteristics of K-NN classification that should be considered:</p>
			<ul>
				<li>The selection of <em class="italics">K</em> is quite important. In this simple example, switching <em class="italics">K</em> from three to five flipped the class prediction due to the proximity of both classes. As the final classification is taken by a majority vote, it is often useful to use odd numbers of <em class="italics">K</em> to ensure that there is a winner in the voting process. If an even value of <em class="italics">K</em> is selected, and a tie in the vote occurs, then there are a number of different methods available for breaking the tie, including:<p>Reducing <em class="italics">K</em> by one until the tie is broken</p><p>Selecting the class on the basis of the smallest Euclidean distance to the nearest point</p><p>Applying a weighting function to bias the test point toward those neighbors which are closer</p></li>
				<li>K-NN models have the ability to form extremely complex non-linear boundaries, which can be advantageous in classifying images or datasets with interesting boundaries. Considering that, in <em class="italics">Figure 4.35</em>, the test point changes from a hollow circle classification to a cross with an increase in <em class="italics">K</em>, we can see here that a complex boundary could be formed.</li>
				<li>K-NN models can be highly sensitive to local features in the data, given that the classification process is only really dependent on the nearby points.</li>
				<li>As K-NN models memorize all the training information to make predictions, they can struggle with generalizing to new, unseen data.</li>
			</ul>
			<p>There is another variant of K-NN, which, rather than specifying the number of nearest neighbors, specifies the size of the radius around the test point at which to look. This method, known as the <strong class="keyword">radius neighbors classification</strong>, will not be considered in this chapter, but in understanding K-NN, you will also develop an understanding of radius neighbors classification and how to use the model through scikit-learn.</p>
			<h4>Note</h4>
			<p class="callout">Our explanation of K-NN classification and the next exercise examines modeling data with two features or two dimensions as it enables simple visualization and a greater understanding of the K-NN modeling process. K-NN classification, like linear and logistic regression, is not limited to use in only two dimensions; it can be applied in <em class="italics">N</em> dimensional datasets as well. This will be examined in further detail in <em class="italics">Activity 13: K-NN Multiclass Classifier</em>, wherein we'll classify MNIST using K-NN. Remember, just because there are too many dimensions to plot, it doesn't mean it cannot be classified with <em class="italics">N</em> dimensions.</p>
			<h3 id="_idParaDest-120"><a id="_idTextAnchor131"/>Exercise 39: K-NN Classification</h3>
			<p>To allow visualization of the K-NN process, we will turn our attention in this exercise to a different dataset, the well-known Iris dataset. This dataset is provided as part of the accompanying code files for this book:</p>
			<ol>
				<li value="1">For this exercise, we need to import pandas, Matplotlib, and the <strong class="inline">KNeighborsClassifier</strong> of scikit-learn. We will use the shorthand notation <strong class="inline">KNN</strong> for quick access:<p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.neighbors import KNeighborsClassifier as KNN</p></li>
				<li>Load the Iris dataset and examine the first five rows:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df.head()</p><p>The output will be:</p><div id="_idContainer265" class="IMG---Figure"><img src="image/C12622_04_33.jpg" alt="Figure 4.33: First five rows"/></div><h6>Figure 4.33: First five rows</h6></li>
				<li>At this stage, we need to choose the most appropriate features from the dataset for use with the classifier. We could simply select all four (sepal and petal, length and width), however, as this exercise is designed to allow visualization of the K-NN process, we will only select sepal length and petal width. Construct a scatterplot for sepal length versus petal width for each of the classes in the dataset with the corresponding species:<p class="snippet">markers = {</p><p class="snippet">    'Iris-setosa': {'marker': 'x', 'facecolor': 'k', 'edgecolor': 'k'},</p><p class="snippet">    'Iris-versicolor': {'marker': '*', 'facecolor': 'none', 'edgecolor': 'k'},</p><p class="snippet">    'Iris-virginica': {'marker': 'o', 'facecolor': 'none', 'edgecolor': 'k'},</p><p class="snippet">}</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for name, group in df.groupby('Species'):</p><p class="snippet">    plt.scatter(group['Sepal Length'], group['Petal Width'], </p><p class="snippet">                label=name,</p><p class="snippet">                marker=markers[name]['marker'],</p><p class="snippet">                facecolors=markers[name]['facecolor'],</p><p class="snippet">                edgecolor=markers[name]['edgecolor'])</p><p class="snippet">plt.title('Species Classification Sepal Length vs Petal Width');</p><p class="snippet">plt.xlabel('Sepal Length (mm)');</p><p class="snippet">plt.ylabel('Petal Width (mm)');</p><p class="snippet">plt.legend();</p><p>The output will be as follows:</p><div id="_idContainer266" class="IMG---Figure"><img src="image/C12622_04_34.jpg" alt="Figure 4.34: Scatterplot of iris data"/></div><h6>Figure 4.34: Scatterplot of iris data</h6></li>
				<li>Looking at this graph, we can see that the species are reasonably well separated by the petal width, with the greatest similarity between the Iris versicolor and the Iris virginica species. There are a couple of Iris virginica species points that lie within the Iris versicolor cluster. As a test point for later use, select one of these points at the boundary—sample 134:<p class="snippet">df_test = df.iloc[134]</p><p class="snippet">df = df.drop([134]) # Remove the sample</p><p class="snippet">df_test</p><p>The output will be:</p><div id="_idContainer267" class="IMG---Figure"><img src="image/C12622_04_35.jpg" alt="Figure 4.35: Boundary sample"/></div><h6>Figure 4.35: Boundary sample</h6></li>
				<li>Plot the data again, highlighting the location of the test sample/point:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for name, group in df.groupby('Species'):</p><p class="snippet">    plt.scatter(group['Sepal Length'], group['Petal Width'], </p><p class="snippet">                label=name,</p><p class="snippet">                marker=markers[name]['marker'],</p><p class="snippet">                facecolors=markers[name]['facecolor'],</p><p class="snippet">                edgecolor=markers[name]['edgecolor'])</p><p class="snippet">    </p><p class="snippet">plt.scatter(df_test['Sepal Length'], df_test['Petal Width'], label='Test Sample', c='k', marker='D')</p><p class="snippet">plt.title('Species Classification Sepal Length vs Petal Width');</p><p class="snippet">plt.xlabel('Sepal Length (mm)');</p><p class="snippet">plt.ylabel('Petal Width (mm)');</p><p class="snippet">plt.legend();</p><p>The output will be as follows:</p><div id="_idContainer268" class="IMG---Figure"><img src="image/C12622_04_36.jpg" alt="Figure 4.36: Scatterplot with the test sample"/></div><h6>Figure 4.36: Scatterplot with the test sample</h6></li>
				<li>Construct a K-NN classifier model with <em class="italics">K = 3</em> and fit it to the training data:<p class="snippet">model = KNN(n_neighbors=3)</p><p class="snippet">model.fit(X=df[['Petal Width', 'Sepal Length']], y=df.Species)</p><p>The output will be:</p><div id="_idContainer269" class="IMG---Figure"><img src="image/C12622_04_37.jpg" alt="Figure 4.37: K Neighbor classifier"/></div><h6>Figure 4.37: K Neighbor classifier</h6></li>
				<li>Check the performance of the model against the training set:<p class="snippet">model.score(X=df[['Petal Width', 'Sepal Length']], y=df.Species)</p><p>The output will show the performance score:</p><div id="_idContainer270" class="IMG---Figure"><img src="image/C12622_04_38.jpg" alt=""/></div><h6>Figure 4.38: Model score</h6><p>As the accuracy is over 97% on the test set, the next step will be to check the test sample.</p></li>
				<li>Predict the species of the test sample:<p class="snippet">model.predict(df_test[['Petal Width', 'Sepal Length']].values.reshape((-1, 2)))[0]</p><p>The output will be as follows:</p><div id="_idContainer271" class="IMG---Figure"><img src="image/C12622_04_39.jpg" alt="Figure 4.39: Predicted test sample"/></div><h6>Figure 4.39: Predicted test sample</h6></li>
				<li>Verify it with the actual species of the sample:<p class="snippet">df.iloc[134].Species</p><p>The output will be:</p></li>
			</ol>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/C12622_04_40.jpg" alt="Figure 4.40: Species of the sample"/>
				</div>
			</div>
			<h6>Figure 4.40: Species of the sample</h6>
			<p>This prediction is clearly incorrect but is not surprising given the location of the test point on the boundary. What would be helpful would be to know where the boundary for the model is drawn. We will draw this boundary in the next exercise.</p>
			<h3 id="_idParaDest-121"><a id="_idTextAnchor132"/>Exercise 40: Visualizing K-NN Boundaries</h3>
			<p>To visualize the decision boundaries produced by the K-NN classifier, we need to sweep over the prediction space, that is, the minimum and maximum values for petal width and sepal length, and determine the classifications made by the model at those points. Once we have this sweep, we can then plot the classification decisions made by the model.</p>
			<ol>
				<li value="1">Import all the relevant packages. We will also need NumPy for this exercise:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from matplotlib.colors import ListedColormap</p><p class="snippet">from sklearn.neighbors import KNeighborsClassifier as KNN</p></li>
				<li>Load the Iris dataset into a pandas DataFrame:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df.head()</p><p>The output will be:</p><div id="_idContainer273" class="IMG---Figure"><img src="image/C12622_04_41.jpg" alt="Figure 4.41: First five rows"/></div><h6>Figure 4.41: First five rows</h6></li>
				<li>While we could use the species strings to create the model in the previous exercise, in plotting the decision boundaries, it would be more useful to map the species to separate integer values. To do this, create a list of the labels for later reference and iterate through this list, replacing the existing label with the corresponding index in the list:<p class="snippet">labelled_species = [</p><p class="snippet">    'Iris-setosa',</p><p class="snippet">    'Iris-versicolor',</p><p class="snippet">    'Iris-virginica',</p><p class="snippet">]</p><p class="snippet">for idx, label in enumerate(labelled_species):</p><p class="snippet">    df.Species = df.Species.replace(label, idx)</p><p class="snippet">df.head()</p><p>The output will be as follows:</p><div id="_idContainer274" class="IMG---Figure"><img src="image/C12622_04_42.jpg" alt="Figure 4.42: First five rows"/></div><h6>Figure 4.42: First five rows</h6><p>Notice the use of the <strong class="inline">enumerate</strong> function in the <strong class="inline">for</strong> loop definition. When iterating through the <strong class="inline">for</strong> loop, the <strong class="inline">enumerate</strong> function provides the index of the value in the list as well as the value itself through each iteration. We assign the index of the value to the <strong class="inline">idx</strong> variable and the value to <strong class="inline">label</strong>. Using <strong class="inline">enumerate</strong> in this way provides an easy way to replace the species strings with a unique integer label.</p></li>
				<li>Construct a K-NN classification model, again using three nearest neighbors and fit to the sepal length and petal width with the newly labeled species data:<p class="snippet">model = KNN(n_neighbors=3)</p><p class="snippet">model.fit(X=df[['Sepal Length', 'Petal Width']], y=df.Species)</p><p>The output will be as follows:</p><div id="_idContainer275" class="IMG---Figure"><img src="image/C12622_04_43.jpg" alt="Figure 4.43: K Neighbors classifier"/></div><h6>Figure 4.43: K Neighbors classifier</h6></li>
				<li>To visualize our decision boundaries, we need to create a mesh or range of predictions across the information space, that is, all values of sepal length and petal width. Starting with 1mm less than the minimum for both the petal width and sepal length, and finishing at 1mm more than the maximum for petal width and sepal length, use the <strong class="inline">arange</strong> function of NumPy to create a range of values between these limits in increments of 0.1 (spacing):<p class="snippet">spacing = 0.1 # 0.1mm</p><p class="snippet">petal_range = np.arange(df['Petal Width'].min() - 1, df['Petal Width'].max() + 1, spacing)</p><p class="snippet">sepal_range = np.arange(df['Sepal Length'].min() - 1, df['Sepal Length'].max() + 1, spacing)</p></li>
				<li>Use the NumPy <strong class="inline">meshgrid</strong> function to combine the two ranges into a grid:<p class="snippet">xx, yy = np.meshgrid(sepal_range, petal_range) # Create the mesh</p><p>Check out <strong class="inline">xx</strong>:</p><p class="snippet">xx</p><p>The output will be:</p><div id="_idContainer276" class="IMG---Figure"><img src="image/C12622_04_44.jpg" alt="Figure 4.44: Array of meshgrid xx values"/></div><h6>Figure 4.44: Array of meshgrid xx values</h6><p>Check out <strong class="inline">yy</strong>:</p><p class="snippet">yy</p><p>The output will be:</p><div id="_idContainer277" class="IMG---Figure"><img src="image/C12622_04_45.jpg" alt="Figure 4.45: Array of meshgrid yy values"/></div><h6>Figure 4.45: Array of meshgrid yy values</h6></li>
				<li>Concatenate the mesh into a single NumPy array using <strong class="inline">np.c_</strong>:<p class="snippet">pred_x = np.c_[xx.ravel(), yy.ravel()] # Concatenate the results</p><p class="snippet">pred_x</p><p>We'll get the following output:</p><div id="_idContainer278" class="IMG---Figure"><img src="image/C12622_04_46.jpg" alt="Figure 4.46: Array of predicted values"/></div><h6>Figure 4.46: Array of predicted values</h6><p>While this function call looks a little mysterious, it simply concatenates the two separate arrays together (refer to <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html">https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html</a>) and is shorthand for concatenate.</p></li>
				<li>Produce the class predictions for the mesh:<p class="snippet">pred_y = model.predict(pred_x).reshape(xx.shape)</p><p class="snippet">pred_y</p><p>We'll get the following output:</p><div id="_idContainer279" class="IMG---Figure"><img src="image/C12622_04_47.jpg" alt="Figure 4.47: Array of predicted y values"/></div><h6>Figure 4.47: Array of predicted y values</h6></li>
				<li>To consistently visualize the boundaries, we will need two sets of consistent colors; a lighter set of colors for the decision boundaries and a darker set of colors for the points of the training set themselves. Create two <strong class="inline">ListedColormaps</strong>:<p class="snippet"># Create color maps</p><p class="snippet">cmap_light = ListedColormap(['#F6A56F', '#6FF6A5', '#A56FF6'])</p><p class="snippet">cmap_bold = ListedColormap(['#E6640E', '#0EE664', '#640EE6'])</p></li>
				<li>To highlight the decision boundaries, first plot the training data according to the iris species, using the <strong class="inline">cmap_bold</strong> color scheme and different markers for each of the different species:<p class="snippet">markers = {</p><p class="snippet">    'Iris-setosa': {'marker': 'x', 'facecolor': 'k', 'edgecolor': 'k'},</p><p class="snippet">    'Iris-versicolor': {'marker': '*', 'facecolor': 'none', 'edgecolor': 'k'},</p><p class="snippet">    'Iris-virginica': {'marker': 'o', 'facecolor': 'none', 'edgecolor': 'k'},</p><p class="snippet">}</p><p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">for name, group in df.groupby('Species'):</p><p class="snippet">    species = labelled_species[name]</p><p class="snippet">    plt.scatter(group['Sepal Length'], group['Petal Width'],</p><p class="snippet">                c=cmap_bold.colors[name],</p><p class="snippet">                label=labelled_species[name],</p><p class="snippet">                marker=markers[species]['marker']</p><p class="snippet">               )</p><p class="snippet">plt.title('Species Classification Sepal Length vs Petal Width');</p><p class="snippet">plt.xlabel('Sepal Length (mm)');</p><p class="snippet">plt.ylabel('Petal Width (mm)');</p><p class="snippet">plt.legend();</p><p>The output will be as follows:</p><div id="_idContainer280" class="IMG---Figure"><img src="image/C12622_04_48.jpg" alt=""/></div><h6>Figure 4.48: Scatterplot with highlighted decision boundaries</h6></li>
				<li>Using the prediction mesh made previously, plot the decision boundaries in addition to the training data:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.pcolormesh(xx, yy, pred_y, cmap=cmap_light);</p><p class="snippet">plt.scatter(df['Sepal Length'], df['Petal Width'], c=df.Species, cmap=cmap_bold, edgecolor='k', s=20);</p><p class="snippet">plt.title('Species Decision Boundaries Sepal Length vs Petal Width');</p><p class="snippet">plt.xlabel('Sepal Length (mm)');</p><p class="snippet">plt.ylabel('Petal Width (mm)');</p><p>The output will be as follows:</p><div id="_idContainer281" class="IMG---Figure"><img src="image/C12622_04_49.jpg" alt="Figure 4.49: The decision boundaries"/></div></li>
			</ol>
			<h6>Figure 4.49: The decision boundaries</h6>
			<h4>Note</h4>
			<p class="callout"><em class="italics">Figure 4.49</em> has been modified for grayscale print and has additional labels, indicating the prediction boundaries of the classes.</p>
			<h3 id="_idParaDest-122"><a id="_idTextAnchor133"/>Activity 13: K-NN Multiclass Classifier</h3>
			<p>In this activity, we will use the K-NN model to classify the MNIST dataset into 10 different digit-based classes.</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Import the following packages:<p class="snippet">import struct</p><p class="snippet">import numpy as np</p><p class="snippet">import gzip</p><p class="snippet">import urllib.request</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from array import array</p><p class="snippet">from sklearn.neighbors import KNeighborsClassifier as KNN</p></li>
				<li>Load the MNIST data into memory. First the training images, then the training labels, then the test images, and finally, the test labels.</li>
				<li>Visualize a sample of the data.</li>
				<li>Construct a K-NN classifier, with three nearest neighbors to classify the MNIST dataset. Again, to save processing power, randomly sample 5,000 images for use in training.</li>
				<li>In order to provide the image information to the model, we must first flatten the data out so that each image is 1 x 784 pixels in shape.</li>
				<li>Build the three-neighbor KNN model and fit the data to the model. Note that, in this activity, we are providing 784 features or dimensions to the model, not simply 2.</li>
				<li>Determine the score against the training set.</li>
				<li>Display the first two predictions for the model against the training data.</li>
				<li>Compare the performance against the test set.<h4>Note</h4><p class="callout">The solution for this acti<a id="_idTextAnchor134"/>vity can be found on page 360.</p></li>
			</ol>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor135"/>Classification Using Decision Trees</h2>
			<p>The final classification method that we will be examining in this chapter is <strong class="keyword">decision trees</strong>, which have found particular use in applications such as natural language processing. There are a number of different machine learning algorithms that fall within the overall umbrella of decision trees, such as ID3, CART, and the powerful random forest classifiers (covered in <em class="italics">Chapter 5</em>, <em class="italics">Ensemble Modeling</em>). In this chapter, we will investigate the use of the ID3 method in classifying categorical data, and we will use the scikit-learn CART implementation as another means of classifying the Iris dataset. So, what exactly are decision trees?</p>
			<p>As the name suggests, decision trees are a learning algorithm that apply a sequential series of decisions based on input information to make the final classification. Recalling your childhood biology class, you may have used a process similar to decision trees in the classification of different types of animals via dichotomous keys. Just like the example dichotomous key shown in <em class="italics">Figure 4.50</em>, decision trees aim to classify information following the result of a number of decision or question steps:</p>
			<div>
				<div id="_idContainer282" class="IMG---Figure">
					<img src="image/C12622_04_50.jpg" alt="Figure 4.50: Animal classification using dichotomous key"/>
				</div>
			</div>
			<h6>Figure 4.50: Animal classification using dichot<a id="_idTextAnchor136"/>omous key</h6>
			<p>Depending upon the decision tree algorithm being used, the implementation of the decision steps may vary slightly, but we will be considering the implementation of the ID3 algorithm specifically. The <strong class="keyword">Iterative Dichotomiser 3</strong> (<strong class="keyword">ID3</strong>) algorithm aims to classify the data on the basis of each <a id="_idTextAnchor137"/>decision providing the largest information gain. To further understand this design, we also need to understand two additional concepts: entropy and information gain.</p>
			<h4>Note</h4>
			<p class="callout">The ID3 algorithm was first proposed by the Australian researcher Ross Quinlan in 1985 (<a href="https://doi.org/10.1007/BF00116251">https://doi.org/10.1007/BF00116251</a>).</p>
			<ul>
				<li><strong class="keyword">Entropy</strong>: In the context of information theory, entropy is the average rate at which information is provided by a random source of data. Mathematically speaking, this entropy is defined as:</li>
			</ul>
			<div>
				<div id="_idContainer283" class="IMG---Figure">
					<img src="image/C12622_04_51.jpg" alt="Figure 4.51: Entropy equation"/>
				</div>
			</div>
			<h6>Figure 4.51: Entropy equation</h6>
			<p>In this scenario, when the random source of data produces a low probability value, the event carries more information, as it is unexpected compared to when a high-probability event occurs.</p>
			<ul>
				<li><strong class="keyword">Information gain</strong>: Somewhat related to entropy is the amount of information gained about a random variable by observing another random variable. Given a dataset, <em class="italics">S</em>, and an attribute to observe, <em class="italics">a</em>, the information gain is defined mathematically as:</li>
			</ul>
			<div>
				<div id="_idContainer284" class="IMG---Figure">
					<img src="image/C12622_04_52.jpg" alt="Figure 4.52: Information gain equation"/>
				</div>
			</div>
			<h6>Figure 4.52: Information gain equation</h6>
			<p>The information gain of dataset <em class="italics">S</em>, for attribute <em class="italics">a</em>, is equal to the entropy of <em class="italics">S</em> minus the entropy of <em class="italics">S</em> conditional on attribute <em class="italics">a</em>, or the entropy of dataset <em class="italics">S</em> minus the proportion of elements in <em class="italics">t</em> to the elements in <em class="italics">S</em>, times the entropy of <em class="italics">t</em>, where <em class="italics">t</em> is one of the categories in attribute <em class="italics">a</em>.</p>
			<p>If at first you find the mathematics here a little daunting, don't worry, for it is far simpler than it seems. To clarify the ID3 process, we will walk through the process using the same dataset as was provided by Quinlan in the original paper.</p>
			<h3 id="_idParaDest-124"><a id="_idTextAnchor138"/>Exercise 41: ID3 Classification</h3>
			<p>In the original paper, Quinlan provided a small dataset of 10 weather observation samples labeled with either <strong class="bold">P</strong> to indicate that the weather was suitable for, say, a Saturday morning game of cricket, or baseball for our North American friends. If the weather was not suitable for a game, label <strong class="bold">N</strong> was provided. The example dataset described in the paper will be created in the exercise.</p>
			<ol>
				<li value="1">In a Jupyter notebook, create a pandas DataFrame of the following training set:<p class="snippet">df = pd.DataFrame()</p><p class="snippet">df['Outlook'] = [</p><p class="snippet">    'sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain',</p><p class="snippet">    'overcast', 'sunny', 'sunny', 'rain', 'sunny',</p><p class="snippet">    'overcast', 'overcast', 'rain'</p><p class="snippet">]</p><p class="snippet">df['Temperature'] = [</p><p class="snippet">    'hot', 'hot', 'hot', 'mild', 'cool', 'cool', 'cool',</p><p class="snippet">    'mild', 'cool', 'mild', 'mild', 'mild', 'hot', 'mild',</p><p class="snippet">]</p><p class="snippet">df['Humidity'] = [</p><p class="snippet">    'high', 'high', 'high', 'high', 'normal', 'normal', 'normal', </p><p class="snippet">    'high', 'normal', 'normal', 'normal', 'high', 'normal', 'high'</p><p class="snippet">]</p><p class="snippet">df['Windy'] = [</p><p class="snippet">    'Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak',</p><p class="snippet">    'Strong', 'Strong', 'Weak', 'Strong'</p><p class="snippet">]</p><p class="snippet">df['Decision'] = [</p><p class="snippet">    'N', 'N', 'P', 'P', 'P', 'N', 'P', 'N', 'P', 'P',</p><p class="snippet">    'P', 'P', 'P', 'N'</p><p class="snippet">]</p><p class="snippet">df</p><p>The output will be as follows:</p><div id="_idContainer285" class="IMG---Figure"><img src="image/C12622_04_53.jpg" alt="Figure 4.53: Pandas DataFrame"/></div><h6>Figure 4.53: Pandas DataFrame</h6></li>
				<li>In the original paper, the ID3 algorithm starts by taking a small sample of the training set at random and fitting the tree to this window. This can be a useful method for large datasets, but given that ours is quite small, we will simply start with the entire training set. The first step is to calculate the entropy for the <strong class="bold">Decision</strong> column, where there are two possible values, or classes, <strong class="inline">P</strong> and <strong class="inline">N</strong>:<p class="snippet"># Probability of P</p><p class="snippet">p_p = len(df.loc[df.Decision == 'P']) / len(df)</p><p class="snippet"># Probability of N</p><p class="snippet">p_n = len(df.loc[df.Decision == 'N']) / len(df)</p><p class="snippet">entropy_decision = -p_n * np.log2(p_n) - p_p * np.log2(p_p)</p><p class="snippet">print(f'H(S) = {entropy_decision:0.4f}')</p><p>We'll get the following output:</p><div id="_idContainer286" class="IMG---Figure"><img src="image/C12622_04_54.jpg" alt="Figure 4.54: Entropy decision"/></div><h6>Figure 4.54: Entropy decision</h6></li>
				<li>We will need to repeat this calculation, so wrap it into a function:<p class="snippet">def f_entropy_decision(data):</p><p class="snippet">    p_p = len(data.loc[data.Decision == 'P']) / len(data)</p><p class="snippet">    p_n = len(data.loc[data.Decision == 'N']) / len(data)</p><p class="snippet">    return -p_n * np.log2(p_n) - p_p * np.log2(p_p)</p></li>
				<li>The next step is to calculate which attribute provides the highest information gain out of <strong class="bold">Outlook</strong>, <strong class="bold">Temperature</strong>, <strong class="bold">Humidity</strong>, and <strong class="bold">Windy</strong>. Starting with the <strong class="bold">Outlook</strong> parameter, determine the probability of each decision given sunny, overcast, and rainy conditions. We need to evaluate the following equation:<div id="_idContainer287" class="IMG---Figure"><img src="image/C12622_04_55.jpg" alt="Figure 4.55: Information gain"/></div><h6>Figure 4.55: Information gain</h6><p>Construct this equation in Python using the pandas <strong class="inline">groupby</strong> method:</p><p class="snippet">IG_decision_Outlook = entropy_decision # H(S)</p><p class="snippet"># Create a string to print out the overall equation</p><p class="snippet">overall_eqn = 'Gain(Decision, Outlook) = Entropy(Decision)' </p><p class="snippet"># Iterate through the values for outlook and compute the probabilities</p><p class="snippet"># and entropy values</p><p class="snippet">for name, Outlook in df.groupby('Outlook'):</p><p class="snippet">    num_p = len(Outlook.loc[Outlook.Decision == 'P'])</p><p class="snippet">    num_n = len(Outlook.loc[Outlook.Decision != 'P'])</p><p class="snippet">    num_Outlook = len(Outlook)</p><p class="snippet">    print(f'p(Decision=P|Outlook={name}) = {num_p}/{num_Outlook}')</p><p class="snippet">    print(f'p(Decision=N|Outlook={name}) = {num_n}/{num_Outlook}')    </p><p class="snippet">    print(f'p(Decision|Outlook={name}) = {num_Outlook}/{len(df)}')</p><p class="snippet">    print(f'Entropy(Decision|Outlook={name}) = '\</p><p class="snippet">         f'-{num_p}/{num_Outlook}.log2({num_p}/{num_Outlook}) - '\</p><p class="snippet">         f'{num_n}/{num_Outlook}.log2({num_n}/{num_Outlook})')</p><p class="snippet">    </p><p class="snippet">    entropy_decision_outlook = 0</p><p class="snippet">    </p><p class="snippet">    # Cannot compute log of 0 so add checks</p><p class="snippet">    if num_p != 0:</p><p class="snippet">        entropy_decision_outlook -= (num_p / num_Outlook) \</p><p class="snippet">            * np.log2(num_p / num_Outlook)</p><p class="snippet">     </p><p class="snippet">    # Cannot compute log of 0 so add checks</p><p class="snippet">    if num_n != 0:</p><p class="snippet">        entropy_decision_outlook -= (num_n / num_Outlook) \</p><p class="snippet">            * np.log2(num_n / num_Outlook)</p><p class="snippet">    </p><p class="snippet">    IG_decision_Outlook -= (num_Outlook / len(df)) * entropy_decision_outlook</p><p class="snippet">    print()</p><p class="snippet">    overall_eqn += f' - p(Decision|Outlook={name}).'</p><p class="snippet">    overall_eqn += f'Entropy(Decision|Outlook={name})'</p><p class="snippet">print(overall_eqn)</p><p class="snippet">print(f'Gain(Decision, Outlook) = {IG_decision_Outlook:0.4f}')</p><p>The output will be as follows:</p><div id="_idContainer288" class="IMG---Figure"><img src="image/C12622_04_56.jpg" alt="Figure 4.56: Probabilities entropies and gains"/></div><h6>Figure 4.56: Probabilities entropies and gains</h6><p>The final gain equation for <strong class="bold">Outlook</strong> can be re-written as:</p><div id="_idContainer289" class="IMG---Figure"><img src="image/C12622_04_57.jpg" alt="Figure 4.57: Equation of information gain"/></div><h6>Figure 4.57: Equation of information gain</h6></li>
				<li>We need to repeat this process quite a few times, so wrap it in a function for ease of use later:<p class="snippet">def IG(data, column, ent_decision=entropy_decision):</p><p class="snippet">    IG_decision = ent_decision</p><p class="snippet">    for name, temp in data.groupby(column):</p><p class="snippet">        p_p = len(temp.loc[temp.Decision == 'P']) / len(temp)</p><p class="snippet">        p_n = len(temp.loc[temp.Decision != 'P']) / len(temp)</p><p class="snippet">        entropy_decision = 0</p><p class="snippet">        if p_p != 0:</p><p class="snippet">            entropy_decision -= (p_p) * np.log2(p_p)</p><p class="snippet">        if p_n != 0:</p><p class="snippet">            entropy_decision -= (p_n) * np.log2(p_n)</p><p class="snippet">        IG_decision -= (len(temp) / len(df)) * entropy_decision</p><p class="snippet">    return IG_decision</p></li>
				<li>Repeat this process for each of the other columns to compute the corresponding information gain:<p class="snippet">for col in df.columns[:-1]:</p><p class="snippet">    print(f'Gain(Decision, {col}) = {IG(df, col):0.4f}')</p><p>We'll get the following output:</p><div id="_idContainer290" class="IMG---Figure"><img src="image/C12622_04_58.jpg" alt="Figure 4.58: Gains"/></div><h6>Figure 4.58: Gains</h6></li>
				<li>This information provides the first decision of the tree. We want to split on the maximum information gain, thus we split on <strong class="bold">Outlook</strong>. Look at the data splitting on <strong class="bold">Outlook</strong>:<p class="snippet">for name, temp in df.groupby('Outlook'):</p><p class="snippet">    print('-' * 15)</p><p class="snippet">    print(name)</p><p class="snippet">    print('-' * 15)</p><p class="snippet">    print(temp)</p><p class="snippet">    print('-' * 15)</p><p>The output will be as follows:</p><div id="_idContainer291" class="IMG---Figure"><img src="image/C12622_04_59.jpg" alt="Figure 4.59: Information gain"/></div><h6>Figure 4.59: Information gain</h6><p>Notice that all the overcast records have a decision of <strong class="bold">P</strong>. This provides our first terminating leaf of the decision tree. If it is overcast, we are going to play, while if it is rainy or sunny, there is a chance we will not play. The decision tree so far can be represented as in the following figure:</p><div id="_idContainer292" class="IMG---Figure"><img src="image/C12622_04_60.jpg" alt="Figure 4.60: Decision tree"/></div><h6>Figure 4.60: Decision tree</h6><h4>Note</h4><p class="callout">This figure was created manually for reference and is not contained in or obtained from the accompanying source code.</p></li>
				<li>We now repeat this process, splitting by information gain until all the data is allocated and all branches of the tree terminate. First, remove the overcast samples, as they no longer provide any additional information:<p class="snippet">df_next = df.loc[df.Outlook != 'overcast']</p><p class="snippet">df_next</p><p>We'll get the following output:</p><div id="_idContainer293" class="IMG---Figure"><img src="image/C12622_04_61.jpg" alt="Figure 4.61: Data after removing the overcast samples"/></div><h6>Figure 4.61: Data after removing the overcast samples</h6></li>
				<li>Now, we will turn our attention to the sunny samples and will rerun the gain calculations to determine the best way to split the sunny information:<p class="snippet">df_sunny = df_next.loc[df_next.Outlook == 'sunny']</p></li>
				<li>Recompute the entropy for the sunny samples:<p class="snippet">entropy_decision = f_entropy_decision(df_sunny)</p><p class="snippet">entropy_decision</p><p>The output will be:</p><div id="_idContainer294" class="IMG---Figure"><img src="image/C12622_04_62.jpg" alt="Figure 4.62: Entropy decision"/></div><h6>Figure 4.62: Entropy decision</h6></li>
				<li>Run the gain calculations for the sunny samples:<p class="snippet">for col in df_sunny.columns[1:-1]:</p><p class="snippet">    print(f'Gain(Decision, {col}) = {IG(df_sunny, col, entropy_decision):0.4f}')</p><p>The output will be as follows:</p><div id="_idContainer295" class="IMG---Figure"><img src="image/C12622_04_63.jpg" alt="Figure 4.63: Gains"/></div><h6>Figure 4.63: Gains</h6></li>
				<li>Again, we select the largest gain, which is <strong class="bold">Humidity</strong>. Group the data by <strong class="bold">Humidity</strong>:<p class="snippet">for name, temp in df_sunny.groupby('Humidity'):</p><p class="snippet">    print('-' * 15)</p><p class="snippet">    print(name)</p><p class="snippet">    print('-' * 15)</p><p class="snippet">    print(temp)</p><p class="snippet">    print('-' * 15)</p><p>The output will be:</p><div id="_idContainer296" class="IMG---Figure"><img src="image/C12622_04_64.jpg" alt="Figure 4.64: After grouping data according to humidity"/></div><h6>Figure 4.64: After grouping data according to humidity</h6><p>We can see here that we have two terminating leaves in that when the <strong class="bold">Humidity</strong> is high there is a decision not to play, and, vice versa, when the <strong class="bold">Humidity</strong> is normal, there is the decision to play. So, updating our representation of the decision tree, we have:</p><div id="_idContainer297" class="IMG---Figure"><img src="image/C12622_04_65.jpg" alt="Figure 4.65: Decision tree with two values"/></div><h6>Figure 4.65: Decision tree with two values</h6></li>
				<li>So, the last set of data that requires classification is the rainy outlook data. Extract only the <strong class="bold">rain</strong> data and rerun the entropy calculation:<p class="snippet">df_rain = df_next.loc[df_next.Outlook == 'rain']</p><p class="snippet">entropy_decision = f_entropy_decision(df_rain)</p><p class="snippet">entropy_decision</p><p>The output will be:</p><div id="_idContainer298" class="IMG---Figure"><img src="image/C12622_04_66.jpg" alt="Figure 4.66: Entropy decision"/></div><h6>Figure 4.66: Entropy decision</h6></li>
				<li>Repeat the gain calculation with the <strong class="bold">rain</strong> subset:<p class="snippet">for col in df_rain.columns[1:-1]:</p><p class="snippet">    print(f'Gain(Decision, {col}) = {IG(df_rain, col, entropy_decision):0.4f}')</p><p>The output will be:</p><div id="_idContainer299" class="IMG---Figure"><img src="image/C12622_04_67.jpg" alt="Figure 4.67: Gains"/></div><h6>Figure 4.67: Gains</h6></li>
				<li>Again, splitting on the attribute with the largest gain value requires splitting on the <strong class="bold">Windy</strong> values. So, group the remaining information by <strong class="bold">Windy</strong>:<p class="snippet">for name, temp in df_rain.groupby('Windy'):</p><p class="snippet">    print('-' * 15)</p><p class="snippet">    print(name)</p><p class="snippet">    print('-' * 15)</p><p class="snippet">    print(temp)</p><p class="snippet">    print('-' * 15)</p><p>The output will be:</p></li>
			</ol>
			<div>
				<div id="_idContainer300" class="IMG---Figure">
					<img src="image/C12622_04_68.jpg" alt="Figure 4.68: Data grouped according to Windy"/>
				</div>
			</div>
			<h6>Figure 4.68: Data grouped according to Windy</h6>
			<p>Finally, we have all the terminating leaves required to complete the tree, as splitting on <strong class="bold">Windy</strong> provides two sets, all of which indicate either play (<strong class="bold">P</strong>) or no-play (<strong class="bold">N</strong>) values. Our complete decision tree is as follows:</p>
			<div>
				<div id="_idContainer301" class="IMG---Figure">
					<img src="image/C12622_04_69.jpg" alt="Figure 4.69: Final decision tree"/>
				</div>
			</div>
			<h6>Figure 4.69: Final decision tree</h6>
			<p>We can see here that decision trees, very much like K-NN models, consume the entire training set to construct the model. So, how do we make predictions with unseen information? Simply follow the tree. Look at the decision being made at each node and apply the data from the unseen sample. The prediction will then end up being the label specified at the terminating leaf. Let's say we had a weather forecast for the upcoming Saturday and we wanted to predict whether we were going to play or not. The weather forecast is as follows:</p>
			<div>
				<div id="_idContainer302" class="IMG---Figure">
					<img src="image/C12622_04_70.jpg" alt="Figure 4.70: Weather forecast for upcoming Saturday"/>
				</div>
			</div>
			<h6>Figure 4.70: Weather forecast for upcoming Saturday</h6>
			<p>The decision tree for this would be as follows (the dashed circles indicate selected leaves in the tree):</p>
			<div>
				<div id="_idContainer303" class="IMG---Figure">
					<img src="image/C12622_04_71.jpg" alt="Figure 4.71: Making a new prediction using a decision tree"/>
				</div>
			</div>
			<h6>Figure 4.71: Making a new prediction using a decision tree</h6>
			<p>Now, hopefully, you have a reasonable understanding of the underlying concept of decision trees and the process of making sequential decisions. We have covered one of the first decision tree methodologies in this exercise, but there are certainly more available, and many of the more modern methods, such as random forests, do not suffer as much from overfitting (see <em class="italics">Chapter 5</em>, <em class="italics">Ensemble Modeling</em>, for more information) as ID3. With the principles of decision trees in our toolkit, we will now look at applying a more complicated model using the functionality provided in scikit-learn.</p>
			<p>The scikit-learn decision tree methods implement the <strong class="keyword">CART</strong> (<strong class="keyword">Classification and Regression Tree</strong>) method, which provides the ability to use decision trees in both classification and regression problems. CART differs from ID3 in that the decisions are made by comparing the values of features against a calculated value, for example, for the Iris dataset, <em class="italics">is the petal width less than x mm</em>?</p>
			<h3 id="_idParaDest-125"><a id="_idTextAnchor139"/>Exercise 42: Iris Classification Using a CART Decision Tree</h3>
			<p>In this exercise, we will classify the Iris data using scikit-learn's decision tree classifier, which can be used in both classification and regression problems:</p>
			<ol>
				<li value="1">Import the required packages:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.tree import DecisionTreeClassifier</p></li>
				<li>Load the Iris dataset:<p class="snippet">df = pd.read_csv('iris-data.csv')</p><p class="snippet">df.head()</p><p>The output will be as follows:</p><div id="_idContainer304" class="IMG---Figure"><img src="image/C12622_04_72.jpg" alt="Figure 4.72: First five rows"/></div><h6>Figure 4.72: First five rows</h6></li>
				<li>Take a random sample of 10 rows to use for testing. Decision trees can overfit the training data, so this will provide an independent measure of the accuracy of the tree:<p class="snippet">np.random.seed(10)</p><p class="snippet">samples = np.random.randint(0, len(df), 10)</p><p class="snippet">df_test = df.iloc[samples]</p><p class="snippet">df = df.drop(samples)</p></li>
				<li>Fit the model to the training data and check the corresponding accuracy:<p class="snippet">model = DecisionTreeClassifier()</p><p class="snippet">model = model.fit(df[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']], df.Species)</p><p class="snippet">model.score(df[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']], df.Species)</p><p>The output will be as follows:</p><div id="_idContainer305" class="IMG---Figure"><img src="image/C12622_04_73.jpg" alt="Figure 4.73: Output of the model score"/></div><h6>Figure 4.73: Output of the model score</h6><p>Our model achieves 100% accuracy on the training set.</p></li>
				<li>Check the performance against the test set:<p class="snippet">model.score(df_test[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']], df_test.Species)</p><p>The output will be as follows:</p><div id="_idContainer306" class="IMG---Figure"><img src="image/C12622_04_74.jpg" alt="Figure 4.74: Output of the model score using df_test"/></div><h6>Figure 4.74: Output of the model score using df_test</h6></li>
				<li>One of the great things about decision trees is that we can visually represent the model and see exactly what is going on. Install the required dependency:<p class="snippet">!conda install python-graphviz</p></li>
				<li>Import the graphing package:<p class="snippet">import graphviz</p><p class="snippet">from sklearn.tree import export_graphviz</p></li>
				<li>Plot the model:<p class="snippet">dot_data = export_graphviz(model, out_file=None)</p><p class="snippet">graph = graphviz.Source(dot_data)</p><p class="snippet">graph</p><p>We'll get the following output:</p></li>
			</ol>
			<div>
				<div id="_idContainer307" class="IMG---Figure">
					<img src="image/C12622_04_75.jpg" alt="Figure 4.75: Decisions of the CART decision tree"/>
				</div>
			</div>
			<h6>Figure 4.75: Decisions of the CART decision tree</h6>
			<p>This figure illustrates the decisions of the CART decision tree in the scikit-learn model. The first line of the node is the decision that is made at each step. The first node <em class="italics">X[2] &lt;= 2.45</em> indicates that the training data is split on column two (petal length) on the basis of being less than or equal to 2.45. Those samples with a petal length of less than 2.45 (of which there are 46) are all of the iris setosa class, and, as such, all of a <strong class="inline">gini</strong> (a metric similar to information gain) of zero. If the petal length is greater than 2.45, the next decision is whether the petal width (column three) is less than or equal to 1.75mm. This decision/branching process continues until the tree has been exhausted and all terminating leaves have been constructed.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor140"/>Summary</h2>
			<p>We covered a number of powerful and extremely useful classification models in this chapter, starting with the use of linear regression as a classifier, then we observed a significant performance increase through the use of the logistic regression classifier. We then moved on to <strong class="keyword">memorizing</strong> models, such as K-NN, which, while simple to fit, was able to form complex non-linear boundaries in the classification process, even with images as input information into the model. We then finished our introduction to classification problems, looking at decision trees and the ID3 algorithm. We saw how decision trees, like K-NN models, memorize the training data using rules and decision gates to make predictions with quite a high degree of accuracy.</p>
			<p>In the next chapter, we will be extending what we have learned in this chapter. It will cover ensemble techniques, including boosting and the very effective random forest method.</p>
		</div>
	</body></html>
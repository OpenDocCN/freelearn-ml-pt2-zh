<html><head></head><body>
		<div id="_idContainer034">
			<h1 id="_idParaDest-23"><em class="italic"><a id="_idTextAnchor022"/>Chapter 1</em>: Machine Learning Landscape</h1>
			<p>Welcome to <em class="italic">Hands-On Gradient Boosting with XGBoost and Scikit-Learn</em>, a book that will teach you the foundations, tips, and tricks of XGBoost, the best machine learning algorithm for making predictions from tabular data.</p>
			<p>The focus of<a id="_idIndexMarker000"/> this book is <strong class="bold">XGBoost</strong>, also known as <strong class="bold">Extreme Gradient Boosting</strong>. The structure, function, and raw power of XGBoost will be fleshed out in increasing detail in each chapter. The chapters unfold to tell an incredible story: the story of XGBoost. By the end of this book, you will be an expert in leveraging XGBoost to make predictions from real data.</p>
			<p>In the first chapter, XGBoost is presented in a sneak preview. It makes a guest appearance in the larger context of <strong class="bold">machine learning</strong> regression and classification to set the stage for what's to come. </p>
			<p>This chapter focuses <a id="_idIndexMarker001"/>on preparing data for machine learning, a process also known as <strong class="bold">data wrangling</strong>. In addition to building machine learning models, you will learn about using efficient <strong class="bold">Python</strong> code to load data, describe data, handle null values, transform data into numerical columns, split data into training and test sets, build machine learning models, and implement <strong class="bold">cross-validation</strong>, as well as comparing <strong class="bold">linear regression</strong> and <strong class="bold">logistic regression</strong> models with XGBoost.</p>
			<p>The concepts and libraries presented in this chapter are used throughout the book.</p>
			<p>This chapter consists of the following topics:</p>
			<ul>
				<li><p>Previewing XGBoost</p></li>
				<li><p>Wrangling data</p></li>
				<li><p>Predicting regression</p></li>
				<li><p>Predicting classification</p></li>
			</ul>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Previewing XGBoost</h1>
			<p>Machine learning<a id="_idIndexMarker002"/> gained recognition with the first neural network in the 1940s, followed by the first machine learning checker champion in the 1950s. After some quiet decades, the field of machine learning took off when <strong class="bold">Deep Blue</strong> famously beat world chess champion Gary Kasparov in the 1990s. With a surge in computational power, the 1990s and early 2000s produced a plethora of academic papers revealing<a id="_idIndexMarker003"/> new machine<a id="_idIndexMarker004"/> learning algorithms such as <strong class="bold">random forests</strong> and <strong class="bold">AdaBoost</strong>. </p>
			<p>The general idea behind boosting is to transform weak learners into strong learners by iteratively improving upon errors. The key idea behind <strong class="bold">gradient boosting</strong> is to use gradient descent to minimize the errors of the residuals. This evolutionary strand, from standard machine learning algorithms to gradient boosting, is the focus of the first four chapters of this book.</p>
			<p>XGBoost is short for Extreme Gradient Boosting. The <em class="italic">Extreme</em> part refers to pushing the limits of computation to achieve gains in accuracy and speed. XGBoost's surging popularity is largely due to its unparalleled success in <strong class="bold">Kaggle competitions</strong>. In Kaggle competitions, competitors build machine learning models in attempts to make the best predictions and win lucrative cash prizes. In comparison to other models, XGBoost has been crushing the competition.</p>
			<p>Understanding the details of XGBoost requires understanding the landscape of machine learning within the context of gradient boosting. In order to paint a full picture, we start at the beginning, with the basics of machine learning.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>What is machine learning?</h2>
			<p>Machine learning<a id="_idIndexMarker005"/> is the ability of computers to learn from data. In 2020, machine learning predicts human behavior, recommends products, identifies faces, outperforms poker professionals, discovers exoplanets, identifies diseases, operates self-driving cars, personalizes the internet, and communicates directly with humans. Machine learning is leading the artificial intelligence revolution and affecting the bottom line of nearly every major corporation.</p>
			<p>In practice, machine learning means implementing computer algorithms whose weights are adjusted when new data comes in. Machine learning algorithms learn from datasets to make predictions about species classification, the stock market, company profits, human decisions, subatomic particles, optimal traffic routes, and more.</p>
			<p>Machine learning is the best tool at our disposal for transforming big data into accurate, actionable predictions. Machine learning, however, does not occur in a vacuum. Machine<a id="_idIndexMarker006"/> learning requires rows and columns of data.</p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>Data wrangling</h1>
			<p>Data wrangling<a id="_idIndexMarker007"/> is a comprehensive term that encompasses the various stages of data preprocessing before machine learning can begin. Data loading, data cleaning, data analysis, and data manipulation are all included within the sphere of data wrangling.</p>
			<p>This first chapter presents data wrangling in detail. The examples are meant to cover standard data wrangling challenges that can be swiftly handled by <strong class="bold">pandas</strong>, Python's special library for handling data analytics. Although no experience with <strong class="bold">pandas</strong> is required, basic knowledge of <strong class="bold">pandas</strong> will be beneficial. All code is explained so that readers new to <strong class="bold">pandas</strong> may follow along.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Dataset 1 – Bike rentals</h2>
			<p>The bike<a id="_idIndexMarker008"/> rentals<a id="_idIndexMarker009"/> dataset is our first dataset. The data source is the UCI Machine Learning Repository (<a href="https://archive.ics.uci.edu/ml/index.php">https://archive.ics.uci.edu/ml/index.php</a>), a world-famous data warehouse that is free to the public. Our bike rentals dataset has been adjusted from the original dataset (<a href="https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset">https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset</a>) by sprinkling in null values so that you can gain practice in correcting them.</p>
			<h3>Accessing the data</h3>
			<p>The first step in data <a id="_idIndexMarker010"/>wrangling is to access the data. This may be achieved with the following steps:</p>
			<ol>
				<li><p>Download the data. All files for this book have been stored on GitHub. You may download all files to your local computer by pressing the <strong class="bold">Clone</strong> button. Here is a visual:</p><div id="_idContainer016" class="IMG---Figure"><img src="image/B15551_01_01.jpg" alt="Figure 1.1 – Accessing data"/></div><p class="figure-caption">Figure 1.1 – Accessing data</p><p>After downloading the data, move it to a convenient location, such as a <strong class="source-inline">Data</strong> folder on your desktop.</p></li>
				<li><p>Open a Jupyter Notebook. You will find the link to download Jupyter Notebooks in the preface. Click on <strong class="bold">Anaconda</strong>, and then click on <strong class="bold">Jupyter Notebooks</strong>. Alternatively, type <strong class="source-inline">jupyter notebook</strong> in the terminal. After the web browser opens, you should see a list of folders and files. Go to the same folder as the <a id="_idIndexMarker011"/>bike rentals dataset and select <strong class="bold">New: Notebook: Python 3</strong>. Here is a visual guide:</p><div id="_idContainer017" class="IMG---Figure"><img src="image/B15551_01_02.jpg" alt="Figure 1.2 – Visual guide to accessing the Jupyter Notebook"/></div><p class="figure-caption">Figure 1.2 – Visual guide to accessing the Jupyter Notebook</p><p class="callout-heading">Tip</p><p class="callout">If you are having difficulties opening a Jupyter Notebook, see Jupyter's official trouble-shooting guide: <a href="https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html">https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html</a>.</p></li>
				<li><p>Enter the following code in the first cell of your Jupyter Notebook: </p><p class="source-code">import pandas as pd </p><p>Press <em class="italic">Shift</em> + <em class="italic">Enter</em> to run the cell. Now you may access the <strong class="source-inline">pandas</strong> library when you write <strong class="source-inline">pd</strong>. </p></li>
				<li><p>Load the data using <strong class="source-inline">pd.read_csv</strong>. Loading data requires a <strong class="source-inline">read</strong> method. The <strong class="source-inline">read</strong> method stores the data as a DataFrame, a <strong class="source-inline">pandas</strong> object for viewing, analyzing, and manipulating data. When loading the data, place the filename in quotation marks, and then run the cell:</p><p class="source-code">df_bikes = pd.read_csv('bike_rentals.csv')</p><p>If your data file is in a different location than your Jupyter Notebook, you must provide a file directory, such as <strong class="source-inline">Downloads/bike_rental.csv</strong>. </p><p>Now the data <a id="_idIndexMarker012"/>has been properly stored in a DataFrame called <strong class="source-inline">df_bikes</strong>.</p><p class="callout-heading">Tip</p><p class="callout"><strong class="bold">Tab completion</strong>: When <a id="_idIndexMarker013"/>coding in Jupyter Notebooks, after typing a few characters, press the <em class="italic">Tab</em> button. For CSV files, you should see the filename appear. Highlight the name with your cursor and press <em class="italic">Enter</em>. If the filename is the only available option, you may press <em class="italic">Enter</em>. Tab completion will make your coding experience faster and more reliable.</p></li>
				<li><p>Display the data using <strong class="source-inline">.head()</strong>. The final step is to view the data to ensure that it has loaded correctly. <strong class="source-inline">.head()</strong> is a DataFrame method that displays the first five rows of the DataFrame. You may place any positive integer in parentheses to view any number of rows. Enter the following code and press <em class="italic">Shift</em> + <em class="italic">Enter</em>:</p><p class="source-code">df_bikes.head()</p><p>Here is a screenshot of the first few lines along with the expected output:</p></li>
			</ol>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B15551_01_03.jpg" alt="Figure 1.3 –The bike_rental.csv output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 –The bike_rental.csv output</p>
			<p>Now that we have<a id="_idIndexMarker014"/> access to the data, let's take a look at three methods to understand the data.</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Understanding the data</h2>
			<p>Now that the data <a id="_idIndexMarker015"/>has been loaded, it's time to make sense of the data. Understanding the data is essential to making informed decisions down the road. Here are three great methods for making sense of the data. </p>
			<h3>.head()</h3>
			<p>You have <a id="_idIndexMarker016"/>already seen <strong class="source-inline">.head()</strong>, a widely used method to interpret column names and numbers. As the preceding output reveals, <strong class="source-inline">dteday</strong> is a date, while <strong class="source-inline">instant</strong> is an ordered index.</p>
			<h3> .describe()</h3>
			<p>Numerical <a id="_idIndexMarker017"/>statistics may be viewed by using <strong class="source-inline">.describe()</strong> as follows:</p>
			<p class="source-code">df_bikes.describe()</p>
			<p>Here is the expected output:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B15551_01_04.jpg" alt="Figure 1.4 – The .describe() output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – The .describe() output</p>
			<p>You may need to scroll to the right to see all of the columns.</p>
			<p>Comparing the mean and median (50%) gives an indication of skewness. As you can see, <strong class="source-inline">mean</strong> and <strong class="source-inline">median</strong> are close to one another, so the data is roughly symmetrical. The <strong class="source-inline">max</strong> and <strong class="source-inline">min</strong> values <a id="_idIndexMarker018"/>of each column, along with the quartiles and standard deviation (<strong class="source-inline">std</strong>), are also presented.</p>
			<h3>.info()</h3>
			<p>Another great<a id="_idIndexMarker019"/> method is <strong class="source-inline">.info()</strong>, which displays general information about the columns and rows:</p>
			<p class="source-code">df_bikes.info()</p>
			<p>Here is the expected output:</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">RangeIndex: 731 entries, 0 to 730</p>
			<p class="source-code">Data columns (total 16 columns):</p>
			<p class="source-code"> #   Column      Non-Null Count  Dtype  </p>
			<p class="source-code">---  ------      --------------  -----  </p>
			<p class="source-code"> 0   instant     731 non-null    int64  </p>
			<p class="source-code"> 1   dteday      731 non-null    object </p>
			<p class="source-code"> 2   season      731 non-null    float64</p>
			<p class="source-code"> 3   yr          730 non-null    float64</p>
			<p class="source-code"> 4   mnth        730 non-null    float64</p>
			<p class="source-code"> 5   holiday     731 non-null    float64</p>
			<p class="source-code"> 6   weekday     731 non-null    float64</p>
			<p class="source-code"> 7   workingday  731 non-null    float64</p>
			<p class="source-code"> 8   weathersit  731 non-null    int64  </p>
			<p class="source-code"> 9   temp        730 non-null    float64</p>
			<p class="source-code"> 10  atemp       730 non-null    float64</p>
			<p class="source-code"> 11  hum         728 non-null    float64</p>
			<p class="source-code"> 12  windspeed   726 non-null    float64</p>
			<p class="source-code"> 13  casual      731 non-null    int64  </p>
			<p class="source-code"> 14  registered  731 non-null    int64  </p>
			<p class="source-code"> 15  cnt         731 non-null    int64  </p>
			<p class="source-code">dtypes: float64(10), int64(5), object(1)</p>
			<p class="source-code">memory usage: 91.5+ KB</p>
			<p>As you can see, <strong class="source-inline">.info()</strong> gives<a id="_idIndexMarker020"/> the number of rows, number of columns, column types, and non-null values. Since the number of non-null values differs between columns, null values must be present.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Correcting null values</h2>
			<p>If null values are<a id="_idIndexMarker021"/> not corrected, unexpected errors may arise down the road. In this subsection, we present a variety of methods that may be used to correct null values. Our examples are designed not only to handle null values but also to highlight the breadth and depth of <strong class="source-inline">pandas</strong>.</p>
			<p>The following methods may be used to correct null values.</p>
			<h3>Finding the number of null values</h3>
			<p>The following <a id="_idIndexMarker022"/>code displays the total number of null values:</p>
			<p class="source-code">df_bikes.isna().sum().sum()</p>
			<p>Here is the outcome:</p>
			<p class="source-code">12</p>
			<p>Note that two <strong class="source-inline">.sum()</strong> methods are required. The first method sums the null values of each column, while<a id="_idIndexMarker023"/> the second method sums the column counts.</p>
			<h3>Displaying null values</h3>
			<p>You can display<a id="_idIndexMarker024"/> all rows containing null values with the following code:</p>
			<p class="source-code"> df_bikes[df_bikes.isna().any(axis=1)]</p>
			<p>This code may be broken down as follows: <strong class="source-inline">df_bikes[conditional]</strong> is a subset of <strong class="source-inline">df_bikes</strong> that meets the condition in brackets. <strong class="source-inline">.df_bikes.isna().any</strong> gathers any and all null values while <strong class="source-inline">(axis=1)</strong> specifies values in the columns. In pandas, rows are <strong class="source-inline">axis 0</strong> and columns are <strong class="source-inline">axis 1</strong>. </p>
			<p>Here is the expected output:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B15551_01_05.jpg" alt="Figure 1.5 – Bike Rentals dataset null values"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – Bike Rentals dataset null values</p>
			<p>As you can see from the output, there are null values in the <strong class="source-inline">windspeed</strong>, <strong class="source-inline">humidity</strong>, and <strong class="source-inline">temperature</strong> columns along with the last row.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If this is your first time working with <strong class="bold">pandas</strong>, it may take time to get used to the notation. Check out Packt's <em class="italic">Hands-On Data Analysis with Pandas</em> for a great introduction: <a href="https://subscription.packtpub.com/book/data/9781789615326">https://subscription.packtpub.com/book/data/9781789615326</a>.</p>
			<h3>Correcting null values</h3>
			<p>Correcting null <a id="_idIndexMarker025"/>values depends on the column and dataset. Let's go over some strategies.</p>
			<h4>Replacing with the median/mean</h4>
			<p>One common<a id="_idIndexMarker026"/> strategy is to replace null values with the median or mean. The idea here is to replace null values with the average column value. </p>
			<p>For the <strong class="source-inline">'windspeed'</strong> column, the null values may be replaced with the <strong class="source-inline">median</strong> value as follows:</p>
			<p class="source-code">df_bikes['windspeed'].fillna((df_bikes['windspeed'].median()), inplace=True)</p>
			<p><strong class="source-inline">df_bikes['windspeed'].fillna</strong> means that the null values of the <strong class="source-inline">'windspeed'</strong> column will be filled. <strong class="source-inline">df_bikes['windspeed'].median()</strong> is the median of the <strong class="source-inline">'windspeed'</strong> column. Finally, <strong class="source-inline">inplace=True</strong> ensures that the changes are permanent.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The median is often a better choice than the mean. The median guarantees that half the data is greater than the given value and half the data is lower. The mean, by contrast, is vulnerable to <strong class="bold">outliers</strong>.</p>
			<p>In the previous cell, <strong class="source-inline">df_bikes[df_bikes.isna().any(axis=1)]</strong> revealed rows <strong class="source-inline">56</strong> and <strong class="source-inline">81</strong> with null values for <strong class="source-inline">windspeed</strong>. These rows may be displayed using <strong class="source-inline">.iloc</strong>, short for <strong class="bold">index location</strong>: </p>
			<p class="source-code">df_bikes.iloc[[56, 81]]</p>
			<p>Here is the expected output:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B15551_01_06.jpg" alt="Figure 1.6 – Rows 56 and 81"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – Rows 56 and 81</p>
			<p>As expected, the null <a id="_idIndexMarker027"/>values have been replaced with the windspeed median. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">It's common for users to make mistakes with single or double brackets when using <strong class="bold">pandas</strong>. <strong class="source-inline">.iloc</strong> uses single brackets for one index as follows: <strong class="source-inline">df_bikes.iloc[56]</strong>. Now, <strong class="source-inline">df_bikes</strong> also accepts a list inside brackets to allow multiple indices. Multiple indices require double brackets as follows: <strong class="source-inline">df_bikes.iloc[[56, 81]]</strong>. Please see <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html</a> for further documentation.</p>
			<h4>Groupby with the median/mean</h4>
			<p>It's possible to get<a id="_idIndexMarker028"/> more nuanced when correcting null values by using a <strong class="bold">groupby</strong>. </p>
			<p>A groupby organizes rows by shared values. Since there are four shared seasons spread out among the rows, a groupby of seasons results in a total of four rows, one for each season. But each season comes from many different rows with different values. We need a way to combine, or aggregate, the values. Choices for the aggregate include <strong class="source-inline">.sum()</strong>, <strong class="source-inline">.count()</strong>, <strong class="source-inline">.mean()</strong>, and <strong class="source-inline">.median()</strong>. We use <strong class="source-inline">.median()</strong>.</p>
			<p>Grouping <strong class="source-inline">df_bikes</strong> by season with the <strong class="source-inline">.median()</strong> aggregate is achieved as follows: </p>
			<p class="source-code">df_bikes.groupby(['season']).median()</p>
			<p>Here is the expected output:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B15551_01_07.jpg" alt="Figure 1.7 – The output of grouping df_bikes by season"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – The output of grouping df_bikes by season</p>
			<p>As you can see, the column values are the medians.</p>
			<p>To correct the null values in the <strong class="source-inline">hum</strong> column, short for <strong class="bold">humidity</strong>, we can take the median humidity by season.</p>
			<p>The code for correcting null values in the <strong class="source-inline">hum</strong> column is <strong class="source-inline">df_bikes['hum'] = df_bikes['hum'].fillna()</strong>.</p>
			<p>The code that goes inside <strong class="source-inline">fillna</strong> is the desired values. The values obtained from <strong class="source-inline">groupby</strong> require<a id="_idIndexMarker029"/> the <strong class="source-inline">transform</strong> method as follows: </p>
			<p class="source-code">df_bikes.groupby('season')['hum'].transform('median')</p>
			<p>Here is the combined code in one long step:</p>
			<p class="source-code">df_bikes['hum'] = df_bikes['hum'].fillna(df_bikes.groupby('season')['hum'].transform('median'))</p>
			<p>You may verify the transformation by checking <strong class="source-inline">df_bikes.iloc[[129, 213, 388]]</strong>.</p>
			<h4>Obtaining the median/mean from specific rows</h4>
			<p>In some <a id="_idIndexMarker030"/>cases, it may be advantageous to replace null values with data from specific rows.</p>
			<p>When correcting temperature, aside from consulting historical records, taking the mean temperature of the day before and the day after should give a good estimate. </p>
			<p>To find null values of the <strong class="source-inline">'temp'</strong> column, enter the following code:</p>
			<p class="source-code">df_bikes[df_bikes['temp'].isna()]</p>
			<p>Here is the expected output:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B15551_01_08.jpg" alt="Figure 1.8 – The output of the 'temp' column"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – The output of the 'temp' column</p>
			<p>As you can see, index <strong class="source-inline">701</strong> contains null values.</p>
			<p>To find the mean temperature of the day before and the day after the <strong class="source-inline">701</strong> index, complete the following steps:</p>
			<ol>
				<li value="1"><p>Sum the temperatures in rows <strong class="source-inline">700</strong> and <strong class="source-inline">702</strong> and divide by <strong class="source-inline">2</strong>. Do this for the <strong class="source-inline">'temp'</strong> and <strong class="source-inline">'atemp'</strong> columns: </p><p class="source-code">mean_temp = (df_bikes.iloc[700]['temp'] + df_bikes.iloc[702]['temp'])/2</p><p class="source-code">mean_atemp = (df_bikes.iloc[700]['atemp'] + df_bikes.iloc[702]['atemp'])/2</p></li>
				<li><p>Replace the null values: </p><p class="source-code">df_bikes['temp'].fillna((mean_temp), inplace=True)</p><p class="source-code">df_bikes['atemp'].fillna((mean_atemp), inplace=True)</p></li>
			</ol>
			<p>You may verify on <a id="_idIndexMarker031"/>your own that the null values have been filled as expected.</p>
			<h4>Extrapolate dates</h4>
			<p>Our final strategy <a id="_idIndexMarker032"/>to correct null values involves dates. When real dates are provided, date values may be extrapolated.</p>
			<p><strong class="source-inline">df_bikes['dteday']</strong> is a date column; however, the type of column revealed by <strong class="source-inline">df_bikes.info()</strong> is an object, commonly represented as a string. Date objects such as years and months must be extrapolated from <strong class="source-inline">datetime</strong> types. <strong class="source-inline">df_bikes['dteday']</strong> may be converted to a <strong class="source-inline">'datetime'</strong> type using the <strong class="source-inline">to_datetime</strong> method, as follows:</p>
			<p class="source-code">df_bikes['dteday'] = pd.to_datetime(df_bikes['dteday'],infer_datetime_format=True)</p>
			<p><strong class="source-inline">infer_datetime_format=True</strong> allows <strong class="bold">pandas</strong> to decide the kind of datetime object to store, a safe option in most cases.</p>
			<p>To extrapolate individual columns, first import the <strong class="source-inline">datetime</strong> library:</p>
			<p class="source-code">import datetime as dt</p>
			<p>We can now extrapolate dates for the null values using some different approaches. A standard approach is convert the '<strong class="source-inline">mnth</strong>' column to the correct months extrapolated from the 'dteday' column. This has the advantage of correcting any additional errors that may have surfaced in conversions, assuming of course that the '<strong class="source-inline">dteday</strong>' column is correct.</p>
			<p>The code is as follows:</p>
			<p class="source-code">ddf_bikes['mnth'] = df_bikes['dteday'].dt.month</p>
			<p>It's important to verify <a id="_idIndexMarker033"/>the changes. Since the null date values were in the last row, we can use <strong class="source-inline">.tail()</strong>, a DataFrame method similar to <strong class="source-inline">.head()</strong>, that shows the last five rows:</p>
			<p class="source-code">df_bikes.tail()</p>
			<p>Here is the expected output:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B15551_01_09.jpg" alt="Figure 1.9 – The output of the extrapolated date values"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9 – The output of the extrapolated date values</p>
			<p>As you can see, the month values are all correct, but the year value needs to be changed.</p>
			<p>The years of the last five rows in the '<strong class="source-inline">dteday</strong>' column are all <strong class="source-inline">2012</strong>, but the corresponding year provided by the '<strong class="source-inline">yr</strong>' column is <strong class="source-inline">1.0</strong>. Why?</p>
			<p>The data is normalized, meaning it's converted to values between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>.</p>
			<p>Normalized data is often more efficient because machine learning weights do not have to adjust for different ranges.</p>
			<p>You can use the .loc method to fill in the correct value. The <strong class="source-inline">.loc</strong> method is used to locate entries by row and column as follows:</p>
			<p class="source-code">df_bikes.loc[730, 'yr'] = 1.0</p>
			<p>Now that you have practiced correcting null values and have gained significant experience <a id="_idIndexMarker034"/>with <strong class="bold">pandas</strong>, it's time to address non-numerical columns.</p>
			<h3>Deleting non-numerical columns</h3>
			<p>For machine<a id="_idIndexMarker035"/> learning, all data columns<a id="_idIndexMarker036"/> should be numerical. According to <strong class="source-inline">df.info()</strong>, the only column that is not numerical is <strong class="source-inline">df_bikes['dteday']</strong>. Furthermore, it's redundant since all date information exists in other columns.</p>
			<p>The column may be deleted as follows:</p>
			<p class="source-code">df_bikes = df_bikes.drop('dteday', axis=1)</p>
			<p>Now that we have all numerical columns and no null values, we are ready for machine learning.</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Predicting regression</h1>
			<p>Machine learning <a id="_idIndexMarker037"/>algorithms aim to predict the values of one output column using data from one or more input columns. The predictions rely on mathematical equations determined by the general class of machine learning problems being addressed. Most supervised learning problems are classified as regression or classification. In this section, machine learning is introduced in the context of regression.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Predicting bike rentals</h2>
			<p>In the bike<a id="_idIndexMarker038"/> rentals dataset, <strong class="source-inline">df_bikes['cnt']</strong> is the <a id="_idIndexMarker039"/>number of bike rentals in a given day. Predicting this column would be of great use to a bike rental company. Our problem is to predict the correct number of bike rentals on a given day based on data such as whether this day is a holiday or working day, forecasted temperature, humidity, windspeed, and so on.</p>
			<p>According to the dataset, <strong class="source-inline">df_bikes['cnt']</strong> is the sum of <strong class="source-inline">df_bikes['casual']</strong> and <strong class="source-inline">df_bikes['registered']</strong>. If <strong class="source-inline">df_bikes['registered']</strong> and <strong class="source-inline">df_bikes['casual']</strong> were included as input columns, predictions would always be 100% accurate since these columns would always sum to the correct result. Although perfect predictions are ideal in theory, it makes no sense to include input columns that would be unknown in reality.</p>
			<p>All current <a id="_idIndexMarker040"/>columns may be used to predict <strong class="source-inline">df_bikes['cnt']</strong> except<a id="_idIndexMarker041"/> for <strong class="source-inline">'casual'</strong> and <strong class="source-inline">'registered'</strong>, as explained previously. Drop the <strong class="source-inline">'casual'</strong> and <strong class="source-inline">'registered'</strong> columns using the <strong class="source-inline">.drop</strong> method as follows:</p>
			<p class="source-code">df_bikes = df_bikes.drop(['casual', 'registered'], axis=1)</p>
			<p>The dataset is now ready.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Saving data for future use</h2>
			<p>The bike rentals <a id="_idIndexMarker042"/>dataset will be used multiple times in<a id="_idIndexMarker043"/> this book. Instead of running this notebook each time to perform data wrangling, you can export the clean dataset to a CSV file for future use: </p>
			<p class="source-code">df_bikes.to_csv('bike_rentals_cleaned.csv', index=False)</p>
			<p>The <strong class="source-inline">index=False</strong> parameter prevents an additional column from being created by the index.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Declaring predictor and target columns</h2>
			<p>Machine<a id="_idIndexMarker044"/> learning works by performing mathematical <a id="_idIndexMarker045"/>operations on each of the predictor columns (input columns) to determine the target column (output column).</p>
			<p>It's standard to group the predictor columns with a capital <strong class="source-inline">X</strong>, and the target column as a lowercase <strong class="source-inline">y</strong>. Since our target column is the last column, splitting the data into predictor and target columns may be done via slicing using index notation:</p>
			<p class="source-code">X = df_bikes.iloc[:,:-1]y = df_bikes.iloc[:,-1]</p>
			<p>The comma separates columns from rows. The first colon, <strong class="source-inline">:</strong>, means that all rows are included. After the comma, <strong class="source-inline">:-1</strong> means start at the first column and go all the way to the last <a id="_idIndexMarker046"/>column without including it. The second <strong class="source-inline">-1</strong> takes the<a id="_idIndexMarker047"/> last column only. </p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Understanding regression</h2>
			<p>Predicting the number <a id="_idIndexMarker048"/>of bike rentals, in reality, could result in any <strong class="bold">non-negative integer</strong>. When the target column includes a range of unlimited values, the machine<a id="_idIndexMarker049"/> learning problem is classified as <strong class="bold">regression</strong>.</p>
			<p>The most common regression algorithm is linear regression. Linear regression takes each predictor column as a <strong class="bold">polynomial variable</strong> and multiplies the values by <strong class="bold">coefficients</strong> (also<a id="_idIndexMarker050"/> called <strong class="bold">weights</strong>) to predict the target column. <strong class="bold">Gradient descent</strong> works under the hood to minimize the error. The predictions of linear regression could be any real number.</p>
			<p>Before running linear regression, we must split the data into a training set and a test set. The training set fits the data to the algorithm, using the target column to minimize the error. After a model is built, it's scored against the test data. </p>
			<p>The importance of holding out a test set to score the model cannot be overstated. In the world of big data, it's common to <strong class="bold">overfit</strong> the data to the training set because there are so many data points to train on. Overfitting is generally bad because the model adjusts itself too closely to outliers, unusual instances, and temporary trends. Strong machine learning models strike a nice balance between generalizing well to new data and accurately picking up on the nuances of the data at hand, a concept explored in detail in <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a><em class="italic">, Decision Trees in Depth</em>.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Accessing scikit-learn</h2>
			<p>All machine <a id="_idIndexMarker051"/>learning libraries will be handled through <strong class="bold">scikit-learn</strong>. Scikit-learn's <a id="_idIndexMarker052"/>range, ease of use, and computational power place it among the most widespread machine learning libraries in the world.</p>
			<p>Import <strong class="source-inline">train_test_split</strong> and <strong class="source-inline">LinearRegression</strong> from scikit-learn as follows:</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">from sklearn.linear_model import LinearRegression</p>
			<p>Next, split the data into the training set and test set:	</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)</p>
			<p>Note the <strong class="source-inline">random_state=2</strong> parameter. Whenever you see <strong class="source-inline">random_state=2</strong>, this means that <a id="_idIndexMarker053"/>you are choosing the seed of a pseudo-random<a id="_idIndexMarker054"/> number generator to ensure reproducible results.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Silencing warnings</h2>
			<p>Before building<a id="_idIndexMarker055"/> your first machine learning model, silence all warnings. Scikit-learn includes warnings to notify users of future changes. In general, it's not advisable to silence warnings, but since our code has been tested, it's recommended to save space in your Jupyter Notebook.</p>
			<p>Warnings may be silenced as follows:</p>
			<p class="source-code">import warnings</p>
			<p class="source-code">warnings.filterwarnings('ignore')</p>
			<p>It's time to build your first model.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Modeling linear regression</h2>
			<p>A linear<a id="_idIndexMarker056"/> regression model may be built with the following steps:</p>
			<ol>
				<li value="1"><p>Initialize a machine learning model:</p><p class="source-code">lin_reg = LinearRegression()</p></li>
				<li><p>Fit the model on the training set. This is where the machine learning model is built. Note that <strong class="source-inline">X_train</strong> is the predictor column and <strong class="source-inline">y_train</strong> is the target column.</p><p class="source-code">lin_reg.fit(X_train, y_train)</p></li>
				<li><p>Make predictions for the test set. The predictions of <strong class="source-inline">X_test</strong>, the predictor columns in the test set, are stored as <strong class="source-inline">y_pred</strong> using the <strong class="source-inline">.predict</strong> method on <strong class="source-inline">lin_reg</strong>:</p><p class="source-code">y_pred = lin_reg.predict(X_test)</p></li>
				<li><p>Compare the predictions with the test set. Scoring the model requires a basis of comparison. The standard <a id="_idIndexMarker057"/>for linear regression is the <strong class="bold">root mean squared error </strong>(<strong class="bold">RMSE</strong>). The RMSE requires two pieces: <strong class="source-inline">mean_squared_error</strong>, the sum of the squares of differences between predicted and actual values, and the square root, to keep the units the same. <strong class="source-inline">mean_squared_error</strong> may be imported, and the square root may be taken with <strong class="bold">Numerical Python</strong>, popularly<a id="_idIndexMarker058"/> known<a id="_idIndexMarker059"/> as <strong class="bold">NumPy</strong>, a blazingly fast library designed to work with <strong class="bold">pandas</strong>. </p></li>
				<li><p>Import <strong class="source-inline">mean_squared_error</strong> and NumPy, and then compute the mean squared error and take the square root:</p><p class="source-code">from sklearn.metrics import mean_squared_error</p><p class="source-code">import numpy as np</p><p class="source-code">mse = mean_squared_error(y_test, y_pred)</p><p class="source-code">rmse = np.sqrt(mse)</p></li>
				<li><p>Print your results:</p><p class="source-code">print("RMSE: %0.2f" % (rmse))</p><p>The outcome is as follows:</p><p class="source-code">RMSE: 898.21</p><p>Here is a screenshot of all the code to build your first machine learning model:</p></li>
			</ol>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B15551_01_10.jpg" alt="Figure 1.10 – Code to build your machine learning model"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10 – Code to build your machine learning model</p>
			<p>It's hard to know whether an error of <strong class="source-inline">898</strong> rentals is good or bad without knowing the expected range <a id="_idIndexMarker060"/>of rentals per day.</p>
			<p>The <strong class="source-inline">.describe()</strong> method may be used on the <strong class="source-inline">df_bikes['cnt']</strong> column to obtain the range and more:</p>
			<p class="source-code">df_bikes['cnt'].describe()</p>
			<p>Here is the output:</p>
			<p class="source-code">count     731.000000</p>
			<p class="source-code">mean     4504.348837</p>
			<p class="source-code">std      1937.211452</p>
			<p class="source-code">min        22.000000</p>
			<p class="source-code">25%      3152.000000</p>
			<p class="source-code">50%      4548.000000</p>
			<p class="source-code">75%      5956.000000</p>
			<p class="source-code">max      8714.000000</p>
			<p class="source-code">Name: cnt, dtype: float64</p>
			<p>With a range of <strong class="source-inline">22</strong> to <strong class="source-inline">8714</strong>, a mean of <strong class="source-inline">4504</strong>, and a standard deviation of <strong class="source-inline">1937</strong>, an RMSE of <strong class="source-inline">898</strong> isn't<a id="_idIndexMarker061"/> bad, but it's not great either.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>XGBoost</h2>
			<p>Linear regression is<a id="_idIndexMarker062"/> one of many algorithms <a id="_idIndexMarker063"/>that may be used to solve regression problems. It's possible that other regression algorithms will produce better results. The general strategy is to experiment with different regressors to compare scores. Throughout this book, you will experiment with a wide range of regressors, including decision trees, random forests, gradient boosting, and the focus of this book, XGBoost.</p>
			<p>A comprehensive introduction to XGBoost will be provided later in this book. For now, note that XGBoost includes a regressor, called <strong class="source-inline">XGBRegressor</strong>, that may be used on any regression dataset, including the bike rentals dataset that has just been scored. Let's now use the <strong class="source-inline">XGBRegressor</strong> to compare results on the bike rentals dataset with linear regression.</p>
			<p>You should have already installed XGBoost in the preface. If you have not done so, install XGBoost now.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>XGBRegressor</h2>
			<p>After XGBoost <a id="_idIndexMarker064"/>has been installed, the XGBoost regressor may be<a id="_idIndexMarker065"/> imported as follows:</p>
			<p class="source-code">from xgboost import XGBRegressor</p>
			<p>The general steps for building <strong class="source-inline">XGBRegressor</strong> are the same as with <strong class="source-inline">LinearRegression</strong>. The only difference is to initialize <strong class="source-inline">XGBRegressor</strong> instead of <strong class="source-inline">LinearRegression</strong>:</p>
			<ol>
				<li value="1"><p>Initialize a machine learning model:</p><p class="source-code">xg_reg = XGBRegressor()</p></li>
				<li><p>Fit the model on the training set. If you get some warnings from XGBoost here, don't worry:</p><p class="source-code">xg_reg.fit(X_train, y_train)</p></li>
				<li><p>Make predictions for the test set:</p><p class="source-code">y_pred = xg_reg.predict(X_test)</p></li>
				<li><p>Compare the predictions with the test set:</p><p class="source-code">mse = mean_squared_error(y_test, y_pred)</p><p class="source-code">rmse = np.sqrt(mse)</p></li>
				<li><p>Print your results:</p><p class="source-code">print("RMSE: %0.2f" % (rmse))</p><p>The output is as follows:</p><p class="source-code">RMSE: 705.11</p></li>
			</ol>
			<p><strong class="source-inline">XGBRegressor</strong> performs substantially better! </p>
			<p>The reason why XGBoost often<a id="_idIndexMarker066"/> performs better than others will be<a id="_idIndexMarker067"/> explored in <a href="B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117"><em class="italic">Chapter 5</em></a><em class="italic">, XGBoost Unveiled</em>.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Cross-validation</h2>
			<p>One test score is not<a id="_idIndexMarker068"/> reliable because splitting the data into different training and test sets would give different results. In effect, splitting the data into a training set and a test set is arbitrary, and a different <strong class="source-inline">random_state</strong> will give a different RMSE.</p>
			<p>One way to address<a id="_idIndexMarker069"/> the score discrepancies between different splits is <strong class="bold">k-fold cross-validation</strong>. The<a id="_idIndexMarker070"/> idea is to split the data multiple times into different training sets and test sets, and then to take the mean of the scores. The number of splits, called <strong class="bold">folds</strong>, is denoted by <strong class="bold">k</strong>. It's standard to use k = 3, 4, 5, or 10 splits.</p>
			<p>Here is a visual description of cross-validation:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B15551_01_11.jpg" alt="Figure 1.11 – Cross-validation"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – Cross-validation</p>
			<p class="figure-caption">(Redrawn from <a href="https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.svg">https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.svg</a>)</p>
			<p>Cross-validation works by fitting a machine learning model on the first training set and scoring it against the<a id="_idIndexMarker071"/> first test set. A different training set and test set are provided for the second split, resulting in a new machine learning model with its own score. A third split results in a new model and scores it against another test set.</p>
			<p>There is going to be overlap in the training sets, but not the test sets.</p>
			<p>Choosing the number of folds is flexible and depends on the data. Five folds is standard because 20% of the test set is held back each time. With 10 folds, only 10% of the data is held back; however, 90% of the data is available for training and the mean is less vulnerable to outliers. For a smaller datatset, three folds may work better. </p>
			<p>At the end, there will be k different scores evaluating the model against k different test sets. Taking the mean score of the k folds gives a more reliable score than any single fold.</p>
			<p><strong class="source-inline">cross_val_score</strong> is a convenient way to implement cross-validation. <strong class="source-inline">cross_val_score</strong> takes a machine learning algorithm as input, along with the predictor and target columns, with optional additional parameters that include a scoring metric and the desired number of folds.</p>
			<h3>Cross-validation with linear regression</h3>
			<p>Let's use<a id="_idIndexMarker072"/> cross-validation <a id="_idIndexMarker073"/>with <strong class="source-inline">LinearRegression</strong>. </p>
			<p>First, import <strong class="source-inline">cross_val_score</strong> from the <strong class="source-inline">cross_val_score</strong> library:</p>
			<p class="source-code">from sklearn.model_selection import cross_val_score</p>
			<p>Now use cross-validation to build and score a machine learning model in the following steps:</p>
			<ol>
				<li value="1"><p>Initialize a machine learning model:</p><p class="source-code">model = LinearRegression()</p></li>
				<li><p>Implement <strong class="source-inline">cross_val_score</strong> with the model, <strong class="source-inline">X</strong>, <strong class="source-inline">y</strong>, <strong class="source-inline">scoring='neg_mean_squared_error'</strong>, and the number of folds, <strong class="source-inline">cv=10</strong>, as input:</p><p class="source-code">scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)</p><p class="callout-heading">Tip</p><p class="callout">Why <strong class="source-inline">scoring='neg_mean_squared_error'</strong>? Scikit-learn is designed to select the highest score when training models. This works well for accuracy, but not for errors when the lowest is best. By taking the negative of each mean squared error, the lowest ends up being the highest. This is compensated for later with <strong class="source-inline">rmse = np.sqrt(-scores)</strong>, so the final results are positive.</p></li>
				<li><p>Find the RMSE by taking the square root of the negative scores:</p><p class="source-code">rmse = np.sqrt(-scores)</p></li>
				<li><p>Display the results:</p><p class="source-code">print('Reg rmse:', np.round(rmse, 2))</p><p class="source-code">print('RMSE mean: %0.2f' % (rmse.mean()))</p><p>The output is as follows:</p><p class="source-code">Reg rmse: [ 504.01  840.55 1140.88  728.39  640.2   969.95 </p><p class="source-code">1133.45 1252.85 1084.64  1425.33]</p><p class="source-code">RMSE mean: 972.02</p></li>
			</ol>
			<p>Linear regression has a mean error of <strong class="source-inline">972.06</strong>. This is slightly better than the <strong class="source-inline">980.38</strong> obtained before. The point here is not whether the score is better or worse. The point is that it's a better <a id="_idIndexMarker074"/>estimation of how linear regression will perform <a id="_idIndexMarker075"/>on unseen data.</p>
			<p>Using cross-validation is always recommended for a better estimate of the score.</p>
			<p class="callout-heading">About the print function</p>
			<p class="callout">When running your own machine learning code, the global <strong class="source-inline">print</strong> function is often not necessary, but it is helpful if you want to print out multiple lines and format the output as shown here.</p>
			<h3>Cross-validation with XGBoost</h3>
			<p>Now let's use<a id="_idIndexMarker076"/> cross-validation with <strong class="source-inline">XGBRegressor</strong>. The <a id="_idIndexMarker077"/>steps are the same, except for initializing the model:</p>
			<ol>
				<li value="1"><p>Initialize a machine learning model:</p><p class="source-code">model = XGBRegressor()</p></li>
				<li><p>Implement <strong class="source-inline">cross_val_score</strong> with the model, <strong class="source-inline">X</strong>, <strong class="source-inline">y</strong>, scoring, and the number of folds, <strong class="source-inline">cv</strong>, as input:</p><p class="source-code">scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)</p></li>
				<li><p>Find the RMSE by taking the square root of the negative scores:</p><p class="source-code">rmse = np.sqrt(-scores)</p></li>
				<li><p>Print the results:</p><p class="source-code">print('Reg rmse:', np.round(rmse, 2))</p><p class="source-code">print('RMSE mean: %0.2f' % (rmse.mean()))</p><p>The output is as follows:</p><p class="source-code">Reg rmse: [ 717.65  692.8   520.7   737.68  835.96 1006.24  991.34  747.61  891.99 1731.13]</p><p class="source-code">RMSE mean: 887.31</p></li>
			</ol>
			<p><strong class="source-inline">XGBRegressor</strong> wins <a id="_idIndexMarker078"/>again, besting linear regression by<a id="_idIndexMarker079"/> about 10%.</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Predicting classification</h1>
			<p>You learned that<a id="_idIndexMarker080"/> XGBoost may have an edge in regression, but what about classification? XGBoost has a classification model, but will it perform as accurately as well tested classification models such as logistic regression? Let's find out.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>What is classification?</h2>
			<p>Unlike with <a id="_idIndexMarker081"/>regression, when predicting target columns with a limited number of outputs, a machine learning algorithm is categorized as a classification algorithm. The possible outputs may include the following:</p>
			<ul>
				<li><p>Yes, No</p></li>
				<li><p>Spam, Not Spam</p></li>
				<li><p>0, 1</p></li>
				<li><p>Red, Blue, Green, Yellow, Orange</p></li>
			</ul>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Dataset 2 – The census</h2>
			<p>We will move a little <a id="_idIndexMarker082"/>more swiftly through the second dataset, the Census Income Data Set (<a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">https://archive.ics.uci.edu/ml/datasets/Census+Income</a>), to predict personal income.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Data wrangling</h2>
			<p>Before<a id="_idIndexMarker083"/> implementing machine learning, the dataset must be preprocessed. When testing new algorithms, it's essential to have all numerical columns with no null values.</p>
			<h3>Data loading</h3>
			<p>Since this dataset<a id="_idIndexMarker084"/> is hosted directly on the UCI Machine Learning website, it can be downloaded directly from the internet using <strong class="source-inline">pd.read_csv</strong>:</p>
			<p class="source-code">df_census = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data')</p>
			<p class="source-code">df_census.head()</p>
			<p>Here is the expected output:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B15551_01_12.jpg" alt="Figure 1.12 – The Census Income DataFrame"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – The Census Income DataFrame</p>
			<p>The output reveals that the column headings represent the entries of the first row. When this happens, the data may be reloaded with the <strong class="source-inline">header=None</strong> parameter:</p>
			<p class="source-code">df_census = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header=None)</p>
			<p class="source-code">df_census.head()</p>
			<p>Here is the expected output without the header:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B15551_01_13.jpg" alt="Figure 1.13 – The header=None parameter output"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13 – The header=None parameter output</p>
			<p>As you can see, the column names are still missing. They are listed on the Census Income Data Set website (<a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">https://archive.ics.uci.edu/ml/datasets/Census+Income</a>) under <em class="italic">Attribute Information</em>.</p>
			<p>Column<a id="_idIndexMarker085"/> names may be changed as follows:</p>
			<p class="source-code">df_census.columns=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']</p>
			<p class="source-code">df_census.head()</p>
			<p>Here is the expected output with column names:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B15551_01_14.jpg" alt="Figure 1.14 – Expected column names"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14 – Expected column names</p>
			<p>As you can see, the column names have been restored.</p>
			<h3>Null values</h3>
			<p>A great way<a id="_idIndexMarker086"/> to check null <a id="_idIndexMarker087"/>values is to look at the DataFrame <strong class="source-inline">.info()</strong> method:</p>
			<p class="source-code">df_census.info()</p>
			<p>The output is as follows:</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">RangeIndex: 32561 entries, 0 to 32560</p>
			<p class="source-code">Data columns (total 15 columns):</p>
			<p class="source-code"> #   Column          Non-Null Count  Dtype </p>
			<p class="source-code">---  ------          --------------  ----- </p>
			<p class="source-code"> 0   age             32561 non-null  int64 </p>
			<p class="source-code"> 1   workclass       32561 non-null  object</p>
			<p class="source-code"> 2   fnlwgt          32561 non-null  int64 </p>
			<p class="source-code"> 3   education       32561 non-null  object</p>
			<p class="source-code"> 4   education-num   32561 non-null  int64 </p>
			<p class="source-code"> 5   marital-status  32561 non-null  object</p>
			<p class="source-code"> 6   occupation      32561 non-null  object</p>
			<p class="source-code"> 7   relationship    32561 non-null  object</p>
			<p class="source-code"> 8   race            32561 non-null  object</p>
			<p class="source-code"> 9   sex             32561 non-null  object</p>
			<p class="source-code"> 10  capital-gain    32561 non-null  int64 </p>
			<p class="source-code"> 11  capital-loss    32561 non-null  int64 </p>
			<p class="source-code"> 12  hours-per-week  32561 non-null  int64 </p>
			<p class="source-code"> 13  native-country  32561 non-null  object</p>
			<p class="source-code"> 14  income          32561 non-null  object</p>
			<p class="source-code">dtypes: int64(6), object(9)</p>
			<p class="source-code">memory usage: 3.7+ MB</p>
			<p>Since all columns have the same number of non-null rows, we can infer that there are no null values. </p>
			<h3>Non-numerical columns</h3>
			<p>All columns of the <strong class="source-inline">dtype</strong> object must be <a id="_idIndexMarker088"/>transformed into <a id="_idIndexMarker089"/>numerical columns. A <strong class="bold">pandas</strong> <strong class="source-inline">get_dummies</strong> method takes the non-numerical unique values of every column and converts them into their own column, with <strong class="source-inline">1</strong> indicating presence and <strong class="source-inline">0</strong> indicating absence. For instance, if the column values of a DataFrame called "Book Types" were "hardback," "paperback," or "ebook," <strong class="source-inline">pd.get_dummies</strong> would create three new columns called "hardback," "paperback," and "ebook" replacing the "Book Types" column.</p>
			<p>Here is a "Book Types" DataFrame:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B15551_01_15.jpg" alt="Figure 1.15 – A &quot;Book Types&quot; DataFrame"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 1.15 – A "Book Types" DataFrame</p>
			<p>Here is the same DataFrame after <strong class="source-inline">pd.get_dummies</strong>:</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B15551_01_16.jpg" alt="Figure 1.16 – The new DataFrame"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.16 – The new DataFrame</p>
			<p><strong class="source-inline">pd.get_dummies</strong> will create many new columns, so it's worth checking to see whether any columns may be eliminated. A quick review of the <strong class="source-inline">df_census</strong> data reveals an <strong class="source-inline">'education'</strong> column and an <strong class="source-inline">education_num</strong> column. The <strong class="source-inline">education_num</strong> column is a numerical conversion of <strong class="source-inline">'education'</strong>. Since the information is the same, the <strong class="source-inline">'education'</strong> column may be deleted:</p>
			<p class="source-code">df_census = df_census.drop(['education'], axis=1)</p>
			<p>Now use <strong class="source-inline">pd.get_dummies</strong> to transform the non-numerical columns into numerical columns:</p>
			<p class="source-code">df_census = pd.get_dummies(df_census)</p>
			<p class="source-code">df_census.head()</p>
			<p>Here <a id="_idIndexMarker090"/>is the<a id="_idIndexMarker091"/> expected output:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B15551_01_17.jpg" alt="Figure 1.17 – pd.get_dummies – non-numerical to numerical columns"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.17 – pd.get_dummies – non-numerical to numerical columns</p>
			<p>As you can see, new columns are created using a <strong class="source-inline">column_value</strong> syntax referencing the original column. For example, <strong class="source-inline">native-country</strong> is an original column, and Taiwan is one of many values. The new <strong class="source-inline">native-country_Taiwan</strong> column has a value of <strong class="source-inline">1</strong> if the person is from Taiwan and <strong class="source-inline">0</strong> otherwise.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Using <strong class="source-inline">pd.get_dummies</strong> may increase memory usage, as can be verified using the <strong class="source-inline">.info()</strong> method on the DataFrame in question and checking the last line. <strong class="bold">Sparse matrices</strong> may be used to save memory where only values of <strong class="source-inline">1</strong> are stored and values of <strong class="source-inline">0</strong> are not stored. For more information on sparse matrices, see <a href="B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230"><em class="italic">Chapter 10</em></a>, <em class="italic">XGBoost Model Deployment</em>, or visit SciPy's official documentation at <a href="https://docs.scipy.org/doc/scipy/reference/">https://docs.scipy.org/doc/scipy/reference/</a>.</p>
			<h3>Target and predictor columns</h3>
			<p>Since all <a id="_idIndexMarker092"/>columns<a id="_idIndexMarker093"/> are numerical with no <a id="_idIndexMarker094"/>null <a id="_idIndexMarker095"/>values, it's time to split the data into target and predictor columns.</p>
			<p>The target column is whether or not someone makes 50K. After <strong class="source-inline">pd.get_dummies</strong>, two columns, <strong class="source-inline">df_census['income_&lt;=50K']</strong> and <strong class="source-inline">df_census['income_&gt;50K']</strong>, are used to determine whether someone makes 50K. Since either column will work, we delete <strong class="source-inline">df_census['income_ &lt;=50K']</strong>:</p>
			<p class="source-code">df_census = df_census.drop('income_ &lt;=50K', axis=1)</p>
			<p>Now split the data into <strong class="source-inline">X</strong> (predictor columns) and <strong class="source-inline">y</strong> (target column). Note that <strong class="source-inline">-1</strong> is used for indexing since the last column is the target column:</p>
			<p class="source-code">X = df_census.iloc[:,:-1]y = df_census.iloc[:,-1]</p>
			<p>It's time to build machine learning classifiers!</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Logistic regression</h2>
			<p>Logistic regression<a id="_idIndexMarker096"/> is the most fundamental classification algorithm. Mathematically, logistic regression works in a manner similar to linear regression. For each column, logistic regression finds an appropriate weight, or coefficient, that maximizes model accuracy. The primary difference is that instead of summing each term, as in linear regression, logistic <a id="_idIndexMarker097"/>regression uses the <strong class="bold">sigmoid function</strong>.</p>
			<p>Here is the sigmoid function and the corresponding graph:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B15551_01_18.jpg" alt="Figure 1.18 – Sigmoid function graph"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.18 – Sigmoid function graph</p>
			<p>The sigmoid is commonly used for classification. All values greater than 0.5 are matched to 1, and all values less than 0.5 are matched to 0.</p>
			<p>Implementing logistic regression with scikit-learn is nearly the same as implementing linear <a id="_idIndexMarker098"/>regression. The main differences are that the predictor column should fit into categories, and the error should be in terms of accuracy. As a bonus, the error is in terms of accuracy by default, so explicit scoring parameters are not required.</p>
			<p>You may import logistic regression as follows:</p>
			<p class="source-code">from sklearn.linear_model import LogisticRegression</p>
			<h3>The cross-validation function</h3>
			<p>Let's use cross-validation<a id="_idIndexMarker099"/> on logistic regression to predict whether someone makes over 50K.</p>
			<p>Instead of copying and pasting, let's build a cross-validation classification function that takes a machine learning algorithm as input and has the accuracy score as output using <strong class="source-inline">cross_val_score</strong>:</p>
			<p class="source-code">def cross_val(classifier, num_splits=10):    model = classifier     scores = cross_val_score(model, X, y, cv=num_splits)    print('Accuracy:', np.round(scores, 2))    print('Accuracy mean: %0.2f' % (scores.mean()))</p>
			<p>Now call the function with logistic regression:</p>
			<p class="source-code">cross_val(LogisticRegression())</p>
			<p>The output is as follows:</p>
			<p class="source-code">Accuracy: [0.8  0.8  0.79 0.8  0.79 0.81 0.79 0.79 0.8  0.8 ]</p>
			<p class="source-code">Accuracy mean: 0.80</p>
			<p>80% accuracy isn't bad out of the box. </p>
			<p>Let's see<a id="_idIndexMarker100"/> whether XGBoost can do better.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Any time you find yourself copying and pasting code, look for a better way! One aim of computer science is to avoid repetition. Writing your own data analysis and machine learning functions will make your life easier and your work more efficient in the long run.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>The XGBoost classifier</h2>
			<p>XGBoost has a<a id="_idIndexMarker101"/> regressor and a classifier. To use the classifier, import the following algorithm:</p>
			<p class="source-code">from xgboost import XGBClassifier</p>
			<p>Now run the classifier in the <strong class="source-inline">cross_val</strong> function with one important addition. Since there are 94 columns, and XGBoost is an ensemble method, meaning that it combines many models for each run, each of which includes 10 splits, we are going to limit <strong class="source-inline">n_estimators</strong>, the number of models, to <strong class="source-inline">5</strong>. Normally, XGBoost is very fast. In fact, it has a reputation for being the fastest boosting ensemble method out there, a reputation that we will check in this book! For our initial purposes, however, <strong class="source-inline">5</strong> estimators, though not as robust as the default of <strong class="source-inline">100</strong>, is sufficient. Details on choosing <strong class="source-inline">n_estimators</strong> will be a focal point of <a href="B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a><em class="italic">, From Gradient Boosting to XGBoost</em>:</p>
			<p class="source-code">cross_val(XGBClassifier(n_estimators=5))</p>
			<p>The output is as follows:</p>
			<p class="source-code">Accuracy: [0.85 0.86 0.87 0.85 0.86 0.86 0.86 0.87 0.86 0.86]</p>
			<p class="source-code">Accuracy mean: 0.86</p>
			<p>As you can see, XGBoost<a id="_idIndexMarker102"/> scores higher than logistic regression out of the box.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Summary</h1>
			<p>Your journey through XGBoost has officially begun! You started this chapter by learning the fundamentals of data wrangling and <strong class="bold">pandas</strong>, essential skills for all machine learning practitioners, with a focus on correcting null values. Next, you learned how to build machine learning models in scikit-learn by comparing linear regression with XGBoost. Then, you prepared a dataset for classification and compared logistic regression with XGBoost. In both cases, XGBoost was the clear winner.</p>
			<p>Congratulations on building your first XGBoost models! Your initiation into data wrangling and machine learning using the <strong class="bold">pandas</strong>, NumPy, and scikit-learn libraries is complete.</p>
			<p>In <a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a><em class="italic">, Decision Trees in Depth</em>, you will improve your machine learning skills by building decision trees, the base learners of XGBoost machine learning models, and fine-tuning hyperparameters to improve results. </p>
		</div>
	</body></html>
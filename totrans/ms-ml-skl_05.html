<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Nonlinear Classification and Regression with Decision Trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Nonlinear Classification and Regression with Decision Trees</h1></div></div></div><p>In the previous chapters we discussed generalized linear models, which relate a linear combination of explanatory variables to one or more response variables using a link function. You learned to use multiple linear regression to solve regression problems, and we used logistic regression for classification tasks. In this chapter we will discuss a simple, nonlinear model for classification and regression tasks: the decision tree. We'll use decision trees to build an ad blocker that can learn to classify images on a web page as banner advertisements or page content. Finally, we will introduce ensemble learning methods, which combine a set of models to produce an estimator with better predictive performance than any of its component estimators.</p><div class="section" title="Decision trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec43"/>Decision trees</h1></div></div></div><p>Decision trees are <a class="indexterm" id="id286"/>tree-like graphs that model a decision. They are analogous to the parlor game Twenty Questions. In Twenty Questions, one player, called the answerer, chooses an object but does not reveal the object to the other players, who are called questioners. The <a class="indexterm" id="id287"/>object should be a common noun, such as "guitar" or "sandwich", but not "1969 Gibson Les Paul Custom" or "North Carolina". The questioners must guess the object by asking as many as twenty questions that can be answered with <code class="literal">yes</code>, <code class="literal">no</code>, or <code class="literal">maybe</code>. An intuitive strategy for questioners is to ask questions of increasing specificity; asking "<span class="emphasis"><em>is it a musical instrument?</em></span>" as the first question will not efficiently reduce the number of possibilities. The branches of a decision tree specify the shortest sequences of explanatory variables that can be examined in order to estimate the value of a response variable. To continue the analogy, in Twenty Questions the questioner and the answerers all have knowledge of the training data, but only the answerer knows the values of the features for the test instance.</p><p>Decision trees are commonly learned by recursively splitting the set of training instances into subsets based on the instances' values for the explanatory variables. The following diagram depicts a decision tree that we will look at in more detail later in the chapter.</p><div class="mediaobject"><img alt="Decision trees" src="graphics/8365OS_05_01.jpg"/></div><p>Represented by boxes, the interior nodes of the decision tree test explanatory variables. These nodes are connected <a class="indexterm" id="id288"/>by edges that specify the possible outcomes of the tests. The training instances are divided into subsets based on the outcomes of the tests. For example, a node might test whether or not the value of an explanatory variable exceeds a threshold. The instances that pass the test will follow an edge to the node's right child, and the instances that fail the test will follow an edge to the node's left child. The children nodes similarly test their subsets of the training instances until a stopping criterion is satisfied. In classification tasks, the leaf nodes of the decision tree represent classes. In regression tasks, the values of the response variable for the instances contained in a leaf node may be averaged to produce the estimate for the response variable. After the <a class="indexterm" id="id289"/>decision tree has been constructed, making a prediction for a test instance requires only following the edges until a leaf node is reached.</p></div></div>
<div class="section" title="Training decision trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec44"/>Training decision trees</h1></div></div></div><p>Let's create a decision <a class="indexterm" id="id290"/>tree using an algorithm called <span class="strong"><strong>Iterative Dichotomiser 3</strong></span> (<span class="strong"><strong>ID3</strong></span>). Invented by Ross Quinlan, ID3 was one of the first algorithms used to train <a class="indexterm" id="id291"/>decision trees. Assume that you have to classify animals as cats or dogs. Unfortunately, you cannot observe the animals directly and must use only a few attributes of the animals to make your decision. For each animal, you are told whether or not it likes to play fetch, whether or not it is frequently grumpy, and its favorite of three types of food.</p><p>To classify new animals, the decision tree will examine an explanatory variable at each node. The edge it follows to the next node will depend on the outcome of the test. For example, the first node might ask whether or not the animal likes to play fetch. If the animal does, we will follow the edge to the left child node; if not, we will follow the edge to the right child node. Eventually an edge will connect to a leaf node that indicates whether the animal is a cat or a dog.</p><p>The following fourteen instances comprise our training data:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Training instance</p>
</th><th style="text-align: left" valign="bottom">
<p>Plays fetch</p>
</th><th style="text-align: left" valign="bottom">
<p>Is grumpy</p>
</th><th style="text-align: left" valign="bottom">
<p>Favorite food</p>
</th><th style="text-align: left" valign="bottom">
<p>Species</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Bacon</p>
</td><td style="text-align: left" valign="top">
<p>Dog</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Dog Food</p>
</td><td style="text-align: left" valign="top">
<p>Dog</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Cat food</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Bacon</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Cat food</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Bacon</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Cat Food</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Dog Food</p>
</td><td style="text-align: left" valign="top">
<p>Dog</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Cat food</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Dog Food</p>
</td><td style="text-align: left" valign="top">
<p>Dog</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Bacon</p>
</td><td style="text-align: left" valign="top">
<p>Dog</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Cat food</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>13</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Cat food</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>14</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Bacon</p>
</td><td style="text-align: left" valign="top">
<p>Dog</p>
</td></tr></tbody></table></div><p>From this data we can see that cats are generally grumpier than the dogs. Most dogs play fetch and most cats refuse. Dogs prefer dog food and bacon, whereas cats only like cat food and bacon. The <code class="literal">is grumpy</code> and <code class="literal">plays fetch</code> explanatory variables can be easily converted to binary-valued features. The <code class="literal">favorite food</code> explanatory variable is a categorical variable that has three possible values; we will one-hot encode it. Recall from <a class="link" href="ch03.html" title="Chapter 3. Feature Extraction and Preprocessing">Chapter 3</a>, <span class="emphasis"><em>Feature Extraction and Preprocessing</em></span>, that one-hot encoding represents a categorical variable with as many binary-valued features as there are values for variable. Representing the categorical <a class="indexterm" id="id292"/>variable with a single integer-valued feature will encode an artificial order to its values. Since <code class="literal">favorite food</code> has three possible states, we will represent it with three binary-valued features. From this table, we can manually construct classification rules. For example, an animal that is grumpy and likes cat food must be a cat, while an animal that plays fetch and likes bacon must be a dog. Constructing these classification rules by hand for even a small data set is cumbersome. Instead, we will learn these rules by creating a decision tree.</p><div class="section" title="Selecting the questions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec23"/>Selecting the questions</h2></div></div></div><p>Like Twenty <a class="indexterm" id="id293"/>Questions, the decision tree will estimate the value of the response variable by testing the values of a sequence of explanatory variables. Which explanatory variable should be tested first? Intuitively, a test that produces subsets <a class="indexterm" id="id294"/>that contain all cats or all dogs is better than a test that produces subsets that still contain both cats and dogs. If the members of a subset are of different classes, we are still uncertain about how to classify the instance. We should also avoid creating tests that separate only a single cat or dog from the others; such tests are analogous to asking specific questions in the first few rounds of Twenty Questions. More formally, these tests can infrequently classify an instance and might not reduce our uncertainty. The tests that reduce our uncertainty about the classification the most are the best. We can quantify the amount of uncertainty using a measure <a class="indexterm" id="id295"/>called <span class="strong"><strong>entropy</strong></span>.</p><p>Measured in bits, entropy quantifies the amount of uncertainty in a variable. Entropy is given by the following equation, where <span class="inlinemediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_30.jpg"/></span> is the number of outcomes and <span class="inlinemediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_31.jpg"/></span> is the probability of the outcome <span class="inlinemediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_32.jpg"/></span>. Common values for <span class="inlinemediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_33.jpg"/></span> are <code class="literal">2</code>, <span class="inlinemediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_33a.jpg"/></span>, and <code class="literal">10</code>. Because the log of a number less than one will be negative, the entire sum is negated to return a positive value.</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_02.jpg"/></div><p>For example, a single toss of a fair coin has only two outcomes: heads and tails. The probability that the coin will land on heads is 0.5, and the probability that it will land on tails is 0.5. The entropy of the coin toss is equal to the following:</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_03.jpg"/></div><p>That is, only one bit is required to represent the two equally probable outcomes, heads and tails. Two tosses of a fair coin can result in four possible outcomes: heads and heads, heads and tails, tails and heads, and tails and tails. The probability of each outcome is <span class="emphasis"><em>0.5 x 0.5 = 0.25</em></span>. The entropy of two tosses is equal to the following:</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_04.jpg"/></div><p>If the coin has the same face <a class="indexterm" id="id296"/>on both sides, the variable representing its outcome has 0 bits of entropy; that is, we are always certain of the outcome and the variable will never represent new information. Entropy can also be represented as a fraction of a bit. For example, an unfair coin has two different faces, but is weighted such that the faces are not equally likely to land in a toss. Assume that the probability that an unfair <a class="indexterm" id="id297"/>coin will land on heads is 0.8, and the probability that it will land on tails is 0.2. The entropy of a single toss of this coin is equal to the following:</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_05.jpg"/></div><p>The outcome of a single toss of an unfair coin can have a fraction of one bit of entropy. There are two possible outcomes of the toss, but we are not totally uncertain since one outcome is more frequent.</p><p>Let's calculate the entropy of classifying an unknown animal. If an equal number of dogs and cats comprise our animal classification training data and we do not know anything else about the animal, the entropy of the decision is equal to one. All we know is that the animal could be either a cat or a dog; like the fair coin toss, both outcomes are equally likely. Our training data, however, contains six dogs and eight cats. If we do not know anything else about the unknown animal, the entropy of the decision is given by the following:</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_06.jpg"/></div><p>Since cats are more common, we are less uncertain about the outcome. Now let's find the explanatory variable that will be most helpful in classifying the animal; that is, let's find the explanatory variable that reduces the entropy the most. We can test the <code class="literal">plays fetch</code> explanatory variable and divide the training instances into animals that play fetch and animals that don't. This produces the two following subsets:</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_07.jpg"/></div><p>Decision trees are often visualized as diagrams that are similar to flowcharts. The top box of the previous diagram is the root node; it contains all of our training instances and specifies the explanatory variable that will be tested. At the root node we have not eliminated any instances from the training set and the entropy is equal to approximately 0.985. The root <a class="indexterm" id="id298"/>node tests the <code class="literal">plays fetch</code> explanatory variable. Recall that we converted this Boolean explanatory variable to a binary-valued feature. Training <a class="indexterm" id="id299"/>instances for which <code class="literal">plays fetch</code> is equal to zero follow the edge to the root's left child, and training instances for animals that do play fetch follow the edge to the root's right child node. The left child node contains a subset of the training data with seven cats and two dogs that do not like to play fetch. The entropy at this node is given by the following:</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_08.jpg"/></div><p>The right child contains a subset with one cat and four dogs that do like to play fetch. The entropy at this node is given by the following:</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_09.jpg"/></div><p>Instead of testing the <code class="literal">plays fetch</code> explanatory variable, we could test the <code class="literal">is grumpy</code> explanatory variable. This test produces the following tree. As with the previous tree, instances that fail the test follow the left edge, and instances that pass the test follow the right edge.</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_10.jpg"/></div><p>We could also <a class="indexterm" id="id300"/>divide the instances into animals that prefer <a class="indexterm" id="id301"/>cat food and animals that don't to produce the following tree:</p><div class="mediaobject"><img alt="Selecting the questions" src="graphics/8365OS_05_11.jpg"/></div></div><div class="section" title="Information gain"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec24"/>Information gain</h2></div></div></div><p>Testing for the <a class="indexterm" id="id302"/>animals that prefer cat food resulted in one subset with six cats, zero dogs, and 0 bits of entropy and another subset with two cats, six dogs, and 0.811 bits of entropy. How can we measure which of these tests reduced our uncertainty about the classification the most? Averaging the entropies of the subsets may seem to be an appropriate measure of the reduction in entropy. In this example, the subsets produced by the cat food test have the lowest average entropy. Intuitively, this test seems to be effective, as we can use it to classify almost half of the training instances. However, selecting the test that produces the subsets with the lowest average entropy can produce a suboptimal tree. For example, imagine a test that produced one subset with two dogs and no cats and another subset with four dogs and eight cats. The entropy of the first subset is equal to the following (note that the second term is omitted because <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_35.jpg"/></span> is undefined):</p><div class="mediaobject"><img alt="Information gain" src="graphics/8365OS_05_12.jpg"/></div><p>The entropy of the second subset is equal to the following:</p><div class="mediaobject"><img alt="Information gain" src="graphics/8365OS_05_13.jpg"/></div><p>The average of these subsets' entropies is only 0.459, but the subset containing most of the instances has almost one bit of entropy. This is analogous to asking specific questions early in Twenty Questions; we could get lucky and win within the first few attempts, but it is more likely that we will squander our questions without eliminating many possibilities. Instead, we <a class="indexterm" id="id303"/>will measure the reduction in entropy using a metric called <span class="strong"><strong>information gain</strong></span>. Calculated with the following equation, information gain is the difference between the entropy of the parent node, <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_36.jpg"/></span>, and the weighted average of the children nodes' entropies. <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_37.jpg"/></span> is the set of instances, and <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_41.jpg"/></span> is the explanatory variable under test. <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_38.jpg"/></span> is the value of attribute <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_41.jpg"/></span> for instance <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_46.jpg"/></span>. <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_39.jpg"/></span> is the number of instances for which attribute <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_41.jpg"/></span> is equal to the value <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_42.jpg"/></span>. <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_40.jpg"/></span> is the entropy of the subset of instances for which the value of the explanatory variable <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_41.jpg"/></span> is <span class="inlinemediaobject"><img alt="Information gain" src="graphics/8365OS_05_42.jpg"/></span>.</p><div class="mediaobject"><img alt="Information gain" src="graphics/8365OS_05_14.jpg"/></div><p>The following table contains the information gains for all of the tests. In this case, the cat food test is still the best, as it increases the information gain the most.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Test</p>
</th><th style="text-align: left" valign="bottom">
<p>Parent's entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Child's entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Child's entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Weighted average</p>
</th><th style="text-align: left" valign="bottom">
<p>IG</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>plays fetch?</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9852</p>
</td><td style="text-align: left" valign="top">
<p>0.7642</p>
</td><td style="text-align: left" valign="top">
<p>0.7219</p>
</td><td style="text-align: left" valign="top">
<p>0.7490 * 9/14 + 0.7219 * 5/14 = 0.7491</p>
</td><td style="text-align: left" valign="top">
<p>0.2361</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>is grumpy?</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9852</p>
</td><td style="text-align: left" valign="top">
<p>0.9183</p>
</td><td style="text-align: left" valign="top">
<p>0.8113</p>
</td><td style="text-align: left" valign="top">
<p>0.9183 * 6/14 + 0.8113 * 8/14 = 0.85710.8572</p>
</td><td style="text-align: left" valign="top">
<p>0.1280</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>favorite food = cat food</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9852</p>
</td><td style="text-align: left" valign="top">
<p>0.8113</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.8113 * 8 /14 + 0.0 * 6/14 = 0.4636</p>
</td><td style="text-align: left" valign="top">
<p>0.5216</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>favorite food = dog food</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9852</p>
</td><td style="text-align: left" valign="top">
<p>0.8454</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.8454 * 11/14 + 0.0 * 3/14 = 0.6642</p>
</td><td style="text-align: left" valign="top">
<p>0.3210</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>favorite food = bacon</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.9852</p>
</td><td style="text-align: left" valign="top">
<p>0.9183</p>
</td><td style="text-align: left" valign="top">
<p>0.971</p>
</td><td style="text-align: left" valign="top">
<p>0.9183 * 9/14 + 0.9710 * 5/14 = 0.9371</p>
</td><td style="text-align: left" valign="top">
<p>0.0481</p>
</td></tr></tbody></table></div><p>Now let's add another node to the tree. One of the child nodes produced by the test is a leaf node that contains only cats. The <a class="indexterm" id="id304"/>other node still contains two cats and six dogs. We will add a test to this node. Which of the remaining explanatory <a class="indexterm" id="id305"/>variables reduces our uncertainty the most? The following table contains the information gains for all of the possible tests:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Test</p>
</th><th style="text-align: left" valign="bottom">
<p>Parent's entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Child's entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Child's entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Weighted average</p>
</th><th style="text-align: left" valign="bottom">
<p>IG</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>plays fetch?</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.8113</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1.0 * 4/8 + 0 * 4/8 = 0.5</p>
</td><td style="text-align: left" valign="top">
<p>0.3113</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>is grumpy?</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.8113</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0.0 * 4/8 + 1 * 4/8 = 0.5</p>
</td><td style="text-align: left" valign="top">
<p>0.3113</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>favorite food=dog food</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.8113</p>
</td><td style="text-align: left" valign="top">
<p>0.9710</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.9710 * 5/8 + 0.0 * 3/8 = 0.6069</p>
</td><td style="text-align: left" valign="top">
<p>0.2044</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>favorite food=bacon</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.8113</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.9710</p>
</td><td style="text-align: left" valign="top">
<p>0.0 * 3/8 + 0.9710 * 5/8 = 0.6069</p>
</td><td style="text-align: left" valign="top">
<p>0.2044</p>
</td></tr></tbody></table></div><p>All of the tests produce subsets with 0 bits of entropy, but the <code class="literal">is grumpy</code> and <code class="literal">plays fetch</code> tests produce the greatest information gain. ID3 breaks ties by selecting one of the best tests arbitrarily. We will select the <code class="literal">is grumpy</code> test, which splits its parent's eight instances into a leaf node containing four dogs and a node containing two cats and two dogs. The following is a diagram of the current tree:</p><div class="mediaobject"><img alt="Information gain" src="graphics/8365OS_05_15.jpg"/></div><p>We will now select another explanatory variable to test the child node's four instances. The remaining tests, <code class="literal">favorite food=bacon</code>, <code class="literal">favorite food=dog food</code>, and <code class="literal">plays fetch</code>, all produce a leaf <a class="indexterm" id="id306"/>node containing one dog or cat and a node <a class="indexterm" id="id307"/>containing the remaining animals. The remaining tests produce equal information gains, as shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Test</p>
</th><th style="text-align: left" valign="bottom">
<p>Parent's Entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Child Entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Child Entropy</p>
</th><th style="text-align: left" valign="bottom">
<p>Weighted Average</p>
</th><th style="text-align: left" valign="bottom">
<p>Information Gain</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>plays fetch?</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0.9183</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.688725</p>
</td><td style="text-align: left" valign="top">
<p>0.311275</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>favorite food=dog food</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0.9183</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.688725</p>
</td><td style="text-align: left" valign="top">
<p>0.311275</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>favorite food=bacon</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.9183</p>
</td><td style="text-align: left" valign="top">
<p>0.688725</p>
</td><td style="text-align: left" valign="top">
<p>0.311275</p>
</td></tr></tbody></table></div><p>We will arbitrarily select the <code class="literal">plays fetch</code> test to produce a leaf node containing one dog and a node containing two cats and a dog. Two explanatory variables remain; we can test for animals that like bacon, or we can test for animals that like dog food. Both of the tests will produce the same subsets and create a leaf node containing one dog and a leaf node containing two cats. We will arbitrarily choose to test for animals that like dog food. The following is a diagram of the completed decision tree:</p><div class="mediaobject"><img alt="Information gain" src="graphics/8365OS_05_16.jpg"/></div><p>Let's classify some <a class="indexterm" id="id308"/>animals from the following test <a class="indexterm" id="id309"/>data:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Testing instance</p>
</th><th style="text-align: left" valign="bottom">
<p>Plays fetch</p>
</th><th style="text-align: left" valign="bottom">
<p>Is grumpy</p>
</th><th style="text-align: left" valign="bottom">
<p>Favorite food</p>
</th><th style="text-align: left" valign="bottom">
<p>Species</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Bacon</p>
</td><td style="text-align: left" valign="top">
<p>Dog</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Dog Food</p>
</td><td style="text-align: left" valign="top">
<p>Dog</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Dog Food</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Bacon</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Cat food</p>
</td><td style="text-align: left" valign="top">
<p>Cat</p>
</td></tr></tbody></table></div><p>Let's classify the first animal, which likes to plays fetch, is infrequently grumpy, and loves bacon. We will follow the edge to the root node's left child since the animal's favorite food is not cat food. The animal is not grumpy, so we will follow the edge to the second-level node's left child. This is a leaf node containing only dogs; we have correctly classified this instance. To classify the third test instance as a cat, we follow the edge to the root node's left child, follow the edge to the second-level node's right child, follow the edge to the third-level node's left child, and finally follow the edge to the fourth-level node's right child.</p><p>Congratulations! You've constructed a decision tree using the ID3 algorithm. Other algorithms can be used to <a class="indexterm" id="id310"/>train decision trees. <span class="strong"><strong>C4.5</strong></span> is a <a class="indexterm" id="id311"/>modified version of ID3 that can be used with continuous explanatory variables <a class="indexterm" id="id312"/>and can accommodate missing values for features. C4.5 also <a class="indexterm" id="id313"/>can <span class="strong"><strong>prune</strong></span> trees. Pruning reduces the size of a tree by replacing branches that classify few instances with leaf nodes. Used by scikit-learn's implementation of decision trees, <span class="strong"><strong>CART</strong></span> is another learning algorithm that supports pruning. </p></div><div class="section" title="Gini impurity"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec25"/>Gini impurity</h2></div></div></div><p>In the previous <a class="indexterm" id="id314"/>section, we built a decision tree by creating nodes that produced the greatest information gain. Another common heuristic for learning decision trees is <a class="indexterm" id="id315"/>
<span class="strong"><strong>Gini impurity</strong></span>, which measures the proportions of classes in a set. Gini impurity is given by the following equation, where <span class="inlinemediaobject"><img alt="Gini impurity" src="graphics/8365OS_05_45.jpg"/></span> is the number of classes, <span class="inlinemediaobject"><img alt="Gini impurity" src="graphics/8365OS_05_44.jpg"/></span> is the subset of instances for the node, and <span class="inlinemediaobject"><img alt="Gini impurity" src="graphics/8365OS_05_43.jpg"/></span> is the probability of selecting an element of class <span class="inlinemediaobject"><img alt="Gini impurity" src="graphics/8365OS_05_32.jpg"/></span> from the node's subset:</p><div class="mediaobject"><img alt="Gini impurity" src="graphics/8365OS_05_17.jpg"/></div><p>Intuitively, Gini impurity is zero when all of the elements of the set are the same class, as the probability of selecting an element of that class is equal to one. Like entropy, Gini impurity is greatest when each class has an equal probability of being selected. The maximum value of Gini impurity depends on the number of possible classes, and it is given by the following equation:</p><div class="mediaobject"><img alt="Gini impurity" src="graphics/8365OS_05_18.jpg"/></div><p>Our problem has two classes, so the maximum value of the Gini impurity measure will be equal to one half. scikit-learn supports learning decision trees using both information gain and Gini impurity. There are no firm rules to help you decide when to use one criterion or the other; in practice, they <a class="indexterm" id="id316"/>often produce similar results. As with many decisions <a class="indexterm" id="id317"/>in machine learning, it is best to compare the performances of models trained using both options.</p></div></div>
<div class="section" title="Decision trees with scikit-learn"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec45"/>Decision trees with scikit-learn</h1></div></div></div><p>Let's use <a class="indexterm" id="id318"/>decision trees to create software that can block banner ads on web pages. This program will predict whether each of the images on a web page is an advertisement or article content. Images that are classified as being advertisements <a class="indexterm" id="id319"/>could then be hidden using Cascading Style <a class="indexterm" id="id320"/>Sheets. We will train a decision tree classifier using the <span class="emphasis"><em>Internet Advertisements Data Set</em></span> from <a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements">http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements</a>, which contains data for 3,279 images. The proportions of the classes are skewed; 459 of the images are advertisements and 2,820 are content. Decision tree learning algorithms can produce biased trees from data with unbalanced class proportions; we will evaluate a model on the unaltered data set before deciding if it is worth balancing the training data by over- or under-sampling instances. The explanatory variables are the dimensions of the image, words from the containing page's URL, words from the image's URL, the image's alt text, the image's anchor text, and a window of words surrounding the image tag. The response variable is the image's class. The explanatory variables have already been transformed into feature representations. The first three features are real numbers that encode the width, height, and aspect ratio of the images. The remaining features encode binary term frequencies for the text variables. In the following sample, we will grid search for the hyperparameter values that produce the decision tree with the greatest accuracy, and then evaluate the tree's performance on a test set:</p><div class="informalexample"><pre class="programlisting">import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.cross_validation import train_test_split
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV</pre></div><p>First we read the <code class="literal">.csv</code> file using pandas. The <code class="literal">.csv</code> does not have a header row, so we split the last column containing the response variable's values from the features using its index:</p><div class="informalexample"><pre class="programlisting">if __name__ == '__main__':
    df = pd.read_csv('data/ad.data', header=None)
    explanatory_variable_columns = set(df.columns.values)
    response_variable_column = df[len(df.columns.values)-1]
    # The last column describes the targets
    explanatory_variable_columns.remove(len(df.columns.values)-1)

    y = [1 if e == 'ad.' else 0 for e in response_variable_column]
    X = df[list(explanatory_variable_columns)]</pre></div><p>We encoded the advertisements as the positive class and the content as the negative class. More than one quarter of the instances are missing at least one of the values for the image's dimensions. These missing values are marked by whitespace and a question mark. We replaced the missing <a class="indexterm" id="id321"/>values with negative one, but we could have imputed the missing values; for instance, we could have replaced the missing height values with the average height value:</p><div class="informalexample"><pre class="programlisting">    X.replace(to_replace=' *\?', value=-1, regex=True, inplace=True)</pre></div><p>We then split the <a class="indexterm" id="id322"/>data into training and test sets:</p><div class="informalexample"><pre class="programlisting">    X_train, X_test, y_train, y_test = train_test_split(X, y)</pre></div><p>We created a pipeline and an instance of <code class="literal">DecisionTreeClassifier</code>. Then, we set the <code class="literal">criterion</code> keyword argument to <code class="literal">entropy</code> to build the tree using the information gain heuristic:</p><div class="informalexample"><pre class="programlisting">    pipeline = Pipeline([
        ('clf', DecisionTreeClassifier(criterion='entropy'))
    ])</pre></div><p>Next, we specified the hyperparameter space for the grid search:</p><div class="informalexample"><pre class="programlisting">    parameters = {
        'clf__max_depth': (150, 155, 160),
        'clf__min_samples_split': (1, 2, 3),
        'clf__min_samples_leaf': (1, 2, 3)
    }</pre></div><p>We set <code class="literal">GridSearchCV()</code> to maximize the model's F1 score:</p><div class="informalexample"><pre class="programlisting">    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')
    grid_search.fit(X_train, y_train)
    print 'Best score: %0.3f' % grid_search.best_score_
    print 'Best parameters set:'
    best_parameters = grid_search.best_estimator_.get_params()
    for param_name in sorted(parameters.keys()):
        print '\t%s: %r' % (param_name, best_parameters[param_name])

    predictions = grid_search.predict(X_test)
    print classification_report(y_test, predictions)

Fitting 3 folds for each of 27 candidates, totalling 81 fits
[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    1.7s
[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   15.0s
[Parallel(n_jobs=-1)]: Done  71 out of  81 | elapsed:   20.7s remaining:    2.9s
[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:   23.3s finished
Best score: 0.878
Best parameters set:
	clf__max_depth: 155
	clf__min_samples_leaf: 2
	clf__min_samples_split: 1
             precision    recall  f1-score   support

          0       0.97      0.99      0.98       710
          1       0.92      0.81      0.86       110

avg / total       0.96      0.96      0.96       820</pre></div><p>The classifier detected more than 80 percent of the ads in the test set, and approximately 92 percent of the images that it <a class="indexterm" id="id323"/>predicted were ads were truly ads. Overall, the performance is promising; in following sections, we will try to modify our model to <a class="indexterm" id="id324"/>improve its performance.</p><div class="section" title="Tree ensembles"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec26"/>Tree ensembles</h2></div></div></div><p>
<span class="strong"><strong>Ensemble learning</strong></span> <a class="indexterm" id="id325"/>methods combine a set of models to produce an estimator that has better predictive performance than its individual components. A <span class="strong"><strong>random forest</strong></span> is a <a class="indexterm" id="id326"/>collection of decision trees that have been trained on randomly <a class="indexterm" id="id327"/>selected subsets of the training instances and explanatory variables. Random forests usually make predictions by returning the mode or mean of the predictions of their constituent trees; scikit-learn's implementations return the mean of the trees' predictions. Random forests are less prone to overfitting than decision trees because no single tree can learn from all of the instances and explanatory variables; no single tree can memorize all of the noise in the representation.</p><p>Let's update our ad blocker's classifier to use a random forest. It is simple to replace the <code class="literal">DecisionTreeClassifier</code> using scikit-learn's API; we simply replace the object with an instance of <code class="literal">RandomForestClassifier</code>. Like the previous example, we will grid search to find the values of the hyperparameters that produce the random forest with the best predictive performance.</p><p>First, import the <code class="literal">RandomForestClassifier</code> class from the <code class="literal">ensemble</code> module:</p><div class="informalexample"><pre class="programlisting">from sklearn.ensemble import RandomForestClassifier</pre></div><p>Next, replace <a class="indexterm" id="id328"/>the <code class="literal">DecisionTreeClassifier</code> in the <code class="literal">pipeline</code> with an instance of <code class="literal">RandomForestClassifier</code> and update the hyperparameter space:</p><div class="informalexample"><pre class="programlisting">    pipeline = Pipeline([
        ('clf', RandomForestClassifier(criterion='entropy'))
    ])
    parameters = {
        'clf__n_estimators': (5, 10, 20, 50),
        'clf__max_depth': (50, 150, 250),
        'clf__min_samples_split': (1, 2, 3),
        'clf__min_samples_leaf': (1, 2, 3)
    }</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">Fitting 3 folds for each of 108 candidates, totalling 324 fits
[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    1.1s
[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   17.4s
[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:  1.0min
[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:  1.6min finished
Best score: 0.936
Best parameters set:
   clf__max_depth: 250
   clf__min_samples_leaf: 1
   clf__min_samples_split: 3
   clf__n_estimators: 20
             precision    recall  f1-score   support

          0       0.97      1.00      0.98       705
          1       0.97      0.83      0.90       115

avg / total       0.97      0.97      0.97       820</pre></div><p>Replacing the single decision tree with a random forest resulted in a significant reduction of the error rate; the random <a class="indexterm" id="id329"/>forest improves the precision and recall for ads to 0.97 and 0.83.</p></div><div class="section" title="The advantages and disadvantages of decision trees"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec27"/>The advantages and disadvantages of decision trees</h2></div></div></div><p>The compromises <a class="indexterm" id="id330"/>associated with using decision trees are different than those of the other models we discussed. Decision trees are easy to use. Unlike many learning <a class="indexterm" id="id331"/>algorithms, decision trees do not require the data to have zero mean and unit variance. While decision trees can tolerate missing values for explanatory variables, scikit-learn's current implementation cannot. Decision trees can even learn to ignore explanatory variables that are not relevant to the task.</p><p>Small decision trees can be easy to interpret and visualize with the <code class="literal">export_graphviz</code> function from scikit-learn's <code class="literal">tree</code> module. The branches of a decision tree are conjunctions of logical predicates, and they are easily visualized as flowcharts. Decision trees support multioutput tasks, and a single decision tree can be used for multiclass classification without employing a strategy like one-versus-all.</p><p>Like the other models we discussed, decision <a class="indexterm" id="id332"/>trees are <span class="strong"><strong>eager learners</strong></span>. Eager learners must build an input-independent model from the training data before they can be <a class="indexterm" id="id333"/>used to estimate the values of test instances, but can predict relatively quickly once the model has been built. In contrast, <span class="strong"><strong>lazy </strong></span>
<a class="indexterm" id="id334"/>
<span class="strong"><strong>learners</strong></span> such as the k-nearest neighbors algorithm defer all generalization until <a class="indexterm" id="id335"/>they must make a prediction. Lazy learners do not spend time training, but often predict slowly compared to eager learners.</p><p>Decision trees are more prone to overfitting than many of the models we discussed, as their learning algorithms can produce large, complicated decision trees that perfectly model every training instance but fail to generalize the real relationship. Several techniques can mitigate over-fitting in <a class="indexterm" id="id336"/>decision trees. <span class="strong"><strong>Pruning</strong></span> is a common strategy <a class="indexterm" id="id337"/>that removes some of the tallest nodes and leaves of a decision tree, but it is not currently implemented in scikit-learn. However, similar effects can be achieved by setting a maximum depth for the tree or by creating child nodes only when the number of training instances they will contain exceeds a threshold. The <code class="literal">DecisionTreeClassifier</code> and <code class="literal">DecisionTreeRegressor</code> classes provide keyword arguments to set these constraints. Creating a random forest can also reduce over-fitting. </p><p>Efficient decision tree learning <a class="indexterm" id="id338"/>algorithms like ID3 are <span class="strong"><strong>greedy</strong></span>. They learn <a class="indexterm" id="id339"/>efficiently by making locally optimal decisions, but are not guaranteed to produce the globally optimal tree. ID3 constructs a tree by selecting a sequence of explanatory variables to test. Each explanatory variable is selected because it reduces the uncertainty in the node more than the other variables. It is possible, however, that locally suboptimal tests are required in order to find the globally optimal tree.</p><p>In our toy examples, the size of the tree did not matter since we retained all of nodes. In a real application, however, the tree's growth could be limited by pruning or similar mechanisms. Pruning trees with different shapes can produce trees with different performances. In practice, locally optimal decisions that are guided by the information gain or Gini impurity heuristics often result in an acceptable decision trees.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec46"/>Summary</h1></div></div></div><p>In this chapter we learned about simple nonlinear models for classification and regression called decision trees. Like the parlor game Twenty Questions, decision trees are composed of sequences of questions that examine a test instance. The branches of a decision tree terminate in leaves that specify the predicted value of the response variable. We discussed how to train decision trees using the ID3 algorithm, which recursively splits the training instances into subsets that reduce our uncertainty about the value of the response variable. We also discussed ensemble learning methods, which combine the predictions from a set of models to produce an estimator with better predictive performance. Finally, we used random forests to predict whether or not an image on a web page is a banner advertisement. In the next chapter, we will introduce our first unsupervised learning task: clustering.</p></div></body></html>
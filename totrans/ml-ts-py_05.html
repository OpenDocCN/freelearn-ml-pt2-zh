<html><head></head><body>
  <div id="_idContainer261">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-97" class="chapterTitle">Unsupervised Methods for Time-Series</h1>
    <p class="normal">We've discussed forecasting in the previous chapter, and we'll talk about predictions from time-series in the next chapter. The performance of these predictive models is easily undermined by major changes in the data. Recognizing these changes is the domain of unsupervised learning.</p>
    <p class="normal">In this chapter, we'll describe the specific challenges of unsupervised learning with time-series data. At the core of unsupervised learning is the extraction of structure from time-series, most importantly recognizing similarities between subsequences. This is the essence of anomaly detection (also: outlier detection), where we want to identify sequences that are notably different from the rest of the series. </p>
    <p class="normal">Time-series data is usually non-stationary, non-linear, and dynamically evolving. An important challenge of working with time-series is recognizing the changes in the underlying processes. This is called change point detection (CPD) or drift detection. Data changes over time, and it is crucial to recognize how much it changes. This is worth diving into, because the existence of change points and anomalous points are common problems with real-world applications.</p>
    <p class="normal">In this chapter, we'll concentrate on anomaly detection and CPD, while in <em class="chapterRef">Chapter 8</em>, <em class="italic">Online Learning for Time-Series</em>, we'll look at drift detection in more detail. We'll start with an overview and definitions, before looking at industry practices at big tech companies.</p>
    <p class="normal">We're going to cover the following topics:</p>
    <ul>
      <li class="bullet">Unsupervised methods for time-series</li>
      <li class="bullet">Anomaly detection</li>
      <li class="bullet">Change detection</li>
      <li class="bullet">Clustering</li>
      <li class="bullet">Python practice</li>
    </ul>
    <p class="normal">We'll start with a general introduction to unsupervised learning with time-series.</p>
    <h1 id="_idParaDest-98" class="title">Unsupervised methods for time-series</h1>
    <p class="normal">The main<a id="_idIndexMarker501"/> difference<a id="_idIndexMarker502"/> between time-series and other types of data is the dependence on the temporal axis; a correlation structure at one point <em class="italic">t</em><sub class="" style="font-style: italic;">1</sub> could have very different information to the same structure at point <em class="italic">t</em><sub class="" style="font-style: italic;">2</sub>. Time-series often contain lots of noise and have high dimensionality.</p>
    <p class="normal">To reduce the noise and decrease the dimensionality, dimensionality reduction, wavelet analysis, or signal processing techniques such as filtering, for example, Fourier decomposition, can be applied. This is often at the basis of anomaly detection or CPD, the techniques we are discussing in this chapter. We'll discuss drift detection in <em class="chapterRef">Chapter 8</em>, <em class="italic">Online Methods for Time-Series</em>.</p>
    <p class="normal">We'll be talking in detail about anomalies and change points, and it might be helpful to see an illustration of how anomalies and change points can look like. In the article "<em class="italic">Social tipping dynamics for stabilizing Earth's climate by 2050</em>" by Ilona Otto and others, they analyzed whether and how a change in greenhouse gas emissions based on social dynamics could transform countries into carbon-neutral societies. They project global warming according to different scenarios in the following plot with a tipping point (another word for change point) around 2010 and the early 2020s (chart adapted from their article):</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.1: Possible change points based on greenhouse gas emissions</p>
    <p class="normal">Global <a id="_idIndexMarker503"/>temperatures have cycled between periods of<a id="_idIndexMarker504"/> glaciations and warm periods, each cycle taking somewhere in the order of tens of thousands of years. For the last few thousand years, the climate was cooling leading to widespread speculations as late as the 1970s around a cooling trend that could lead to another ice age. However, data indicates that since the beginning of industrialization, largely driven by the burning of fossil fuels, global temperature has increased about a full 1°C. </p>
    <p class="normal">Therefore, the beginning of the industrial period could be considered a change point for global temperatures as the following graph illustrates (source: Wikimedia Commons):</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_02.png" alt="ile:Temperature reconstruction last two millennia.svg"/></figure>
    <p class="packt_figref">Figure 6.2: Change point in global temperatures: the beginning of the industrial age</p>
    <p class="normal">In the <a id="_idIndexMarker505"/>graph<a id="_idIndexMarker506"/> above, the change point at the onset of the industrial revolution precedes the anomaly of modern temperature rises.</p>
    <p class="normal">For a human, it's relatively easy to point out change points or anomalies, especially in hindsight when the full historical data is available. For automatic detection, there are lots of different ways to find salient points. In a practical context, it's important to carefully balance detection rates and false positives.</p>
    <h1 id="_idParaDest-99" class="title">Anomaly detection</h1>
    <p class="normal">In anomaly<a id="_idIndexMarker507"/> detection, we want to identify sequences that are notably different from the rest of the series. Anomalies or outliers can sometimes be the result of measurement error or noise, but they could indicate changes to behavior or aberrant behavior in the system under observation, which could require urgent action. </p>
    <p class="normal">An important application of anomaly detection is automatic real-time monitoring of potentially complex, high-dimensional datasets.</p>
    <p class="normal">It's time for an attempt at a definition (after D.M. Hawkins, 1980, "<em class="italic">Identification of Outliers</em>"):</p>
    <p class="Information-Box--PACKT-"><strong class="keyword">Definition</strong>: An outlier is a data point that deviates so significantly from other observations that it could have been generated by a different mechanism.</p>
    <p class="normal">Let's start with a plot, so we can see how an anomaly might look graphically. This will also provide us context for our discussion. </p>
    <p class="normal">Anomaly detection methods can be distinguished between univariate and multivariate methods. Parametric anomaly detection methods, by the choice of their distribution parameters (for example, the arithmetic mean), place an assumption on the underlying distribution – often<a id="_idIndexMarker508"/> the Gaussian distribution. These methods flag outliers, points that deviate from the model's assumptions. </p>
    <p class="normal">In the simplest case, we can define an outlier as follows as the z-score of the observation <em class="italic">x</em><sub class="" style="font-style: italic;">i</sub> with respect to the distribution parameters:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_001.png" alt="" style="height: 3em;"/></figure>
    <p class="normal">The z-score measures the distance of each point from the moving average or sample mean, <img src="../Images/B17577_06_002.png" alt="" style="height: 1em;"/>, in units of the moving or sample standard deviation <img src="../Images/B17577_06_003.png" alt="" style="height: 1em;"/>. It is positive for values that lie above the mean, and negative for those that lie below the mean.</p>
    <p class="normal">In this formula, <img src="../Images/B17577_06_004.png" alt="" style="height: 1em;"/> and <img src="../Images/B17577_06_005.png" alt="" style="height: 1em;"/> are the estimated mean and standard deviation of the time-series and <em class="italic">x</em> is the point that we want to test. Finally, <img src="../Images/B17577_06_006.png" alt="" style="height: 0.24em;"/> is a threshold dependent on the confidence interval that we are interested in – often, 2 or 1.96 are chosen for this, corresponding to a confidence interval of 95%. In this way, outliers are points that occur 5% or less of the time. </p>
    <p class="normal">The z-score makes an assumption of normal-distributed data; however, the mean and standard deviation used in the outlier formula above can be replaced by other measures that do away with this assumption. Measures such as the median or the interquartile range (as<a id="_idIndexMarker509"/> discussed in <em class="chapterRef">Chapter 2</em>, <em class="italic">Time-Series Analysis with Python</em>) are more robust to the distribution. </p>
    <p class="normal">The Hampel <a id="_idIndexMarker510"/>filter (also: Hampel identifier) is a special case for this, where the median and the <strong class="keyword">median absolute deviation</strong> (<strong class="keyword">MAD</strong>) are employed:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_007.png" alt="" style="height: 2.5em;"/></figure>
    <p class="normal">In this equation, the sample mean is replaced by the (sample) median and the standard deviation by the MAD, which is defined as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_008.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">The median, in turn, is the middle number in a sorted list of numbers.</p>
    <p class="normal">In the Hampel filter, each observation, x, will be compared to the median. In the case of the normal distribution, the Hampel filter is equivalent to the z-score, and epsilon can be chosen the same way as for the z-score.</p>
    <p class="normal">In the multivariate case, the outlier function can be expressed as the distance (or, inversely: similarity) to a point in the model distribution such as the center of gravity, the mean. For example, we could take the covariance of the new observation to the mean.</p>
    <p class="normal">While these former methods are restricted to low-dimensional or univariate time-series, distance-based methods can handle much larger spaces. Distance-based anomaly detection methods effectively cluster points into different groups, where small groups will be labeled outliers. In these methods, the choice of distance measure is crucial. </p>
    <p class="normal">Some of the challenges to detect anomalies in time-series are:</p>
    <ul>
      <li class="bullet">Lack of a definition of outliers</li>
      <li class="bullet">Noise within the input data</li>
      <li class="bullet">Complexity of time-series</li>
      <li class="bullet">High imbalance</li>
    </ul>
    <p class="normal">We often <a id="_idIndexMarker511"/>don't really know what outliers look like. In practical settings, we often don't have labels for the outliers – rendering benchmarking against real cases impossible. As for the complexity, time-series change over time, they are often non-stationary and the dependence between variables can be non-linear. Finally, we typically have a lot more normal observations than outliers.</p>
    <p class="normal">A requirement for deploying anomaly detection models as services at scale is that they should be able to detect anomalies in real time.</p>
    <p class="normal">Applications for anomaly detection encompass the ones in this diagram:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_03.png" alt="anomaly_detection_application.png"/></figure>
    <p class="packt_figref">Figure 6.3: Applications for anomaly detection</p>
    <p class="normal">Some examples <a id="_idIndexMarker512"/>could be fraud detection with payments, network security (cyber intrusion), medical monitoring, or sensor networks. In medical monitoring, we want to work with real-time monitoring of physiological variables including heart rate, electroencephalogram, and electrocardiogram for alerting in case of acute emergencies. Anomaly alerts in sensor networks can help prevent cases of industrial damage, for example.</p>
    <p class="normal">This diagram illustrates the main types of anomaly detection methods depending on the available knowledge of the dataset:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_04.png" alt="Anomaly_Detection.png"/></figure>
    <p class="packt_figref">Figure 6.4: Anomaly detection methods depending on available knowledge</p>
    <p class="normal">The earliest examples for anomaly detection consist of rule-based systems. This works when the patterns can be clearly defined. When we have an annotated set of anomalies, we can apply supervised or semi-supervised methods such as classifiers or regression models. The most common use case, however, is when anomalies are not annotated and we need unsupervised approaches to detect anomalous points or sections based on densities or distributions.</p>
    <p class="normal">It's instructive to see what the big technology companies, Alphabet (Google), Amazon, Facebook, Apple, and<a id="_idIndexMarker513"/> Microsoft (GAFAM), do for anomaly detection. Let's go over each in turn and see how they handle anomaly detection.</p>
    <h2 id="_idParaDest-100" class="title">Microsoft</h2>
    <p class="normal">In <a id="_idIndexMarker514"/>the<a id="_idIndexMarker515"/> paper "<em class="italic">Time-Series Anomaly Detection Service at Microsoft</em>" (Hansheng Ren and others, 2019) a time-series service is presented that's deployed for anomaly detection for production data at Microsoft. Its core is a <strong class="keyword">Spectral Residual</strong> (<strong class="keyword">SR</strong>) and Convolutional Neural Network (CNN) that's applied to unsupervised online anomaly detection in univariate time-series.</p>
    <p class="normal">They borrowed the SR method from the concept of the saliency map in vision. Saliency maps highlight the points in images, which stand out to human observers. The algorithm performs the Fourier Transform of the data, then it applies the SR of the log amplitude of the transformed signal, and finally projects the spectral data back to the temporal domain with the Inverse Fourier Transform. </p>
    <p class="normal">As an extension, they train a CNN based on artificial data using the SR method. They show benchmarks on publicly available data that support their claim that their method is the state of the art for anomaly detection. </p>
    <p class="normal">They further claim that their detection accuracy (F1-score) is improved by more than 20% on Microsoft production data. You can find the<a id="_idIndexMarker516"/> basic implementation in the alibi-detect library (the "<em class="italic">Spectral Residual</em>" method).</p>
    <h2 id="_idParaDest-101" class="title">Google</h2>
    <p class="normal">In<a id="_idIndexMarker517"/> their<a id="_idIndexMarker518"/> frequently asked questions on Google Analytics (<a href="https://support.google.com/analytics/answer/7507748?hl=en"><span class="url">https://support.google.com/analytics/answer/7507748?hl=en</span></a>), Google refer to a Bayesian state space-time-series model ("<em class="italic">Predicting the Present with Bayesian Structural Time-Series</em>" by Steven L. Scott and Hal Varian, 2013) for change point and anomaly detection. </p>
    <p class="normal">Google released an R package with more specific time-series functionality – CausalImpact. A paper describing the research behind the package was published in 2015 ("<em class="italic">Inferring causal impact using Bayesian structural time-series models</em>" by Kay H. Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L. Scott). CausalImpact estimates the causal effect of interventions based on a structural Bayesian time-series model. This has been ported to Python (the pycausalimpact library).We are going to experiment with causal impact analysis using Bayesian Structural Time-Series (BSTS) in <em class="chapterRef">Chapter 9</em>, <em class="italic">Probabilistic Models for Time-Series</em>.</p>
    <h2 id="_idParaDest-102" class="title">Amazon</h2>
    <p class="normal">Amazon, providing <a id="_idIndexMarker519"/>machine <a id="_idIndexMarker520"/>learning solutions at scale through their <strong class="keyword">Amazon Web Services</strong> (<strong class="keyword">AWS</strong>) platform, have anomaly detection as part of their resource and application monitoring solution, CloudWatch. It's unclear how their solution works, but Corey Quinn, an economist, theorized in a tweet that their solution is exponential smoothing. As part of that it is likely that they apply seasonal decomposition as a first step of their algorithm.</p>
    <p class="normal">They also have a second service for anomaly detection: Amazon Lookout for Metrics. It's also unclear how this works under the hood. The service is geared toward monitoring business indicators, and – according to the documentation – is used internally within Amazon for large-scale monitoring. In the service, users can select fields from data sources with different breakdowns, for example, by selecting database columns <code class="Code-In-Text--PACKT-">page_views</code> and <code class="Code-In-Text--PACKT-">device_type</code>, users could look for abnormal changes in page views for every device type separately.</p>
    <p class="normal">As for Amazon research in anomaly detection, they clinched the top three spots, out of 117 submissions in a challenge at the Workshop on the <strong class="keyword">Detection and Classification of Acoustic Scenes and Events</strong> (<strong class="keyword">DCASE</strong> 2020). This is a challenge comparable to time-series anomaly <a id="_idIndexMarker521"/>detection. They<a id="_idIndexMarker522"/> won the best paper award with "<em class="italic">Group Masked Autoencoder Based Density Estimator for Audio Anomaly Detection</em>" (Ritwik Giri and others, 2020).</p>
    <h2 id="_idParaDest-103" class="title">Facebook</h2>
    <p class="normal">Facebook's <a id="_idIndexMarker523"/>Core <a id="_idIndexMarker524"/>Data Science team open-sourced their implementation for time-series forecasting and anomaly detection on GitHub. Their library is called Prophet. In their blog post announcing the library in 2017, they state that Prophet had been a key piece in Facebook's ability to create forecasts at scale and trusted as an important piece of information in decision-making. </p>
    <p class="normal">The paper "<em class="italic">Forecasting at scale</em>" by Sean J Taylor and Benjamin Letham (2017) describes their setup at Facebook that includes an analyst in the loop and can automatically flag forecasts for manual review and adjustment. The anomaly detection builds on the uncertainty <a id="_idIndexMarker525"/>around the forecast from a Generalized Additive Model (GAM). </p>
    <p class="normal">Prophet has been compared in benchmarks to other probabilistic and non-probabilistic models, and has rarely shown outstanding success. The Elo ratings at microprediction.com indicate that Prophet performs worse at univariate forecasts than exponential moving averages and many other standard methods.</p>
    <h2 id="_idParaDest-104" class="title">Twitter</h2>
    <p class="normal">Twitter<a id="_idIndexMarker526"/> released <a id="_idIndexMarker527"/>an R package as well, called Anomaly<a id="_idIndexMarker528"/>Detection. Their method is based on the Generalized Extreme Studentized Deviate (ESD) test for detecting anomalies in univariate approximately normally distributed time-series. Their method was published in 2017 ("<em class="italic">Automatic Anomaly Detection in the Cloud Via Statistical Learning</em>", Jordan Hochenbaum, Owen Vallis, Arun Kejariwal). </p>
    <p class="normal">For their adaption of the ESD test, the Seasonal Hybrid ESD, they included a Seasonal-Trend decomposition using LOESS (STL) before applying a threshold on the z-score (as mentioned above) or – for datasets with a high number of anomalies – thresholding based on the <a id="_idIndexMarker529"/>median and MAD. The Twitter model has been ported to <a id="_idIndexMarker530"/>Python (the sesd library).</p>
    <h2 id="_idParaDest-105" class="title">Implementations</h2>
    <p class="normal">We'll end with <a id="_idIndexMarker531"/>an overview of readily available implementations for anomaly detection in Python. Lots of implementations are available. Their use cases are very similar, however, the implementations and the user bases are widely different.</p>
    <p class="normal">Here's a list ordered by the number of stars on GitHub (as per May 2021):</p>
    <table id="table001-4" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Library</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Implementations</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Maintainer</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Stars</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Prophet</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Uncertainty interval around the estimated trend component from the forecast</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Facebook Core Research</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">12.7k</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">PyOD</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">30 detection algorithms for multivariate time-series—from classical LOF (SIGMOD 2000) to COPOD (ICDM 2020)</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Yue Zhao and others</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">4.5k</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">alibi-detect</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Many anomaly detection algorithms—specific to time-series there are Likelihood Ratios, Prophet, Spectral Residual, Seq2Seq, Model distillation</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Seldon Technologies Ltd</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">683</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Scikit-Lego</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Reconstruction through PCA/UMAP</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Vincent D. Warmerdam and others</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">499</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Luminaire</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Luminaire Window Density Model</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Zillow</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">371</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Donut</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Variational Auto-Encoder for Seasonal KPIs</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Tsinghua Netman Lab</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">327</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">rrcf</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Robust Random Cut Forest algorithm for anomaly detection on streams</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Real-time water systems lab, University of Michigan</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">302</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">banpei</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Hotelling's theory</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Hirofumi Tsuruta</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">245</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">STUMPY</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Matrix Profile algorithms for uni- and multivariate time-series such as STUMP, FLUSS, and FLOSS (also compare matrixprofile-ts)</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">TD Ameritrade</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">169</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">PySAD</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">More than a dozen algorithms for streaming outlier anomaly detection</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Selim Yilmaz, Selim and Suleyman Kozat</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">98</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Figure 6.5: Anomaly detection methods in Python</p>
    <p class="normal">Each of these <a id="_idIndexMarker532"/>methods has their own background and formal underpinning; however, it's out of scope in this chapter to describe all of them.</p>
    <p class="normal">This chart shows the star history (from star-history.t9t.io) of the top three repositories:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_05.png" alt="anomaly_detection-star_history.png"/></figure>
    <p class="packt_figref">Figure 6.6: Star history of Prophet, PyOD, and alibi-detect</p>
    <p class="normal">Both Prophet and PyOD have been seeing a continuous rise in popularity (GitHub stars).</p>
    <p class="normal">Many deep learning algorithms have been applied more recently to anomaly detection, both with univariate and multivariate time-series. </p>
    <p class="normal">What's particularly interesting about deep learning models is that the applications can be much broader: anomaly detection in video surveillance in closed-circuit<a id="_idIndexMarker533"/> television. We'll go more into detail of deep learning architectures in <em class="chapterRef">Chapter 10</em>, <em class="italic">Deep Learning for Time-Series</em>.</p>
    <h1 id="_idParaDest-106" class="title">Change point detection</h1>
    <p class="normal">A common<a id="_idIndexMarker534"/> problem with time-series is changes in the behavior of the observed system. Generally speaking, a change point signals an abrupt and significant transition between states in the process generating the series. For example, the trend can suddenly change, and a change point can signal where the trend of the series changes. This is well known under the guise of technical chart pattern analysis in trading. </p>
    <p class="normal">This list captures some applications for <strong class="keyword">Change point detection</strong> (<strong class="keyword">CPD</strong>):</p>
    <ul>
      <li class="bullet">Speech recognition: Detection of word and sentence boundaries</li>
      <li class="bullet">Image analysis: Surveillance on video footage of closed-circuit television</li>
      <li class="bullet">Fitness: Segmenting human activities over time based on data from motion sensors from smart devices such as watches or phones</li>
      <li class="bullet">Finance: Identifying changes to trend patterns that could indicate changes from bear to bull markets, or the other way around</li>
    </ul>
    <p class="normal">As an example for the importance of CPD, consider the stock market. Time-series data that describes the evolution of a market, such as stock prices, follows trends – it either rises, falls, or doesn't change significantly (stagnation). </p>
    <p class="normal">When a stock rises, the investor wants to buy the stock. Otherwise, when the stock is falling, the investors doesn't want to keep the stock, but rather to sell it. Not changing the position will cause a loss of book value – in the best case, this will cause a problem with liquidity. </p>
    <p class="normal">For investors, it is therefore key to know, when the market changes from rising to falling or the other way around. Recognizing these changes can make the difference between winning or losing.</p>
    <p class="normal">In forecasting, special events like Black Friday, Christmas, an election, a press release, or changes in regulation can cause short-term (perhaps then classed as an anomaly) or long-term change to the trend or to the level of the series. This will inevitably lead to strange predictions from traditional models.</p>
    <p class="normal">A particularly interesting challenge with CPD algorithms is detecting these inflection points in real time. This means detecting a change point as soon as it arrives (or, at the very least, before the next change point occurs).</p>
    <p class="normal">We can distinguish <a id="_idIndexMarker535"/>online and offline methods for CPD, where online refers to processing on the fly, dealing with each new data point as it becomes available. On the other hand, offline algorithms can work on the whole time-series at once. We'll deal more with online processing in <em class="chapterRef">Chapter 8</em>, <em class="italic">Online Learning for Time-Series</em>.</p>
    <p class="normal">CPD is related to segmentation, edge detection, event detection, and anomaly detection, and similar techniques can be applied to all these applications. CPD can be viewed as very much like anomaly detection, since one way to identify change points is by anomaly scores from an anomaly detection algorithm. </p>
    <p class="normal">From this perspective, change points are identical to highly anomalous points, and anything above a certain threshold corresponds to a change. In the same way as anomaly detection, CPD can be defined as the problem of hypothesis testing between two alternatives, the null hypothesis being "<em class="italic">no change occurs</em>," and the alternative hypothesis of "<em class="italic">a change occurs</em>."</p>
    <p class="normal">CPD algorithms are composed of three components: cost functions, search methods, and constraints. We'll go through these in turn. Cost functions are distance functions that can be applied to a subsection of the time-series (multivariate or univariate). </p>
    <p class="normal">An example for a cost function is <strong class="keyword">least absolute deviation</strong> (<strong class="keyword">LAD</strong>), which is an estimator of a shift in the central point (mean, median, and mode) of a distribution defined as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_009.png" alt="" style="height: 2.8em;"/></figure>
    <p class="normal">In this definition <em class="italic">l</em> is an index to a subsection in the time-series <em class="italic">x</em>, and <img src="../Images/B17577_06_010.png" alt="" style="height: 1em;"/> is the central point of <em class="italic">x</em>.</p>
    <p class="normal">The search function then iterates over the time-series to detect change points. This can be done approximately, such as in window-based detection, bottom-up methods, or binary <a id="_idIndexMarker536"/>segmentation, or it can be exhaustive as in the case of dynamic programming or <strong class="keyword">Pruned Exact Linear Time</strong> (<strong class="keyword">Pelt</strong>). </p>
    <p class="normal">Pelt (Gachomo Dorcas Wambui and others, 2015) relies on pruning heuristics, and has a computational<a id="_idIndexMarker537"/> cost that is linear to the number of points of the time-series, <img src="../Images/B17577_06_011.png" alt="" style="height: 1em;"/>. Dynamic programming methods have a much higher computational cost of <img src="../Images/B17577_06_012.png" alt="" style="height: 1em;"/>, where n is the maximum number of expected change points. </p>
    <p class="normal">Finally, the constraint can come into play as a penalty in the search algorithm. This penalty term can encode a cost budget or knowledge of the number of change points that we would expect to find.</p>
    <p class="normal">It is notoriously difficult to evaluate the performance of CPD algorithms, because of the lack of benchmark datasets. Only very recently (2020) Gerrit van den Burg and Christopher Williams from the Alan Turing Institute and the University of Edinburgh published a benchmark consisting of 37 time-series from sources such as the World Bank, EuroStat, U.S. Census Bureau, GapMinder, and Wikipedia. Their benchmark is available on GitHub, and they mention change point annotations for datasets centered around the financial crisis of 2007-08, legislation on seat belts in the U.K., the Montreal Protocol regulating chlorofluorocarbon emissions, or the regulation of automated phone calls in the U.S. </p>
    <p class="normal">In the same paper ("<em class="italic">An Evaluation of Change Point Detection Algorithm</em>"), the authors evaluated a whole range of methods for CPD. They note that their "zero" baseline method, which assumes no change points all, outperforms many of the other methods, according to F1-measure and a cluster overlap measure based on the Jaccard index. This is because of the small proportion of change points in the dataset, and the high number of false positives the methods return. They concluded that binary segmentation and Bayesian<a id="_idIndexMarker538"/> online CPD are among the best methods across the time-series.</p>
    <p class="normal">Binary segmentation ("<em class="italic">On Tests for Detecting Change in Mean</em>" by Ashish Sen and Muni S. Srivastava, 1975) falls into the category of window-based CPD. Binary segmentation is a greedy algorithm that minimizes the sum of costs the most as defined like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_013.png" alt="" style="height: 1.6em;"/></figure>
    <p class="normal"><img src="../Images/B17577_06_014.png" alt="" style="height: 0.42em;"/> is the found change point and <em class="italic">c() </em>is a cost function similar to LAD, which we saw earlier in this section. The general idea is that when two subsequences are highly dissimilar, this indicates a change point.</p>
    <p class="normal">Binary segmentation is sequential in the sense that the change point is detected first on the complete time-series, then again on the two sub-sequences before and after the change point. This explains its low complexity of <img src="../Images/B17577_06_015.png" alt="" style="height: 1em;"/>, where T is the length of the time-series. This computational cost makes it scalable to larger datasets. </p>
    <p class="normal">This table presents an overview of methods for CPD:</p>
    <table id="table002-3" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Library</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Implementations</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Maintainer</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Greykite</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">CPD via adaptive lasso</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">LinkedIn</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">ruptures</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Offline CPD: binary segmentation, dynamic programming, Pelt, window-based</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Charles Truong</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Bayesian Changepoint Detection</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Bayesian CPD</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Johannes Kulick</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">banpei</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Singular spectrum transformation</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Hirofumi Tsuruta</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">changepy</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Pelt algorithm</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Rui Gil</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">onlineRPCA</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Online Moving Window Robust Principal Component Analysis</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Wei Xiao</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Figure 6.7: CPD methods in Python</p>
    <p class="normal">We've omitted <a id="_idIndexMarker539"/>Facebook's Prophet library since it's not a dedicated CPD package.</p>
    <p class="normal">The chart below illustrates the popularity of CPD methods over time.</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_06.png" alt="change_point_detection-star_history.png"/></figure>
    <p class="packt_figref">Figure 6.8: Star history of CPD methods</p>
    <p class="normal">LinkedIn's Greykite has been seeing a meteoric rise in GitHub stars since its release. Also ruptures has <a id="_idIndexMarker540"/>seen a huge increase in popularity.</p>
    <h1 id="_idParaDest-107" class="title">Clustering</h1>
    <p class="normal">Cluster analysis <a id="_idIndexMarker541"/>or clustering is the process of finding meaningful groups (clusters) of points or objects in a dataset based on their similarity. As the result of this unsupervised data mining technique, we want points in each cluster to be similar to each other, while being different to points in other clusters. </p>
    <p class="normal">Clustering of time-series is challenging because each data point is a period of time (an ordered sequence). It has found application in diverse areas to discover patterns that empower time-series analysis, extracting insights from complex datasets.</p>
    <p class="normal">We are not going to get into details on time-series clustering, but the following table gives an overview of Python libraries for time-series clustering:</p>
    <table id="table003-1" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Library</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Implementations</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Maintainer</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Stars</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">tslearn</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Time-Series K-Means, K-Shape clustering, KernelKMeans</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Romain Tavenard</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">1.7k</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">river</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">DBStream, Time-Series K-Means, CluStream, DenStream, STREAMKMeans</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Albert Bifet and others</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">1.7k</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Figure 6.9: Clustering methods for time-series in Python</p>
    <p class="normal">You can see the GitHub stars for the top implementation over history here:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_07.png" alt="clustering-star_history.png"/></figure>
    <p class="packt_figref">Figure 6.10: Star history of tslearn and river</p>
    <p class="normal">Both libraries<a id="_idIndexMarker542"/> are going strong. We'll revisit river in <em class="chapterRef">Chapter 8</em>, <em class="italic">Online Learning for Time-Series</em>.</p>
    <h1 id="_idParaDest-108" class="title">Python practice</h1>
    <p class="normal">Let's do first an <a id="_idIndexMarker543"/>example of anomaly detection, then another for CPD. Let's first look at the needed libraries in the next section.</p>
    <h2 id="_idParaDest-109" class="title">Requirements</h2>
    <p class="normal">In this chapter, we'll <a id="_idIndexMarker544"/>use several libraries, which we can quickly install from the terminal (or similarly from the anaconda navigator):</p>
    <pre class="programlisting con"><code class="hljs-con">pip install ruptures alibi_detect
</code></pre>
    <p class="normal">We'll execute the commands from the Python (or IPython) terminal, but equally we could execute them from a Jupyter notebook (or a different environment).</p>
    <p class="normal">We should be ready now to get into the woods with implementing unsupervised time-series algorithms in Python.</p>
    <h2 id="_idParaDest-110" class="title">Anomaly detection</h2>
    <p class="normal">alibi-detect<a id="_idIndexMarker545"/> comes with several benchmark datasets<a id="_idIndexMarker546"/> for time-series anomaly detection:</p>
    <ul>
      <li class="bullet">fetch_ecg—ECG dataset from the BIDMC Congestive Heart Failure Database</li>
      <li class="bullet">fetch_nab—Numenta Anomaly Benchmark</li>
      <li class="bullet">fetch_kdd—KDD Cup '99 dataset of computer network intrusions</li>
    </ul>
    <p class="normal">The last of these is loaded through scikit-learn.</p>
    <p class="normal">Let's load the time-series of computer network intrusions (KDD99):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> alibi_detect.datasets <span class="hljs-keyword">import</span> fetch_kdd
intrusions = fetch_kdd()
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">intrusions</code> is a dictionary, where the <code class="Code-In-Text--PACKT-">data</code> key returns a matrix of 494021x18. The 18 dimensions of the time-series are the continuous features of the dataset, mostly error rates and counts:</p>
    <pre class="programlisting con"><code class="hljs-con">intrusions['feature_names']
['srv_count',
 'serror_rate',
 'srv_serror_rate',
 'rerror_rate',
 'srv_rerror_rate',
 'same_srv_rate',
 'diff_srv_rate',
 'srv_diff_host_rate',
 'dst_host_count',
 'dst_host_srv_count',
 'dst_host_same_srv_rate',
 'dst_host_diff_srv_rate',
 'dst_host_same_src_port_rate',
 'dst_host_srv_diff_host_rate',
 'dst_host_serror_rate',
 'dst_host_srv_serror_rate',
 'dst_host_rerror_rate',
 'dst_host_srv_rerror_rate']
</code></pre>
    <p class="normal">Another key, <code class="Code-In-Text--PACKT-">target</code> contains the annotations of anomalies. </p>
    <p class="normal">Since we have the annotations ready we could train a classifier, however, we'll stick to unsupervised methods. Further, since the Spectral Method that we'll use is for univariate data and we'll only take a single dimension out of our multivariate dataset, we'll completely ignore the annotations.</p>
    <p class="normal">Here's a quick <a id="_idIndexMarker547"/>plot of our time-series (we'll choose – arbitrarily – the <a id="_idIndexMarker548"/>first dimension of our dataset): </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
pd.Series(intrusions[<span class="hljs-string">'data'</span>][:, <span class="hljs-number">0</span>]).plot()
</code></pre>
    <p class="normal">This is the plot:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_08.png" alt="intrusions_dim1.png"/></figure>
    <p class="packt_figref">Figure 6.11: Time-series chart</p>
    <p class="normal">We'll load and run the SpectralResidual model that implements the method proposed by Microsoft:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> alibi_detect.od <span class="hljs-keyword">import</span> SpectralResidual
od = SpectralResidual(
    threshold=<span class="hljs-number">1.</span>,
    window_amp=<span class="hljs-number">20</span>,
    window_local=<span class="hljs-number">20</span>,
    n_est_points=<span class="hljs-number">10</span>,
    n_grad_points=<span class="hljs-number">5</span>
)
</code></pre>
    <p class="normal">We can then <a id="_idIndexMarker549"/>get the anomaly scores for each point in our<a id="_idIndexMarker550"/> time-series:</p>
    <pre class="programlisting code"><code class="hljs-code">scores = od.score(intrusions[<span class="hljs-string">'data'</span>][:, <span class="hljs-number">0</span>])
</code></pre>
    <p class="normal">Let's plot the scores imposed on top of our time-series!</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib
ax = pd.Series(intrusions[<span class="hljs-string">'data'</span>][:, <span class="hljs-number">0</span>], name=<span class="hljs-string">'data'</span>).plot(legend=<span class="hljs-literal">False</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
ax2 = ax.twinx()
ax = pd.Series(scores, name=<span class="hljs-string">'scores'</span>).plot(ax=ax2, legend=<span class="hljs-literal">False</span>, color=<span class="hljs-string">"r"</span>, marker=matplotlib.markers.CARETDOWNBASE)
ax.figure.legend(bbox_to_anchor=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), loc=<span class="hljs-string">'upper left'</span>);
</code></pre>
    <p class="normal">We are using a dual y-axis for plotting the scores and the data within the same plot. Here it is:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_09.png" alt="intrusions_scores.png"/></figure>
    <p class="packt_figref">Figure 6.12: Time-series with anomalies</p>
    <p class="normal">Some points <a id="_idIndexMarker551"/>are not recognized as outliers since the periodic<a id="_idIndexMarker552"/> nature of the signal is removed by the Fourier filter.</p>
    <h2 id="_idParaDest-111" class="title">Change point detection</h2>
    <p class="normal">We'll first <a id="_idIndexMarker553"/>create a synthetic multivariate<a id="_idIndexMarker554"/> time-series with the ruptures library. We'll set the number of dimensions to 3 and the length of the time-series to 500, and our time-series will have 3 change points and a Gaussian noise of standard deviation 5.0 will be over imposed:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> ruptures <span class="hljs-keyword">as</span> rpt
signal, bkps = rpt.pw_constant(
  n_samples=<span class="hljs-number">500</span>, n_features=<span class="hljs-number">3</span>, n_bkps=<span class="hljs-number">3</span>,
  noise_std=<span class="hljs-number">5.0</span>, delta=(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>)
)
</code></pre>
    <p class="normal">Signal is a NumPy array of 500x3. <code class="Code-In-Text--PACKT-">bkps</code> is the array of the change points (123, 251, and 378). </p>
    <p class="normal">We can plot this time-series with a utility function that highlights the subsections separated by changepoints:</p>
    <pre class="programlisting code"><code class="hljs-code"> rpt.display(signal, bkps)
</code></pre>
    <p class="normal">Here's the plot of our time-series with three change points:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_10.png" alt="ruptures_time_series.png"/></figure>
    <p class="packt_figref">Figure 6.13: Time-series with change points</p>
    <p class="normal">We can<a id="_idIndexMarker555"/> apply <a id="_idIndexMarker556"/>Binary Segmentation to this time-series. ruptures follows the scikit-learn conventions, so if you have used scikit-learn before, the usage should be very intuitive:</p>
    <pre class="programlisting code"><code class="hljs-code"> algo = rpt.Binseg(model=<span class="hljs-string">"l1"</span>).fit(signal)
my_bkps = algo.predict(n_bkps=<span class="hljs-number">3</span>)
</code></pre>
    <p class="normal">We have several options for the Binary Segmentation constraint – we have the choice between <code class="Code-In-Text--PACKT-">l1</code>, <code class="Code-In-Text--PACKT-">l2</code>, <code class="Code-In-Text--PACKT-">rbf</code>, <code class="Code-In-Text--PACKT-">linear</code>, <code class="Code-In-Text--PACKT-">normal</code>, and <code class="Code-In-Text--PACKT-">ar</code>.</p>
    <p class="normal">We can plot the predictions of the Binary Segmentation with another utility function:</p>
    <pre class="programlisting code"><code class="hljs-code"> rpt.show.display(signal, bkps, my_bkps, figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))
</code></pre>
    <p class="normal">Here's the plot of our change point predictions from the Binary Segmentation model:</p>
    <figure class="mediaobject"><img src="../Images/B17577_06_11.png" alt="rupture_bs_predictions.png"/></figure>
    <p class="packt_figref">Figure 6.14: Time-series with detected change points (Binary Segmentation)</p>
    <p class="normal">Let's<a id="_idIndexMarker557"/> summarize<a id="_idIndexMarker558"/> some of the information in this chapter!</p>
    <h1 id="_idParaDest-112" class="title">Summary</h1>
    <p class="normal">In this chapter, we have concentrated on two aspects of unsupervised methods for time-series:</p>
    <ul>
      <li class="bullet">Anomaly detection</li>
      <li class="bullet">Change point detection</li>
    </ul>
    <p class="normal">The essence of anomaly detection (also: outlier detection) is to identify sequences that are notably different from the rest of the series. We've investigated different anomaly detection methods, and how several major companies are dealing with it at scale.</p>
    <p class="normal">When working with time-series, it's important to be aware of changes in the data over time that makes models useless (model staleness). This is called change point detection and drift detection. </p>
    <p class="normal">We've looked at change point detection in this chapter. In <em class="chapterRef">Chapter 8</em>, <em class="italic">Online Learning for Time-Series</em>, we'll look at drift detection in more detail.</p>
  </div>
</body></html>
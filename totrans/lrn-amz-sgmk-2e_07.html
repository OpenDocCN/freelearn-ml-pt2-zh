<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer111">
			<h1 id="_idParaDest-93"><a id="_idTextAnchor091"/>Chapter 5: Training CV Models</h1>
			<p>In the previous chapter, you learned how to use SageMaker's built-in algorithms for traditional machine learning problems, including classification, regression, and anomaly detection. We saw that these algorithms work well on tabular data, such as CSV files. However, they are not well suited for image datasets, and they generally perform very poorly on <strong class="bold">CV</strong> (<strong class="bold">CV</strong>) tasks.</p>
			<p>For a few years now, CV has taken the world by storm, and not a month goes by without a new breakthrough in extracting patterns from images and videos. In this chapter, you will learn about three built-in algorithms designed specifically for CV tasks. We'll discuss the types of problems that you can solve with them. We'll also spend a lot of time explaining how to prepare image datasets, as this crucial topic is often inexplicably overlooked. Of course, we'll train and deploy models too.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Discovering the CV built-in algorithms in Amazon SageMaker</li>
				<li>Preparing image datasets</li>
				<li>Using the built-in CV algorithms: <strong class="bold">image classification</strong>, <strong class="bold">object detection</strong>, and <strong class="bold">semantic segmentation</strong></li>
			</ul>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor092"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you haven't got one already, please point your browser at <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create one. You should also familiarize yourself with the AWS Free Tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the AWS <strong class="bold">Command Line Interface</strong> (<strong class="bold">CLI</strong>) for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).  </p>
			<p>You will need a working Python 3.x environment. Installing the Anaconda distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory, but strongly encouraged, as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>The code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor093"/>Discovering the CV built-in algorithms in Amazon SageMaker</h1>
			<p>SageMaker includes three <a id="_idIndexMarker385"/>CV algorithms, based on proven deep learning networks. In this section, you'll learn about <a id="_idIndexMarker386"/>these algorithms, what kind of problem they can help you solve, and what their training scenarios are:</p>
			<ul>
				<li><strong class="bold">Image classification</strong> assigns<a id="_idIndexMarker387"/> one or more labels to an image.</li>
				<li><strong class="bold">Object detection</strong> detects <a id="_idIndexMarker388"/>and classifies objects in an image.</li>
				<li><strong class="bold">Semantic segmentation</strong> assigns <a id="_idIndexMarker389"/>every pixel of an image to a specific class.</li>
			</ul>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor094"/>Discovering the image classification algorithm</h2>
			<p>Starting from<a id="_idIndexMarker390"/> an input image, the <strong class="bold">image classification</strong> algorithm predicts a probability for each class present in the training dataset. This algorithm is based on<a id="_idIndexMarker391"/> the <strong class="bold">ResNet</strong> convolutional neural network (<a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>). Published in 2015, <strong class="bold">ResNet</strong> won the ILSVRC classification task that <a id="_idIndexMarker392"/>same year (<a href="http://www.image-net.org/challenges/LSVRC/">http://www.image-net.org/challenges/LSVRC/</a>). Since then, it has become a popular and versatile choice for image classification.</p>
			<p>Many hyperparameters can be set, including the depth of the network, which can range from 18 to 200 layers. In general, the more layers the network has, the better it will learn, at the expense of increased training times.</p>
			<p>Please note that<a id="_idIndexMarker393"/> the <strong class="bold">image classification</strong> algorithm supports both <strong class="bold">single-label</strong> and <strong class="bold">multi-label</strong> classification. We will focus on single-label classification<a id="_idIndexMarker394"/> in this chapter. Working with several labels is very similar, and you'll find a complete <a id="_idIndexMarker395"/>example at <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/</a>.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor095"/>Discovering the object detection algorithm</h2>
			<p>Starting<a id="_idIndexMarker396"/> from an input image, the <strong class="bold">object detection</strong> algorithm predicts both the class and the location of each object in the image. Of course, the algorithm can only <a id="_idIndexMarker397"/>detect object classes present in the training dataset. The location of each <a id="_idIndexMarker398"/>object is defined by a set of four coordinates, called a <strong class="bold">bounding box</strong>.</p>
			<p>This algorithm is<a id="_idIndexMarker399"/> based on the <strong class="bold">Single Shot MultiBox Detector</strong> (<strong class="bold">SSD</strong>) architecture (<a href="https://arxiv.org/abs/1512.02325">https://arxiv.org/abs/1512.02325</a>). For classification, you<a id="_idIndexMarker400"/> can pick from two base <a id="_idIndexMarker401"/>networks: <strong class="bold">VGG-16</strong> (<a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a>) or <strong class="bold">ResNet-50</strong>.</p>
			<p>The following output shows an example of object detection (source: <a href="https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg">https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg</a>):</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="Images/B17705_05_1.jpg" alt="Figure 5.1 – Test image&#13;&#10;" width="374" height="206"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Test image</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor096"/>Discovering the semantic segmentation algorithm</h2>
			<p>Starting<a id="_idIndexMarker402"/> from an input image, the <strong class="bold">semantic segmentation</strong> algorithm predicts the class of every pixel of the image. This is a much harder problem than image <a id="_idIndexMarker403"/>classification (which only considers the full image) or object detection (which only focuses on specific parts of the image). Using the probabilities contained in a prediction, it's possible to build <strong class="bold">segmentation masks</strong> that cover specific objects in the<a id="_idIndexMarker404"/> picture. </p>
			<p>Three<a id="_idIndexMarker405"/> neural <a id="_idIndexMarker406"/>networks may be used for <a id="_idIndexMarker407"/>segmentation:</p>
			<ul>
				<li><strong class="bold">Fully Convolutional Networks</strong> (<strong class="bold">FCNs</strong>): <a href="https://arxiv.org/abs/1411.4038">https://arxiv.org/abs/1411.4038</a> </li>
				<li><strong class="bold">Pyramid Scene Parsing</strong> (<strong class="bold">PSP</strong>): <a href="https://arxiv.org/abs/1612.01105">https://arxiv.org/abs/1612.01105</a></li>
				<li><strong class="bold">DeepLab</strong> v3: <a href="https://arxiv.org/abs/1706.05587">https://arxiv.org/abs/1706.05587</a> </li>
			</ul>
			<p>The encoder <a id="_idIndexMarker408"/>network is <strong class="bold">ResNet</strong>, with either 50 or 101 layers.</p>
			<p>The following output shows the result of segmenting the previous image. We see the segmentation masks, and each class is assigned a unique color; the background is black, and so on:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="Images/B17705_05_2.jpg" alt="Figure 5.2 – Segmented test image&#13;&#10;" width="378" height="206"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Segmented test image</p>
			<p>Now let's see how we can train these algorithms on our own data.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor097"/>Training with CV algorithms</h2>
			<p>All three algorithms are <a id="_idIndexMarker409"/>based on <strong class="bold">supervised learning</strong>, so our starting point will be <a id="_idIndexMarker410"/>a labeled dataset. Of course, the nature of these labels will be different for each algorithm:</p>
			<ul>
				<li>Class labels for <strong class="bold">image classification</strong></li>
				<li>Bounding boxes and class labels for <strong class="bold">object detection</strong></li>
				<li>Segmentation masks and class labels for <strong class="bold">semantic segmentation</strong></li>
			</ul>
			<p>Annotating image datasets is a lot of work. If you need to build your own dataset, <strong class="bold">Amazon SageMaker Ground Truth</strong> can definitely <a id="_idIndexMarker411"/>help, and we studied it in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>. Later in this chapter, we'll show you how to use image datasets labeled with Ground Truth.</p>
			<p>When it comes to packaging <a id="_idIndexMarker412"/>datasets, the use of <strong class="bold">RecordIO</strong> files is strongly recommended (<a href="https://mxnet.apache.org/api/faq/recordio">https://mxnet.apache.org/api/faq/recordio</a>). Packaging images in a small number of record-structured files makes it much easier to move datasets around and to split them for distributed training. Having said that, you can also train on individual image files if you prefer. </p>
			<p>Once your dataset is ready in S3, you need to decide whether you'd like to train from scratch, or whether you'd like to start from a pretrained network. </p>
			<p>Training from scratch is fine if you have plenty of data, and if you're convinced that there's value in building a specific model with it. However, this will take a lot of time, possibly hundreds of epochs, and hyperparameter selection will be absolutely critical in getting good results.</p>
			<p>Using a pretrained network is generally a better option, even if you have lots of data. Thanks to <strong class="bold">transfer learning</strong>, you <a id="_idIndexMarker413"/>can start from a model trained on a huge collection of images (think millions) and fine-tune it on your data and classes. Training will be much shorter, and you will get models with higher accuracy rates quicker.</p>
			<p>Given the complexity of the models and the size of datasets, training with CPU instances is simply not an option. We'll use GPU instances for all examples. </p>
			<p>Last but not least, all three algorithms are based <a id="_idIndexMarker414"/>on <strong class="bold">Apache MXNet</strong>. This lets you export their models outside of SageMaker and deploy them anywhere you like. </p>
			<p>In the next sections, we're going to zoom in on image datasets, and how to prepare them for training.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor098"/>Preparing image datasets</h1>
			<p>Input formats are more complex <a id="_idIndexMarker415"/>for image datasets than for tabular datasets, and we need to get them exactly right. The CV algorithms in SageMaker support three input formats: </p>
			<ul>
				<li>Image files</li>
				<li><strong class="bold">RecordIO</strong> files</li>
				<li>Augmented manifests built by <strong class="bold">SageMaker Ground Truth</strong></li>
			</ul>
			<p>In this section, you'll learn how to prepare datasets in these different formats. To the best of my knowledge, this topic has rarely been addressed in such detail. Get ready to learn a lot!</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor099"/>Working with image files</h2>
			<p>This is the simplest format, and it's<a id="_idIndexMarker416"/> supported by all three algorithms. Let's see how to use it with the <strong class="bold">image classification</strong> algorithm.</p>
			<h3>Converting an image classification dataset to image format</h3>
			<p>A dataset in image<a id="_idIndexMarker417"/> format has to be stored in S3. Image files don't need to be sorted in any way, and you simply could store all of them in the same bucket. </p>
			<p>Images are described in a <strong class="bold">list file</strong>, a text file <a id="_idIndexMarker418"/>containing a line per image. For <strong class="bold">image classification</strong>, three columns are present: the unique identifier of the image, its class label, and its path. Here's an example:</p>
			<p class="source-code">1023  5  prefix/image2753.jpg</p>
			<p class="source-code">38    6  another_prefix/image72.jpg</p>
			<p class="source-code">983   2  yet_another_prefix/image863.jpg</p>
			<p>The first line tells us that <strong class="source-inline">image2753.jpg</strong> belongs to class 5 and has been assigned ID 1023.</p>
			<p>You need a list file for each channel, so you would need one for the training dataset, one for the validation dataset, and so on. You can either write bespoke code to generate them, or you can use a simple program that is part of <strong class="bold">Apache MXNet</strong>. This program is called <strong class="source-inline">im2rec</strong>, and it's available in Python and C++. We'll use the Python version.</p>
			<p>Let's use the Dogs vs. Cats dataset available on <strong class="bold">Kaggle</strong> (<a href="https://www.kaggle.com/c/dogs-vs-cats">https://www.kaggle.com/c/dogs-vs-cats</a>). This dataset is 812 MB. Unsurprisingly, it contains two classes: dogs and cats. It's already split for training and testing (25,000 and 12,500 images, respectively). Here's how:</p>
			<ol>
				<li>We create a <strong class="bold">Kaggle</strong> account, accept the rules of the Dogs vs. Cats competition, and install the <strong class="source-inline">kaggle</strong> CLI (<a href="https://github.com/Kaggle/kaggle-api">https://github.com/Kaggle/kaggle-api</a>). </li>
				<li>On our local machine, we download and extract the training dataset (you can ignore the test set, which is only needed for the competition):<p class="source-code"><strong class="bold">$ kaggle competitions download -c dogs-vs-cats</strong></p><p class="source-code"><strong class="bold">$ sudo yum -y install zip unzip</strong></p><p class="source-code"><strong class="bold">$ unzip dogs-vs-cats.zip</strong></p><p class="source-code"><strong class="bold">$ unzip train.zip</strong></p></li>
				<li>Dog and cat images are mixed up in the same folder. We create a subfolder for each class, and move the appropriate images there:<p class="source-code"><strong class="bold">$ cd train</strong></p><p class="source-code"><strong class="bold">$ mkdir dog cat</strong></p><p class="source-code"><strong class="bold">$ find . -name 'dog.*' -exec mv {} dog \;</strong></p><p class="source-code"><strong class="bold">$ find . -name 'cat.*' -exec mv {} cat \;</strong></p></li>
				<li>We'll need validation images, so let's move 1,250 random dog images and 1,250 random cat<a id="_idIndexMarker419"/> images to specific directories. I'm using <strong class="source-inline">bash</strong> scripting here, but feel free to use any tool you like:<p class="source-code"><strong class="bold">$ mkdir -p val/dog val/cat</strong></p><p class="source-code"><strong class="bold">$ ls dog | sort -R | tail -1250 | while read file;</strong></p><p class="source-code"><strong class="bold">do mv dog/$file val/dog; done</strong></p><p class="source-code"><strong class="bold">$  ls cat | sort -R | tail -1250 | while read file;</strong></p><p class="source-code"><strong class="bold">do mv cat/$file val/cat; done</strong></p></li>
				<li> We move the remaining 22,500 images to the training folder:<p class="source-code"><strong class="bold">$ mkdir train</strong></p><p class="source-code"><strong class="bold">$ mv dog cat train</strong></p></li>
				<li>Our dataset now looks like this:<p class="source-code"><strong class="bold">$ du -h</strong></p><p class="source-code"><strong class="bold">33M     ./val/dog</strong></p><p class="source-code"><strong class="bold">28M     ./val/cat</strong></p><p class="source-code"><strong class="bold">60M     ./val</strong></p><p class="source-code"><strong class="bold">289M    ./train/dog</strong></p><p class="source-code"><strong class="bold">248M    ./train/cat</strong></p><p class="source-code"><strong class="bold">537M    ./train</strong></p><p class="source-code"><strong class="bold">597M    .</strong></p></li>
				<li>We download the <strong class="source-inline">im2rec</strong> tool from GitHub (<a href="https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py">https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py</a>). It requires dependencies, which we need to install (you may have to adapt the command to your<a id="_idIndexMarker420"/> own environment and flavor of Linux):<p class="source-code"><strong class="bold">$ wget https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py</strong></p><p class="source-code"><strong class="bold">$ sudo yum -y install python-devel python-pip opencv opencv-devel opencv-python</strong></p><p class="source-code"><strong class="bold">$ pip3 install mxnet opencv-python</strong></p></li>
				<li>We run <strong class="source-inline">im2rec</strong> to build two list files, one for training data and one for validation data:<p class="source-code"><strong class="bold">$ python3 im2rec.py --list --recursive dogscats-train train</strong></p><p class="source-code"><strong class="bold">$ python3 im2rec.py --list --recursive dogscats-val val</strong></p><p>This creates the <strong class="source-inline">dogscats-train.lst</strong> and <strong class="source-inline">dogscats-val.lst</strong> files. Their three columns are a unique image identifier, the class label (0 for cats, 1 for dogs), and the image path, as follows:</p><p class="source-code"><strong class="bold">3197  0.000000  cat/cat.1625.jpg</strong></p><p class="source-code"><strong class="bold">15084 1.000000  dog/dog.12322.jpg</strong></p><p class="source-code"><strong class="bold">1479  0.000000  cat/cat.11328.jpg</strong></p><p class="source-code"><strong class="bold">5262  0.000000  cat/cat.3484.jpg</strong></p><p class="source-code"><strong class="bold">20714 1.000000  dog/dog.6140.jpg</strong></p></li>
				<li>We move the list files to specific directories. This is required because they will be passed to the <strong class="source-inline">Estimator</strong> as two new channels, <strong class="source-inline">train_lst</strong> and <strong class="source-inline">validation_lst</strong>:<p class="source-code"><strong class="bold">$ mkdir train_lst val_lst</strong></p><p class="source-code"><strong class="bold">$ mv dogscats-train.lst train_lst</strong></p><p class="source-code"><strong class="bold">$ mv dogscats-val.lst val_lst</strong></p></li>
				<li>The dataset <a id="_idIndexMarker421"/>now looks like this:<p class="source-code"><strong class="bold">$ du -h</strong></p><p class="source-code"><strong class="bold">33M     ./val/dog</strong></p><p class="source-code"><strong class="bold">28M     ./val/cat</strong></p><p class="source-code"><strong class="bold">60M     ./val</strong></p><p class="source-code"><strong class="bold">700K    ./train_lst</strong></p><p class="source-code"><strong class="bold">80K     ./val_lst</strong></p><p class="source-code"><strong class="bold">289M    ./train/dog</strong></p><p class="source-code"><strong class="bold">248M    ./train/cat</strong></p><p class="source-code"><strong class="bold">537M    ./train</strong></p><p class="source-code"><strong class="bold">597M    .</strong></p></li>
				<li>Finally, we sync this folder to the SageMaker default bucket for future use. Please make sure to only sync the four folders, and nothing else:<p class="source-code"><strong class="bold">$ aws s3 sync . </strong></p><p class="source-code"><strong class="bold">  s3://sagemaker-eu-west-1-123456789012/dogscats-images/input/</strong></p></li>
			</ol>
			<p>Now, let's move on to using the image format with the object detection algorithms.</p>
			<h3>Converting detection datasets to image format</h3>
			<p>The general <a id="_idIndexMarker422"/>principle is identical. We need to build a file tree<a id="_idIndexMarker423"/> representing the four channels: <strong class="source-inline">train</strong>, <strong class="source-inline">validation</strong>, <strong class="source-inline">train_annotation</strong>, and <strong class="source-inline">validation_annotation</strong>.</p>
			<p>The main difference lies in how labeling information is stored. Instead of list files, we need to build JSON files. </p>
			<p>Here's an example of a fictitious picture in an object detection dataset. For each object in the picture, we define the coordinates of the top-left corner of its bounding box, its height, and its width. We also define the class identifier, which points to a category array that also stores class names:</p>
			<p class="source-code">{</p>
			<p class="source-code">   "file": " my-prefix/my-picture.jpg",</p>
			<p class="source-code">   "image_size": [{"width": 512,"height": 512,"depth": 3}],</p>
			<p class="source-code">   "annotations": [</p>
			<p class="source-code">      {</p>
			<p class="source-code">       "class_id": 1, </p>
			<p class="source-code">       "left": 67, "top": 121, "width": 61, "height": 128</p>
			<p class="source-code">      },</p>
			<p class="source-code">      {</p>
			<p class="source-code">       "class_id": 5, </p>
			<p class="source-code">       "left": 324, "top": 134, "width": 112, "height": 267</p>
			<p class="source-code">      }</p>
			<p class="source-code">   ],</p>
			<p class="source-code">   "categories": [</p>
			<p class="source-code">      { "class_id": 1, "name": "pizza" },</p>
			<p class="source-code">      { "class_id": 5, "name": "beer" }</p>
			<p class="source-code">   ]</p>
			<p class="source-code">}</p>
			<p>We would <a id="_idIndexMarker424"/>need to do this for every picture in the <a id="_idIndexMarker425"/>dataset, building a JSON file for the training set and one for the validation set.</p>
			<p>Finally, let's see how to use the image format with the semantic segmentation algorithm.</p>
			<h3>Converting segmentation datasets to image format</h3>
			<p>Image format is <a id="_idIndexMarker426"/>the only format supported<a id="_idIndexMarker427"/> by the image segmentation algorithm.</p>
			<p>This time, we need to build a file tree representing the four channels: <strong class="source-inline">train</strong>, <strong class="source-inline">validation</strong>, <strong class="source-inline">train_annotation</strong>, and <strong class="source-inline">validation_annotation</strong>. The first two channels contain the source images, and the last two contain the segmentation mask images. </p>
			<p>File naming is critical in matching an image to its mask: the source image and the mask image must have the same name in their respective channels. Here's an example:</p>
			<p class="source-code">├── train</p>
			<p class="source-code">│   ├── image001.png</p>
			<p class="source-code">│   ├── image007.png</p>
			<p class="source-code">│   └── image042.png</p>
			<p class="source-code">├── train_annotation</p>
			<p class="source-code">│   ├── image001.png</p>
			<p class="source-code">│   ├── image007.png</p>
			<p class="source-code">│   └── image042.png </p>
			<p class="source-code">├── validation</p>
			<p class="source-code">│   ├── image059.png</p>
			<p class="source-code">│   ├── image062.png</p>
			<p class="source-code">│   └── image078.png</p>
			<p class="source-code">└── validation_annotation</p>
			<p class="source-code">│   ├── image059.png</p>
			<p class="source-code">│   ├── image062.png</p>
			<p class="source-code">│   └── image078.png</p>
			<p>You can see sample pictures in the following figure. The source image on the left would go to the <strong class="source-inline">train</strong> folder and<a id="_idIndexMarker428"/> the mask picture on the right would go to the <strong class="source-inline">train_annotation</strong> folder. They would have to have <a id="_idIndexMarker429"/>exactly the same name so that the algorithm could match them:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="Images/B17705_05_3.jpg" alt="Figure 5.3 – Sample image from the Pascal VOC dataset&#13;&#10;" width="662" height="487"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Sample image from the Pascal VOC dataset</p>
			<p>One clever feature of this format is how it matches class identifiers to mask colors. Mask images are PNG files with a 256-color palette. Each class in the dataset is assigned a specific entry in the color palette. These colors are the ones you see in masks for objects belonging to that class.</p>
			<p>If your labeling tool or your existing dataset doesn't support this PNG feature, you can add your own color mapping file. Please refer to the AWS documentation for details: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html">https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html</a>.</p>
			<p>Now, let's prepare<a id="_idIndexMarker430"/> the <strong class="bold">Pascal VOC</strong> dataset. This dataset is frequently used to benchmark object detection and semantic segmentation model:</p>
			<ol>
				<li value="1">We first<a id="_idIndexMarker431"/> download and extract the<a id="_idIndexMarker432"/> 2012 version of the dataset. Again, I recommend using an AWS-hosted instance to speed up network transfers:<p class="source-code"><strong class="bold">$ wget https://data.deepai.org/PascalVOC2012.zip</strong></p><p class="source-code"><strong class="bold">$ unzip PascalVOC2012.zip </strong></p></li>
				<li>We create a work directory where we'll build the four channels:<p class="source-code"><strong class="bold">$ mkdir input</strong></p><p class="source-code"><strong class="bold">$ cd input</strong></p><p class="source-code"><strong class="bold">$ mkdir train validation train_annotation validation_annotation</strong></p></li>
				<li>Using the list of training files defined in the dataset, we copy the corresponding images to the <strong class="source-inline">train</strong> folder. I'm using <strong class="source-inline">bash</strong> scripting here; feel free to use your tool of choice:<p class="source-code"><strong class="bold">$ for file in 'cat ../ImageSets/Segmentation/train.txt | xargs'; do cp ../JPEGImages/$file".jpg" train; done</strong></p></li>
				<li>We then do the same for validation images, training masks, and validation masks:<p class="source-code"><strong class="bold">$ for file in 'cat ../ImageSets/Segmentation/val.txt | xargs'; do cp ../JPEGImages/$file".jpg" validation; done</strong></p><p class="source-code"><strong class="bold">$ for file in 'cat ../ImageSets/Segmentation/train.txt | xargs'; do cp ../SegmentationClass/$file".png" train_annotation; done</strong></p><p class="source-code"><strong class="bold">$ for file in 'cat ../ImageSets/Segmentation/val.txt | xargs'; do cp ../SegmentationClass/$file".png" validation_annotation; done</strong></p></li>
				<li>We check that we have the same number of images in the two training channels, and in the two validation channels:<p class="source-code"><strong class="bold">$ for dir in train train_annotation validation validation_annotation; do find $dir -type f | wc -l; done</strong></p><p>We see 1,464 training files and masks, and 1,449 validation files and masks. We're all set:</p><p class="source-code"><strong class="bold">1464</strong></p><p class="source-code"><strong class="bold">1464</strong></p><p class="source-code"><strong class="bold">1449</strong></p><p class="source-code"><strong class="bold">1449</strong></p></li>
				<li>The last <a id="_idIndexMarker433"/>step is to sync the file tree<a id="_idIndexMarker434"/> to S3 for later use. Again, please make sure to sync only the four folders:<p class="source-code"><strong class="bold">$ aws s3 sync . s3://sagemaker-eu-west-1-123456789012/pascalvoc-segmentation/input/</strong></p></li>
			</ol>
			<p>We know how to prepare classification, detection, and segmentation datasets in image format. This is a critical step, and you have to get things exactly right.</p>
			<p>Still, I'm sure that you found the steps in this section a little painful. So did I! Now imagine doing the same with millions of images. That doesn't sound very exciting, does it? </p>
			<p>We need an easier way to prepare image datasets. Let's see how we can simplify dataset preparation with <strong class="bold">RecordIO</strong> files.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor100"/>Working with RecordIO files</h2>
			<p>RecordIO files are<a id="_idIndexMarker435"/> easier to move around. It's much more efficient for an algorithm to read a large sequential file than to read lots of tiny files stored at random disk locations.</p>
			<h3>Converting an image classification dataset to RecordIO</h3>
			<p>Let's convert the Dogs vs. Cats dataset to RecordIO:</p>
			<ol>
				<li value="1">Starting from a <a id="_idIndexMarker436"/>freshly extracted<a id="_idIndexMarker437"/> copy of the dataset, we move the images to the appropriate class folder:<p class="source-code"><strong class="bold">$ cd train</strong></p><p class="source-code"><strong class="bold">$ mkdir dog cat</strong></p><p class="source-code"><strong class="bold">$ find . -name 'dog.*' -exec mv {} dog \;</strong></p><p class="source-code"><strong class="bold">$ find . -name 'cat.*' -exec mv {} cat \;</strong></p></li>
				<li>We run <strong class="source-inline">im2rec</strong> to generate list files for the training dataset (90%) and the validation dataset (10%). There's no need to split the dataset ourselves!<p class="source-code"><strong class="bold">$ python3 im2rec.py --list --recursive --train-ratio 0.9 dogscats .</strong></p></li>
				<li>We run <strong class="source-inline">im2rec</strong> once more to generate the RecordIO files:<p class="source-code"><strong class="bold">$ python3 im2rec.py --num-thread 8 dogscats .</strong></p><p>This creates four new files: two RecordIO files (<strong class="source-inline">.rec</strong>) containing the packed images, and two index files (<strong class="source-inline">.idx</strong>) containing the offsets of these images inside the record files:</p><p class="source-code"><strong class="bold">$ ls dogscats*</strong></p><p class="source-code"><strong class="bold">dogscats_train.idx dogscats_train.lst dogscats_train.rec</strong></p><p class="source-code"><strong class="bold">dogscats_val.idx dogscats_val.lst dogscats_val.rec</strong></p></li>
				<li>Let's store the RecordIO files in S3, as we'll use them later:<p class="source-code"><strong class="bold">$ aws s3 cp dogscats_train.rec s3://sagemaker-eu-west-1-123456789012/dogscats/input/train/</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp dogscats_val.rec s3://sagemaker-eu-west-1-123456789012/dogscats/input/validation/</strong></p></li>
			</ol>
			<p>This was much<a id="_idIndexMarker438"/> simpler, wasn't it? <strong class="source-inline">im2rec</strong> has <a id="_idIndexMarker439"/>additional options to resize images and more. It can also break the dataset into several chunks, a <a id="_idIndexMarker440"/>useful technique<a id="_idIndexMarker441"/> for <strong class="bold">Pipe Mode</strong> and <strong class="bold">Distributed Training</strong>. We'll study them in <a href="B17705_09_Final_JM_ePub.xhtml#_idTextAnchor168"><em class="italic">Chapter 9</em></a>, <em class="italic">Scaling Your Training Jobs</em>.</p>
			<p>Now, let's move on to using RecordIO files for object detection.</p>
			<h3>Converting an object detection dataset to RecordIO</h3>
			<p>The process is<a id="_idIndexMarker442"/> very similar. A major difference is the format of <a id="_idIndexMarker443"/>list files. Instead of dealing only with class labels, we also need to store bounding boxes.</p>
			<p>Let's see what this means for the Pascal VOC dataset. The following image is taken from the dataset:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="Images/B17705_05_4.jpg" alt="Figure 5.4 – Sample image from the Pascal VOC dataset&#13;&#10;" width="448" height="336"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Sample image from the Pascal VOC dataset</p>
			<p>It contains three chairs. The labeling information is stored in an individual <strong class="bold">XML</strong> file, shown in slightly abbreviated form:</p>
			<p class="source-code">&lt;annotation&gt;</p>
			<p class="source-code">        &lt;folder&gt;VOC2007&lt;/folder&gt;</p>
			<p class="source-code">        &lt;filename&gt;003988.jpg&lt;/filename&gt;</p>
			<p class="source-code">        . . .</p>
			<p class="source-code">        &lt;object&gt;</p>
			<p class="source-code">                &lt;name&gt;chair&lt;/name&gt;</p>
			<p class="source-code">                &lt;pose&gt;Unspecified&lt;/pose&gt;</p>
			<p class="source-code">                &lt;truncated&gt;1&lt;/truncated&gt;</p>
			<p class="source-code">                &lt;difficult&gt;0&lt;/difficult&gt;</p>
			<p class="source-code">                &lt;bndbox&gt;</p>
			<p class="source-code">                    &lt;xmin&gt;1&lt;/xmin&gt;</p>
			<p class="source-code">                    &lt;ymin&gt;222&lt;/ymin&gt;</p>
			<p class="source-code">                    &lt;xmax&gt;117&lt;/xmax&gt;</p>
			<p class="source-code">                    &lt;ymax&gt;336&lt;/ymax&gt;</p>
			<p class="source-code">                &lt;/bndbox&gt;</p>
			<p class="source-code">        &lt;/object&gt;</p>
			<p class="source-code">        &lt;object&gt;</p>
			<p class="source-code">                &lt;name&gt;chair&lt;/name&gt;</p>
			<p class="source-code">                &lt;pose&gt;Unspecified&lt;/pose&gt;</p>
			<p class="source-code">                &lt;truncated&gt;1&lt;/truncated&gt;</p>
			<p class="source-code">                &lt;difficult&gt;1&lt;/difficult&gt;</p>
			<p class="source-code">                &lt;bndbox&gt;</p>
			<p class="source-code">                    &lt;xmin&gt;429&lt;/xmin&gt;</p>
			<p class="source-code">                    &lt;ymin&gt;216&lt;/ymin&gt;</p>
			<p class="source-code">                    &lt;xmax&gt;448&lt;/xmax&gt;</p>
			<p class="source-code">                    &lt;ymax&gt;297&lt;/ymax&gt;</p>
			<p class="source-code">                &lt;/bndbox&gt;</p>
			<p class="source-code">        &lt;/object&gt;</p>
			<p class="source-code">        &lt;object&gt;</p>
			<p class="source-code">                &lt;name&gt;chair&lt;/name&gt;</p>
			<p class="source-code">                &lt;pose&gt;Unspecified&lt;/pose&gt;</p>
			<p class="source-code">                &lt;truncated&gt;0&lt;/truncated&gt;</p>
			<p class="source-code">                &lt;difficult&gt;1&lt;/difficult&gt;</p>
			<p class="source-code">                &lt;bndbox&gt;</p>
			<p class="source-code">                    &lt;xmin&gt;281&lt;/xmin&gt;</p>
			<p class="source-code">                    &lt;ymin&gt;149&lt;/ymin&gt;</p>
			<p class="source-code">                    &lt;xmax&gt;317&lt;/xmax&gt;</p>
			<p class="source-code">                    &lt;ymax&gt;211&lt;/ymax&gt;</p>
			<p class="source-code">                &lt;/bndbox&gt;</p>
			<p class="source-code">        &lt;/object&gt;</p>
			<p class="source-code">&lt;/annotation&gt;</p>
			<p>Converting<a id="_idIndexMarker444"/> this to a list file entry should <a id="_idIndexMarker445"/>look like this:</p>
			<p class="source-code">9404 2 6  8.0000  0.0022  0.6607  0.2612  1.0000  0.0000 8.0000  0.9576  0.6429  1.0000  0.8839  1.0000 8.0000  0.6272  0.4435  0.7076  0.6280  1.0000 VOC2007/JPEGImages/003988.jpg </p>
			<p>Let's decode each column:</p>
			<ul>
				<li><strong class="source-inline">9404</strong> is a unique image identifier.</li>
				<li><strong class="source-inline">2</strong> is the number of columns containing header information, including this one.</li>
				<li><strong class="source-inline">6</strong> is the number of columns for labeling information. These six columns are the class identifier, the four bounding-box coordinates, and a flag telling us whether the object is difficult to see (we won't use it).</li>
				<li>The following is for the first object:<p>a) <strong class="source-inline">8</strong> is the class identifier. Here, <strong class="source-inline">8</strong> is the <strong class="source-inline">chair</strong> class.</p><p>b) <strong class="source-inline">0.0022 0.6607 0.2612 1.0000</strong> are the relative coordinates of the <strong class="bold">bounding box</strong> with <a id="_idIndexMarker446"/>respect to the height and width of the image.</p><p>c) <strong class="source-inline">0</strong> means that the object is not difficult.</p></li>
				<li>For the <a id="_idIndexMarker447"/>second object, we have the following:<p>a) <strong class="source-inline">8</strong> is the class identifier.</p><p>b) <strong class="source-inline">0.9576 0.6429 1.0000 0.8839</strong> are the coordinates of the second object.</p><p>c) <strong class="source-inline">1</strong> means that the object is difficult.</p></li>
				<li>The third <a id="_idIndexMarker448"/>object has the following:<p>a) <strong class="source-inline">8</strong> is the class identifier.</p><p>b) <strong class="source-inline">0.6272 0.4435 0.7076 0.628</strong> are the coordinates of the third object.</p><p>c) <strong class="source-inline">1</strong> means that the object is difficult.</p></li>
				<li><strong class="source-inline">VOC2007/JPEGImages/003988.jpg</strong> is the path to the image.</li>
			</ul>
			<p>So, how do we convert thousands of XML files into a couple of list files? Unless you enjoy writing parsers, this isn't a very exciting task.</p>
			<p>Fortunately, our work has been cut out for us. Apache MXNet includes a Python script, <strong class="source-inline">prepare_dataset.py</strong>, that will handle this task. Let's see how it works:</p>
			<ol>
				<li value="1">For the next steps, I recommend using an Amazon Linux 2 EC2 instance with at least 10 GB of storage. Here are the setup steps:<p class="source-code"><strong class="bold">$ sudo yum -y install git python3-devel python3-pip opencv opencv-devel opencv-python</strong></p><p class="source-code"><strong class="bold">$ pip3 install mxnet opencv-python --user</strong></p><p class="source-code"><strong class="bold">$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/ec2-user/.local/lib/python3.7/site-packages/mxnet/</strong></p><p class="source-code"><strong class="bold">$ sudo ldconfig</strong></p></li>
				<li>Download the 2007 and 2012 Pascal VOC datasets with <strong class="source-inline">wget</strong>, and extract them in the same directory:<p class="source-code"><strong class="bold">$ mkdir pascalvoc</strong></p><p class="source-code"><strong class="bold">$ cd pascalvoc</strong></p><p class="source-code"><strong class="bold">$ wget https://data.deepai.org/PascalVOC2012.zip</strong></p><p class="source-code"><strong class="bold">$ wget https://data.deepai.org/PASCALVOC2007.zip</strong></p><p class="source-code"><strong class="bold">$ unzip PascalVOC2012.zip</strong></p><p class="source-code"><strong class="bold">$ unzip PASCALVOC2007.zip </strong></p><p class="source-code"><strong class="bold">$ mv VOC2012 VOCtrainval_06-Nov-2007/VOCdevkit</strong></p></li>
				<li>Clone the <a id="_idIndexMarker449"/>Apache MXNet <a id="_idIndexMarker450"/>repository (<a href="https://github.com/apache/incubator-mxnet/">https://github.com/apache/incubator-mxnet/</a>):<p class="source-code"><strong class="bold">$ git clone --single-branch --branch v1.4.x https://github.com/apache/incubator-mxnet</strong></p></li>
				<li>Run the <strong class="source-inline">prepare_dataset.py</strong> script to build our training dataset, merging the training and validation sets of the 2007 and 2012 versions:<p class="source-code"><strong class="bold">$ cd VOCtrainval_06-Nov-2007</strong></p><p class="source-code"><strong class="bold">$ python3 ../incubator-mxnet/example/ssd/tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --root VOCdevkit --target VOCdevkit/train.lst</strong></p><p class="source-code"><strong class="bold">$ mv VOCdevkit/train.* ..</strong></p></li>
				<li>Let's follow similar steps to generate our validation dataset, using the test set of the 2007 version:<p class="source-code"><strong class="bold">$ cd ../VOCtest_06-Nov-2007</strong></p><p class="source-code"><strong class="bold">$ python3 ../incubator-mxnet/example/ssd/tools/prepare_dataset.py --dataset pascal --year 2007 --set test --root VOCdevkit --target VOCdevkit/val.lst</strong></p><p class="source-code"><strong class="bold">$ mv VOCdevkit/val.* ..</strong></p><p class="source-code"><strong class="bold">$ cd ..</strong></p></li>
				<li>In the top-level directory, we see the files generated by the script. Feel free to take a look at the list files; they should have the format presented previously:<p class="source-code"><strong class="bold">train.idx  train.lst  train.rec  </strong></p><p class="source-code"><strong class="bold">val.idx  val.lst  val.rec  </strong></p></li>
				<li>Let's store<a id="_idIndexMarker451"/> the RecordIO files in<a id="_idIndexMarker452"/> S3 as we'll use them later:<p class="source-code"><strong class="bold">$ aws s3 cp train.rec s3://sagemaker-eu-west-1-123456789012/pascalvoc/input/train/</strong></p><p class="source-code"><strong class="bold">$ aws s3 cp val.rec s3://sagemaker-eu-west-1-123456789012/pascalvoc/input/validation/</strong></p></li>
			</ol>
			<p>The <strong class="source-inline">prepare_dataset.py</strong> script has really made things simple here. It also supports the <strong class="bold">COCO</strong> dataset (<a href="http://cocodataset.org">http://cocodataset.org</a>), and the <a id="_idIndexMarker453"/>workflow is extremely similar. </p>
			<p>What about converting other public datasets? Well, your mileage may vary. You'll find more examples at <a href="https://gluon-cv.mxnet.io/build/examples_datasets/index.html">https://gluon-cv.mxnet.io/build/examples_datasets/index.html</a>.  </p>
			<p>RecordIO is definitely a step forward. Still, when working with custom datasets, it's very likely that you'll have to write your own list file generator. That's not a huge deal, but it's extra work.</p>
			<p>Datasets labeled with <strong class="bold">Amazon SageMaker Ground Truth</strong> solve these problems altogether. Let's see how this works!</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor101"/>Working with SageMaker Ground Truth files</h2>
			<p>In <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>, you learned about SageMaker Ground <a id="_idIndexMarker454"/>Truth workflows and<a id="_idIndexMarker455"/> their outcome, an <strong class="bold">augmented manifest</strong> file. This file is in <strong class="bold">JSON Lines</strong> format: each JSON<a id="_idIndexMarker456"/> object describes a specific annotation. </p>
			<p>Here's an example from the semantic segmentation job we ran in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em> (the story is the same for other task types). We see the paths to the source image and the segmentation mask, as well as color map information telling us how to match mask colors to classes:</p>
			<p class="source-code">{"<strong class="bold">source-ref</strong>":"s3://julien-sagemaker-book/chapter2/cat/cat1.jpg",</p>
			<p class="source-code">"<strong class="bold">my-cat-job-ref</strong>":"s3://julien-sagemaker-book/chapter2/cat/output/my-cat-job/annotations/consolidated-annotation/output/0_2020-04-21T13:48:00.091190.png",</p>
			<p class="source-code">"my-cat-job-ref-metadata":{</p>
			<p class="source-code">  "<strong class="bold">internal-color-map</strong>":{</p>
			<p class="source-code">   "0":{"class-name":"BACKGROUND","hex-color": "#ffffff", </p>
			<p class="source-code">        "confidence": 0.8054600000000001}, </p>
			<p class="source-code">   "1":{"class-name":"cat","hex-color": "#2ca02c", </p>
			<p class="source-code">        "confidence":0.8054600000000001}</p>
			<p class="source-code">}, </p>
			<p class="source-code">"type":"groundtruth/semantic-segmentation",</p>
			<p class="source-code">"human-annotated":"yes",</p>
			<p class="source-code">"creation-date":"2020-04-21T13:48:00.562419",</p>
			<p class="source-code">"job-name":"labeling-job/my-cat-job"}}</p>
			<p>The following images are the ones referenced in the preceding JSON document:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="Images/B17705_05_5.jpg" alt="Figure 5.5 – Source image and segmented image&#13;&#10;" width="1646" height="389"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Source image and segmented image</p>
			<p>This is exactly what we would need to train our model. In fact, we can pass the augmented <a id="_idIndexMarker457"/>manifest to the SageMaker <strong class="source-inline">Estimator</strong> as is. No data processing is required whatsoever.</p>
			<p>To use an <strong class="bold">augmented manifest</strong> pointing at<a id="_idIndexMarker458"/> labeled images in S3, we would simply pass its location and the name of the JSON attributes (highlighted in the previous example):</p>
			<p class="source-code">training_data_channel = sagemaker.s3_input(</p>
			<p class="source-code">    s3_data=augmented_manifest_file_path, </p>
			<p class="source-code">    s3_data_type='AugmentedManifestFile',</p>
			<p class="source-code">    attribute_names=['source-ref', 'my-job-cat-ref'])</p>
			<p>That's it! This is much simpler than anything we've seen before.</p>
			<p>You can find more examples of using SageMaker Ground Truth at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs</a>. </p>
			<p>Now that we know how to prepare image datasets for training, let's put the CV algorithms to work.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor102"/>Using the built-in CV algorithms</h1>
			<p>In this section, we're<a id="_idIndexMarker459"/> going to train and deploy models with all three algorithms using public image datasets. We will cover both training from scratch and transfer learning.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor103"/>Training an image classification model</h2>
			<p>In this first example, let's<a id="_idIndexMarker460"/> use the image classification algorithm to build a model classifying the Dogs vs. Cats dataset that we prepared in a previous section. We'll first train using image format, and then using RecordIO format.</p>
			<h3>Training in image format</h3>
			<p>We will begin <a id="_idIndexMarker461"/>training using the following steps:</p>
			<ol>
				<li value="1">In a Jupyter notebook, we define the appropriate data paths:<p class="source-code">import sagemaker</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()</p><p class="source-code">prefix = 'dogscats-images'</p><p class="source-code">s3_train_path = </p><p class="source-code">  's3://{}/{}/input/train/'.format(bucket, prefix)</p><p class="source-code">s3_val_path = </p><p class="source-code">  's3://{}/{}/input/val/'.format(bucket, prefix)</p><p class="source-code">s3_train_lst_path = </p><p class="source-code">  's3://{}/{}/input/train_lst/'.format(bucket, prefix)</p><p class="source-code">s3_val_lst_path = </p><p class="source-code">  's3://{}/{}/input/val_lst/'.format(bucket, prefix)</p><p class="source-code">s3_output = 's3://{}/{}/output/'.format(bucket, prefix)</p></li>
				<li>We <a id="_idIndexMarker462"/>configure the <strong class="source-inline">Estimator</strong> for the image classification algorithm:<p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region_name = sess.boto_session.boto_region_name</p><p class="source-code">container = retrieve('image-classification', region)</p><p class="source-code">ic = sagemaker.estimator.Estimator(container,</p><p class="source-code">              sagemaker.get_execution_role(),</p><p class="source-code">              instance_count=1,</p><p class="source-code">              instance_type='ml.p3.2xlarge', </p><p class="source-code">              output_path=s3_output)</p><p>We use a GPU instance called <strong class="source-inline">ml.p3.2xlarge</strong>, which packs more than enough punch for this dataset ($4.131/hour in eu-west-1).</p><p>What about hyperparameters (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html">https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html</a>)? We set the number of classes (2) and the number of training samples (22,500). Since we're working with the image format, we need to resize images explicitly, setting the smallest dimension to 224 pixels. As we have enough <a id="_idIndexMarker463"/>data, we decide to train from scratch. In order to keep the training time low, we settle for an 18-layer <strong class="bold">ResNet</strong> model, and we train only for 10 epochs:</p><p class="source-code">ic.set_hyperparameters(num_layers=18,</p><p class="source-code">                       use_pretrained_model=0,</p><p class="source-code">                       num_classes=2,</p><p class="source-code">                       num_training_samples=22500,</p><p class="source-code">                       resize=224,</p><p class="source-code">                       mini_batch_size=128,</p><p class="source-code">                       epochs=10)</p></li>
				<li>We define the four<a id="_idIndexMarker464"/> channels, setting their content type to <strong class="source-inline">application/x-image</strong>:<p class="source-code">from sagemaker import TrainingInput</p><p class="source-code">train_data = TrainingInput (</p><p class="source-code">    s3_train_path,                              </p><p class="source-code">    content_type='application/x-image')                                       </p><p class="source-code">val_data = TrainingInput (</p><p class="source-code">    s3_val_path,                                                                  </p><p class="source-code">    content_type='application/x-image')</p><p class="source-code">train_lst_data = TrainingInput (</p><p class="source-code">    s3_train_lst_path,                                  </p><p class="source-code">    content_type='application/x-image')</p><p class="source-code">val_lst_data = TrainingInput (</p><p class="source-code">    s3_val_lst_path,                                    </p><p class="source-code">    content_type='application/x-image')                                      </p><p class="source-code">s3_channels = {'train': train_data, </p><p class="source-code">               'validation': val_data,</p><p class="source-code">               'train_lst': train_lst_data, </p><p class="source-code">               'validation_lst': val_lst_data}</p></li>
				<li>We launch the <a id="_idIndexMarker465"/>training job as follows:<p class="source-code">ic.fit(inputs=s3_channels)</p><p>In the training log, we see that data download takes about 3 minutes. Surprise, surprise: we also see that the algorithm builds RecordIO files before training. This step lasts about 1 minute:</p><p class="source-code"><strong class="bold">Searching for .lst files in /opt/ml/input/data/train_lst.</strong></p><p class="source-code"><strong class="bold">Creating record files for dogscats-train.lst</strong></p><p class="source-code"><strong class="bold">Done creating record files...</strong></p><p class="source-code"><strong class="bold">Searching for .lst files in /opt/ml/input/data/validation_lst.</strong></p><p class="source-code"><strong class="bold">Creating record files for dogscats-val.lst</strong></p><p class="source-code"><strong class="bold">Done creating record files...</strong></p></li>
				<li>As<a id="_idIndexMarker466"/> training starts, we see that an epoch takes approximately 22 seconds:<p class="source-code"><strong class="bold">Epoch[0] Time cost=22.337</strong></p><p class="source-code"><strong class="bold">Epoch[0] Validation-accuracy=0.605859</strong></p></li>
				<li>The job lasts 506 seconds in total (about 8 minutes), costing us (506/3600)*$4.131=$0.58. It reaches a validation accuracy of <strong class="bold">91.2%</strong> (hopefully, you see something similar). This is pretty good considering that we haven't even tweaked the hyperparameters yet.</li>
				<li>We then deploy the model on a small CPU instance as follows:<p class="source-code">ic_predictor = ic.deploy(initial_instance_count=1,</p><p class="source-code">                         instance_type='ml.t2.medium')</p></li>
				<li>We download the following test image and send it for prediction in <strong class="source-inline">application/x-image</strong> format.<div id="_idContainer106" class="IMG---Figure"><img src="Images/B17705_05_6.jpg" alt="Figure 5.6 – Test picture&#13;&#10;" width="600" height="630"/></div><p class="figure-caption">Figure 5.6 – Test picture</p><p>The<a id="_idIndexMarker467"/> simplest way to predict with built-in CV<a id="_idIndexMarker468"/> models is to use the <strong class="source-inline">invoke_endpoint()</strong> API in <strong class="bold">boto3</strong>. We'll use the following code to apply predictions to the image:</p><p class="source-code">import boto3, json</p><p class="source-code">import numpy as np</p><p class="source-code">with open('test.jpg', 'rb') as f:</p><p class="source-code">    payload = f.read()</p><p class="source-code">    payload = bytearray(payload)</p><p class="source-code">runtime = boto3.Session().client(</p><p class="source-code">    service_name='runtime.sagemaker')</p><p class="source-code">response = runtime.invoke_endpoint(</p><p class="source-code">    EndpointName=ic_predictor.endpoint_name,                                  </p><p class="source-code">    ContentType='application/x-image',</p><p class="source-code">    Body=payload)</p><p class="source-code">result = response['Body'].read()</p><p class="source-code">result = json.loads(result)</p><p class="source-code">index = np.argmax(result)</p><p class="source-code">print(result[index], index)</p><p>Printing out the probability and the class, our model indicates that this image is a dog with 99.997% confidence and that the image belongs to class 1:</p><p class="source-code"><strong class="bold">0.9999721050262451 1</strong></p></li>
				<li>When <a id="_idIndexMarker469"/>we're done, we delete the endpoint as follows:<p class="source-code">ic_predictor.delete_endpoint()</p></li>
			</ol>
			<p>Now let's run the same training job with the dataset in RecordIO format.</p>
			<h3>Training in RecordIO format</h3>
			<p>The only difference is<a id="_idIndexMarker470"/> how we define the input channels. We only <a id="_idIndexMarker471"/>need two channels this time in order to serve the RecordIO files we uploaded to S3. Accordingly, the content type is set to <strong class="source-inline">application/x-recordio</strong>:</p>
			<p class="source-code">from sagemaker import TrainingInput</p>
			<p class="source-code">prefix = 'dogscats'</p>
			<p class="source-code">s3_train_path=</p>
			<p class="source-code">  's3://{}/{}/input/train/'.format(bucket, prefix)</p>
			<p class="source-code">s3_val_path=</p>
			<p class="source-code">  's3://{}/{}/input/validation/'.format(bucket, prefix)</p>
			<p class="source-code">train_data = TrainingInput(</p>
			<p class="source-code">    s3_train_path,</p>
			<p class="source-code">    content_type='application/x-recordio')</p>
			<p class="source-code">validation_data = TrainingInput(</p>
			<p class="source-code">    s3_val_path,                                        </p>
			<p class="source-code">    content_type='application/x-recordio')</p>
			<p>Training again, we see that data download takes 1 minute and that the file generation step has disappeared. Although it's difficult to draw any conclusion from a single run, using RecordIO datasets will generally save you time and money, even when training on a single instance.</p>
			<p>The Dogs vs. Cats dataset has over 10,000 samples per class, which is more than enough to train from scratch. Now, let's try a dataset where that's not the case.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor104"/>Fine-tuning an image classification model</h2>
			<p>Please consider the <strong class="bold">Caltech-256</strong> dataset, a popular<a id="_idIndexMarker472"/> public dataset of 15,240 images in 256 classes, plus a clutter class (<a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">http://www.vision.caltech.edu/Image_Datasets/Caltech256/</a>). Browsing image categories, we see that all <a id="_idIndexMarker473"/>classes have a small number of samples. For instance, the "duck" class only has 60 images: it's doubtful that a deep learning algorithm, no matter how sophisticated, could extract the unique visual features of ducks with that little data.</p>
			<p>In such cases, training from scratch is simply not an option. Instead, we will use a technique called <strong class="bold">transfer learning</strong>, where <a id="_idIndexMarker474"/>we start from a network that has already been trained<a id="_idIndexMarker475"/> on a very large and diverse image dataset. <strong class="bold">ImageNet</strong> (<a href="http://www.image-net.org/">http://www.image-net.org/</a>) is probably the most popular choice for pretraining, with 1,000 classes and millions of images.</p>
			<p>The pretrained network has already learned how to extract patterns from complex images. Assuming that the images in our dataset are similar enough to those in the pretraining dataset, our model should be able to inherit that knowledge. Training for only a few more epochs on our dataset, we should be able to <strong class="bold">fine-tune</strong> the pretrained model on our data and classes.</p>
			<p>Let's see how we can easily do this with SageMaker. In fact, we'll reuse the code for the previous example with minimal changes. Let's get into it:</p>
			<ol>
				<li value="1">We download the Caltech-256 in RecordIO format (if you'd like, you could download it in image format, and convert it to RecordIO: practice makes perfect!):<p class="source-code">%%sh</p><p class="source-code">wget http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec</p><p class="source-code">wget http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec</p></li>
				<li>We upload the dataset to S3:<p class="source-code">import sagemaker</p><p class="source-code">session = sagemaker.Session()</p><p class="source-code">bucket = session.default_bucket()</p><p class="source-code">prefix = 'caltech256/'</p><p class="source-code">s3_train_path = session.upload_data(</p><p class="source-code">    path='caltech-256-60-train.rec',</p><p class="source-code">    bucket=bucket, key_prefix=prefix+'input/train')</p><p class="source-code">s3_val_path = session.upload_data(</p><p class="source-code">    path='caltech-256-60-val.rec',</p><p class="source-code">    bucket=bucket, key_prefix=prefix+'input/validation')</p></li>
				<li>We configure the <strong class="source-inline">Estimator</strong> function for the image classification algorithm. The code is strictly identical to <em class="italic">step 2</em> in the previous example.</li>
				<li>We <a id="_idIndexMarker476"/>use <strong class="bold">ResNet-50</strong> this time, as it should be able to cope with the complexity of our images. Of course, we set <strong class="source-inline">use_pretrained_network</strong> to 1. The final fully connected layer of the pretrained network will be resized to the number of classes present in our dataset, and its weights will be assigned random values.<p>We set the correct number of classes (256+1) and training samples as follows:</p><p class="source-code">ic.set_hyperparameters(num_layers=50,</p><p class="source-code">                       use_pretrained_model=1,</p><p class="source-code">                       num_classes=257,</p><p class="source-code">                       num_training_samples=15240,</p><p class="source-code">                       learning_rate=0.001,</p><p class="source-code">                       epochs=5)</p><p>Since we're fine-tuning, we only train for 5 epochs, with a smaller learning rate of 0.001.</p></li>
				<li>We configure channels and we launch the training job. The code is strictly identical to <em class="italic">step 4</em> in the previous example.</li>
				<li>After 5 epochs and 272 seconds, we see the following metric in the training log:<p class="source-code"><strong class="bold">Epoch[4] Validation-accuracy=0.8119</strong></p><p>This is quite good for just a few minutes of training. Even with enough data, it would have taken much longer to get that result from scratch.</p></li>
				<li>To deploy<a id="_idIndexMarker477"/> and test the model, we would reuse <em class="italic">steps 7-9</em> in the previous example.</li>
			</ol>
			<p> As you can see, transfer learning is a very powerful technique. It can deliver excellent results, even when you have little data. You will also train for fewer epochs, saving time and money in the process.</p>
			<p>Now, let's move on to the next algorithm, <strong class="bold">object detection</strong>.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor105"/>Training an object detection model</h2>
			<p>In this example, we'll use the <a id="_idIndexMarker478"/>object detection algorithm to build a model on the Pascal VOC dataset that we prepared in a previous section:</p>
			<ol>
				<li value="1">We start by defining data paths:<p class="source-code">import sagemaker</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()</p><p class="source-code">prefix = 'pascalvoc'</p><p class="source-code">s3_train_data = 's3://{}/{}/input/train'.format(bucket, prefix)</p><p class="source-code">s3_validation_data = 's3://{}/{}/input/validation'.format(bucket, prefix)</p><p class="source-code">s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)</p></li>
				<li>We select the object detection algorithm:<p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region = sess.boto_region_name</p><p class="source-code">container = retrieve('object-detection', region)</p></li>
				<li>We <a id="_idIndexMarker479"/>configure the <strong class="source-inline">Estimator</strong>:<p class="source-code">od = sagemaker.estimator.Estimator(</p><p class="source-code">         container,</p><p class="source-code">         sagemaker.get_execution_role(),</p><p class="source-code">         instance_count=1,</p><p class="source-code">         instance_type='ml.p3.2xlarge',</p><p class="source-code">         output_path=s3_output_location)</p></li>
				<li>We set the required hyperparameters. We select a pretrained ResNet-50 network for the base network. We set the number of classes and training samples. We settle on 30 epochs, which should be enough to start seeing results:<p class="source-code">od.set_hyperparameters(base_network='resnet-50',</p><p class="source-code">                       use_pretrained_model=1,</p><p class="source-code">                       num_classes=20,</p><p class="source-code">                       num_training_samples=16551,</p><p class="source-code">                       epochs=30)</p></li>
				<li>We then configure the two channels, and we launch the training job:<p class="source-code">from sagemaker.session import TrainingInput</p><p class="source-code">train_data = TrainingInput (</p><p class="source-code">      s3_train_data,</p><p class="source-code">      content_type='application/x-recordio')</p><p class="source-code">validation_data = TrainingInput (</p><p class="source-code">      s3_validation_data, </p><p class="source-code">      content_type='application/x-recordio')</p><p class="source-code">data_channels = {'train': train_data, </p><p class="source-code">                 'validation': validation_data}</p><p class="source-code">od.fit(inputs=data_channels)</p><p>Selecting our job in <strong class="bold">SageMaker components and registries</strong> | <strong class="bold">Experiments and trials</strong>, we can see near-real-time metrics and charts. The next image <a id="_idIndexMarker480"/>shows the validation <strong class="bold">mean average precision metric (mAP)</strong>, a key<a id="_idIndexMarker481"/> metric for object detection models.</p><div id="_idContainer107" class="IMG---Figure"><img src="Images/B17705_05_7.jpg" alt="Figure 5.7 – Validation mAP &#13;&#10;" width="562" height="317"/></div><p class="figure-caption">Figure 5.7 – Validation mAP </p><p>Please explore the other tabs (<strong class="bold">Metrics</strong>, <strong class="bold">Parameters</strong>, <strong class="bold">Artifacts</strong>, and so on). They contain everything you need to know about a particular job. Please note the <strong class="bold">Stop training job</strong> button in the top-right corner, which you can use to terminate a job at any time. </p></li>
				<li>Training lasts for 1 hour and 40 minutes. This is a pretty heavy model! We get a <strong class="bold">mean average precision metric</strong> (<strong class="bold">mAP</strong>) of 0.5151. Production use would require more training, but we should be<a id="_idIndexMarker482"/> able to test the model already.</li>
				<li>Given its <a id="_idIndexMarker483"/>complexity, we deploy the model to a larger CPU instance:<p class="source-code">od_predictor = od.deploy(</p><p class="source-code">    initial_instance_count = 1, </p><p class="source-code">    instance_type= 'ml.c5.2xlarge')</p></li>
				<li>We download a test image from Wikipedia and predict it with our model:<p class="source-code">import boto3,json</p><p class="source-code">with open('test.jpg', 'rb') as image:</p><p class="source-code">    payload = image.read()</p><p class="source-code">    payload = bytearray(payload)</p><p class="source-code">runtime = boto3.Session().client(</p><p class="source-code">    service_name='runtime.sagemaker')</p><p class="source-code">response = runtime.invoke_endpoint(</p><p class="source-code">    EndpointName=od_predictor.endpoint_name,                                  </p><p class="source-code">    ContentType='image/jpeg',</p><p class="source-code">    Body=payload)</p><p class="source-code">response = response['Body'].read()</p><p class="source-code">response = json.loads(response)</p></li>
				<li>The response contains a list of predictions. Each individual prediction contains a class identifier, the confidence score, and the relative coordinates of the bounding box. Here are the first predictions in the response:<p class="source-code"><strong class="bold">{'prediction': </strong></p><p class="source-code"><strong class="bold">[[14.0, 0.7515302300453186, 0.39770469069480896, 0.37605002522468567, 0.5998836755752563, 1.0], </strong></p><p class="source-code"><strong class="bold">[14.0, 0.6490200161933899, 0.8020403385162354, 0.2027685046195984, 0.9918708801269531, 0.8575668931007385]</strong></p><p>Using this information, we could plot the bounding boxes on the source image. For the sake of brevity, I will not include the code here, but you'll find it in the GitHub<a id="_idIndexMarker484"/> repository for this book. The following output shows the result:</p><p class="figure-caption"> </p><div id="_idContainer108" class="IMG---Figure"><img src="Images/B17705_05_8.jpg" alt="Figure 5.8 – Test image&#13;&#10;" width="358" height="251"/></div><p class="figure-caption">Figure 5.8 – Test image</p></li>
				<li>When we're done, we delete the endpoint as follows:<p class="source-code">od_predictor.delete_endpoint()</p></li>
			</ol>
			<p>This concludes our exploration of object detection. We have one more algorithm to go: <strong class="bold">Semantic Segmentation</strong>.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor106"/>Training a semantic segmentation model</h2>
			<p>In this example, we'll use the<a id="_idIndexMarker485"/> semantic segmentation algorithm to build a model on the Pascal VOC dataset that we prepared in a previous section:</p>
			<ol>
				<li value="1">As usual, we define the data paths, as follows:<p class="source-code">import sagemaker</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()  </p><p class="source-code">prefix = 'pascalvoc-segmentation'</p><p class="source-code">s3_train_data = 's3://{}/{}/input/train'.format(bucket, prefix)</p><p class="source-code">s3_validation_data = 's3://{}/{}/input/validation'.format(bucket, prefix)</p><p class="source-code">s3_train_annotation_data = 's3://{}/{}/input/train_annotation'.format(bucket, prefix)</p><p class="source-code">s3_validation_annotation_data = 's3://{}/{}/input/validation_annotation'.format(bucket, prefix)</p><p class="source-code">s3_output_location = </p><p class="source-code">'s3://{}/{}/output'.format(bucket, prefix)</p></li>
				<li>We select the<a id="_idIndexMarker486"/> semantic segmentation algorithm, and we configure the <strong class="source-inline">Estimator</strong> function:<p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">container = retrieve('semantic-segmentation', region)</p><p class="source-code">seg = sagemaker.estimator.Estimator(</p><p class="source-code">          container,</p><p class="source-code">          sagemaker.get_execution_role(),                                     </p><p class="source-code">          instance_count = 1,</p><p class="source-code">          instance_type = 'ml.p3.2xlarge',</p><p class="source-code">          output_path = s3_output_location)</p></li>
				<li>We define the required hyperparameters. We select a pretrained ResNet-50 network for the base network and a pretrained <strong class="bold">FCN</strong> for detection. We set the number of classes and training samples. Again, we settle on 30 epochs, which should be enough to start seeing results:<p class="source-code">seg.set_hyperparameters(backbone='resnet-50',</p><p class="source-code">                        algorithm='fcn',</p><p class="source-code">                        use_pretrained_model=True,</p><p class="source-code">                        num_classes=21,</p><p class="source-code">                        num_training_samples=1464,</p><p class="source-code">                        epochs=30)</p></li>
				<li>We configure the <a id="_idIndexMarker487"/>four channels, setting the content type to <strong class="source-inline">image/jpeg</strong> for source images, and <strong class="source-inline">image/png</strong> for mask images. Then, we launch the training job:<p class="source-code">from sagemaker import TrainingInput</p><p class="source-code">train_data = TrainingInput(</p><p class="source-code">    s3_train_data, </p><p class="source-code">    content_type='image/jpeg')</p><p class="source-code">validation_data = TrainingInput(</p><p class="source-code">    s3_validation_data,</p><p class="source-code">    content_type='image/jpeg')</p><p class="source-code">train_annotation = TrainingInput(</p><p class="source-code">    s3_train_annotation_data,</p><p class="source-code">    content_type='image/png')</p><p class="source-code">validation_annotation = TrainingInput(</p><p class="source-code">    s3_validation_annotation_data,  </p><p class="source-code">    content_type='image/png')</p><p class="source-code">data_channels = {</p><p class="source-code">  'train': train_data,</p><p class="source-code">  'validation': validation_data,</p><p class="source-code">  'train_annotation': train_annotation,           </p><p class="source-code">  'validation_annotation':validation_annotation</p><p class="source-code">}</p><p class="source-code">seg.fit(inputs=data_channels)</p></li>
				<li>Training lasts about<a id="_idIndexMarker488"/> 32 minutes. We get a <strong class="bold">mean intersection-over-union metric</strong> (<strong class="bold">mIOU</strong>) of 0.4874, as shown in the following plot:<div id="_idContainer109" class="IMG---Figure"><img src="Images/B17705_05_9.jpg" alt="Figure 5.9 – Validation mIOU&#13;&#10;" width="513" height="330"/></div><p class="figure-caption">Figure 5.9 – Validation mIOU</p></li>
				<li>We <a id="_idIndexMarker489"/>deploy the model to a CPU instance:<p class="source-code">seg_predictor = seg.deploy(</p><p class="source-code">    initial_instance_count=1, </p><p class="source-code">    instance_type='ml.c5.2xlarge')</p></li>
				<li>Once the endpoint is in service, we grab a test image, and we send it for prediction as a byte array with the appropriate content type:<p class="source-code">!wget -O test.jpg https://bit.ly/3yhXB9l </p><p class="source-code">filename = 'test.jpg'</p><p class="source-code">with open(filename, 'rb') as f:</p><p class="source-code">    payload = f.read()</p><p class="source-code">    payload = bytearray(payload)</p><p class="source-code">runtime = boto3.Session().client(</p><p class="source-code">    service_name='runtime.sagemaker')</p><p class="source-code">response = runtime.invoke_endpoint(</p><p class="source-code">    EndpointName=od_predictor.endpoint_name,</p><p class="source-code">    ContentType='image/jpeg',</p><p class="source-code">    Body=payload)</p><p class="source-code">response = response['Body'].read()</p><p class="source-code">response = json.loads(response)</p></li>
				<li>Using<a id="_idIndexMarker490"/> the <strong class="bold">Python Imaging Library</strong> (<strong class="bold">PIL</strong>), we process<a id="_idIndexMarker491"/> the response mask and display it:<p class="source-code">import PIL</p><p class="source-code">from PIL import Image</p><p class="source-code">import numpy as np</p><p class="source-code">import io</p><p class="source-code">num_classes = 21</p><p class="source-code">mask = np.array(Image.open(io.BytesIO(response)))</p><p class="source-code">plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='gray_r')</p><p class="source-code">plt.show()</p><p>The following images show the source image and the predicted mask. This result is promising, and would improve with more training:</p><div id="_idContainer110" class="IMG---Figure"><img src="Images/B17705_05_10.jpg" alt="Figure 5.10 – Test image and segmented test image&#13;&#10;" width="673" height="290"/></div><p class="figure-caption">           </p><p class="figure-caption">Figure 5.10 – Test image and segmented test image</p></li>
				<li>Predicting<a id="_idIndexMarker492"/> again with the <strong class="source-inline">application/x-protobuf</strong> accept type, we receive class probabilities for all pixels in the source image. The response is a protobuf buffer, which we save to a binary file:<p class="source-code">response = runtime.invoke_endpoint(</p><p class="source-code">    EndpointName=seg_predictor.endpoint_name,</p><p class="source-code">    ContentType='image/jpeg',</p><p class="source-code">    Accept='application/x-protobuf',</p><p class="source-code">    Body=payload)</p><p class="source-code">result = response['Body'].read()</p><p class="source-code">seg_predictor.accept = 'application/x-protobuf'</p><p class="source-code">response = seg_predictor.predict(img)</p><p class="source-code">results_file = 'results.rec'</p><p class="source-code">with open(results_file, 'wb') as f:</p><p class="source-code">    f.write(response)</p></li>
				<li>The buffer contains<a id="_idIndexMarker493"/> two tensors: one with the shape of the probability tensor, and one with the actual probabilities. We load <a id="_idIndexMarker494"/>them using <strong class="bold">Apache MXNet</strong> and print their shape as follows:<p class="source-code">from sagemaker.amazon.record_pb2 import Record</p><p class="source-code">import mxnet as mx</p><p class="source-code">rec = Record()</p><p class="source-code">recordio = mx.recordio.MXRecordIO(results_file, 'r')</p><p class="source-code">protobuf = rec.ParseFromString(recordio.read())</p><p class="source-code">shape = list(rec.features["shape"].int32_tensor.values)</p><p class="source-code">values = list(rec.features["target"].float32_tensor.values)</p><p class="source-code">print(shape.shape)</p><p class="source-code">print(values.shape)</p><p>The output is as follows:</p><p class="source-code"><strong class="bold">[1, 21, 289, 337]</strong></p><p class="source-code"><strong class="bold">2045253</strong></p><p>This tells us that the <strong class="source-inline">values</strong> tensor describes one image of size 289x337, where each<a id="_idIndexMarker495"/> pixel is assigned 21 probabilities, one for each of the Pascal VOC classes. You can check that 289*337*21=2,045,253.</p></li>
				<li>Knowing that, we can now reshape the <strong class="source-inline">values</strong> tensor, retrieve the 21 probabilities for the (0,0) pixel, and print the class identifier with the highest probability:<p class="source-code">mask = np.reshape(np.array(values), shape)</p><p class="source-code">pixel_probs = mask[0,:,0,0]</p><p class="source-code">print(pixel_probs)</p><p class="source-code">print(np.argmax(pixel_probs))</p><p>Here is the output:</p><p class="source-code"><strong class="bold">[9.68291104e-01 3.72813665e-04 8.14868137e-04 1.22414716e-03</strong></p><p class="source-code"><strong class="bold"> 4.57380433e-04 9.95167647e-04 4.29908326e-03 7.52388616e-04</strong></p><p class="source-code"><strong class="bold"> 1.46311778e-03 2.03254796e-03 9.60668200e-04 1.51833100e-03</strong></p><p class="source-code"><strong class="bold"> 9.39570891e-04 1.49350625e-03 1.51627266e-03 3.63648031e-03</strong></p><p class="source-code"><strong class="bold"> 2.17934581e-03 7.69103528e-04 3.05095245e-03 2.10589729e-03</strong></p><p class="source-code"><strong class="bold"> 1.12741732e-03]</strong></p><p class="source-code"><strong class="bold">0</strong></p><p>The highest <a id="_idIndexMarker496"/>probability is at index 0: the predicted class for pixel (0,0) is class 0, the background class.</p></li>
				<li>When we're done, we delete the endpoint as follows:<p class="source-code">seg_predictor.delete_endpoint()</p></li>
			</ol>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor107"/>Summary</h1>
			<p>As you can see, these three algorithms make it easy to train CV models. Even with default hyperparameters, we get good results pretty quickly. Still, we start feeling the need to scale our training jobs. Don't worry once the relevant features have been covered in future chapters, we'll revisit some of our CV examples and we'll scale them radically!</p>
			<p>In this chapter, you learned about image classification, object detection, and semantic segmentation algorithms. You also learned how to prepare datasets in Image, RecordIO, and SageMaker Ground Truth formats. Labeling and preparing data is a critical step that takes a lot of work, and we covered it in great detail. Finally, you learned how to use the SageMaker SDK to train and deploy models with the three algorithms, as well as how to interpret results.</p>
			<p>In the next chapter, you will learn how to use built-in algorithms for natural language processing.</p>
		</div>
	</div></body></html>
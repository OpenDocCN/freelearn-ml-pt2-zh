["```py\n    get_ipython().run_line_magic('matplotlib', 'inline')\n    import matplotlib.pyplot as plt\n    import mpl_toolkits.basemap\n    import numpy\n    import pandas\n    import scipy.stats\n    import seaborn\n    import sklearn.model_selection\n    import sklearn.neighbors\n    seaborn.set()\n    ```", "```py\n    x_vec = numpy.linspace(-30, 30, 10000)[:, numpy.newaxis]\n    numpy.random.seed(42)\n    vals = numpy.concatenate(( \\\n           numpy.random.normal(loc=1, scale=2.5, size=500), \\\n           numpy.random.normal(loc=10, scale=4, size=500), \\\n           numpy.random.normal(loc=-12, scale=5, size=500) \\\n    ))[:, numpy.newaxis]\n    true_density = ((1 / 3) * scipy.stats.norm(1, 2.5)\\\n                              .pdf(x_vec[:, 0]) \\\n                    + (1 / 3) * scipy.stats.norm(10, 4)\\\n                                .pdf(x_vec[:, 0]) \\\n                    + (1 / 3) * scipy.stats.norm(-12, 5)\\\n                                .pdf(x_vec[:, 0]))\n    ```", "```py\n    position_bandwidth_vec = [(0, 0, 0.1), (0, 1, 0.4), (0, 2, 0.7), \\\n                              (1, 0, 1.0), (1, 1, 1.3), (1, 2, 1.6), \\\n                              (2, 0, 1.9), (2, 1, 2.5), (2, 2, 5.0)]\n    ```", "```py\n    fig, ax = plt.subplots(3, 3, sharex=True, \\\n                           sharey=True, figsize=(12, 9))\n    fig.suptitle('The Effect of the Bandwidth Value', fontsize=16)\n    for r, c, b in position_bandwidth_vec:\n        kde = sklearn.neighbors.KernelDensity(bandwidth=b).fit(vals)\n        log_density = kde.score_samples(x_vec)\n        ax[r, c].hist(vals, bins=50, density=True, alpha=0.5)\n        ax[r, c].plot(x_vec[:, 0], numpy.exp(log_density), \\\n                      '-', linewidth=2)\n        ax[r, c].set_title('Bandwidth = {}'.format(b))\n    plt.show()\n    ```", "```py\n    # define a grid of 100 possible bandwidth values\n    bandwidths = 10 ** numpy.linspace(-1, 1, 100)\n    # define the grid search cross validation model\n    grid = sklearn.model_selection.GridSearchCV\\\n           (estimator=sklearn.neighbors.KernelDensity(),\\\n            param_grid={\"bandwidth\": bandwidths},\\\n            cv=10)\n    # run the model on the previously defined data\n    grid.fit(vals)\n    ```", "```py\n    best_bandwidth = grid.best_params_[\"bandwidth\"]\n    print(\"Best Bandwidth Value: {}\" \\\n          .format(best_bandwidth))\n    ```", "```py\n    fig, ax = plt.subplots(figsize=(14, 10))\n    ax.hist(vals, bins=50, density=True, alpha=0.5, \\\n            label='Sampled Values')\n    ax.fill(x_vec[:, 0], true_density,\\\n            fc='black', alpha=0.3, label='True Distribution')\n    log_density = numpy.exp(grid.best_estimator_\\\n                            .score_samples(x_vec))\n    ax.plot(x_vec[:, 0], log_density,\\\n            '-', linewidth=2, label='Kernel = Gaussian')\n    ax.legend(loc='upper right')\n    plt.show()\n    ```", "```py\n    position_kernel_vec = [(0, 0, 'gaussian'), (0, 1, 'tophat'), \\\n                           (1, 0, 'epanechnikov'), \\\n                           (1, 1, 'exponential'), \\\n                           (2, 0, 'linear'), (2, 1, 'cosine'),]\n    ```", "```py\n    fig, ax = plt.subplots(3, 2, sharex=True, \\\n                           sharey=True, figsize=(12, 9))\n    fig.suptitle('The Effect of Different Kernels', fontsize=16)\n    for r, c, k in position_kernel_vec:\n        kde = sklearn.neighbors.KernelDensity(\\\n              kernel=k, bandwidth=best_bandwidth).fit(vals)\n        log_density = kde.score_samples(x_vec)\n        ax[r, c].hist(vals, bins=50, density=True, alpha=0.5)\n        ax[r, c].plot(x_vec[:, 0], numpy.exp(log_density), \\\n                      '-', linewidth=2)\n        ax[r, c].set_title('Kernel = {}'.format(k.capitalize()))\n    plt.show()\n    ```", "```py\n    def eval_gaussian(x, m, b):\n        numerator = numpy.exp(-numpy.power(x - m, 2) \\\n                              / (2 * numpy.power(b, 2)))\n        denominator = b * numpy.sqrt(2 * numpy.pi)\n        return numerator / denominator\n    ```", "```py\n    m = numpy.array([5.1])\n    b_vec = [0.1, 0.35, 0.8]\n    x_vec = numpy.linspace(1, 10, 100)[:, None]\n    figOne, ax = plt.subplots(2, 3, sharex=True, \\\n                              sharey=True, figsize=(15, 10))\n    for i, b in enumerate(b_vec):\n        ax[0, i].hist(m[:], bins=1, fc='#AAAAFF', density=True)\n        ax[0, i].set_title(\"Histogram: Normed\")\n        evaluation = eval_gaussian(x_vec, m=m[0], b=b)\n        ax[1, i].fill(x_vec, evaluation, '-k', fc='#AAAAFF')\n        ax[1, i].set_title(\"Gaussian Dist: b={}\".format(b))\n    plt.show()\n    ```", "```py\n    m = numpy.random.normal(4.7, 0.88, 16)\n    n = len(m)\n    b_vec = [0.1, 0.35, 1.1]\n    x_vec = numpy.linspace(-1, 11, 100)[:, None]\n    figMulti, ax = plt.subplots(2, 3, sharex=True, \\\n                                sharey=True, figsize=(15, 10))\n    for i, b in enumerate(b_vec):\n        ax[0, i].hist(m[:], bins=n, fc='#AAAAFF', density=True)\n        ax[0, i].set_title(\"Histogram: Normed\")\n        sum_evaluation = numpy.zeros(len(x_vec))\n        for j in range(n):\n            evaluation = eval_gaussian(x_vec, m=m[j], b=b) / n\n            sum_evaluation += evaluation[:, 0]\n            ax[1, i].plot(x_vec, evaluation, \\\n                         '-k', linestyle=\"dashed\")\n        ax[1, i].fill(x_vec, sum_evaluation, '-k', fc='#AAAAFF')\n        ax[1, i].set_title(\"Gaussian Dist: b={}\".format(b))\n    plt.show()\n    ```", "```py\n    df = pandas.read_csv('./california_housing.csv', header=0)\n    df.head()\n    ```", "```py\n    dfLess15 = df[df['HouseAge'] <= 15.0]\n    dfLess15 = dfLess15[['Latitude', 'Longitude']]\n    dfLess15.head()\n    ```", "```py\n    seaborn.jointplot(\"Longitude\", \"Latitude\", dfLess15, kind=\"kde\")\n    ```", "```py\n    dfMore40 = df[df['HouseAge'] > 40.0]\n    dfMore40 = dfMore40[['Latitude', 'Longitude']]\n    dfMore40.head()\n    ```", "```py\n    seaborn.jointplot(\"Longitude\", \"Latitude\", dfMore40, kind=\"kde\")\n    ```", "```py\n    dfLess5 = df[df['HouseAge'] <= 5]\n    x_vals = dfLess5.Population.values\n    y_vals = dfLess5.MedInc.values\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(x_vals, y_vals, c='black')\n    plt.xlabel('Population', fontsize=18)\n    plt.ylabel('Median Income', fontsize=16)\n    ```", "```py\n    fig = plt.figure(figsize=(10, 10))\n    ax = seaborn.kdeplot(x_vals, \\\n                         y_vals,\\\n                         kernel='gau',\\\n                         cmap='Blues', \\\n                         shade=True, \\\n                         shade_lowest=False)\n    plt.scatter(x_vals, y_vals, c='black', alpha=0.05)\n    plt.xlabel('Population', fontsize=18)\n    plt.ylabel('Median Income', fontsize=18)\n    plt.title('Density Estimation With Scatterplot Overlay', size=18)\n    ```", "```py\n    xgrid15 = numpy.sort(list(dfLess15['Longitude']))\n    ygrid15 = numpy.sort(list(dfLess15['Latitude']))\n    x15, y15 = numpy.meshgrid(xgrid15, ygrid15)\n    print(\"X Grid Component:\\n{}\\n\".format(x15))\n    print(\"Y Grid Component:\\n{}\\n\".format(y15))\n    xy15 = numpy.vstack([y15.ravel(), x15.ravel()]).T \n    ```", "```py\n    kde15 = sklearn.neighbors.KernelDensity(bandwidth=0.05, \\\n                                            metric='minkowski', \\\n                                            kernel='gaussian', \\\n                                            algorithm='ball_tree')\n    kde15.fit(dfLess15.values)\n    ```", "```py\n    KernelDensity(algorithm='ball_tree', atol=0, bandwidth=0.05, \\\n                  breadth_first=True, kernel='gaussian', \\\n                  leaf_size=40, metric='minkowski', \\\n                  metric_params=None, rtol=0)\n    ```", "```py\n    log_density = kde15.score_samples(xy15)\n    density = numpy.exp(log_density)\n    density = density.reshape(x15.shape)\n    print(\"Shape of Density Values:\\n{}\\n\".format(density.shape))\n    ```", "```py\n    Shape of Density Values:\n    (3287, 3287)\n    ```", "```py\n    Exercise9.01-Exercise9.06.ipynb\n    fig15 = plt.figure(figsize=(10, 10))\n    fig15.suptitle(\\\n        \"\"\"\n        Density Estimation:\n        Location of Housing Blocks\n        Where the Median Home Age <= 15 Years\n        \"\"\",\\\n        fontsize=16)\n    The complete code for this step can be found at https://packt.live/38DbmTo.\n    ```", "```py\n    xgrid40 = numpy.sort(list(dfMore40['Longitude']))\n    ygrid40 = numpy.sort(list(dfMore40['Latitude']))\n    x40, y40 = numpy.meshgrid(xgrid40, ygrid40)\n    print(\"X Grid Component:\\n{}\\n\".format(x40))\n    print(\"Y Grid Component:\\n{}\\n\".format(y40))\n    xy40 = numpy.vstack([y40.ravel(), x40.ravel()]).T \n    ```", "```py\n    kde40 = sklearn.neighbors.KernelDensity(bandwidth=0.05, \\\n                                            metric='minkowski', \\\n                                            kernel='gaussian', \\\n                                            algorithm='ball_tree')\n    kde40.fit(dfMore40.values)\n    log_density = kde40.score_samples(xy40)\n    density = numpy.exp(log_density)\n    density = density.reshape(x40.shape)\n    print(\"Shape of Density Values:\\n{}\\n\".format(density.shape))\n    ```", "```py\nExercise9.01-Exercise9.06.ipynb\nfig40 = plt.figure(figsize=(10, 10))\nfig40.suptitle(\\\n    \"\"\"\n    Density Estimation:\n    Location of Housing Blocks\n    Where the Median Home Age > 40 Years\n    \"\"\", \\\n    fontsize=16)\nThe complete code for this step can be found at https://packt.live/38DbmTo.\n```"]
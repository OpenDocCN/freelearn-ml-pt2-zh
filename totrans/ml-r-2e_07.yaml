- en: Chapter 7. Black Box Methods – Neural Networks and Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The late science fiction author Arthur C. Clarke wrote, "any sufficiently advanced
    technology is indistinguishable from magic."This chapter covers a pair of machine
    learning methods that may appear at first glance to be magic. Though they are
    extremely powerful, their inner workings can be difficult to understand.
  prefs: []
  type: TYPE_NORMAL
- en: In engineering, these are referred to as **black box** processes because the
    mechanism that transforms the input into the output is obfuscated by an imaginary
    box. For instance, the black box of closed-source software intentionally conceals
    proprietary algorithms, the black box of political lawmaking is rooted in the
    bureaucratic processes, and the black box of sausage-making involves a bit of
    purposeful (but tasty) ignorance. In the case of machine learning, the black box
    is due to the complex mathematics allowing them to function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although they may not be easy to understand, it is dangerous to apply black
    box models blindly. Thus, in this chapter, we''ll peek inside the box and investigate
    the statistical sausage-making involved in fitting such models. You''ll discover:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks mimic the structure of animal brains to model arbitrary functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines use multidimensional surfaces to define the relationship
    between features and outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite their complexity, these can be applied easily to real-world problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With any luck, you'll realize that you don't need a black belt in statistics
    to tackle black box machine's learning methods—there's no need to be intimidated!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **Artificial Neural Network** (**ANN**) models the relationship between a
    set of input signals and an output signal using a model derived from our understanding
    of how a biological brain responds to stimuli from sensory inputs. Just as a brain
    uses a network of interconnected cells called **neurons** to create a massive
    parallel processor, ANN uses a network of artificial neurons or **nodes** to solve
    learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: The human brain is made up of about 85 billion neurons, resulting in a network
    capable of representing a tremendous amount of knowledge. As you might expect,
    this dwarfs the brains of other living creatures. For instance, a cat has roughly
    a billion neurons, a mouse has about 75 million neurons, and a cockroach has only
    about a million neurons. In contrast, many ANNs contain far fewer neurons, typically
    only several hundred, so we're in no danger of creating an artificial brain anytime
    in the near future—even a fruit fly brain with 100,000 neurons far exceeds the
    current state-of-the-art ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Though it may be unfeasible to completely model a cockroach's brain, a neural
    network may still provide an adequate heuristic model of its behavior. Suppose
    that we develop an algorithm that can mimic how a roach flees when discovered.
    If the behavior of the robot roach is convincing, does it matter whether its brain
    is as sophisticated as the living creature's? This question is the basis of the
    controversial **Turing test**, proposed in 1950 by the pioneering computer scientist
    Alan Turing, proposed in 1950 by the pioneering computer scientist Alan Turing,
    which grades a machine as intelligent if a human being cannot distinguish its
    behavior from a living creature's.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rudimentary ANNs have been used for over 50 years to simulate the brain''s
    approach to problem-solving. At first, this involved learning simple functions
    like the logical AND function or the logical OR function. These early exercises
    were used primarily to help scientists understand how biological brains might
    operate. However, as computers have become increasingly powerful in the recent
    years, the complexity of ANNs has likewise increased so much that they are now
    frequently applied to more practical problems including:'
  prefs: []
  type: TYPE_NORMAL
- en: Speech and handwriting recognition programs like those used by voicemail transcription
    services and postal mail sorting machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The automation of smart devices like an office building's environmental controls
    or self-driving cars and self-piloting drones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sophisticated models of weather and climate patterns, tensile strength, fluid
    dynamics, and many other scientific, social, or economic phenomena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Broadly speaking, ANNs are versatile learners that can be applied to nearly
    any learning task: classification, numeric prediction, and even unsupervised pattern
    recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether deserving or not, ANN learners are often reported in the media with
    great fanfare. For instance, an "*artificial brain*" developed by Google was recently
    touted for its ability to identify cat videos on YouTube. Such hype may have less
    to do with anything unique to ANNs and more to do with the fact that ANNs are
    captivating because of their similarities to living minds.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are best applied to problems where the input data and output data are well-defined
    or at least fairly simple, yet the process that relates the input to output is
    extremely complex. As a black box method, they work well for these types of black
    box problems.
  prefs: []
  type: TYPE_NORMAL
- en: From biological to artificial neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because ANNs were intentionally designed as conceptual models of human brain
    activity, it is helpful to first understand how biological neurons function. As
    illustrated in the following figure, incoming signals are received by the cell's
    **dendrites** through a biochemical process. The process allows the impulse to
    be weighted according to its relative importance or frequency. As the **cell body**
    begins accumulating the incoming signals, a threshold is reached at which the
    cell fires and the output signal is transmitted via an electrochemical process
    down the **axon**. At the axon's terminals, the electric signal is again processed
    as a chemical signal to be passed to the neighboring neurons across a tiny gap
    known as a **synapse**.
  prefs: []
  type: TYPE_NORMAL
- en: '![From biological to artificial neurons](img/3905_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The model of a single artificial neuron can be understood in terms very similar
    to the biological model. As depicted in the following figure, a directed network
    diagram defines a relationship between the input signals received by the dendrites
    (*x* variables), and the output signal (*y* variable). Just as with the biological
    neuron, each dendrite''s signal is weighted (*w* values) according to its importance—ignore,
    for now, how these weights are determined. The input signals are summed by the
    cell body and the signal is passed on according to an **activation function**
    denoted by *f*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![From biological to artificial neurons](img/3905_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A typical artificial neuron with *n* input dendrites can be represented by
    the formula that follows. The *w* weights allow each of the *n* inputs (denoted
    by *x[i]*) to contribute a greater or lesser amount to the sum of input signals.
    The net total is used by the activation function *f(x)*, and the resulting signal,
    *y(x)*, is the output axon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![From biological to artificial neurons](img/3905_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Neural networks use neurons defined this way as building blocks to construct
    complex models of data. Although there are numerous variants of neural networks,
    each can be defined in terms of the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: An **activation function**, which transforms a neuron's combined input signals
    into a single output signal to be broadcasted further in the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **network topology** (or architecture), which describes the number of neurons
    in the model as well as the number of layers and manner in which they are connected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **training algorithm** that specifies how connection weights are set in
    order to inhibit or excite neurons in proportion to the input signal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take a look at some of the variations within each of these categories
    to see how they can be used to construct typical neural network models.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The activation function is the mechanism by which the artificial neuron processes
    incoming information and passes it throughout the network. Just as the artificial
    neuron is modeled after the biological version, so is the activation function
    modeled after nature's design.
  prefs: []
  type: TYPE_NORMAL
- en: In the biological case, the activation function could be imagined as a process
    that involves summing the total input signal and determining whether it meets
    the firing threshold. If so, the neuron passes on the signal; otherwise, it does
    nothing. In ANN terms, this is known as a **threshold activation function**, as
    it results in an output signal only once a specified input threshold has been
    attained.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure depicts a typical threshold function; in this case, the
    neuron fires when the sum of input signals is at least zero. Because its shape
    resembles a stair, it is sometimes called a **unit step activation function**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Activation functions](img/3905_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although the threshold activation function is interesting due to its parallels
    with biology, it is rarely used in artificial neural networks. Freed from the
    limitations of biochemistry, the ANN activation functions can be chosen based
    on their ability to demonstrate desirable mathematical characteristics and accurately
    model relationships among data.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most commonly used alternative is the **sigmoid activation function**
    (more specifically, the *logistic* sigmoid) shown in the following figure. Note
    that in the formula shown, *e* is the base of the natural logarithm (approximately
    2.72). Although it shares a similar step or "S" shape with the threshold activation
    function, the output signal is no longer binary; output values can fall anywhere
    in the range from 0 to 1\. Additionally, the sigmoid is **differentiable**, which
    means that it is possible to calculate the derivative across the entire range
    of inputs. As you will learn later, this feature is crucial to create efficient
    ANN optimization algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Activation functions](img/3905_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Although sigmoid is perhaps the most commonly used activation function and
    is often used by default, some neural network algorithms allow a choice of alternatives.
    A selection of such activation functions is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Activation functions](img/3905_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The primary detail that differentiates these activation functions is the output
    signal range. Typically, this is one of (0, 1), (-1, +1), or (-inf, +inf). The
    choice of activation function biases the neural network such that it may fit certain
    types of data more appropriately, allowing the construction of specialized neural
    networks. For instance, a linear activation function results in a neural network
    very similar to a linear regression model, while a Gaussian activation function
    results in a model called a **Radial Basis Function** (**RBF**) network. Each
    of these has strengths better suited for certain learning tasks and not others.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to recognize that for many of the activation functions, the range
    of input values that affect the output signal is relatively narrow. For example,
    in the case of sigmoid, the output signal is always nearly 0 or 1 for an input
    signal below *-5* or above *+5*, respectively. The compression of signal in this
    way results in a saturated signal at the high and low ends of very dynamic inputs,
    just as turning a guitar amplifier up too high results in a distorted sound due
    to clipping of the peaks of sound waves. Because this essentially squeezes the
    input values into a smaller range of outputs, activation functions like the sigmoid
    are sometimes called **squashing functions**.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to the squashing problem is to transform all neural network inputs
    such that the features' values fall within a small range around 0\. Typically,
    this involves standardizing or normalizing the features. By restricting the range
    of input values, the activation function will have action across the entire range,
    preventing large-valued features such as household income from dominating small-valued
    features such as the number of children in the household. A side benefit is that
    the model may also be faster to train, since the algorithm can iterate more quickly
    through the actionable range of input values.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although theoretically a neural network can adapt to a very dynamic feature
    by adjusting its weight over many iterations. In extreme cases, many algorithms
    will stop iterating long before this occurs. If your model is making predictions
    that do not make sense, double-check whether you've correctly standardized the
    input data.
  prefs: []
  type: TYPE_NORMAL
- en: Network topology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ability of a neural network to learn is rooted in its **topology**, or
    the patterns and structures of interconnected neurons. Although there are countless
    forms of network architecture, they can be differentiated by three key characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether information in the network is allowed to travel backward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of nodes within each layer of the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The topology determines the complexity of tasks that can be learned by the network.
    Generally, larger and more complex networks are capable of identifying more subtle
    patterns and complex decision boundaries. However, the power of a network is not
    only a function of the network size, but also the way units are arranged.
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To define topology, we need a terminology that distinguishes artificial neurons
    based on their position in the network. The figure that follows illustrates the
    topology of a very simple network. A set of neurons called **input nodes** receives
    unprocessed signals directly from the input data. Each input node is responsible
    for processing a single feature in the dataset; the feature's value will be transformed
    by the corresponding node's activation function. The signals sent by the input
    nodes are received by the output node, which uses its own activation function
    to generate a final prediction (denoted here as *p*).
  prefs: []
  type: TYPE_NORMAL
- en: The input and output nodes are arranged in groups known as **layers**. Because
    the input nodes process the incoming data exactly as it is received, the network
    has only one set of connection weights (labeled here as *w[1]*, *w[2]*, and *w[3]*).
    It is therefore termed a **single-layer network**. Single-layer networks can be
    used for basic pattern classification, particularly for patterns that are linearly
    separable, but more sophisticated networks are required for most learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![The number of layers](img/3905_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you might expect, an obvious way to create more complex networks is by adding
    additional layers. As depicted here, a **multilayer network** adds one or more
    **hidden layers** that process the signals from the input nodes prior to it reaching
    the output node. Most multilayer networks are **fully connected**, which means
    that every node in one layer is connected to every node in the next layer, but
    this is not required.
  prefs: []
  type: TYPE_NORMAL
- en: '![The number of layers](img/3905_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The direction of information travel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed that in the prior examples, arrowheads were used to indicate
    signals traveling in only one direction. Networks in which the input signal is
    fed continuously in one direction from connection to connection until it reaches
    the output layer are called **feedforward** networks.
  prefs: []
  type: TYPE_NORMAL
- en: In spite of the restriction on information flow, feedforward networks offer
    a surprising amount of flexibility. For instance, the number of levels and nodes
    at each level can be varied, multiple outcomes can be modeled simultaneously,
    or multiple hidden layers can be applied. A neural network with multiple hidden
    layers is called a **Deep Neural Network** (**DNN**) and the practice of training
    such network is sometimes referred to as **deep learning**.
  prefs: []
  type: TYPE_NORMAL
- en: '![The direction of information travel](img/B03905_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In contrast, a **recurrent network** (or **feedback network**) allows signals
    to travel in both directions using loops. This property, which more closely mirrors
    how a biological neural network works, allows extremely complex patterns to be
    learned. The addition of a short-term memory, or **delay**, increases the power
    of recurrent networks immensely. Notably, this includes the capability to understand
    the sequences of events over a period of time. This could be used for stock market
    prediction, speech comprehension, or weather forecasting. A simple recurrent network
    is depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The direction of information travel](img/3905_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In spite of their potential, recurrent networks are still largely theoretical
    and are rarely used in practice. On the other hand, feedforward networks have
    been extensively applied to real-world problems. In fact, the multilayer feedforward
    network, sometimes called the **Multilayer Perceptron** (**MLP**), is the de facto
    standard ANN topology. If someone mentions that they are fitting a neural network,
    they are most likely referring to a MLP.
  prefs: []
  type: TYPE_NORMAL
- en: The number of nodes in each layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the variations in the number of layers and the direction of information
    travel, neural networks can also vary in complexity by the number of nodes in
    each layer. The number of input nodes is predetermined by the number of features
    in the input data. Similarly, the number of output nodes is predetermined by the
    number of outcomes to be modeled or the number of class levels in the outcome.
    However, the number of hidden nodes is left to the user to decide prior to training
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there is no reliable rule to determine the number of neurons
    in the hidden layer. The appropriate number depends on the number of input nodes,
    the amount of training data, the amount of noisy data, and the complexity of the
    learning task, among many other factors.
  prefs: []
  type: TYPE_NORMAL
- en: In general, more complex network topologies with a greater number of network
    connections allow the learning of more complex problems. A greater number of neurons
    will result in a model that more closely mirrors the training data, but this runs
    a risk of overfitting; it may generalize poorly to future data. Large neural networks
    can also be computationally expensive and slow to train.
  prefs: []
  type: TYPE_NORMAL
- en: The best practice is to use the fewest nodes that result in adequate performance
    in a validation dataset. In most cases, even with only a small number of hidden
    nodes—often as few as a handful—the neural network can offer a tremendous amount
    of learning ability.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It has been proven that a neural network with at least one hidden layer of sufficient
    neurons is a **universal function approximator**. This means that neural networks
    can be used to approximate any continuous function to an arbitrary precision over
    a finite interval.
  prefs: []
  type: TYPE_NORMAL
- en: Training neural networks with backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The network topology is a blank slate that by itself has not learned anything.
    Like a newborn child, it must be trained with experience. As the neural network
    processes the input data, connections between the neurons are strengthened or
    weakened, similar to how a baby's brain develops as he or she experiences the
    environment. The network's connection weights are adjusted to reflect the patterns
    observed over time.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network by adjusting connection weights is very computationally
    intensive. Consequently, though they had been studied for decades prior, ANNs
    were rarely applied to real-world learning tasks until the mid-to-late 1980s,
    when an efficient method of training an ANN was discovered. The algorithm, which
    used a strategy of back-propagating errors, is now known simply as **backpropagation**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Coincidentally, several research teams independently discovered and published
    the backpropagation algorithm around the same time. Among them, perhaps the most
    often cited work is: Rumelhart DE, Hinton GE, Williams RJ. Learning representations
    by back-propagating errors. *Nature*. 1986; 323:533-566.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although still notoriously slow relative to many other machine learning algorithms,
    the backpropagation method led to a resurgence of interest in ANNs. As a result,
    multilayer feedforward networks that use the backpropagation algorithm are now
    common in the field of data mining. Such models offer the following strengths
    and weaknesses:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Can be adapted to classification or numeric prediction problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capable of modeling more complex patterns than nearly any algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makes few assumptions about the data's underlying relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Extremely computationally intensive and slow to train, particularly if the network
    topology is complex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very prone to overfitting training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results in a complex black box model that is difficult, if not impossible, to
    interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'In its most general form, the backpropagation algorithm iterates through many
    cycles of two processes. Each cycle is known as an **epoch**. Because the network
    contains no *a priori* (existing) knowledge, the starting weights are typically
    set at random. Then, the algorithm iterates through the processes, until a stopping
    criterion is reached. Each epoch in the backpropagation algorithm includes:'
  prefs: []
  type: TYPE_NORMAL
- en: A **forward phase** in which the neurons are activated in sequence from the
    input layer to the output layer, applying each neuron's weights and activation
    function along the way. Upon reaching the final layer, an output signal is produced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **backward phase** in which the network's output signal resulting from the
    forward phase is compared to the true target value in the training data. The difference
    between the network's output signal and the true value results in an error that
    is propagated backwards in the network to modify the connection weights between
    neurons and reduce future errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Over time, the network uses the information sent backward to reduce the total
    error of the network. Yet one question remains: because the relationship between
    each neuron''s inputs and outputs is complex, how does the algorithm determine
    how much a weight should be changed? The answer to this question involves a technique
    called **gradient descent**. Conceptually, it works similarly to how an explorer
    trapped in the jungle might find a path to water. By examining the terrain and
    continually walking in the direction with the greatest downward slope, the explorer
    will eventually reach the lowest valley, which is likely to be a riverbed.'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar process, the backpropagation algorithm uses the derivative of each
    neuron's activation function to identify the gradient in the direction of each
    of the incoming weights—hence the importance of having a differentiable activation
    function. The gradient suggests how steeply the error will be reduced or increased
    for a change in the weight. The algorithm will attempt to change the weights that
    result in the greatest reduction in error by an amount known as the **learning
    rate**. The greater the learning rate, the faster the algorithm will attempt to
    descend down the gradients, which could reduce the training time at the risk of
    overshooting the valley.
  prefs: []
  type: TYPE_NORMAL
- en: '![Training neural networks with backpropagation](img/B03905_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although this process seems complex, it is easy to apply in practice. Let's
    apply our understanding of multilayer feedforward networks to a real-world problem.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Modeling the strength of concrete with ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the field of engineering, it is crucial to have accurate estimates of the
    performance of building materials. These estimates are required in order to develop
    safety guidelines governing the materials used in the construction of buildings,
    bridges, and roadways.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the strength of concrete is a challenge of particular interest. Although
    it is used in nearly every construction project, concrete performance varies greatly
    due to a wide variety of ingredients that interact in complex ways. As a result,
    it is difficult to accurately predict the strength of the final product. A model
    that could reliably predict concrete strength given a listing of the composition
    of the input materials could result in safer construction practices.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this analysis, we will utilize data on the compressive strength of concrete
    donated to the UCI Machine Learning Data Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))
    by I-Cheng Yeh. As he found success using neural networks to model these data,
    we will attempt to replicate his work using a simple neural network model in R.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information on Yeh''s approach to this learning task, refer to: Yeh
    IC. Modeling of strength of high performance concrete using artificial neural
    networks. *Cement and Concrete Research*. 1998; 28:1797-1808.'
  prefs: []
  type: TYPE_NORMAL
- en: According to the website, the concrete dataset contains 1,030 examples of concrete
    with eight features describing the components used in the mixture. These features
    are thought to be related to the final compressive strength and they include the
    amount (in kilograms per cubic meter) of cement, slag, ash, water, superplasticizer,
    coarse aggregate, and fine aggregate used in the product in addition to the aging
    time (measured in days).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To follow along with this example, download the `concrete.csv` file from the
    Packt Publishing website and save it to your R working directory.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we''ll begin our analysis by loading the data into an R object using
    the `read.csv()` function, and confirming that it matches the expected structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The nine variables in the data frame correspond to the eight features and one
    outcome we expected, although a problem has become apparent. Neural networks work
    best when the input data are scaled to a narrow range around zero, and here, we
    see values ranging anywhere from zero up to over a thousand.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the solution to this problem is to rescale the data with a normalizing
    or standardization function. If the data follow a bell-shaped curve (a normal
    distribution as described in [Chapter 2](ch02.html "Chapter 2. Managing and Understanding
    Data"), *Managing and Understanding Data*), then it may make sense to use standardization
    via R's built-in scale() function. On the other hand, if the data follow a uniform
    distribution or are severely nonnormal, then normalization to a 0-1 range may
    be more appropriate. In this case, we'll use the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html "Chapter 3. Lazy Learning – Classification Using Nearest
    Neighbors"), *Lazy Learning – Classification Using Nearest Neighbors*, we defined
    our own `normalize()` function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing this code, our `normalize()` function can be applied to every
    column in the concrete data frame using the `lapply()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that the normalization worked, we can see that the minimum and maximum
    strength are now 0 and 1, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In comparison, the original minimum and maximum values were 2.33 and 82.60:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any transformation applied to the data prior to training the model will have
    to be applied in reverse later on, in order to convert back to the original units
    of measurement. To facilitate the rescaling, it is wise to save the original data
    or at least the summary statistics of the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following Yeh''s precedent in the original publication, we will partition the
    data into a training set with 75 percent of the examples and a testing set with
    25 percent. The CSV file we used was already sorted in random order, so we simply
    need to divide it into two portions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We'll use the training dataset to build the neural network and the testing dataset
    to evaluate how well the model generalizes to future results. As it is easy to
    overfit a neural network, this step is very important.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To model the relationship between the ingredients used in concrete and the strength
    of the finished product, we will use a multilayer feedforward neural network.
    The `neuralnet` package by Stefan Fritsch and Frauke Guenther provides a standard
    and easy-to-use implementation of such networks. It also offers a function to
    plot the network topology. For these reasons, the `neuralnet` implementation is
    a strong choice for learning more about neural networks, though this is not to
    say that it cannot be used to accomplish real work as well—it's quite a powerful
    tool, as you will soon see.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several other commonly used packages to train ANN models in R, each
    with unique strengths and weaknesses. Because it ships as a part of the standard
    R installation, the `nnet` package is perhaps the most frequently cited ANN implementation.
    It uses a slightly more sophisticated algorithm than standard backpropagation.
    Another strong option is the `RSNNS` package, which offers a complete suite of
    neural network functionality with the downside being that it is more difficult
    to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'As `neuralnet` is not included in base R, you will need to install it by typing
    `install.packages("neuralnet")` and load it with the `library(neuralnet)` command.
    The included `neuralnet()` function can be used for training neural networks for
    numeric prediction using the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 3 – training a model on the data](img/3905_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll begin by training the simplest multilayer feedforward network with only
    a single hidden node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then visualize the network topology using the `plot()` function on the
    resulting model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Step 3 – training a model on the data](img/3905_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this simple model, there is one input node for each of the eight features,
    followed by a single hidden node and a single output node that predicts the concrete
    strength. The weights for each of the connections are also depicted, as are the
    **bias terms** (indicated by the nodes labeled with the number **1**). The bias
    terms are numeric constants that allow the value at the indicated nodes to be
    shifted upward or downward, much like the intercept in a linear equation.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A neural network with a single hidden node can be thought of as a distant cousin
    of the linear regression models we studied in [Chapter 6](ch06.html "Chapter 6. Forecasting
    Numeric Data – Regression Methods"), *Forecasting Numeric Data – Regression Methods*.
    The weight between each input node and the hidden node is similar to the regression
    coefficients, and the weight for the bias term is similar to the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom of the figure, R reports the number of training steps and an error
    measure called the **Sum of Squared Errors** (**SSE**), which as you might expect,
    is the sum of the squared predicted minus actual values. A lower SSE implies better
    predictive performance. This is helpful for estimating the model's performance
    on the training data, but tells us little about how it will perform on unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The network topology diagram gives us a peek into the black box of the ANN,
    but it doesn''t provide much information about how well the model fits future
    data. To generate predictions on the test dataset, we can use the `compute()`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `compute()` function works a bit differently from the `predict()` functions
    we''ve used so far. It returns a list with two components: `$neurons`, which stores
    the neurons for each layer in the network, and `$net.result`, which stores the
    predicted values. We''ll want the latter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Because this is a numeric prediction problem rather than a classification problem,
    we cannot use a confusion matrix to examine model accuracy. Instead, we must measure
    the correlation between our predicted concrete strength and the true value. This
    provides insight into the strength of the linear association between the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the `cor()` function is used to obtain a correlation between two
    numeric vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don't be alarmed if your result differs. Because the neural network begins with
    random weights, the predictions can vary from model to model. If you'd like to
    match these results exactly, try using `set.seed(12345)` before building the neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Correlations close to 1 indicate strong linear relationships between two variables.
    Therefore, the correlation here of about 0.806 indicates a fairly strong relationship.
    This implies that our model is doing a fairly good job, even with only a single
    hidden node.
  prefs: []
  type: TYPE_NORMAL
- en: Given that we only used one hidden node, it is likely that we can improve the
    performance of our model. Let's try to do a bit better.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As networks with more complex topologies are capable of learning more difficult
    concepts, let''s see what happens when we increase the number of hidden nodes
    to five. We use the `neuralnet()` function as before, but add the `hidden = 5`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the network again, we see a drastic increase in the number of connections.
    We can see how this impacted the performance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Step 5 – improving model performance](img/3905_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the reported error (measured again by SSE) has been reduced from
    5.08 in the previous model to 1.63 here. Additionally, the number of training
    steps rose from 4,882 to 86,849, which should come as no surprise given how much
    more complex the model has become. More complex networks take many more iterations
    to find the optimal weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the same steps to compare the predicted values to the true values,
    we now obtain a correlation around 0.92, which is a considerable improvement over
    the previous result of 0.80 with a single hidden node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, in the original publication, Yeh reported a mean correlation
    of 0.885 using a very similar neural network. This means that with relatively
    little effort, we were able to match the performance of a subject-matter expert.
    If you'd like more practice with neural networks, you might try applying the principles
    learned earlier in this chapter to see how it impacts model performance. Perhaps
    try using different numbers of hidden nodes, applying different activation functions,
    and so on. The `?neuralnet` help page provides more information on the various
    parameters that can be adjusted.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Support Vector Machine** (**SVM**) can be imagined as a surface that creates
    a boundary between points of data plotted in multidimensional that represent examples
    and their feature values. The goal of a SVM is to create a flat boundary called
    a **hyperplane**, which divides the space to create fairly homogeneous partitions
    on either side. In this way, the SVM learning combines aspects of both the instance-based
    nearest neighbor learning presented in [Chapter 3](ch03.html "Chapter 3. Lazy
    Learning – Classification Using Nearest Neighbors"), *Lazy Learning – Classification
    Using Nearest Neighbors*, and the linear regression modeling described in [Chapter
    6](ch06.html "Chapter 6. Forecasting Numeric Data – Regression Methods"), *Forecasting
    Numeric Data – Regression Methods*. The combination is extremely powerful, allowing
    SVMs to model highly complex relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Although the basic mathematics that drive SVMs have been around for decades,
    they have recently exploded in popularity. This is, of course, rooted in their
    state-of-the-art performance, but perhaps also due to the fact that award winning
    SVM algorithms have been implemented in several popular and well-supported libraries
    across many programming languages, including R. SVMs have thus been adopted by
    a much wider audience, might have otherwise been unable to apply the somewhat
    complex math needed to implement a SVM. The good news is that although the math
    may be difficult, the basic concepts are understandable.
  prefs: []
  type: TYPE_NORMAL
- en: 'SVMs can be adapted for use with nearly any type of learning task, including
    both classification and numeric prediction. Many of the algorithm''s key successes
    have come in pattern recognition. Notable applications include:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification of microarray gene expression data in the field of bioinformatics
    to identify cancer or other genetic diseases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text categorization such as identification of the language used in a document
    or the classification of documents by subject matter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The detection of rare yet important events like combustion engine failure, security
    breaches, or earthquakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMs are most easily understood when used for binary classification, which is
    how the method has been traditionally applied. Therefore, in the remaining sections,
    we will focus only on SVM classifiers. Don't worry, however, as the same principles
    you learn here will apply while adapting SVMs to other learning tasks such as
    numeric prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Classification with hyperplanes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As noted previously, SVMs use a boundary called a hyperplane to partition data
    into groups of similar class values. For example, the following figure depicts
    hyperplanes that separate groups of circles and squares in two and three dimensions.
    Because the circles and squares can be separated perfectly by the straight line
    or flat surface, they are said to be **linearly separable**. At first, we'll consider
    only the simple case where this is true, but SVMs can also be extended to problems
    where the points are not linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification with hyperplanes](img/3905_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For convenience, the hyperplane is traditionally depicted as a line in 2D space,
    but this is simply because it is difficult to illustrate space in greater than
    two dimensions. In reality, the hyperplane is a flat surface in a high-dimensional
    space—a concept that can be difficult to get your mind around.
  prefs: []
  type: TYPE_NORMAL
- en: In two dimensions, the task of the SVM algorithm is to identify a line that
    separates the two classes. As shown in the following figure, there is more than
    one choice of dividing line between the groups of circles and squares. Three such
    possibilities are labeled **a**, **b**, and **c**. How does the algorithm choose?
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification with hyperplanes](img/3905_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The answer to that question involves a search for the **Maximum Margin Hyperplane**
    (**MMH**) that creates the greatest separation between the two classes. Although
    any of the three lines separating the circles and squares would correctly classify
    all the data points, it is likely that the line that leads to the greatest separation
    will generalize the best to the future data. The maximum margin will improve the
    chance that, in spite of random noise, the points will remain on the correct side
    of the boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The **support vectors** (indicated by arrows in the figure that follows) are
    the points from each class that are the closest to the MMH; each class must have
    at least one support vector, but it is possible to have more than one. Using the
    support vectors alone, it is possible to define the MMH. This is a key feature
    of SVMs; the support vectors provide a very compact way to store a classification
    model, even if the number of features is extremely large.
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification with hyperplanes](img/3905_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm to identify the support vectors relies on vector geometry and
    involves some fairly tricky math that is outside the scope of this book. However,
    the basic principles of the process are fairly straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information on the mathematics of SVMs can be found in the classic paper:
    Cortes C, Vapnik V. Support-vector network. *Machine Learning*. 1995; 20:273-297\.
    A beginner level discussion can be found in: Bennett KP, Campbell C. Support vector
    machines: hype or hallelujah. SIGKDD Explorations. 2003; 2:1-13\. A more in-depth
    look can be found in: Steinwart I, Christmann A. *Support Vector Machines*. New
    York: Springer; 2008.'
  prefs: []
  type: TYPE_NORMAL
- en: The case of linearly separable data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is easiest to understand how to find the maximum margin under the assumption
    that the classes are linearly separable. In this case, the MMH is as far away
    as possible from the outer boundaries of the two groups of data points. These
    outer boundaries are known as the **convex hull**. The MMH is then the perpendicular
    bisector of the shortest line between the two convex hulls. Sophisticated computer
    algorithms that use a technique known as **quadratic optimization** are capable
    of finding the maximum margin in this way.
  prefs: []
  type: TYPE_NORMAL
- en: '![The case of linearly separable data](img/3905_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An alternative (but equivalent) approach involves a search through the space
    of every possible hyperplane in order to find a set of two parallel planes that
    divide the points into homogeneous groups yet themselves are as far apart as possible.
    To use a metaphor, one can imagine this process as similar to trying to find the
    thickest mattress that can fit up a stairwell to your bedroom.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this search process, we''ll need to define exactly what we mean
    by a hyperplane. In *n*-dimensional space, the following equation is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The case of linearly separable data](img/3905_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you aren't familiar with this notation, the arrows above the letters indicate
    that they are vectors rather than single numbers. In particular, *w* is a vector
    of *n* weights, that is, *{w[1], w[2], ..., w[n]}*, and *b* is a single number
    known as the **bias**. The bias is conceptually equivalent to the intercept term
    in the slope-intercept form discussed in [Chapter 6](ch06.html "Chapter 6. Forecasting
    Numeric Data – Regression Methods"), *Forecasting Numeric Data – Regression Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you're having trouble imagining the plane, don't worry about the details.
    Simply think of the equation as a way to specify a surface, much like when the
    slope-intercept form (*y = mx + b*) is used to specify lines in 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this formula, the goal of the process is to find a set of weights that
    specify two hyperplanes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The case of linearly separable data](img/3905_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will also require that these hyperplanes are specified such that all the
    points of one class fall above the first hyperplane and all the points of the
    other class fall beneath the second hyperplane. This is possible so long as the
    data are linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector geometry defines the distance between these two planes as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The case of linearly separable data](img/3905_07_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *||w||* indicates the **Euclidean norm** (the distance from the origin
    to vector *w*). Because *||w||* is in the denominator, to maximize distance, we
    need to minimize *||w||*. The task is typically reexpressed as a set of constraints,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The case of linearly separable data](img/3905_07_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although this looks messy, it's really not too complicated to understand conceptually.
    Basically, the first line implies that we need to minimize the Euclidean norm
    (squared and divided by two to make the calculation easier). The second line notes
    that this is subject to (*s.t.*), the condition that each of the *y[i]* data points
    is correctly classified. Note that *y* indicates the class value (transformed
    to either +1 or -1) and the upside down "A" is shorthand for "for all."
  prefs: []
  type: TYPE_NORMAL
- en: As with the other method for finding the maximum margin, finding a solution
    to this problem is a task best left for quadratic optimization software. Although
    it can be processor-intensive, specialized algorithms are capable of solving these
    problems quickly even on fairly large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The case of nonlinearly separable data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we''ve worked through the theory behind SVMs, you may be wondering about
    the elephant in the room: what happens if the data are not linearly separable?
    The solution to this problem is the use of a **slack variable**, which creates
    a soft margin that allows some points to fall on the incorrect side of the margin.
    The figure that follows illustrates two points falling on the wrong side of the
    line with the corresponding slack terms (denoted with the Greek letter Xi):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The case of nonlinearly separable data](img/3905_07_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A cost value (denoted as *C*) is applied to all points that violate the constraints,
    and rather than finding the maximum margin, the algorithm attempts to minimize
    the total cost. We can therefore revise the optimization problem to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The case of nonlinearly separable data](img/3905_07_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you're still confused, don't worry, you're not alone. Luckily, SVM packages
    will happily optimize this for you without you having to understand the technical
    details. The important piece to understand is the addition of the cost parameter
    *C*. Modifying this value will adjust the penalty, for example, the fall on the
    wrong side of the hyperplane. The greater the cost parameter, the harder the optimization
    will try to achieve 100 percent separation. On the other hand, a lower cost parameter
    will place the emphasis on a wider overall margin. It is important to strike a
    balance between these two in order to create a model that generalizes well to
    future data.
  prefs: []
  type: TYPE_NORMAL
- en: Using kernels for non-linear spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many real-world applications, the relationships between variables are nonlinear.
    As we just discovered, a SVM can still be trained on such data through the addition
    of a slack variable, which allows some examples to be misclassified. However,
    this is not the only way to approach the problem of nonlinearity. A key feature
    of SVMs is their ability to map the problem into a higher dimension space using
    a process known as the **kernel trick**. In doing so, a nonlinear relationship
    may suddenly appear to be quite linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though this seems like nonsense, it is actually quite easy to illustrate by
    example. In the following figure, the scatterplot on the left depicts a nonlinear
    relationship between a weather class (sunny or snowy) and two features: latitude
    and longitude. The points at the center of the plot are members of the snowy class,
    while the points at the margins are all sunny. Such data could have been generated
    from a set of weather reports, some of which were obtained from stations near
    the top of a mountain, while others were obtained from stations around the base
    of the mountain.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernels for non-linear spaces](img/3905_07_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the right side of the figure, after the kernel trick has been applied, we
    look at the data through the lens of a new dimension: altitude. With the addition
    of this feature, the classes are now perfectly linearly separable. This is possible
    because we have obtained a new perspective on the data. In the left figure, we
    are viewing the mountain from a bird''s eye view, while in the right one, we are
    viewing the mountain from a distance at the ground level. Here, the trend is obvious:
    snowy weather is found at higher altitudes.'
  prefs: []
  type: TYPE_NORMAL
- en: SVMs with nonlinear kernels add additional dimensions to the data in order to
    create separation in this way. Essentially, the kernel trick involves a process
    of constructing new features that express mathematical relationships between measured
    characteristics. For instance, the altitude feature can be expressed mathematically
    as an interaction between latitude and longitude—the closer the point is to the
    center of each of these scales, the greater the altitude. This allows SVM to learn
    concepts that were not explicitly measured in the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 'SVMs with nonlinear kernels are extremely powerful classifiers, although they
    do have some downsides, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strengths | Weaknesses |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Can be used for classification or numeric prediction problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not overly influenced by noisy data and not very prone to overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May be easier to use than neural networks, particularly due to the existence
    of several well-supported SVM algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaining popularity due to its high accuracy and high-profile wins in data mining
    competitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best model requires testing of various combinations of kernels and
    model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be slow to train, particularly if the input dataset has a large number of
    features or examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Results in a complex black box model that is difficult, if not impossible, to
    interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel functions, in general, are of the following form. The function denoted
    by the Greek letter phi, that is, ϕ(x), is a mapping of the data into another
    space. Therefore, the general kernel function applies some transformation to the
    feature vectors *x[i]* and *x[j]* and combines them using the **dot product**,
    which takes two vectors and returns a single number.
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernels for non-linear spaces](img/3905_07_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using this form, kernel functions have been developed for many different domains
    of data. A few of the most commonly used kernel functions are listed as follows.
    Nearly all SVM software packages will include these kernels, among many others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **linear kernel** does not transform the data at all. Therefore, it can
    be expressed simply as the dot product of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernels for non-linear spaces](img/3905_07_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **polynomial kernel** of degree *d* adds a simple nonlinear transformation
    of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernels for non-linear spaces](img/3905_07_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **sigmoid kernel** results in a SVM model somewhat analogous to a neural
    network using a sigmoid activation function. The Greek letters kappa and delta
    are used as kernel parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernels for non-linear spaces](img/3905_07_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Gaussian RBF kernel** is similar to a RBF neural network. The RBF kernel
    performs well on many types of data and is thought to be a reasonable starting
    point for many learning tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernels for non-linear spaces](img/3905_07_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There is no reliable rule to match a kernel to a particular learning task. The
    fit depends heavily on the concept to be learned as well as the amount of training
    data and the relationships among the features. Often, a bit of trial and error
    is required by training and evaluating several SVMs on a validation dataset. This
    said, in many cases, the choice of kernel is arbitrary, as the performance may
    vary slightly. To see how this works in practice, let's apply our understanding
    of SVM classification to a real-world problem.
  prefs: []
  type: TYPE_NORMAL
- en: Example – performing OCR with SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image processing is a difficult task for many types of machine learning algorithms.
    The relationships linking patterns of pixels to higher concepts are extremely
    complex and hard to define. For instance, it's easy for a human being to recognize
    a face, a cat, or the letter "A", but defining these patterns in strict rules
    is difficult. Furthermore, image data is often noisy. There can be many slight
    variations in how the image was captured, depending on the lighting, orientation,
    and positioning of the subject.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs are well-suited to tackle the challenges of image data. Capable of learning
    complex patterns without being overly sensitive to noise, they are able to recognize
    visual patterns with a high degree of accuracy. Moreover, the key weakness of
    SVMs—the black box model representation—is less critical for image processing.
    If an SVM can differentiate a cat from a dog, it does not matter much how it is
    doing so.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will develop a model similar to those used at the core of
    the **Optical Character Recognition** (**OCR**) software often bundled with desktop
    document scanners. The purpose of such software is to process paper-based documents
    by converting printed or handwritten text into an electronic form to be saved
    in a database. Of course, this is a difficult problem due to the many variants
    in handwritten style and printed fonts. Even so, software users expect perfection,
    as errors or typos can result in embarrassing or costly mistakes in a business
    environment. Let's see whether our SVM is up to the task.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When OCR software first processes a document, it divides the paper into a matrix
    such that each cell in the grid contains a single **glyph**, which is just a term
    referring to a letter, symbol, or number. Next, for each cell, the software will
    attempt to match the glyph to a set of all characters it recognizes. Finally,
    the individual characters would be combined back together into words, which optionally
    could be spell-checked against a dictionary in the document's language.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we'll assume that we have already developed the algorithm
    to partition the document into rectangular regions each consisting of a single
    character. We will also assume the document contains only alphabetic characters
    in English. Therefore, we'll simulate a process that involves matching glyphs
    to one of the 26 letters, A through Z.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we'll use a dataset donated to the UCI Machine Learning Data Repository
    ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)) by W. Frey and
    D. J. Slate. The dataset contains 20,000 examples of 26 English alphabet capital
    letters as printed using 20 different randomly reshaped and distorted black and
    white fonts.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on this dataset, refer to Slate DJ, Frey W. Letter recognition
    using Holland-style adaptive classifiers*. Machine Learning.* 1991; 6:161-182.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure, published by Frey and Slate, provides an example of some
    of the printed glyphs. Distorted in this way, the letters are challenging for
    a computer to identify, yet are easily recognized by a human being:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 1 – collecting data](img/3905_07_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Step 2 – exploring and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to the documentation provided by Frey and Slate, when the glyphs are
    scanned into the computer, they are converted into pixels and 16 statistical attributes
    are recorded.
  prefs: []
  type: TYPE_NORMAL
- en: The attributes measure such characteristics as the horizontal and vertical dimensions
    of the glyph, the proportion of black (versus white) pixels, and the average horizontal
    and vertical position of the pixels. Presumably, differences in the concentration
    of black pixels across various areas of the box should provide a way to differentiate
    among the 26 letters of the alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To follow along with this example, download the `letterdata.csv` file from the
    Packt Publishing website, and save it to your R working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading the data into R, we confirm that we have received the data with the
    16 features that define each example of the letter class. As expected, letter
    has 26 levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Recall that SVM learners require all features to be numeric, and moreover, that
    each feature is scaled to a fairly small interval. In this case, every feature
    is an integer, so we do not need to convert any factors into numbers. On the other
    hand, some of the ranges for these integer variables appear fairly wide. This
    indicates that we need to normalize or standardize the data. However, we can skip
    this step for now, because the R package that we will use for fitting the SVM
    model will perform the rescaling automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the data preparation has been largely done for us, we can move directly
    to the training and testing phases of the machine learning process. In the previous
    analyses, we randomly divided the data between the training and testing sets.
    Although we could do so here, Frey and Slate have already randomized the data,
    and therefore suggest using the first 16,000 records (80 percent) to build the
    model and the next 4,000 records (20 percent) to test. Following their advice,
    we can create training and testing data frames as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With our data ready to go, let's start building our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to fitting an SVM model in R, there are several outstanding packages
    to choose from. The `e1071` package from the Department of Statistics at the Vienna
    University of Technology (TU Wien) provides an R interface to the award winning
    LIBSVM library, a widely used open source SVM program written in C++. If you are
    already familiar with LIBSVM, you may want to start here.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on LIBSVM, refer to the authors' website at [http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if you're already invested in the SVMlight algorithm, the `klaR`
    package from the Department of Statistics at the Dortmund University of Technology
    (TU Dortmund) provides functions to work with this SVM implementation directly
    within R.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For information on SVMlight, have a look at [http://svmlight.joachims.org/](http://svmlight.joachims.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you are starting from scratch, it is perhaps best to begin with
    the SVM functions in the `kernlab` package. An interesting advantage of this package
    is that it was developed natively in R rather than C or C++, which allows it to
    be easily customized; none of the internals are hidden behind the scenes. Perhaps
    even more importantly, unlike the other options, `kernlab` can be used with the
    `caret` package, which allows SVM models to be trained and evaluated using a variety
    of automated methods (covered in [Chapter 11](ch11.html "Chapter 11. Improving
    Model Performance"), *Improving Model Performance*).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a more thorough introduction to `kernlab`, please refer to the authors'
    paper at [http://www.jstatsoft.org/v11/i09/](http://www.jstatsoft.org/v11/i09/).
  prefs: []
  type: TYPE_NORMAL
- en: The syntax for training SVM classifiers with `kernlab` is as follows. If you
    do happen to be using one of the other packages, the commands are largely similar.
    By default, the `ksvm()` function uses the Gaussian RBF kernel, but a number of
    other options are provided.
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 3 – training a model on the data](img/3905_07_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To provide a baseline measure of SVM performance, let''s begin by training
    a simple linear SVM classifier. If you haven''t already, install the `kernlab`
    package to your library, using the `install.packages("kernlab")` command. Then,
    we can call the `ksvm()` function on the training data and specify the linear
    (that is, vanilla) kernel using the `vanilladot` option, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the performance of your computer, this operation may take some
    time to complete. When it finishes, type the name of the stored model to see some
    basic information about the training parameters and the fit of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This information tells us very little about how well the model will perform
    in the real world. We'll need to examine its performance on the testing dataset
    to know whether it generalizes well to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `predict()` function allows us to use the letter classification model to
    make predictions on the testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we didn''t specify the type parameter, the `type = "response"` default
    was used. This returns a vector containing a predicted letter for each row of
    values in the test data. Using the `head()` function, we can see that the first
    six predicted letters were `U`, `N`, `V`, `X`, `N`, and `H`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To examine how well our classifier performed, we need to compare the predicted
    letter to the true letter in the testing dataset. We''ll use the `table()` function
    for this purpose (only a portion of the full table is shown here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The diagonal values of `144`, `121`, `120`, `156`, and `127` indicate the total
    number of records where the predicted letter matches the true value. Similarly,
    the number of mistakes is also listed. For example, the value of `5` in row `B`
    and column `D` indicates that there were five cases where the letter `D` was misidentified
    as a `B`.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at each type of mistake individually may reveal some interesting patterns
    about the specific types of letters the model has trouble with, but this is time
    consuming. We can simplify our evaluation instead by calculating the overall accuracy.
    This considers only whether the prediction was correct or incorrect, and ignores
    the type of error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command returns a vector of `TRUE` or `FALSE` values, indicating
    whether the model''s predicted letter agrees with (that is, matches) the actual
    letter in the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `table()` function, we see that the classifier correctly identified
    the letter in 3,357 out of the 4,000 test records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In percentage terms, the accuracy is about 84 percent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that when Frey and Slate published the dataset in 1991, they reported a
    recognition accuracy of about 80 percent. Using just a few lines of R code, we
    were able to surpass their result, although we also have the benefit of over two
    decades of additional machine learning research. With this in mind, it is likely
    that we are able to do even better.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our previous SVM model used the simple linear kernel function. By using a more
    complex kernel function, we can map the data into a higher dimensional space,
    and potentially obtain a better model fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be challenging, however, to choose from the many different kernel functions.
    A popular convention is to begin with the Gaussian RBF kernel, which has been
    shown to perform well for many types of data. We can train an RBF-based SVM, using
    the `ksvm()` function as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we make predictions as done earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll compare the accuracy to our linear SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Your results may differ from those shown here due to randomness in the `ksvm`
    RBF kernel. If you'd like them to match exactly, use `set.seed(12345)` prior to
    running the ksvm() function.
  prefs: []
  type: TYPE_NORMAL
- en: By simply changing the kernel function, we were able to increase the accuracy
    of our character recognition model from 84 percent to 93 percent. If this level
    of performance is still unsatisfactory for the OCR program, other kernels could
    be tested, or the cost of constraints parameter C could be varied to modify the
    width of the decision boundary. As an exercise, you should experiment with these
    parameters to see how they impact the success of the final model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we examined two machine learning methods that offer a great
    deal of potential, but are often overlooked due to their complexity. Hopefully,
    you now see that this reputation is at least somewhat undeserved. The basic concepts
    that drive ANNs and SVMs are fairly easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, because ANNs and SVMs have been around for many decades,
    each of them has numerous variations. This chapter just scratches the surface
    of what is possible with these methods. By utilizing the terminology you learned
    here, you should be capable of picking up the nuances that distinguish the many
    advancements that are being developed every day.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have spent some time learning about many different types of predictive
    models from simple to sophisticated; in the next chapter, we will begin to consider
    methods for other types of learning tasks. These unsupervised learning techniques
    will bring to light fascinating patterns within the data.
  prefs: []
  type: TYPE_NORMAL

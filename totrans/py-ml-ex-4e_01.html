<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer039">
    <h1 class="chapterNumber">1</h1>
    <h1 class="chapterTitle" id="_idParaDest-15">Getting Started with Machine Learning and Python</h1>
    <p class="normal">The concept of <strong class="keyWord">artificial intelligence</strong> (<strong class="keyWord">AI</strong>) outpacing human knowledge is often referred to as the “technological singularity.” Some predictions in the AI research community and other fields suggest that the singularity could happen within the next 30 years. Regardless of its time horizon, one thing is clear: the rise of AI highlights the growing importance of analytical and machine learning skills. Mastering these disciplines equips us to not only understand and interact with increasingly complex AI systems but also actively participate in shaping their development and application, ensuring they benefit humanity.</p>
    <p class="normal">In this chapter, we will kick off our machine learning journey with the basic, yet important, concepts of machine learning. We will start with what machine learning is all about, why we need it, and its evolution over a few decades. We will then discuss typical machine learning tasks and explore several essential techniques to work with data and models.</p>
    <p class="normal">At the end of the chapter, we will set up the software for Python, the most popular language for machine learning and data science, and the libraries and tools that are required for this book.</p>
    <p class="normal">We will go into detail on the following topics:</p>
    <ul>
      <li class="bulletList">An introduction to machine learning</li>
      <li class="bulletList">Knowing the prerequisites</li>
      <li class="bulletList">Getting started with three types of machine learning</li>
      <li class="bulletList">Digging into the core of machine learning</li>
      <li class="bulletList">Data preprocessing and feature engineering</li>
      <li class="bulletList">Combining models</li>
      <li class="bulletList">Installing software and setting up</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-16">An introduction to machine learning</h1>
    <p class="normal">In this first section, we will kick off our machine learning journey with a brief introduction to machine learning, why we need it, how it differs from automation, and how it improves our lives.</p>
    <p class="normal"><strong class="keyWord">Machine learning</strong> is a term that was coined around 1960, consisting of two words—<strong class="keyWord">machine</strong>, which corresponds to a<a id="_idIndexMarker000"/> computer, robot, or other device, and <strong class="keyWord">learning</strong>, which refers to an activity intended to acquire or discover event patterns, which we humans are good at. Interesting examples include facial recognition, language translation, responding to emails, making data-driven business decisions, and creating various types of content. You will see many more examples throughout this book.</p>
    <h2 class="heading-2" id="_idParaDest-17">Understanding why we need machine learning</h2>
    <p class="normal">Why do we need machine learning, and why do we want a machine to learn the same way as a human? We can look at it from<a id="_idIndexMarker001"/> three main perspectives: maintenance, risk mitigation, and enhanced performance.</p>
    <p class="normal">First and foremost, of course, computers and robots can work 24/7 and don’t get tired. Machines cost a lot less in the long run. Also, for sophisticated problems that involve a variety of huge datasets or complex calculations, it’s much more justifiable, not to mention intelligent, to let computers do all the work. Machines driven by algorithms that are designed by humans can learn latent rules and inherent patterns, enabling them to carry out tasks effectively.</p>
    <p class="normal">Learning machines are better suited than humans for tasks that are routine, repetitive, or tedious. Beyond that, automation by machine learning can mitigate risks caused by fatigue or inattention. Self-driving cars, as shown in <em class="italic">Figure 1.1</em>, are a great example: a vehicle is capable of navigating by sensing its environment and making decisions without human input. Another example is the use of robotic arms in production lines, which are capable of causing a significant reduction in injuries and costs.</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B21047_01_01.png"/></figure>
    <p class="packt_figref">Figure 1.1: An example of a self-driving car</p>
    <p class="normal">Let’s assume that humans don’t fatigue or we have the resources to hire enough shift workers; would machine<a id="_idIndexMarker002"/> learning still have a place? Of course it would! There are many cases, reported and unreported, where machines perform comparably, or even better, than domain experts. As algorithms are designed to learn from the ground truth and the best thought-out decisions made by human experts, machines can perform just as well as experts.</p>
    <p class="normal">In reality, even the best expert makes mistakes. Machines can minimize the chance of making wrong decisions by utilizing collective intelligence from individual experts. A major study that identified that machines are better than doctors at diagnosing certain types of cancer is proof of this<a id="_idIndexMarker003"/> philosophy (<a href="https://www.nature.com/articles/d41586-020-00847-2"><span class="url">https://www.nature.com/articles/d41586-020-00847-2</span></a>). <strong class="keyWord">AlphaGo</strong> (<a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far"><span class="url">https://deepmind.com/research/case-studies/alphago-the-story-so-far</span></a>) is probably the best-known example of machines beating humans—an AI program created by DeepMind defeated Lee Sedol, a world champion Go player, in a five-game Go match.</p>
    <p class="normal">Also, it’s much more scalable to deploy learning machines than to train individuals to become experts, from the perspective of economic and social barriers. Current diagnostic devices can achieve a level of performance similar to that of qualified doctors. We can distribute thousands of diagnostic devices across the globe within a week, but it’s almost impossible to recruit and assign the same number of qualified doctors within the same period.</p>
    <p class="normal">You may argue against this: what if we have sufficient resources and the capacity to hire the best domain experts <a id="_idIndexMarker004"/>and later aggregate their opinions—would machine learning still have a place? Probably not (at least right now)—learning machines might not perform better than the joint efforts of the most intelligent humans. However, individuals equipped with learning machines can outperform the best group of experts. This is an emerging <a id="_idIndexMarker005"/>concept called <strong class="keyWord">AI-based assistance</strong> or <strong class="keyWord">AI plus human intelligence</strong>, which<a id="_idIndexMarker006"/> advocates for combining the efforts of machines and humans. It provides support, guidance, or solutions to users. And more importantly, it can adapt and learn from user interactions to improve performance<a id="_idIndexMarker007"/> over time.</p>
    <p class="normal">We can summarize the previous statement in the following inequality:</p>
    <p class="normal"><em class="italic">human + machine learning → most intelligent tireless human ≥ machine learning &gt; human</em></p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Artificial intelligence-generated content</strong> (<strong class="keyWord">AIGC</strong>) is one of the recent breakthroughs. It uses AI technologies to create or assist in creating various types of content, such as articles, product descriptions, music, images, and videos. </p>
    </div>
    <p class="normal">A medical operation involving robots is one great example of human and machine learning synergy. <em class="italic">Figure 1.2</em> shows robotic arms in an operation room alongside a surgeon:</p>
    <figure class="mediaobject"><img alt="A picture containing person, clothing, medical equipment, technician  Description automatically generated" src="../Images/B21047_01_02.png"/></figure>
    <p class="packt_figref">Figure 1.2: AI-assisted surgery</p>
    <h2 class="heading-2" id="_idParaDest-18">Differentiating between machine learning and automation</h2>
    <p class="normal">So does machine learning simply equate to automation that involves the programming and execution of human-crafted or <a id="_idIndexMarker008"/>human-curated rule sets? A popular myth says that <a id="_idIndexMarker009"/>machine learning is the same as automation because it performs instructive and repetitive tasks and thinks no further. If the answer to that question is <em class="italic">yes</em>, why can’t we just hire many software programmers and continue programming new rules or extending old rules?</p>
    <p class="normal">One reason is that defining, maintaining, and updating rules becomes increasingly expensive over time. The number of possible patterns for an activity or event could be enormous, and therefore, exhausting all enumeration isn’t practically feasible. It gets even more challenging when it comes to events that are dynamic, ever-changing, or evolve in real time. It’s much easier and more efficient to develop learning algorithms that command computers to learn, extract patterns, and figure things out themselves from abundant data.</p>
    <p class="normal">The difference between machine learning and traditional programming can be seen in <em class="italic">Figure 1.3</em>:</p>
    <figure class="mediaobject"><img alt="A diagram of a computer model  Description automatically generated with low confidence" src="../Images/B21047_01_03.png"/></figure>
    <p class="packt_figref">Figure 1.3: Machine learning versus traditional programming</p>
    <p class="normal">In traditional programming, the computer follows a set of predefined rules to process the input data and produce the <a id="_idIndexMarker010"/>outcome. In machine learning, the computer tries to mimic human thinking. It interacts <a id="_idIndexMarker011"/>with the input data, expected output, and the environment, and it derives patterns that are represented by one or more mathematical models. The models are then used to interact with future input data and generate outcomes. Unlike in automation, the computer in a machine learning setting doesn’t receive explicit and instructive coding.</p>
    <p class="normal">The volume of data is growing exponentially. Nowadays, the floods of textual, audio, image, and video data are hard to fathom. The <strong class="keyWord">Internet of Things</strong> (<strong class="keyWord">IoT</strong>) is a recent development of a new kind of internet, which<a id="_idIndexMarker012"/> interconnects everyday devices. The IoT will bring data from household appliances and autonomous cars to the fore. This trend is likely to continue, and we will have more data that is generated and processed. Besides the quantity, the quality of data available has kept increasing in the past few years, partly due to cheaper storage. This has empowered the evolution of machine learning algorithms and data-driven solutions.</p>
    <h2 class="heading-2" id="_idParaDest-19">Machine learning applications</h2>
    <p class="normal">Jack Ma, co-founder of the e-commerce company Alibaba, explained in a speech in 2018 that IT was the focus of the past 20 years, but for the next 30 years, we will be in the age of <strong class="keyWord">data technology</strong> (<strong class="keyWord">DT</strong>) (<a href="https://www.alizila.com/jack-ma-dont-fear-smarter-computers/"><span class="url">https://www.alizila.com/jack-ma-dont-fear-smarter-computers/</span></a>). During the age of IT, companies <a id="_idIndexMarker013"/>grew larger and stronger thanks to<a id="_idIndexMarker014"/> computer software and infrastructure. Now that businesses in most industries have already gathered enormous amounts of data, it’s presently the right time to exploit DT to unlock insights, derive patterns, and boost new business growth. Broadly speaking, machine learning technologies enable businesses to better understand customer behavior, engage with customers, and optimize operations management.</p>
    <p class="normal">As for us individuals, machine learning technologies are already making our lives better every day. One application of machine learning with which we’re all familiar is spam email filtering. Another is online advertising, where adverts are served automatically based on information advertisers have collected about us. Stay tuned for the next few chapters, where you will learn how to develop algorithms to solve these two problems and more.</p>
    <p class="normal">A search engine is an application of machine learning we can’t imagine living without. It involves information retrieval, which parses what we look for, queries the related top records, and applies contextual ranking and personalized ranking, which sorts pages by topical relevance and user preference. E-commerce and media companies have been at the forefront of employing recommendation systems, which help customers find products, services, and articles faster.</p>
    <p class="normal">The application of machine learning is boundless, and we just keep hearing new examples everyday: credit card fraud detection, presidential election prediction, instant speech translation, robo advisors, AI-generated art, chatbots for customer support, and medical or legal advice provided by generative AI technologies—you name it!</p>
    <p class="normal">In the 1983 <em class="italic">War Games</em> movie, a computer made life-and-death decisions that could have resulted in World War III. As far as we know, technology wasn’t able to pull off such feats at the time. However, in 1997, the Deep Blue supercomputer did manage to beat a world chess champion (<a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"><span class="url">https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)</span></a>). In 2005, a Stanford self-driving car drove by itself for more than 130 miles in a desert (<a href="https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2005)"><span class="url">https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2005)</span></a>). In 2007, the car of another team drove through regular urban traffic for more than 60 miles (<a href="https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007)"><span class="url">https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007)</span></a>). In 2011, the Watson computer won a quiz against human opponents (<a href="https://en.wikipedia.org/wiki/Watson_(computer)"><span class="url">https://en.wikipedia.org/wiki/Watson_(computer)</span></a>). As mentioned earlier, the AlphaGo program beat one of the best Go players in the world in 2016. As of 2023, ChatGPT has been widely used across various industries, such as customer support, content generation, market research, and training and education (<a href="https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023"><span class="url">https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023</span></a>).</p>
    <p class="normal">If we assume that computer hardware is the limiting factor, then we can try to extrapolate into the future. A famous American inventor and futurist, Ray Kurzweil, did just that, predicting in 2017<a id="_idIndexMarker015"/> that we can expect AI to gain human-level intelligence around 2029 (<a href="https://aibusiness.com/responsible-ai/ray-kurzweil-predicts-that-the-singularity-will-take-place-by-2045"><span class="url">https://aibusiness.com/responsible-ai/ray-kurzweil-predicts-that-the-singularity-will-take-place-by-2045</span></a>). What’s next?</p>
    <p class="normal">Can’t wait to launch your own machine learning journey? Let’s start with the prerequisites and the basic types of machine learning.</p>
    <h1 class="heading-1" id="_idParaDest-20">Knowing the prerequisites</h1>
    <p class="normal">Machine learning mimicking human intelligence is a subfield of AI—a field of computer science concerned with creating systems. Software<a id="_idIndexMarker016"/> engineering is another field in computer science. Generally, we can label Python programming as a type of software engineering. Machine learning is also closely related to linear algebra, probability theory, statistics, and mathematical optimization. We usually build machine learning models based on statistics, probability theory, and linear algebra, and then optimize the models using mathematical optimization.</p>
    <p class="normal">Most of you reading this book should have a good, or at least sufficient, command of Python programming. Those who aren’t feeling confident about mathematical knowledge might be wondering how much time should be spent learning or brushing up on the aforementioned subjects. Don’t panic; we will get machine learning to work for us without going into any deep mathematical details in this book. It just requires some basic 101 knowledge of probability theory and linear algebra, which helps us to understand the mechanics of machine learning techniques and algorithms. And it gets easier, as we will build models both from scratch and with popular packages in Python, a language we like and are familiar with.</p>
    <p class="normal">For those who want to learn or brush up on probability theory and linear algebra, feel free to search for basic probability theory and basic linear algebra. There are a lot of resources available online, for example, <a href="https://people.ucsc.edu/~abrsvn/intro_prob_1.pdf"><span class="url">https://people.ucsc.edu/~abrsvn/intro_prob_1.pdf</span></a>, the online course <em class="italic">Introduction to Probability</em> by Harvard University (<a href="https://pll.harvard.edu/course/introduction-probability-edx"><span class="url">https://pll.harvard.edu/course/introduction-probability-edx</span></a>) regarding <em class="italic">probability 101</em>, and the following paper regarding basic linear algebra: <a href="http://www.maths.gla.ac.uk/~ajb/dvi-ps/2w-notes.pdf"><span class="url">http://www.maths.gla.ac.uk/~ajb/dvi-ps/2w-notes.pdf</span></a>.</p>
    <p class="normal">Those who want to study machine learning systematically can enroll in computer science, AI, and, more recently, data science and AI master’s programs. There are also various data science boot camps. However, the selection for boot camps is usually stricter, as they’re more job-oriented<a id="_idIndexMarker017"/> and the program duration is often short, ranging from 4 to 10 weeks. Another option is free <strong class="keyWord">Massive Open Online Courses</strong> (<strong class="keyWord">MOOCs</strong>), such as Andrew<a id="_idIndexMarker018"/> Ng’s popular course on machine learning. Last but not least, industry blogs and websites are great resources for us to keep up with the latest developments.</p>
    <p class="normal">Machine learning is not only a skill but also a bit of a sport. We can compete in several machine learning competitions, such as Kaggle (<a href="https://www.kaggle.com"><span class="url">www.kaggle.com</span></a>)—sometimes for decent cash prizes, sometimes for joy, but most of the time to play to our strengths. However, to win these competitions, we may need to utilize certain techniques, which are only useful in the context of competitions and not in the context of trying to solve a business problem. That’s right—the <strong class="keyWord">no free lunch</strong> theorem (<a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem"><span class="url">https://en.wikipedia.org/wiki/No_free_lunch_theorem</span></a>) applies here. In the context of machine learning, this<a id="_idIndexMarker019"/> theorem suggests that no single algorithm is universally superior across all possible datasets and problem domains.</p>
    <p class="normal">Next, we’ll take a look at the three types of machine learning.</p>
    <h1 class="heading-1" id="_idParaDest-21">Getting started with three types of machine learning</h1>
    <p class="normal">A machine learning system is fed with input data—this can be numerical, textual, visual, or audiovisual. The system usually <a id="_idIndexMarker020"/>has an output—this can be a floating-point number, for instance, the <a id="_idIndexMarker021"/>acceleration of a self-driving car, or an integer representing a category (also called a <strong class="keyWord">class</strong>), for example, a cat or tiger from image recognition.</p>
    <p class="normal">The main task of machine learning is to explore and construct algorithms that can learn from historical data and make predictions on new input data. For a data-driven solution, we need to define (or have it defined by an algorithm) an evaluation function called a <strong class="keyWord">loss</strong> or <strong class="keyWord">cost function</strong>, which measures <a id="_idIndexMarker022"/>how well the models learn. In this setup, we create an optimization<a id="_idIndexMarker023"/> problem with the goal of learning most efficiently and effectively.</p>
    <p class="normal">Depending on the nature of the learning data, machine learning tasks can be broadly classified into the following three<a id="_idIndexMarker024"/> categories:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Unsupervised learning</strong>: When the learning data only contains indicative signals without any description<a id="_idIndexMarker025"/> attached (we call this <strong class="keyWord">unlabeled data</strong>), it’s up to us to find the structure of the data underneath, discover hidden information, or determine how to describe the data. Unsupervised learning can be<a id="_idIndexMarker026"/> used to detect anomalies, such as fraud or defective equipment, or group customers with similar online behaviors for a marketing campaign. Data visualization that makes <a id="_idIndexMarker027"/>data more digestible, as well as dimensionality reduction that distills relevant information from noisy data, are also in the family of unsupervised learning.</li>
      <li class="bulletList"><strong class="keyWord">Supervised learning</strong>: When <a id="_idIndexMarker028"/>learning data comes with a description, targets, or desired output besides indicative signals (we<a id="_idIndexMarker029"/> call this <strong class="keyWord">labeled data</strong>), the learning goal is to find a general rule that maps input to output. The learned rule is then used to label new data with unknown <a id="_idIndexMarker030"/>output. The labels are usually provided by event-logging systems or evaluated by human experts. Also, if feasible, they may be produced by human raters, through crowd-sourcing, for instance.</li>
    </ul>
    <p class="normal-one">Supervised learning is commonly used in daily applications, such as face and speech recognition, product or movie recommendations, sales forecasting, and spam email detection.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Reinforcement learning</strong>: Learning data<a id="_idIndexMarker031"/> provides feedback so that a system adapts to dynamic conditions in order to ultimately achieve a certain goal. The system evaluates its<a id="_idIndexMarker032"/> performance based on the feedback responses and reacts accordingly. The best-known instances include robotics for industrial automation, self-driving cars, and the chess master AlphaGo. The key difference between reinforcement learning and supervised learning is the interaction with the environment.</li>
    </ul>
    <p class="normal">The following diagram depicts the types of machine learning tasks:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B21047_01_04.png"/></figure>
    <p class="packt_figref">Figure 1.4: Types of machine learning tasks</p>
    <p class="normal">As shown in the diagram, we can further subdivide supervised learning into regression and classification. <strong class="keyWord">Regression</strong> trains <a id="_idIndexMarker033"/>on and predicts continuous-valued<a id="_idIndexMarker034"/> responses, for example, predicting house prices, while <strong class="keyWord">classification</strong> attempts to<a id="_idIndexMarker035"/> find the appropriate<a id="_idIndexMarker036"/> class label, such as analyzing a positive/negative sentiment and predicting a loan default.</p>
    <p class="normal">If not all learning samples are labeled, but some are, we have <strong class="keyWord">semi-supervised</strong> <strong class="keyWord">learning</strong>. This makes use of unlabeled data (typically a large amount) for training, besides a small amount <a id="_idIndexMarker037"/>of labeled data. Semi-supervised learning is applied in cases where it is expensive to acquire a fully labeled dataset and more practical to label a small subset. For example, it often requires skilled experts to label hyperspectral remote sensing images, while acquiring unlabeled data is relatively easy.</p>
    <p class="normal">Feeling a little bit confused by the abstract concepts? Don’t worry. We will encounter many concrete examples of these types of machine learning tasks later in this book. For example, in <em class="chapterRef">Chapter 2</em>, <em class="italic">Building a Movie Recommendation Engine with Naïve Bayes</em>, we will dive into supervised learning classification and its popular algorithms and applications. Similarly, in <em class="chapterRef">Chapter 5</em>, <em class="italic">Predicting Stock Prices with Regression Algorithms</em>, we will explore supervised learning regression. </p>
    <p class="normal">We will focus on unsupervised techniques and algorithms in <em class="chapterRef">Chapter 8</em>, <em class="italic">Discovering Underlying Topics in the Newsgroups Dataset with Clustering and Topic Modeling</em>. Last but not least, the third machine learning task, reinforcement learning, will be covered in <em class="chapterRef">Chapter 15</em>, <em class="italic">Making Decisions in Complex Environments with Reinforcement Learning</em>.</p>
    <p class="normal">Besides categorizing machine learning based on the learning task, we can categorize it chronologically.</p>
    <h2 class="heading-2" id="_idParaDest-22">A brief history of the development of machine learning algorithms</h2>
    <p class="normal">In fact, we have a whole zoo of machine learning algorithms that have experienced varying popularity over time. We can roughly categorize them into five main approaches: logic-based learning, statistical<a id="_idIndexMarker038"/> learning, artificial neural networks, genetic algorithms, and deep learning.</p>
    <p class="normal">The <strong class="keyWord">logic-based</strong> systems were the first <a id="_idIndexMarker039"/>to be dominant. They used basic rules specified by human experts, and with these rules, systems tried to reason using formal logic, background knowledge, and hypotheses.</p>
    <p class="normal"><strong class="keyWord">Statistical learning</strong> theory attempts to find a <a id="_idIndexMarker040"/>function to formalize the relationships <a id="_idIndexMarker041"/>between variables. In the mid-1980s, <strong class="keyWord">artificial neural networks</strong> (<strong class="keyWord">ANNs</strong>) came to the fore. ANNs<a id="_idIndexMarker042"/> imitate animal brains and consist of interconnected neurons that are also an imitation of biological neurons. They try to model complex relationships between input and output values and capture patterns in data. ANNs were superseded by statistical learning systems in the 1990s.</p>
    <p class="normal"><strong class="keyWord">Genetic algorithms</strong> (<strong class="keyWord">GA</strong>) were popular<a id="_idIndexMarker043"/> in the 1990s. They mimic the biological process of evolution and try to find optimal solutions, using methods such as mutation and crossover.</p>
    <p class="normal">In the 2000s, ensemble<a id="_idIndexMarker044"/> learning methods gained attention, which combined multiple models to improve performance.</p>
    <p class="normal">We have seen <strong class="keyWord">deep learning </strong>become a <a id="_idIndexMarker045"/>dominant force since the late 2010s. The term deep learning was coined around 2006 and refers to deep neural networks with many layers. The breakthrough in deep<a id="_idIndexMarker046"/> learning was the result of the integration and utilization of <strong class="keyWord">Graphical Processing Units</strong> (<strong class="keyWord">GPUs</strong>), which massively speed up computation. The availability of large datasets also fuels the deep learning revolution.</p>
    <p class="normal">GPUs were originally developed to render video games and are very good in parallel matrix and vector algebra. It’s believed that deep learning resembles the way humans learn. Therefore, it may be able to deliver on the promise of sentient machines. Of course, in this book, we will dig deep into deep learning in <em class="chapterRef">Chapter 11</em>, <em class="italic">Categorizing Images of Clothing with Convolutional Neural Networks</em>, and <em class="chapterRef">Chapter 12</em>, <em class="italic">Making Predictions with Sequences Using Recurrent Neural Networks</em>, after touching on it in <em class="chapterRef">Chapter 6</em>, <em class="italic">Predicting Stock Prices with Artificial Neural Networks</em>.</p>
    <p class="normal">Machine learning algorithms continue to evolve rapidly, with ongoing research in areas including <strong class="keyWord">transfer learning</strong>, <strong class="keyWord">generative models</strong>, and reinforcement<a id="_idIndexMarker047"/> learning, which are the backbone of AIGC. We will explore the<a id="_idIndexMarker048"/> latest developments in <em class="chapterRef">Chapter 13</em><em class="italic">, Advancing Language Understanding and Generation with the Transformer Models</em>, and <em class="chapterRef">Chapter 14</em><em class="italic">, Building an Image Search Engine Using CLIP: a Multimodal Approach</em>.</p>
    <p class="normal">Some of us may have heard of <strong class="keyWord">Moore’s law</strong>—an<a id="_idIndexMarker049"/> empirical observation claiming that computer hardware improves exponentially with time. The law was first formulated by Gordon Moore, the co-founder of Intel, in 1965. According to the law, the number of transistors on a chip should double every two years. In the following diagram, you can see that the law holds up nicely (the size of the bubbles corresponds to the average transistor count in GPUs):</p>
    <figure class="mediaobject"><img alt="A picture containing text, screenshot, plot, line  Description automatically generated" src="../Images/B21047_01_05.png"/></figure>
    <p class="packt_figref">Figure 1.5: Transistor counts over the past decades</p>
    <p class="normal">The consensus seems to be that Moore’s law should continue to be valid for a couple of decades. This gives some <a id="_idIndexMarker050"/>credibility to Ray Kurzweil’s predictions of achieving true machine intelligence by 2029.</p>
    <h1 class="heading-1" id="_idParaDest-23">Digging into the core of machine learning</h1>
    <p class="normal">After discussing the <a id="_idIndexMarker051"/>categorization of machine learning algorithms, we are now going to dig into the core of machine learning—generalizing with data, the different levels of generalization, as well as the approaches to attain the right level of generalization.</p>
    <h2 class="heading-2" id="_idParaDest-24">Generalizing with data</h2>
    <p class="normal">The good thing about data is that there’s a lot of it in the world. The bad thing is that it’s hard to process this data. The challenge stems from the diversity and noisiness of it. We humans usually process data coming into our ears and eyes. These inputs are transformed into electrical or <a id="_idIndexMarker052"/>chemical signals. On a very basic level, computers and robots also work with electrical signals. </p>
    <p class="normal">These electrical signals are then translated into ones and zeros. However, we program in Python in this book, and on that level, normally we represent the data either as numbers or texts. However, text isn’t very convenient, so we need to transform this into numerical values.</p>
    <p class="normal">Especially in the context of supervised learning, we have a scenario similar to studying for an exam. We have a set of practice questions and the actual exams. We should be able to answer exam questions without being<a id="_idIndexMarker053"/> exposed to identical questions beforehand. This is called <strong class="keyWord">generalization</strong>—we learn something from our practice questions and, hopefully, can apply this knowledge to other similar questions. In machine learning, these practice <a id="_idIndexMarker054"/>questions are called <strong class="keyWord">training sets</strong> or <strong class="keyWord">training samples</strong>. This is where<a id="_idIndexMarker055"/> the machine learning models derive patterns from. And the actual exams are <strong class="keyWord">testing sets</strong> or <strong class="keyWord">testing samples</strong>. They are where the models are eventually applied. Learning effectiveness is measured by the compatibility of the learning models and the testing.</p>
    <p class="normal">Sometimes, between practice questions and actual exams, we have mock exams to assess how well we will <a id="_idIndexMarker056"/>do in actual exams and to aid revision. These mock exams are known as <strong class="keyWord">validation sets</strong> or <strong class="keyWord">validation samples</strong> in <a id="_idIndexMarker057"/>machine learning. They help us to verify how well the models will perform in a simulated setting, and then we fine-tune the models accordingly in order to achieve greater accuracy.</p>
    <p class="normal">An old-fashioned programmer would talk to a business analyst or other expert, and then implement a tax rule that adds a certain value multiplied by another corresponding value, for instance. In a machine learning setting, we can give the computer a bunch of input and output examples; alternatively, if we want to be more ambitious, we can feed the program the actual tax texts. We can let the machine consume the data and figure out the tax rule, just as an autonomous car doesn’t need a lot of explicit human input.</p>
    <p class="normal">In physics, we have almost the same situation. We want to know how the universe works and formulate laws in a mathematical language. Since we don’t know how it works, all we can do is measure the error produced in our attempt at law formulation and try to minimize it. In supervised learning tasks, we compare our results against the expected values. In unsupervised learning, we measure our success with related metrics. For instance, we want data points to be grouped based on similarities, forming clusters; the metrics could be how similar the data points within one cluster are, or how different the data points from two clusters are. In reinforcement learning, a program evaluates its moves, for example, by using a predefined function in a chess game.</p>
    <p class="normal">Aside from correct <a id="_idIndexMarker058"/>generalization with data, there are two levels of generalization, overfitting and underfitting, which we will explore in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-25">Overfitting, underfitting, and the bias-variance trade-off</h2>
    <p class="normal">In this section, let’s take a look at both levels of generalization in detail and explore the bias-variance trade-off.</p>
    <h3 class="heading-3" id="_idParaDest-26">Overfitting</h3>
    <p class="normal">Reaching the right fit model is the<a id="_idIndexMarker059"/> goal of a machine learning task. What if the model overfits? <strong class="keyWord">Overfitting</strong> means a model fits the existing observations <strong class="keyWord">too well</strong> but fails to predict future new observations. Let’s look at the following analogy.</p>
    <p class="normal">If we go through many practice questions for an exam, we may start to find ways to answer questions that have nothing to do with the subject material. For instance, given only five practice questions, we might find that if there are two occurrences of <em class="italic">potatoes</em>, one of <em class="italic">tomato</em>, and three of <em class="italic">banana</em> in a multiple-choice question, the answer is always <em class="italic">A</em>, and if there is one occurrence of <em class="italic">potato</em>, three of <em class="italic">tomato</em>, and two of <em class="italic">banana</em> in a question, the answer is always <em class="italic">B</em>. We could then conclude that this is always true and apply such a theory later, even though the subject or answer may not be relevant to potatoes, tomatoes, or bananas. Or, even worse, we might memorize the answers to each question verbatim. We would then score highly on the practice questions, leading us to hope that the questions in the actual exams would be the same as the practice questions. However, in reality, we would score very low on the exam questions, as it’s rare that the exact same questions occur in exams.</p>
    <p class="normal">The phenomenon of memorization can cause overfitting. This can occur when we’re over-extracting too much information from the training sets and making our model just work well with them. At the same time, however, overfitting won’t help us to generalize it to new data and derive true patterns from it. The model, as a result, will perform poorly on datasets that weren’t seen before. We call this situation <strong class="keyWord">high variance</strong> in machine learning. Let’s quickly recap variance: <em class="italic">variance</em> measures the spread of the prediction, which is the variability of the prediction. It can be calculated as follows:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_001.png"/></p>
    <p class="normal">Here, <em class="italic">ŷ</em> is the prediction, and E[] is the expectation or expected value that represents the average value<a id="_idIndexMarker060"/> of a random variable, based on its probability distribution in statistics.</p>
    <p class="normal">The following example demonstrates what a typical instance of overfitting looks like, where the regression curve tries to flawlessly accommodate all observed samples:</p>
    <figure class="mediaobject"><img alt="A picture containing map, screenshot  Description automatically generated" src="../Images/B21047_01_06.png"/></figure>
    <p class="packt_figref">Figure 1.6: Example of overfitting</p>
    <p class="normal">Overfitting occurs when we try to describe the learning rules based on too many parameters relative to the small number of observations, instead of the underlying relationship, such as the preceding potato, tomato, and banana example, where we deduced three parameters from only five learning samples. Overfitting also takes place when we make the model so excessively complex that it fits every training sample, such as memorizing the answers for all questions, as mentioned previously.</p>
    <h3 class="heading-3" id="_idParaDest-27">Underfitting</h3>
    <p class="normal">The opposite scenario is <strong class="keyWord">underfitting</strong>. When a model is underfit, it doesn’t perform well on the training sets and won’t do so on<a id="_idIndexMarker061"/> the testing sets, which means it fails to capture the underlying trend of the data. Underfitting may occur if we don’t use enough data to train the model, just like we will fail the exam if we don’t review enough material; this may also happen if we try to fit a wrong model to the data, just as we will score low in any exercises or exams if we take the wrong approach and learn in the wrong way. We describe any of these situations <a id="_idIndexMarker062"/>as <strong class="keyWord">high</strong> <strong class="keyWord">bias</strong> in machine learning, although its variance is low, as the performance in training and test sets is consistent, in a bad way. If you need a quick<a id="_idIndexMarker063"/> recap of bias, here it is: <strong class="keyWord">bias</strong> is the difference between the average prediction and the true value. It is computed as follows:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_002.png"/></p>
    <p class="normal">Here, <em class="italic">ŷ</em> is the prediction and <em class="italic">y</em> is the ground truth.</p>
    <p class="normal">The following example shows what typical underfitting looks like, where the regression curve doesn’t fit the data well enough or capture enough of the underlying pattern of the data:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, line, diagram, plot  Description automatically generated" src="../Images/B21047_01_07.png"/></figure>
    <p class="packt_figref">Figure 1.7: Example of underfitting</p>
    <p class="normal">Now, let’s look at what a<a id="_idIndexMarker064"/> well-fitting example should look like:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, line, plot  Description automatically generated" src="../Images/B21047_01_08.png"/></figure>
    <p class="packt_figref">Figure 1.8: Example of desired fitting</p>
    <h2 class="heading-2" id="_idParaDest-28">The bias-variance trade-off</h2>
    <p class="normal">Obviously, we want to avoid both <a id="_idIndexMarker065"/>overfitting and underfitting. Recall that <strong class="keyWord">bias</strong> is the error stemming from incorrect assumptions in the learning algorithm; high bias results in underfitting. <strong class="keyWord">Variance</strong> measures how sensitive the model prediction is to variations in the datasets. Hence, we need to avoid cases where either bias or variance gets high. So, does it mean we should always make both bias and variance as low as possible? The answer is yes, if we can. But, in practice, there is an explicit trade-off between them, where decreasing one increases the other. This is the so-called <strong class="keyWord">bias-variance trade-off</strong>. Sounds <a id="_idIndexMarker066"/>abstract? Let’s look at the next example.</p>
    <p class="normal">Let’s say we’re asked to build a model to predict the probability of a candidate being the next president of America based on phone poll data. The poll is conducted using zip codes. We randomly choose samples from one zip code, and we estimate there’s a 61% chance the candidate will win. However, it turns out they lost the election. Where did our model go wrong? The first thing we might think of is the small size of samples from only one zip code. It’s a source of high bias also, as people in a geographic area tend to share similar demographics, although it results in a low variance of estimates. So can we fix it simply by using samples from a large number of zip codes? Yes, but don’t get happy too soon. This might cause an increased variance of estimates at the same time. We need to find the optimal sample size—the best number of zip codes to achieve the lowest overall bias and variance.</p>
    <p class="normal">Minimizing the total error <a id="_idIndexMarker067"/>of a model requires a careful balancing of bias and variance. Given a set of training samples, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">1</sub>, <em class="italic">x</em><sub class="subscript">2</sub>, …, <em class="italic">x</em><sub class="subscript">n</sub>, and their targets, <em class="italic">y</em><sub class="subscript">1</sub>, <em class="italic">y</em><sub class="subscript">2</sub>, …, <em class="italic">y</em><sub class="subscript">n</sub>, we want to find a regression function <em class="italic">ŷ</em>(<em class="italic">x</em>) that estimates the true relation <em class="italic">y</em>(<em class="italic">x</em>) as correctly as possible. We <a id="_idIndexMarker068"/>measure the error of estimation, i.e., how good (or bad) the regression model is, in <strong class="keyWord">mean squared error</strong> (<strong class="keyWord">MSE</strong>):</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_003.png"/></p>
    <p class="normal">The <em class="italic">E</em> denotes the expectation. This error can be decomposed into bias and variance components following the analytical derivation, as shown in the following formula (although it requires a bit of basic probability theory to understand):</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_004.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_005.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_006.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_007.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_008.png"/></p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_01_009.png"/></p>
    <p class="normal">The term<em class="italic"> Bias</em> measures the error of estimations, and the term<em class="italic"> Variance</em> describes how much the estimation, <em class="italic">ŷ</em>, moves around its mean, <em class="italic">E</em>[<em class="italic">ŷ</em>]. The more complex the learning model <em class="italic">ŷ</em>(<em class="italic">x</em>) is, and the larger the size of the training samples is, the lower the bias will become. However, this will <a id="_idIndexMarker069"/>also create more adjustments to the model to better fit the increased data points. As a result, the variance will be lifted.</p>
    <p class="normal">We usually employ the cross-validation technique, as well as regularization and feature reduction, to find the optimal model balancing bias and variance and diminish overfitting. We will discuss these next.</p>
    <p class="normal">You may ask why we only want to deal with overfitting: how about underfitting? This is because underfitting can be easily recognized: it occurs if a model doesn’t work well on a training set. When this occurs, we need to find a better model or tweak some parameters to better fit the data, which is a must under all circumstances. On the other hand, overfitting is hard to spot. Oftentimes, when we achieve a model that performs well on a training set, we are overly happy and think it is ready for production right away. This can be very dangerous. We should instead take extra steps to ensure that the great performance isn’t due to overfitting and that the great performance applies to data that excludes the training data.</p>
    <h2 class="heading-2" id="_idParaDest-29">Avoiding overfitting with cross-validation</h2>
    <p class="normal">You will see cross-validation in action multiple times later in this book. So don’t panic if you find this section difficult to understand, as you will become an expert on cross-validation very soon.</p>
    <p class="normal">Recall that between practice questions <a id="_idIndexMarker070"/>and actual exams, there are mock exams where we can assess how well we will perform in actual exams and use that information to conduct the necessary revision. In machine learning, the validation procedure helps to evaluate how models will generalize to independent or unseen datasets in a simulated setting. In a conventional validation setting, the original data is partitioned into three subsets, usually 60% for the training set, 20% for the validation set, and the rest (20%) for the testing set. This setting suffices if we have enough training samples after partitioning and we only need a rough estimate of simulated performance. Otherwise, cross-validation is preferable. Cross-validation helps to reduce variability and, therefore, limit overfitting.</p>
    <p class="normal">In one round of cross-validation, the original data is divided into two subsets, for <strong class="keyWord">training</strong> and <strong class="keyWord">testing</strong> (or <strong class="keyWord">validation</strong>), respectively. The testing performance is recorded. Similarly, multiple rounds of cross-validation are performed under different partitions. Testing results from all rounds are finally averaged to generate a more reliable estimate of model prediction performance.</p>
    <p class="normal">When the training size is very large, it’s often sufficient to split it into training, validation, and testing (three subsets) and conduct a performance check on the latter two. Cross-validation is less preferable in this case, since it’s computationally costly to train a model for each single round. But if you can afford it, there’s no reason not to use cross-validation. When the size isn’t so large, cross-validation is definitely a good choice.</p>
    <p class="normal">There are mainly two cross-validation schemes in use: exhaustive and non-exhaustive. In the <strong class="keyWord">exhaustive scheme</strong>, we leave<a id="_idIndexMarker071"/> out a fixed number of observations in each round as testing (or validation) samples and use the remaining observations as training samples. This process is repeated until all possible different subsets of samples are used for testing once. For instance, we can<a id="_idIndexMarker072"/> apply <strong class="keyWord">Leave-One-Out-Cross-Validation</strong> (<strong class="keyWord">LOOCV</strong>), which lets each sample be in the<a id="_idIndexMarker073"/> testing set once. For a dataset of the size <em class="italic">n</em>, LOOCV requires <em class="italic">n</em> rounds of cross-validation. This can be slow when <em class="italic">n</em> gets large. The following diagram presents the workflow of LOOCV:</p>
    <figure class="mediaobject"><img alt="A screenshot of a test  Description automatically generated with low confidence" src="../Images/B21047_01_09.png"/></figure>
    <p class="packt_figref">Figure 1.9: Workflow of leave-one-out-cross-validation</p>
    <p class="normal">A <strong class="keyWord">non-exhaustive scheme</strong>, on the<a id="_idIndexMarker074"/> other hand, as the name implies, doesn’t try out all possible partitions. The most widely used type of this scheme is <strong class="keyWord">k-fold cross-validation</strong>. First, we <a id="_idIndexMarker075"/>randomly split the<a id="_idIndexMarker076"/> original data into <strong class="keyWord">k equal-sized</strong> folds. In each trial, one of these folds becomes the testing set, and the<a id="_idIndexMarker077"/> rest of the data becomes the training set.</p>
    <p class="normal">We repeat this process <em class="italic">k</em> times, with each fold being the designated testing set once. Finally, we average the <em class="italic">k</em> sets of test results for the purpose of evaluation. Common values for <em class="italic">k</em> are 3, 5, and 10. The following table illustrates the setup for five-fold cross-validation:</p>
    <table class="table-container" id="table001">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Round</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Fold 1</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Fold 2</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Fold 3</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Fold 4</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Fold 5</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Testing</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Testing</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Testing</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Testing</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal">Training</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Testing</strong></p>
          </td>
        </tr>
      </tbody>
    </table>
    <figure class="mediaobject">Table 1.1: Setup for five-fold cross-validation</figure>
    <p class="normal">K-fold cross-validation often has a lower variance compared to LOOCV, since we’re using a chunk of samples instead of a single one for validation.</p>
    <p class="normal">We can also randomly split the <a id="_idIndexMarker078"/>data into training and testing sets numerous<a id="_idIndexMarker079"/> times. This is formally called the <strong class="keyWord">holdout</strong> method. The problem with this algorithm is that some samples may never end up in the testing set, while some may be selected multiple times in the testing set.</p>
    <p class="normal">Last but not least, <strong class="keyWord">nested cross-validation</strong> is a<a id="_idIndexMarker080"/> combination of cross-validations. It consists of the following two phases:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Inner cross-validation</strong>: This phase is conducted to find the best fit and can be implemented as a <em class="italic">k</em>-fold cross-validation</li>
      <li class="bulletList"><strong class="keyWord">Outer cross-validation</strong>: This phase is <a id="_idIndexMarker081"/>used for performance evaluation and statistical analysis</li>
    </ul>
    <p class="normal">We will apply cross-validation very intensively throughout this entire book. Before that, let’s look at cross-validation with an analogy next, which will help us to better understand it.</p>
    <p class="normal">A data scientist plans to take his car to work, and his goal is to arrive before 9 a.m. every day. He needs to decide the departure time and the route to take. He tries out different combinations of these two parameters on certain Mondays, Tuesdays, and Wednesdays and records the arrival time for each trial. He then figures out the best schedule and applies it every day. However, it doesn’t work quite as well as expected.</p>
    <p class="normal">It turns out the scheduling <strong class="keyWord">model</strong> is overfitted to the data points gathered in the first three days and may not work well on Thursdays and Fridays. A better solution would be to test the best combination of parameters derived from Mondays to Wednesdays on Thursdays and Fridays and similarly repeat this process, based on different sets of learning days and testing days of the week. This analogized cross-validation ensures that the selected schedule works for the whole week.</p>
    <p class="normal">In summary, cross-validation derives a more accurate assessment of model performance by combining measures of prediction performance on different subsets of data. This technique not only reduces<a id="_idIndexMarker082"/> variance and avoids overfitting but also gives an insight into how a model will generally perform in practice.</p>
    <h2 class="heading-2" id="_idParaDest-30">Avoiding overfitting with regularization</h2>
    <p class="normal">Another way of preventing overfitting is <strong class="keyWord">regularization</strong>. Recall that the unnecessary complexity of a model is a source of overfitting. Regularization adds extra parameters to the error function we’re trying to minimize, in order to penalize complex models.</p>
    <p class="normal">According to the principle of Occam’s <a id="_idIndexMarker083"/>razor, simpler methods are to be favored. William Occam was a monk and philosopher who, around the year 1320, came up with the idea that the simplest hypothesis that fits data should be preferred. One justification for this is that we can invent fewer simple models than complex models. For instance, intuitively, we know that there are more high-polynomial models than linear ones. The reason is that a line (<em class="italic">y</em> = <em class="italic">ax</em> + <em class="italic">b</em>) is governed by only two parameters—the intercept, <em class="italic">b</em>, and slope, <em class="italic">a</em>. The possible coefficients for a line span two-dimensional space. A quadratic polynomial adds an extra coefficient for the quadratic term, and we can span a three-dimensional space with the coefficients. Therefore, it is much easier to find a model that perfectly captures all training data points with a <strong class="keyWord">high-order polynomial function</strong>, as its search space is much larger than that of a linear function. However, these<a id="_idIndexMarker084"/> easily obtained models generalize worse than linear models, which are more prone to overfitting. Also, of course, simpler models require less computation time. The following diagram displays how we try to fit a linear function and a high order polynomial function, respectively, to the data:</p>
    <figure class="mediaobject"><img alt="A picture containing line, screenshot, text, diagram  Description automatically generated" src="../Images/B21047_01_10.png"/></figure>
    <p class="packt_figref">Figure 1.10: Fitting data with a linear function and a polynomial function</p>
    <p class="normal">The linear model is preferable, as it may generalize better to more data points drawn from the underlying distribution. We can use regularization to reduce the influence of the high orders of a polynomial by <a id="_idIndexMarker085"/>imposing penalties on them. This will discourage complexity, even though a less accurate and less strict rule is learned from the training data.</p>
    <p class="normal">We will employ regularization quite often in this book, starting from <em class="chapterRef">Chapter 4</em>, <em class="italic">Predicting Online Ad Click-Through with Logistic Regression</em>. For now, let’s look at an analogy that can help you better understand regularization.</p>
    <p class="normal">A data scientist wants to equip his robotic guard dog with the ability to identify strangers and his friends. He feeds it with the following learning samples:</p>
    <table class="table-container" id="table002">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal">Male</p>
          </td>
          <td class="table-cell">
            <p class="normal">Young</p>
          </td>
          <td class="table-cell">
            <p class="normal">Tall</p>
          </td>
          <td class="table-cell">
            <p class="normal">With glasses</p>
          </td>
          <td class="table-cell">
            <p class="normal">In grey</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Friend</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Female</p>
          </td>
          <td class="table-cell">
            <p class="normal">Middle</p>
          </td>
          <td class="table-cell">
            <p class="normal">Average</p>
          </td>
          <td class="table-cell">
            <p class="normal">Without glasses</p>
          </td>
          <td class="table-cell">
            <p class="normal">In black</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Stranger</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Male</p>
          </td>
          <td class="table-cell">
            <p class="normal">Young</p>
          </td>
          <td class="table-cell">
            <p class="normal">Short</p>
          </td>
          <td class="table-cell">
            <p class="normal">With glasses</p>
          </td>
          <td class="table-cell">
            <p class="normal">In white</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Friend</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Male</p>
          </td>
          <td class="table-cell">
            <p class="normal">Senior</p>
          </td>
          <td class="table-cell">
            <p class="normal">Short</p>
          </td>
          <td class="table-cell">
            <p class="normal">Without glasses</p>
          </td>
          <td class="table-cell">
            <p class="normal">In black</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Stranger</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Female</p>
          </td>
          <td class="table-cell">
            <p class="normal">Young</p>
          </td>
          <td class="table-cell">
            <p class="normal">Average</p>
          </td>
          <td class="table-cell">
            <p class="normal">With glasses</p>
          </td>
          <td class="table-cell">
            <p class="normal">In white</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Friend</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Male</p>
          </td>
          <td class="table-cell">
            <p class="normal">Young</p>
          </td>
          <td class="table-cell">
            <p class="normal">Short</p>
          </td>
          <td class="table-cell">
            <p class="normal">Without glasses</p>
          </td>
          <td class="table-cell">
            <p class="normal">In red</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Friend</strong></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 1.2: Training samples for the robotic guard dog</p>
    <p class="normal">The robot may quickly learn the following rules:</p>
    <ul>
      <li class="bulletList">Any middle-aged<a id="_idIndexMarker086"/> female of average height without glasses and dressed in black is a stranger</li>
      <li class="bulletList">Any senior short male without glasses and dressed in black is a stranger</li>
      <li class="bulletList">Anyone else is his friend</li>
    </ul>
    <p class="normal">Although these perfectly fit the training data, they seem too complicated and unlikely to generalize well to new visitors. In contrast, the data scientist limits the learning aspects. A loose rule that can work well for hundreds of other visitors could be as follows: anyone without glasses dressed in black is a stranger.</p>
    <p class="normal">Besides penalizing complexity, we can also stop a training procedure early as a technique to prevent overfitting. If we limit the time a model spends learning or set some internal stopping criteria, it’s more likely to produce a simpler model. The model complexity will be controlled in this way; hence, overfitting becomes less probable. This approach is called <strong class="keyWord">early stopping</strong> in <a id="_idIndexMarker087"/>machine learning.</p>
    <p class="normal">Last but not least, it’s worth noting that regularization should be kept at a moderate level or, to be more precise, fine-tuned to an optimal level. Too small a regularization doesn’t make any impact; too large a regularization will result in underfitting, as it moves the model away from the ground truth. We will explore how to achieve optimal regularization in <em class="chapterRef">Chapter 4</em>, <em class="italic">Predicting Online Ad Click-Through with Logistic Regression</em>, <em class="chapterRef">Chapter 5</em>, <em class="italic">Predicting Stock Prices with Regression Algorithms</em>, and <em class="chapterRef">Chapter 6</em>, <em class="italic">Predicting Stock Prices with Artificial Neural Networks</em>.</p>
    <h2 class="heading-2" id="_idParaDest-31">Avoiding overfitting with feature selection and dimensionality reduction</h2>
    <p class="normal">We typically represent data as a grid of numbers (a <strong class="keyWord">matrix</strong>). Each column represents a variable, which we call a <strong class="keyWord">feature</strong> in<a id="_idIndexMarker088"/> machine learning. In supervised learning, one of the variables is actually not a feature but the label that we’re trying to predict. And in supervised learning, each row is an example that we can use for training or testing.</p>
    <p class="normal">The number of features corresponds to the dimensionality of the data. Our machine learning approach depends on the number of dimensions versus the number of examples. For instance, text and image data are very high dimensional, while sensor data (such as temperature, pressure, or GPS) has relatively fewer dimensions.</p>
    <p class="normal">Fitting high-dimensional data is computationally expensive and prone to overfitting, due to the high complexity. Higher dimensions are also impossible to visualize, and therefore, we can’t use simple diagnostic methods.</p>
    <p class="normal">Not all of the features are useful, and they may only add randomness to our results. Therefore, it’s often important to do good <a id="_idIndexMarker089"/>feature selection. <strong class="keyWord">Feature selection</strong> is the process of picking a subset of significant features for use in better <a id="_idIndexMarker090"/>model construction. In practice, not every feature in a dataset carries information useful for discriminating samples; some features are either redundant or irrelevant and, hence, can be discarded with little loss.</p>
    <p class="normal">In principle, feature selection boils down to multiple binary decisions about whether to include a feature. For <em class="italic">n</em> features, we get <em class="italic">2</em><sup class="superscript-italic" style="font-style: italic;">n</sup> feature sets, which can be a very large number for a large number of features. For example, for 10 features, we have 1,024 possible feature sets (for instance, if we’re deciding what clothes to wear, the features can be temperature, rain, the weather forecast, and where we’re going). Basically, we have two options: we either start with all of the features and remove features iteratively, or we start with a minimum set of features and add features iteratively. We then take the best feature sets for each iteration and compare them. At a certain point, brute-force evaluation becomes infeasible. Hence, more advanced feature selection algorithms were invented to distill the most useful features/signals. We will discuss in detail how to perform feature selection in <em class="chapterRef">Chapter 4</em>, <em class="italic">Predicting Online Ad Click-Through with Logistic Regression</em>.</p>
    <p class="normal">Another common approach to reducing dimensionality is to transform high-dimensional data into lower-dimensional<a id="_idIndexMarker091"/> space. This is known as <strong class="keyWord">dimensionality reduction</strong> or <strong class="keyWord">feature projection</strong>. We will get into <a id="_idIndexMarker092"/>this in detail in <em class="chapterRef">Chapter 7</em>, <em class="italic">Mining the 20 Newsgroups Dataset with Text Analysis Techniques</em>, where we will encode text data into two dimensions, and <em class="chapterRef">Chapter 9</em>, <em class="italic">Recognizing Faces with Support Vector Machine</em>, where we will talk about projecting high-dimensional image data into low-dimensional space.</p>
    <p class="normal">In this section, we talked about how the goal of machine learning is to find the optimal generalization to the data, and how to avoid ill-generalization. In the next two sections, we will explore tricks to get closer to the goal throughout individual phases of machine learning, including data preprocessing and feature engineering in the next section, and modeling in the section after that.</p>
    <h1 class="heading-1" id="_idParaDest-32">Data preprocessing and feature engineering</h1>
    <p class="normal">Data preprocessing and feature engineering play a crucial and foundational role in machine learning. It’s like laying the groundwork for a building – the stronger and better prepared the foundation, the better the final structure (machine learning model) will be. Here is a breakdown of their relationship:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Preprocessing prepares data for efficient learning</strong>: Raw data from various sources often contains inconsistencies, errors, and irrelevant information. Preprocessing cleans, organizes, and transforms the data into a format suitable for the chosen<a id="_idIndexMarker093"/> machine learning algorithm. This allows the algorithm to understand the data more easily and efficiently, leading to better model performance.</li>
      <li class="bulletList"><strong class="keyWord">Preprocessing helps improve model accuracy and generalizability</strong>: By handling missing values, outliers, and inconsistencies, preprocessing reduces noise in data. This enables a model to focus on the true patterns and relationships within the data, leading to more accurate predictions and better generalization on unseen data.</li>
      <li class="bulletList"><strong class="keyWord">Feature engineering provides meaningful input variables</strong>: Raw data is transformed and manipulated to create new features or select relevant ones. New features potentially improve model performance and insight generation.</li>
    </ul>
    <p class="normal">Overall, data preprocessing and feature engineering is an essential step in the machine learning workflow. By dedicating time and effort to proper preprocessing and feature engineering, you lay the foundation to build reliable, accurate, and generalizable machine learning models. We will cover the preprocessing phase first in this section.</p>
    <h2 class="heading-2" id="_idParaDest-33">Preprocessing and exploration</h2>
    <p class="normal">When we learn, we require high-quality learning material. We can’t learn from gibberish, so we automatically ignore anything that doesn’t make sense. A machine learning system isn’t able to recognize <a id="_idIndexMarker094"/>gibberish, so we need to help it by cleaning the input data. It’s often claimed that cleaning the data forms a large part of machine learning. Sometimes, the cleaning is already done for us, but you shouldn’t count on it.</p>
    <p class="normal">To decide how to clean data, we need to be familiar with it. There are some projects that try to automatically explore the data and do something intelligent, such as producing a report. For now, unfortunately, we don’t have a solid solution in general, so you need to do some work.</p>
    <p class="normal">We can do two things, which aren’t mutually exclusive: first, scan the data, and second, visualize the data. This also depends on the type of data we’re dealing with—whether we have a grid of numbers, images, audio, text, or something else.</p>
    <p class="normal">Ultimately, a grid of numbers is the most convenient form, and we will always work toward having numerical features. Let’s pretend that we have a table of numbers in the rest of this section.</p>
    <p class="normal">We want to know whether features have missing values, how the values are distributed, and what type of features we have. Values can approximately follow a normal distribution, a binomial distribution, a Poisson distribution, or another distribution altogether. Features can be binary: either yes or no, positive or negative, and so on. They can also be categorical: pertaining to a category, such as continents (Africa, Asia, Europe, South America, North America, and so on). Categorical variables can also be ordered, for instance, high, medium, and low. Features can also be quantitative, for example, the temperature in degrees or the price in dollars. Now, let’s dive into how we can cope with each of these situations.</p>
    <h3 class="heading-3" id="_idParaDest-34">Dealing with missing values</h3>
    <p class="normal">Quite often, we miss values for certain<a id="_idIndexMarker095"/> features. This could happen for various reasons. It can be inconvenient, expensive, or even impossible to always have a value. Maybe we weren’t able to measure a certain quantity in the past because we didn’t have the right equipment or just didn’t know that the feature was relevant. However, we’re stuck with missing values from the past.</p>
    <p class="normal">Sometimes, it’s easy to figure out that we’re missing values, and we can discover this just by scanning the data or counting the number of values we have for a feature and comparing this figure with the number of values we expect, based on the number of rows. Certain systems encode missing values with, for example, values such as 999,999 or -1. This makes sense if the valid values are much smaller than 999,999. If you’re lucky, you’ll have information about the features provided by whoever created the data in the form of a data dictionary or metadata.</p>
    <p class="normal">Once we know that we’re<a id="_idIndexMarker096"/> missing values, the question arises of how to deal with them. The simplest answer is to just ignore them. However, some algorithms can’t deal with missing values, and the program will just refuse to continue. In other circumstances, ignoring missing values will lead to inaccurate results. The second solution is to substitute missing values with a fixed value—this is called <strong class="keyWord">imputing</strong>. We can impute the arithmetic <strong class="keyWord">mean</strong>, <strong class="keyWord">median</strong>, or <strong class="keyWord">mode</strong> of the valid values of<a id="_idIndexMarker097"/> a certain feature. Ideally, we will have some prior knowledge of a variable that is somewhat reliable. For instance, we may know the seasonal averages of temperature for a certain location and be able to impute guesses for missing temperature values, given a date. We will talk about dealing with missing data in detail in <em class="chapterRef">Chapter 10</em>, <em class="italic">Machine Learning Best Practices</em>. Similarly, techniques in the following sections will be discussed and employed in later chapters, just in case you feel uncertain about how they can be used.</p>
    <h3 class="heading-3" id="_idParaDest-35">Label encoding</h3>
    <p class="normal">Humans are able to deal with various types of values. Machine learning algorithms (with some exceptions) require<a id="_idIndexMarker098"/> numerical values. If we offer a string <a id="_idIndexMarker099"/>such as <code class="inlineCode">Ivan</code>, unless we’re using specialized software, the program won’t know what to do. In this example, we’re dealing with a categorical feature—names, probably. We can consider each unique value to be a label. (In this particular example, we also need to decide what to do with the case—is <code class="inlineCode">Ivan</code> the same as <code class="inlineCode">ivan</code>?). We can then replace each label with an integer—<strong class="keyWord">label encoding</strong>.</p>
    <p class="normal">The following example shows how label encoding works:</p>
    <table class="table-container" id="table003">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Label</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Encoded Label</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Africa</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Asia</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Europe</p>
          </td>
          <td class="table-cell">
            <p class="normal">3</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">South America</p>
          </td>
          <td class="table-cell">
            <p class="normal">4</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">North America</p>
          </td>
          <td class="table-cell">
            <p class="normal">5</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Other</p>
          </td>
          <td class="table-cell">
            <p class="normal">6</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 1.3: Example of label encoding</p>
    <p class="normal">This approach can be problematic in some cases because the learner may conclude that there is an order (unless it is expected, for example, <em class="italic">bad=0</em>, <em class="italic">ok=1</em>, <em class="italic">good=2</em>, and <em class="italic">excellent=3</em>). In the preceding mapping table, <code class="inlineCode">Asia</code> and <code class="inlineCode">North America</code> in the preceding case differ by <code class="inlineCode">4</code> after encoding, which<a id="_idIndexMarker100"/> is a bit<a id="_idIndexMarker101"/> counterintuitive, as it’s hard to quantify them. One-hot encoding in the next section takes an alternative approach.</p>
    <h3 class="heading-3" id="_idParaDest-36">One-hot encoding</h3>
    <p class="normal">The <strong class="keyWord">one-of-K</strong>, or <strong class="keyWord">one-hot encoding</strong>, scheme uses<a id="_idIndexMarker102"/> dummy variables to encode categorical features. Originally, it was applied to digital circuits. The dummy variables have binary values<a id="_idIndexMarker103"/> such as bits, so they take the values zero or one (equivalent to true or false). For instance, if we want to <a id="_idIndexMarker104"/>encode continents, we will have dummy variables, such as <code class="inlineCode">is_asia</code>, which will be true if the continent is <code class="inlineCode">Asia</code> and false otherwise. In general, we need as many dummy variables as there are unique values minus one (or sometimes the exact number of unique values). We can determine one of the labels automatically from the dummy variables because they are exclusive.</p>
    <p class="normal">If the dummy variables all have a false value, then the correct label is the label for which we don’t have a dummy variable. The following table illustrates the encoding for continents:</p>
    <table class="table-container" id="table004">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Continent</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Is_africa</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Is_asia</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Is_europe</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Is_sam</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Is_nam</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Africa</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Asia</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Europe</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">South America</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">North America</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Other</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
          <td class="table-cell">
            <p class="normal">0</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 1.4: Example of one-hot encoding</p>
    <p class="normal">The encoding produces a matrix (grid of numbers) with lots of zeros (false values) and occasional ones (true values). This <a id="_idIndexMarker105"/>type of matrix is called a <strong class="keyWord">sparse matrix</strong>. The sparse matrix representation is<a id="_idIndexMarker106"/> handled well by the <code class="inlineCode">scipy</code> package, which we will discuss later in this chapter.</p>
    <h3 class="heading-3" id="_idParaDest-37">Dense embedding</h3>
    <p class="normal">While one-hot encoding is a simple and sparse<a id="_idIndexMarker107"/> representation of categorical features, <strong class="keyWord">dense embedding</strong> provides a compact, continuous <a id="_idIndexMarker108"/>representation that captures semantic relationships based on the co-occurrence patterns in data. For example, using dense embedding, the continent categories might be represented by 3-dimensional continuous vectors like:</p>
    <ul>
      <li class="bulletList">Africa: [0.9, -0.2, 0.5]</li>
      <li class="bulletList">Asia: [-0.1, 0.8, 0.6]</li>
      <li class="bulletList">Europe: [0.6, 0.3, -0.7]</li>
      <li class="bulletList">South America: [0.5, 0.2, 0.1]</li>
      <li class="bulletList">North America: [0.4, 0.3, 0.2]</li>
      <li class="bulletList">Other: [-0.8, -0.5, 0.4]</li>
    </ul>
    <p class="normal">In this example, you may notice the vectors of South America and North America are closer together than those of Africa and Asia. Dense embedding can capture the similarities between categories. In another example, you may see more closeness of the vectors of Europe and North <a id="_idIndexMarker109"/>America, based on cultural similarity.</p>
    <p class="normal">We will explore dense embedding further in <em class="chapterRef">Chapter 7</em>, <em class="italic">Mining the 20 Newsgroups Dataset with Text Analysis Techniques</em>.</p>
    <h3 class="heading-3" id="_idParaDest-38">Scaling</h3>
    <p class="normal">Values of different features can differ by orders of magnitude. Sometimes, this can mean that the larger values dominate the smaller values. This depends on the algorithm we use. For certain algorithms to work properly, we’re required to scale data.</p>
    <p class="normal">There are the following several common strategies that we can apply:</p>
    <ul>
      <li class="bulletList">Standardization removes the mean of a feature and divides it by the standard deviation. If the feature values are normally distributed, we will get a <strong class="keyWord">Gaussian</strong>, which is <a id="_idIndexMarker110"/>centered around zero with a variance of one.</li>
      <li class="bulletList">If the feature values aren’t normally distributed, we can remove the median and divide by the interquartile <a id="_idIndexMarker111"/>range. The <strong class="keyWord">interquartile range</strong> is the range between the first and <a id="_idIndexMarker112"/>third quartile (or 25<sup class="superscript">th</sup> and 75<sup class="superscript">th</sup> percentile).</li>
      <li class="bulletList">A range between zero and one is a common choice of range for feature scaling.</li>
    </ul>
    <p class="normal">We will use scaling in many projects throughout the book.</p>
    <p class="normal">An advanced version of data preprocessing is usually called feature engineering. We will cover that next.</p>
    <h2 class="heading-2" id="_idParaDest-39">Feature engineering</h2>
    <p class="normal"><strong class="keyWord">Feature engineering</strong> is the process of <a id="_idIndexMarker113"/>creating or improving features. Features are often created based on common sense, domain knowledge, or prior experience. There are certain common techniques for feature creation; however, there is no guarantee that creating new features will improve your results. We are sometimes able to use the clusters found by unsupervised learning as extra features. <strong class="keyWord">Deep neural networks</strong> are often able to<a id="_idIndexMarker114"/> derive features automatically.</p>
    <p class="normal">We will briefly look at some feature engineering techniques: polynomial transformation and binning.</p>
    <h3 class="heading-3" id="_idParaDest-40">Polynomial transformation</h3>
    <p class="normal">If we have two features, <em class="italic">a</em> and <em class="italic">b</em>, we can <a id="_idIndexMarker115"/>suspect that there is a polynomial relationship, such as <em class="italic">a</em><sup class="superscript">2</sup> + <em class="italic">ab</em> + <em class="italic">b</em><sup class="superscript">2</sup>. We can consider a new feature an <strong class="keyWord">interaction</strong> between <em class="italic">a</em> and <em class="italic">b</em>, such as the product <em class="italic">ab</em>. An interaction doesn’t have to be a product—although this is the most common choice—it can also be a sum, a difference, or a ratio. If we use a ratio to avoid dividing by zero, we should add a small constant to the divisor and dividend.</p>
    <p class="normal">The number of features and the order of the polynomial for a polynomial relationship aren’t limited. However, if we follow the Occam’s razor principle, we should avoid higher-order polynomials and interactions of many features. In practice, complex polynomial relations tend to <a id="_idIndexMarker116"/>be more difficult to compute and tend to overfit, but if you really need better results, they may be worth considering. We will see polynomial transformation in action in <em class="italic">Best practice 12</em> – <em class="italic">performing feature engineering without domain expertise</em> section in <em class="chapterRef">Chapter 10</em>, <em class="italic">Machine Learning Best Practices</em>.</p>
    <h3 class="heading-3" id="_idParaDest-41">Binning</h3>
    <p class="normal">Sometimes, it’s useful to separate<a id="_idIndexMarker117"/> feature values into several bins. For example, we may only be interested in whether it rained on a particular day. Given the precipitation values, we can binarize the values so that we get a true value if the precipitation value isn’t zero, and a false value otherwise. We can also use statistics to divide values into high, low, and medium bins. In marketing, we often care more about the age group, such as 18 to 24, than a specific age, such as 23.</p>
    <p class="normal">The binning process inevitably leads to a loss of information. However, depending on your goals, this may not be an issue, actually reducing the chance of overfitting. Certainly, there will be improvements in speed and a reduction of memory or storage requirements and redundancy.</p>
    <p class="normal">Any real-world machine learning system should have two modules: a data preprocessing module, which we just covered in this section, and a modeling module, which will be covered next.</p>
    <h1 class="heading-1" id="_idParaDest-42">Combining models</h1>
    <p class="normal">A model takes in data (usually preprocessed) and produces predictive results. What if we employ multiple models? Will we make better decisions by combining predictions from individual models? We will talk about this<a id="_idIndexMarker118"/> in this section.</p>
    <p class="normal">Let’s start with an analogy. In high school, we sit together with other students and learn together, but we aren’t supposed to work together during the exam. The reason is, of course, that teachers want to know what we’ve learned, and if we just copy exam answers from friends, we may not have learned anything. Later in life, we discover that teamwork is important. For example, this book is the product of a whole team, or possibly a group of teams.</p>
    <p class="normal">Clearly, a team can produce better results than a single person. However, this goes against Occam’s razor, since a single person can come up with simpler theories compared to what a team will produce. In machine learning, we nevertheless prefer to have our models cooperate<a id="_idIndexMarker119"/> with the following model combination schemes:</p>
    <ul>
      <li class="bulletList">Voting and averaging</li>
      <li class="bulletList">Bagging</li>
      <li class="bulletList">Boosting</li>
      <li class="bulletList">Stacking</li>
    </ul>
    <p class="normal">Let’s dive into each of them now.</p>
    <h2 class="heading-2" id="_idParaDest-43">Voting and averaging</h2>
    <p class="normal">This is probably the most understandable type of model aggregation. It just means the final output will be <a id="_idIndexMarker120"/>the <strong class="keyWord">majority</strong> or <strong class="keyWord">average</strong> of prediction output values from multiple models. It is also possible to assign different<a id="_idIndexMarker121"/> weights to individual models in the ensemble; for example, some models that are more reliable might be given two votes.</p>
    <p class="normal">Nonetheless, combining the results of models that are highly correlated to each other doesn’t guarantee a spectacular improvement. It is better to somehow diversify the models by using different features or different algorithms. If you find two models are strongly correlated, you may, for example, decide to remove one of them from the ensemble and increase proportionally the weight of the other model.</p>
    <h2 class="heading-2" id="_idParaDest-44">Bagging</h2>
    <p class="normal"><strong class="keyWord">Bootstrap aggregating</strong>, or <strong class="keyWord">bagging</strong>, is an algorithm introduced <a id="_idIndexMarker122"/>by Leo Breiman, a distinguished<a id="_idIndexMarker123"/> statistician at the University of California, Berkeley, in 1994, which<a id="_idIndexMarker124"/> applies <strong class="keyWord">bootstrapping</strong> to machine learning<a id="_idIndexMarker125"/> problems. Bootstrapping is a statistical procedure that creates multiple datasets from an existing one by sampling data with replacement. Bootstrapping can be used to measure the properties of a model, such as bias and variance.</p>
    <p class="normal">In general, a bagging algorithm follows these steps:</p>
    <ol>
      <li class="numberedList" value="1">We generate new training sets from input training data by sampling with replacement.</li>
      <li class="numberedList">For each generated training set, we fit a new model.</li>
      <li class="numberedList">We combine the results of the models by averaging or majority voting.</li>
    </ol>
    <p class="normal">The following diagram illustrates the <a id="_idIndexMarker126"/>steps for bagging, using classification as an example (the circles and crosses represent samples from two classes):</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B21047_01_11.png"/></figure>
    <p class="packt_figref">Figure 1.11: Workflow of bagging for classification</p>
    <p class="normal">As you can imagine, bagging can<a id="_idIndexMarker127"/> reduce the chance of overfitting.</p>
    <p class="normal">We will study bagging in depth in <em class="chapterRef">Chapter 3</em>, <em class="italic">Predicting Online Ad Click-Through with Tree-Based Algorithms</em>.</p>
    <h2 class="heading-2" id="_idParaDest-45">Boosting</h2>
    <p class="normal">In the context of supervised learning, we define <strong class="keyWord">weak learners</strong> as learners who are just a little better than a baseline, such as randomly <a id="_idIndexMarker128"/>assigning classes or average values. Much like ants, weak learners <a id="_idIndexMarker129"/>are weak individually, but together, they have the power to do amazing things.</p>
    <p class="normal">It makes sense to take into account the strength of each individual learner using weights. This general idea is called <strong class="keyWord">boosting</strong>. In boosting, all models are trained in sequence, instead of in parallel as<a id="_idIndexMarker130"/> in bagging. Each model is trained on the same dataset, but each data sample has a different weight, factoring in the previous model’s success. The weights are reassigned after a model is trained, which will be used for the next training round. In general, weights for mispredicted samples are increased to stress their prediction difficulty.</p>
    <p class="normal">The following diagram illustrates the steps for boosting, again using classification as an example (the circles and crosses represent samples from two classes, and the size of a circle or cross indicates the weight assigned to it):</p>
    <p class="packt_figref"><img alt="A screenshot of a device  Description automatically generated with low confidence" src="../Images/B21047_01_12.png"/></p>
    <p class="packt_figref">Figure 1.12: Workflow of boosting for classification</p>
    <p class="normal">There are many boosting algorithms; boosting algorithms differ mostly in their weighting scheme. If you’ve studied for an exam, you may have applied a similar technique by identifying the type of practice questions you had trouble with and focusing on the hard problems.</p>
    <p class="normal">Viola-Jones, a popular face <a id="_idIndexMarker131"/>detection framework, leverages the boosting algorithm to efficiently identify faces in images. Detecting faces in images or videos is supervised learning. We give the learner examples of regions containing faces. There’s an imbalance, since we usually have far more regions that don’t have faces than those that do (about 10,000 times more).</p>
    <p class="normal">A cascade of classifiers<a id="_idIndexMarker132"/> progressively filters out these negative image areas stage by stage. In each progressive stage, the classifiers use progressively more features on fewer image windows. The idea is to spend the majority of time on image patches that contain faces. In this context, boosting is used to select features and combine results.</p>
    <h2 class="heading-2" id="_idParaDest-46">Stacking</h2>
    <p class="normal"><strong class="keyWord">Stacking</strong> takes the output values <a id="_idIndexMarker133"/>of machine learning models and then uses them as input values for another algorithm. You can, of course, feed the output of the higher-level algorithm to another predictor. It’s possible to use any arbitrary topology, but for practical reasons, you should try a simple setup first, as also dictated by Occam’s razor.</p>
    <p class="normal">A fun fact is that stacking is commonly <a id="_idIndexMarker134"/>used in the winning models in the Kaggle competition. For instance, the first place for the Otto Group Product Classification Challenge (<a href="https://www.kaggle.com/c/otto-group-product-classification-challenge"><span class="url">www.kaggle.com/c/otto-group-product-classification-challenge</span></a>) went to a stacking model composed of more than 30 different models.</p>
    <p class="normal">So far, we have covered the tricks required to more easily reach the right generalization for a machine learning model throughout the data preprocessing and modeling phase. I know you can’t wait to start working on a machine learning project. Let’s get ready by setting up the working environment.</p>
    <h1 class="heading-1" id="_idParaDest-47">Installing software and setting up</h1>
    <p class="normal">As the book title says, Python is the language we<a id="_idIndexMarker135"/> will use to implement all machine learning <a id="_idIndexMarker136"/>algorithms and techniques throughout the entire book. We will also exploit many popular Python packages and tools, such as NumPy, SciPy, scikit-learn, TensorFlow, and PyTorch. By the end of this initial chapter, make sure you have set up the tools and working environment properly, even if you are already an<a id="_idIndexMarker137"/> expert in Python or familiar with some of the aforementioned tools.</p>
    <h2 class="heading-2" id="_idParaDest-48">Setting up Python and environments</h2>
    <p class="normal">We will use Python 3 in this book. The Anaconda Python 3 distribution is one of the best options for data science and machine<a id="_idIndexMarker138"/> learning practitioners.</p>
    <p class="normal"><strong class="keyWord">Anaconda</strong> is a free Python<a id="_idIndexMarker139"/> distribution for data analysis and scientific computing. It has its own package manager, <code class="inlineCode">conda</code>. The distribution (<a href="https://docs.anaconda.com/free/anaconda/"><span class="url">https://docs.anaconda.com/free/anaconda/</span></a>, depending on your OS, or Python version 3.7 to 3.11) includes around 700 Python packages (as of 2023), which makes it very convenient. For casual users, the <strong class="keyWord">Miniconda</strong> (<a href="https://conda.io/miniconda.html"><span class="url">https://conda.io/miniconda.html</span></a>) distribution may be the better <a id="_idIndexMarker140"/>choice. Miniconda contains the <code class="inlineCode">conda</code> package<a id="_idIndexMarker141"/> manager and Python. Obviously, Miniconda takes up much less disk space than Anaconda.</p>
    <p class="normal">The procedures to install Anaconda and Miniconda are similar. You can follow the instructions from <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/"><span class="url">https://docs.conda.io/projects/conda/en/latest/user-guide/install/</span></a>. First, you must download the appropriate installer for your OS and Python version, as follows:</p>
    <figure class="mediaobject"><img alt="A picture containing text, screenshot, font  Description automatically generated" src="../Images/B21047_01_13.png"/></figure>
    <p class="packt_figref">Figure 1.13: Installation entry based on your OS</p>
    <p class="normal">Follow the steps listed in your OS. You can choose between a GUI and a CLI. I personally find the latter easier.</p>
    <p class="normal">Anaconda comes with its own Python installation. On my machine, the Anaconda installer created an <code class="inlineCode">anaconda</code> directory in my home directory and required about 900 MB. Similarly, the <code class="inlineCode">Miniconda</code> installer installs a <code class="inlineCode">miniconda</code> directory in your home directory.</p>
    <p class="normal">Feel free to play around with it after you set it up. One way to verify that you have set up Anaconda properly is by entering the following command line in your terminal on Linux/Mac or Command Prompt on Windows (from now on, we will just mention Terminal):</p>
    <pre class="programlisting con"><code class="hljs-con">python
</code></pre>
    <p class="normal">The preceding command line will <a id="_idIndexMarker142"/>display your Python running environment, as shown in the following screenshot:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B21047_01_14.png"/></figure>
    <p class="packt_figref">Figure 1.14: Screenshot after running “python” in the terminal</p>
    <p class="normal">If you don’t see this, please check the system path or the path Python is running from.</p>
    <p class="normal">To wrap up this section, I want to<a id="_idIndexMarker143"/> emphasize the reasons why Python is the most popular language for machine learning and data science. First of all, Python is famous for its high readability and simplicity, which makes it easy to build machine learning models. We spend less time worrying about getting the right syntax and compilation and, as a result, have more time to find the right machine learning solution. Second, we have an extensive selection of Python libraries and frameworks for machine learning:</p>
    <table class="table-container" id="table005">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Tasks</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Python libraries</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Data analysis</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">NumPy, SciPy, and pandas</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Data visualization</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">Matplotlib, and Seaborn</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Modeling</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">scikit-learn, TensorFlow, Keras, and PyTorch</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 1.5: Popular Python libraries for machine learning</p>
    <p class="normal">The next step involves setting up some of the packages that we will use throughout this book.</p>
    <h2 class="heading-2" id="_idParaDest-49">Installing the main Python packages</h2>
    <p class="normal">For <a id="_idIndexMarker144"/>most projects in <a id="_idIndexMarker145"/>this book, we will use NumPy (<a href="http://www.numpy.org/"><span class="url">http://www.numpy.org/</span></a>), SciPy (<a href="https://scipy.org/"><span class="url">https://scipy.org/</span></a>), the <code class="inlineCode">pandas</code> library (<a href="https://pandas.pydata.org/"><span class="url">https://pandas.pydata.org/</span></a>), scikit-learn (<a href="http://scikit-learn.org/stable/"><span class="url">http://scikit-learn.org/stable/</span></a>), TensorFlow (<a href="https://www.tensorflow.org/"><span class="url">https://www.tensorflow.org/</span></a>), and <a id="_idIndexMarker146"/>PyTorch (<a href="https://pytorch.org/"><span class="url">https://pytorch.org/</span></a>).</p>
    <p class="normal">In the sections<a id="_idIndexMarker147"/> that follow, we <a id="_idIndexMarker148"/>will cover the<a id="_idIndexMarker149"/> installation of several<a id="_idIndexMarker150"/> Python packages that we will mainly use in this book.</p>
    <div class="note">
      <p class="normal">Conda environments provide a way to isolate dependencies and packages for different projects. So it is recommended to create and use an environment for a new project. Let’s create one using the following command to create an environment called “<code class="inlineCode">pyml</code>":</p>
      <pre class="programlisting con"><code class="hljs-con">conda create --name pyml python=3.10
</code></pre>
      <p class="normal">Here, we also specify the Python version, <code class="inlineCode">3.10</code>, which is optional but highly recommended. This is to avoid using the latest Python version by default, which may not be compatible with many Python packages. For example, at the time of writing (late 2023), PyTorch does not support Python <code class="inlineCode">3.11</code>.</p>
      <p class="normal">To activate the newly created environment, we use the following command:</p>
      <pre class="programlisting con"><code class="hljs-con">conda activate pyml
</code></pre>
      <p class="normal">The activated environment is displayed in front of your prompt like this:</p>
      <pre class="programlisting con"><code class="hljs-con">(pyml) hayden@haydens-Air ~ %
</code></pre>
    </div>
    <h3 class="heading-3" id="_idParaDest-50">NumPy</h3>
    <p class="normal">NumPy is the fundamental<a id="_idIndexMarker151"/> package for machine learning with Python. It offers <a id="_idIndexMarker152"/>powerful tools including the following:</p>
    <ul>
      <li class="bulletList">The <em class="italic">N</em>-dimensional array (<code class="inlineCode">ndarray</code>) class and several subclasses representing matrices and arrays</li>
      <li class="bulletList">Various sophisticated array functions</li>
      <li class="bulletList">Useful linear algebra capabilities</li>
    </ul>
    <p class="normal">Installation instructions for NumPy can be found at <a href="https://numpy.org/install/"><span class="url">https://numpy.org/install/</span></a>. Alternatively, an easier<a id="_idIndexMarker153"/> method involves installing it with <code class="inlineCode">conda</code> or <code class="inlineCode">pip</code> in the command line, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">conda install numpy
</code></pre>
    <p class="normal">or</p>
    <pre class="programlisting con"><code class="hljs-con">pip install numpy
</code></pre>
    <p class="normal">A quick way to verify your installation is to import it in Python, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> numpy</span>
</code></pre>
    <p class="normal">It is installed correctly if no<a id="_idIndexMarker154"/> error message is visible.</p>
    <h3 class="heading-3" id="_idParaDest-51">SciPy</h3>
    <p class="normal">In machine learning, we mainly <a id="_idIndexMarker155"/>use NumPy arrays to store data vectors or matrices<a id="_idIndexMarker156"/> composed of feature vectors. SciPy (<a href="https://scipy.org/"><span class="url">https://scipy.org/</span></a>) uses<a id="_idIndexMarker157"/> NumPy arrays and offers a variety of scientific and mathematical functions. Installing SciPy in the terminal is similar, again as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">conda install scipy
</code></pre>
    <p class="normal">or</p>
    <pre class="programlisting con"><code class="hljs-con">pip install scipy
</code></pre>
    <h3 class="heading-3" id="_idParaDest-52">pandas</h3>
    <p class="normal">We also use the <code class="inlineCode">pandas</code><code class="inlineCode"><a id="_idIndexMarker158"/></code> library (<a href="https://pandas.pydata.org/"><span class="url">https://pandas.pydata.org/</span></a>) for data wrangling later in this book. The best way to<a id="_idIndexMarker159"/> get <code class="inlineCode">pandas</code> is via <code class="inlineCode">pip</code> or <code class="inlineCode">conda</code>, for example:</p>
    <pre class="programlisting con"><code class="hljs-con">conda install pandas
</code></pre>
    <h3 class="heading-3" id="_idParaDest-53">scikit-learn</h3>
    <p class="normal">The <code class="inlineCode">scikit-learn</code> library is a Python machine learning package optimized for performance, as a lot of its code runs<a id="_idIndexMarker160"/> almost as fast as equivalent C code. The same<a id="_idIndexMarker161"/> statement is true for NumPy and SciPy. <code class="inlineCode">scikit-learn</code> requires both NumPy and SciPy to be installed. As the installation guide in <a href="http://scikit-learn.org/stable/install.html"><span class="url">http://scikit-learn.org/stable/install.html</span></a> states, the easiest way to install <code class="inlineCode">scikit-learn</code> is to use <code class="inlineCode">pip</code> or <code class="inlineCode">conda</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install -U scikit-learn
</code></pre>
    <p class="normal">or</p>
    <pre class="programlisting con"><code class="hljs-con">conda install -c conda-forge scikit-learn
</code></pre>
    <p class="normal">Here, we use the “<code class="inlineCode">-c conda-forge</code>" option to tell <code class="inlineCode">conda</code> to search for packages in the <code class="inlineCode">conda-forge</code> channel, which is a community-driven channel with a wide range of open-source packages.</p>
    <h3 class="heading-3" id="_idParaDest-54">TensorFlow</h3>
    <p class="normal">TensorFlow is a Python-friendly<a id="_idIndexMarker162"/> open-source library invented by the Google Brain team for high-performance numerical computation. It makes machine learning faster and<a id="_idIndexMarker163"/> deep learning easier, with the Python-based convenient frontend API and high-performance C++-based backend execution. TensorFlow 2 was largely a redesign of its first mature version, 1.0, and was released at the end of 2019.</p>
    <p class="normal">TensorFlow has been widely known for its deep learning modules. However, its most powerful point is <strong class="keyWord">computation graphs</strong>, which <a id="_idIndexMarker164"/>algorithms are built on. Basically, a computation graph is used to convey relationships between the input and the output via tensors. </p>
    <p class="normal">For instance, if we want to evaluate a linear relationship, <em class="italic">y = 3 * a + 2 * b</em>, we can represent it in the following computation graph:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, circle, diagram, sketch  Description automatically generated" src="../Images/B21047_01_15.png"/></figure>
    <p class="packt_figref">Figure 1.15: Computation graph for a y = 3 * a + 2 * b machine</p>
    <p class="normal">Here, <em class="italic">a</em> and <em class="italic">b</em> are the input tensors, <em class="italic">c</em> and <em class="italic">d</em> are the intermediate tensors, and y is the output.</p>
    <p class="normal">You can think of a computation graph as a network of nodes connected by edges. Each node is a tensor, and each edge is an operation or function that takes its input node and returns a value to its <a id="_idIndexMarker165"/>output node. To train a machine learning model, TensorFlow builds the computation graph and computes the <strong class="keyWord">gradients</strong> accordingly (gradients are vectors that provide<a id="_idIndexMarker166"/> the steepest <a id="_idIndexMarker167"/>direction where an optimal solution is reached). In the upcoming chapters, you will see some examples of training machine learning models using <code class="inlineCode">TensorFlow</code>.</p>
    <div class="packt_tip">
      <p class="normal">We highly recommend you go through <a href="https://www.tensorflow.org/guide/data"><span class="url">https://www.tensorflow.org/guide/data</span></a> if you are interested in exploring more about TensorFlow and computation graphs.</p>
    </div>
    <p class="normal">TensorFlow allows easy deployment of computation across CPUs and GPUs, which empowers expensive and large-scale machine learning. In this book, we will focus on the CPU as our computation platform. Hence, according to <a href="https://www.tensorflow.org/install/"><span class="url">https://www.tensorflow.org/install/</span></a>, installing<a id="_idIndexMarker168"/> TensorFlow 2 is done via the following command line:</p>
    <pre class="programlisting con"><code class="hljs-con">conda install -c conda-forge tensorflow
</code></pre>
    <p class="normal">or</p>
    <pre class="programlisting con"><code class="hljs-con">pip install tensorflow
</code></pre>
    <p class="normal">You can always verify the installation by importing it in Python.</p>
    <h3 class="heading-3" id="_idParaDest-55">PyTorch</h3>
    <p class="normal">PyTorch is an open-source machine learning library primarily used to develop deep learning models. It provides a flexible and efficient framework to build neural networks and perform computations on GPUs. PyTorch was developed by Facebook’s AI Research lab and is widely used in both research and industry.</p>
    <p class="normal">Similar to TensorFlow, PyTorch <a id="_idIndexMarker169"/>performs its computations based <a id="_idIndexMarker170"/>on a <strong class="keyWord">directed acyclic graph</strong> (<strong class="keyWord">DAG</strong>). The difference is that PyTorch<a id="_idIndexMarker171"/> utilizes a <strong class="keyWord">dynamic computational graph</strong>, which allows for on-the-fly graph construction during runtime, while TensorFlow uses a <strong class="keyWord">static</strong> computational <a id="_idIndexMarker172"/>graph, where the graph structure is defined upfront and then executed. This dynamic nature enables greater flexibility in model design and easier debugging, and also facilitates dynamic control flow, making it suitable for a wide range of applications.</p>
    <p class="normal">PyTorch has become a popular choice among researchers and practitioners in the field of deep learning, due to its flexibility, ease of use, and efficient computational capabilities. Its intuitive interface and strong community support make it a powerful tool for various applications, including computer vision, natural language processing, reinforcement learning, and more.</p>
    <p class="normal">To install PyTorch, it is recommended to look up the command in the latest instructions on <a href="https://pytorch.org/get-started/locally/"><span class="url">https://pytorch.org/get-started/locally/</span></a>, based on the system and method.</p>
    <p class="normal">As an example, we install the latest stable version (<code class="inlineCode">2.2.0</code> as of late 2023) via <code class="inlineCode">conda</code> on a Mac using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">conda install pytorch::pytorch torchvision  -c pytorch
</code></pre>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">If you encounter issues in installation, please read more about the platform and package-specific recommendations provided on the instructions page. All PyTorch code in this book can be run on your CPU, unless specifically indicated for a GPU only. However, using a GPU is recommended if you want to expedite training neural network models and fully enjoy the benefits of PyTorch. If you have a graphics card, refer to the instructions and set up PyTorch with the appropriate compute platform. For example, I install it on Windows with a GPU using the following command:</p>
      <pre class="programlisting con"><code class="hljs-con">conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia
</code></pre>
      <p class="normal">To check if PyTorch with GPU support is installed correctly, run the following Python code:</p>
      <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> torch</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.cuda.is_available()</span>
True
</code></pre>
      <p class="normal">Alternatively, you can use Google Colab (<a href="https://colab.research.google.com/"><span class="url">https://colab.research.google.com/</span></a>) to train some neural network models using GPUs for free.</p>
    </div>
    <p class="normal">There are many other <a id="_idIndexMarker173"/>packages we will use intensively, for example, <strong class="keyWord">Matplotlib</strong> for <a id="_idIndexMarker174"/>plotting and visualization, <strong class="keyWord">Seaborn</strong> for visualization, <strong class="keyWord">NLTK</strong> for natural language <a id="_idIndexMarker175"/>processing tasks, <strong class="keyWord">transformers</strong> for state-of-the-art <a id="_idIndexMarker176"/>models pretrained on large datasets, and <strong class="keyWord">OpenAI Gym</strong> for reinforcement<a id="_idIndexMarker177"/> learning. We will provide installation details for any package when we first encounter it in this book.</p>
    <h1 class="heading-1" id="_idParaDest-56">Summary</h1>
    <p class="normal">We just finished our first mile on the Python and machine learning journey! Throughout this chapter, we became familiar with the basics of machine learning. We started with what machine learning is all about, the importance of machine learning and its brief history, and looked at recent developments as well. We also learned typical machine learning tasks and explored several essential techniques to work with data and models. Now that we’re equipped with basic machine learning knowledge and have set up the software and tools, let’s get ready for the real-world machine learning examples ahead.</p>
    <p class="normal">In the next chapter, we will build a movie recommendation engine as our first machine learning project!</p>
    <h1 class="heading-1" id="_idParaDest-57">Exercises</h1>
    <ol>
      <li class="numberedList" value="1">Can you tell the difference between machine learning and traditional programming (rule-based automation)?</li>
      <li class="numberedList">What’s overfitting, and how do we avoid it?</li>
      <li class="numberedList">Name two feature engineering approaches.</li>
      <li class="numberedList">Name two ways to combine multiple models.</li>
      <li class="numberedList">Install Matplotlib (<a href="https://matplotlib.org/"><span class="url">https://matplotlib.org/</span></a>) if this is of interest to you. We will use it for data visualization throughout the book.</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-58">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/yuxi"><span class="url">https://packt.link/yuxi</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code1878468721786989681.png"/></p>
  </div>
</body></html>
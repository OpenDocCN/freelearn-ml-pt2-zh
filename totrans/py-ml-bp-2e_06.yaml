- en: Predict whether Your Content Will Go Viral
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like many great things, this all begins with a bet. It was 2001, and Jonah Peretti,
    a graduate student at MIT at the time, was procrastinating. Instead of writing
    his thesis, he had decided to take up Nike on their offer to personalize a pair
    of sneakers. Under a recently launched program, anyone could do so from their
    website, NIKEiD. The only problem, at least from Nike's point of view, was that
    emblazoning them with the word *sweatshop*, as Peretti had requested, was a non-starter.
    Peretti, in a series of emails, demurred pointing out that in no way did the word
    fall into any of the categories of objectionable terms that would result in his
    personalization request being rejected.
  prefs: []
  type: TYPE_NORMAL
- en: Peretti, believing others might find the back-and-forth with Nike's customer
    service representatives amusing as well, forwarded them to a number of close friends.
    Within days, the emails had found their way into inboxes across the world. Major
    media outlets, such as Time, Salon, The Guardian, and even the Today Show, had
    picked up on it. Peretti was at the center of a viral sensation.
  prefs: []
  type: TYPE_NORMAL
- en: But the question that began nagging at Peretti was, could this sort of thing
    be replicated? His friend, Cameron Marlow, had been preparing to write his PhD
    thesis on viral phenomena, and was adamant that such things were far too complex
    for anyone to engineer. And it is here that the bet comes into play. Marlow wagered
    that Peretti could not repeat the success he had enjoyed with that original set
    of emails with Nike.
  prefs: []
  type: TYPE_NORMAL
- en: Fast forward 15 years, and Jonah Peretti leads the website whose name has become
    synonymous with virality—BuzzFeed. With more than 77 million unique visitors in
    2015, it ranked higher than the New York Times in total reach. I think it's safe
    to say that Peretti won that bet.
  prefs: []
  type: TYPE_NORMAL
- en: But how exactly did Peretti do it? How did he piece together the secret formula
    for creating content that spreads like wildfire? In this chapter, we'll attempt
    to unravel some of these mysteries. We'll examine some of the most shared content
    and attempt to find the common elements that differentiate it from the content
    people were less willing to share.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What does research tell us about virality?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sourcing shared counts and content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the features of shareability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a predictive content scoring model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does research tell us about virality?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding sharing behavior is big business. As consumers become increasingly
    blind to traditional advertising year on year, the push is on to go beyond simple
    pitches to tell engaging stories. And increasingly, the success of these endeavors
    is measured in social shares. Why go to so much trouble? Because, as a brand,
    every share I receive represents another consumer I've reached—all without spending
    an additional cent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of this value, several researchers have examined sharing behavior in
    the hope of understanding what motivates it. Among the reasons researchers have
    found are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: To provide practical value to others (an altruistic motive)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To associate ourselves with certain ideas and concepts (an identity motive)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To bond with others around a common emotion (a communal motive)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With regard to the last motive, one particularly well-designed study looked
    at the 7,000 pieces of content from the New York Times to examine the effect of
    emotion on sharing. They found that simple emotional sentiment was not enough
    to explain sharing behavior, but when combined with emotional arousal, the explanatory
    power was greater.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, while sadness has a strong negative valence, it is considered
    to be a low arousal state. Anger, on the other hand, has a negative valence, which
    is paired with a high arousal state. As such, stories that sadden the reader tend
    to generate far fewer stories than anger-inducing stories. Is it any wonder then
    that much of the *fake news* that plays such a large part in politics these days
    comes in this form? Following image shows the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b9b8c3b-d282-4c1f-890b-1b728d14e6cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure taken from *What Makes Online Content Viral?* by Jonah Berger and Katherine
    L. Milkman, Journal of Marketing Research, available at: http://jonahberger.com/wp-content/uploads/2013/02/ViralityB.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'This covers the motivational aspects, but if we hold those factors constant,
    how do other attributes affect the virality of a piece of content? Some of these
    factors could include the following: headline wording, headline length, headline
    parts of speech, content length, social network of post, the topic, the timeliness
    of the subject matter, and so on. Without a doubt, a person could spend their
    entire life studying this phenomenon. For now, however, we''ll just spend the
    next 30 or so pages doing so. From there, you can decide whether you''d like to
    take it further.'
  prefs: []
  type: TYPE_NORMAL
- en: Sourcing shared counts and content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can begin exploring which features make content shareable, we need
    to get our hands on a fair amount of content, as well as data on how often it's
    shared. Unfortunately, securing this type of data has gotten more difficult in
    the last few years. In fact, when the first edition of this book came out in 2016,
    this data was easily obtainable. But today, there appears to be no free sources
    of this type of data, though if you are willing to pay, you can still find it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately for us, I have a dataset that was collected from a now defunct
    website, `ruzzit.com`. This site, when it was active, tracked the most shared
    content over time, which is exactly what we require for this project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bca0d0d0-360c-413e-a842-42d21fb7502f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll begin by loading our imports into our notebook, as we always do, and
    then load in the data. This particular data is in the form of a JSON file. We
    can read it in using the pandas `read_json()` method, as demonstrated in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89f77732-6fef-499e-b962-3e4066a30c3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the columns of this dataset to better understand what
    we''ll be working with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65c84592-3c86-4e83-8d6d-788da676f882.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s walk through what each of these columns represents:'
  prefs: []
  type: TYPE_NORMAL
- en: '`title`: The title of the article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`link`: The `ruzzit.com` link'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bb`: The number of Facebook likes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lnkdn`: The number of LinkedIn shares'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pins`: The number of Pinterest pins'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date`: The date of the article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`redirect`: The link to the original article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pg_missing`: A field that describes whether that page is available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`img_link`: The link to the image for the article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`json_data`: Additional data pertaining to the article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`site`: The domain the article is hosted on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`img_count`: The number of images contained in the article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`entities`: Person-, place-, and thing-related features of the article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`html`: The body of the article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text`: The text of the body of the article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another feature that will be instructive is the word count of each article.
    We don''t have that in our data currently, so let''s go ahead and create a function
    that will provide this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efe55afa-952b-41db-b88c-7a3f2dae6855.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s add more features. We''ll add the most prominent color of the first
    image on the page. The colors for each image are listed by RGB value in the JSON
    data, so we can extract it from there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8be8d5f-ad65-4e2d-8fbb-ddf8ec321b25.png)'
  prefs: []
  type: TYPE_IMG
- en: We've pulled the most prominent color from the first image as an RGB value,
    but we have also transformed that into a hex value. We'll use that later when
    we examine the image colors.
  prefs: []
  type: TYPE_NORMAL
- en: With our data now ready, we can begin to perform our analysis. We're going to
    attempt to find what makes content highly shareable.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the features of shareability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The stories we have collected here represent roughly the 500 most shared pieces
    of content in 2015 and early 2016\. We're going to try to deconstruct these articles
    to find the common traits that make them so shareable. We'll begin by looking
    at the image data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring image data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin by looking at the number of images included with each story. We''ll
    run a value count and then plot the numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This should display an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6207023-2df1-40aa-a943-0b94c084e7bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s plot that same information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/420c6673-50df-4242-bbff-e11f21996146.png)'
  prefs: []
  type: TYPE_IMG
- en: Already, I'm surprised by the numbers. The vast majority of stories have five
    pictures in them, while those stories that have either one or no pictures at all
    are quite rare.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we can see that people tend to share content with lots of images. Now,
    let''s take a look at the most common colors in those images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb32ef86-1152-4fdf-8a25-5b11cac04f3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I don''t know about you, but this isn''t extremely helpful given that I don''t
    see hex values as colors. We can, however, use a new feature in pandas called
    conditional formatting to help us out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebb9a741-3b26-4c14-9955-65c53d80d847.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This certainly helps, but the colors are so granular that we have over 450 unique colors
    in total. Let's use a bit of clustering to get this down to a more manageable
    range. Since we have the RBG values for each color, we can create a three-dimensional
    space to cluster them using the k-means algorithm. I won't go into the details
    of the algorithm here, but it is a fairly simple iterative algorithm based upon
    generating clusters by measuring the distance to centers and repeating. The algorithm
    does require us to select the *k*, or the number of clusters we expect. Because
    RGB ranges from 0 to 256, we'll use the square root of 256, which is 16\. That
    should give us a manageable number while retaining the characteristics of our
    palette.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll split our RGB values into individual columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll use this to run our k-means model and retrieve the center values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99774024-acd8-44f3-afc6-9911ae7f455c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we have the sixteen most popular dominant colors from the first image
    in each picture. Let''s check whether they are using our pandas `DataFrame.style()`
    method and the function we created previously to color our cells. We''ll need
    to set our index equal to the hex value of the three columns to use our `color_cells`
    function, so we''ll do that as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b49f4800-869d-4ba3-b975-48fc31cfe1b2.png)'
  prefs: []
  type: TYPE_IMG
- en: So there you have it; those are the most common colors you will see (at least
    for the first image) in the most frequently shared content. This is a bit more
    on the drab side than I had expected as the first several all seem to be shades
    of beige and gray.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on and examine the headlines of our stories.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the headlines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by creating a function we can use to examine the most common tuples.
    We''ll set it up so that we can use it later on the body text as well. We''ll
    do this using the Python **Natural Language Toolkit** (**NLTK**) library. This
    can be pip installed if you don''t have it currently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot in there, so let's unpack it. We created a function that takes
    in a series, an integer, and a Boolean value. The integer determines the *n* we'll
    use for n-gram parsing, while the Boolean determines whether or not we exclude
    stop words. The function returns the number of tuples per row and the frequency
    for each tuple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run it on our headlines, while retaining the stop words. We''ll begin
    with just single words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b68bf0e-a421-4183-8ac8-162b1fa0612f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we have the word count for each headline. Let''s see what the stats on
    this look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a848a0e-9844-406f-9848-57aba1428cfc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the median headline length for our viral stories comes in at
    exactly 11 words. Let''s take a look at the most frequently used words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94de7857-688e-4dcc-af0f-68970d4e398e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That is not exactly useful, but is in keeping with what we might expect. Now,
    let''s take a look at the same information for bi-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b85a514-c0c3-4493-bf6c-804bb0a83c1a.png)'
  prefs: []
  type: TYPE_IMG
- en: This is definitely more interesting. We can start to see some of the components
    of the headlines over and over again. The two that stand out are `(donald, trump)`
    and `(dies, at)`. Trump makes sense as he said some headline-grabbing statements
    during the election, but I was surprised by the *dies* headlines. I took a look
    at the headlines, and apparently a number of high-profile people died in the year
    in question, so that also makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run this with the stop words removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e644b50e-4c13-449f-87e8-70d0bf290465.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, we can see many things we might expect. It looks like if we changed how
    we parsed numbers (replacing each of them with a single identifier like number),
    we would likely see more of these bubble up. I'll leave that as an exercise to
    the reader, if you'd like to attempt that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at tri-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d842935-566f-41b8-9acd-1dd4dfcc85c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It seems that the more words we include, the more the headlines come to resemble
    the classic BuzzFeed prototype. In fact, let''s see whether that''s the case.
    We haven''t looked at which sites produce the most viral stories; let''s see whether
    BuzzFeed leads the charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca85393f-97c5-4256-a0cf-771359f589e1.png)'
  prefs: []
  type: TYPE_IMG
- en: We can clearly see that BuzzFeed dominates the list. In a distant second place,
    we can see The Huffington Post, which incidentally is another site that Jonah
    Peretti worked for. It appears that studying the science of virality can pay big
    dividends.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have examined images and headlines. Now, let's move on to examining
    the full text of the stories.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the story content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we created a function to examine the common n-grams that
    are found in the headlines of our stories. Now, let's apply that to explore the
    full content of our stories.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by exploring bi-grams with the stop words removed. Since headlines
    are so short compared to the body of the stories, it makes sense to look at them
    with the stop words intact, although within the story, it typically makes sense
    to eliminate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/134d9d43-1e9e-4c0d-9f18-fc64adf7b4be.png)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, we can see that the frivolity we saw in the headlines has completely
    disappeared. The text is now filled with content discussing terrorism, politics,
    and race relations.
  prefs: []
  type: TYPE_NORMAL
- en: How is it possible that the headlines are light-hearted, while the text is dark
    and controversial? I would suggest that this is because articles such as *13 Puppies
    Who Look Like Elvis* are going to have substantially less text than *The History
    of the Islamic State*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at one more. We''ll evaluate the tri-grams for the story
    bodies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c15e854-45cf-4047-9aa0-8b7fc40323a0.png)'
  prefs: []
  type: TYPE_IMG
- en: We appear to have suddenly entered the land of advertising and social pandering.
    With that, let's move on to building a predictive model for content scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Building a predictive content scoring model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's use what we have learned to create a model that can estimate the share
    counts for a given piece of content. We'll use the features we have already created,
    along with a number of additional ones.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would have a much larger sample of content—especially content that
    had more typical share counts—but we'll have to make do with what we have here.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to be using an algorithm called **random forest regression**. In
    previous chapters, we looked at a more typical implementation of random forests
    that is based on classification, but here we're going to attempt to predict the
    share counts. We could consolidate our share classes into ranges, but it is preferable
    to use regression when dealing with continuous variables, which is what we're
    working with here.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we''ll create a bare-bones model. We''ll use the number of images,
    the site, and the word count. We''ll train our model in terms of the number of
    Facebook likes. We''re also going to be splitting our data into two sets: a training
    set and a test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll import the scikit-learn library, and then we''ll prepare our
    data by removing the rows with nulls, resetting our index, and finally splitting
    the frame into our training and test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We used a random number generator with a probability set for approximately
    two-thirds and one-third to determine which row items (based on their `index`)
    would be placed in each set. Setting the probabilities like this ensures that
    we get approximately twice the number of rows in our training set compared to
    the test set. We can see this in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a1c3eea-65ba-4512-b7f8-98619dfc242d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we''ll continue with preparing our data. Next, we need to set up categorical
    encoding for our sites. Currently, our DataFrame has the name for each site represented
    with a string. We need to use dummy encoding. This creates a column for each site,
    and if the row has that particular site, then that column will be filled with
    a `1`, while all the other columns for sites will be coded with a `0`. Let''s
    do that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e33a452e-3b1d-4e81-bcad-405e5ca0f5ab.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see from the preceding output how the dummy encoding appears.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll now continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we''ve set up our `X_test`, `X_train`, `y_test`, and `y_train` variables.
    Now, we''re going to use our training data to build our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With those two lines of code, we have trained our model. Let''s use it to predict
    the Facebook likes for our test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/652a88fe-3dc0-4869-9bbf-bae54f746b38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see the predicted values, the actual value, and the difference
    as a percentage  side by side. Let''s take a look at the descriptive stats for
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85467ba6-ebe3-4d00-bb24-7d9c290c189a.png)'
  prefs: []
  type: TYPE_IMG
- en: This looks amazing. Our median error is 0! Well, unfortunately, this is a particularly
    useful bit of information as errors are on both sides—positive and negative—and
    tend to average out, which is what we can see here. Let's look at a more informative
    metric to evaluate our model. We're going to look at root mean square error as
    a percentage of the actual mean.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate why this is more useful, let''s run the following scenario on
    two sample series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e6c455f-9fd7-426a-ad2c-83d1f49e5d69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, compare that to the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8601f8fe-5653-4dea-acef-3aa10929af7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, the latter is the more meaningful statistic. Now, let''s run it for
    our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31be237c-21be-42c2-8426-e4c19010c4ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Suddenly, our awesome model looks a lot less awesome. Let''s take a look at
    some of the predictions our model made versus the actual values that can be seen
    in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/651e35ce-564f-46c1-9ab5-84c5368adffc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on what we can see here, the model—at least for this sample—tends to
    modestly underpredict the virality of the typical article, but then heavily underpredicts
    the virality for a small number. Let''s see what those are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc0d413e-750f-4ab1-80ac-4ed042511fca.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, we can see that an article on *Malala* and an article
    on *a husband complaining about how much his stay-at-home wife costs him* greatly
    overshot the predicted numbers of our model. Both would seem to have high emotional
    valence.
  prefs: []
  type: TYPE_NORMAL
- en: Adding new features to our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s add another feature to our model. Let''s see whether adding the
    counts for words will help our model. We''ll use a `CountVectorizer` to do this.
    Much like what we did with the site names, we''ll be transforming individual words
    and n-grams into features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding lines, we have joined our existing features to our new n-gram
    features. Let''s train our model and see whether we have any improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6334cdc0-b783-401e-b824-f4f149adc245.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And if we check our error again, we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f1d6244-a80d-4cfa-8456-5544081a5801.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So it appears that we have a modestly improved model. Let''s add one more feature
    to our model—the word count of the title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f26bec4-8c20-45ec-b2ec-44f44c37328f.png)'
  prefs: []
  type: TYPE_IMG
- en: It appears that each feature has modestly improved our model. There are certainly
    more features we could add to it. For example, we could add the day of the week
    and the hour of the posting, we could determine whether the article is a listicle
    by running a regex on the headline, or we could examine the sentiment of each
    article. But this only just touches on the features that could be important for
    modeling virality. We would certainly need to go much further to continue reducing
    the number of errors in our model.
  prefs: []
  type: TYPE_NORMAL
- en: I should also note that we have done only the most cursory testing of our model.
    Each measurement should be run multiple times to get a more accurate representation
    of the actual error rate. It is possible that there is no statistically discernible
    difference between our last two models since we only performed one test.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we examined what the common features of viral content are and
    how we can build a model to predict virality using a random forest regression.
    We also learned how to combine multiple types of features and how to split our
    model into training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you will take what you've learned here to build the next viral empire.
    If that doesn't work out, perhaps the next chapter on mastering the stock market
    will.
  prefs: []
  type: TYPE_NORMAL

["```py\n>>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> X = iris.data[:, 2:4]\n>>> y = iris.target \n```", "```py\n>>> import numpy as np\n>>> from matplotlib import pyplot as plt\n>>> plt.scatter(X[:,0], X[:,1], c=y)\n>>> plt.show() \n```", "```py\n>>> k = 3\n>>> np.random.seed(0)\n>>> random_index = np.random.choice(range(len(X)), k)\n>>> centroids = X[random_index] \n```", "```py\n>>> def visualize_centroids(X, centroids):\n...     plt.scatter(X[:, 0], X[:, 1])\n...     plt.scatter(centroids[:, 0], centroids[:, 1], marker='*',\n                                             s=200, c='#050505')\n...     plt.show()\n>>> visualize_centroids(X, centroids) \n```", "```py\n>>> def dist(a, b):\n...     return np.linalg.norm(a - b, axis=1) \n```", "```py\n>>> def assign_cluster(x, centroids):\n...     distances = dist(x, centroids)\n...     cluster = np.argmin(distances)\n...     return cluster \n```", "```py\n>>> def update_centroids(X, centroids, clusters):\n...     for i in range(k):\n...         cluster_i = np.where(clusters == i)\n...         centroids[i] = np.mean(X[cluster_i], axis=0) \n```", "```py\n>>> tol = 0.0001\n>>> max_iter = 100 \n```", "```py\n>>> iter = 0\n>>> centroids_diff = 100000\n>>> clusters = np.zeros(len(X)) \n```", "```py\n>>> from copy import deepcopy\n>>> while iter < max_iter and centroids_diff > tol:\n...     for i in range(len(X)):\n...         clusters[i] = assign_cluster(X[i], centroids)\n...     centroids_prev = deepcopy(centroids)\n...     update_centroids(X, centroids, clusters)\n...     iter += 1\n...     centroids_diff = np.linalg.norm(centroids -\n                                       centroids_prev)\n...     print('Iteration:', str(iter))\n...     print('Centroids:\\n', centroids)\n...     print(f'Centroids move: {centroids_diff:5.4f}')\n...     visualize_centroids(X, centroids) \n```", "```py\n    Iteration: 1\n    Centroids:\n    [[1.462      0.246     ]\n    [5.80285714 2.11142857]\n    [4.42307692 1.44153846]]\n    Centroids move: 0.8274 \n    ```", "```py\n    Iteration: 2\n    Centroids:\n    [[1.462      0.246     ]\n    [5.73333333 2.09487179]\n    [4.37704918 1.40819672]]\n    Centroids move: 0.0913 \n    ```", "```py\n    Iteration: 6\n    Centroids:\n    [[1.462      0.246     ]\n    [5.62608696 2.04782609]\n    [4.29259259 1.35925926]]\n    Centroids move: 0.0225 \n    ```", "```py\n    Iteration: 7\n    Centroids:\n    [[1.462      0.246     ]\n    [5.62608696 2.04782609]\n    [4.29259259 1.35925926]]\n    Centroids move: 0.0000 \n    ```", "```py\n>>> plt.scatter(X[:, 0], X[:, 1], c=clusters)\n>>> plt.scatter(centroids[:, 0], centroids[:, 1], marker='*',\n                                           s=200, c='r')\n>>> plt.show() \n```", "```py\n    >>> from sklearn.cluster import KMeans\n    >>> kmeans_sk = KMeans(n_clusters=3, n_init='auto', random_state=42) \n    ```", "```py\n    >>> kmeans_sk.fit(X) \n    ```", "```py\n    >>> clusters_sk = kmeans_sk.labels_\n    >>> centroids_sk = kmeans_sk.cluster_centers_ \n    ```", "```py\n    >>> plt.scatter(X[:, 0], X[:, 1], c=clusters_sk)\n    >>> plt.scatter(centroids_sk[:, 0], centroids_sk[:, 1], marker='*', s=200, c='r')\n    >>> plt.show() \n    ```", "```py\n>>> X = iris.data\n>>> y = iris.target\n>>> k_list = list(range(1, 7))\n>>> sse_list = [0] * len(k_list) \n```", "```py\n>>> for k_ind, k in enumerate(k_list):\n...     kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n...     kmeans.fit(X)\n...     clusters = kmeans.labels_\n...     centroids = kmeans.cluster_centers_\n...     sse = 0\n...     for i in range(k):\n...         cluster_i = np.where(clusters == i)\n...         sse += np.linalg.norm(X[cluster_i] - centroids[i])\n...     print(f'k={k}, SSE={sse}')\n...     sse_list[k_ind] = sse\nk=1, SSE=26.103076447039722\nk=2, SSE=16.469773740281195\nk=3, SSE=15.089477089696558\nk=4, SSE=15.0307321707491\nk=5, SSE=14.858930749063735\nk=6, SSE=14.883090350867239 \n```", "```py\n>>> plt.plot(k_list, sse_list)\n>>> plt.show() \n```", "```py\n>>> from sklearn.datasets import fetch_20newsgroups\n>>> categories = [\n...     'alt.atheism',\n...     'talk.religion.misc',\n...     'comp.graphics',\n...     'sci.space',\n... ]\n>>> groups = fetch_20newsgroups(subset='all',\n                                categories=categories)\n>>> labels = groups.target\n>>> label_names = groups.target_names\n>>> from nltk.corpus import names\n>>> from nltk.stem import WordNetLemmatizer\n>>> all_names = set(names.words())\n>>> lemmatizer = WordNetLemmatizer()\n>>> def get_cleaned_data(groups, lemmatizer, remove_words):\n        data_cleaned = []\n        for doc in groups.data:\n...         doc = doc.lower()\n...         doc_cleaned = ' '.join(lemmatizer.lemmatize(word)\n                                  for word in doc.split()\n                                  if word.isalpha() and\n                                  word not in remove_words)\n...         data_cleaned.append(doc_cleaned)\n...     return data_cleaned\n>>> data_cleaned = get_cleaned_data(groups, lemmatizer, all_names) \n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> count_vector = CountVectorizer(stop_words=\"english\",\n                        max_features=None, max_df=0.5, min_df=2)\n>>> data_cv = count_vector.fit_transform(data_cleaned) \n```", "```py\n>>> k = 4\n>>> kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n>>> kmeans.fit(data_cv) \n```", "```py\n>>> clusters = kmeans.labels_\n>>> from collections import Counter\n>>> print(Counter(clusters))\nCounter({3: 3360, 0: 17, 1: 7, 2: 3}) \n```", "```py\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> tfidf_vector = TfidfVectorizer(stop_words='english',\n                                  max_features=None, max_df=0.5, min_df=2) \n```", "```py\n>>> data_tv = tfidf_vector.fit_transform(data_cleaned)\n>>> kmeans.fit(data_tv)\n>>> clusters = kmeans.labels_\n>>> print(Counter(clusters))\nCounter({1: 1478, 2: 797, 3: 601, 0: 511}) \n```", "```py\n>>> cluster_label = {i: labels[np.where(clusters == i)] for i in\n                                                        range(k)}\n>>> terms = tfidf_vector.get_feature_names_out()\n>>> centroids = kmeans.cluster_centers_\n>>> for cluster, index_list in cluster_label.items():\n...     counter = Counter(cluster_label[cluster])\n...     print(f'cluster_{cluster}: {len(index_list)} samples')\n...     for label_index, count in sorted(counter.items(),\n                               key=lambda x: x[1], reverse=True):\n...         print(f'- {label_names[label_index]}: {count} samples')\n...     print('Top 10 terms:')\n...     for ind in centroids[cluster].argsort()[-10:]:\n...         print(' %s' % terms[ind], end=\"\")\n...     print()\ncluster_0: 601 samples\n- sci.space: 598 samples\n- alt.atheism: 1 samples\n- talk.religion.misc: 1 samples\n- comp.graphics: 1 samples\nTop 10 terms: just orbit moon hst nasa mission launch wa shuttle space\ncluster_1: 1478 samples\n- alt.atheism: 522 samples\n- talk.religion.misc: 387 samples\n- sci.space: 338 samples\n- comp.graphics: 231 samples\nTop 10 terms: say people know like think ha just university wa article\ncluster_2: 797 samples\n- comp.graphics: 740 samples\n- sci.space: 49 samples\n- talk.religion.misc: 5 samples\n- alt.atheism: 3 samples\nTop 10 terms: computer need know looking thanks university program file graphic image\ncluster_3: 511 samples\n- alt.atheism: 273 samples\n- talk.religion.misc: 235 samples\n- sci.space: 2 samples\n- comp.graphics: 1 samples\nTop 10 terms: doe bible think believe say people christian jesus wa god \n```", "```py\n>>> keywords = ' '.join(\n                      terms[ind] for ind in centroids[0].argsort()[-100:])\n>>> print(keywords)\nbig power vehicle using alaska look mass money marketing company loss pluto russian scheduled office express probably research software funding billboard online pat access doe telescope april jet usa digest light want prize forwarded way large mar project sci center command technology air government commercial good work servicing know going comet world propulsion people idea design data university day international use orbital long science need time sky program thing make spencer new year earth spacecraft flight henry billion rocket think ha station lunar solar like cost satellite article toronto zoology just orbit moon hst nasa mission launch wa shuttle space \n```", "```py\n    pip install openai \n    ```", "```py\nconda install openai \n```", "```py\n    >>> import openai\n    >>> openai.api_key = '<YOUR API KEY>' \n    ```", "```py\n    >>> def get_completion(prompt, model=\"text-davinci-003\"):\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=messages,\n            temperature=0\n        )\n        return response.choices[0].message[\"content\"] \n    ```", "```py\n    >>> response = get_completion(f\"Describe a common topic based on the \n        following keywords: {keywords}\")\n    >>> print(response) \n    ```", "```py\n>>> from sklearn.decomposition import NMF\n>>> t = 20\n>>> nmf = NMF(n_components=t, random_state=42) \n```", "```py\n>>> nmf.fit(data_cv) \n```", "```py\n>>> print(nmf.components_)\n[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n  0.00000000e+00 1.82524532e-04]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n  7.77697392e-04 3.85995474e-03]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n  0.00000000e+00 0.00000000e+00]\n ...\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.71332203e-02\n  0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n  0.00000000e+00 4.31048632e-05]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n  0.00000000e+00 0.00000000e+00]] \n```", "```py\n>>> terms_cv = count_vector.get_feature_names_out()\n>>> for topic_idx, topic in enumerate(nmf.components_):\n...         print(\"Topic {}:\" .format(topic_idx))\n...         print(\" \".join([terms_cv[i] for i in topic.argsort()[-10:]]))\nTopic 0:\navailable quality program free color version gif file image jpeg\nTopic 1:\nha article make know doe say like just people think\nTopic 2:\ninclude available analysis user software ha processing data tool image\nTopic 3:\natmosphere kilometer surface ha earth wa planet moon spacecraft solar\nTopic 4:\ncommunication technology venture service market ha commercial space satellite launch\nTopic 5:\nverse wa jesus father mormon shall unto mcconkie lord god\nTopic 6:\nformat message server object image mail file ray send graphic\nTopic 7:\nchristian people doe atheism believe religion belief religious god atheist\nTopic 8:\nfile graphic grass program ha package ftp available image data\nTopic 9:\nspeed material unified star larson book universe theory physicist physical\nTopic 10:\nplanetary station program group astronaut center mission shuttle nasa space\nTopic 11:\ninfrared high astronomical center acronym observatory satellite national telescope space\nTopic 12:\nused occurs true form ha ad premise conclusion argument fallacy\nTopic 13:\ngospel people day psalm prophecy christian ha matthew wa jesus\nTopic 14:\ndoe word hanging say greek matthew mr act wa juda\nTopic 15:\nsiggraph graphic file information format isbn data image ftp available\nTopic 16:\nvenera mar lunar surface space venus soviet mission wa probe\nTopic 17:\napril book like year time people new did article wa\nTopic 18:\nsite retrieve ftp software data information client database gopher search\nTopic 19:\nuse look xv color make program correction bit gamma image \n```", "```py\nDocument 1: This restaurant is famous for fish and chips.\nDocument 2: I had fish and rice for lunch.\nDocument 3: My sister bought me a cute kitten.\nDocument 4: Some research shows eating too much rice is bad.\nDocument 5: I always forget to feed fish to my cat. \n```", "```py\nTopic 1: 30% fish, 20% chip, 30% rice, 10% lunch, 10% restaurant (which we can interpret Topic 1 to be food related)\nTopic 2: 40% cute, 40% cat, 10% fish, 10% feed (which we can interpret Topic 1 to be about pet) \n```", "```py\nDocument 1: 85% Topic 1, 15% Topic 2\nDocument 2: 88% Topic 1, 12% Topic 2\nDocument 3: 100% Topic 2\nDocument 4: 100% Topic 1\nDocument 5: 33% Topic 1, 67% Topic 2 \n```", "```py\n>>> from sklearn.decomposition import LatentDirichletAllocation\n>>> t = 20\n>>> lda = LatentDirichletAllocation(n_components=t,\n                      learning_method='batch',random_state=42) \n```", "```py\n>>> lda.fit(data_cv) \n```", "```py\n>>> print(lda.components_)\n[[0.05     2.05    2.05    ...   0.05      0.05    0.05 ]\n [0.05     0.05    0.05    ...   0.05      0.05    0.05 ]\n [0.05     0.05    0.05    ...   4.0336285 0.05    0.05 ]\n ...\n [0.05     0.05    0.05    ...   0.05      0.05    0.05 ]\n [0.05     0.05    0.05    ...   0.05      0.05    0.05 ]\n [0.05     0.05    0.05    ...   0.05      0.05    3.05 ]] \n```", "```py\n>>> for topic_idx, topic in enumerate(lda.components_):\n...         print(\"Topic {}:\" .format(topic_idx))\n...         print(\" \".join([terms_cv[i] for i in\n                                   topic.argsort()[-10:]]))\nTopic 0:\natheist doe ha believe say jesus people christian wa god\nTopic 1:\nmoment just adobe want know ha wa hacker article radius\nTopic 2:\ncenter point ha wa available research computer data graphic hst\nTopic 3:\nobjective argument just thing doe people wa think say article\nTopic 4:\ntime like brian ha good life want know just wa\nTopic 5:\ncomputer graphic think know need university just article wa like\nTopic 6:\nfree program color doe use version gif jpeg file image\nTopic 7:\ngamma ray did know university ha just like article wa\nTopic 8:\ntool ha processing using data software color program bit image\nTopic 9:\napr men know ha think woman just university article wa\nTopic 10:\njpl propulsion mission april mar jet command data spacecraft wa\nTopic 11:\nrussian like ha university redesign point option article space station\nTopic 12:\nha van book star material physicist universe physical theory wa\nTopic 13:\nbank doe book law wa article rushdie muslim islam islamic\nTopic 14:\nthink gopher routine point polygon book university article know wa\nTopic 15:\nha rocket new lunar mission satellite shuttle nasa launch space\nTopic 16:\nwant right article ha make like just think people wa\nTopic 17:\njust light space henry wa like zoology sky article toronto\nTopic 18:\ncomet venus solar moon orbit planet earth probe ha wa\nTopic 19:\nsite format image mail program available ftp send file graphic \n```"]